<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JAIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jair---104">JAIR - 104</h2>
<ul>
<li><details>
<summary>
(2021). Constraint-based diversification of JOP gadgets.
<em>JAIR</em>, <em>72</em>, 1471–1505. (<a
href="https://doi.org/10.1613/jair.1.12848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern software deployment process produces software that is uniform, and hence vulnerable to large-scale code-reuse attacks, such as Jump-Oriented Programming (JOP) attacks. Compiler-based diversification improves the resilience and security of software systems by automatically generating different assembly code versions of a given program. Existing techniques are efficient but do not have a precise control over the quality, such as the code size or speed, of the generated code variants.&amp;nbsp; This paper introduces Diversity by Construction (DivCon), a constraint-based compiler approach to software diversification. Unlike previous approaches, DivCon allows users to control and adjust the conflicting goals of diversity and code quality. A key enabler is the use of Large Neighborhood Search (LNS) to generate highly diverse assembly code efficiently. For larger problems, we propose a combination of LNS with a structural decomposition&amp;nbsp; of the problem. To further improve the diversification efficiency of DivCon against JOP attacks, we propose an application-specific distance measure tailored to the characteristics of JOP attacks.&amp;nbsp; We evaluate DivCon with 20 functions from a popular benchmark suite for embedded systems. These experiments show that DivCon&#39;s combination of LNS and our application-specific distance measure generates binary programs that are highly resilient against JOP&amp;nbsp; attacks (they share between 0.15\% to 8\% of JOP gadgets) with an optimality gap of 10\%. Our results confirm that there is a trade-off between the quality of each assembly code version and the diversity of the entire pool of versions. In particular, the experiments&amp;nbsp; show that DivCon is able to generate binary programs that share a very small number of&amp;nbsp; gadgets, while delivering near-optimal code.&amp;nbsp; For constraint programming researchers and practitioners, this paper demonstrates that LNS is a valuable technique for finding diverse solutions. For security researchers and software&amp;nbsp; engineers, DivCon extends the scope of compiler-based diversification to performance-critical and resource-constrained applications. &amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Rodothea Myrsini Tsoupidi and Roberto Castañeda Lozano and Benoit Baudry},
  doi          = {10.1613/jair.1.12848},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1471-1505},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Constraint-based diversification of JOP gadgets},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from disagreement: A survey. <em>JAIR</em>,
<em>72</em>, 1385–1470. (<a
href="https://doi.org/10.1613/jair.1.12752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many tasks in Natural Language Processing (NLP) and Computer Vision (CV) offer evidence that humans disagree, from objective tasks such as part-of-speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises. While most learning in artificial intelligence (AI) still relies on the assumption that a single (gold) interpretation exists for each item, a growing body of research aims to develop learning methods that do not rely on this assumption. In this survey, we review the evidence for disagreements on NLP and CV tasks, focusing on tasks for which substantial datasets containing this information have been created. We discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement. We systematically compare these different approaches by training them with each of the available datasets, considering several ways to evaluate the resulting models. Finally, we discuss the results in depth, focusing on four key research questions, and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions. Our results suggest, first of all, that even if we abandon the assumption of a gold standard, it is still essential to reach a consensus on how to evaluate models. This is because the relative performance of the various training methods is critically affected by the chosen form of evaluation. Secondly, we observed a strong dataset effect. With substantial datasets, providing many judgments by high-quality coders for each item, training directly with soft labels achieved better results than training from aggregated or even gold labels. This result holds for both hard and soft evaluation. But when the above conditions do not hold, leveraging both gold and soft labels generally achieved the best results in the hard evaluation. All datasets and models employed in this paper are freely available as supplementary materials.},
  archive      = {J_JAIR},
  author       = {Alexandra N. Uma and Tommaso Fornaciari and Dirk Hovy and Silviu Paun and Barbara Plank and Massimo Poesio},
  doi          = {10.1613/jair.1.12752},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1385-1470},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning from disagreement: A survey},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The rediscovery hypothesis: Language models need to meet
linguistics. <em>JAIR</em>, <em>72</em>, 1343–1384. (<a
href="https://doi.org/10.1613/jair.1.12788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an ongoing debate in the NLP community whether modern language models contain linguistic knowledge, recovered through so-called probes. In this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the rediscovery hypothesis. In the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. This result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. This framework also provides a metric to measure the impact of linguistic information on the word prediction task. We reinforce our analytical results with various experiments, both on synthetic and on real NLP tasks in English.},
  archive      = {J_JAIR},
  author       = {Vassilina Nikoulina and Maxat Tezekbayev and Nuradil Kozhakhmet and Madina Babazhanova and Matthias Gallé and Zhenisbek Assylbekov},
  doi          = {10.1613/jair.1.12788},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1343-1384},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The rediscovery hypothesis: Language models need to meet linguistics},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum mathematics in artificial intelligence.
<em>JAIR</em>, <em>72</em>, 1307–1341. (<a
href="https://doi.org/10.1613/jair.1.12702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. At the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news. The mathematical techniques underlying both these areas have more in common than is sometimes realized. Vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. Quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks. This paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (AI), particularly in automated reasoning and natural language processing (NLP). Techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. Application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition. Some of these approaches can potentially be implemented on quantum hardware. Many of the practical steps in this implementation are in early stages, and some are already realized. Explaining some of the common mathematical tools can help researchers in both AI and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way.},
  archive      = {J_JAIR},
  author       = {Dominic Widdows and Kirsty Kitto and Trevor Cohen},
  doi          = {10.1613/jair.1.12702},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1307-1341},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Quantum mathematics in artificial intelligence},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A word selection method for producing interpretable
distributional semantic word vectors. <em>JAIR</em>, <em>72</em>,
1281–1305. (<a href="https://doi.org/10.1613/jair.1.13353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributional semantic models represent the meaning of words as vectors. We introduce a selection method to learn a vector space that each of its dimensions is a natural word. The selection method starts from the most frequent words and selects a subset, which has the best performance. The method produces a vector space that each of its dimensions is a word. This is the main advantage of the method compared to fusion methods such as NMF, and neural embedding models. We apply the method to the ukWaC corpus and train a vector space of N=1500 basis words. We report tests results on word similarity tasks for MEN, RG-65, SimLex-999, and WordSim353 gold datasets. Also, results show that reducing the number of basis vectors from 5000 to 1500 reduces accuracy by about 1.5-2\%. So, we achieve good interpretability without a large penalty. Interpretability evaluation results indicate that the word vectors obtained by the proposed method using N=1500 are more interpretable than word embedding models, and the baseline method. We report the top 15 words of 1500 selected basis words in this paper.},
  archive      = {J_JAIR},
  author       = {Atefe Pakzad and Morteza Analoui},
  doi          = {10.1613/jair.1.13353},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1281-1305},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A word selection method for producing interpretable distributional semantic word vectors},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning optimal decision sets and lists with SAT.
<em>JAIR</em>, <em>72</em>, 1251–1279. (<a
href="https://doi.org/10.1613/jair.1.12719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision sets and decision lists are two of the most easily explainable machine learning models. Given the renewed emphasis on explainable machine learning decisions, both of these machine learning models are becoming increasingly attractive, as they combine small size and clear explainability. In this paper, we define size as the total number of literals in the SAT encoding of these rule-based models as opposed to earlier work that concentrates on the number of rules. In this paper, we develop approaches to computing minimum-size “perfect” decision sets and decision lists, which are perfectly accurate on the training data, and minimal in size, making use of modern SAT solving technology. We also provide a new method for determining optimal sparse alternatives, which trade off size and accuracy. The experiments in this paper demonstrate that the optimal decision sets computed by the SAT-based approach are comparable with the best heuristic methods, but much more succinct, and thus, more explainable. We contrast the size and test accuracy of optimal decisions lists versus optimal decision sets, as well as other state-of-the-art methods for determining optimal decision lists. Finally, we examine the size of average explanations generated by decision sets and decision lists.},
  archive      = {J_JAIR},
  author       = {Jinqiang Yu and Alexey Ignatiev and Peter J. Stuckey and Pierre Le Bodic},
  doi          = {10.1613/jair.1.12719},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1251-1279},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning optimal decision sets and lists with SAT},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the online coalition structure generation problem.
<em>JAIR</em>, <em>72</em>, 1215–1250. (<a
href="https://doi.org/10.1613/jair.1.12989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the online version of the coalition structure generation problem, in which agents, corresponding to the vertices of a graph, appear in an online fashion and have to be partitioned into coalitions by an authority (i.e., an online algorithm). When an agent appears, the algorithm has to decide whether to put the agent into an existing coalition or to create a new one containing, at this moment, only her. The decision is irrevocable. The objective is partitioning agents into coalitions so as to maximize the resulting social welfare that is the sum of all coalition values. We consider two cases for the value of a coalition: (1) the sum of the weights of its edges, and (2) the sum of the weights of its edges divided by its size. Coalition structures appear in a variety of application in AI, multi-agent systems, networks, as well as in social networks, data analysis, computational biology, game theory, and scheduling. For each of the coalition value functions we consider the bounded and unbounded cases depending on whether or not the size of a coalition can exceed a given value α. Furthermore, we consider the case of a limited number of coalitions and various weight functions for the edges, i.e., unrestricted, positive and constant weights. We show tight or nearly tight bounds for the competitive ratio in each case.},
  archive      = {J_JAIR},
  author       = {Michele Flammini and Gianpiero Monaco and Luca Moscardelli and Mordechai Shalom and Shmuel Zaks},
  doi          = {10.1613/jair.1.12989},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1215-1250},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the online coalition structure generation problem},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning realistic patterns from visually unrealistic
stimuli: Generalization and data anonymization. <em>JAIR</em>,
<em>72</em>, 1163–1214. (<a
href="https://doi.org/10.1613/jair.1.13252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Good training data is a prerequisite to develop useful Machine Learning applications. However, in many domains existing data sets cannot be shared due to privacy regulations (e.g., from medical studies). This work investigates a simple yet unconventional approach for anonymized data synthesis to enable third parties to benefit from such anonymized data. We explore the feasibility of learning implicitly from visually unrealistic, task-relevant stimuli, which are synthesized by exciting the neurons of a trained deep neural network. As such, neuronal excitation can be used to generate synthetic stimuli. The stimuli data is used to train new classification models. Furthermore, we extend this framework to inhibit representations that are associated with specific individuals. We use sleep monitoring data from both an open and a large closed clinical study, and Electroencephalogram sleep stage classification data, to evaluate whether (1) end-users can create and successfully use customized classification models, and (2) the identity of participants in the study is protected. Extensive comparative empirical investigation shows that different algorithms trained on the stimuli are able to generalize successfully on the same task as the original model. Architectural and algorithmic similarity between new and original models play an important role in performance. For similar architectures, the performance is close to that of using the original data (e.g., Accuracy difference of 0.56\%-3.82\%, Kappa coefficient difference of 0.02-0.08). Further experiments show that the stimuli can provide state-ofthe-art resilience against adversarial association and membership inference attacks.},
  archive      = {J_JAIR},
  author       = {Konstantinos Nikolaidis and Stein Kristiansen and Thomas Plagemann and Vera Goebel and Knut Liestøl and Mohan Kankanhalli and Gunn Marit Traaen and Britt Overland and Harriet Akre and Lars Aakerøy and Sigurd Steinshamn},
  doi          = {10.1613/jair.1.13252},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1163-1214},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning realistic patterns from visually unrealistic stimuli: Generalization and data anonymization},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reasoning with PCP-nets. <em>JAIR</em>, <em>72</em>,
1103–1161. (<a href="https://doi.org/10.1613/jair.1.13009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce PCP-nets, a formalism to model qualitative conditional preferences with probabilistic uncertainty. PCP-nets generalise CP-nets by allowing for uncertainty over the preference orderings. We define and study both optimality and dominance queries in PCP-nets, and we propose a tractable approximation of dominance which we show to be very accurate in our experimental setting. Since PCP-nets can be seen as a way to model a collection of weighted CP-nets, we also explore the use of PCP-nets in a multi-agent context, where individual agents submit CP-nets which are then aggregated into a single PCP-net. We consider various ways to perform such aggregation and we compare them via two notions of scores, based on well known voting theory concepts. Experimental results allow us to identify the aggregation method that better represents the given set of CP-nets and the most efficient dominance procedure to be used in the multi-agent context.},
  archive      = {J_JAIR},
  author       = {Cristina Cornelio and Judy Goldsmith and Umberto Grandi and Nicholas Mattei and Francesca Rossi and K. Brent Venable},
  doi          = {10.1613/jair.1.13009},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1103-1161},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Reasoning with PCP-nets},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teaching people by justifying tree search decisions: An
empirical study in curling. <em>JAIR</em>, <em>72</em>, 1083–1102. (<a
href="https://doi.org/10.1613/jair.1.13219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research note we show that a simple justification system can be used to teach humans non-trivial strategies of the Olympic sport of curling. This is achieved by justifying the decisions of Kernel Regression UCT (KR-UCT), a tree search algorithm that derives curling strategies by playing the game with itself. Given an action returned by KR-UCT and the expected outcome of that action, we use a decision tree to produce a counterfactual justification of KR-UCT’s decision. The system samples other possible outcomes and selects for presentation the outcomes that are most similar to the expected outcome in terms of visual features and most different in terms of expected end-game value. A user study with 122 people shows that the participants who had access to the justifications produced by our system achieved much higher scores in a curling test than those who only observed the decision made by KR-UCT and those with access to the justifications of a baseline system. This is, to the best of our knowledge, the first work showing that a justification system is able to teach humans non-trivial strategies learned by an algorithm operating in self play.},
  archive      = {J_JAIR},
  author       = {Cleyton R. Silva and Michael Bowling and Levi H.S. Lelis},
  doi          = {10.1613/jair.1.13219},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1083-1102},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Teaching people by justifying tree search decisions: An empirical study in curling},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Steady-state planning in expected reward multichain MDPs.
<em>JAIR</em>, <em>72</em>, 1029–1082. (<a
href="https://doi.org/10.1613/jair.1.12611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The planning domain has experienced increased interest in the formal synthesis of decision-making policies. This formal synthesis typically entails finding a policy which satisfies formal specifications in the form of some well-defined logic. While many such logics have been proposed with varying degrees of expressiveness and complexity in their capacity to capture desirable agent behavior, their value is limited when deriving decision-making policies which satisfy certain types of asymptotic behavior in general system models. In particular, we are interested in specifying constraints on the steady-state behavior of an agent, which captures the proportion of time an agent spends in each state as it interacts for an indefinite period of time with its environment. This is sometimes called the average or expected behavior of the agent and the associated planning problem is faced with significant challenges unless strong restrictions are imposed on the underlying model in terms of the connectivity of its graph structure. In this paper, we explore this steady-state planning problem that consists of deriving a decision-making policy for an agent such that constraints on its steady-state behavior are satisfied. A linear programming solution for the general case of multichain Markov Decision Processes (MDPs) is proposed and we prove that optimal solutions to the proposed programs yield stationary policies with rigorous guarantees of behavior.},
  archive      = {J_JAIR},
  author       = {George K. Atia and Andre Beckus and Ismail Alkhouri and Alvaro Velasquez},
  doi          = {10.1613/jair.1.12611},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1029-1082},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Steady-state planning in expected reward multichain MDPs},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph kernels: A survey. <em>JAIR</em>, <em>72</em>,
943–1027. (<a href="https://doi.org/10.1613/jair.1.13225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. During the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. The goal of this survey is to provide a unifying view of the literature on graph kernels. In particular, we present a comprehensive overview of a wide range of graph kernels. Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.},
  archive      = {J_JAIR},
  author       = {Giannis Nikolentzos and Giannis Siglidis and Michalis Vazirgiannis},
  doi          = {10.1613/jair.1.13225},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {943-1027},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Graph kernels: A survey},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible bayesian nonlinear model configuration.
<em>JAIR</em>, <em>72</em>, 901–942. (<a
href="https://doi.org/10.1613/jair.1.13047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression models are used in a wide range of applications providing a powerful scientific tool for researchers from different fields. Linear, or simple parametric, models are often not sufficient to describe complex relationships between input variables and a response. Such relationships can be better described through &amp;nbsp;flexible approaches such as neural networks, but this results in less interpretable models and potential overfitting. Alternatively, specific parametric nonlinear functions can be used, but the specification of such functions is in general complicated. In this paper, we introduce a &amp;nbsp;flexible approach for the construction and selection of highly &amp;nbsp;flexible nonlinear parametric regression models. Nonlinear features are generated hierarchically, similarly to deep learning, but have additional &amp;nbsp;flexibility on the possible types of features to be considered. This &amp;nbsp;flexibility, combined with variable selection, allows us to find a small set of important features and thereby more interpretable models. Within the space of possible functions, a Bayesian approach, introducing priors for functions based on their complexity, is considered. A genetically modified mode jumping Markov chain Monte Carlo algorithm is adopted to perform Bayesian inference and estimate posterior probabilities for model averaging. In various applications, we illustrate how our approach is used to obtain meaningful nonlinear models. Additionally, we compare its predictive performance with several machine learning algorithms. &amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Aliaksandr Hubin and Geir Storvik and Florian Frommlet},
  doi          = {10.1613/jair.1.13047},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {901-942},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Flexible bayesian nonlinear model configuration},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Experimental comparison and survey of twelve time series
anomaly detection algorithms. <em>JAIR</em>, <em>72</em>, 849–899. (<a
href="https://doi.org/10.1613/jair.1.12698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of an anomaly detection method that is optimal for all domains is a myth. Thus, there exists a plethora of anomaly detection methods which increases every year for a wide variety of domains. But a strength can also be a weakness; given this massive library of methods, how can one select the best method for their application? Current literature is focused on creating new anomaly detection methods or large frameworks for experimenting with multiple methods at the same time. However, and especially as the literature continues to expand, an extensive evaluation of every anomaly detection method is simply not feasible. To reduce this evaluation burden, we present guidelines to intelligently choose the optimal anomaly detection methods based on the characteristics the time series displays such as seasonality, trend, level change concept drift, and missing time steps. We provide a comprehensive experimental validation and survey of twelve anomaly detection methods over different time series characteristics to form guidelines based on several metrics: the AUC (Area Under the Curve), windowed F-score, and Numenta Anomaly Benchmark (NAB) scoring model. Applying our methodologies can save time and effort by surfacing the most promising anomaly detection methods instead of experimenting extensively with a rapidly expanding library of anomaly detection methods, especially in an online setting.},
  archive      = {J_JAIR},
  author       = {Cynthia Freeman and Jonathan Merriman and Ian Beaver and Abdullah Mueen},
  doi          = {10.1613/jair.1.12698},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {849-899},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Experimental comparison and survey of twelve time series anomaly detection algorithms},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-aware verifiable RNN-based policies for partially
observable markov decision processes. <em>JAIR</em>, <em>72</em>,
819–847. (<a href="https://doi.org/10.1613/jair.1.12963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially observable Markov decision processes (POMDPs) are models for sequential decision-making under uncertainty and incomplete information. Machine learning methods typically train recurrent neural networks (RNN) as effective representations of POMDP policies that can efficiently process sequential data. However, it is hard to verify whether the POMDP driven by such RNN-based policies satisfies safety constraints, for instance, given by temporal logic specifications. We propose a novel method that combines techniques from machine learning with the field of formal methods: training an RNN-based policy and then automatically extracting a so-called finite-state controller (FSC) from the RNN. Such FSCs offer a convenient way to verify temporal logic constraints. Implemented on a POMDP, they induce a Markov chain, and probabilistic verification methods can efficiently check whether this induced Markov chain satisfies a temporal logic specification. Using such methods, if the Markov chain does not satisfy the specification, a byproduct of verification is diagnostic information about the states in the POMDP that are critical for the specification. The method exploits this diagnostic information to either adjust the complexity of the extracted FSC or improve the policy by performing focused retraining of the RNN. The method synthesizes policies that satisfy temporal logic specifications for POMDPs with up to millions of states, which are three orders of magnitude larger than comparable approaches.},
  archive      = {J_JAIR},
  author       = {Steven Carr and Nils Jansen and Ufuk Topcu},
  doi          = {10.1613/jair.1.12963},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {819-847},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Task-aware verifiable RNN-based policies for partially observable markov decision processes},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label classification neural networks with hard logical
constraints. <em>JAIR</em>, <em>72</em>, 759–818. (<a
href="https://doi.org/10.1613/jair.1.12850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification (MC) is a standard machine learning problem in which a data point can be associated with a set of classes. A more challenging scenario is given by hierarchical multi-label classification (HMC) problems, in which every prediction must satisfy a given set of hard constraints expressing subclass relationships between classes. In this article, we propose C-HMCNN(h), a novel approach for solving HMC problems, which, given a network h for the underlying MC problem, exploits the hierarchy information in order to produce predictions coherent with the constraints and to improve performance. Furthermore, we extend the logic used to express HMC constraints in order to be able to specify more complex relations among the classes and propose a new model CCN(h), which extends C-HMCNN(h) and is again able to satisfy and exploit the constraints to improve performance. We conduct an extensive experimental analysis showing the superior performance of both C-HMCNN(h) and CCN(h) when compared to state-of-the-art models in both the HMC and the general MC setting with hard logical constraints.},
  archive      = {J_JAIR},
  author       = {Eleonora Giunchiglia and Thomas Lukasiewicz},
  doi          = {10.1613/jair.1.12850},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {759-818},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Multi-label classification neural networks with hard logical constraints},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of the impact of randomization of search-control
parameters in monte-carlo tree search. <em>JAIR</em>, <em>72</em>,
717–757. (<a href="https://doi.org/10.1613/jair.1.12065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte-Carlo Tree Search (MCTS) has been applied successfully in many domains, including games. However, its performance is not uniform on all domains, and it also depends on how parameters that control the search are set. Parameter values that are optimal for a task might be sub-optimal for another. In a domain that tackles many games with different characteristics, like general game playing (GGP), selecting appropriate parameter settings is not a trivial task. Games are unknown to the player, thus, finding optimal parameters for a given game in advance is not feasible. Previous work has looked into tuning parameter values online, while the game is being played, showing some promising results. This tuning approach looks for optimal parameter values, balancing exploitation of values that performed well so far in the search and exploration of less sampled values. Continuously changing parameter values while performing the search, combined also with exploration of multiple values, introduces some randomization in the process. In addition, previous research indicates that adding randomization to certain components of MCTS might increase the diversification of the search and improve the performance. Therefore, this article investigates the effect of randomly selecting values for MCTS search-control parameters online among predefined sets of reasonable values. For the GGP domain, this article evaluates four different online parameter randomization strategies by comparing them with other methods to set parameter values: online parameter tuning, offline parameter tuning and sub-optimal parameter choices. Results on a set of 14 heterogeneous abstract games show that randomizing parameter values before each simulation has a positive effect on the search in some of the tested games, with respect to using fixed offline-tuned parameters. Moreover, results show a clear distinction between games for which online parameter tuning works best and games for which online randomization works best. In addition, the overall performance of online parameter randomization is closer to the one of online parameter turning than the one of sub-optimal parameter values, showing that online randomization is a reasonable parameter selection strategy. When analyzing the structure of the search trees generated by agents that use the different parameters selection strategies, it is clear that randomization causes MCTS to become more explorative, which is helpful for alignment games that present many winning paths in their trees. Online parameter tuning, instead, seems more suitable for games that present narrow winning paths and many losing paths.},
  archive      = {J_JAIR},
  author       = {Chiara F. Sironi and Mark H. M. Winands},
  doi          = {10.1613/jair.1.12065},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {717-757},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Analysis of the impact of randomization of search-control parameters in monte-carlo tree search},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Output space entropy search framework for multi-objective
bayesian optimization. <em>JAIR</em>, <em>72</em>, 667–715. (<a
href="https://doi.org/10.1613/jair.1.12966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of black-box multi-objective optimization (MOO) using expensive function evaluations (also referred to as experiments), where the goal is to approximate the true Pareto set of solutions by minimizing the total resource cost of experiments. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. The key challenge is to select the sequence of experiments to uncover high-quality solutions using minimal resources. In this paper, we propose a general framework for solving MOO problems based on the principle of output space entropy (OSE) search: select the experiment that maximizes the information gained per unit resource cost about the true Pareto front. We appropriately instantiate the principle of OSE search to derive efficient algorithms for the following four MOO problem settings: 1) The most basic single-fidelity setting, where experiments are expensive and accurate; 2) Handling black-box constraints which cannot be evaluated without performing experiments; 3) The discrete multi-fidelity setting, where experiments can vary in the amount of resources consumed and their evaluation accuracy; and 4) The continuous-fidelity setting, where continuous function approximations result in a huge space of experiments. Experiments on diverse synthetic and real-world benchmarks show that our OSE search based algorithms improve over state-of-the-art methods in terms of both computational-efficiency and accuracy of MOO solutions.},
  archive      = {J_JAIR},
  author       = {Syrine Belakaria and Aryan Deshwal and Janardhan Rao Doppa},
  doi          = {10.1613/jair.1.12966},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {667-715},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Output space entropy search framework for multi-objective bayesian optimization},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilabel classification with partial abstention:
Bayes-optimal prediction under label independence. <em>JAIR</em>,
<em>72</em>, 613–665. (<a
href="https://doi.org/10.1613/jair.1.12610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to conventional (single-label) classification, the setting of multilabel classification (MLC) allows an instance to belong to several classes simultaneously. Thus, instead of selecting a single class label, predictions take the form of a subset of all labels. In this paper, we study an extension of the setting of MLC, in which the learner is allowed to partially abstain from a prediction, that is, to deliver predictions on some but not necessarily all class labels. This option is useful in cases of uncertainty, where the learner does not feel confident enough on the entire label set. Adopting a decision-theoretic perspective, we propose a formal framework of MLC with partial abstention, which builds on two main building blocks: First, the extension of underlying MLC loss functions so as to accommodate abstention in a proper way, and second the problem of optimal prediction, that is, finding the Bayes-optimal prediction minimizing this generalized loss in expectation. It is well known that different (generalized) loss functions may have different risk-minimizing predictions, and finding the Bayes predictor typically comes down to solving a computationally complexity optimization problem. In the most general case, given a prediction of the (conditional) joint distribution of possible labelings, the minimizer of the expected loss needs to be found over a number of candidates which is exponential in the number of class labels. We elaborate on properties of risk minimizers for several commonly used (generalized) MLC loss functions, show them to have a specific structure, and leverage this structure to devise efficient methods for computing Bayes predictors. Experimentally, we show MLC with partial abstention to be effective in the sense of reducing loss when being allowed to abstain.},
  archive      = {J_JAIR},
  author       = {Vu-Linh Nguyen and Eyke Hüllermeier},
  doi          = {10.1613/jair.1.12610},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {613-665},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Multilabel classification with partial abstention: Bayes-optimal prediction under label independence},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive explanations of plans through model
restrictions. <em>JAIR</em>, <em>72</em>, 533–612. (<a
href="https://doi.org/10.1613/jair.1.12813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In automated planning, the need for explanations arises when there is a mismatch between a proposed plan and the user’s expectation. We frame Explainable AI Planning as an iterative plan exploration process, in which the user asks a succession of contrastive questions that lead to the generation and solution of hypothetical planning problems that are restrictions of the original problem. The object of the exploration is for the user to understand the constraints that govern the original plan and, ultimately, to arrive at a satisfactory plan. We present the results of a user study that demonstrates that when users ask questions about plans, those questions are usually contrastive, i.e. “why A rather than B?”. We use the data from this study to construct a taxonomy of user questions that often arise during plan exploration. Our approach to iterative plan exploration is a process of successive model restriction. Each contrastive user question imposes a set of constraints on the planning problem, leading to the construction of a new hypothetical planning problem as a restriction of the original. Solving this restricted problem results in a plan that can be compared with the original plan, admitting a contrastive explanation. We formally define model-based compilations in PDDL2.1 for each type of constraint derived from a contrastive user question in the taxonomy, and empirically evaluate the compilations in terms of computational complexity. The compilations were implemented as part of an explanation framework supporting iterative model restriction. We demonstrate its benefits in a second user study.},
  archive      = {J_JAIR},
  author       = {Benjamin Krarup and Senka Krivic and Daniele Magazzeni and Derek Long and Michael Cashmore and David E. Smith},
  doi          = {10.1613/jair.1.12813},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {533-612},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Contrastive explanations of plans through model restrictions},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimally deceiving a learning leader in stackelberg games.
<em>JAIR</em>, <em>72</em>, 507–531. (<a
href="https://doi.org/10.1613/jair.1.12542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent results have shown that algorithms for learning the optimal commitment in a Stackelberg game are susceptible to manipulation by the follower. These learning algorithms operate by querying the best responses of the follower, who consequently can deceive the algorithm by using fake best responses, typically by responding according to fake payoffs that are different from the actual ones. For this strategic behavior to be successful, the main challenge faced by the follower is to pinpoint the fake payoffs that would make the learning algorithm output a commitment that benefits them the most. While this problem has been considered before, the related literature has only focused on a simple setting where the follower can only choose from a finite set of payoff matrices, thus leaving the general version of the problem unanswered. In this paper, we fill this gap by showing that it is always possible for the follower to efficiently compute (near-)optimal fake payoffs, for various scenarios of learning interaction between the leader and the follower. Our results also establish an interesting connection between the follower’s deception and the leader’s maximin utility: through deception, the follower can induce almost any (fake) Stackelberg equilibrium if and only if the leader obtains at least their maximin utility in this equilibrium.},
  archive      = {J_JAIR},
  author       = {Georgios Birmpas and Jiarui Gan and Alexandros Hollender and Francisco J. Marmolejo-Cossío and Ninad Rajgopal and Alexandros A. Voudouris},
  doi          = {10.1613/jair.1.12542},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {507-531},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimally deceiving a learning leader in stackelberg games},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal any-angle pathfinding on a sphere. <em>JAIR</em>,
<em>72</em>, 475–505. (<a
href="https://doi.org/10.1613/jair.1.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathfinding in Euclidean space is a common problem faced in robotics and computer&amp;nbsp; games. For long-distance navigation on the surface of the earth or in outer space however,&amp;nbsp; approximating the geometry as Euclidean can be insufficient for real-world applications&amp;nbsp; such as the navigation of spacecraft, aeroplanes, drones and ships. This article describes an any-angle pathfinding algorithm for calculating the shortest path between point pairs&amp;nbsp; over the surface of a sphere. Introducing several novel adaptations, it is shown that Anya&amp;nbsp; as described by Harabor &amp;amp; Grastien for Euclidean space can be extended to Spherical&amp;nbsp; geometry. There, where the shortest-distance line between coordinates is defined instead by a great-circle path, the optimal solution is typically a curved line in Euclidean space.&amp;nbsp; In addition the turning points for optimal paths in Spherical geometry are not necessarily&amp;nbsp; corner points as they are in Euclidean space, as will be shown, making further substantial&amp;nbsp; adaptations to Anya necessary. Spherical Anya returns the optimal path on the sphere,&amp;nbsp; given these different properties of world maps defined in Spherical geometry. It preserves all primary benefits of Anya in Euclidean geometry, namely the Spherical Anya algorithm always returns an optimal path on a sphere and does so entirely on-line, without any&amp;nbsp; preprocessing or large memory overheads. Performance benchmarks are provided for several&amp;nbsp; game maps including Starcraft and Warcraft III as well as for sea navigation on Earth&amp;nbsp; using the NOAA bathymetric dataset. Always returning the shorter path compared with&amp;nbsp; the Euclidean approximation yielded by Anya, Spherical Anya is shown to be faster than&amp;nbsp; Anya for the majority of sea routes and slower for Game Maps and Random Maps.&amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Volodymyr Rospotniuk and Rupert Small},
  doi          = {10.1613/jair.1.12483},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {475-505},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimal any-angle pathfinding on a sphere},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NLP methods for extraction of symptoms from unstructured
data for use in prognostic COVID-19 analytic models. <em>JAIR</em>,
<em>72</em>, 429–474. (<a
href="https://doi.org/10.1613/jair.1.12631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modeling of outcomes based on a patient&#39;s presenting symptoms (symptomatology) can help deliver high quality care and allocate essential resources, which is especially important during the COVID-19 pandemic. Patient symptoms are typically found in unstructured notes, and thus not readily available for clinical decision making. In an attempt to fill this gap, this study compared two methods for symptom extraction from Emergency Department (ED) admission notes. Both methods utilized a lexicon derived by expanding The Center for Disease Control and Prevention&#39;s (CDC) Symptoms of Coronavirus list. The first method utilized a word2vec model to expand the lexicon using a dictionary mapping to the Uni ed Medical Language System (UMLS). The second method utilized the expanded lexicon as a rule-based gazetteer and the UMLS. These methods were evaluated against a manually annotated reference (f1-score of 0.87 for UMLS-based ensemble; and 0.85 for rule-based gazetteer with UMLS). Through analyses of associations of extracted symptoms used as features against various outcomes, salient risks among the population of COVID-19 patients, including increased risk of in-hospital mortality (OR 1.85, p-value &amp;lt; 0.001), were identified for patients presenting with dyspnea. Disparities between English and non-English speaking patients were also identified, the most salient being a concerning finding of opposing risk signals between fatigue and in-hospital mortality (non-English: OR 1.95, p-value = 0.02; English: OR 0.63, p-value = 0.01). While use of symptomatology for modeling of outcomes is not unique, unlike previous studies this study showed that models built using symptoms with the outcome of in-hospital mortality were not significantly different from models using data collected during an in-patient encounter (AUC of 0.9 with 95\% CI of [0.88, 0.91] using only vital signs; AUC of 0.87 with 95\% CI of [0.85, 0.88] using only symptoms). These findings indicate that prognostic models based on symptomatology could aid in extending COVID-19 patient care through telemedicine, replacing the need for in-person options. The methods presented in this study have potential for use in development of symptomatology-based models for other diseases, including for the study of Post-Acute Sequelae of COVID-19 (PASC).},
  archive      = {J_JAIR},
  author       = {Greg M. Silverman and Himanshu S. Sahoo and Nicholas E. Ingraham and Monica Lupei and Michael A. Puskarich and Michael Usher and James Dries and Raymond L. Finzel and Eric Murray and John Sartori and Gyorgy Simon and Rui Zhang and Genevieve B. Melton and Christopher J. Tignanelli and Serguei VS Pakhomov},
  doi          = {10.1613/jair.1.12631},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {429-474},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {NLP methods for extraction of symptoms from unstructured data for use in prognostic COVID-19 analytic models},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of algorithms for black-box safety validation of
cyber-physical systems. <em>JAIR</em>, <em>72</em>, 377–428. (<a
href="https://doi.org/10.1613/jair.1.12716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous cyber-physical systems (CPS) can improve safety and efficiency for safety-critical applications, but require rigorous testing before deployment. The complexity of these systems often precludes the use of formal verification and real-world testing can be too dangerous during development. Therefore, simulation-based techniques have been developed that treat the system under test as a black box operating in a simulated environment. Safety validation tasks include finding disturbances in the environment that cause the system to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. Motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques for CPS with a focus on applied algorithms and their modifications for the safety validation problem. We present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. Problem decomposition techniques are presented to help scale algorithms to large state spaces, which are common for CPS. A brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. Finally, we present a survey of existing academic and commercially available safety validation tools.},
  archive      = {J_JAIR},
  author       = {Anthony Corso and Robert Moss and Mark Koren and Ritchie Lee and Mykel Kochenderfer},
  doi          = {10.1613/jair.1.12716},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {377–428},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A survey of algorithms for black-box safety validation of cyber-physical systems},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sunny-as2: Enhancing SUNNY for algorithm selection.
<em>JAIR</em>, <em>72</em>, 329–376. (<a
href="https://doi.org/10.1613/jair.1.13116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SUNNY is an Algorithm Selection (AS) technique originally tailored for Constraint Programming (CP). SUNNY is based on the k-nearest neighbors algorithm and enables one to schedule, from a portfolio of solvers, a subset of solvers to be run on a given CP problem. This approach has proved to be effective for CP problems. In 2015, the ASlib benchmarks were released for comparing AS systems coming from disparate fields (e.g., ASP, QBF, and SAT) and SUNNY was extended to deal with generic AS problems. This led to the development of sunny-as, a prototypical algorithm selector based on SUNNY for ASlib scenarios. A major improvement of sunny-as, called sunny-as2, was then submitted to the Open Algorithm Selection Challenge (OASC) in 2017, where it turned out to be the best approach for the runtime minimization of decision problems. In this work we present the technical advancements of sunny-as2, by detailing through several empirical evaluations and by providing new insights. Its current version, built on the top of the preliminary version submitted to OASC, is able to outperform sunny-as and other state-of-the-art AS methods, including those who did not attend the challenge.},
  archive      = {J_JAIR},
  author       = {Tong Liu and Roberto Amadini and Maurizio Gabbrielli and Jacopo Mauro},
  doi          = {10.1613/jair.1.13116},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {329-376},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Sunny-as2: Enhancing SUNNY for algorithm selection},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On quantifying literals in boolean logic and its
applications to explainable AI. <em>JAIR</em>, <em>72</em>, 285–328. (<a
href="https://doi.org/10.1613/jair.1.12756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantified Boolean logic results from adding operators to Boolean logic for existentially and universally quantifying variables. This extends the reach of Boolean logic by enabling a variety of applications that have been explored over the decades. The existential quantification of literals (variable states) and its applications have also been studied in the literature. In this paper, we complement this by introducing and studying universal literal quantification and its applications, particularly to explainable AI. We also provide a novel semantics for quantification, discuss the interplay between variable/literal and existential/universal quantification, and identify some classes of Boolean formulas and circuits on which quantification can be done efficiently. Literal quantification is more fine-grained than variable quantification as the latter can be defined in terms of the former, leading to a refinement of quantified Boolean logic with literal quantification as its primitive.},
  archive      = {J_JAIR},
  author       = {Adnan Darwiche and Pierre Marquis},
  doi          = {10.1613/jair.1.12756},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {285-328},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On quantifying literals in boolean logic and its applications to explainable AI},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relevance in belief update. <em>JAIR</em>, <em>72</em>,
251–283. (<a href="https://doi.org/10.1613/jair.1.12772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been pointed out by Katsuno and Mendelzon that the so-called AGM revision operators, defined by Alchourrón, Gärdenfors and Makinson, do not behave well in dynamically-changing applications. On that premise, Katsuno and Mendelzon formally characterized a different type of belief-change operators, typically referred to as KM update operators, which, to this date, constitute a benchmark in belief update. In this article, we show that there exist KM update operators that yield the same counter-intuitive results as any AGM revision operator. Against this non-satisfactory background, we prove that a translation of Parikh’s relevance-sensitive axiom (P), in the realm of belief update, suffices to block this liberal behaviour of KM update operators. It is shown, both axiomatically and semantically, that axiom (P) for belief update, essentially, encodes a type of relevance that acts at the possible-worlds level, in the context of which each possible world is locally modified, in the light of new information. Interestingly, relevance at the possible-worlds level is shown to be equivalent to a form of relevance that acts at the sentential level, by considering the building blocks of relevance to be the sentences of the language. Furthermore, we concretely demonstrate that Parikh’s notion of relevance in belief update can be regarded as (at least a partial) solution to the frame, ramification and qualification problems, encountered in dynamically-changing worlds. Last but not least, a whole new class of well-behaved, relevance-sensitive KM update operators is introduced, which generalize Forbus’ update operator and are perfectly-suited for real-world implementations.},
  archive      = {J_JAIR},
  author       = {Theofanis Aravanis},
  doi          = {10.1613/jair.1.12772},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {251-283},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Relevance in belief update},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A theoretical perspective on hyperdimensional computing.
<em>JAIR</em>, <em>72</em>, 215–249. (<a
href="https://doi.org/10.1613/jair.1.12664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional (HD) computing is a set of neurally inspired methods for obtaining highdimensional, low-precision, distributed representations of data. These representations can be combined with simple, neurally plausible algorithms to effect a variety of information processing tasks. HD computing has recently garnered significant interest from the computer hardware community as an energy-efficient, low-latency, and noise-robust tool for solving learning problems. In this review, we present a unified treatment of the theoretical foundations of HD computing with a focus on the suitability of representations for learning.},
  archive      = {J_JAIR},
  author       = {Anthony Thomas and Sanjoy Dasgupta and Tajana Rosing},
  doi          = {10.1613/jair.1.12664},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {215-249},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A theoretical perspective on hyperdimensional computing},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pure nash equilibria in resource graph games. <em>JAIR</em>,
<em>72</em>, 185–213. (<a
href="https://doi.org/10.1613/jair.1.12668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the existence of pure Nash equilibria in resource graph games, a general class of strategic games succinctly representing the players’ private costs. These games are defined relative to a finite set of resources and the strategy set of each player corresponds to a set of subsets of resources. The cost of a resource is an arbitrary function of the load vector of a certain subset of resources. As our main result, we give complete characterizations of the cost functions guaranteeing the existence of pure Nash equilibria for weighted and unweighted players, respectively. For unweighted players, pure Nash equilibria are guaranteed to exist for any choice of the players’ strategy space if and only if the cost of each resource is an arbitrary function of the load of the resource itself and linear in the load of all other resources where the linear coefficients of mutual influence of different resources are symmetric. This implies in particular that for any other cost structure there is a resource graph game that does not have a pure Nash equilibrium. For weighted games where players have intrinsic weights and the cost of each resource depends on the aggregated weight of its users, pure Nash equilibria are guaranteed to exist if and only if the cost of a resource is linear in all resource loads, and the linear factors of mutual influence are symmetric, or there is no interaction among resources and the cost is an exponential function of the local resource load. We further discuss the computational complexity of pure Nash equilibria in resource graph games showing that for unweighted games where pure Nash equilibria are guaranteed to exist, it is coNP-complete to decide for a given strategy profile whether it is a pure Nash equilibrium. For general resource graph games, we prove that the decision whether a pure Nash equilibrium exists is Σ p 2 -complete.},
  archive      = {J_JAIR},
  author       = {Tobias Harks and Max Klimm and Jannik Matuschke},
  doi          = {10.1613/jair.1.12668},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {185–213},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Pure nash equilibria in resource graph games},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the computational complexity of non-dictatorial
aggregation. <em>JAIR</em>, <em>72</em>, 137–183. (<a
href="https://doi.org/10.1613/jair.1.12476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate when non-dictatorial aggregation is possible from an algorithmic perspective, where non-dictatorial aggregation means that the votes cast by the members of a society can be aggregated in such a way that there is no single member of the society that always dictates the collective outcome. We consider the setting in which the members of a society take a position on a fixed collection of issues, where for each issue several different alternatives are possible, but the combination of choices must belong to a given set X of allowable voting patterns. Such a set X is called a possibility domain if there is an aggregator that is non-dictatorial, operates separately on each issue, and returns values among those cast by the society on each issue. We design a polynomial-time algorithm that decides, given a set X of voting patterns, whether or not X is a possibility domain. Furthermore, if X is a possibility domain, then the algorithm constructs in polynomial time a non-dictatorial aggregator for X. Furthermore, we show that the question of whether a Boolean domain X is a possibility domain is in NLOGSPACE. We also design a polynomial-time algorithm that decides whether X is a uniform possibility domain, that is, whether X admits an aggregator that is non-dictatorial even when restricted to any two positions for each issue. As in the case of possibility domains, the algorithm also constructs in polynomial time a uniform non-dictatorial aggregator, if one exists. Then, we turn our attention to the case where X is given implicitly, either as the set of assignments satisfying a propositional formula, or as a set of consistent evaluations of a sequence of propositional formulas. In both cases, we provide bounds to the complexity of deciding if X is a (uniform) possibility domain. Finally, we extend our results to four types of aggregators that have appeared in the literature: generalized dictatorships, whose outcome is always an element of their input, anonymous aggregators, whose outcome is not affected by permutations of their input, monotone, whose outcome does not change if more individuals agree with it and systematic, which aggregate every issue in the same way.},
  archive      = {J_JAIR},
  author       = {Lefteris Kirousis and Phokion G. Kolaitis and John Livieratos},
  doi          = {10.1613/jair.1.12476},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {137-183},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the computational complexity of non-dictatorial aggregation},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Worst-case bounds on power vs. Proportion in weighted voting
games with an application to false-name manipulation. <em>JAIR</em>,
<em>72</em>, 99–135. (<a
href="https://doi.org/10.1613/jair.1.13136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighted voting games apply to a wide variety of multi-agent settings. They enable the formalization of power indices which quantify the coalitional power of players. We take a novel approach to the study of the power of big vs. small players in these games. We model small (big) players as having single (multiple) votes. The aggregate relative power of big players is measured w.r.t. their votes proportion. For this ratio, we show small constant worst-case bounds for the Shapley-Shubik and the Deegan-Packel indices. In sharp contrast, this ratio is unbounded for the Banzhaf index. As an application, we define a false-name strategic normal form game where each big player may split its votes between false identities, and study its various properties. Together, our results provide foundations for the implications of players’ size, modeled as their ability to split, on their relative power.},
  archive      = {J_JAIR},
  author       = {Yotam Gafni and Ron Lavi and Moshe Tennenholtz},
  doi          = {10.1613/jair.1.13136},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {99-135},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Worst-case bounds on power vs. proportion in weighted voting games with an application to false-name manipulation},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding the hardest formulas for resolution. <em>JAIR</em>,
<em>72</em>, 69–97. (<a
href="https://doi.org/10.1613/jair.1.12589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A CNF formula is harder than another CNF formula with the same number of clauses if it requires a longer resolution proof. In this paper we introduce resolution hardness numbers; they give for m=1,2,... the length of a shortest proof of a hardest formula on m clauses. We compute the first ten resolution hardness numbers, along with the corresponding hardest formulas. To achieve this, we devise a candidate filtering and symmetry breaking search scheme for limiting the number of potential candidates for hardest for- mulas, and an efficient SAT encoding for computing a shortest resolution proof of a given candidate formula.},
  archive      = {J_JAIR},
  author       = {Tomáš Peitl and Stefan Szeider},
  doi          = {10.1613/jair.1.12589},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {69-97},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Finding the hardest formulas for resolution},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semi-exact algorithm for quickly computing a maximum
weight clique in large sparse graphs. <em>JAIR</em>, <em>72</em>, 39–67.
(<a href="https://doi.org/10.1613/jair.1.12327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores techniques to quickly solve the maximum weight clique problem (MWCP) in very large scale sparse graphs. Due to their size, and the hardness of MWCP, it is infeasible to solve many of these graphs with exact algorithms. Although recent heuristic algorithms make progress in solving MWCP in large graphs, they still need considerable time to get a high-quality solution. In this work, we focus on solving MWCP for large sparse graphs within a short time limit. We propose a new method for MWCP which interleaves clique finding with data reduction rules. We propose novel ideas to make this process efficient, and develop an algorithm called FastWClq. Experiments on a broad range of large sparse graphs show that FastWClq finds better solutions than state-of-the-art algorithms while the running time of FastWClq is much shorter than the competitors for most instances. Further, FastWClq proves the optimality of its solutions for roughly half of the graphs, all with at least 105 vertices, with an average time of 21 seconds.},
  archive      = {J_JAIR},
  author       = {Shaowei Cai and Jinkun Lin and Yiyuan Wang and Darren Strash},
  doi          = {10.1613/jair.1.12327},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {39-67},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A semi-exact algorithm for quickly computing a maximum weight clique in large sparse graphs},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing for interpretability in deep neural networks with
tree regularization. <em>JAIR</em>, <em>72</em>, 1–37. (<a
href="https://doi.org/10.1613/jair.1.12558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models have advanced prediction in many domains, but their lack of interpretability&amp;nbsp; remains a key barrier to the adoption in many real world applications. There exists a large&amp;nbsp; body of work aiming to help humans understand these black box functions to varying levels&amp;nbsp; of granularity – for example, through distillation, gradients, or adversarial examples. These&amp;nbsp; methods however, all tackle interpretability as a separate process after training. In this&amp;nbsp; work, we take a different approach and explicitly regularize deep models so that they are&amp;nbsp; well-approximated by processes that humans can step through in little time. Specifically,&amp;nbsp; we train several families of deep neural networks to resemble compact, axis-aligned decision&amp;nbsp; trees without significant compromises in accuracy. The resulting axis-aligned decision&amp;nbsp; functions uniquely make tree regularized models easy for humans to interpret. Moreover,&amp;nbsp; for situations in which a single, global tree is a poor estimator, we introduce a regional tree regularizer that encourages the deep model to resemble a compact, axis-aligned decision&amp;nbsp; tree in predefined, human-interpretable contexts. Using intuitive toy examples, benchmark&amp;nbsp; image datasets, and medical tasks for patients in critical care and with HIV, we demonstrate&amp;nbsp; that this new family of tree regularizers yield models that are easier for humans to simulate&amp;nbsp; than L1 or L2 penalties without sacrificing predictive power.&amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Mike Wu and Sonali Parbhoo and Michael C. Hughes and Volker Roth and Finale Doshi-Velez},
  doi          = {10.1613/jair.1.12558},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-37},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimizing for interpretability in deep neural networks with tree regularization},
  volume       = {72},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trends in integration of vision and language research: A
survey of tasks, datasets, and methods. <em>JAIR</em>, <em>71</em>,
1183–1317. (<a href="https://doi.org/10.1613/jair.1.11688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.},
  archive      = {J_JAIR},
  author       = {Aditya Mogadala and Marimuthu Kalimuthu and Dietrich Klakow},
  doi          = {10.1613/jair.1.11688},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1183-1317},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Trends in integration of vision and language research: A survey of tasks, datasets, and methods},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Socially responsible AI algorithms: Issues, purposes, and
challenges. <em>JAIR</em>, <em>71</em>, 1137–1181. (<a
href="https://doi.org/10.1613/jair.1.12814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI’s indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation. This article appears in the special track on AI &amp;amp; Society.},
  archive      = {J_JAIR},
  author       = {Lu Cheng and Kush R. Varshney and Huan Liu},
  doi          = {10.1613/jair.1.12814},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1137-1181},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Socially responsible AI algorithms: Issues, purposes, and challenges},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic temporal networks with ordinary distributions:
Theory, robustness and expected utility. <em>JAIR</em>, <em>71</em>,
1091–1136. (<a href="https://doi.org/10.1613/jair.1.13019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing works in Probabilistic Simple Temporal Networks (PSTNs) base their frameworks on well-defined, parametric probability distributions. Under the operational contexts of both strong and dynamic control, this paper addresses robustness measure of PSTNs, i.e. the execution success probability, where the probability distributions of the contingent durations are ordinary, not necessarily parametric, nor symmetric (e.g. histograms, PERT), as long as these can be discretized. In practice, one would obtain ordinary distributions by considering empirical observations (compiled as histograms), or even hand-drawn by field experts. In this new realm of PSTNs, we study and formally define concepts such as degree of weak/strong/dynamic controllability, robustness under a predefined dispatching protocol, and introduce the concept of PSTN expected execution utility. We also discuss the limitation of existing controllability levels, and propose new levels within dynamic controllability, to better characterize dynamic controllable PSTNs based on based practical complexity considerations. We propose a novel fixed-parameter pseudo-polynomial time computation method to obtain both the success probability and expected utility measures. We apply our computation method to various PSTN datasets, including realistic planetary exploration scenarios in the context of the Mars 2020 rover. Moreover, we propose additional original applications of the method.},
  archive      = {J_JAIR},
  author       = {Michael Saint-Guillain and Tiago Vaquero and Steve Chien and Jagriti Agrawal and Jordan Abrahams},
  doi          = {10.1613/jair.1.13019},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1091-1136},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Probabilistic temporal networks with ordinary distributions: Theory, robustness and expected utility},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Election manipulation on social networks: Seeding, edge
removal, edge addition. <em>JAIR</em>, <em>71</em>, 1049–1090. (<a
href="https://doi.org/10.1613/jair.1.12826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the election manipulation problem through social influence, where a manipulator exploits a social network to make her most preferred candidate win an election. Influence is due to information in favor of and/or against one or multiple candidates, sent&amp;nbsp; by seeds and spreading through the network according to the independent cascade model.&amp;nbsp; We provide a comprehensive theoretical study of the election control problem, investigating&amp;nbsp; two forms of manipulations: seeding to buy influencers given a social network and removing&amp;nbsp; or adding edges in the social network given the set of the seeds and the information sent.&amp;nbsp; In particular, we study a wide range of cases distinguishing in the number of candidates or&amp;nbsp; the kind of information spread over the network. Our main result shows that the election manipulation problem is not affordable in&amp;nbsp; the worst-case, even when one accepts to get an approximation of the optimal margin of&amp;nbsp; victory, except for the case of seeding when the number of hard-to-manipulate voters is not&amp;nbsp; too large, and the number of uncertain voters is not too small, where we say that a voter&amp;nbsp; that does not vote for the manipulator&#39;s candidate is hard-to-manipulate if there is no way&amp;nbsp; to make her vote for this candidate, and uncertain otherwise. We also provide some results showing the hardness of the problems in special cases.&amp;nbsp; More precisely, in the case of seeding, we show that the manipulation is hard even if the&amp;nbsp; graph is a line and that a large class of algorithms, including most of the approaches&amp;nbsp; recently adopted for social-influence problems (e.g., greedy, degree centrality, PageRank, VoteRank), fails to compute a bounded approximation even on elementary networks, such&amp;nbsp; as undirected graphs with every node having a degree at most two or directed trees. In the&amp;nbsp; case of edge removal or addition, our hardness results also apply to election manipulation&amp;nbsp; when the manipulator has an unlimited budget, being allowed to remove or add an arbitrary&amp;nbsp; number of edges, and to the basic case of social influence maximization/minimization in&amp;nbsp; the restricted case of finite budget. Interestingly, our hardness results for seeding and edge removal/addition still hold&amp;nbsp; in a re-optimization variant, where the manipulator already knows an optimal solution&amp;nbsp; to the problem and computes a new solution once a local modification occurs, e.g., the&amp;nbsp; removal/addition of a single edge.},
  archive      = {J_JAIR},
  author       = {Matteo Castiglioni and Diodato Ferraioli and Nicola Gatti and Giulia Landriani},
  doi          = {10.1613/jair.1.12826},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1049-1090},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Election manipulation on social networks: Seeding, edge removal, edge addition},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bribery and control in stable marriage. <em>JAIR</em>,
<em>71</em>, 993–1048. (<a
href="https://doi.org/10.1613/jair.1.12755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study of external manipulations in Stable Marriage by considering&amp;nbsp; several manipulative actions as well as several manipulation goals. For instance, one goal&amp;nbsp; is to make sure that a given pair of agents is matched in a stable solution, and this may be&amp;nbsp; achieved by the manipulative action of reordering some agents&#39; preference lists. We present&amp;nbsp; a comprehensive study of the computational complexity of all problems arising in this way.&amp;nbsp; We find several polynomial-time solvable cases as well as NP-hard ones. For the NP-hard&amp;nbsp; cases, focusing on the natural parameter &quot;budget&quot; (that is, the number of manipulative&amp;nbsp; actions one is allowed to perform), we also conduct a parameterized complexity analysis&amp;nbsp; and encounter mostly parameterized hardness results.&amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Niclas Boehmer and Robert Bredereck and Klaus Heeger and Rolf Niedermeier},
  doi          = {10.1613/jair.1.12755},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {993-1048},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Bribery and control in stable marriage},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Agent-based markov modeling for improved COVID-19 mitigation
policies. <em>JAIR</em>, <em>71</em>, 953–992. (<a
href="https://doi.org/10.1613/jair.1.12632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The year 2020 saw the covid-19 virus lead to one of the worst global pandemics in history. As a result, governments around the world have been faced with the challenge of protecting public health while keeping the economy running to the greatest extent possible. Epidemiological models provide insight into the spread of these types of diseases and predict the effects of possible intervention policies. However, to date, even the most data-driven intervention policies rely on heuristics. In this paper, we study how reinforcement learning (RL) and Bayesian inference can be used to optimize mitigation policies that minimize economic impact without overwhelming hospital capacity. Our main contributions are (1) a novel agent-based pandemic simulator which, unlike traditional models, is able to model fine-grained interactions among people at specific locations in a community; (2) an RLbased methodology for optimizing fine-grained mitigation policies within this simulator; and (3) a Hidden Markov Model for predicting infected individuals based on partial observations regarding test results, presence of symptoms, and past physical contacts. This article is part of the special track on AI and COVID-19.},
  archive      = {J_JAIR},
  author       = {Roberto Capobianco and Varun Kompella and James Ault and Guni Sharon and Stacy Jong and Spencer Fox and Lauren Meyers and Peter R. Wurman and Peter Stone},
  doi          = {10.1613/jair.1.12632},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {953-992},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Agent-based markov modeling for improved COVID-19 mitigation policies},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating strategic structures in multi-agent inverse
reinforcement learning. <em>JAIR</em>, <em>71</em>, 925–951. (<a
href="https://doi.org/10.1613/jair.1.12594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A core question in multi-agent systems is understanding the motivations for an agent&#39;s actions based on their behavior. Inverse reinforcement learning provides a framework for extracting utility functions from observed agent behavior, casting the problem as finding domain parameters which induce such a behavior from rational decision makers.&amp;nbsp; We show how to efficiently and scalably extend inverse reinforcement learning to multi-agent settings, by reducing the multi-agent problem to N single-agent problems while still satisfying rationality conditions such as strong rationality. However, we observe that rewards learned naively tend to lack insightful structure, which causes them to produce undesirable behavior when optimized in games with different players from those encountered during training. We further investigate conditions under which rewards or utility functions can be precisely identified, on problem domains such as normal-form and Markov games, as well as auctions, where we show we can learn reward functions that properly generalize to new settings.},
  archive      = {J_JAIR},
  author       = {Justin Fu and Andrea Tacchetti and Julien Perolat and Yoram Bachrach},
  doi          = {10.1613/jair.1.12594},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {925-951},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Evaluating strategic structures in multi-agent inverse reinforcement learning},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Set-to-sequence methods in machine learning: A review.
<em>JAIR</em>, <em>71</em>, 885–924. (<a
href="https://doi.org/10.1613/jair.1.12839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modelling and meta-learning to multi-agent strategy games and power grid optimization. Combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. This paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures.},
  archive      = {J_JAIR},
  author       = {Mateusz Jurewicz and Leon Derczynski},
  doi          = {10.1613/jair.1.12839},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {885-924},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Set-to-sequence methods in machine learning: A review},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Merge-and-shrink: A compositional theory of transformations
of factored transition systems. <em>JAIR</em>, <em>71</em>, 781–883. (<a
href="https://doi.org/10.1613/jair.1.12557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The merge-and-shrink framework has been introduced as a general approach for defining abstractions of large state spaces arising in domain-independent planning and related areas. The distinguishing characteristic of the merge-and-shrink approach is that it operates directly on the factored representation of state spaces, repeatedly modifying this representation through transformations such as shrinking (abstracting a factor of the representation), merging (combining two factors), label reduction (abstracting the way in which different factors interact), and pruning (removing states or transitions of a factor). We provide a novel view of the merge-and-shrink framework as a “toolbox” or “algebra” of transformations on factored transition systems, with the construction of abstractions as only one possible application. For each transformation, we study desirable properties such as conservativeness (overapproximating the original transition system), inducedness (absence of spurious states and transitions), and refinability (reconstruction of paths in the original transition system from the transformed one). We provide the first complete characterizations of the conditions under which these desirable properties can be achieved. We also provide the first full formal account of factored mappings, the mechanism used within the merge-and-shrink framework to establish the relationship between states in the original and transformed factored transition system. Unlike earlier attempts to develop a theory for merge-and-shrink, our approach is fully compositional: the properties of a sequence of transformations can be entirely understood by the properties of the individual transformations involved. This aspect is key to the use of merge-and-shrink as a general toolbox for transforming factored transition systems. New transformations can easily be added to our theory, with compositionality taking care of the seamless integration with the existing components. Similarly, new properties of transformations can be integrated into the theory by showing their compositionality and studying under which conditions they are satisfied by the building blocks of merge-and-shrink.},
  archive      = {J_JAIR},
  author       = {Silvan Sievers and Malte Helmert},
  doi          = {10.1613/jair.1.12557},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {781-883},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Merge-and-shrink: A compositional theory of transformations of factored transition systems},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dimensional inconsistency measures and postulates in
spatio-temporal databases. <em>JAIR</em>, <em>71</em>, 733–780. (<a
href="https://doi.org/10.1613/jair.1.12435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of managing spatio-temporal data arises in many applications, such as location-based services, environmental monitoring, geographic information systems, and many others. Often spatio-temporal data arising from such applications turn out to be inconsistent, i.e., representing an impossible situation in the real world. Though several inconsistency measures have been proposed to quantify in a principled way inconsistency in propositional knowledge bases, little effort has been done so far on inconsistency measures tailored for the spatio-temporal setting. In this paper, we define and investigate new measures that are particularly suitable for dealing with inconsistent spatio-temporal information, because they explicitly take into account the spatial and temporal dimensions, as well as the dimension concerning the identifiers of the monitored objects. Specifically, we first define natural measures that look at individual dimensions (time, space, and objects), and then propose measures based on the notion of a repair. We then analyze their behavior w.r.t. common postulates defined for classical propositional knowledge bases, and find that the latter are not suitable for spatio-temporal databases, in that the proposed inconsistency measures do not often satisfy them. In light of this, we argue that also postulates should explicitly take into account the spatial, temporal, and object dimensions and thus define “dimension-aware” counterparts of common postulates, which are indeed often satisfied by the new inconsistency measures. Finally, we study the complexity of the proposed inconsistency measures.},
  archive      = {J_JAIR},
  author       = {John Grant and Maria Vanina Martinez and Cristian Molinaro and Francesco Parisi},
  doi          = {10.1613/jair.1.12435},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {733–780},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Dimensional inconsistency measures and postulates in spatio-temporal databases},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Goal recognition for deceptive human agents through planning
and gaze. <em>JAIR</em>, <em>71</em>, 697–732. (<a
href="https://doi.org/10.1613/jair.1.12518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye gaze has the potential to provide insight into the minds of individuals, and this idea has been used in prior research to improve human goal recognition by combining human&#39;s actions and gaze. However, most existing research assumes that people are rational and honest. In adversarial scenarios, people may deliberately alter their actions and gaze, which presents a challenge to goal recognition systems. In this paper, we present new models for goal recognition under deception using a combination of gaze behaviour and observed movements of the agent. These models aim to detect when a person is deceiving by analysing their gaze patterns and use this information to adjust the goal recognition. We evaluated our models in two human-subject studies: (1) using data collected from 30 individuals playing a navigation game inspired by an existing deception study and (2) using data collected from 40 individuals playing a competitive game (Ticket To Ride). We found that one of our models (Modulated Deception Gaze+Ontic) offers promising results compared to the previous state-of-the-art model in both studies. Our work complements existing adversarial goal recognition systems by equipping these systems with the ability to tackle ambiguous gaze behaviours.},
  archive      = {J_JAIR},
  author       = {Thao Le and Ronal Singh and Tim Miller},
  doi          = {10.1613/jair.1.12518},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {697-732},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Goal recognition for deceptive human agents through planning and gaze},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the effectiveness and efficiency of stochastic
neighbour embedding with isolation kernel. <em>JAIR</em>, <em>71</em>,
667–695. (<a href="https://doi.org/10.1613/jair.1.12904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new insight into improving the performance of Stochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of Gaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects. First, the use of Isolation kernel in t-SNE overcomes the drawback of misrepresenting some structures in the data, which often occurs when Gaussian kernel is applied in t-SNE. This is because Gaussian kernel determines each local bandwidth based on one local point only, while Isolation kernel is derived directly from the data based on space partitioning. Second, the use of Isolation kernel yields a more efficient similarity computation because data-dependent Isolation kernel has only one parameter that needs to be tuned. In contrast, the use of data-independent Gaussian kernel increases the computational cost by determining n bandwidths for a dataset of n points. As the root cause of these deficiencies in t-SNE is Gaussian kernel, we show that simply replacing Gaussian kernel with Isolation kernel in t-SNE significantly improves the quality of the final visualisation output (without creating misrepresented structures) and removes one key obstacle that prevents t-SNE from processing large datasets. Moreover, Isolation kernel enables t-SNE to deal with large-scale datasets in less runtime without trading off accuracy, unlike existing methods in speeding up t-SNE.},
  archive      = {J_JAIR},
  author       = {Ye Zhu and Kai Ming Ting},
  doi          = {10.1613/jair.1.12904},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {667-695},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improving the effectiveness and efficiency of stochastic neighbour embedding with isolation kernel},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ethics and governance of artificial intelligence: Evidence
from a survey of machine learning researchers. <em>JAIR</em>,
<em>71</em>, 591–666. (<a
href="https://doi.org/10.1613/jair.1.12895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) and artificial intelligence (AI) researchers play an important role in the ethics and governance of AI, including through their work, advocacy, and choice of employment. Nevertheless, this influential group&#39;s attitudes are not well understood, undermining our ability to discern consensuses or disagreements between AI/ML researchers. To examine these researchers&#39; views, we conducted a survey of those who published in two top AI/ML conferences (N = 524). We compare these results with those from a 2016 survey of AI/ML researchers (Grace et al., 2018) and a 2018 survey of the US public (Zhang &amp;amp; Dafoe, 2020). We find that AI/ML researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of AI in the public interest; moderate trust in most Western tech companies; and low trust in national militaries, Chinese tech companies, and Facebook. While the respondents were overwhelmingly opposed to AI/ML researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of AI, particularly logistics algorithms. A strong majority of respondents think that AI safety research should be prioritized and that ML institutions should conduct pre-publication review to assess potential harms. Being closer to the technology itself, AI/ML researchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. The findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for AI. This article appears in the special track on AI &amp;amp; Society.},
  archive      = {J_JAIR},
  author       = {Baobao Zhang and Markus Anderljung and Lauren Kahn and Noemi Dreksler and Michael C. Horowitz and Allan Dafoe},
  doi          = {10.1613/jair.1.12895},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {591–666},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Ethics and governance of artificial intelligence: Evidence from a survey of machine learning researchers},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conceptual modeling of explainable recommender systems: An
ontological formalization to guide their design and development.
<em>JAIR</em>, <em>71</em>, 557–589. (<a
href="https://doi.org/10.1613/jair.1.12789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. This is one of the main goals of recommender systems. However, users’ trust may be compromised if they do not understand how or why the recommendation was achieved. Here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful. Providing explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation’s goal, the user’s expectation, the knowledge available, or the presentation method. Therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. Our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user’s needs. Although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. Moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems.},
  archive      = {J_JAIR},
  author       = {Marta Caro-Martínez and Guillermo Jiménez-Díaz and Juan A. Recio-García},
  doi          = {10.1613/jair.1.12789},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {557-589},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Conceptual modeling of explainable recommender systems: An ontological formalization to guide their design and development},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligence in strategic games. <em>JAIR</em>, <em>71</em>,
521–556. (<a href="https://doi.org/10.1613/jair.1.12883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If an agent, or a coalition of agents, has a strategy, knows that she has a strategy, and knows what the strategy is, then she has a know-how strategy. Several modal logics of coalition power for know-how strategies have been studied before. The contribution of the article is three-fold. First, it proposes a new class of know-how strategies that depend on the intelligence information about the opponents’ actions. Second, it shows that the coalition power modality for the proposed new class of strategies cannot be expressed through the standard know-how modality. Third, it gives a sound and complete logical system that describes the interplay between the coalition power modality with intelligence and the distributed knowledge modality in games with imperfect information.},
  archive      = {J_JAIR},
  author       = {Pavel Naumov and Yuan Yuan},
  doi          = {10.1613/jair.1.12883},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {521-556},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Intelligence in strategic games},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EpidemiOptim: A toolbox for the optimization of control
policies in epidemiological models. <em>JAIR</em>, <em>71</em>, 479–519.
(<a href="https://doi.org/10.1613/jair.1.12588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the dynamics of epidemics helps to propose control strategies based on pharmaceuticaland non-pharmaceutical interventions (contact limitation, lockdown, vaccination,etc). Hand-designing such strategies is not trivial because of the number of possibleinterventions and the difficulty to predict long-term effects. This task can be cast as an optimization problem where state-of-the-art machine learning methods such as deep reinforcement learning might bring significant value. However, the specificity of each domain|epidemic modeling or solving optimization problems|requires strong collaborationsbetween researchers from different fields of expertise. This is why we introduce EpidemiOptim, a Python toolbox that facilitates collaborations between researchers inepidemiology and optimization. EpidemiOptim turns epidemiological models and cost functions into optimization problems via a standard interface commonly used by optimization practitioners (OpenAI Gym). Reinforcement learning algorithms based on QLearning with deep neural networks (DQN) and evolutionary algorithms (NSGA-II) are already implemented. We illustrate the use of EpidemiOptim to find optimal policies fordynamical on-o  lockdown control under the optimization of the death toll and economic recess using a Susceptible-Exposed-Infectious-Removed (SEIR) model for COVID-19. Using EpidemiOptim and its interactive visualization platform in Jupyter notebooks, epidemiologists, optimization practitioners and others (e.g. economists) can easily compare epidemiological models, costs functions and optimization algorithms to address important choicesto be made by health decision-makers. Trained models can be explored by experts and non-experts via a web interface. This article is part of the special track on AI and COVID-19.},
  archive      = {J_JAIR},
  author       = {Cédric Colas and Boris Hejblum and Sebastien Rouillon and Rodolphe Thiébaut and Pierre-Yves Oudeyer and Clément Moulin-Frier and Mélanie Prague},
  doi          = {10.1613/jair.1.12588},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {479-519},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {EpidemiOptim: A toolbox for the optimization of control policies in epidemiological models},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confronting abusive language online: A survey from the
ethical and human rights perspective. <em>JAIR</em>, <em>71</em>,
431–478. (<a href="https://doi.org/10.1613/jair.1.12590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. We review a large body of NLP research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. In many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. We highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including ‘nudging’, ‘quarantining’, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including &#39;nudging&#39;, &#39;quarantining&#39;, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.},
  archive      = {J_JAIR},
  author       = {Svetlana Kiritchenko and Isar Nejadgholi and Kathleen C. Fraser},
  doi          = {10.1613/jair.1.12590},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {431-478},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Confronting abusive language online: A survey from the ethical and human rights perspective},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Representative committees of peers. <em>JAIR</em>,
<em>71</em>, 401–429. (<a
href="https://doi.org/10.1613/jair.1.12521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A population of voters must elect representatives among themselves to decide on a sequence of possibly unforeseen binary issues. Voters care only about the final decision, not the elected representatives. The disutility of a voter is proportional to the fraction of issues, where his preferences disagree with the decision. While an issue-by-issue vote by all voters would maximize social welfare, we are interested in how well the preferences of the population can be approximated by a small committee. We show that a k-sortition (a random committee of k voters with the majority vote within the committee) leads to an outcome within the factor 1+O(1/√ k) of the optimal social cost for any number of voters n, any number of issues&amp;nbsp;m, and any preference profile.&amp;nbsp;For a small number of issues m, the social cost can be made even closer to optimal by delegation procedures that weigh committee members according to their number of followers. However, for large m, we demonstrate that the k-sortition is the worst-case optimal rule within a broad family of committee-based rules that take into account metric information about the preference profile of the whole population.},
  archive      = {J_JAIR},
  author       = {Reshef Meir and Fedor Sandomirskiy and Moshe Tennenholtz},
  doi          = {10.1613/jair.1.12521},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {401-429},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Representative committees of peers},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-document summarization with determinantal point
process attention. <em>JAIR</em>, <em>71</em>, 371–399. (<a
href="https://doi.org/10.1613/jair.1.12522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to convey relevant and diverse information is critical in multi-document summarization and yet remains elusive for neural seq-to-seq models whose outputs are often redundant and fail to correctly cover important details. In this work, we propose an attention mechanism which encourages greater focus on relevance and diversity. Attention weights are computed based on (proportional) probabilities given by Determinantal Point Processes (DPPs) defined on the set of content units to be summarized. DPPs have been successfully used in extractive summarisation, here we use them to select relevant and diverse content for neural abstractive summarisation. We integrate DPP-based attention with various seq-to-seq architectures ranging from CNNs to LSTMs, and Transformers. Experimental evaluation shows that our attention mechanism consistently improves summarization and delivers performance comparable with the state-of-the-art on the MultiNews dataset},
  archive      = {J_JAIR},
  author       = {Laura Perez-Beltrachini and Mirella Lapata},
  doi          = {10.1613/jair.1.12522},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {371-399},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Multi-document summarization with determinantal point process attention},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tight bound for stochastic submodular cover.
<em>JAIR</em>, <em>71</em>, 347–370. (<a
href="https://doi.org/10.1613/jair.1.12368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the Adaptive Greedy algorithm of Golovin and Krause achieves an approximation bound of (ln(Q/η)+1) for Stochastic Submodular Cover: here Q is the “goal value” and η is the minimum gap between Q and any attainable utility value Q&#39;&amp;lt;Q. &amp;nbsp;Although this bound was claimed by Golovin and Krause in the original version of their paper, the proof was later shown to be incorrect by Nan &amp;amp; Saligrama. The subsequent corrected proof of Golovin and Krause gives a quadratic bound of (ln(Q/η)+1)2.&amp;nbsp; A bound of 56(ln(Q/η)+1) is implied by work of Im et al.&amp;nbsp; Other bounds for the problem depend on quantities other than Q and η. Our bound restores the original bound claimed by Golovin and Krause, generalizing the well-known&amp;nbsp; (ln m + 1) approximation bound on the greedy algorithm for the classical Set Cover problem, where m is the size of the ground set.},
  archive      = {J_JAIR},
  author       = {Lisa Hellerstein and Devorah Kletenik and Srinivasan Parthasarathy},
  doi          = {10.1613/jair.1.12368},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {347-370},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A tight bound for stochastic submodular cover},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Playing codenames with language graphs and word embeddings.
<em>JAIR</em>, <em>71</em>, 319–346. (<a
href="https://doi.org/10.1613/jair.1.12665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although board games and video games have been studied for decades in artificial intelligence research, challenging word games remain relatively unexplored. Word games are not as constrained as games like chess or poker. Instead, word game strategy is defined by the players’ understanding of the way words relate to each other. The word game Codenames provides a unique opportunity to investigate common sense understanding of relationships between words, an important open challenge. We propose an algorithm that can generate Codenames clues from the language graph BabelNet or from any of several embedding methods – word2vec, GloVe, fastText or BERT. We introduce a new scoring function that measures the quality of clues, and we propose a weighting term called DETECT that incorporates dictionary-based word representations and document frequency to improve clue selection. We develop BabelNet-Word Selection Framework (BabelNet-WSF) to improve BabelNet clue quality and overcome the computational barriers that previously prevented leveraging language graphs for Codenames. Extensive experiments with human evaluators demonstrate that our proposed innovations yield state-of-the-art performance, with up to 102.8\% improvement in precision@2 in some cases. Overall, this work advances the formal study of word games and approaches for common sense language understanding.},
  archive      = {J_JAIR},
  author       = {Divya Koyyalagunta and Anna Sun and Rachel Lea Draelos and Cynthia Rudin},
  doi          = {10.1613/jair.1.12665},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {319-346},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Playing codenames with language graphs and word embeddings},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Declarative algorithms and complexity results for
assumption-based argumentation. <em>JAIR</em>, <em>71</em>, 265–318. (<a
href="https://doi.org/10.1613/jair.1.12479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of computational models for argumentation is a vibrant area of artificial intelligence and, in particular, knowledge representation and reasoning research. Arguments most often have an intrinsic structure made explicit through derivations from more basic structures. Computational models for structured argumentation enable making the internal structure of arguments explicit. Assumption-based argumentation (ABA) is a central structured formalism for argumentation in AI. In this article, we make both algorithmic and complexity-theoretic advances in the study of ABA. In terms of algorithms, we propose a new approach to reasoning in a commonly studied fragment of ABA (namely the logic programming fragment) with and without preferences. While previous approaches to reasoning over ABA frameworks apply either specialized algorithms or translate ABA reasoning to reasoning over abstract argumentation frameworks, we develop a direct declarative approach to ABA reasoning by encoding ABA reasoning tasks in answer set programming. We show via an extensive empirical evaluation that our approach significantly improves on the empirical performance of current ABA reasoning systems. In terms of computational complexity, while the complexity of reasoning over ABA frameworks is well-understood, the complexity of reasoning in the ABA+ formalism integrating preferences into ABA is currently not fully established. Towards bridging this gap, our results suggest that the integration of preferential information into ABA via so-called reverse attacks results in increased problem complexity for several central argumentation semantics.},
  archive      = {J_JAIR},
  author       = {Tuomo Lehtonen and Johannes P. Wallner and Matti Järvisalo},
  doi          = {10.1613/jair.1.12479},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {265-318},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Declarative algorithms and complexity results for assumption-based argumentation},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RWNE: A scalable random-walk based network embedding
framework with personalized higher-order proximity preserved.
<em>JAIR</em>, <em>71</em>, 237–263. (<a
href="https://doi.org/10.1613/jair.1.12567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order proximity preserved network embedding has attracted increasing attention. In particular, due to the superior scalability, random-walk-based network embedding has also been well developed, which could efficiently explore higher-order neighborhoods via multi-hop random walks. However, despite the success of current random-walk-based methods, most of them are usually not expressive enough to preserve the personalized higher-order proximity and lack a straightforward objective to theoretically articulate what and how network proximity is preserved. In this paper, to address the above issues, we present a general scalable random-walk-based network embedding framework, in which random walk is explicitly incorporated into a sound objective designed theoretically to preserve arbitrary higher-order proximity. Further, we introduce the random walk with restart process into the framework to naturally and effectively achieve personalized-weighted preservation of proximities of different orders. We conduct extensive experiments on several real-world networks and demonstrate that our proposed method consistently and substantially outperforms the state-of-the-art network embedding methods.},
  archive      = {J_JAIR},
  author       = {Jianxin Li and Cheng Ji and Hao Peng and Yu He and Yangqiu Song and Xinmiao Zhang and Fanzhang Peng},
  doi          = {10.1613/jair.1.12567},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {237-263},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {RWNE: A scalable random-walk based network embedding framework with personalized higher-order proximity preserved},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring the occupational impact of AI: Tasks, cognitive
abilities and AI benchmarks. <em>JAIR</em>, <em>71</em>, 191–236. (<a
href="https://doi.org/10.1613/jair.1.12647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we develop a framework for analysing the impact of Artificial Intelligence (AI) on occupations. This framework maps 59 generic tasks from worker surveys and an occupational database to 14 cognitive abilities (that we extract from the cognitive science literature) and these to a comprehensive list of 328 AI benchmarks used to evaluate research intensity across a broad range of different AI areas. The use of cognitive abilities as an intermediate layer, instead of mapping work tasks to AI benchmarks directly, allows for an identification of potential AI exposure for tasks for which AI applications have not been explicitly created. An application of our framework to occupational databases gives insights into the abilities through which AI is most likely to affect jobs and allows for a ranking of occupations with respect to AI exposure. Moreover, we show that some jobs that were not known to be affected by previous waves of automation may now be subject to higher AI exposure. Finally, we find that some of the abilities where AI research is currently very intense are linked to tasks with comparatively limited labour input in the labour markets of advanced economies (e.g., visual and auditory processing using deep learning, and sensorimotor interaction through (deep) reinforcement learning). This article appears in the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Songül Tolan and Annarosa Pesole and Fernando Martínez-Plumed and Enrique Fernández-Macías and José Hernández-Orallo and Emilia Gómez},
  doi          = {10.1613/jair.1.12647},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {191-236},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Measuring the occupational impact of AI: Tasks, cognitive abilities and AI benchmarks},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewpoint: AI as author – bridging the gap between machine
learning and literary theory. <em>JAIR</em>, <em>71</em>, 175–189. (<a
href="https://doi.org/10.1613/jair.1.12593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticipating the rise in Artificial Intelligence’s ability to produce original works of literature, this study suggests that literariness, or that which constitutes a text as literary, is understudied in relation to text generation. From a computational perspective, literature is particularly challenging because it typically employs figurative and ambiguous language. Literary expertise would be beneficial to understanding how meaning and emotion are conveyed in this art form but is often overlooked. We propose placing experts from two dissimilar disciplines – machine learning and literary studies – in conversation to improve the quality of AI writing. Concentrating on evaluation as a vital stage in the text generation process, the study demonstrates that benefit could be derived from literary theoretical perspectives. This knowledge would improve algorithm design and enable a deeper understanding of how AI learns and generates. This article appears in the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Imke van Heerden and Anil Bas},
  doi          = {10.1613/jair.1.12593},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {175-189},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Viewpoint: AI as author – bridging the gap between machine learning and literary theory},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Welfare guarantees in schelling segregation. <em>JAIR</em>,
<em>71</em>, 143–174. (<a
href="https://doi.org/10.1613/jair.1.12771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schelling’s model is an influential model that reveals how individual perceptions and incentives can lead to residential segregation. Inspired by a recent stream of work, we study welfare guarantees and complexity in this model with respect to several welfare measures. First, we show that while maximizing the social welfare is NP-hard, computing an assignment of agents to the nodes of any topology graph with approximately half of the maximum welfare can be done in polynomial time. We then consider Pareto optimality, introduce two new optimality notions based on it, and establish mostly tight bounds on the worst-case welfare loss for assignments satisfying these notions as well as the complexity of computing such assignments. In addition, we show that for tree topologies, it is possible to decide whether there exists an assignment that gives every agent a positive utility in polynomial time; moreover, when every node in the topology has degree at least 2, such an assignment always exists and can be found efficiently.},
  archive      = {J_JAIR},
  author       = {Martin Bullinger and Warut Suksompong and Alexandros A. Voudouris},
  doi          = {10.1613/jair.1.12771},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {143-174},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Welfare guarantees in schelling segregation},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning over no-preferred and preferred sequence of items
for robust recommendation. <em>JAIR</em>, <em>71</em>, 121–142. (<a
href="https://doi.org/10.1613/jair.1.12562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a theoretically supported sequential strategy for training a large-scale Recommender System (RS) over implicit feedback, mainly in the form of clicks. The proposed approach consists in minimizing pairwise ranking loss over blocks of consecutive items constituted by a sequence of non-clicked items followed by a clicked one for each user. We present two variants of this strategy where model parameters are updated using either the momentum method or a gradient-based approach. To prevent updating the parameters for an abnormally high number of clicks over some targeted items (mainly due to bots), we introduce an upper and a lower threshold on the number of updates for each user. These thresholds are estimated over the distribution of the number of blocks in the training set. They affect the decision of RS by shifting the distribution of items that are shown to the users. Furthermore, we provide a convergence analysis of both algorithms and demonstrate their practical efficiency over six large-scale collections with respect to various ranking measures and computational time.},
  archive      = {J_JAIR},
  author       = {Aleksandra Burashnikova and Yury Maximov and Marianne Clausel and Charlotte Laclau and Franck Iutzeler and Massih-Reza Amini},
  doi          = {10.1613/jair.1.12562},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {121-142},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning over no-preferred and preferred sequence of items for robust recommendation},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient local search based on dynamic connectivity
maintenance for minimum connected dominating set. <em>JAIR</em>,
<em>71</em>, 89–119. (<a
href="https://doi.org/10.1613/jair.1.12618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum connected dominating set (MCDS) problem is an important extension of the minimum dominating set problem, with wide applications, especially in wireless networks. Most previous works focused on solving MCDS problem in graphs with relatively small size, mainly due to the complexity of maintaining connectivity. This paper explores techniques for solving MCDS problem in massive real-world graphs with wide practical importance. Firstly, we propose a local greedy construction method with reasoning rule called 1hopReason. Secondly and most importantly, a hybrid dynamic connectivity maintenance method (HDC+) is designed to switch alternately between a novel fast connectivity maintenance method based on spanning tree and its previous counterpart. Thirdly, we adopt a two-level vertex selection heuristic with a newly proposed scoring function called chronosafety to make the algorithm more considerate when selecting vertices. We design a new local search algorithm called FastCDS based on the three ideas. Experiments show that FastCDS significantly outperforms five state-of-the-art MCDS algorithms on both massive graphs and classic benchmarks.},
  archive      = {J_JAIR},
  author       = {Xindi Zhang and Bohan Li and Shaowei Cai and Yiyuan Wang},
  doi          = {10.1613/jair.1.12618},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {89-119},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient local search based on dynamic connectivity maintenance for minimum connected dominating set},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Game plan: What AI can do for football, and what football
can do for AI. <em>JAIR</em>, <em>71</em>, 41–88. (<a
href="https://doi.org/10.1613/jair.1.12505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid progress in artificial intelligence (AI) and machine learning has opened unprecedented analytics possibilities in various team and individual sports, including baseball, basketball, and tennis. More recently, AI techniques have been applied to football, due to a huge increase in data collection by professional teams, increased computational power, and advances in machine learning, with the goal of better addressing new scientific challenges involved in the analysis of both individual players’ and coordinated teams’ behaviors. The research challenges associated with predictive and prescriptive football analytics require new developments and progress at the intersection of statistical learning, game theory, and computer vision. In this paper, we provide an overarching perspective highlighting how the combination of these fields, in particular, forms a unique microcosm for AI research, while offering mutual benefits for professional teams, spectators, and broadcasters in the years to come. We illustrate that this duality makes football analytics a game changer of tremendous value, in terms of not only changing the game of football itself, but also in terms of what this domain can mean for the field of AI. We review the state-of-the-art and exemplify the types of analysis enabled by combining the aforementioned fields, including illustrative examples of counterfactual analysis using predictive models, and the combination of game-theoretic analysis of penalty kicks with statistical learning of player attributes. We conclude by highlighting envisioned downstream impacts, including possibilities for extensions to other sports (real and virtual).},
  archive      = {J_JAIR},
  author       = {Karl Tuyls and Shayegan Omidshafiei and Paul Muller and Zhe Wang and Jerome Connor and Daniel Hennes and Ian Graham and William Spearman and Tim Waskett and Dafydd Steel and Pauline Luc and Adria Recasens and Alexandre Galashov and Gregory Thornton and Romuald Elie and Pablo Sprechmann and Pol Moreno and Kris Cao and Marta Garnelo and Praneet Dutta and Michal Valko and Nicolas Heess and Alex Bridgland and Julien Pérolat and Bart De Vylder and S. M. Ali Eslami and Mark Rowland and Andrew Jaegle and Remi Munos and Trevor Back and Razia Ahamed and Simon Bouton and Nathalie Beauguerlange and Jackson Broshear and Thore Graepel and Demis Hassabis},
  doi          = {10.1613/jair.1.12505},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {41-88},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Game plan: What AI can do for football, and what football can do for AI},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the cluster admission problem for cloud computing.
<em>JAIR</em>, <em>71</em>, 1–40. (<a
href="https://doi.org/10.1613/jair.1.12346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing providers face the problem of matching heterogeneous customer workloads to resources that will serve them. This is particularly challenging if customers, who are already running a job on a cluster, scale their resource usage up and down over time. The provider therefore has to continuously decide whether she can add additional workloads to a given cluster or if doing so would impact existing workloads’ ability to scale. Currently, this is often done using simple threshold policies to reserve large parts of each cluster, which leads to low efficiency (i.e., low average utilization of the cluster). We propose more sophisticated policies for controlling admission to a cluster and demonstrate that they significantly increase cluster utilization. We first introduce the cluster admission problem and formalize it as a constrained Partially Observable Markov Decision Process (POMDP). As it is infeasible to solve the POMDP optimally, we then systematically design admission policies that estimate moments of each workload’s distribution of future resource usage. Via extensive simulations grounded in a trace from Microsoft Azure, we show that our admission policies lead to a substantial improvement over the simple threshold policy. We then show that substantial further gains are possible if high-quality information is available about arriving workloads. Based on this, we propose an information elicitation approach to incentivize users to provide this information and simulate its effects.},
  archive      = {J_JAIR},
  author       = {Ludwig Dierks and Ian Kash and Sven Seuken},
  doi          = {10.1613/jair.1.12346},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-40},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the cluster admission problem for cloud computing},
  volume       = {71},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Labeled bipolar argumentation frameworks. <em>JAIR</em>,
<em>70</em>, 1557–1636. (<a
href="https://doi.org/10.1613/jair.1.12394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential part of argumentation-based reasoning is to identify arguments in favor and against a statement or query, select the acceptable ones, and then determine whether or not the original statement should be accepted. We present here an abstract framework that considers two independent forms of argument interaction—support and conflict—and is able to represent distinctive information associated with these arguments. This information can enable additional actions such as: (i) a more in-depth analysis of the relations between the arguments; (ii) a representation of the user’s posture to help in focusing the argumentative process, optimizing the values of attributes associated with certain arguments; and (iii) an enhancement of the semantics taking advantage of the availability of richer information about argument acceptability. Thus, the classical semantic definitions are enhanced by analyzing a set of postulates they satisfy. Finally, a polynomial-time algorithm to perform the labeling process is introduced, in which the argument interactions are considered.},
  archive      = {J_JAIR},
  author       = {Melisa G. Escañuela Gonzalez and Maximiliano C. D. Budán and Gerardo I. Simari and Guillermo R. Simari},
  doi          = {10.1613/jair.1.12394},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1557-1636},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Labeled bipolar argumentation frameworks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MADRaS: Multi agent driving simulator. <em>JAIR</em>,
<em>70</em>, 1517–1555. (<a
href="https://doi.org/10.1613/jair.1.12531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving has emerged as one of the most active areas of research as it has the promise of making transportation safer and more efficient than ever before. Most real-world autonomous driving pipelines perform perception, motion planning and action in a loop. In this work we present MADRaS, an open-source multi-agent driving simulator for use in the design and evaluation of motion planning algorithms for autonomous driving. Given a start and a goal state, the task of motion planning is to solve for a sequence of position, orientation and speed values in order to navigate between the states while adhering to safety constraints. These constraints often involve the behaviors of other agents in the environment. MADRaS provides a platform for constructing a wide variety of highway and track driving scenarios where multiple driving agents can be trained for motion planning tasks using reinforcement learning and other machine learning algorithms. MADRaS is built on TORCS, an open-source car-racing simulator. TORCS offers a variety of cars with different dynamic properties and driving tracks with different geometries and surface.&amp;nbsp; MADRaS inherits these functionalities from TORCS and introduces support for multi-agent training, inter-vehicular communication, noisy observations, stochastic actions, and custom traffic cars whose behaviors can be programmed to simulate challenging traffic conditions encountered in the real world. MADRaS can be used to create driving tasks whose complexities can be tuned along eight axes in well-defined steps. This makes it particularly suited for curriculum and continual learning. MADRaS is lightweight and it provides a convenient OpenAI Gym interface for independent control of each car. Apart from the primitive steering-acceleration-brake control mode of TORCS, MADRaS offers a hierarchical track-position – speed control mode that can potentially be used to achieve better generalization. MADRaS uses a UDP based client server model where the simulation engine is the server and each client is a driving agent. MADRaS uses multiprocessing to run each agent as a parallel process for efficiency and integrates well with popular reinforcement learning libraries like RLLib. We show experiments on single and multi-agent reinforcement learning with and without curriculum},
  archive      = {J_JAIR},
  author       = {Anirban Santara and Sohan Rudra and Sree Aditya Buridi and Meha Kaushik and Abhishek Naik and Bharat Kaul and Balaraman Ravindran},
  doi          = {10.1613/jair.1.12531},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1517-1555},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {MADRaS: Multi agent driving simulator},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Loss functions, axioms, and peer review. <em>JAIR</em>,
<em>70</em>, 1481–1515. (<a
href="https://doi.org/10.1613/jair.1.12554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to see a handful of reviewers reject a highly novel paper, because they view, say, extensive experiments as far more important than novelty, whereas the community as a whole would have embraced the paper. More generally, the disparate mapping of criteria scores to final recommendations by different reviewers is a major source of inconsistency in peer review. In this paper we present a framework inspired by empirical risk minimization (ERM) for learning the community&#39;s aggregate mapping. The key challenge that arises is the specification of a loss function for ERM. We consider the class of L(p,q) loss functions, which is a matrix-extension of the standard class of Lp losses on vectors; here the choice of the loss function amounts to choosing the hyperparameters p and q. To deal with the absence of ground truth in our problem, we instead draw on computational social choice to identify desirable values of the hyperparameters p and q. Specifically, we characterize p=q=1 as the only choice of these hyperparameters that satisfies three natural axiomatic properties. Finally, we implement and apply our approach to reviews from IJCAI 2017.},
  archive      = {J_JAIR},
  author       = {Ritesh Noothigattu and Nihar Shah and Ariel Procaccia},
  doi          = {10.1613/jair.1.12554},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1481-1515},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Loss functions, axioms, and peer review},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient retrieval of matrix factorization-based top-k
recommendations: A survey of recent approaches. <em>JAIR</em>,
<em>70</em>, 1441–1479. (<a
href="https://doi.org/10.1613/jair.1.12403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Top-k recommendation seeks to deliver a personalized list of k items to each individual user. An established methodology in the literature based on matrix factorization (MF), which usually represents users and items as vectors in low-dimensional space, is an effective approach to recommender systems, thanks to its superior performance in terms of recommendation quality and scalability. A typical matrix factorization recommender system has two main phases: preference elicitation and recommendation retrieval. The former analyzes user-generated data to learn user preferences and item characteristics in the form of latent feature vectors, whereas the latter ranks the candidate items based on the learnt vectors and returns the top-k items from the ranked list. For preference elicitation, there have been numerous works to build accurate MF-based recommendation algorithms that can learn from large datasets. However, for the recommendation retrieval phase, naively scanning a large number of items to identify the few most relevant ones may inhibit truly real-time applications. In this work, we survey recent advances and state-of-the-art approaches in the literature that enable fast and accurate retrieval for MF-based personalized recommendations. Also, we include analytical discussions of approaches along different dimensions to provide the readers with a more comprehensive understanding of the surveyed works.},
  archive      = {J_JAIR},
  author       = {Dung D. Le and Hady Lauw},
  doi          = {10.1613/jair.1.12403},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1441-1479},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient retrieval of matrix factorization-based top-k recommendations: A survey of recent approaches},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggregation over metric spaces: Proposing and voting in
elections, budgeting, and legislation. <em>JAIR</em>, <em>70</em>,
1413–1439. (<a href="https://doi.org/10.1613/jair.1.12388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a unifying framework encompassing a plethora of social choice settings. Viewing each social choice setting as voting in a suitable metric space, we offer a general model of social choice over metric spaces, in which—similarly to the spatial model of elections—each voter specifies an ideal element of the metric space. The ideal element acts as a vote, where each voter prefers elements that are closer to her ideal element. But it also acts as a proposal, thus making all participants equal not only as voters but also as proposers. We consider Condorcet aggregation and a continuum of solution concepts, ranging from minimizing the sum of distances to minimizing the maximum distance. We study applications of our abstract model to various social choice settings, including single-winner elections, committee elections, participatory budgeting, and participatory legislation. For each setting, we compare each solution concept to known voting rules and study various properties of the resulting voting rules. Our framework provides expressive aggregation for a broad range of social choice settings while remaining simple for voters; and may enable a unified and integrated implementation for all these settings, as well as unified extensions such as sybil-resiliency, proxy voting, and deliberative decision making.&amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Laurent Bulteau and Gal Shahaf and Ehud Shapiro and Nimrod Talmon},
  doi          = {10.1613/jair.1.12388},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1413-1439},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Aggregation over metric spaces: Proposing and voting in elections, budgeting, and legislation},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confident learning: Estimating uncertainty in dataset
labels. <em>JAIR</em>, <em>70</em>, 1373–1411. (<a
href="https://doi.org/10.1613/jair.1.12125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.},
  archive      = {J_JAIR},
  author       = {Curtis Northcutt and Lu Jiang and Isaac Chuang},
  doi          = {10.1613/jair.1.12125},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1373-1411},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Confident learning: Estimating uncertainty in dataset labels},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance-level update in DL-lite ontologies through
first-order rewriting. <em>JAIR</em>, <em>70</em>, 1335–1371. (<a
href="https://doi.org/10.1613/jair.1.12414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study instance-level update in DL-LiteA , a well-known description logic that influenced the OWL 2 QL standard. Instance-level update regards insertions and deletions in the ABox of an ontology. In particular we focus on formula-based approaches to instance-level update. We show that DL-LiteA , which is well-known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for instance-level update. That is, every update can be reformulated into a set of insertion and deletion instructions computable through a non-recursive Datalog program with negation. Such a program is readily translatable into a first-order query over the ABox considered as a database, and hence into SQL. By exploiting this result, we implement an update component for DL-LiteA-based systems and perform some experiments showing that the approach works in practice.},
  archive      = {J_JAIR},
  author       = {Giuseppe De Giacomo and Xavier Oriol and Riccardo Rosati and Domenico Fabio Savo},
  doi          = {10.1613/jair.1.12414},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1335-1371},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Instance-level update in DL-lite ontologies through first-order rewriting},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The AI liability puzzle and a fund-based work-around.
<em>JAIR</em>, <em>70</em>, 1309–1334. (<a
href="https://doi.org/10.1613/jair.1.12580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence in the regulatory environment is crucial to enable responsible AI innovation and foster the social acceptance of these powerful new technologies. One notable source of uncertainty is, however, that the existing legal liability system is unable to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI difficult to calculate with confidence, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory frameworks, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose it to be at least partially industry-funded. Funding arrangements should depend on whether it would pursue other potential policy goals aimed more broadly at controlling the trajectory of AI innovation to increase economic and social welfare worldwide. Because of the global relevance of the issue, rather than focusing on any particular legal system, we trace relevant developments across multiple jurisdictions and engage in a high-level, comparative conceptual debate around the suitability of the foreseeability concept to limit legal liability. The paper also refrains from confronting the intricacies of the case law of specific jurisdictions for now and—recognizing the importance of this task—leaves this to further research in support of the legal system’s incremental adaptation to the novel challenges of present and future AI technologies. This article appears in the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Olivia J. Erdelyi and Gabor Erdelyi},
  doi          = {10.1613/jair.1.12580},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1309-1334},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The AI liability puzzle and a fund-based work-around},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted first-order model counting in the two-variable
fragment with counting quantifiers. <em>JAIR</em>, <em>70</em>,
1281–1307. (<a href="https://doi.org/10.1613/jair.1.12320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known due to the work of Van den Broeck, Meert and Darwiche that weighted first-order model counting (WFOMC) in the two-variable fragment of first-order logic can be solved in time polynomial in the number of domain elements. In this paper we extend this result to the two-variable fragment with counting quantifiers.},
  archive      = {J_JAIR},
  author       = {Ondrej Kuzelka},
  doi          = {10.1613/jair.1.12320},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1281-1307},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Weighted first-order model counting in the two-variable fragment with counting quantifiers},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategyproof mechanisms for additively separable and
fractional hedonic games. <em>JAIR</em>, <em>70</em>, 1253–1279. (<a
href="https://doi.org/10.1613/jair.1.12107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additively separable hedonic games and fractional hedonic games have received considerable attention in the literature. They are coalition formation games among selfish agents based on their mutual preferences. Most of the work in the literature characterizes the existence and structure of stable outcomes (i.e., partitions into coalitions) assuming that preferences are given. However, there is little discussion of this assumption. In fact, agents receive different utilities if they belong to different coalitions, and thus it is natural for them to declare their preferences strategically in order to maximize their benefit. In this paper we consider strategyproof mechanisms for additively separable hedonic games and fractional hedonic games, that is, partitioning methods without payments such that utility maximizing agents have no incentive to lie about their true preferences. We focus on social welfare maximization and provide several lower and upper bounds on the performance achievable by strategyproof mechanisms for general and specific additive functions. In most of the cases we provide tight or asymptotically tight results. All our mechanisms are simple and can be run in polynomial time. Moreover, all the lower bounds are unconditional, that is, they do not rely on any computational complexity assumptions.},
  archive      = {J_JAIR},
  author       = {Michele Flammini and Bojana Kodric and Gianpiero Monaco and Qiang Zhang},
  doi          = {10.1613/jair.1.12107},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1253–1279},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Strategyproof mechanisms for additively separable and fractional hedonic games},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Liquid democracy: An algorithmic perspective. <em>JAIR</em>,
<em>70</em>, 1223–1252. (<a
href="https://doi.org/10.1613/jair.1.12261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study liquid democracy, a collective decision making paradigm that allows voters to transitively delegate their votes, through an algorithmic lens. In our model, there are two alternatives, one correct and one incorrect, and we are interested in the probability that the majority opinion is correct. Our main question is whether there exist delegation mechanisms that are guaranteed to outperform direct voting, in the sense of being always at least as likely, and sometimes more likely, to make a correct decision. Even though we assume that voters can only delegate their votes to better-informed voters, we show that local delegation mechanisms, which only take the local neighborhood of each voter as input (and, arguably, capture the spirit of liquid democracy), cannot provide the foregoing guarantee. By contrast, we design a non-local delegation mechanism that does provably outperform direct voting under mild assumptions about voters.},
  archive      = {J_JAIR},
  author       = {Anson Kahng and Simon Mackenzie and Ariel Procaccia},
  doi          = {10.1613/jair.1.12261},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1223-1252},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Liquid democracy: An algorithmic perspective},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational complexity of computing symmetries in
finite-domain planning. <em>JAIR</em>, <em>70</em>, 1183–1221. (<a
href="https://doi.org/10.1613/jair.1.12283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JAIR},
  author       = {Alexander Shleyfman and Peter Jonsson},
  doi          = {10.1613/jair.1.12283},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1183-1221},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Computational complexity of computing symmetries in finite-domain planning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lilotane: A lifted SAT-based approach to hierarchical
planning. <em>JAIR</em>, <em>70</em>, 1117–1181. (<a
href="https://doi.org/10.1613/jair.1.12520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the oldest and most popular approaches to automated planning is to encode the problem at hand into a propositional formula and use a Satisfiability (SAT) solver to find a solution. In all established SAT-based approaches for Hierarchical Task Network (HTN) planning, grounding the problem is necessary and oftentimes introduces a combinatorial blowup in terms of the number of actions and reductions to encode. Our contribution named Lilotane (Lifted Logic for Task Networks) eliminates this issue for Totally Ordered HTN planning by directly encoding the lifted representation of the problem at hand. We lazily instantiate the problem hierarchy layer by layer and use a novel SAT encoding which allows us to defer decisions regarding method arguments to the stage of SAT solving. We show the correctness of our encoding and compare it to the best performing prior SAT encoding in a worst-case analysis. Empirical evaluations confirm that Lilotane outperforms established SAT-based approaches, often by orders of magnitude, produces much smaller formulae on average, and compares favorably to other state-of-the-art HTN planners regarding robustness and plan quality. In the International Planning Competition (IPC) 2020, a preliminary version of Lilotane scored the second place. We expect these considerable improvements to SAT-based HTN planning to open up new perspectives for SAT-based approaches in related problem classes.},
  archive      = {J_JAIR},
  author       = {Dominik Schreiber},
  doi          = {10.1613/jair.1.12520},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1117-1181},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Lilotane: A lifted SAT-based approach to hierarchical planning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Induction and exploitation of subgoal automata for
reinforcement learning. <em>JAIR</em>, <em>70</em>, 1031–1116. (<a
href="https://doi.org/10.1613/jair.1.12372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present ISA, an approach for learning and exploiting subgoals in episodic reinforcement learning (RL) tasks. ISA interleaves reinforcement learning with the induction of a subgoal automaton, an automaton whose edges are labeled by the task’s subgoals expressed as propositional logic formulas over a set of high-level events. A subgoal automaton also consists of two special states: a state indicating the successful completion of the task, and a state indicating that the task has finished without succeeding. A state-of-the-art inductive logic programming system is used to learn a subgoal automaton that covers the traces of high-level events observed by the RL agent. When the currently exploited automaton does not correctly recognize a trace, the automaton learner induces a new automaton that covers that trace. The interleaving process guarantees the induction of automata with the minimum number of states, and applies a symmetry breaking mechanism to shrink the search space whilst remaining complete. We evaluate ISA in several gridworld and continuous state space problems using different RL algorithms that leverage the automaton structures. We provide an in-depth empirical analysis of the automaton learning performance in terms of the traces, the symmetry breaking and specific restrictions imposed on the final learnable automaton. For each class of RL problem, we show that the learned automata can be successfully exploited to learn policies that reach the goal, achieving an average reward comparable to the case where automata are not learned but handcrafted and given beforehand.},
  archive      = {J_JAIR},
  author       = {Daniel Furelos-Blanco and Mark Law and Anders Jonsson and Krysia Broda and Alessandra Russo},
  doi          = {10.1613/jair.1.12372},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1031-1116},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Induction and exploitation of subgoal automata for reinforcement learning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The societal implications of deep reinforcement learning.
<em>JAIR</em>, <em>70</em>, 1003–1030. (<a
href="https://doi.org/10.1613/jair.1.12360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications. This article appears in the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Jess Whittlestone and Kai Arulkumaran and Matthew Crosby},
  doi          = {10.1613/jair.1.12360},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1003–1030},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The societal implications of deep reinforcement learning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained multiagent markov decision processes: A taxonomy
of problems and algorithms. <em>JAIR</em>, <em>70</em>, 955–1001. (<a
href="https://doi.org/10.1613/jair.1.12233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In domains such as electric vehicle charging, smart distribution grids and autonomous warehouses, multiple agents share the same resources. When planning the use of these resources, agents need to deal with the uncertainty in these domains. Although several models and algorithms for such constrained multiagent planning problems under uncertainty have been proposed in the literature, it remains unclear when which algorithm can be applied. In this survey we conceptualize these domains and establish a generic problem class based on Markov decision processes. We identify and compare the conditions under which algorithms from the planning literature for problems in this class can be applied: whether constraints are soft or hard, whether agents are continuously connected, whether the domain is fully observable, whether a constraint is momentarily (instantaneous) or on a budget, and whether the constraint is on a single resource or on multiple. Further we discuss the advantages and disadvantages of these algorithms. We conclude by identifying open problems that are directly related to the conceptualized domains, as well as in adjacent research areas.},
  archive      = {J_JAIR},
  author       = {Frits de Nijs and Erwin Walraven and Mathijs De Weerdt and Matthijs Spaan},
  doi          = {10.1613/jair.1.12233},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {955-1001},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Constrained multiagent markov decision processes: A taxonomy of problems and algorithms},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe multi-agent pathfinding with time uncertainty.
<em>JAIR</em>, <em>70</em>, 923–954. (<a
href="https://doi.org/10.1613/jair.1.12397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world scenarios, the time it takes for a mobile agent, e.g., a robot, to move from one location to another may vary due to exogenous events and be difficult to predict accurately. Planning in such scenarios is challenging, especially in the context of Multi-Agent Pathfinding (MAPF), where the goal is to find paths to multiple agents and temporal coordination is necessary to avoid collisions. In this work, we consider a MAPF problem with this form of time uncertainty, where we are only given upper and lower bounds on the time it takes each agent to move. The objective is to find a safe solution, which is a solution that can be executed by all agents and is guaranteed to avoid collisions. We propose two complete and optimal algorithms for finding safe solutions based on well-known MAPF algorithms, namely, A* with Operator Decomposition (A* + OD) and Conflict-Based Search (CBS). Experimentally, we observe that on several standard MAPF grids the CBS-based algorithm performs better. We also explore the option of online replanning in this context, i.e., modifying the agents&#39; plans during execution, to reduce the overall execution cost. We consider two online settings: (a) when an agent can sense the current time and its current location, and (b) when the agents can also communicate seamlessly during execution. For each setting, we propose a replanning algorithm and analyze its behavior theoretically and empirically. Our experimental evaluation confirms that indeed online replanning in both settings can significantly reduce solution cost.},
  archive      = {J_JAIR},
  author       = {Tomer Shahar and Shashank Shekhar and Dor Atzmon and Abdallah Saffidine and Brendan Juba and Roni Stern},
  doi          = {10.1613/jair.1.12397},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {923–954},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Safe multi-agent pathfinding with time uncertainty},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the evolvability of monotone conjunctions with an
evolutionary mutation mechanism. <em>JAIR</em>, <em>70</em>, 891–921.
(<a href="https://doi.org/10.1613/jair.1.12050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bernoulli(p)n distribution Bn,p over {0, 1}n is a product distribution where each variable is satisfied with the same constant probability p. Diochnos (2016) showed that Valiant&#39;s swapping algorithm for monotone conjunctions converges efficiently under Bn,p distributions over {0, 1}n for any 0 &amp;lt; p &amp;lt; 1. We continue the study of monotone conjunctions in Valiant&#39;s framework of evolvability. In particular, we prove that given a Bn,p distribution characterized by some p ∈ (0, 1/3] ∪ {1/2}, then an evolutionary mechanism that relies on the basic mutation mechanism of a (1+1) evolutionary algorithm converges efficiently, with high probability, to an ε-optimal hypothesis. Furthermore, for 0 &amp;lt; α ≤ 3/13, a slight modification of the algorithm, with a uniform setup this time, evolves with high probability an ε-optimal hypothesis, for every Bn,p distribution such that p ∈ [α, 1/3 - 4α/9] ∪ {1/3} ∪ {1/2}.},
  archive      = {J_JAIR},
  author       = {Dimitrios Diochnos},
  doi          = {10.1613/jair.1.12050},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {891-921},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the evolvability of monotone conjunctions with an evolutionary mutation mechanism},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Taking principles seriously: A hybrid approach to value
alignment in artificial intelligence. <em>JAIR</em>, <em>70</em>,
871–890. (<a href="https://doi.org/10.1613/jair.1.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important step in the development of value alignment (VA) systems in artificial intelligence (AI) is understanding how VA can reflect valid ethical principles. We propose that designers of VA systems incorporate ethics by utilizing a hybrid approach in which both ethical reasoning and empirical observation play a role. This, we argue, avoids committing “naturalistic fallacy,” which is an attempt to derive “ought” from “is,” and it provides a more adequate form of ethical reasoning when the fallacy is not committed. Using quantified modal logic, we precisely formulate principles derived from deontological ethics and show how they imply particular “test propositions” for any given action plan in an AI rule base. The action plan is ethical only if the test proposition is empirically true, a judgment that is made on the basis of empirical VA. This permits empirical VA to integrate seamlessly with independently justified ethical principles. This article is part of the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Tae Wan Kim and John Hooker and Thomas Donaldson},
  doi          = {10.1613/jair.1.12481},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {871-890},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Taking principles seriously: A hybrid approach to value alignment in artificial intelligence},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A sufficient statistic for influence in structured
multiagent environments. <em>JAIR</em>, <em>70</em>, 789–870. (<a
href="https://doi.org/10.1613/jair.1.12136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making decisions in complex environments is a key challenge in artificial intelligence (AI). Situations involving multiple decision makers are particularly complex, leading to computational intractability of principled solution methods. A body of work in AI has tried to mitigate this problem by trying to distill interaction to its essence: how does the policy of one agent influence another agent? If we can find more compact representations of such influence, this can help us deal with the complexity, for instance by searching the space of influences rather than the space of policies. However, so far these notions of influence have been restricted in their applicability to special cases of interaction. In this paper we formalize influence-based abstraction (IBA), which facilitates the elimination of latent state factors without any loss in value, for a very general class of problems described as factored partially observable stochastic games (fPOSGs). On the one hand, this generalizes existing descriptions of influence, and thus can serve as the foundation for improvements in scalability and other insights in decision making in complex multiagent settings. On the other hand, since the presence of other agents can be seen as a generalization of single agent settings, our formulation of IBA also provides a sufficient statistic for decision making under abstraction for a single agent. We also give a detailed discussion of the relations to such previous works, identifying new insights and interpretations of these approaches. In these ways, this paper deepens our understanding of abstraction in a wide range of sequential decision making settings, providing the basis for new approaches and algorithms for a large class of problems.},
  archive      = {J_JAIR},
  author       = {Frans Oliehoek and Stefan Witwicki and Leslie Kaelbling},
  doi          = {10.1613/jair.1.12136},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {789-870},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A sufficient statistic for influence in structured multiagent environments},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient large-scale multi-drone delivery using transit
networks. <em>JAIR</em>, <em>70</em>, 757–788. (<a
href="https://doi.org/10.1613/jair.1.12450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of routing a large fleet of drones to deliver packages simultaneously across broad urban areas. Besides flying directly, drones can use public transit vehicles such as buses and trams as temporary modes of transportation to conserve energy. Adding this capability to our formulation augments effective drone travel range and the space of possible deliveries but also increases problem input size due to the large transit networks. We present a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery and addresses the multifaceted computational challenges of our problem through a two-layer approach. First, the upper layer assigns drones to package delivery sequences with an approximately optimal polynomial time allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network, using efficient, bounded suboptimal multi-agent pathfinding techniques tailored to our setting. We demonstrate the efficiency of our approach on simulations with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in San Francisco and the Washington DC Metropolitan Area. Our framework computes solutions for most settings within a few seconds on commodity hardware and enables drones to extend their effective range by a factor of nearly four using transit.},
  archive      = {J_JAIR},
  author       = {Shushman Choudhury and Kiril Solovey and Mykel J. Kochenderfer and Marco Pavone},
  doi          = {10.1613/jair.1.12450},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {757-788},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient large-scale multi-drone delivery using transit networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-facility location games with minimum distance
requirement. <em>JAIR</em>, <em>70</em>, 719–756. (<a
href="https://doi.org/10.1613/jair.1.12319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the mechanism design problem of a social planner for locating two facilities on a line interval [0, 1], where a set of n strategic agents report their locations and a mechanism determines the locations of the two facilities. We consider the requirement of a minimum distance 0 ≤ d ≤ 1 between the two facilities. Given the two facilities are heterogeneous, we model the cost/utility of an agent as the sum of his distances to both facilities. In the heterogeneous two-facility location game to minimize the social cost, we show that the optimal solution can be computed in polynomial time and prove that carefully choosing one optimal solution as output is strategyproof. We also design a strategyproof mechanism minimizing the maximum cost. Given the two facilities are homogeneous, we model the cost/utility of an agent as his distance to the closer facility. In the homogeneous two-facility location game for minimizing the social cost, we show that any deterministic strategyproof mechanism has unbounded approximation ratio. Moreover, in the obnoxious heterogeneous two-facility location game for maximizing the social utility, we propose new deterministic group strategyproof mechanisms with provable approximation ratios and establish a lower bound (7 − d)/6 for any deterministic strategyproof mechanism. We also design a strategyproof mechanism maximizing the minimum utility. In the obnoxious homogeneous two-facility location game for maximizing the social utility, we propose deterministic group strategyproof mechanisms with provable approximation ratios and establish a lower bound 4/3. Besides, in the two-facility location game with triple-preference, where each facility may be favorable, obnoxious, indifferent for any agent, we further motivate agents to report both their locations and preferences towards the two facilities truthfully, and design a deterministic group strategyproof mechanism with an approximation ratio 4.},
  archive      = {J_JAIR},
  author       = {Xinping Xu and Bo Li and Minming Li and Lingjie Duan},
  doi          = {10.1613/jair.1.12319},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {719-756},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Two-facility location games with minimum distance requirement},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classifier chains: A review and perspectives. <em>JAIR</em>,
<em>70</em>, 683–718. (<a
href="https://doi.org/10.1613/jair.1.12376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The family of methods collectively known as classifier chains has become a popular approach to multi-label learning problems. This approach involves chaining together off-the-shelf binary classifiers in a directed structure, such that individual label predictions become features for other classifiers. Such methods have proved flexible and effective and have obtained state-of-the-art empirical performance across many datasets and multi-label evaluation metrics. This performance led to further studies of the underlying mechanism and efficacy, and investigation into how it could be improved. In the recent decade, numerous studies have explored the theoretical underpinnings of classifier chains, and many improvements have been made to the training and inference procedures, such that this method remains among the best options for multi-label learning. Given this past and ongoing interest, which covers a broad range of applications and research themes, the goal of this work is to provide a review of classifier chains, a survey of the techniques and extensions provided in the literature, as well as perspectives for this approach in the domain of multi-label classification in the future. We conclude positively, with a number of recommendations for researchers and practitioners, as well as outlining key issues for future research.},
  archive      = {J_JAIR},
  author       = {Jesse Read and Bernhard Pfahringer and Geoffrey Holmes and Eibe Frank},
  doi          = {10.1613/jair.1.12376},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {683-718},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Classifier chains: A review and perspectives},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regarding goal bounding and jump point search.
<em>JAIR</em>, <em>70</em>, 631–681. (<a
href="https://doi.org/10.1613/jair.1.12255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jump Point Search (JPS) is a well known symmetry-breaking algorithm that can substantially improve performance for grid-based optimal pathfinding. When the input grid is static further speedups can be obtained by combining JPS with goal bounding techniques such as Geometric Containers (instantiated as Bounding Boxes) and Compressed Path Databases. Two such methods, JPS+BB and Two-Oracle Path PlannING (Topping), are currently among the fastest known approaches for computing shortest paths on grids. The principal drawback for these algorithms is the overhead costs: each one requires an all-pairs precomputation step, the running time and subsequent storage costs of which can be prohibitive. In this work we consider an alternative approach where we precompute and store goal bounding data only for grid cells which are also jump points. Since the number of jump points is usually much smaller than the total number of grid cells, we can save up to orders of magnitude in preprocessing time and space. Considerable precomputation savings do not necessarily mean performance degradation. For a second contribution we show how canonical orderings, partial expansion strategies and enhanced intermediate pruning can be leveraged to improve online query performance despite a reduction in preprocessed data. The combination of faster preprocessing and stronger online reasoning leads to three new and highly performant algorithms: JPS+BB+ and Two-Oracle Pathfinding Search (TOPS) based on search, and Topping+ based on path extraction. We give a theoretical analysis showing that each method is complete and optimal. We also report convincing gains in a comprehensive empirical evaluation that includes almost all current and cutting-edge algorithms for grid-based pathfinding.},
  archive      = {J_JAIR},
  author       = {Yue Hu and Daniel Harabor and Long Qin and Quanjun Yin},
  doi          = {10.1613/jair.1.12255},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {631-681},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Regarding goal bounding and jump point search},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generic constraint-based block modeling using constraint
programming. <em>JAIR</em>, <em>70</em>, 597–630. (<a
href="https://doi.org/10.1613/jair.1.12280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block modeling has been used extensively in many domains including social science, spatial temporal data analysis and even medical imaging. Original formulations of the problem modeled it as a mixed integer programming problem, but were not scalable. Subsequent work relaxed the discrete optimization requirement, and showed that adding constraints is not straightforward in existing approaches. In this work, we present a new approach based on constraint programming, allowing discrete optimization of block modeling in a manner that is not only scalable, but also allows the easy incorporation of constraints. We introduce a new constraint filtering algorithm that outperforms earlier approaches, in both constrained and unconstrained settings, for an exhaustive search and for a type of local search called Large Neighborhood Search. We show its use in the analysis of real datasets. Finally, we show an application of the CP framework for model selection using the Minimum Description Length principle.},
  archive      = {J_JAIR},
  author       = {Alex Mattenet and Ian Davidson and Siegfried Nijssen and Pierre Schaus},
  doi          = {10.1613/jair.1.12280},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {597-630},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Generic constraint-based block modeling using constraint programming},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the distortion value of elections with abstention.
<em>JAIR</em>, <em>70</em>, 567–595. (<a
href="https://doi.org/10.1613/jair.1.12306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Spatial Voting Theory, distortion is a measure of how good the winner is. It has been proved that no deterministic voting mechanism can guarantee a distortion better than 3, even for simple metrics such as a line. In this study, we wish to answer the following question: how does the distortion value change if we allow less motivated agents to abstain from the election? We consider an election with two candidates and suggest an abstention model, which is a general form of the abstention model proposed by Kirchgässner. Our results characterize the distortion ¨ value and provide a rather complete picture of the model.},
  archive      = {J_JAIR},
  author       = {Masoud Seddighin and Mohammad Latifian and Mohammad Ghodsi},
  doi          = {10.1613/jair.1.12306},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {567-595},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the distortion value of elections with abstention},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An external knowledge enhanced graph-based neural network
for sentence ordering. <em>JAIR</em>, <em>70</em>, 545–566. (<a
href="https://doi.org/10.1613/jair.1.12078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important text coherence modeling task, sentence ordering aims to coherently organize a given set of unordered sentences. To achieve this goal, the most important step is to effectively capture and exploit global dependencies among these sentences. In this paper, we propose a novel and flexible external knowledge enhanced graph-based neural network for sentence ordering. Specifically, we first represent the input sentences as a graph, where various kinds of relations (i.e., entity-entity, sentence-sentence and entity-sentence) are exploited to make the graph representation more expressive and less noisy. Then, we introduce graph recurrent network to learn semantic representations of the sentences. To demonstrate the effectiveness of our model, we conduct experiments on several benchmark datasets. The experimental results and in-depth analysis show our model significantly outperforms the existing state-of-the-art models.},
  archive      = {J_JAIR},
  author       = {Yongjing Yin and Shaopeng Lai and Linfeng Song and Chulun Zhou and Xianpei Han and Junfeng Yao and Jinsong Su},
  doi          = {10.1613/jair.1.12078},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {545-566},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An external knowledge enhanced graph-based neural network for sentence ordering},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). General value function networks. <em>JAIR</em>, <em>70</em>,
497–543. (<a href="https://doi.org/10.1613/jair.1.12105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State construction is important for learning in partially observable environments. A general purpose strategy for state construction is to learn the state update using a Recurrent Neural Network (RNN), which updates the internal state using the current internal state and the most recent observation. This internal state provides a summary of the observed sequence, to facilitate accurate predictions and decision-making. At the same time, specifying and training RNNs is notoriously tricky, particularly as the common strategy to approximate gradients back in time, called truncated Back-prop Through Time (BPTT), can be sensitive to the truncation window. Further, domain-expertise—which can usually help constrain the function class and so improve trainability—can be difficult to incorporate into complex recurrent units used within RNNs. In this work, we explore how to use multi-step predictions to constrain the RNN and incorporate prior knowledge. In particular, we revisit the idea of using predictions to construct state and ask: does constraining (parts of) the state to consist of predictions about the future improve RNN trainability? We formulate a novel RNN architecture, called a General Value Function Network (GVFN), where each internal state component corresponds to a prediction about the future represented as a value function. We first provide an objective for optimizing GVFNs, and derive several algorithms to optimize this objective. We then show that GVFNs are more robust to the truncation level, in many cases only requiring one-step gradient updates.},
  archive      = {J_JAIR},
  author       = {Matthew Schlegel and Andrew Jacobsen and Zaheer Abbas and Andrew Patterson and Adam White and Martha White},
  doi          = {10.1613/jair.1.12105},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {497-543},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {General value function networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On super strong ETH. <em>JAIR</em>, <em>70</em>, 473–495.
(<a href="https://doi.org/10.1613/jair.1.11859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple known algorithmic paradigms (backtracking, local search and the polynomial method) only yield a 2n(1-1/O(k)) time algorithm for k-SAT in the worst case. For this reason, it has been hypothesized that the worst-case k-SAT problem cannot be solved in 2n(1-f(k)/k) time for any unbounded function f. This hypothesis has been called the &quot;Super-Strong ETH&quot;, modelled after the ETH and the Strong ETH. It has also been hypothesized that k-SAT is hard to solve for randomly chosen instances near the &quot;critical threshold&quot;, where the clause-to-variable ratio is such that randomly chosen instances are satisfiable with probability 1/2. We give a randomized algorithm which refutes the Super-Strong ETH for the case of random k-SAT and planted k-SAT for any clause-to-variable ratio. For example, given any random k-SAT instance F with n variables and m clauses, our algorithm decides satisfiability for F in&amp;nbsp; 2n(1-c*log(k)/k) time with high probability (over the choice of the formula and the randomness of the algorithm). It turns out that a well-known algorithm from the literature on SAT algorithms does the job: the PPZ algorithm of Paturi, Pudlak, and Zane (1999).&amp;nbsp;&amp;nbsp; The Unique k-SAT problem is the special case where there is at most one satisfying assignment. Improving prior reductions, we show that the Super-Strong ETHs for Unique k-SAT and k-SAT are equivalent. More precisely, we show the time complexities of Unique k-SAT and k-SAT are very tightly correlated: if Unique k-SAT is in&amp;nbsp; 2n(1-f(k)/k) time for an unbounded f, then k-SAT is in 2n(1-f(k)/(2k)) time.},
  archive      = {J_JAIR},
  author       = {Nikhil Vyas and Ryan Williams},
  doi          = {10.1613/jair.1.11859},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {473-495},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On super strong ETH},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmark and survey of automated machine learning
frameworks. <em>JAIR</em>, <em>70</em>, 409–472. (<a
href="https://doi.org/10.1613/jair.1.11854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 data sets from established AutoML benchmark suites.},
  archive      = {J_JAIR},
  author       = {Marc-André Zöller and Marco F. Huber},
  doi          = {10.1613/jair.1.11854},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {409-472},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Benchmark and survey of automated machine learning frameworks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid-order network consensus for distributed multi-agent
systems. <em>JAIR</em>, <em>70</em>, 389–407. (<a
href="https://doi.org/10.1613/jair.1.12061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important field of Distributed artificial intelligence (DAI), multi-agent systems (MASs) have attracted the attention of extensive research scholars. Consensus as the most important issue in MAS, much progress has been made in studying the consensus control of MAS, but there are some problems remained largely unaddressed which cause the MAS to lose some useful network structure information. First, multi-agent consensus protocol usually proceeds over the low-order structure by only considering the direct edges between agents, but ignores the higher-order structure of the whole topology network. Second, the existing work assumes all the edges in a topology network have the same weight without exploring the potential diversity of the connections. In this way, multi-agent systems fail to enforce consensus, resulting in fragmentation into multiple clusters. To address the above issues, this paper proposes a Motif-aware Weighted Multi-agent System (MWMS) method for consensus control. We focus more on triangle motif in the network, but it can be extended to other kinds of motifs as well. First, a novel weighted network is used which is the combination of the edge-based lower-order structure and the motif-based higher-order structure, i.e., hybrid-order structure. Subsequently, by simultaneously considering the quantity and the quality of the connections in the network, a novel consensus framework for MAS is designed to update agents. Then, two baseline consensus algorithms are used in MWMS. In our experiments, we use ten topologies of different shapes, densities and ranges to comprehensively analyze the performance of our proposed algorithms. The simulation results show that the hybrid higher-order network can effectively enhance the consensus of the multi-agent system in different network topologies.},
  archive      = {J_JAIR},
  author       = {Guangqiang Xie and Junyu Chen and Yang Li},
  doi          = {10.1613/jair.1.12061},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {389-407},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Hybrid-order network consensus for distributed multi-agent systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The computational complexity of understanding binary
classifier decisions. <em>JAIR</em>, <em>70</em>, 351–387. (<a
href="https://doi.org/10.1613/jair.1.12359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a d-ary Boolean function Φ: {0, 1}d → {0, 1} and an assignment to its variables x = (x1, x2, . . . , xd) we consider the problem of finding those subsets of the variables that are sufficient to determine the function value with a given probability δ. This is motivated by the task of interpreting predictions of binary classifiers described as Boolean circuits, which can be seen as special cases of neural networks. We show that the problem of deciding whether such subsets of relevant variables of limited size k ≤ d exist is complete for the complexity class NPPP and thus, generally, unfeasible to solve. We then introduce a variant, in which it suffices to check whether a subset determines the function value with probability at least δ or at most δ − γ for 0 &amp;lt; γ &amp;lt; δ. This promise of a probability gap reduces the complexity to the class NPBPP. Finally, we show that finding the minimal set of relevant variables cannot be reasonably approximated, i.e. with an approximation factor d1−α for α &amp;gt; 0, by a polynomial time algorithm unless P = NP. This holds even with the promise of a probability gap.},
  archive      = {J_JAIR},
  author       = {Stephan Waeldchen and Jan Macdonald and Sascha Hauch and Gitta Kutyniok},
  doi          = {10.1613/jair.1.12359},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {351–387},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The computational complexity of understanding binary classifier decisions},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient multi-objective reinforcement learning via
multiple-gradient descent with iteratively discovered weight-vector
sets. <em>JAIR</em>, <em>70</em>, 319–349. (<a
href="https://doi.org/10.1613/jair.1.12270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving multi-objective optimization problems is important in various applications where users are interested in obtaining optimal policies subject to multiple (yet often conflicting) objectives. A typical approach to obtain the optimal policies is to first construct a loss function based on the scalarization of individual objectives and then derive optimal policies that minimize the scalarized loss function. Albeit simple and efficient, the typical approach provides no insights/mechanisms on the optimization of multiple objectives due to the lack of ability to quantify the inter-objective relationship. To address the issue, we propose to develop a new efficient gradient-based multi-objective reinforcement learning approach that seeks to iteratively uncover the quantitative inter-objective relationship via finding a minimum-norm point in the convex hull of the set of multiple policy gradients when the impact of one objective on others is unknown a priori. In particular, we first propose a new PAOLS algorithm that integrates pruning and approximate optimistic linear support algorithm to efficiently discover the weight-vector sets of multiple gradients that quantify the inter-objective relationship. Then we construct an actor and a multi-objective critic that can co-learn the policy and the multi-objective vector value function. Finally, the weight discovery process and the policy and vector value function learning process can be iteratively executed to yield stable weight-vector sets and policies. To validate the effectiveness of the proposed approach, we present a quantitative evaluation of the approach based on three case studies.},
  archive      = {J_JAIR},
  author       = {Yongcan Cao and Huixin Zhan},
  doi          = {10.1613/jair.1.12270},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {319-349},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient multi-objective reinforcement learning via multiple-gradient descent with iteratively discovered weight-vector sets},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on the explainability of supervised machine
learning. <em>JAIR</em>, <em>70</em>, 245–317. (<a
href="https://doi.org/10.1613/jair.1.12228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
  archive      = {J_JAIR},
  author       = {Nadia Burkart and Marco F. Huber},
  doi          = {10.1613/jair.1.12228},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {245-317},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A survey on the explainability of supervised machine learning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning temporal causal sequence relationships from
real-time time-series. <em>JAIR</em>, <em>70</em>, 205–243. (<a
href="https://doi.org/10.1613/jair.1.12395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to mine temporal causal sequences that explain observed events (consequents) in time-series traces. Causal explanations of key events in a time-series have applications in design debugging, anomaly detection, planning, root-cause analysis and many more. We make use of decision trees and interval arithmetic to mine sequences that explain defining events in the time-series. We propose modified decision tree construction metrics to handle the non-determinism introduced by the temporal dimension. The mined sequences are expressed in a readable temporal logic language that is easy to interpret. The application of the proposed methodology is illustrated through various examples.},
  archive      = {J_JAIR},
  author       = {Antonio Anastasio Bruto da Costa and Pallab Dasgupta},
  doi          = {10.1613/jair.1.12395},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {205-243},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning temporal causal sequence relationships from real-time time-series},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-optimal planning, delete relaxation, approximability,
and heuristics. <em>JAIR</em>, <em>70</em>, 169–204. (<a
href="https://doi.org/10.1613/jair.1.12278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-optimal planning is a very well-studied topic within planning, and it has proven to be computationally hard both in theory and in practice. Since cost-optimal planning is an optimisation problem, it is natural to analyse it through the lens of approximation. An important reason for studying cost-optimal planning is heuristic search; heuristic functions that guide the search in planning can often be viewed as algorithms solving or approximating certain optimisation problems. Many heuristic functions (such as the ubiquitious h+ heuristic) are based on delete relaxation, which ignores negative effects of actions. Planning for instances where the actions have no negative effects is often referred to as monotone planning. The aim of this article is to analyse the approximability of cost-optimal monotone planning, and thus the performance of relevant heuristic functions. Our findings imply that it may be beneficial to study these kind of problems within the framework of parameterised complexity and we initiate work in this direction.},
  archive      = {J_JAIR},
  author       = {Christer Bäckström and Peter Jonsson and Sebastian Ordyniak},
  doi          = {10.1613/jair.1.12278},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {169-204},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Cost-optimal planning, delete relaxation, approximability, and heuristics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zone pAth construction (ZAC) based approaches for effective
real-time ridesharing. <em>JAIR</em>, <em>70</em>, 119–167. (<a
href="https://doi.org/10.1613/jair.1.11998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time ridesharing systems such as UberPool, Lyft Line and GrabShare have become hugely popular as they reduce the costs for customers, improve per trip revenue for drivers and reduce traffic on the roads by grouping customers with similar itineraries. The key challenge in these systems is to group the “right” requests to travel together in the “right” available vehicles in real-time, so that the objective (e.g., requests served, revenue or delay) is optimized. This challenge has been addressed in existing work by: (i) generating as many relevant feasible combinations of requests (with respect to the available delay for customers) as possible in real-time; and then (ii) optimizing assignment of the feasible request combinations to vehicles. Since the number of request combinations increases exponentially with the increase in vehicle capacity and number of requests, unfortunately, such approaches have to employ ad hoc heuristics to identify a subset of request combinations for assignment. Our key contribution is in developing approaches that employ zone (abstraction of individual locations) paths instead of request combinations. Zone paths allow for generation of significantly more “relevant” combinations (in comparison to ad hoc heuristics) in real-time than competing approaches due to two reasons: (i) Each zone path can typically represent multiple request combinations; (ii) Zone paths are generated using a combination of offline and online methods. Specifically, we contribute both myopic (ridesharing assignment focussed on current requests only) and non-myopic (ridesharing assignment considers impact on expected future requests) approaches that employ zone paths. In our experimental results, we demonstrate that our myopic approach outperforms the current best myopic approach for ridesharing on both real-world and synthetic datasets (with respect to both objective and runtime). We also show that our non-myopic approach obtains 14.7\% improvement over existing myopic approach. Our non-myopic approach gets improvements of up to 12.48\% over a recent non-myopic approach, NeurADP. Even when NeurADP is allowed to optimize learning over test settings, results largely remain comparable except in a couple of cases, where NeurADP performs better.},
  archive      = {J_JAIR},
  author       = {Meghna Lowalekar and Pradeep Varakantham and Patrick Jaillet},
  doi          = {10.1613/jair.1.11998},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {119-167},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Zone pAth construction (ZAC) based approaches for effective real-time ridesharing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrated offline and online decision making under
uncertainty. <em>JAIR</em>, <em>70</em>, 77–117. (<a
href="https://doi.org/10.1613/jair.1.12333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers multi-stage optimization problems under uncertainty that involve distinct offline and online phases. In particular it addresses the issue of integrating these phases to show how the two are often interrelated in real-world applications. Our methods are applicable under two (fairly general) conditions: 1) the uncertainty is exogenous; 2) it is possible to define a greedy heuristic for the online phase that can be modeled as a parametric convex optimization problem. We start with a baseline composed by a two-stage offline approach paired with the online greedy heuristic. We then propose multiple methods to tighten the offline/online integration, leading to significant quality improvements, at the cost of an increased computation effort either in the offline or the online phase. Overall, our methods provide multiple options to balance the solution quality/time trade-off, suiting a variety of practical application scenarios. To test our methods, we ground our approaches on two real cases studies with both offline and online decisions: an energy management problem with uncertain renewable generation and demand, and a vehicle routing problem with uncertain travel times. The application domains feature respectively continuous and discrete decisions. An extensive analysis of the experimental results shows that indeed offline/online integration may lead to substantial benefits.},
  archive      = {J_JAIR},
  author       = {Allegra De Filippo and Michele Lombardi and Michela Milano},
  doi          = {10.1613/jair.1.12333},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {77-117},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Integrated offline and online decision making under uncertainty},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superintelligence cannot be contained: Lessons from
computability theory. <em>JAIR</em>, <em>70</em>, 65–76. (<a
href="https://doi.org/10.1613/jair.1.12202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. In light of recent advances in machine intelligence, a number of scientists, philosophers and technologists have revived the discussion about the potentially catastrophic risks entailed by such an entity. In this article, we trace the origins and development of the neo-fear of superintelligence, and some of the major proposals for its containment. We argue that total containment is, in principle, impossible, due to fundamental limits inherent to computing itself. Assuming that a superintelligence will contain a program that includes all the programs that can be executed by a universal Turing machine on input potentially as complex as the state of the world, strict containment requires simulations of such a program, something theoretically (and practically) impossible. This article is part of the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Manuel Alfonseca and Manuel Cebrian and Antonio Fernandez Anta and Lorenzo Coviello and Andrés Abeliuk and Iyad Rahwan},
  doi          = {10.1613/jair.1.12202},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {65-76},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Superintelligence cannot be contained: Lessons from computability theory},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the decomposition of abstract dialectical frameworks and
the complexity of naive-based semantics. <em>JAIR</em>, <em>70</em>,
1–64. (<a href="https://doi.org/10.1613/jair.1.11348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract dialectical frameworks (ADFs) are a recently introduced powerful generalization of Dung’s popular abstract argumentation frameworks (AFs). Inspired by similar work for AFs, we introduce a decomposition scheme for ADFs, which proceeds along the ADF’s strongly connected components. We find that, for several semantics, the decomposition-based version coincides with the original semantics, whereas for others, it gives rise to a new semantics. These new semantics allow us to deal with pertinent problems such as odd-length negative cycles in a more general setting, that for instance also encompasses logic programs. We perform an exhaustive analysis of the computational complexity of these new, so-called naive-based semantics. The results are quite interesting, for some of them involve little-known classes of the so-called Boolean hierarchy (another hierarchy in between classes of the polynomial hierarchy). Furthermore, in credulous and sceptical entailment, the complexity can be different depending on whether we check for truth or falsity of a specific statement.},
  archive      = {J_JAIR},
  author       = {Sarah Alice Gaggl and Sebastian Rudolph and Hannes Straß},
  doi          = {10.1613/jair.1.11348},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-64},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the decomposition of abstract dialectical frameworks and the complexity of naive-based semantics},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
