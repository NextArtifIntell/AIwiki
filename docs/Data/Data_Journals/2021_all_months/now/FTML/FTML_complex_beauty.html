<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftml---6">FTML - 6</h2>
<ul>
<li><details>
<summary>
(2021). Dynamical variational autoencoders: A comprehensive review.
<em>FTML</em>, <em>15</em>(1-2), 1–175. (<a
href="https://doi.org/10.1561/2200000089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this monograph, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The monograph concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.},
  archive      = {J_FTML},
  author       = {Laurent Girin and Simon Leglaive and Xiaoyu Bie and Julien Diard and Thomas Hueber and Xavier Alameda-Pineda},
  doi          = {10.1561/2200000089},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {1-2},
  pages        = {1-175},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Dynamical variational autoencoders: A comprehensive review},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning for automated theorem proving: Learning to
solve SAT and QSAT. <em>FTML</em>, <em>14</em>(6), 807–989. (<a
href="https://doi.org/10.1561/2200000081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision problem for Boolean satisfiability, generally referred to as SAT, is the archetypal NP-complete problem, and encodings of many problems of practical interest exist allowing them to be treated as SAT problems. Its generalization to quantified SAT (QSAT) is PSPACE-complete, and is useful for the same reason. Despite the computational complexity of SAT and QSAT, methods have been developed allowing large instances to be solved within reasonable resource constraints. These techniques have largely exploited algorithmic developments; however machine learning also exerts a significant influence in the development of state-ofthe- art solvers. Here, the application of machine learning is delicate, as in many cases, even if a relevant learning problem can be solved, it may be that incorporating the result into a SAT or QSAT solver is counterproductive, because the run-time of such solvers can be sensitive to small implementation changes. The application of better machine learning methods in this area is thus an ongoing challenge, with characteristics unique to the field. This work provides a comprehensive review of the research to date on incorporating machine learning into SAT and QSAT solvers, as a resource for those interested in further advancing the field.},
  archive      = {J_FTML},
  author       = {Sean B. Holden},
  doi          = {10.1561/2200000081},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {6},
  pages        = {807-989},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Machine learning for automated theorem proving: Learning to solve SAT and QSAT},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral methods for data science: A statistical
perspective. <em>FTML</em>, <em>14</em>(5), 566–806. (<a
href="https://doi.org/10.1561/2200000079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, imaging science, financial and econometric modeling, and signal processing, including recommendation systems, community detection, ranking, structured matrix recovery, tensor data estimation, joint shape matching, blind deconvolution, financial investments, risk managements, treatment evaluations, causal inference, amongst others. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to facilitate other more sophisticated algorithms to enhance performance.While the studies of spectral methods can be traced back to classical matrix perturbation theory and the method of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of concentration inequalities and non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional ℓ2 perturbation analysis, we present a systematic ℓ∞ and ℓ2,∞ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful “leave-one-out” analysis framework.},
  archive      = {J_FTML},
  author       = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma},
  doi          = {10.1561/2200000079},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {5},
  pages        = {566-806},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Spectral methods for data science: A statistical perspective},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor regression. <em>FTML</em>, <em>14</em>(4), 379–565.
(<a href="https://doi.org/10.1561/2200000087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of multidirectional correlations in emerging multidimensional data poses a challenge to traditional regression modeling methods. Traditional modeling methods based on matrix or vector, for example, not only overlook the data’s multidimensional information and lower model performance, but also add additional computations and storage requirements. Driven by the recent advances in applied mathematics, tensor regression has been widely used and proven effective in many fields, such as sociology, climatology, geography, economics, computer vision, chemometrics, and neuroscience. Tensor regression can explore multidirectional relatedness, reduce the number of model parameters and improve model robustness and efficiency. It is timely and valuable to summarize the developments of tensor regression in recent years and discuss promising future directions, which will help accelerate the research process of tensor regression, broaden the research direction, and provide tutorials for researchers interested in high dimensional regression tasks.The fundamentals, motivations, popular algorithms, related applications, available datasets, and software resources for tensor regression are all covered in this monograph. The first part focuses on the key concepts for tensor regression, mainly analyzing existing tensor regression algorithms from the perspective of regression families. Meanwhile, the adopted low rank tensor representations and optimization frameworks are also summarized. In addition, several extensions in online learning and sketching are described. The second part covers related applications, widely used public datasets and software resources, as well as some real-world examples, such as multitask learning, spatiotemporal learning, human motion analysis, facial image analysis, neuroimaging analysis (disease diagnosis, neuron decoding, brain activation, and connectivity analysis) and chemometrics. This survey can be used as a basic reference in tensor-regression-related fields and assist readers in efficiently dealing with high dimensional regression tasks.},
  archive      = {J_FTML},
  author       = {Jiani Liu and Ce Zhu and Zhen Long and Yipeng Liu},
  doi          = {10.1561/2200000087},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {4},
  pages        = {379-565},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Tensor regression},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimum-distortion embedding. <em>FTML</em>, <em>14</em>(3),
211–378. (<a href="https://doi.org/10.1561/2200000090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the vector embedding problem. We are given a finite set of items, with the goal of assigning a representative vector to each one, possibly under some constraints (such as the collection of vectors being standardized, i.e., having zero mean and unit covariance). We are given data indicating that some pairs of items are similar, and optionally, some other pairs are dissimilar. For pairs of similar items, we want the corresponding vectors to be near each other, and for dissimilar pairs, we want the vectors to not be near each other, measured in Euclidean distance. We formalize this by introducing distortion functions, defined for some pairs of items. Our goal is to choose an embedding that minimizes the total distortion, subject to the constraints. We call this the minimum-distortion embedding (MDE) problem.The MDE framework is simple but general. It includes a wide variety of specific embedding methods, such as spectral embedding, principal component analysis, multidimensional scaling, Euclidean distance problems, dimensionality reduction methods (like Isomap and UMAP), semi-supervised learning, sphere packing, force-directed layout, and others. It also includes new embeddings, and provides principled ways of validating or sanity-checking historical and new embeddings alike.In a few special cases, MDE problems can be solved exactly. For others, we develop a projected quasi-Newton method that approximately minimizes the distortion and scales to very large data sets, while placing few assumptions on the distortion functions and constraints. This monograph is accompanied by an open-source Python package, PyMDE, for approximately solving MDE problems. Users can select from a library of distortion functions and constraints or specify custom ones, making it easy to rapidly experiment with new embeddings. Because our algorithm is scalable, and because PyMDE can exploit GPUs, our software scales to problems with millions of items and tens of millions of distortion functions. Additionally, PyMDE is competitive in runtime with specialized implementations of specific embedding methods. To demonstrate our method, we compute embeddings for several real-world data sets, including images, an academic co-author network, US county demographic data, and single-cell mRNA transcriptomes.},
  archive      = {J_FTML},
  author       = {Akshay Agrawal and Alnur Ali and Stephen Boyd},
  doi          = {10.1561/2200000090},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {3},
  pages        = {211-378},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Minimum-distortion embedding},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advances and open problems in federated learning.
<em>FTML</em>, <em>14</em>(1–2), 1–210. (<a
href="https://doi.org/10.1561/2200000083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g., service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this monograph discusses recent advances and presents an extensive collection of open problems and challenges.},
  archive      = {J_FTML},
  author       = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D’Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konecný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
  doi          = {10.1561/2200000083},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {1–2},
  pages        = {1-210},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Advances and open problems in federated learning},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
