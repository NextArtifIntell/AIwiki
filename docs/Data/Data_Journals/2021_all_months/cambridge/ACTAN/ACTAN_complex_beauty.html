<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ACTAN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="actan---9">ACTAN - 9</h2>
<ul>
<li><details>
<summary>
(2021). ANU volume 30 cover and back matter. <em>ACTAN</em>,
<em>30</em>, b1. (<a
href="https://doi.org/10.1017/S0962492921000106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ACTAN},
  doi          = {10.1017/S0962492921000106},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {b1},
  shortjournal = {Acta Numer.},
  title        = {ANU volume 30 cover and back matter},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling and computation of liquid crystals.
<em>ACTAN</em>, <em>30</em>, 765–851. (<a
href="https://doi.org/10.1017/S0962492921000088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liquid crystals are a type of soft matter that is intermediate between crystalline solids and isotropic fluids. The study of liquid crystals has made tremendous progress over the past four decades, which is of great importance for fundamental scientific research and has widespread applications in industry. In this paper we review the mathematical models and their connections to liquid crystals, and survey the developments of numerical methods for finding rich configurations of liquid crystals.},
  archive      = {J_ACTAN},
  author       = {Wei Wang and Lei Zhang and Pingwen Zhang},
  doi          = {10.1017/S0962492921000088},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {765-851},
  shortjournal = {Acta Numer.},
  title        = {Modelling and computation of liquid crystals},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensors in computations. <em>ACTAN</em>, <em>30</em>,
555–764. (<a href="https://doi.org/10.1017/S0962492921000076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of a tensor captures three great ideas: equivariance, multilinearity, separability. But trying to be three things at once makes the notion difficult to understand. We will explain tensors in an accessible and elementary way through the lens of linear algebra and numerical linear algebra, elucidated with examples from computational and applied mathematics.},
  archive      = {J_ACTAN},
  author       = {Lek-Heng Lim},
  doi          = {10.1017/S0962492921000076},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {555-764},
  shortjournal = {Acta Numer.},
  title        = {Tensors in computations},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning physics-based models from data: Perspectives from
inverse problems and model reduction. <em>ACTAN</em>, <em>30</em>,
445–554. (<a href="https://doi.org/10.1017/S0962492921000064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the inference of physics models from data, from the perspectives of inverse problems and model reduction. These fields develop formulations that integrate data into physics-based models while exploiting the fact that many mathematical models of natural and engineered systems exhibit an intrinsically low-dimensional solution manifold. In inverse problems, we seek to infer uncertain components of the inputs from observations of the outputs, while in model reduction we seek low-dimensional models that explicitly capture the salient features of the input–output map through approximation in a low-dimensional subspace. In both cases, the result is a predictive model that reflects data-driven learning yet deeply embeds the underlying physics, and thus can be used for design, control and decision-making, often with quantified uncertainties. We highlight recent developments in scalable and efficient algorithms for inverse problems and model reduction governed by large-scale models in the form of partial differential equations. Several illustrative applications to large-scale complex problems across different domains of science and engineering are provided.},
  archive      = {J_ACTAN},
  author       = {Omar Ghattas and Karen Willcox},
  doi          = {10.1017/S0962492921000064},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {445-554},
  shortjournal = {Acta Numer.},
  title        = {Learning physics-based models from data: Perspectives from inverse problems and model reduction},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network approximation. <em>ACTAN</em>, <em>30</em>,
327–444. (<a href="https://doi.org/10.1017/S0962492921000052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) are the method of choice for building learning algorithms. They are now being investigated for other numerical tasks such as solving high-dimensional partial differential equations. Their popularity stems from their empirical success on several challenging learning problems (computer chess/Go, autonomous navigation, face recognition). However, most scholars agree that a convincing theoretical explanation for this success is still lacking. Since these applications revolve around approximating an unknown function from data observations, part of the answer must involve the ability of NNs to produce accurate approximations. This article surveys the known approximation properties of the outputs of NNs with the aim of uncovering the properties that are not present in the more traditional methods of approximation used in numerical analysis, such as approximations using polynomials, wavelets, rational functions and splines. Comparisons are made with traditional approximation methods from the viewpoint of rate distortion, i.e. error versus the number of parameters used to create the approximant. Another major component in the analysis of numerical approximation is the computational time needed to construct the approximation, and this in turn is intimately connected with the stability of the approximation algorithm. So the stability of numerical approximation using NNs is a large part of the analysis put forward. The survey, for the most part, is concerned with NNs using the popular ReLU activation function. In this case the outputs of the NNs are piecewise linear functions on rather complicated partitions of the domain of f into cells that are convex polytopes. When the architecture of the NN is fixed and the parameters are allowed to vary, the set of output functions of the NN is a parametrized nonlinear manifold. It is shown that this manifold has certain space-filling properties leading to an increased ability to approximate (better rate distortion) but at the expense of numerical stability. The space filling creates the challenge to the numerical method of finding best or good parameter choices when trying to approximate.},
  archive      = {J_ACTAN},
  author       = {Ronald DeVore and Boris Hanin and Guergana Petrova},
  doi          = {10.1017/S0962492921000052},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {327-444},
  shortjournal = {Acta Numer.},
  title        = {Neural network approximation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal transportation, modelling and numerical simulation.
<em>ACTAN</em>, <em>30</em>, 249–325. (<a
href="https://doi.org/10.1017/S0962492921000040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an overviewof the basic theory, modern optimal transportation extensions and recent algorithmic advances. Selected modelling and numerical applications illustrate the impact of optimal transportation in numerical analysis.},
  archive      = {J_ACTAN},
  author       = {Jean-David Benamou},
  doi          = {10.1017/S0962492921000040},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {249-325},
  shortjournal = {Acta Numer.},
  title        = {Optimal transportation, modelling and numerical simulation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fit without fear: Remarkable mathematical phenomena of deep
learning through the prism of interpolation. <em>ACTAN</em>,
<em>30</em>, 203–248. (<a
href="https://doi.org/10.1017/S0962492921000039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade the mathematical theory of machine learning has lagged far behind the triumphs of deep neural networks on practical challenges. However, the gap between theory and practice is gradually starting to close. In this paper I will attempt to assemble some pieces of the remarkable and still incomplete mathematical mosaic emerging from the efforts to understand the foundations of deep learning. The two key themes will be interpolation and its sibling over-parametrization. Interpolation corresponds to fitting data, even noisy data, exactly. Over-parametrization enables interpolation and provides flexibility to select a suitable interpolating model. As we will see, just as a physical prism separates colours mixed within a ray of light, the figurative prism of interpolation helps to disentangle generalization and optimization properties within the complex picture of modern machine learning. This article is written in the belief and hope that clearer understanding of these issues will bring us a step closer towards a general theory of deep learning and machine learning.},
  archive      = {J_ACTAN},
  author       = {Mikhail Belkin},
  doi          = {10.1017/S0962492921000039},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {203-248},
  shortjournal = {Acta Numer.},
  title        = {Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning: A statistical viewpoint. <em>ACTAN</em>,
<em>30</em>, 87–201. (<a
href="https://doi.org/10.1017/S0962492921000027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting, that is, accurate predictions despite overfitting training data. In this article, we survey recent progress in statistical learning theory that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behaviour of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favourable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.},
  archive      = {J_ACTAN},
  author       = {Peter L. Bartlett and Andrea Montanari and Alexander Rakhlin},
  doi          = {10.1017/S0962492921000027},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {87-201},
  shortjournal = {Acta Numer.},
  title        = {Deep learning: A statistical viewpoint},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Numerical homogenization beyond scale separation.
<em>ACTAN</em>, <em>30</em>, 1–86. (<a
href="https://doi.org/10.1017/S0962492921000015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical homogenization is a methodology for the computational solution of multiscale partial differential equations. It aims at reducing complex large-scale problems to simplified numerical models valid on some target scale of interest, thereby accounting for the impact of features on smaller scales that are otherwise not resolved. While constructive approaches in the mathematical theory of homogenization are restricted to problems with a clear scale separation, modern numerical homogenization methods can accurately handle problems with a continuum of scales. This paper reviews such approaches embedded in a historical context and provides a unified variational framework for their design and numerical analysis. Apart from prototypical elliptic model problems, the class of partial differential equations covered here includes wave scattering in heterogeneous media and serves as a template for more general multi-physics problems.},
  archive      = {J_ACTAN},
  author       = {Robert Altmann and Patrick Henning and Daniel Peterseim},
  doi          = {10.1017/S0962492921000015},
  journal      = {Acta Numerica},
  month        = {5},
  pages        = {1-86},
  shortjournal = {Acta Numer.},
  title        = {Numerical homogenization beyond scale separation},
  volume       = {30},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
