<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIREV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sirev---45">SIREV - 45</h2>
<ul>
<li><details>
<summary>
(2021a). Book reviews. <em>SIREV</em>, <em>63</em>(4), 867–875. (<a
href="https://doi.org/10.1137/21N975369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first half of this section is devoted to books related to data science. The featured review on the book Insights from Data with R. An Introduction for the Life and Environmental Sciences, authored by Owen L. Petchey, Andrew P. Beckerman, Natalie Cooper, and Dylan Z. Childs, praises the didactic approach of the book, which starts with a complex case study. Reviewer Laura Vana recommends the book as a valuable addition, if one chooses R as the programming language. We continue with Anita Layton&#39;s insightful review on the book Deep Learning for Medical Decision Support Systems, by Utku Kose, Omer Deperlioglu, Jafar Alzubi, and Bogdan Patruti. Anita compares the book in her diligent review with a long review article on the book topic. This first part of our reviews on books in the field of data science is concluded by the book Low-Rank Approximation: Algorithms, Implementation, Approximation, which is authored by Ivan Markovsky. In his review, Boris Khoromskij praises the mutual feedback between theory and software implementation in the book, which “allow the reader to adapt the presented examples to some modified problems and to continue development of the new schemes for different problem classes.” The second half of this section is related to more classical topics. First, there is the book Introduction to Functional Analysis, by Christian Clason. Armin Schikorra points out in his review that “the main difference between this book and the existing literature I am aware of is the relentless focus on providing lecture notes that actually can be realistically taught in a one-semester course.” Then we have the book on Controllability and Stabilization of Parabolic Equations, written by Viorel Barbu, which is reviewed by Mohamed Ouzahra, who mentions that the focus of the book is on the control of parabolic problems. Finally, Stéphanie Abo and Anita Layton review the book The Dynamics of Biological Systems, edited by Arianna Bianchi, Thomas Hillen, Mark A. Lewis, and Yingfei Yi. Stéphanie and Anita praise the “eclectic content on the theory of dynamical systems and associated biological applications” as a strong point of this collection of nine minicourses from a summer school.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/21N975369},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {867-875},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A precise and reliable multivariable chain rule.
<em>SIREV</em>, <em>63</em>(4), 854–864. (<a
href="https://doi.org/10.1137/19M1296732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariable chain rule is often challenging to students because it is usually presented with ambiguities and other defects that hamper systematic and reliable application. A very simple formulation combines the derivation operators for functions and for expressions in a manner not found elsewhere due to common confusion between them. Some issues are rooted more deeply than others and are discussed in a broader perspective, starting with the function concept. The approach is illustrated using various applications including the transport equation, partial derivatives of a definite integral, and the distortionless (but not lossless) transmission line. This note is suitable for a lecture in any first-year course covering partial derivatives, as a complement to the other course material.},
  archive      = {J_SIREV},
  author       = {Raymond Boute},
  doi          = {10.1137/19M1296732},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {854-864},
  shortjournal = {SIAM Rev.},
  title        = {A precise and reliable multivariable chain rule},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding graph embedding methods and their
applications. <em>SIREV</em>, <em>63</em>(4), 825–853. (<a
href="https://doi.org/10.1137/20M1386062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph analytics can lead to better quantitative understanding and control of complex networks, but traditional methods suffer from the high computational cost and excessive memory requirements associated with the high-dimensionality and heterogeneous characteristics of industrial size networks. Graph embedding techniques can be effective in converting high-dimensional sparse graphs into low-dimensional, dense, and continuous vector spaces, preserving maximally the graph structure properties. Another type of emerging graph embedding employs Gaussian distribution--based graph embedding with important uncertainty estimation. The main goal of graph embedding methods is to pack every node&#39;s properties into a vector with a smaller dimension; hence, node similarity in the original complex irregular spaces can be easily quantified in the embedded vector spaces using standard metrics. The nonlinear and highly informative graph embeddings generated in the latent space can be conveniently used to address different downstream graph analytics tasks (e.g., node classification, link prediction, community detection, visualization, etc.). In this review, we present some fundamental concepts in graph analytics and graph embedding methods, focusing in particular on random walk--based and neural network--based methods. We also discuss the emerging deep learning--based dynamic graph embedding methods. We highlight the distinct advantages of graph embedding methods in four diverse applications, and we present implementation details and references to open-source software as well as available databases in the supplementary material to help interested readers start their exploration into graph analytics.},
  archive      = {J_SIREV},
  author       = {Mengjia Xu},
  doi          = {10.1137/20M1386062},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {825-853},
  shortjournal = {SIAM Rev.},
  title        = {Understanding graph embedding methods and their applications},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Education. <em>SIREV</em>, <em>63</em>(4), 823. (<a
href="https://doi.org/10.1137/21N975357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Education section in this issue presents two papers. The first paper, “Understanding Graph Embedding Methods and Their Applications,” is written by Mengjia Xu. Graph theory is part of discrete mathematics, whose origin is attributed to the paper of Leonhard Euler, “Seven Bridges of Königsberg,” published in 1736. With the rise of data science and machine learning, graphs have found applications in very many diverse fields. Next to applications in knots and group theory, graphs provide a popular modeling framework in reliability theory, in analysis of molecular networks, images, brain networks, protein--protein interaction networks, social networks, banking-institution networks, transportation, computer and wireless networks, and many others. Modern applications of graph theory to large-scale complex networks require efficient methods to store and analyze large graphs. This paper surveys techniques for creating low-dimensional, dense, and possibly continuous models of high-dimensional sparse graphs, so that the structural properties of the graphs are maximally preserved. Three major approaches to graph embedding methods exist in the literature: methods based on matrix factorization, on random walks, or on neural networks. The latter two frameworks are the main focus of this paper. The author outlines the key ideas and steps in the graph embedding techniques, pointing to the articles where the specifics are discussed. Structural preservation is assessed via quantifying proximity preservation, neighbor-based node similarity, similarity based on the adjacency matrix, and other criteria. In most applications, the systems represented by graphs evolve in time. Therefore, the development of dynamic models, which would properly reflect networks changing in time, is of great interest. Some ideas for dynamic embeddings are surveyed in this paper as well. The last part of the paper visits several popular applications: analysis of social networks, evaluation of the impact of scientific papers based on their citations, models of the brain operations, and genomic networks. The paper includes a substantial literature review and would be useful to anyone who looks for an introduction to the numerical techniques for compact representations of large graphs. The second paper, “A Precise and Reliable Multivariable Chain Rule,” by Raymond Boute, is directed to instructors of undergraduate students. It aims to clarify imprecise language and presentations pertaining to the notion of derivatives, to the process of calculating them, as well as to the notation used in this context. The paper pays special attention to derivatives of a composition of functions. A reliable formulation of the multivariable chain rule is presented and illustrated in the context of two examples. The discussion starts with clarifications of the notions of a function, its restriction to a certain domain, and its co-domain. The author analyzes various expressions occurring in the context of differentiation and comments on proper notation and careful delineation between numbers and functional expressions. Further comments pertain to the use of dimensional analysis in verifying the calculations. The examples demonstrate how the chain rule facilitates changing variables without errors and how it conveys structure to the calculations. The author concludes that “adhering to better practices than in the textbook can be problematic for instructors. However, even when some flawed notation is considered `common,&#39; it is in the best interest of the students to provide a better alternative.”},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/21N975357},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {823},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convex relaxation of discrete vector-valued optimization
problems. <em>SIREV</em>, <em>63</em>(4), 783–821. (<a
href="https://doi.org/10.1137/21M1426237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of infinite-dimensional optimization problems in which a distributed vector-valued variable should pointwise almost everywhere take values from a given finite set $\mathcal{M}\subset\mathbb{R}^m$. Such hybrid discrete-continuous problems occur in, e.g., topology optimization or medical imaging and are challenging due to their lack of weak lower semicontinuity. To circumvent this difficulty, we introduce as a regularization term a convex integral functional with an integrand that has a polyhedral epigraph with vertices corresponding to the values of $\mathcal{M}$; similar to the $L^1$ norm in sparse regularization, this “vector multibang penalty” promotes solutions with the desired structure while allowing the use of tools from convex optimization for the analysis as well as the numerical solution of the resulting problem. We show well-posedness of the regularized problem and analyze stability properties of its solution in a general setting. We then illustrate the approach for three specific model optimization problems of broader interest: optimal control of the Bloch equation, optimal control of an elastic deformation, and a multimaterial branched transport problem. In the first two cases, we derive explicit characterizations of the penalty and its generalized derivatives for a concrete class of sets $\mathcal{M}$. For the third case, we discuss the algorithmic computation of these derivatives for general sets. These derivatives are then used in a superlinearly convergent semismooth Newton method applied to a sequence of regularized optimization problems. We illustrate the behavior of this approach for the three model problems with numerical examples.},
  archive      = {J_SIREV},
  author       = {Christian Clason and Carla Tameling and Benedikt Wirth},
  doi          = {10.1137/21M1426237},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {783-821},
  shortjournal = {SIAM Rev.},
  title        = {Convex relaxation of discrete vector-valued optimization problems},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). SIGEST. <em>SIREV</em>, <em>63</em>(4), 781. (<a
href="https://doi.org/10.1137/21N975345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue, which comes from the SIAM Journal on Control and Optimization (SICON), is “Convex Relaxation of Discrete Vector-Valued Optimization Problems,” by Christian Clason, Carla Tameling, and Benedikt Wirth. The authors tackle a new class of problems where a vector-valued control must take pointwise values in a given, finite set. The resulting optimization task involves a regularization term akin to the classical $L_1$ regularization that has been used to promote sparsity. This paper expertly combines techniques from optimal control theory, convex analysis, relaxation, and numerical algorithms. Meaningful computational examples involving nuclear magnetic resonance imaging and linearized elasticity are used to illustrate the theory. A further example concerning transport of material through a street or pipe network has been added to this SIGEST version in order to showcase the broad applicability of the approach beyond typical optimal control problems. MATLAB code is available for these experiments. The revision for SIGEST also includes extra details in section 4 on the explicit characterization of the convex penalty and its generalized derivatives for general constraint sets and a discussion on how to evaluate them algorithmically; this procedure in particular is used in the numerical computations on the new example. In addition, the authors have extended the abstract and introduction, and provided more details and references in sections 2 and 3 on convex analysis and Gamma convergence, and on related work that has been published since the original 2018 SICON version appeared.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/21N975345},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {781},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supraconservative finite-volume methods for the euler
equations of subsonic compressible flow. <em>SIREV</em>, <em>63</em>(4),
756–779. (<a href="https://doi.org/10.1137/20M1317050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been found advantageous for finite-volume discretizations of flow equations to possess additional (secondary) invariants, besides the (primary) invariants from the constituting conservation laws. This paper presents general (necessary and sufficient) requirements for a method to convectively preserve discrete kinetic energy. The key ingredient is a close discrete consistency between the convective term in the momentum equation and the terms in the other conservation equations (mass, internal energy). As examples, the Euler equations for subsonic (in)compressible flow are discretized with such supraconservative finite-volume methods on structured as well as unstructured grids.},
  archive      = {J_SIREV},
  author       = {Arthur E. P. Veldman},
  doi          = {10.1137/20M1317050},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {756-779},
  shortjournal = {SIAM Rev.},
  title        = {Supraconservative finite-volume methods for the euler equations of subsonic compressible flow},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Industry-relevant implicit large-eddy simulation of a
high-performance road car via spectral/hp element methods.
<em>SIREV</em>, <em>63</em>(4), 723–755. (<a
href="https://doi.org/10.1137/20M1345359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a successful deployment of high-fidelity large-eddy simulation (LES) technologies based on spectral/$hp$ element methods to industrial flow problems, which are characterized by high Reynolds numbers and complex geometries. In particular, we describe the numerical methods, software development, and steps that were required to perform the implicit LES of a real automotive car, namely, the Elemental Rp1 model. To the best of the authors&#39; knowledge, this simulation represents the first high-order accurate transient LES of an entire real car geometry. Moreover, it constitutes a key milestone toward considerably expanding the computational design envelope currently allowed in industry, where steady-state modeling remains the standard. A number of novel developments had to be made in order to overcome obstacles in mesh generation and solver technology to achieve this simulation, which we detail in this paper. The main objective is to present to the industrial and applied mathematics community a viable pathway to translating academic developments into industrial tools that can substantially advance the analysis and design capabilities of high-end engineering stakeholders. The novel developments and results were achieved using the academic-driven open-source framework \nekpp.},
  archive      = {J_SIREV},
  author       = {Gianmarco Mengaldo and David Moxey and Michael Turner and Rodrigo Costa Moura and Ayad Jassim and Mark Taylor and Joaquim Peiró and Spencer Sherwin},
  doi          = {10.1137/20M1345359},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {723-755},
  shortjournal = {SIAM Rev.},
  title        = {Industry-relevant implicit large-eddy simulation of a high-performance road car via spectral/hp element methods},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Research spotlights. <em>SIREV</em>, <em>63</em>(4), 721.
(<a href="https://doi.org/10.1137/21N975333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The car-themed cover art on the current issue of SIAM Review is courtesy of the first of two Research Spotlights articles this issue. Indeed, the paper “Industry-Relevant Implicit Large-Eddy Simulation of a High-Performance Road Car via Spectral/$hp$ Element Methods&quot; delivers what the title implies: using a high-fidelity, large-eddy simulation (LES) technology that is based on spectral/$hp$ element methods, the group of eight authors demonstrates in this article how it is possible to simulate performance of an Elemental Rp1 road car. Given the high Reynolds numbers and complex geometry involved, the authors carefully address both the creation of highly accurate meshes while also producing “physically faithful&quot; results in reasonable time. To this end, one section is devoted to the high-order meshing strategy that is used to prevent unphysical diffusion. A separate section on the numerical methods describes the regularization approach the authors use to treat underresolved scales, the de-aliasing strategies employed, and how to use parallelization and preconditioned iterative solvers to address the need for computational efficiency. Colorful and detailed illustrations accompany the simulation studies in section 4, allowing the reader to visually assess the impact of the proposed approach. In short, the authors propose a comprehensive strategy that allows methods that were developed within the academic community along with new innovations to be translated into the space of an industrial CFD application for substantial gain. Their success may inspire other researchers to follow this road in the future. Readers, start your engines! Finite-volume methods for discretization of fluid flow are called supraconservative if, besides preserving primary invariants (e.g., those invariants pertaining to primary variables from the constituting conservation laws), they also preserve secondary (i.e., other) invariants. A finite-volume method having this property ensures better discrete equivalence among the discretizations of the analytically equivalent formulations of the model. The RS paper “Supraconservative Finite-Volume Methods for the Euler Equations of Subsonic Compressible Flow,&quot; authored by Arthur E. P. Veldman and presented in this issue, is concerned with the construction of such finite-volume methods. The starting point is the analytic formulation in (1), and the goal is the secondary conservation of energy in the finite-volume method. According to the author, there is a feature that sets the present approach apart from most of the discretization approaches covered in the extensive literature review contained herein: after discretizing (1.1) in such a way as to ensure the discrete conservation of the primary invariants, the author does not return to the analytical formulation but rather uses the “freedom left in the [discrete level] formulation&quot; to generate the secondary invariants. The paper provides the details for a supraconservative method for incompressible flow on two grid types: (1) structured, staggered and (2) unstructured, collocated. The author leaves readers to consider a few open questions, such as whether “their favorite discretization approach can be made to satisfy [the] requirements&quot; given here that would allow them to view their method through a similar lens.},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/21N975333},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {721},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computation and applications of mathieu functions: A
historical perspective. <em>SIREV</em>, <em>63</em>(4), 653–720. (<a
href="https://doi.org/10.1137/20M135786X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathieu functions of period $\pi$ or $2\pi$, also called elliptic cylinder functions, were introduced in 1868 by Émile Mathieu together with so-called modified Mathieu functions, in order to help understand the vibrations of an elastic membrane set in a fixed elliptical hoop. These functions still occur frequently in applications today; our interest, for instance, was stimulated by a problem of pulsatile blood flow in a blood vessel compressed into an elliptical cross section. This paper surveys and recapitulates the historical development of the theory and methods of computation for Mathieu functions and modified Mathieu functions and identifies some gaps in current software capability, particularly related to double eigenvalues of the Mathieu equation. We demonstrate how to compute Puiseux expansions of the Mathieu eigenvalues about such double eigenvalues and give methods to compute the generalized eigenfunctions that arise there. In examining Mathieu&#39;s original contribution, we bring out that his use of antisecularity predates that of Lindstedt. For historical interest, we also provide short biographies of some of the major mathematical researchers involved in the history of the Mathieu functions: Émile Mathieu, Sir Edmund Whittaker, Edward Ince, and Gertrude Blanch.},
  archive      = {J_SIREV},
  author       = {Chris Brimacombe and Robert M. Corless and Mair Zamir},
  doi          = {10.1137/20M135786X},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {653-720},
  shortjournal = {SIAM Rev.},
  title        = {Computation and applications of mathieu functions: A historical perspective},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Survey and review. <em>SIREV</em>, <em>63</em>(4), 651. (<a
href="https://doi.org/10.1137/21N975321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Survey and Review paper in this issue, “Computation and Applications of Mathieu Functions: A Historical Perspective,” by Chris Brimacombe, Robert M. Corless, and Mair Zamir, deals with a well-known family of special functions. Special functions are of course an important element in the toolkit of many applied mathematicians. For instance, they provide the basis for many modern spectral algorithms to solve differential equations, and their presence is pervasive in mathematical physics. During the mid-1990s, the best-known reference in the area, Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, by Milton Abramowitz and Irene Stegun (1964), was cited every 1.5 hours of each working day. The Digital Library of Mathematical Functions is intended to provide an updated online replacement of the handbook. While today special functions remain beyond doubt a valuable technique, sixty or seventy years ago, in the precomputer world, they often were the only available tool to tackle a given problem. When performing computations was not only extremely expensive (in time and money) but also error prone and therefore rather unreliable, tasks now deemed easy, such as finding the zeros or the maxima of a function arising in an application, could be major challenges. Usually, mathematicians had to limit themselves to a few dozens of well-chosen functions whose properties were available in tabular form; the problem at hand was then simplified or approximated until it could be described in terms of those “known” functions. Elementary functions like \(\exp(x)\), \(łog(x)\), \(\sin(x)\), etc., were of course the first “known” reference functions, but the list increased, mainly in the nineteenth century, with the addition of special functions, like elliptic or Bessel functions. Often---but not always---special functions were defined as solutions of particularly important linear differential equations with variable coefficients. The Mathieu functions in the paper by Brimacombe, Corless, and Zamir appeared as solutions of the ordinary differential equations one obtains when separating variables in the partial differential equation that describes the oscillations of an elliptic membrane. The initial motivation for the paper was the study of hemodynamics in vessels that have been deformed by external forces and cannot be considered to have a circular cross section. The authors describe several applications of the Mathieu functions, such as the study of the vibrational stabilization of the inverted pendulum, and then provide a historical perspective that includes biographies of the main contributors to the subject. This historical component will make readers aware of how much applied mathematics has benefited from computers. For instance, we shall read that Gertrude Blanch (1898--1997) had to organize a group of 450 people to carry out her tabulations (there is little doubt that the combined computing power of those people would have been a tiny fraction of the power of the cheapest contemporary laptop). The paper not only looks at the past. The authors also report on the difficulties they encountered when trying to handle Mathieu functions with available packaged software and describe the alternative numerical and symbolic methods they had to devise. Some of these, like the construction of the high-order Taylor integrator or the use of Newton&#39;s root-finding method with series (as distinct from with numbers), will be of independent value to everybody involved in computing.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/21N975321},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {651},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Book reviews. <em>SIREV</em>, <em>63</em>(3), 641–650. (<a
href="https://doi.org/10.1137/21N975308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of books in this issue are devoted to modeling in one form or another. The featured review by Thomas Wick discusses a new book by an internationally renowned expert in the field of numerical modeling. It is the book A Primer on Mathematical Modelling, by Alfio Quarteroni and Paola Gervasio. Wick talks enthusiastically about the book and recommends it for undergraduate and graduate students. The second book reviewed here is also aimed primarily at students and deals with modeling in a broader sense. Jürgen Müller praises Henry Ricardo&#39;s book on A Modern Introduction to Differential Equations for its many instructive examples. This is followed by a review by Lars Grüne of the book Mathematical Control Theory by Jerzy Zabczyk. Grüne emphasizes the joint treatment of finite- and infinite-dimensional systems as a unique feature. The book Mathematical Models in Epidemiology, by Fred Brauer, Carlos Castillo-Chavez, and Zhilan Feng, covers a very timely topic that has become even more important currently than it was when it was printed two years ago. Reviewer Roslyn Hickson appreciates it as a book she “will likely be referring to \dots for useful examples” in her own teaching. Mathematical Modeling and Computation in Finance, by Cornelis Oosterlee and Lech Grzelak, also deals with modeling. It is reviewed by Alfio Borzì, who emphasizes the interplay of stochastic modeling, probability theory, and numerical analysis as leitmotif. The section is concluded by a review by Anita Layton and Mehrshad Sadria on the book Mining of Massive Datasets, by Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. They praise in particular the extensive exercises found in almost every section.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/21N975308},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {641-650},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approaches toward understanding delay-induced stability and
instability. <em>SIREV</em>, <em>63</em>(3), 625–637. (<a
href="https://doi.org/10.1137/20M1342938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For students in applied mathematics courses, the phenomenon of delay-induced stability and instability offers exciting educational opportunities. Exploration of the onset of instability in delay differential equations (DDEs) invites a blend of analysis (real, complex, and functional), algebra, and computational methods. Moreover, stabilization of unstable but “desirable” equilibria using delayed feedback is of high importance in science and engineering. The primary challenge in classifying the stability of equilibria of DDEs lies in the fact that characteristic equations are transcendental. Here, we survey two approaches for understanding the stability of equilibria of DDEs. The first approach uses a functional analytic framework, a departure from the more familiar textbook methods based upon characteristic equations and completeness-type arguments. The second approach uses Pontryagin&#39;s generalization of the Routh--Hurwitz conditions. We apply the latter approach to illustrate how the deliberate introduction of a second time delay in a single-delay differential equation can stabilize an otherwise unstable equilibrium.},
  archive      = {J_SIREV},
  author       = {Shreya Menon and John W. Cain},
  doi          = {10.1137/20M1342938},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {625-637},
  shortjournal = {SIAM Rev.},
  title        = {Approaches toward understanding delay-induced stability and instability},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How many steps still left to <span
class="math inline"><em>x</em></span>*? <em>SIREV</em>, <em>63</em>(3),
585–624. (<a href="https://doi.org/10.1137/19M1244858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high speed of $x_{k}\rightarrow x^\ast\in{\mathbb R}$ is usually measured using the $C$-, $Q$-, or $R$-orders: \[ \lim \frac {|x^\ast - x_{k+1}|}{|x^\ast - x_k|^{p_0}}\in(0,+\infty), \qquad \lim \frac {\ln |x^\ast - x_{k+1}|}{\ln |x^\ast - x_k|} =q_0, \qquad \mbox{or} \quad \lim \big\vert\ln |x^\ast - x_k| \big \vert ^\frac1k =r_0. \] By connecting them to the natural, term-by-term comparison of the errors of two sequences, we find that the $C$-orders---including (sub)linear---are in agreement. Weird relations may appear though for the $Q$-orders: we expect $|x^\ast - x_k|=\mathcal{O}(|x^\ast - y_k|^\alpha)$ $\forall\alpha&gt;1$ to imply “$\geq$&quot; for the $Q$-orders of ${x_{k}}$ vs. ${y_{k}}$; the contrary is shown by an example providing no vs. infinite $Q$-orders. The $R$-orders appear to be even worse: an ${x_{k}}$ with infinite $R$-order may have unbounded nonmonotone errors: \(\limsup \frac{|x^\ast - x_{k+1}|}{|x^\ast - x_k|}= +\infty\). Such aspects motivate the study of equivalent definitions, computational variants, and so on. These orders are also the perspective from which we analyze the three basic iterative methods for nonlinear equations in $\mathbb R$. The Newton method, widely known for its quadratic convergence, may in fact attain any $C$-order from $[1,+\infty]$ (including sublinear); we briefly recall such convergence results, along with connected aspects (such as historical notes, known asymptotic constants, floating point arithmetic issues, and radius of attraction balls), and provide examples. This approach leads to similar results for the successive approximations method, while the secant method exhibits different behavior: it may not have high $C$-orders, but only $Q$-orders.},
  archive      = {J_SIREV},
  author       = {Emil Cătinaş},
  doi          = {10.1137/19M1244858},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {585-624},
  shortjournal = {SIAM Rev.},
  title        = {How many steps still left to $x$*?},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Education. <em>SIREV</em>, <em>63</em>(3), 583. (<a
href="https://doi.org/10.1137/21N975291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Education section in this issue contains two papers. The first paper, “How Many Steps Still Left to ${x}^*$?,” is written by Emil Cătinaş. This is an important question for everyone who works with numerical methods. One could compare two convergent sequences and analyze which one would get closer to its limit in a given number of steps. However, we also need a way to measure how fast a given sequence converges. This has led to concepts such as $C$-, $Q$-, or $R$-order of convergence, which are the subject of this paper. The author explains these orders for sequences defined on the real line and presents relations among them theoretically and numerically. While theoretical analysis of the speed of convergence assumes that a limit exists and includes asymptotic properties, if we evaluate the speed of convergence in practice, we can only use quantities based solely on information available up to the current step. The paper contains two computational versions of the orders and some results regarding the equivalence of the error-based and computational orders of convergence. The attainable error-based improvements, i.e., the exact asymptotic speed of convergence, are discussed in the context of the Newton and secant methods. The speed of convergence is an important concept for every numerical method, which is of particular interest in the era of big data. This article contains many illustrative examples and exercises and may be helpful to students working on iterative methods for nonlinear equation. The second paper is “Approaches toward Understanding Delay-Induced Stability and Instability.” It is presented by Shreya Menon and John W. Cain. The authors discuss delay differential equations focusing on the phenomenon of delay-induced stability or instability. The topic is interesting from various points of view. At the very beginning of the paper, the authors name several problems in which the issue of stabilizing unstable but desirable equilibrium states of a dynamical system is very important. Those problems include the prevention of abnormal cardiac rhythms and the suppression of vibrations. From an educational point of view, the topic integrates methods of real, complex, and functional analysis, linear algebra, differential equations, and computational methods, thus providing many opportunities for student explorations in seminars and in research projects. The paper contains two approaches for analyzing the stability of equilibria for delay differential equations. It presents a framework based on a theorem of Pontryagin, which is not typically contained in traditional textbooks. There is a worked example illustrating the theoretical statements as well as a discussion of the limitations of the approach. The paper provides many exercises and a discussion aiming to help potential instructors who would like to incorporate the presented ideas into their lectures.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/21N975291},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {583},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From circuit complexity to faster all-pairs shortest paths.
<em>SIREV</em>, <em>63</em>(3), 559–582. (<a
href="https://doi.org/10.1137/21M1418654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a randomized method for computing the min-plus product (a.k.a. tropical product) of two $n \times n$ matrices, yielding a faster algorithm for solving the all-pairs shortest path problem (APSP) in dense $n$-node directed graphs with arbitrary edge weights. In the real random-access machine model, where additions and comparisons of reals are unit cost (but all other operations have logarithmic cost), the algorithm runs in time $\frac{n^3}{2^{\Omega(\log n)^{1/2}}}$ and is correct with high probability. On the word random-access machine which permits constant-time operations on $\log(n)$-bit words, the algorithm runs in $n^3/2^{\Omega(\log n)^{1/2}} + n^{2+o(1)}\log(nM)$ time on graphs with edge weights in $([0,M] \cap \mathbb{Z})\cup{\infty}$. Prior algorithms needed either $\Theta(n^3/\log^c n)$ time for various $c \leq 2$, or $\Theta(M^{\alpha}n^{\beta})$ time for various $\alpha &gt; 0$ and $\beta &gt; 2$. Our algorithm applies a tool from circuit complexity, namely, the Razborov--Smolensky polynomials for approximately representing ${AC}^0[p]$ circuits, to efficiently reduce a matrix product over the min-plus algebra to a relatively small number of rectangular matrix products over $\mathbb{F}_2$. Each rectangular matrix product can be computed using a particularly efficient method due to Coppersmith. We also give a deterministic version of the algorithm running in $n^3/2^{\log^{\delta} n}$ time for some $\delta &gt; 0$, which utilizes the Yao--Beigel--Tarui translation of ${AC}^0[m]$ circuits into “nice” depth-two circuits.},
  archive      = {J_SIREV},
  author       = {R. Ryan Williams},
  doi          = {10.1137/21M1418654},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {559-582},
  shortjournal = {SIAM Rev.},
  title        = {From circuit complexity to faster all-pairs shortest paths},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). SIGEST. <em>SIREV</em>, <em>63</em>(3), 557. (<a
href="https://doi.org/10.1137/21N97528X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “From Circuit Complexity to Faster All-Pairs Shortest Paths,” by R. Ryan Williams. The original paper appeared in SIAM Journal on Computing (SICOMP) in 2018 and, as indicated by the high citation count, ideas from this work have inspired a broad range of future activity. For a given $n$-node graph, the problem tackled here is to compute all shortest paths between any pair of nodes, the so-called all-pairs shortest path (APSP) problem. As explained by the author (see Corollary 1.2) a solution to the APSP problem leads to solutions for many other problems in discrete mathematics. The key result in this work is a new algorithm that improves on the classical $O(n^3)$ complexity for the first time by a factor that is superpolylogarithmic. This is done by introducing a new technique, called the polynomial method. Key ingredients of the work are the use of tropical, or max-plus, algebra, where $\min$ plays the role of addition and $+$ is promoted to multiplication, and a new reduction to rectangular matrix multiplication over the field of two elements. In addition to creating an algorithm that runs faster than $n^3/\log^k n$ for every constant $k$, the author discusses the potential for achieving a truly subcubic speed of $n^{3-\epsilon}$ for some $\epsilon &gt;0$. The original (SICOMP) article has been given an extended introduction to increase accessibility to the SIAM readership. Thanks to the wide applicability of the APSP problem, and the potential for further developments in the field, this paper should be of interest both to researchers working on algorithms and to those working on complexity.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/21N97528X},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {557},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization of the mean first passage time in near-disk and
elliptical domains in 2-d with small absorbing traps. <em>SIREV</em>,
<em>63</em>(3), 525–555. (<a
href="https://doi.org/10.1137/20M1332396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of the mean first passage time (MFPT) for a Brownian particle in a bounded 2-D domain containing small absorbing traps is a fundamental problem with biophysical applications. The average MFPT is the expected capture time assuming a uniform distribution of starting points for the random walk. We develop a hybrid asymptotic-numerical approach to predicting optimal configurations of $m$ small stationary circular absorbing traps that minimize the average MFPT in near-disk and elliptical domains. For a general class of near-disk domains, we illustrate through several specific examples how simple, yet highly accurate, numerical methods can be used to implement the asymptotic theory. From the derivation of a new explicit formula for the Neumann Green&#39;s function and its regular part for the ellipse, a numerical approach based on our asymptotic theory is used to investigate how the spatial distribution of the optimal trap locations changes as the aspect ratio of an ellipse of fixed area is varied. The results from the hybrid theory for the ellipse are compared with full PDE numerical results computed from the closest point method [S. Iyaniwura et al., Multiscale Model. Simul., to appear]. For long and thin ellipses, it is shown that the optimal trap pattern for $m=2,\ldots,5$ identical traps is collinear along the semimajor axis of the ellipse. For such essentially 1-D patterns, a thin-domain asymptotic analysis is formulated and implemented to accurately predict the optimal locations of collinear trap patterns and the corresponding optimal average MFPT.},
  archive      = {J_SIREV},
  author       = {Sarafa A. Iyaniwura and Tony Wong and Colin B. Macdonald and Michael J. Ward},
  doi          = {10.1137/20M1332396},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {525-555},
  shortjournal = {SIAM Rev.},
  title        = {Optimization of the mean first passage time in near-disk and elliptical domains in 2-D with small absorbing traps},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing spectral measures of self-adjoint operators.
<em>SIREV</em>, <em>63</em>(3), 489–524. (<a
href="https://doi.org/10.1137/20M1330944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using the resolvent operator, we develop an algorithm for computing smoothed approximations of spectral measures associated with self-adjoint operators. The algorithm can achieve arbitrarily high orders of convergence in terms of a smoothing parameter for computing spectral measures of general differential, integral, and lattice operators. Explicit pointwise and $L^p$-error bounds are derived in terms of the local regularity of the measure. We provide numerical examples, including a partial differential operator and a magnetic tight-binding model of graphene, and compute 1000 eigenvalues of a Dirac operator to near machine precision without spectral pollution. The algorithm is publicly available in SpecSolve, which is a software package written in MATLAB.},
  archive      = {J_SIREV},
  author       = {Matthew Colbrook and Andrew Horning and Alex Townsend},
  doi          = {10.1137/20M1330944},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {489-524},
  shortjournal = {SIAM Rev.},
  title        = {Computing spectral measures of self-adjoint operators},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Research spotlights. <em>SIREV</em>, <em>63</em>(3), 487.
(<a href="https://doi.org/10.1137/21N975278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first of our two Research Spotlights articles in this issue gives a general framework and corresponding algorithm for computing accurate approximations to the spectral measures of self-adjoint operators, the key to which is the resolvent of the operator. As authors Matthew Colbrook, Andrew Horning, and Alex Townsend detail, the spectral measure is necessary in their applications of interest to give a more complete description of the operator and associated dynamics. Their article, “Computing Spectral Measures of Self-Adjoint Operators,&quot; begins with the formal mathematical description of the spectral measure and associated assumptions. Several applications that rely on the computation of spectral measure are highlighted throughout the article: for example, applications in particle and condensed matter physics are discussed in the context of a survey of previous work in estimation of spectral measure. The crux of the proposed approach, first formulated in section 4 and generalized to higher order kernels for improved accuracy in section 5, is to evaluate a smoothed approximation (i.e., defined through convolution with appropriate kernel) to the spectral measure by evaluating the resolvent, with the latter calculation akin to solving shifted systems of linear equations. The authors detail carefully and demonstrate graphically the challenges associated with designing a tractable and robust scheme. The discussion of algorithmic issues in section 6, including the ability of their approach to dynamically adjust to reach desired accuracy, also features use cases of their associated publicly available MATLAB code. Within the body of the article, the versatility of their proposed framework is well illustrated on differential, integral, and lattice operators. Readers may be interested in the suggestions in section 8 on possible further use cases for the new framework, such as in “understanding the behavior of large real-world networks and new random graph models.&quot; Our second article, “Optimization of the Mean First Passage Time in Near-Disk and Elliptical Domains in 2-D with Small Absorbing Traps,&quot; is coauthored by Sarafa A. Iyaniwura, Tony Wong, Colin B. Macdonald, and Michael J. Ward. Narrow escape or capture problems are those portrayed in the introduction as first passage time problems that describe the expected time for a Brownian particle to reach some absorbing set with small measure. Two of the applications in which such problems arise include the time it takes for a diffusing surface-bound molecule (the “particle” in this case) to reach a localized signaling region on the cell membrane and the time it takes for a predator to locate its prey. The authors define the “average MFPT&quot; for a diffusion process to be the expected time for capture given a uniform distribution of starting points for the random walk. The optimal trap configurations for the average MFPT in geometries other than the disk had been unsolved and provided the impetus for the authors to investigate the question in the context of near-disk and elliptical domains. Through a combination of asymptotic analysis and numerical techniques (e.g., use of numerical quadrature and numerical time stepping for solving equations (3.4) and (4.3)), the authors design “hybrid asymptotic numerical&quot; approaches to predicting optimal configurations of small stationary circular absorbing traps that minimize the average MFPT in these new domains. Though much of the paper is devoted to detailed derivations that will take some time for the reader to absorb, one can get some immediate appreciation for the results from the graphical illustrations in which the new results are compared against numerical PDE generated solutions. Extensions of the approach and remaining open problems are included in the last section for consideration by the reader.},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/21N975278},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {487},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The why, how, and when of representations for complex
systems. <em>SIREV</em>, <em>63</em>(3), 435–485. (<a
href="https://doi.org/10.1137/20M1355896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex systems, composed at the most basic level of units and their interactions, describe phenomena in a wide variety of domains, from neuroscience to computer science and economics. The wide variety of applications has resulted in two key challenges: the generation of many domain-specific strategies for complex systems analyses that are seldom revisited, and the compartmentalization of representation and analysis ideas within a domain due to inconsistency in complex systems language. In this work we propose basic, domain-agnostic language in order to advance toward a more cohesive vocabulary. We use this language to evaluate each step of the complex systems analysis pipeline, beginning with the system under study and data collected, then moving through different mathematical frameworks for encoding the observed data (i.e., graphs, simplicial complexes, and hypergraphs), and relevant computational methods for each framework. At each step we consider different types of dependencies; these are properties of the system that describe how the existence of an interaction among a set of units in a system may affect the possibility of the existence of another relation. We discuss how dependencies may arise and how they may alter the interpretation of results or the entirety of the analysis pipeline. We close with two real-world examples using coauthorship data and email communications data that illustrate how the system under study, the dependencies therein, the research question, and the choice of mathematical representation influence the results. We hope this work can serve as an opportunity for reflection for experienced complex systems scientists, as well as an introductory resource for new researchers.},
  archive      = {J_SIREV},
  author       = {Leo Torres and Ann S. Blevins and Danielle Bassett and Tina Eliassi-Rad},
  doi          = {10.1137/20M1355896},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {435-485},
  shortjournal = {SIAM Rev.},
  title        = {The why, how, and when of representations for complex systems},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Survey and review. <em>SIREV</em>, <em>63</em>(3), 433. (<a
href="https://doi.org/10.1137/21N975266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our Survey and Review paper, “The Why, How, and When of Representations for Complex Systems,” by Leo Torres, Ann S. Blevins, Danielle Bassett, and Tina Eliassi-Rad, lists 233 references. Some of them were published in applied mathematics journals, like SIAM Review or SIAM Journal on Applied Algebra and Geometry. Others appeared in well-known “pure” mathematics journals, including Annals of Mathematics and Bulletin of the American Mathematical Society. There are many references to multidisciplinary outlets like Science, Nature, or Proceedings of the National Academy of Sciences of the USA. But the whole list is astonishing: from the Journal of the Acoustical Society of America to Sociometry to Human Brain Mapping. This diversity of journals is obviously due to the interest in complex systems that now exists in a wide variety of applications. A complex system is a collection of a large number of objects or agents that interact with one another in a way that gives rise to the emergence of collective behaviors not immediately predictable from the aggregation of the individual behaviors of the parts. The generality of this definition---the objects may be anything from molecules in a cell to businesses in the economy of a country---makes it possible for complex system theory to be useful in a large variety of fields, as described above. On the flip side, a theory that is spread over such a diversity of application areas may easily be compartmentalized. The aim of the review in this issue is to propose a common language to increase the cohesion of the theory. Three alternative approaches to the representation of complex systems are discussed in detail: graphs, simplicial complexes, and hypergraphs. The main ideas are illustrated by means of more than a dozen figures, and the paper closes with two real-world examples using coauthorship data and email communication data. In addition to being useful for complex system scientists wishing to broaden their view of the theory, the survey provides an accessible introduction to the field.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/21N975266},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {433},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Book reviews. <em>SIREV</em>, <em>63</em>(2), 419–431. (<a
href="https://doi.org/10.1137/21N975254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The April issue of SIAM News sadly reported on the death of Bob O&#39;Malley (https://sinews.siam.org/Details-Page/obituary-robert-e-omalley-jr). He led the Book Reviews section actively and dynamically for many years. This section and its editors together with the SIAM community owe him a great debt of gratitude. This time, our section has 3 books on timely topics and 3 further books on fundamental dynamical system aspects. It opens with a featured review of the book Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control, by Steven L. Brunton and J. Nathan Kutz. This review, which is very diligently written by Jeff Moehlis, praises this book as an “engaging and highly informative introduction.” We continue with a trendy topic, the review by Krešimir Josić on the book Reinforcement Learning: An Introduction, written by Richard S. Sutton and Andrew G. Barto. Krešimir recommends the book as “a good starting point for anyone who wants to learn more about” reinforcement learning. This is followed by Alfio Borzì&#39;s review of the book Climate Change and Terrestrial Ecosystem Modeling, by Gordon Bonan. Alfio emphasizes the relevance of the book and praises it as an ideal textbook. After that, three books in the large field of dynamical systems are discussed. First, Rachel Dauncey and Anne Skeldon have a look into the book Chaos: An Introduction for Applied Mathematicians, by Andrew Fowler and Mark McGuinness. Rachel and Anne comment on aspects of diversity in formulations in this book and offer mixed and nuanced assessments. Then Georg Stadler presents his views on the book Modelling with Ordinary Differential Equations, by Alfio Borzì, emphasizing its value for teaching. Finally, we review the book Practical Methods for Optimal Control Using Nonlinear Programming, by John T. Betts, which discusses numerical solution methods for optimal control problems in ODEs that “actually work.”},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/21N975254},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {419-431},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vandermonde with arnoldi. <em>SIREV</em>, <em>63</em>(2),
405–415. (<a href="https://doi.org/10.1137/19M130100X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vandermonde matrices are exponentially ill-conditioned, rendering the familiar “polyval(polyfit)” algorithm for polynomial interpolation and least-squares fitting ineffective at higher degrees. We show that Arnoldi orthogonalization fixes the problem. This amounts to on-the-fly construction of discrete orthogonal polynomials by Stieltjes orthogonalization.},
  archive      = {J_SIREV},
  author       = {Pablo D. Brubeck and Yuji Nakatsukasa and Lloyd N. Trefethen},
  doi          = {10.1137/19M130100X},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {405-415},
  shortjournal = {SIAM Rev.},
  title        = {Vandermonde with arnoldi},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rotations in three dimensions. <em>SIREV</em>,
<em>63</em>(2), 395–404. (<a
href="https://doi.org/10.1137/19M128867X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Rodrigues formula for the $3\times 3$ rotation matrix is hardly ever derived from first principles in a simple and intuitive way that is accessible to undergraduate students. Two different derivations of the rotation matrix are presented here. This communication is written in a simple and expository style, so that it could serve as supplementary material to an introductory linear algebra course.},
  archive      = {J_SIREV},
  author       = {Milton F. Maritz},
  doi          = {10.1137/19M128867X},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {395-404},
  shortjournal = {SIAM Rev.},
  title        = {Rotations in three dimensions},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Education. <em>SIREV</em>, <em>63</em>(2), 393. (<a
href="https://doi.org/10.1137/21N975242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SIAM Review presents two papers in the Education section. The first paper, “Rotations in Three Dimensions,” is written by Milton F. Maritz. Current textbooks in calculus and linear algebra typically discuss rotation on the plane and do not present rotation matrices in higher dimensions. The author provides two ways to derive the $3 \times 3$ matrix for a 3D rotation which are based on geometrical arguments. The first approach requires only basic calculus, while the second one makes use of matrix differential equations. Additional questions, such as identifying the rotation angle when the rotation matrix is given, are discussed as well. Several exercises and recommendations to the instructor are provided. This paper is written in an intuitive and accessible style, so that it could be useful in an undergraduate linear algebra course or for an undergraduate seminar. The second paper is “Vandermonde with Arnoldi.” It is presented by Pablo D. Brubeck, Yuji Nakatsukasa, and Lloyd N. Trefethen. When a function is approximated by a polynomial of high degree, one could form the Vandermonde matrix for a set of arguments and formulate a linear model which is then fit to the respective values of the function. The Vandermonde matrix has a large condition number because its columns are powers of the arguments. Therefore, standard numerical techniques for solving the resulting system of normal equations are frequently unstable and inefficient. Numerical stability can be achieved by using the Arnoldi orthogonalization procedure. The authors discuss this idea beyond polynomial approximation. One example constitutes the approximation of a function by Fourier series, and another one deals with the solution of the 2D Laplace equation which is approximated by the real part of a complex polynomial. In their concluding remarks, the authors survey relevant literature and point to further applications of that stabilization technique.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/21N975242},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {393},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general framework for fractional order compartment models.
<em>SIREV</em>, <em>63</em>(2), 375–392. (<a
href="https://doi.org/10.1137/21M1398549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compartment models are a widely used class of models that are useful when considering the flow of objects, people, or energy between different labeled states, referred to as compartments. Classic examples include SIR models in epidemiology and many pharmacokinetic models used in pharmacology. These models are formulated as sets of coupled ordinary differential equations, but in recent years there has been increasing interest in generalizations involving fractional differential equations. The majority of such generalizations have been performed in an ad hoc manner by replacing integer order derivatives with fractional derivatives. Such an approach does allow for the incorporation of history effects into the models, but may be problematic in a number of ways, such as breaking conservation of matter. To overcome these problems we have developed a systematic approach for the inclusion of fractional derivatives into compartment models by deriving the deterministic governing equations from an underlying physical stochastic process. This derivation also reveals the connection between these fractional order models and age-structured models. Unlike the ad hoc addition of fractional derivatives, our approach ensures that the model remains physically reasonable at all times and provides for an easy interpretation of all the parameters in the model. Illustrative examples, drawn from epidemiology, pharmacokinetics, and in-host virus dynamics, are provided.},
  archive      = {J_SIREV},
  author       = {Christopher N. Angstmann and Austen M. Erickson and Bruce I. Henry and Anna V. McGann and John M. Murray and James A. Nichols},
  doi          = {10.1137/21M1398549},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {375-392},
  shortjournal = {SIAM Rev.},
  title        = {A general framework for fractional order compartment models},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). SIGEST. <em>SIREV</em>, <em>63</em>(2), 373. (<a
href="https://doi.org/10.1137/21N975230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “A General Framework for Fractional Order Compartment Models,” by Christopher N. Angstmann, Austen M. Erickson, Bruce I. Henry, Anna V. McGann, John M. Murray, and James A. Nichols. The seminal compartmental publication of Kermack and McKendrick ([24] in the reference list) is approaching its 100th anniversary. Compartmental thinking has proved to be powerful and versatile, and a vast range of compartmental ODE models have been developed and applied in many application fields, perhaps most notably in epidemiology. In recent years, there has been growing interest in the idea of introducing fractional derivatives into classical deterministic ODEs and PDEs as a means to capture complex behavior, for example, around creep, diffusion, and hysteresis. It is no surprise, therefore, that many authors have extended compartmental ODE models to a fractional setting. What sets this SIGEST article apart is that the authors do not change from integer to fractional derivatives in an ad hoc manner; instead they start from first principles. To do this, they follow a thought process that has been used to great effect in many applied mathematics contexts: consider a discrete, stochastic, microscale setting and obtain a deterministic macroscale model by taking a mean field, or ensemble limit, approximation. Fractional calculus then emerges naturally if we assume that particles can be “trapped” so that the probability of escaping a compartment depends on the amount of time spent there. For example, in a model for the spread of disease, an individual with chronic infection may have a chance of recovery (moving from the infected compartment to the recovered compartment) that decreases as the infection persists. Section 2 works through the details for a single compartment model, after which it is straightforward to extend to the multiple compartment case. The general framework is then illustrated in four examples: Susceptible-Infected-Recovered (SIR) and Susceptible-Infected-Susceptible (SIS) models of disease spread, HIV infection in vivo, and chromium clearance in mice. Overall, this systematic framework will be of interest to a wide range of applied mathematicians who model complex systems in life sciences and engineering, and also to applied analysts, numerical analysts, and statisticians who wish to study, simulate, or make predictions with realistic models involving fractional derivatives.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/21N975230},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {373},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A strong law of large numbers for scrambled net integration.
<em>SIREV</em>, <em>63</em>(2), 360–372. (<a
href="https://doi.org/10.1137/20M1320535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a strong law of large numbers for integration on digital nets randomized by a nested uniform scramble. The motivating problem is optimization over some variables of an integral over others, arising in Bayesian optimization. This strong law requires that the integrand have a finite moment of order $p$ for some $p&gt;1$. Previously known results implied a strong law only for Riemann integrable functions. Previous general weak laws of large numbers for scrambled nets require a square integrable integrand. We generalize from $L^2$ to $L^p$ for $p&gt;1$ via the Riesz--Thorin interpolation theorem.},
  archive      = {J_SIREV},
  author       = {Art B. Owen and Daniel Rudolf},
  doi          = {10.1137/20M1320535},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {360-372},
  shortjournal = {SIAM Rev.},
  title        = {A strong law of large numbers for scrambled net integration},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum bound principles for a class of semilinear parabolic
equations and exponential time-differencing schemes. <em>SIREV</em>,
<em>63</em>(2), 317–359. (<a
href="https://doi.org/10.1137/19M1243750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquity of semilinear parabolic equations is clear from their numerous applications ranging from physics and biology to materials and social sciences. In this paper, we consider a practically desirable property for a class of semilinear parabolic equations of the abstract form $u_t={\mathcal{L}} u+f[u]$, with ${\mathcal{L}}$ a linear dissipative operator and $f$ a nonlinear operator in space, namely, a time-invariant maximum bound principle, in the sense that the time-dependent solution $u$ preserves for all time a uniform pointwise bound in absolute value imposed by its initial and boundary conditions. We first study an analytical framework for sufficient conditions on ${\mathcal{L}}$ and $f$ that lead to such a maximum bound principle for the time-continuous dynamic system of infinite or finite dimensions. Then we utilize a suitable exponential time-differencing approach with a properly chosen generator of the contraction semigroup to develop first- and second-order accurate temporal discretization schemes that satisfy the maximum bound principle unconditionally in the time-discrete setting. Error estimates of the proposed schemes are derived along with their energy stability. Extensions to vector- and matrix-valued systems are also discussed. We demonstrate that the abstract framework and analysis techniques developed here offer an effective and unified approach to studying the maximum bound principle of the abstract evolution equation that covers a wide variety of well-known models and their numerical discretization schemes. Some numerical experiments are also carried out to verify the theoretical results.},
  archive      = {J_SIREV},
  author       = {Qiang Du and Lili Ju and Xiao Li and Zhonghua Qiao},
  doi          = {10.1137/19M1243750},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {317-359},
  shortjournal = {SIAM Rev.},
  title        = {Maximum bound principles for a class of semilinear parabolic equations and exponential time-differencing schemes},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Research spotlights. <em>SIREV</em>, <em>63</em>(2), 315.
(<a href="https://doi.org/10.1137/21N975229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under what conditions does a semilinear parabolic equation have the maximum bound principle, and do numerical approximations of it preserve the principle? These are the main questions that are addressed by authors Qiang Du, Lili Ju, Xiao Li, and Zhonghua Qiao in their paper “Maximum Bound Principles for a Class of Semilinear Parabolic Equations and Exponential Time-Differencing Schemes.&quot; The maximum bound principle (MBP) as described by the authors states that if the initial data and/or the boundary values are bounded pointwise in absolute value, then the absolute value of the solution is also bounded everywhere and for all time. One of the motivating examples employed is the Allen--Cahn equation where the MBP is known to hold. To generalize such results to a much broader class of problems, the authors employ an abstract form of the model evolution equations into which this motivating example and many others will fit. Using their formulation, they provide an analysis of what is required of the linear and nonlinear operators for the model equations to satisfy the MBP. The variety of problems that can be assessed according to this new framework is evident throughout. Moreover, the authors consider first- and second-order exponential time decay (ETD) schemes that will preserve the discrete MBP unconditionally. Colorful illustrations accompany the numerical demonstrations of the MBP-preserving properties of these particular ETD methods. Numerical integration is necessary in a great many applications. However, in some situations, estimates can be obtained only via approaches from the Monte Carlo family. The article “A Strong Law of Large Numbers for Scrambled Net Integration,&quot; by Art Owen and Daniel Rudolf, focuses on answering an open question regarding existence of a strong law of large numbers (SLLN) when employing a specific sampling procedure together with the randomized quasi-Monte Carlo (RQMC) for the integration. The sparsity plots in the first figure of the paper give the reader insight into the differences among the sampling in quasi-Monte Carlo (QMC) and RQMC---in QMC, one uses sample points determined (e.g., by digital nets) to cover the space more evenly, while in RQMC the QMC points are scrambled, a step that involves a random permutation in the generation of the sample points. The motivation that the authors give in the introduction for studying the existence of the SLLN in this context is from Bayesian optimization. Specifically, they note that consistent estimation of the optimal parameter can be proved assuming an SLLN for some sample values. Until now, such a tool was missing for RQMC. The authors deliver what is promised in their title, not just once, but twice: the first result is proved assuming a square integrable integrand and geometrically spaced sample sizes, while the second is shown under a relaxation of those assumptions.},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/21N975229},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {315},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic control liaisons: Richard sinkhorn meets gaspard
monge on a schrödinger bridge. <em>SIREV</em>, <em>63</em>(2), 249–313.
(<a href="https://doi.org/10.1137/20M1339982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1931--1932, Erwin Schrödinger studied a hot gas Gedankenexperiment (an instance of large deviations of the empirical distribution). Schrödinger&#39;s problem represents an early example of a fundamental inference method, the so-called maximum entropy method, having roots in Boltzmann&#39;s work and being developed in subsequent years by Jaynes, Burg, Dempster, and Csiszár. The problem, known as the Schrödinger bridge problem (SBP) with “uniform&quot; prior, was more recently recognized as a regularization of the Monge--Kantorovich optimal mass transport (OMT) problem, leading to effective computational schemes for the latter. Specifically, OMT with quadratic cost may be viewed as a zero-temperature limit of the problem posed by Schrödinger in the early 1930s. The latter amounts to minimization of Helmholtz&#39;s free energy over probability distributions that are constrained to possess two given marginals. The problem features a delicate compromise, mediated by a temperature parameter, between minimizing the internal energy and maximizing the entropy. These concepts are central to a rapidly expanding area of modern science dealing with the so-called Sinkhorn algorithm, which appears as a special case of an algorithm first studied in the more challenging continuous space setting by the French analyst Robert Fortet in 1938--1940 specifically for Schrödinger bridges. Due to the constraint on end-point distributions, dynamic programming is not a suitable tool to attack these problems. Instead, Fortet&#39;s iterative algorithm and its discrete counterpart, the Sinkhorn iteration, permit computation of the optimal solution by iteratively solving the so-called Schrödinger system. Convergence of the iteration is guaranteed by contraction along the steps in suitable metrics, such as Hilbert&#39;s projective metric. In both the continuous as well as the discrete time and space settings, stochastic control provides a reformulation of and a context for the dynamic versions of general Schrödinger bridge problems and of their zero-temperature limit, the OMT problem. These problems, in turn, naturally lead to steering problems for flows of one-time marginals which represent a new paradigm for controlling uncertainty. The zero-temperature problem in the continuous-time and space setting turns out to be the celebrated Benamou--Brenier characterization of the McCann displacement interpolation flow in OMT. The formalism and techniques behind these control problems on flows of probability distributions have attracted significant attention in recent years as they lead to a variety of new applications in spacecraft guidance, control of robot or biological swarms, sensing, active cooling, and network routing as well as in computer and data science. This multifaceted and versatile framework, intertwining SBP and OMT, provides the substrate for the historical and technical overview of the field given in this paper. A key motivation has been to highlight links between the classical early work in both topics and the more recent stochastic control viewpoint, which naturally lends itself to efficient computational schemes and interesting generalizations.},
  archive      = {J_SIREV},
  author       = {Yongxin Chen and Tryphon T. Georgiou and Michele Pavon},
  doi          = {10.1137/20M1339982},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {249-313},
  shortjournal = {SIAM Rev.},
  title        = {Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schrödinger bridge},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Survey and review. <em>SIREV</em>, <em>63</em>(2), 247. (<a
href="https://doi.org/10.1137/21N975217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {``Stochastic Control Liaisons: Richard Sinkhorn Meets Gaspard Monge on a Schrödinger Bridge,” by Yongxin Chen, Tryphon T. Georgiou, and Michele Pavon, is the Survey and Review article in this issue. There is little doubt that the authors chose to have a very intriguing title to entice SIAM Review readers into browsing through their work; I will not spoil the effect by trying to explain the title to you. Anyway, “meets” and “liaisons” are perhaps the most important words. Literally dozens of mathematical, scientific, and even philosophical ideas meet each other in this article. There is much material on optimal mass transport, a theory with a huge recent literature and applications in vehicle path planning, machine learning, manufacturing, etc. There are ideas from statistical mechanics (entropy, free energy, \dots), probability (large deviations, couplings, \dots), control (in particular, stochastic control), computational statistics, linear algebra, and the list may go on and on. In addition, readers will find along the way plenty of history of science and a few anecdotes. If you had not heard about Richard Sinkhorn or Gaspard Monge, or if you knew the names but were unaware of their rendezvous, please read.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/21N975217},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {247},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Book reviews. <em>SIREV</em>, <em>63</em>(1), 231–245. (<a
href="https://doi.org/10.1137/21N975205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The section opens with a review well written by a giant in the field of numerical solution of partial differential equations, Roland Glowinski. In this featured review, he analyzes the book Think Before You Compute: A Prelude to Computational Fluid Dynamics, by E. J. Hinch, in much detail and contributes his own thoughts on appraisal or possibilities for improvement to every chapter of the book. The reviewer&#39;s verdict on the book is: “The book is a gem.” The following three reviewed books, written in an earlier and different world, take on a whole new relevance against the background of the current pandemic situation. First, we have the book by Irina Kareva and Georgy Karev, Modeling Evolution of Heterogeneous Populations: Theory and Applications. This book is diligently reviewed by Karen Page, who rates the book as “enlightening&quot; even if it still contains small annoying errors. The following book, Delay-Adaptive Linear Control, by Yang Zhu and Miroslav Krstic, also seems to anticipate some aspects of the current modeling and control of the COVID-19 pandemic. This book is reviewed in detail by Bozenna Pasik-Duncan and is classified as an important contribution to control in delay systems. For the classification and mining of numerous medical data, such as those being generated right now, Mehrshad Sadria and Anita Layton suggest reading Bioinformatics, edited by Andreas D. Baxevanis, Gary D. Bader, and David S. Wishart, and recommend it as an “easy to read, greatly informative, and highly timely” book. Finally, I would like to mention the book by Zhendong Luo and Goong Chen on Proper Orthogonal Decomposition Methods for Partial Differential Equations, which is reviewed by the expert Stefan Volkwein, who praises the book as providing “a really good recipe for the application of the POD method to many nonstationary PDEs,” and also the book Basic Calculus of Planetary Orbits and Interplanetary Flight: The Missions of the Voyagers, Cassini, and Juno, by Alexander J. Hahn, that allows the reader to escape the current narrow earthly confinement in mathematical thoughts and delve into interplanetary explorations.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/21N975205},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {231-245},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepXDE: A deep learning library for solving differential
equations. <em>SIREV</em>, <em>63</em>(1), 208–228. (<a
href="https://doi.org/10.1137/19M1274067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from an implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an educational tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging scientific machine learning field.},
  archive      = {J_SIREV},
  author       = {Lu Lu and Xuhui Meng and Zhiping Mao and George Em Karniadakis},
  doi          = {10.1137/19M1274067},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {208-228},
  shortjournal = {SIAM Rev.},
  title        = {DeepXDE: A deep learning library for solving differential equations},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fluid-structure interaction for the classroom:
Interpolation, hearts, and swimming! <em>SIREV</em>, <em>63</em>(1),
181–207. (<a href="https://doi.org/10.1137/18M1209283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While students may find spline interpolation quite digestible based on their familiarity with the continuity of a function and its derivatives, some of its inherent value may be missed when they only see it applied to standard data interpolation exercises. In this paper, we offer alternatives in which students can qualitatively and quantitatively witness the resulting dynamical differences when objects are driven through a fluid using different spline interpolation methods. They say that seeing is believing; here we showcase the differences between linear and cubic spline interpolation using examples from fluid pumping and aquatic locomotion. Moreover, students can define their own interpolation functions and visualize the dynamics that unfold. To solve the fluid-structure interaction system, the open-source fluid dynamics software IB2d is used. In that spirit, all simulation codes, analysis scripts, and movies are provided for streamlined use.},
  archive      = {J_SIREV},
  author       = {Nicholas A. Battista},
  doi          = {10.1137/18M1209283},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {181-207},
  shortjournal = {SIAM Rev.},
  title        = {Fluid-structure interaction for the classroom: Interpolation, hearts, and swimming!},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the accuracy of the trapezoidal rule.
<em>SIREV</em>, <em>63</em>(1), 167–180. (<a
href="https://doi.org/10.1137/18M1229353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trapezoidal rule uses function values at equispaced nodes. It is very accurate for integrals over periodic intervals, but is usually quite inaccurate in nonperiodic cases. Commonly used improvements, such as Simpson\textquoteright s rule and the Newton--Cotes formulas, are not much (if at all) better than the even more classical quadrature formulas described by James Gregory in 1670. For increasing orders of accuracy, these methods all suffer from the Runge phenomenon (the fact that polynomial interpolants on equispaced grids become violently oscillatory as their degree increases). In the context of quadrature methods on equispaced nodes, and for orders of accuracy around 10 or higher, this leads to weights of oscillating signs and large magnitudes. This article develops further a recently discovered approach for avoiding these adverse effects.},
  archive      = {J_SIREV},
  author       = {Bengt Fornberg},
  doi          = {10.1137/18M1229353},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {167-180},
  shortjournal = {SIAM Rev.},
  title        = {Improving the accuracy of the trapezoidal rule},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Education. <em>SIREV</em>, <em>63</em>(1), 165–166. (<a
href="https://doi.org/10.1137/21N975199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SIAM Review presents three papers in the Education section. The first paper, “Improving the Accuracy of the Trapezoidal Rule,” is written by Bengt Fornberg. The trapezoidal rule is a basic technique for numerical calculation of definite one-dimensional integrals. The author provides a historical perspective of the developments around this rule. He starts with a reference tracing it back to Babylonian astronomers before 50 B.C., discusses the work of James Gregory (1638--1675) connecting to discoveries of Leonhard Euler (1707--1783) and Colin Maclaurin (1698--1746), and concludes with recent work published in the period 2012--2018. The author starts by introducing the basic ideas and observations that motivate the improvements of the trapezoidal rule. The leading numerical errors in that approximation method come from the integration bounds. Hence, one should adjust the quadrature weights to increase the accuracy in this type of procedure. Pioneering work in this direction is contained in the letters of James Gregory. His work is explained in section 2, where it is shown that Gregory&#39;s weights lead to Euler--Maclaurin coefficients while those coefficients became available many years later. This is a remarkable achievement because Gregory&#39;s work also precedes the first publications on calculus by Leibniz and Newton, as well as Brook Taylor&#39;s publication on what we call Taylor expansions. The main portion of the paper surveys recent developments in which Gregory&#39;s method has been revisited. A finite difference approach resulting from radial basis functions was proposed in 2018 by J. Reeger and B. Fornberg. While that work deals with more general numerical quadratures that use nodes over bounded curved surfaces in a three-dimensional space, a special case within the proposed approach produced Gregory-like methods with high orders of accuracy. The author goes further to explain the way of calculating the weights and provides some test results. An appendix contains MATLAB code for calculating quadrature weights. The presentation is very intuitive and accessible to a wide audience of students. The second paper is “Fluid-Structure Interaction for the Classroom: Interpolation, Hearts, and Swimming!” It is presented by Nicholas A. Battista. Here, interpolation of a function by splines is discussed in the context of mathematical modeling of biological fluid dynamics. The interpolation provides the description of the motion of objects in a fluid. Three examples of increasing complexity illustrate this approach. The first example discusses three circles and their movement within a fluid. In this context, various relevant questions are introduced. For example, do we have enough degrees of freedom to enforce continuous derivatives? How do linear interpolating functions compare to higher-order polynomials? The second example uses the same prescribed motion as in the first one but introduces different polynomials to interpolate between successive states. In this way, the motion of a beating heart can be simulated. The final example provides an idealized model of swimming. It is assumed that the swimmer&#39;s body bends so that it switches between two states of distinct curvature. The swimmer&#39;s body is approximated by connecting a line segment and a polynomial section in the interpolation procedure. Locomotion emerges due to the swimmer&#39;s interactions with the surrounding fluid. The article is addressed to teaching faculty. The author suggests how and when to bring up various relevant questions in this module. He provides guidance for the discussions and a code for the examples. References to recent scientific studies that have used this approach to successfully prescribe motion in many fields are included. The article also contains a link to a website with software that allows one to build and test spline interpolation for the purpose of modeling motion between one or more feature states. Two implementations, in MATLAB and in Python, are included, as well as links to tutorials helping with the software. The third contribution to the section is the paper “DeepXDE: A Deep Learning Library for Solving Differential Equations,” co-authored by Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. The paper elucidates recent developments in the numerical solutions of partial differential equations (PDEs) based on deep learning. The paper starts with an overview of physics-informed neural networks (PINNs) and the use of automatic differentiation. Most of the presentation is based on the simplest type of neural networks, called feed-forward (FNN), as they are considered sufficient for most PDE problems. However, the type of network is not crucial for the approach. The authors explain briefly how the algorithmic calculation of derivatives of the network outputs with respect to the network inputs is performed. Then the PINN algorithm is presented. Consider a PDE is given, which is parameterized by $\lambda$ for the solution $u(x)$ with $x$ on a given domain with suitable boundary conditions. A neural network $\hat{u}(x;\theta )$ is constructed as a surrogate of $u(x)$. The net takes the input $x$ and outputs a vector with the same dimension as $u$ depending on the net&#39;s parameters gathered in the vector $\theta$; the weight matrices and the bias vectors of all network layers. The derivatives of $\hat{u}(\cdot;\theta )$ are computed algorithmically as discussed before by applying the chain rule for differentiating compositions of functions. Two sets of randomly selected points in the domain of the PDE and its boundary are taken into account in order to reflect the physics imposed by the PDE and the boundary conditions. The loss function is the weighted $L^2$ norm of the residuals stemming from the PDE equation and its boundary condition on the selected points. The neural network is trained to find the best parameter $\theta^\ast$ by minimizing the loss function. The authors also propose a new adaptive refinement method to improve the training efficiency of PINNs. Further discussion in the paper includes the approximation properties and error analysis for the PINNs. The authors provide an implementation of PINNs in the form of a Python library, called DeepXDE, and discuss its features and customization abilities in section 3. They advocate the PINN algorithm as simple and widely applicable: it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, stochastic PDEs, as well as inverse problems. A representative example of each type is provided in section 4 of the paper. The concluding remarks contain a discussion on the advantages and limitation of the approach and on potential future extensions.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/21N975199},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {165-166},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What tropical geometry tells us about the complexity of
linear programming. <em>SIREV</em>, <em>63</em>(1), 123–164. (<a
href="https://doi.org/10.1137/20M1380211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tropical geometry has been recently used to obtain new complexity results in convex optimization and game theory. In this paper, we present an application of this approach to a famous class of algorithms for linear programming, i.e., log-barrier interior point methods. We show that these methods are not strongly polynomial by constructing a family of linear programs with $3r+1$ inequalities in dimension $2r$ for which the number of iterations performed is in $\Omega(2^r)$. The total curvature of the central path of these linear programs is also exponential in $r$, disproving a continuous analogue of the Hirsch conjecture proposed by Deza, Terlaky, and Zinchenko. These results are obtained by analyzing the tropical central path, which is the piecewise linear limit of the central paths of parameterized families of classical linear programs viewed through “logarithmic glasses.” This allows us to provide combinatorial lower bounds for the number of iterations and the total curvature in a general setting.},
  archive      = {J_SIREV},
  author       = {Xavier Allamigeon and Pascal Benchimol and Stéphane Gaubert and Michael Joswig},
  doi          = {10.1137/20M1380211},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {123-164},
  shortjournal = {SIAM Rev.},
  title        = {What tropical geometry tells us about the complexity of linear programming},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). SIGEST. <em>SIREV</em>, <em>63</em>(1), 121. (<a
href="https://doi.org/10.1137/21N975187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “What Tropical Geometry Tells Us about the Complexity of Linear Programming,” by Xavier Allamigeon, Pascal Benchimol, Stéphane Gaubert, and Michael Joswig. Linear programming means optimizing a linear objective function with respect to linear equality constraints and linear inequality constraints. This remains a pervasive task in modern society, where decisions must be made while keeping control of multiple factors, such as staff time, raw materials, energy, financial outlay, or carbon footprint. Linear programs also arise as subtasks for more general problems, often forming computational bottlenecks. The search for linear programming algorithms with good worst-case complexity was boosted by the work of Khachiyan (1979) and Karmarker (1984), which brought interior point methods to the fore. In this SIGEST article, the authors construct a negative result: they define a linear program for which a widely used class of interior point methods has complexity that is exponential in the number of variables (Theorem A). This leads to a counterexample for the continuous analogue of the Hirsch conjecture, proposed by Deza, Terlaky, and Zinchenko in 2009. The authors&#39; proofs use the tools of tropical geometry, a world where addition is replaced by maximization and multiplication is replaced by standard addition. Intuitively, this approach is likely to have value in scheduling-type problems because (a) for two activities that may be performed concurrently, the time required is the maximum of the individual times, and (b) for two activities that must take place consecutively, the time required is the sum of the individual times. The original version of this article appeared in the SIAM Journal on Applied Algebra and Geometry in 2018. In preparing this SIGEST version, the authors have added new material to sections 1 and 2 in order to increase accessibility, and in section 9 they have included a discussion of further work in this area and relevant open problems.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/21N975187},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {121},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The dynamics of bilateral olfactory search and navigation.
<em>SIREV</em>, <em>63</em>(1), 100–120. (<a
href="https://doi.org/10.1137/19M1265934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animals use stereo sampling of odor concentration to localize sources and follow odor trails. We analyze the dynamics of a bilateral model that depends on the simultaneous comparison between odor concentrations detected by left and right sensors. The general model consists of three differential equations for the positions in the plane and the heading. When the odor landscape is an infinite trail, we reduce the dynamics to a planar system whose dynamics has just two fixed points. Using an integrable approximation (for short sensors) we estimate the basin of attraction. In the case of a radially symmetric landscape, we again can reduce the dynamics to a planar system, but the behavior is considerably richer with multistability, isolas, and limit cycles. As in the linear trail case, there is also an underlying integrable system when the sensors are short. In odor landscapes that consist of multiple spots and trail segments, we find periodic and chaotic dynamics and characterize the behavior on trails with gaps and trails that turn corners.},
  archive      = {J_SIREV},
  author       = {Nour Riman and Jonathan D. Victor and Sebastian D. Boie and Bard Ermentrout},
  doi          = {10.1137/19M1265934},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {100-120},
  shortjournal = {SIAM Rev.},
  title        = {The dynamics of bilateral olfactory search and navigation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Persistent homology of geospatial data: A case study with
voting. <em>SIREV</em>, <em>63</em>(1), 67–99. (<a
href="https://doi.org/10.1137/19M1241519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial step in the analysis of persistent homology is the transformation of data into an appropriate topological object (which, in our case, is a simplicial complex). Software packages for computing persistent homology typically construct Vietoris--Rips or other distance-based simplicial complexes on point clouds because they are relatively easy to compute. We investigate alternative methods of constructing simplicial complexes and the effects of making associated choices during simplicial-complex construction on the output of persistent-homology algorithms. We present two new methods for constructing simplicial complexes from two-dimensional geospatial data (such as maps). We apply these methods to a California precinct-level voting data set, and we thereby demonstrate that our new constructions can capture geometric characteristics that are missed by distance-based constructions. Our new constructions can thus yield more interpretable persistence modules and barcodes for geospatial data. In particular, they are able to distinguish short-persistence features that occur only for a narrow range of distance scales (e.g., voting patterns in densely populated cities) from short-persistence noise by incorporating information about other spatial relationships between regions.},
  archive      = {J_SIREV},
  author       = {Michelle Feng and Mason A. Porter},
  doi          = {10.1137/19M1241519},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {67-99},
  shortjournal = {SIAM Rev.},
  title        = {Persistent homology of geospatial data: A case study with voting},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Research spotlights. <em>SIREV</em>, <em>63</em>(1), 65.
(<a href="https://doi.org/10.1137/21N975175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have two Research Spotlights papers in the lineup this issue. The first of these, “Persistent Homology of Geospatial Data: A Case Study with Voting,&quot; written by Michelle Feng and Mason Porter, is quite timely. The article begins with a gentle introduction into the what and why of persistent homology (PH): after converting the point cloud of data into a series of simplicial complexes at different scales, the homology groups are computed at each scale, with homological features tracked across scales. It is generally assumed that persistence of features can then be used to distinguish signal (important patterns) from noise. Of concern is the method of converting the data into the simplicial complexes. The authors argue that PH using the simplicial complexes computed via the distance-based approach can lead to erroneous assumptions or missed characteristics about the voting data they consider. They propose to counter these deficiencies by developing two new methods, one based on adjacency networks and one that utilizes level sets to capture the manifold nature of the data, for building a filtered simplicial complex. The dramatic differences achieved by varying the complexes on the resulting PH are colorfully illustrated and interpreted for real voter data. Readers may appreciate not only the tools used and results obtained, but also the broad range of problems---transportation networks, spatial demography, granular materials, biological structures---identified by the authors for which the new approaches may prove relevant. An animal&#39;s ability to localize odor sources and follow trails of odor is key to many aspects of its survival. Authors Nour Riman, Jonathan D. Victor, Sebastian D. Boie, and Bard Ermentrout focus their article, “The Dynamics of Bilateral Olfactory Search and Navigation,&quot; on providing a comprehensive mathematical analysis of the dynamics of the differential equations that model the way that animals use bilateral information to navigate odor sources. When odor sensors are located at two different positions of the body, it enables comparison of odor concentrations and thus allows the animal to determine whether to move toward or away from the stimulus. Tropotaxis in the present context refers to the movement of the animal in response to an odor directly toward or away from the source of the odor. The sensor angles, which the animal can control, play an important role in enabling the animal to find and follow a trail. The other model parameters include sensor length and sensitivity to concentration change. The authors investigate tropotaxis as functions of the model parameters under varying conditions for the odor sources and trails. In some cases, the authors are able to reduce the dynamics to a planar system. Graphical illustrations compliment the discussion of the behavior. The paper concludes with a discussion of future research, particularly with respect to exploring the effects of noise on the models. It is the hope that this new “understanding [of] the underlying dynamics of the bilateral model will help in building models that use bilateral information together with other strategies.&quot;},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/21N975175},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {65},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stationary distributions of continuous-time markov chains: A
review of theory and truncation-based approximations. <em>SIREV</em>,
<em>63</em>(1), 3–64. (<a
href="https://doi.org/10.1137/19M1289625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the stationary distributions of a continuous-time Markov chain (CTMC) involves solving a set of linear equations. In most cases of interest, the number of equations is infinite or too large, and the equations cannot be solved analytically or numerically. Several approximation schemes overcome this issue by truncating the state space to a manageable size. In this review, we first give a comprehensive theoretical account of the stationary distributions and their relation to the long-term behaviour of CTMCs that is readily accessible to non-experts and free of irreducibility assumptions made in standard texts. We then review truncation-based approximation schemes for CTMCs with infinite state spaces paying particular attention to the schemes&#39; convergence and the errors they introduce, and we illustrate their performance with an example of a stochastic reaction network of relevance in biology and chemistry. We conclude by elaborating on computational trade-offs associated with error control and several open questions.},
  archive      = {J_SIREV},
  author       = {Juan Kuntz and Philipp Thomas and Guy-Bart Stan and Mauricio Barahona},
  doi          = {10.1137/19M1289625},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {3-64},
  shortjournal = {SIAM Rev.},
  title        = {Stationary distributions of continuous-time markov chains: A review of theory and truncation-based approximations},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Survey and review. <em>SIREV</em>, <em>63</em>(1), 1. (<a
href="https://doi.org/10.1137/21N975163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of probabilistic ideas by applied mathematicians has seen a continued increase in recent decades. Probability now appears frequently at the modeling stage. There is widespread interest in investigating the effects of noise and uncertainty. Probabilistic algorithms are routinely applied with much success to the solution of deterministic problems (think of Monte Carlo quadrature or stochastic gradient descent). Markov chains give a simple, widely used tool to describe systems that evolve randomly. They are also a very popular topic in applied mathematics courses. At times \(0\), \(1\), \(2\), łdots, a Markov chain jumps from its current state \(x\) to a randomly chosen new state \(y\); the set of possible states is discrete. The requirement that the jumping times be uniformly spaced is a clear limitation in many situations, and this shortcoming is avoided by considering continuous-time Markov chains. In the continuous-time setting, if the chain has reached state \(x\) at time \(t_i\), it will jump to the randomly chosen \(y\) at time \(t_i+1=t_i+\tau_i\), where the waiting time \(\tau_i&gt;0\) is itself random. Markov chains in continuous time are featured in many applications, including chemistry, ecology, and epidemiology. In a chemical system containing \(n\) species \(S_1\), łdots, \(S_n\), each state corresponds to vector \((c_1\), łdots, \( c_n)\) where \(c_j\) is the number of molecules of species \(S_j\). The species may take part in a number of chemical reactions, let us say \(S_1+2S_2 \rightarrow S_3\), \(2S_1+3S_3 \rightarrow S_4+2S_6\), and so on. At random times, one or another of the \(m\) possible reactions will take place and the state will change. Such a stochastic, molecular approach typically provides a more accurate description of the system than deterministic treatments where the concentrations of the different species are regarded as continuous variables that evolve according to a set of differential equations. This is particularly true in systems where, for some species, the number \(c_j\) is low, as is the case in many biological reactions. The following Survey and Review paper, “Stationary Distributions of Continuous-Time Markov Chains: A Review of Theory and Truncation-Based Approximations,” has been written by Juan Kuntz, Philipp Thomas, Guy-Bart Stan, and Mauricio Barahona. Section 2 provides a reader-friendly introduction to continuous-time Markov chains and their stationary distributions; these are important because they determine the long-time behavior of the chain. A salient future is that the authors present, in an accessible way, results that do not assume the chain to be irreducible (roughly speaking, irreducibility means that the chain is not the juxtaposition of two or more smaller chains that do not talk to each other; irreducibility simplifies the math, but is not a reasonable hypothesis in some applications). After that, the authors concentrate on how to compute invariant distributions. The paper contains numerical results, an extensive bibliography, and a detailed discussion of open problems.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/21N975163},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {1},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
