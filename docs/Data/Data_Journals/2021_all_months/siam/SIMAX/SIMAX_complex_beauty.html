<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMAX_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simax---70">SIMAX - 70</h2>
<ul>
<li><details>
<summary>
(2021). Computing symplectic eigenpairs of symmetric
positive-definite matrices via trace minimization and riemannian
optimization. <em>SIMAX</em>, <em>42</em>(4), 1732–1757. (<a
href="https://doi.org/10.1137/21M1390621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of computing the smallest symplectic eigenvalues and the corresponding eigenvectors of symmetric positive-definite matrices in the sense of Williamson&#39;s theorem. It is formulated as minimizing a trace cost function over the symplectic Stiefel manifold. We first investigate various theoretical aspects of this optimization problem such as characterizing the sets of critical points, saddle points, and global minimizers as well as proving that nonglobal local minimizers do not exist. Based on our recent results on constructing Riemannian structures on the symplectic Stiefel manifold and the associated optimization algorithms, we then propose a numerical procedure for computing symplectic eigenpairs in the framework of Riemannian optimization. Moreover, a connection of the sought solution with the eigenvalues of a special class of Hamiltonian matrices is discussed. Numerical examples are presented.},
  archive      = {J_SIMAX},
  author       = {Nguyen Thanh Son and P.-A. Absil and Bin Gao and Tatjana Stykel},
  doi          = {10.1137/21M1390621},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1732-1757},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computing symplectic eigenpairs of symmetric positive-definite matrices via trace minimization and riemannian optimization},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor-structured sketching for constrained least squares.
<em>SIMAX</em>, <em>42</em>(4), 1703–1731. (<a
href="https://doi.org/10.1137/20M1374596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained least squares problems arise in many applications. Their memory and computation costs are expensive in practice involving high-dimensional input data. We employ the so-called “sketching” strategy to project the least squares problem onto a space of a much lower “sketching dimension” via a random sketching matrix. The key idea of sketching is to reduce the dimension of the problem as much as possible while maintaining the approximation accuracy. Tensor structure is often present in the data matrices of least squares, including linearized inverse problems and tensor decompositions. In this work, we utilize a general class of row-wise tensorized sub-Gaussian matrices as sketching matrices in constrained optimizations for the sketching design&#39;s compatibility with tensor structures. We provide theoretical guarantees on the sketching dimension in terms of error criterion and probability failure rate. In the context of unconstrained linear regressions, we obtain an optimal estimate for the sketching dimension. For optimization problems with general constraint sets, we show that the sketching dimension depends on a statistical complexity that characterizes the geometry of the underlying problems. Our theories are demonstrated in a few concrete examples, including unconstrained linear regression and sparse recovery problems.},
  archive      = {J_SIMAX},
  author       = {Ke Chen and Ruhui Jin},
  doi          = {10.1137/20M1374596},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1703-1731},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Tensor-structured sketching for constrained least squares},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate computation of generalized eigenvalues of regular
SR-BP pairs. <em>SIMAX</em>, <em>42</em>(4), 1680–1702. (<a
href="https://doi.org/10.1137/21M1392152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the generalized eigenvalue problem (GEP) for bidiagonal-product (BP) pairs with sign regularity (SR), which include structured pairs associated with ill-conditioned matrices, such as Cauchy and Vandermonde matrices, that arise in many applications. A sufficient and necessary condition is provided for an SR-BP pair to be regular. For regular SR-BP pairs having both matrices singular, we establish sharp relative perturbation bounds to show that all the generalized eigenvalues including infinite and zero ones can be accurately determined by their BP representations. By operating on the BP representations, a new method is developed to accurately compute generalized eigenvalues of such a regular SR-BP pair. Our method first transforms the pair into an equivalent pair with a certain sign regularity. Then a technique is proposed to implicitly deflate all the infinite eigenvalues. After deflating infinite eigenvalues, finite eigenvalues are computed by reducing the deflated GEP into a standard eigenvalue problem. An attractive property of our method is that all the generalized eigenvalues are returned to high relative accuracy especially, all the infinite and zero eigenvalues are computed exactly. Error analysis and numerical experiments are performed to confirm the claimed high relative accuracy.},
  archive      = {J_SIMAX},
  author       = {Rong Huang},
  doi          = {10.1137/21M1392152},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1680-1702},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Accurate computation of generalized eigenvalues of regular SR-BP pairs},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localization and pseudospectra of twisted toeplitz matrices
with applications to ion channels. <em>SIMAX</em>, <em>42</em>(4),
1656–1679. (<a href="https://doi.org/10.1137/20M1384920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study pseudospectra of matrices arising in models of ion channel dynamics, bimolecular reactions, and Michaelis--Menten enzyme kinetics. We show that matrices for ion channel models and for bimolecular reactions are examples of a special class known as twisted Toeplitz matrices. Using Michaelis--Menten enzyme kinetics as an example, we suggest more complicated reactions cannot be modeled simply by the twisted Toeplitz ideas currently in the literature, and thus we propose a generalization to what we call block twisted Toeplitz matrices. Furthermore, we show that certain simple cases of ion channel models always exhibit a phenomenon known as localization. This result comes by studying the symbol of the operator and an application of a theorem of Trefethen and Chapman. We further observe that a larger magnitude of the so-called twist-ratio, a number that comes from the symbol, is associated with stronger localization. For certain bimolecular reactions, specialized to a dimerization, we show by the same theorem that there are large regions of parameter space where localization is guaranteed and that this is always guaranteed in the limit as the matrix size becomes very large. However, we also find examples for these reactions where the hypotheses of the theorem of Trefethen and Chapman do not hold, and yet in numerical experiments, we still observe localization. Finally we return to the first of our matrix models for a more detailed study to give further insight into important applications to cardiac ion channels and in particular sodium channels. We demonstrate behavior similar to the so-called cutoff phenomenon of Markov processes, and we show that the stationary distribution, which is so crucial to applications of these models to ion channels and especially sodium channels, can be extremely localized.},
  archive      = {J_SIMAX},
  author       = {Kevin Burrage and Pamela Burrage and Shev Macnamara},
  doi          = {10.1137/20M1384920},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1656-1679},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Localization and pseudospectra of twisted toeplitz matrices with applications to ion channels},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-performance computation of the exponential of a large
sparse matrix. <em>SIMAX</em>, <em>42</em>(4), 1636–1655. (<a
href="https://doi.org/10.1137/20M1342987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation of the large sparse matrix exponential has been an important topic in many fields, such as network and finite-element analysis. The existing scaling and squaring algorithm (SSA) is not suitable for the computation of the large sparse matrix exponential as it requires more memory and has a higher computational cost than is actually needed. By introducing two novel concepts, i.e., real bandwidth and $\varepsilon\text{-}$bandwidth, to measure the sparsity of the matrix, the sparsity of the matrix exponential is analyzed. It is found that for every matrix computed in the squaring phase of the SSA, a corresponding sparse approximate matrix exists. To obtain the sparse approximate matrix, a new filtering technique based on forward error analysis is proposed. Combining the filtering technique with the idea of keeping track of the incremental part, a competitive algorithm is developed for the large sparse matrix exponential. Due to the filtering technique, the proposed method can greatly alleviate the overscaling problem. Three sets of numerical experiments, including one large matrix with a dimension larger than $2 \times {10^6}$, are conducted. The numerical experiments show that, compared with the expm function in MATLAB, the proposed algorithm can provide higher accuracy at lower computational cost and with less memory.},
  archive      = {J_SIMAX},
  author       = {Feng Wu and Kailing Zhang and Li Zhu and Jiayao Hu},
  doi          = {10.1137/20M1342987},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1636-1655},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {High-performance computation of the exponential of a large sparse matrix},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balanced truncation model reduction for symmetric second
order systems—a passivity-based approach. <em>SIMAX</em>,
<em>42</em>(4), 1602–1635. (<a
href="https://doi.org/10.1137/20M1346109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a model reduction approach for linear time-invariant second order systems based on positive real balanced truncation. Our method guarantees to preserve asymptotic stability and passivity of the reduced order model as well as the positive definiteness of the mass and stiffness matrices. Moreover, we receive an a priori gap metric error bound. Finally we show that our method based on positive real balanced truncation preserves the structure of overdamped second order systems.},
  archive      = {J_SIMAX},
  author       = {Ines Dorschky and Timo Reis and Matthias Voigt},
  doi          = {10.1137/20M1346109},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1602-1635},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Balanced truncation model reduction for symmetric second order systems---A passivity-based approach},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mittag–leffler functions and their applications in network
science. <em>SIMAX</em>, <em>42</em>(4), 1581–1601. (<a
href="https://doi.org/10.1137/21M1407276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a complete theory for walk-based centrality indices in complex networks defined in terms of Mittag--Leffler functions. This overarching theory includes as special cases well-known centrality measures like subgraph centrality and Katz centrality. The indices we introduce are parametrized by two numbers; by letting these vary, we show that Mittag--Leffler centralities interpolate between degree and eigenvector centrality, as well as between resolvent-based and exponential-based indices. We further discuss modelling and computational issues, and provide guidelines on parameter selection. The theory is then extended to the case of networks that evolve over time. Numerical experiments on synthetic and real-world networks are provided.},
  archive      = {J_SIMAX},
  author       = {Francesca Arrigo and Fabio Durastante},
  doi          = {10.1137/21M1407276},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1581-1601},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Mittag--leffler functions and their applications in network science},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding the nearest passive or nonpassive system via
hamiltonian eigenvalue optimization. <em>SIMAX</em>, <em>42</em>(4),
1553–1580. (<a href="https://doi.org/10.1137/20M1376972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study an algorithm for computing a nearest passive system to a given nonpassive linear time-invariant system (with much freedom in the choice of the metric defining “nearest,” which may be restricted to structured perturbations), and also a closely related algorithm for computing the structured distance of a given passive system to nonpassivity. Both problems are addressed by solving eigenvalue optimization problems for Hamiltonian matrices that are constructed from perturbed system matrices. The proposed algorithms are two-level methods that optimize the Hamiltonian eigenvalue of the smallest positive real part over perturbations of a fixed size in the inner iteration, using a constrained gradient flow. They optimize over the perturbation size in the outer iteration, which is shown to converge quadratically in the typical case of a defective coalescence of simple eigenvalues approaching the imaginary axis. For large systems, we propose a variant of the algorithm that takes advantage of the inherent low-rank structure of the problem. Numerical experiments illustrate the behavior of the proposed algorithms.},
  archive      = {J_SIMAX},
  author       = {Antonio Fazzi and Nicola Guglielmi and Christian Lubich},
  doi          = {10.1137/20M1376972},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1553-1580},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Finding the nearest passive or nonpassive system via hamiltonian eigenvalue optimization},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularization by inexact krylov methods with applications
to blind deblurring. <em>SIMAX</em>, <em>42</em>(4), 1528–1552. (<a
href="https://doi.org/10.1137/21M1402066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the regularization of large-scale discrete inverse problems by means of inexact Krylov methods. Specifically, we derive two new inexact Krylov methods that can be efficiently applied to unregularized or Tikhonov-regularized least squares problems, and we study their theoretical properties, including links with their exact counterparts and strategies to monitor the amount of inexactness. We then apply the new methods to separable nonlinear inverse problems arising in blind deblurring, where both the sharp image and the parameters defining the blur are unkown. When employing a variable projection method jointly with the new inexact solvers in this setting, the latter can naturally handle varying inexact blurring parameters while solving the linear deblurring subproblems, allowing for a much reduced number of total iterations and substantial computational savings with respect to their exact counterparts.},
  archive      = {J_SIMAX},
  author       = {Silvia Gazzola and Malena Sabaté Landman},
  doi          = {10.1137/21M1402066},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1528-1552},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Regularization by inexact krylov methods with applications to blind deblurring},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Absolute variation of ritz values, principal angles, and
spectral spread. <em>SIMAX</em>, <em>42</em>(4), 1506–1527. (<a
href="https://doi.org/10.1137/20M1386906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $A$ be a $d\times d$ complex self-adjoint matrix, let $\mathcal{X},\mathcal{Y}\subset \mathbb{C}^d$ be $k$-dimensional subspaces, and let $X$ be a $d\times k$ complex matrix whose columns form an orthonormal basis of $\mathcal{X}$; that is, $\mathcal{X}$ is an isometry whose range is the subspace $\mathcal{X}$. We construct a $d\times k$ complex matrix $Y_r$ whose columns form an orthonormal basis of $\mathcal{Y}$ and obtain sharp upper bounds for the singular values $s(X^*AX-Y_r^*\,A\,Y_r)$ in terms of submajorization relations involving the principal angles between $\mathcal{X}$ and $\mathcal{Y}$ and the spectral spread of $A$. We apply these results to obtain sharp upper bounds for the absolute variation of the Ritz values of $A$ associated with the subspaces $\mathcal{X}$ and $\mathcal{Y}$ that partially confirm conjectures by Knyazev and Argentati.},
  archive      = {J_SIMAX},
  author       = {Pedro Massey and Demetrio Stojanoff and Sebastián Zárate},
  doi          = {10.1137/20M1386906},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1506-1527},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Absolute variation of ritz values, principal angles, and spectral spread},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subspace recycling–based regularization methods.
<em>SIMAX</em>, <em>42</em>(4), 1480–1505. (<a
href="https://doi.org/10.1137/20M1379617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace recycling techniques have been used quite successfully for the acceleration of iterative methods for solving large-scale linear systems. These methods often work by augmenting a solution subspace generated iteratively by a known algorithm with a fixed subspace of vectors which are “useful” for solving the problem. Often, this has the effect of inducing a projected version of the original linear system to which the known iterative method is then applied, and this projection can act as a deflation preconditioner, accelerating convergence. Most often, these methods have been applied for the solution of well-posed problems. However, they have also begun to be considered for the solution of ill-posed problems. In this paper, we consider subspace augmentation-type iterative schemes applied to linear ill-posed problems in a continuous Hilbert space setting, based on a recently developed framework describing these methods. We show that under suitable assumptions, a recycling method satisfies the formal definition of a regularization, as long as the underlying scheme is itself a regularization. We then develop an augmented subspace version of the gradient descent method and demonstrate its effectiveness, both on an academic Gaussian blur model and on problems arising from the adaptive optics community for the resolution of large sky images by ground-based extremely large telescopes.},
  archive      = {J_SIMAX},
  author       = {Ronny Ramlau and Kirk M. Soodhalter and Victoria Hutterer},
  doi          = {10.1137/20M1379617},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1480-1505},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Subspace recycling--based regularization methods},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An alternating rank-k nonnegative least squares framework
(ARkNLS) for nonnegative matrix factorization. <em>SIMAX</em>,
<em>42</em>(4), 1451–1479. (<a
href="https://doi.org/10.1137/20M1352405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a prominent technique for data dimensionality reduction that has been widely used for text mining, computer vision, pattern discovery, and bioinformatics. In this paper, a framework called ARkNLS (alternating rank-k nonnegativity-constrained least squares) is proposed for computing NMF. First, a recursive formula for the solution of the rank-k nonnegativity-constrained least squares (NLS) is established. This recursive formula can be used to derive the closed-form solution for the rank-$k$ NLS problem for any integer $k\geq 1$. As a result, each subproblem for an alternating rank-$k$ nonnegative least squares framework can be obtained based on this closed-form solution. Assuming that all matrices involved in rank-$k$ NLS in the context of NMF computation are of full rank, two of the currently best NMF algorithms HALS (hierarchical alternating least squares) and ANLS-BPP (alternating NLS based on block principal pivoting) can be considered as special cases of ARkNLS with $k=1$ and $k=r$ for rank-$r$ NMF, respectively. This paper then focuses on the framework with $k=3$, which leads to a new algorithm for NMF via the closed-form solution of the rank-3 NLS problem. Furthermore, a new strategy that efficiently overcomes the potential singularity problem in rank-3 NLS within the context of NMF computation is also presented. Extensive numerical comparisons using real and synthetic data sets demonstrate that the proposed algorithm provides state-of-the-art performance in terms of computational accuracy and CPU time.},
  archive      = {J_SIMAX},
  author       = {Delin Chu and Wenya Shi and Srinivas Eswar and Haesun Park},
  doi          = {10.1137/20M1352405},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1451-1479},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An alternating rank-k nonnegative least squares framework (ARkNLS) for nonnegative matrix factorization},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uniform error estimates for the lanczos method.
<em>SIMAX</em>, <em>42</em>(3), 1423–1450. (<a
href="https://doi.org/10.1137/20M1331470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lanczos method is one of the most powerful and fundamental techniques for solving an extremal symmetric eigenvalue problem. Convergence-based error estimates depend heavily on the eigenvalue gap. In practice, this gap is often relatively small, resulting in significant overestimates of error. One way to avoid this issue is through the use of uniform error estimates, namely, bounds that depend only on the dimension of the matrix and the number of iterations. In this work, we prove explicit upper and lower uniform error estimates for the Lanczos method. These lower bounds, paired with numerical results, imply that the maximum error of $m$ iterations of the Lanczos method over all $n \times n$ symmetric matrices does indeed depend on the dimension $n$ in practice. The improved bounds for extremal eigenvalues translate immediately to error estimates for the condition number of a symmetric positive definite matrix. In addition, we prove more specific results for matrices that possess some level of eigenvalue regularity or whose eigenvalues converge to some limiting empirical spectral distribution. Through numerical experiments, we show that the theoretical estimates of this paper do apply to practical computations for reasonably sized matrices.},
  archive      = {J_SIMAX},
  author       = {John C. Urschel},
  doi          = {10.1137/20M1331470},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1423-1450},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Uniform error estimates for the lanczos method},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multiprecision derivative-free schur–parlett algorithm for
computing matrix functions. <em>SIMAX</em>, <em>42</em>(3), 1401–1422.
(<a href="https://doi.org/10.1137/20M1365326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Schur--Parlett algorithm, implemented in MATLAB as \textttfunm, evaluates an analytic function $f$ at an $n\times n$ matrix argument by using the Schur decomposition and a block recurrence of Parlett. The algorithm requires the ability to compute $f$ and its derivatives, and it requires that $f$ have a Taylor series expansion with a suitably large radius of convergence. We develop a version of the Schur--Parlett algorithm that requires only function values and not derivatives. The algorithm requires access to arithmetic of a matrix-dependent precision at least double the working precision, which is used to evaluate $f$ on the diagonal blocks of order greater than 2 (if there are any) of the reordered and blocked Schur form. The key idea is to compute by diagonalization the function of a small random diagonal perturbation of each diagonal block, where the perturbation ensures that diagonalization will succeed. Our algorithm is inspired by Davies&#39;s randomized approximate diagonalization method, but we explain why that is not a reliable numerical method for computing matrix functions. This multiprecision Schur--Parlett algorithm is applicable to arbitrary analytic functions $f$ and, like the original Schur--Parlett algorithm, it generally behaves in a numerically stable fashion. The algorithm is especially useful when the derivatives of $f$ are not readily available or accurately computable. We apply our algorithm to the matrix Mittag--Leffler function and show that it yields results of accuracy similar to, and in some cases much greater than, the state-of-the-art algorithm for this function.},
  archive      = {J_SIMAX},
  author       = {Nicholas J. Higham and Xiaobo Liu},
  doi          = {10.1137/20M1365326},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1401-1422},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A multiprecision derivative-free schur--parlett algorithm for computing matrix functions},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Projection method for eigenvalue problems of linear
nonsquare matrix pencils. <em>SIMAX</em>, <em>42</em>(3), 1381–1400. (<a
href="https://doi.org/10.1137/20M1377886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eigensolvers involving complex moments can determine all the eigenvalues in a given region in the complex plane and the corresponding eigenvectors of a regular linear matrix pencil. The complex moment acts as a filter for extracting the eigencomponents of interest from random vectors or matrices. This study extends a projection method for regular eigenproblems to the singular nonsquare case, thus replacing the standard matrix inverse in the resolvent with the pseudoinverse. The extended method involves complex moments given by the contour integrals of generalized resolvents associated with nonsquare matrices. We establish conditions such that the method gives all finite eigenvalues in a prescribed region in the complex plane. In numerical computations, the contour integrals are approximated using numerical quadratures. The primary cost lies in the solutions of linear least squares problems that arise from quadrature points, and they can be readily parallelized in practice. Numerical experiments on large matrix pencils illustrate this method. The new method is more robust and efficient than previous methods, and based on experimental results, it is conjectured to be more efficient in parallelized settings. Notably, the proposed method does not fail in cases involving pairs of extremely close eigenvalues, and it overcomes the issue of problem size.},
  archive      = {J_SIMAX},
  author       = {Keiichi Morikuni},
  doi          = {10.1137/20M1377886},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1381-1400},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Projection method for eigenvalue problems of linear nonsquare matrix pencils},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The stability of block variants of classical gram–schmidt.
<em>SIMAX</em>, <em>42</em>(3), 1365–1380. (<a
href="https://doi.org/10.1137/21M1394424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The block version of the classical Gram--Schmidt (\tt BCGS) method is often employed to efficiently compute orthogonal bases for Krylov subspace methods and eigenvalue solvers, but a rigorous proof of its stability behavior has not yet been established. It is shown that the usual implementation of \tt BCGS can lose orthogonality at a rate worse than $O(\varepsilon) \kappa^{2}({$\mathcalX$})$, where $\mathcal{X}$ is the input matrix and $\varepsilon$ is the unit roundoff. A useful intermediate quantity denoted as the Cholesky residual is given special attention and, along with a block generalization of the Pythagorean theorem, this quantity is used to develop more stable variants of \tt BCGS. These variants are proven to have $O(\varepsilon) \kappa^2({$\mathcalX$})$ loss of orthogonality with relatively relaxed conditions on the intrablock orthogonalization routine satisfied by the most commonly used algorithms. A variety of numerical examples illustrate the theoretical bounds.},
  archive      = {J_SIMAX},
  author       = {Erin Carson and Kathryn Lund and Miroslav Rozložník},
  doi          = {10.1137/21M1394424},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1365-1380},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The stability of block variants of classical gram--schmidt},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast implementation of the traveling-salesman-problem method
for reordering columns within supernodes. <em>SIMAX</em>,
<em>42</em>(3), 1337–1364. (<a
href="https://doi.org/10.1137/20M1368070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2017 Pichon et al. introduced an effective method for reordering columns within supernodes based on reformulating the underlying optimization problem as a set of traveling salesman problems (TSPs), one for each supernode. The primary problem with their approach is its cost in time, and the primary bottleneck is computing the TSP distances. We introduce techniques that dramatically reduce the time required by their method. First, we introduce a straightforward technique for reducing the size of a TSP, thereby reducing the number of TSP distances that must be computed and also reducing the cost of computing the TSP tour afterward. Second, we introduce a more complex and efficient algorithm for computing the TSP distances that exploits the fact that the relevant sets induce subtrees in the supernodal elimination tree. (A corrected version is attached.)},
  archive      = {J_SIMAX},
  author       = {Mathias Jacquelin and Esmond G. Ng and Barry W. Peyton},
  doi          = {10.1137/20M1368070},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1337-1364},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast implementation of the traveling-salesman-problem method for reordering columns within supernodes},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The asymptotic spectrum of flipped multilevel toeplitz
matrices and of certain preconditionings. <em>SIMAX</em>,
<em>42</em>(3), 1319–1336. (<a
href="https://doi.org/10.1137/20M1379666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we perform a spectral analysis of flipped multilevel Toeplitz sequences, i.e., we study the asymptotic spectral behavior of ${Y_{{n}} T_{{n}} (f)}_{{n}}$, where $T_{{n}}(f)$ is a real, square multilevel Toeplitz matrix generated by a function $f\in L^1([-\pi,\pi]^d)$ and $Y_n$ is the exchange matrix, which has 1&#39;s on the main antidiagonal. In line with what we have shown for unilevel flipped Toeplitz matrix sequences, the asymptotic spectrum is determined by a 2 x 2 matrix-valued function whose eigenvalues are $\pm |f|$. Furthermore, we characterize the eigenvalue distribution of certain preconditioned flipped multilevel Toeplitz sequences with an analysis that covers both multilevel Toeplitz and circulant preconditioners. Finally, all our findings are illustrated by several numerical experiments.},
  archive      = {J_SIMAX},
  author       = {M. Mazza and J. Pestana},
  doi          = {10.1137/20M1379666},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1319-1336},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The asymptotic spectrum of flipped multilevel toeplitz matrices and of certain preconditionings},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of probing techniques for sparse approximation and
trace estimation of decaying matrix functions. <em>SIMAX</em>,
<em>42</em>(3), 1290–1318. (<a
href="https://doi.org/10.1137/20M1364461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of matrix functions $f(A)$, or related quantities like their trace, is an important but challenging task, in particular, for large and sparse matrices $A$. In recent years, probing methods have become an often considered tool in this context, as they allow one to replace the computation of $f(A)$ or tr$(f(A))$ by the evaluation of (a small number of) quantities of the form $f(A)v$ or $v^Tf(A)v$, respectively. These quantities can then be efficiently computed by standard techniques like, e.g., Krylov subspace methods. It is well known that probing methods are particularly efficient when $f(A)$ is approximately sparse, e.g., when the entries of $f(A)$ show a strong off-diagonal decay, but a rigorous error analysis is lacking so far. In this paper we develop new theoretical results on the existence of sparse approximations for $f(A)$ and error bounds for probing methods based on graph colorings. As a by-product, by carefully inspecting the proofs of these error bounds, we also gain new insights into when to stop the Krylov iteration used for approximating $f(A)v$ or $v^Tf(A)v$, thus allowing for a practically efficient implementation of the probing methods.},
  archive      = {J_SIMAX},
  author       = {Andreas Frommer and Claudia Schimmel and Marcel Schweitzer},
  doi          = {10.1137/20M1364461},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1290-1318},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Analysis of probing techniques for sparse approximation and trace estimation of decaying matrix functions},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Submatrices with NonUniformly selected random supports and
insights into sparse approximation. <em>SIMAX</em>, <em>42</em>(3),
1268–1289. (<a href="https://doi.org/10.1137/20M1386384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we derive tail bounds for the norms of random submatrices with nonuniformly distributed supports. We apply these results to sparse approximation and conduct an analysis of the average case performance of thresholding, orthogonal matching pursuit, and basis pursuit. As an application of these results, we characterize sensing dictionaries to improve average performance in the nonuniform case and test their performance numerically.},
  archive      = {J_SIMAX},
  author       = {Simon Ruetz and Karin Schnass},
  doi          = {10.1137/20M1386384},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1268-1289},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Submatrices with NonUniformly selected random supports and insights into sparse approximation},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proxy-GMRES: Preconditioning via GMRES in polynomial space.
<em>SIMAX</em>, <em>42</em>(3), 1248–1267. (<a
href="https://doi.org/10.1137/20M1342562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a class of polynomial preconditioners for solving non-Hermitian linear systems of equations. The polynomial is obtained from a least-squares approximation in polynomial space instead of a standard Krylov subspace. The process for building the polynomial relies on an Arnoldi-like procedure in a small dimensional polynomial space and is equivalent to performing GMRES in polynomial space. It is inexpensive and produces the desired polynomial in a numerically stable way. A few improvements to the basic scheme are discussed including the development of a short-term recurrence and the use of compound preconditioners. Numerical experiments, including a test with challenging nonnormal three-dimensional Helmholtz equations and a few publicly available sparse matrices, are provided to illustrate the performance of the proposed preconditioners.},
  archive      = {J_SIMAX},
  author       = {Xin Ye and Yuanzhe Xi and Yousef Saad},
  doi          = {10.1137/20M1342562},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1248-1267},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Proxy-GMRES: Preconditioning via GMRES in polynomial space},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A theory for backtrack-downweighted walks. <em>SIMAX</em>,
<em>42</em>(3), 1229–1247. (<a
href="https://doi.org/10.1137/20M1384725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a complete theory for the combinatorics of walk-counting on a directed graph in the case where each backtracking step is downweighted by a given factor. By deriving expressions for the associated generating functions, we also obtain linear systems for computing centrality measures in this setting. In particular, we show that backtrack-downweighted Katz-style network centrality can be computed at the same cost as standard Katz. Studying the limit of this centrality measure at its radius of convergence also leads to a new expression for backtrack- downweighted eigenvector centrality that generalizes previous work to the case where directed edges are present. The new theory allows us to combine advantages of standard and nonbacktracking cases, avoiding localization while accounting for tree-like structures. We illustrate the behavior of the backtrack-downweighted centrality measure on both synthetic and real networks.},
  archive      = {J_SIMAX},
  author       = {Francesca Arrigo and Desmond J. Higham and Vanni Noferini},
  doi          = {10.1137/20M1384725},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1229-1247},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A theory for backtrack-downweighted walks},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating leverage scores via rank revealing methods and
randomization. <em>SIMAX</em>, <em>42</em>(3), 1199–1228. (<a
href="https://doi.org/10.1137/20M1314471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study algorithms for estimating the statistical leverage scores of rectangular dense or sparse matrices of arbitrary rank. Our approach is based on combining rank revealing methods with compositions of dense and sparse randomized dimensionality reduction transforms. We first develop a set of fast novel algorithms for rank estimation, column subset selection, and least squares preconditioning. We then describe the design and implementation of leverage score estimators based on these primitives. These estimators are also effective for rank deficient input, which is frequently the case in data analytics applications. We provide detailed complexity analyses for all algorithms as well as meaningful approximation bounds and comparisons with the state of the art. We conduct extensive numerical experiments to evaluate our algorithms and to illustrate their properties and performance using synthetic and real world datasets.},
  archive      = {J_SIMAX},
  author       = {Aleksandros Sobczyk and Efstratios Gallopoulos},
  doi          = {10.1137/20M1314471},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1199-1228},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Estimating leverage scores via rank revealing methods and randomization},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A rational even-IRA algorithm for the solution of <span
class="math inline"><em>T</em></span>-even polynomial eigenvalue
problems. <em>SIMAX</em>, <em>42</em>(3), 1172–1198. (<a
href="https://doi.org/10.1137/20M1364485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present a rational Krylov subspace method for solving real large-scale polynomial eigenvalue problems with $T$-even (that is, symmetric/skew-symmetric) structure. Our method is based on the Even-IRA algorithm [V. Mehrmann, C. Schröder, and V. Simoncini, Linear Algebra Appl., 436 (2012), pp. 4070--4087]. To preserve the structure, a sparse $T$-even linearization from the class of block minimal bases pencils is applied (see [F. M. Dopico et al., Numer. Math., 140 (2018), pp. 373--426). Due to this linearization, the Krylov basis vectors can be computed in a cheap way. Based on the ideas developed in [P. Benner and C. Effenberger, Taiwanese J. Math., 14 (2010), pp. 805--823], a rational decomposition is derived so that our method explicitly allows for changes of the shift during the iteration. This leads to a method that is able to compute parts of the spectrum of a $T$-even matrix polynomial in a fast and reliable way.},
  archive      = {J_SIMAX},
  author       = {Peter Benner and Heike Fassbender and Philip Saltenberger},
  doi          = {10.1137/20M1364485},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1172-1198},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A rational even-IRA algorithm for the solution of $T$-even polynomial eigenvalue problems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank-structured QR for chebyshev rootfinding.
<em>SIMAX</em>, <em>42</em>(3), 1148–1171. (<a
href="https://doi.org/10.1137/20M1375115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the computation of roots of polynomials expressed in the Chebyshev basis. We extend the QR iteration presented in [Y. Eidelman, L. Gemignani, and I. Gohberg, Numer. Algorithms, 47 (2008), pp. 253--273] introducing an aggressive early deflation strategy, and showing that the rank-structure allows one to parallelize the algorithm, avoiding data dependencies which would be present in the unstructured QR. We exploit the particular structure of the colleague linearization to achieve quadratic complexity and linear storage requirements. The (unbalanced) QR iteration used for Chebyshev rootfinding does not guarantee backward stability on the polynomial coefficients, unless the vector of coefficients satisfy $\norm{p} \approx 1$, a hypothesis which is almost never verified for polynomials approximating smooth functions. Even though the presented method is mathematically equivalent to the unbalanced QR algorithm, we show that exploiting the rank structure allows one to guarantee a small backward error on the polynomial, up to an explicitly computable amplification factor $\hat\gamma_1(p)$, which depends on the polynomial under consideration. We show that this parameter is almost always of moderate size, making the method accurate on several numerical tests, in contrast with what happens in the unstructured unbalanced QR.},
  archive      = {J_SIMAX},
  author       = {Angelo Casulli and Leonardo Robol},
  doi          = {10.1137/20M1375115},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1148-1171},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Rank-structured QR for chebyshev rootfinding},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank matrix estimation from rank-one projections by
unlifted convex optimization. <em>SIMAX</em>, <em>42</em>(3), 1119–1147.
(<a href="https://doi.org/10.1137/20M1330099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an estimator with a convex formulation for recovery of low-rank matrices from rank-one projections. Using initial estimates of the factors of the target $d_1\times d_2$ matrix of rank-$r$, the estimator admits a practical subgradient method operating in a space of dimension $r(d_1+d_2)$. This property makes the estimator significantly more scalable than the convex estimators based on lifting and semidefinite programming. Furthermore, we present a streamlined analysis for exact recovery under the real Gaussian measurement model, as well as the partially derandomized measurement model by using the spherical $t$-design. We show that under both models the estimator succeeds, with high probability, if the number of measurements exceeds $r^2 (d_1+d_2)$ up to some logarithmic factors. This sample complexity improves on the existing results for nonconvex iterative algorithms.},
  archive      = {J_SIMAX},
  author       = {Sohail Bahmani and Kiryung Lee},
  doi          = {10.1137/20M1330099},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1119-1147},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Low-rank matrix estimation from rank-one projections by unlifted convex optimization},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized and unified framework of local fourier
analysis using matrix-stencils. <em>SIMAX</em>, <em>42</em>(3),
1096–1118. (<a href="https://doi.org/10.1137/20M1355008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces an extension of the classical local Fourier analysis (LFA) in which the discrete operator is described by a scalar stencil or stencils. First, we extend the scalar stencil to a matrix-stencil, whose coefficients are matrices rather than scalars and which is defined simply on a node-based infinite grid based on a recent work [Y. He, Numer. Linear Algebra Appl., to appear]. Meanwhile, we extend the symbols of stencil operators to matrix-stencil operators. Then, we prove that any scalar stencil operator, no matter how complicated it is, can also be described by a matrix-stencil operator. Furthermore, we prove that the symbols based on the scalar stencils and matrix-stencils of a given discrete operator are unitarily similar, i.e., the symbols have the same spectrum and norms. This connection allows us to develop a simple and unified framework of two-grid LFA based on matrix-stencils that are defined on node-based grids. This framework fits the finite element and difference discretizations very well. It results in a unified symbol computation for the discrete operator and the associated grid-transfer operators in a two-grid method. Finally, some discrete operators arising from finite element and difference discretizations are presented to illustrate the simplicity of this generalized LFA and its use.},
  archive      = {J_SIMAX},
  author       = {Yunhui He},
  doi          = {10.1137/20M1355008},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1096-1118},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A generalized and unified framework of local fourier analysis using matrix-stencils},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and accurate gaussian kernel ridge regression using
matrix decompositions for preconditioning. <em>SIMAX</em>,
<em>42</em>(3), 1073–1095. (<a
href="https://doi.org/10.1137/20M1343993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a preconditioner-based method for solving a kernel ridge regression problem. In contrast to other methods, which utilize either fast matrix-vector multiplication or a preconditioner, the suggested approach uses randomized matrix decompositions for building a preconditioner with a special structure that can also utilize fast matrix-vector multiplications. This hybrid approach is efficient in reducing the condition number, exact, and computationally efficient, enabling the processing of large datasets with computational complexity linear to the number of data points. Also, a theoretical upper bound for the condition number is provided. For Gaussian kernels, we show that given a desired condition number, the rank of the needed preconditioner can be determined directly from the dataset.},
  archive      = {J_SIMAX},
  author       = {Gil Shabat and Era Choshen and Dvir Ben Or and Nadav Carmel},
  doi          = {10.1137/20M1343993},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1073-1095},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast and accurate gaussian kernel ridge regression using matrix decompositions for preconditioning},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient algorithms for eigensystem realization using
randomized SVD. <em>SIMAX</em>, <em>42</em>(2), 1045–1072. (<a
href="https://doi.org/10.1137/20M1327616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The eigensystem realization algorithm (ERA) is a data-driven approach for subspace system identification and is widely used in many areas of engineering. However, the computational cost of the ERA is dominated by a step that involves the singular value decomposition (SVD) of a large, dense matrix with block Hankel structure. This paper develops computationally efficient algorithms for reducing the computational cost of the SVD step by using randomized subspace iteration and exploiting the block Hankel structure of the matrix. We provide a detailed analysis of the error in the identified system matrices and the computational cost of the proposed algorithms. We demonstrate the accuracy and computational benefits of our algorithms on two test problems: the first involves a partial differential equation that models the cooling of steel rails, and the second is an application from power systems engineering.},
  archive      = {J_SIMAX},
  author       = {Rachel Minster and Arvind K. Saibaba and Jishnudeep Kar and Aranya Chakrabortty},
  doi          = {10.1137/20M1327616},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {1045-1072},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient algorithms for eigensystem realization using randomized SVD},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A linear relation approach to port-hamiltonian
differential-algebraic equations. <em>SIMAX</em>, <em>42</em>(2),
1011–1044. (<a href="https://doi.org/10.1137/20M1371166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider linear port-Hamiltonian differential-algebraic equations. Inspired by the geometric approach of van der Schaft and Maschke [System Control Lett., 121 (2018), pp. 31--37] and the linear algebraic approach of Mehl, Mehrmann, and Wojtylak [SIAM J. Matrix Anal. Appl., 39 (2018), pp. 1489--1519], we present another view by using the theory of linear relations. We show that this allows us to elaborate the differences and mutualities of the geometric and linear algebraic views, and we introduce a class of DAEs which comprises these two approaches. We further study the properties of matrix pencils arising from our approach via linear relations.},
  archive      = {J_SIMAX},
  author       = {Hannes Gernandt and Frédéric Enrico Haller and Timo Reis},
  doi          = {10.1137/20M1371166},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {1011-1044},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A linear relation approach to port-hamiltonian differential-algebraic equations},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Block low-rank matrices with shared bases: Potential and
limitations of the BLR<span class="math inline"><sup>2</sup></span>
format. <em>SIMAX</em>, <em>42</em>(2), 990–1010. (<a
href="https://doi.org/10.1137/20M1386451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a special class of data sparse rank-structured matrices that combine a flat block low-rank (BLR) partitioning with the use of shared (called nested in the hierarchical case) bases. This format is to $\mathcal{H}^2$ matrices what BLR is to $\mathcal{H}$ matrices: we therefore call it the BLR$^2$ matrix format. We present algorithms for the construction and LU factorization of BLR$^2$ matrices, and perform their cost analysis---both asymptotically and for a fixed problem size. With weak admissibility, BLR$^2$ matrices reduce to block separable matrices (the flat version of HBS/HSS). Our analysis and numerical experiments reveal some limitations of BLR$^2$ matrices with weak admissibility, which we propose to overcome with two approaches: strong admissibility, and the use of multiple shared bases per row and column.},
  archive      = {J_SIMAX},
  author       = {Cleve Ashcraft and Alfredo Buttari and Theo Mary},
  doi          = {10.1137/20M1386451},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {990-1010},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Block low-rank matrices with shared bases: Potential and limitations of the BLR$^2$ format},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On adaptive sketch-and-project for solving linear systems.
<em>SIMAX</em>, <em>42</em>(2), 954–989. (<a
href="https://doi.org/10.1137/19M1285846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We generalize the concept of adaptive sampling rules to the sketch-and-project method for solving linear systems. Analyzing adaptive sampling rules in the sketch-and-project setting yields convergence results that apply to all special cases at once, including the Kaczmarz and coordinate descent. This eliminates the need to separately analyze analogous adaptive sampling rules in each special case. To deduce new sampling rules, we show how the progress of one step of the sketch-and-project method depends directly on a sketched residual. Based on this insight, we derive a (1) max-distance sampling rule, by sampling the sketch with the largest sketched residual, (2) a proportional sampling rule, by sampling proportional to the sketched residual, and finally (3) a capped sampling rule. The capped sampling rule is a generalization of the recently introduced adaptive sampling rules for the Kaczmarz method [Z.-Z. Bai and W.-T. Wu, SIAM J. Sci. Comput., 40 (2018), pp. A592--A606]. We provide a global exponential convergence theorem for each sampling rule and show that the max-distance sampling rule enjoys the fastest convergence. This finding is also verified in extensive numerical experiments that lead us to conclude that the max-distance sampling rule is superior both experimentally and theoretically to the capped sampling rule. We also provide numerical insights into implementing the adaptive strategies so that the per iteration cost is of the same order as using a fixed sampling strategy when the product of the number of sketches with the sketch size is not significantly larger than the number of columns.},
  archive      = {J_SIMAX},
  author       = {Robert M. Gower and Denali Molitor and Jacob Moorman and Deanna Needell},
  doi          = {10.1137/19M1285846},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {954-989},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On adaptive sketch-and-project for solving linear systems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Systems of polynomial equations, higher-order tensor
decompositions, and multidimensional harmonic retrieval: A unifying
framework. Part II: The block term decomposition. <em>SIMAX</em>,
<em>42</em>(2), 913–953. (<a
href="https://doi.org/10.1137/17M1150062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Part I we proposed a multilinear algebra framework to solve 0-dimensional systems of polynomial equations with simple roots. We extend this framework to incorporate multiple roots: a block term decomposition (BTD) of the null space of the Macaulay matrix reveals the dual (sub)space of a disjoint root in each term. The BTD is the joint triangularization of multiplication tables and a three-way generalization of the Jordan canonical form in the matrix case, intimately related to the border rank of a tensor. We hint at and illustrate flexible numerical optimization-based algorithms.},
  archive      = {J_SIMAX},
  author       = {Jeroen Vanderstukken and Patrick Kürschner and Ignat Domanov and Lieven De Lathauwer},
  doi          = {10.1137/17M1150062},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {913-953},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Systems of polynomial equations, higher-order tensor decompositions, and multidimensional harmonic retrieval: a unifying framework. part II: the block term decomposition},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Systems of polynomial equations, higher-order tensor
decompositions, and multidimensional harmonic retrieval: A unifying
framework. Part i: The canonical polyadic decomposition. <em>SIMAX</em>,
<em>42</em>(2), 883–912. (<a
href="https://doi.org/10.1137/17M1150050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multilinear algebra framework to solve systems of polynomial equations with simple roots. We translate connections between univariate polynomial root-finding, eigenvalue decompositions, and harmonic retrieval to their higher-order counterparts: a canonical polyadic decomposition (CPD) that exploits shift invariance structures in the null space of the Macaulay matrix reveals the roots of the polynomial system. The new framework allows us to use numerical CPD algorithms for solving systems of polynomial equations. For the same degree of the Macaulay matrix as in numerical polynomial algebra/polynomial numerical linear algebra, the CPD is interpreted as the joint eigenvalue decomposition of the multiplication tables. In our approach the degree can also be lower. Affine roots and roots at infinity can be handled in the same way. With minor modifications, the technique can be used to estimate approximate roots of overconstrained systems.},
  archive      = {J_SIMAX},
  author       = {Jeroen Vanderstukken and Lieven De Lathauwer},
  doi          = {10.1137/17M1150050},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {883-912},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Systems of polynomial equations, higher-order tensor decompositions, and multidimensional harmonic retrieval: a unifying framework. part i: the canonical polyadic decomposition},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthogonal trace-sum maximization: Applications, local
algorithms, and global optimality. <em>SIMAX</em>, <em>42</em>(2),
859–882. (<a href="https://doi.org/10.1137/20M1363388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of maximizing the sum of traces of matrix quadratic forms on a product of Stiefel manifolds. This orthogonal trace-sum maximization (OTSM) problem generalizes many interesting problems such as generalized canonical correlation analysis (CCA), Procrustes analysis, and cryo-electron microscopy of the Nobel prize fame. For these applications finding global solutions is highly desirable, but it has been unclear how to find even a stationary point, let alone test its global optimality. Through a close inspection of Ky Fan&#39;s classical result [Proc. Natl. Acad. Sci. USA, 35 (1949), pp. 652--655] on the variational formulation of the sum of largest eigenvalues of a symmetric matrix, and a semidefinite programming (SDP) relaxation of the latter, we first provide a simple method to certify global optimality of a given stationary point of OTSM. This method only requires testing whether a symmetric matrix is positive semidefinite. A by-product of this analysis is an unexpected strong duality between Shapiro and Botha [SIAM J. Matrix Anal. Appl., 9 (1988), pp. 378--383] and Zhang and Singer [Linear Algebra Appl., 524 (2017), pp. 159--181]. After showing that a popular algorithm for generalized CCA and Procrustes analysis may generate oscillating iterates, we propose a simple fix that provably guarantees convergence to a stationary point. The combination of our algorithm and certificate reveals novel global optima of various instances of OTSM.},
  archive      = {J_SIMAX},
  author       = {Joong-Ho Won and Hua Zhou and Kenneth Lange},
  doi          = {10.1137/20M1363388},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {859-882},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Orthogonal trace-sum maximization: Applications, local algorithms, and global optimality},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental CP tensor decomposition by alternating
minimization method. <em>SIMAX</em>, <em>42</em>(2), 832–858. (<a
href="https://doi.org/10.1137/20M1319097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications, incremental tensors are very common: only a portion of tensor data is available, and new data are arriving in the next time step or continuously over time. To handle this type of tensors time-saving algorithms are required for online computation. In this paper, we consider incremental CP (CANDECOMP/PARAFAC) decomposition, which requires one to update the CP decomposition after new tensor data, together with the exiting tensor, are ready for analysis. There exist several incremental CP decomposition algorithms, but almost all of these algorithms assume that the number of CP decomposition components remains fixed in the incremental process. The main contribution of this paper is the study of how to add components in the incremental CP decomposition. We derive the coordinate representation of the incremental CP decomposition with respect to a special basis and show related properties of tensor rank and the uniqueness of such incremental CP decomposition. Under the framework of this representation, the proposed method can be solved by using an alternating minimization algorithm. Numerical examples are presented to show the good performance of the proposed algorithms in terms of computational time and data fitting compared with existing methods.},
  archive      = {J_SIMAX},
  author       = {Chao Zeng and Michael K. Ng},
  doi          = {10.1137/20M1319097},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {832-858},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Incremental CP tensor decomposition by alternating minimization method},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An implicit representation and iterative solution of
randomly sketched linear systems. <em>SIMAX</em>, <em>42</em>(2),
800–831. (<a href="https://doi.org/10.1137/19M1259481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized linear system solvers have become popular as they have the potential to reduce floating point complexity while still achieving desirable convergence rates. One particularly promising class of methods, random sketching solvers, has achieved the best known computational complexity bounds in theory, but is blunted by two practical considerations: there is no clear way of choosing the size of the sketching matrix a priori; and there is a nontrivial storage cost of the sketched system. In this work, we make progress towards addressing these issues by implicitly generating the sketched system and solving it simultaneously through an iterative procedure. As a result, we replace the question of the size of the sketching matrix with determining appropriate stopping criteria; we also avoid the costs of explicitly representing the sketched linear system; and our implicit representation also solves the system at the same time, which controls the per-iteration computational costs. Additionally, our approach allows us to generate a connection between random sketching methods and randomized iterative solvers (e.g., the randomized Kaczmarz method and randomized Gauss--Seidel). As a consequence, we exploit this connection to (1) produce a stronger, more precise convergence theory for such randomized iterative solvers under arbitrary sampling schemes (i.i.d., adaptive, permutation, dependent, etc.), and (2) improve the rates of convergence of randomized iterative solvers at the expense of a user-determined increase in per-iteration computational and storage costs. We demonstrate these concepts on numerical examples on 49 distinct linear systems.},
  archive      = {J_SIMAX},
  author       = {Vivak Patel and Mohammad Jahangoshahi and Daniel A. Maldonado},
  doi          = {10.1137/19M1259481},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {800-831},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An implicit representation and iterative solution of randomly sketched linear systems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinearizing two-parameter eigenvalue problems.
<em>SIMAX</em>, <em>42</em>(2), 775–799. (<a
href="https://doi.org/10.1137/19M1274316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a technique to transform a linear two-parameter eigenvalue problem into a nonlinear eigenvalue problem (NEP). The transformation stems from an elimination of one of the equations in the two-parameter eigenvalue problem, by considering it as a (standard) generalized eigenvalue problem. We characterize the equivalence between the original and the nonlinearized problem theoretically and show how to use the transformation computationally. Special cases of the transformation can be interpreted as a reversed companion linearization for polynomial eigenvalue problems, as well as a reversed (less known) linearization technique for certain algebraic eigenvalue problems with square-root terms. Moreover, by exploiting the structure of the NEP we present algorithm specializations for NEP methods, although the technique also allows general solution methods for NEPs to be directly applied. The nonlinearization is illustrated in examples and simulations, with focus on problems where the eliminated equation is of much smaller size than the other two-parameter eigenvalue equation. This situation arises naturally in domain decomposition techniques. A general error analysis is also carried out under the assumption that a backward stable eigensolver is used to solve the eliminated problem, leading to the conclusion that the error is benign in this situation.},
  archive      = {J_SIMAX},
  author       = {Emil Ringh and Elias Jarlebring},
  doi          = {10.1137/19M1274316},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {775-799},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Nonlinearizing two-parameter eigenvalue problems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multishift, multipole rational QZ method with aggressive
early deflation. <em>SIMAX</em>, <em>42</em>(2), 753–774. (<a
href="https://doi.org/10.1137/19M1249631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the article “A Rational QZ Method” by D. Camps, K. Meerbergen, and R. Vandebril [SIAM J. Matrix Anal. Appl., 40 (2019), pp. 943--972], we introduced rational QZ (RQZ) methods. Our theoretical examinations revealed that the convergence of the RQZ method is governed by rational subspace iteration, thereby generalizing the classical QZ method, whose convergence relies on polynomial subspace iteration. Moreover the RQZ method operates on a pencil more general than Hessenberg---upper triangular, namely, a Hessenberg pencil, which is a pencil consisting of two Hessenberg matrices. However, the RQZ method can only be made competitive to advanced QZ implementations by using crucial add-ons such as small bulge multishift sweeps, aggressive early deflation, and optimal packing. In this paper we develop these techniques for the RQZ method. In the numerical experiments we compare the results with state-of-the-art routines for the generalized eigenvalue problem and show that the presented method is competitive in terms of speed and accuracy.},
  archive      = {J_SIMAX},
  author       = {Thijs Steel and Daan Camps and Karl Meerbergen and Raf Vandebril},
  doi          = {10.1137/19M1249631},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {753-774},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A multishift, multipole rational QZ method with aggressive early deflation},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiplicative updates for NMF with <span
class="math inline"><em>β</em></span>-divergences under disjoint
equality constraints. <em>SIMAX</em>, <em>42</em>(2), 730–752. (<a
href="https://doi.org/10.1137/20M1377278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is the problem of approximating an input nonnegative matrix, $V$, as the product of two smaller nonnegative matrices, $W$ and $H$. In this paper, we introduce a general framework to design multiplicative updates (MU) for NMF based on $\beta$-divergences ($\beta$-NMF) with disjoint equality constraints, and with penalty terms in the objective function. By disjoint, we mean that each variable appears in at most one equality constraint. Our MU satisfy the set of constraints after each update of the variables during the optimization process, while guaranteeing that the objective function decreases monotonically. We showcase this framework on three NMF models, and show that it competes favorably with the state of the art: (1) $\beta$-NMF with sum-to-one constraints on the columns of $H$, (2) minimum-volume $\beta$-NMF with sum-to-one constraints on the columns of $W$, and (3) sparse $\beta$-NMF with $\ell_2$-norm constraints on the columns of $W$.},
  archive      = {J_SIMAX},
  author       = {Valentin Leplat and Nicolas Gillis and Jérôme Idier},
  doi          = {10.1137/20M1377278},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {730-752},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Multiplicative updates for NMF with $\beta$-divergences under disjoint equality constraints},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A convex relaxation to compute the nearest structured rank
deficient matrix. <em>SIMAX</em>, <em>42</em>(2), 708–729. (<a
href="https://doi.org/10.1137/19M1257640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an affine space of matrices $\mathcal{L}$ and a matrix $\Theta\in \mathcal{L}$, consider the problem of computing the closest rank deficient matrix to $\Theta$ on $\mathcal{L}$ with respect to the Frobenius norm. This is a nonconvex problem with several applications in control theory, computer algebra, and computer vision. We introduce a novel semidefinite programming (SDP) relaxation and prove that it always gives the global minimizer of the nonconvex problem in the low noise regime, i.e., when $\Theta$ is close to be rank deficient. Our SDP is the first convex relaxation for this problem with provable guarantees. We evaluate the performance of our SDP relaxation in examples from system identification, approximate greatest common divisor, triangulation, and camera resectioning. Our relaxation reliably obtains the global minimizer under nonadversarial noise, and its noise tolerance is significantly better than state of the art methods.},
  archive      = {J_SIMAX},
  author       = {Diego Cifuentes},
  doi          = {10.1137/19M1257640},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {708-729},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A convex relaxation to compute the nearest structured rank deficient matrix},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient construction of an HSS preconditioner for
symmetric positive definite <span
class="math inline">ℋ<sup>2</sup></span> matrices. <em>SIMAX</em>,
<em>42</em>(2), 683–707. (<a
href="https://doi.org/10.1137/20M1365776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an iterative approach for solving linear systems with dense, ill-conditioned, symmetric positive definite (SPD) kernel matrices, both fast matrix-vector products and fast preconditioning operations are required. Fast (linear-scaling) matrix-vector products are available by expressing the kernel matrix in an $\mathcal{H}^2$ representation or an equivalent fast multipole method representation. This paper is concerned with preconditioning such matrices using the hierarchically semiseparable (HSS) matrix representation. Previously, an algorithm was presented to construct an HSS approximation to an SPD kernel matrix that is guaranteed to be SPD. However, this algorithm has quadratic cost and was only designed for recursive binary partitionings of the points defining the kernel matrix. This paper presents a general algorithm for constructing an SPD HSS approximation. Importantly, the algorithm uses the $\mathcal{H}^2$ representation of the SPD matrix to reduce its computational complexity from quadratic to quasilinear. Numerical experiments illustrate how this SPD HSS approximation performs as a preconditioner for solving linear systems arising from a range of kernel functions.},
  archive      = {J_SIMAX},
  author       = {Xin Xing and Hua Huang and Edmond Chow},
  doi          = {10.1137/20M1365776},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {683-707},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient construction of an HSS preconditioner for symmetric positive definite $\mathcal{H}^2$ matrices},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A power schur complement low-rank correction preconditioner
for general sparse linear systems. <em>SIMAX</em>, <em>42</em>(2),
659–682. (<a href="https://doi.org/10.1137/20M1316445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A parallel preconditioner is proposed for general large sparse linear systems that combines a power series expansion method with low-rank correction techniques. To enhance convergence, a power series expansion is added to a basic Schur complement iterative scheme by exploiting a standard matrix splitting of the Schur complement. One of the goals of the power series approach is to improve the eigenvalue separation of the preconditioner thus allowing an effective application of a low-rank correction technique. Experiments indicate that this combination can be quite robust when solving highly indefinite linear systems. The preconditioner exploits a domain-decomposition approach, and its construction starts with the use of a graph partitioner to reorder the original coefficient matrix. In this framework, unknowns corresponding to interface variables are obtained by solving a linear system whose coefficient matrix is the Schur complement. Unknowns associated with the interior variables are obtained by solving a block diagonal linear system where parallelism can be easily exploited. Numerical examples are provided to illustrate the effectiveness of the proposed preconditioner, with an emphasis on highlighting its robustness properties in the indefinite case.},
  archive      = {J_SIMAX},
  author       = {Qingqing Zheng and Yuanzhe Xi and Yousef Saad},
  doi          = {10.1137/20M1316445},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {659-682},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A power schur complement low-rank correction preconditioner for general sparse linear systems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel prony’s method with multivariate matrix pencil
approach and its numerical aspects. <em>SIMAX</em>, <em>42</em>(2),
635–658. (<a href="https://doi.org/10.1137/20M1343658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prony&#39;s method is a standard tool exploited for solving many imaging and data analysis problems that result in parameter identification in sparse exponential sums $f(k)=\sum_{j=1}^{M}c_{j}e^{-2\pi i\langle t_{j},k\rangle}$, $k\in \mathbb{Z}^{d}$, where the parameters are pairwise different ${ t_{j}}_{j=1}^{M}\subset [0,1)^{d}$, and ${ c_{j}}_{j=1}^{M}\subset \mathbb{C}\setminus { 0}$ are nonzero. The focus of our investigation is on a Prony&#39;s method variant based on a multivariate matrix pencil approach. The method constructs matrices $S_{1}$, łdots , $S_{d}$ from the sampling values, and their simultaneous diagonalization yields the parameters ${ t_{j}}_{j=1}^{M}$. The parameters ${ c_{j}}_{j=1}^{M}$ are computed as the solution of an linear least squares problem, where the matrix of the problem is determined by ${ t_{j}}_{j=1}^{M}$. Since the method involves independent generation and manipulation of a certain number of matrices, there is an intrinsic capacity for parallelization of the whole computational process on several levels. Hence, we propose a parallel version of the Prony&#39;s method in order to increase its efficiency. The tasks concerning the generation of matrices are divided among the block of threads of the graphics processing unit (GPU) and the central processing unit (CPU), where heavier load is put on the GPU. From the algorithmic point of view, the CPU is dedicated to the more complex tasks of computing the singular value decomposition, the eigendecomposition, and the solution of the least squares problem, while the GPU is performing matrix--matrix multiplications and summations. With careful choice of algorithms solving the subtasks, the load between CPU and GPU is balanced. Besides the parallelization techniques, we are also concerned with some numerical issues, and we provide detailed numerical analysis of the method in case of noisy input data. Finally, we performed a set of numerical tests which confirm superior efficiency of the parallel algorithm and consistency of the forward error with the results of numerical analysis.},
  archive      = {J_SIMAX},
  author       = {Nela Bosner},
  doi          = {10.1137/20M1343658},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {635-658},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Parallel prony&#39;s method with multivariate matrix pencil approach and its numerical aspects},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure preserving quaternion generalized minimal residual
method. <em>SIMAX</em>, <em>42</em>(2), 616–634. (<a
href="https://doi.org/10.1137/20M133751X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of this paper is to develop the quaternion generalized minimal residual method (QGMRES) for solving quaternion linear systems. Quaternion linear systems arise from three-dimensional or color imaging filtering problems. The proposed quaternion Arnoldi procedure can preserve quaternion Hessenberg form during the iterations. The main advantage is that the storage of the proposed iterative method can be reduced by comparing with the Hessenberg form constructed by the classical GMRES iterations for the real representation of quaternion linear systems. The convergence of the proposed QGMRES is also established. Numerical examples are presented to demonstrate the effectiveness of the proposed QGMRES compared with the traditional GMRES in terms of storage and computing time.},
  archive      = {J_SIMAX},
  author       = {ZhiGang Jia and Michael K. Ng},
  doi          = {10.1137/20M133751X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {616-634},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structure preserving quaternion generalized minimal residual method},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomized kaczmarz converges along small singular vectors.
<em>SIMAX</em>, <em>42</em>(2), 608–615. (<a
href="https://doi.org/10.1137/20M1350947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized Kaczmarz is a simple iterative method for finding solutions of linear systems $Ax = b$. We point out that the arising sequence $(x_k)_{k=1}^{\infty}$ tends to converge to the solution $x$ in an interesting way: generically, as $k \rightarrow \infty$, $x_k - x$ tends to the singular vector of $A$ corresponding to the smallest singular value. This has interesting consequences: in particular, the error analysis of Strohmer and Vershynin is optimal. It also quantifies the “preconvergence” phenomenon where the method initially seems to converge faster. This fact also allows for a fast computation of vectors $x$ for which the Rayleigh quotient $\|Ax\|/\|x\|$ is small: solve $Ax = 0$ via randomized Kaczmarz.},
  archive      = {J_SIMAX},
  author       = {Stefan Steinerberger},
  doi          = {10.1137/20M1350947},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {608-615},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Randomized kaczmarz converges along small singular vectors},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast interpolation-based globality certificates for
computing kreiss constants and the distance to uncontrollability.
<em>SIMAX</em>, <em>42</em>(2), 578–607. (<a
href="https://doi.org/10.1137/20M1358955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach to computing global minimizers of singular value functions in two real variables. Specifically, we present new algorithms to compute the Kreiss constant of a matrix and the distance to uncontrollability of a linear control system, both to arbitrary accuracy. Previous state-of-the-art methods for these two quantities rely on 2D level-set tests that are based on solving large eigenvalue problems. Consequently, these methods are costly, i.e., $\mathcal{O}(n^6)$ work using dense eigensolvers, and often multiple tests are needed before convergence. Divide-and-conquer techniques have been proposed that reduce the work complexity to $\mathcal{O}(n^4)$ on average and $\mathcal{O}(n^5)$ in the worst case, but these variants are nevertheless still very expensive and can be numerically unreliable. In contrast, our new interpolation-based globality certificates perform level-set tests by building interpolant approximations to certain one-variable continuous functions that are both relatively cheap and numerically robust to evaluate. Our new approach has an $\mathcal{O}(kn^3)$ work complexity and uses $\mathcal{O}(n^2)$ memory, where $k$ is the number of function evaluations necessary to build the interpolants. Not only is this interpolation process mostly “embarrassingly parallel,&quot; but also low-fidelity approximations typically suffice for all but the final interpolant, which must be built to high accuracy. Even without taking advantage of the aforementioned parallelism, $k$ is sufficiently small that our new approach is generally orders of magnitude faster than the previous state of the art.},
  archive      = {J_SIMAX},
  author       = {Tim Mitchell},
  doi          = {10.1137/20M1358955},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {578-607},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast interpolation-based globality certificates for computing kreiss constants and the distance to uncontrollability},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured inversion of the bernstein–vandermonde matrix.
<em>SIMAX</em>, <em>42</em>(2), 557–577. (<a
href="https://doi.org/10.1137/20M1336606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bernstein polynomials, long a staple of approximation theory and computational geometry, have also increasingly become of interest in finite element methods. Many fundamental problems in interpolation and approximation give rise to interesting linear algebra questions. When attempting to find a polynomial approximation of boundary or initial data, one encounters the Bernstein--Vandermonde matrix, which is found to be highly ill conditioned. In [L. Allen and R. C. Kirby, SIAM J. Matrix Anal. Appl., 41 (2020), pp. 413--431], we used the relationship between monomial Bézout matrices and the inverse of Hankel matrices to obtain a decomposition of the inverse of the Bernstein mass matrix in terms of Hankel, Toeplitz, and diagonal matrices. In this paper, we use properties of the Bernstein--Bézout matrix to factor the inverse of the Bernstein--Vandermonde matrix into a difference of products of Hankel, Toeplitz, and diagonal matrices. We also use the nonstandard matrix norm defined in [L. Allen and R. C. Kirby, SIAM J. Matrix Anal. Appl., 41 (2020), pp. 413--431] to study the conditioning of the Bernstein--Vandermonde matrix, showing that the conditioning in this case is better than in the standard 2 norm. Additionally, we use properties of multivariate Bernstein polynomials to derive a block $LU$ decomposition of the Bernstein--Vandermonde matrix corresponding to equispaced nodes on the $d$-simplex.},
  archive      = {J_SIMAX},
  author       = {Larry Allen and Robert C. Kirby},
  doi          = {10.1137/20M1336606},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {557-577},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structured inversion of the bernstein--vandermonde matrix},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geodesically parameterized covariance estimation.
<em>SIMAX</em>, <em>42</em>(2), 528–556. (<a
href="https://doi.org/10.1137/19M1284646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modeling of spatiotemporal phenomena often requires selecting a covariance matrix from a covariance class. Yet standard parametric covariance families can be insufficiently flexible for practical applications, while nonparametric approaches may not easily allow certain kinds of prior knowledge to be incorporated. We propose instead to build covariance families out of geodesic curves. These covariances offer more flexibility for problem-specific tailoring than classical parametric families and are preferable to simple convex combinations. Once the covariance family has been chosen, one typically needs to select a representative member by solving an optimization problem, e.g., by maximizing the likelihood of a data set. We consider instead a differential geometric interpretation of this problem: minimizing the geodesic distance to a sample covariance matrix (``natural projection&#39;&#39;). Our approach is consistent with the notion of distance employed to build the covariance family and does not require assuming a particular probability distribution for the data. We show that natural projection and maximum likelihood estimation within the covariance family are locally equivalent up to second order. We also demonstrate that natural projection may yield more accurate estimates with noise-corrupted data.},
  archive      = {J_SIMAX},
  author       = {Antoni Musolas and Steven T. Smith and Youssef Marzouk},
  doi          = {10.1137/19M1284646},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {528-556},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Geodesically parameterized covariance estimation},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Border rank nonadditivity for higher order tensors.
<em>SIMAX</em>, <em>42</em>(2), 503–527. (<a
href="https://doi.org/10.1137/20M1357366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whereas matrix rank is additive under direct sum, in 1981 Schönhage showed that one of its generalizations to the tensor setting, tensor border rank, can be strictly subadditive for tensors of order three. Whether border rank is additive for higher order tensors has remained open. In this work, we settle this problem by providing analogues of Schönhage&#39;s construction for tensors of order four and higher. Schönhage&#39;s work was motivated by the study of the computational complexity of matrix multiplication; we discuss implications of our results for the asymptotic rank of higher order generalizations of the matrix multiplication tensor.},
  archive      = {J_SIMAX},
  author       = {M. Christandl and F. Gesmundo and M. Michałek and J. Zuiddam},
  doi          = {10.1137/20M1357366},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {503-527},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Border rank nonadditivity for higher order tensors},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal complex relaxation parameters in multigrid for
complex-shifted linear systems. <em>SIMAX</em>, <em>42</em>(2), 475–502.
(<a href="https://doi.org/10.1137/20M1342161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive optimal complex relaxation parameters minimizing smoothing factors associated with multigrid using red-black successive overrelaxation or damped Jacobi smoothing applied to a class of linear systems arising from discretized linear partial differential equations with a complex shift. Our analysis yields analytical formulas for smoothing factors as a function of the complex relaxation parameter, which may then be efficiently numerically minimized. Our results are applicable to second-order discretizations in arbitrary dimensions, and generalize earlier work of Irad Yavneh on optimal relaxation parameters in the real case. Our analysis is based on deriving a novel connection between the performance of successive overrelaxation as a smoother and as a solver, and is validated by numerical experiments on problems in two and three spatial dimensions, using both vertex- and cell-centered multigrid, with both constant and variable coefficients. In the variable coefficient case we assign different relaxation parameters to different grids points, which our framework allows us to do efficiently.},
  archive      = {J_SIMAX},
  author       = {L. Robert Hocking and Chen Greif},
  doi          = {10.1137/20M1342161},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {475-502},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Optimal complex relaxation parameters in multigrid for complex-shifted linear systems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From computation to comparison of tensor decompositions.
<em>SIMAX</em>, <em>42</em>(2), 449–474. (<a
href="https://doi.org/10.1137/20M1349370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decompositions of higher-order tensors into sums of simple terms are ubiquitous. We show that in order to verify that two tensors are generated by the same (possibly scaled) terms it is not necessary to compute the individual decompositions. In general the explicit computation of such a decomposition may have high complexity and can be ill-conditioned. We now show that under some assumptions the verification can be reduced to a comparison of both the column and row spaces of the corresponding matrix representations of the tensors. We consider rank-1 terms as well as low multilinear rank terms (also known as block terms) and show that the number of the terms and their multilinear rank can be inferred as well. The comparison relies only on numerical linear algebra and can be done in a numerically reliable way. We also illustrate how our results can be applied to solve a multilabel classification problem that appears in the context of blind source separation.},
  archive      = {J_SIMAX},
  author       = {Ignat Domanov and Lieven De Lathauwer},
  doi          = {10.1137/20M1349370},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {449-474},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {From computation to comparison of tensor decompositions},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stability of linear GMRES convergence with respect to
compact perturbations. <em>SIMAX</em>, <em>42</em>(1), 436–447. (<a
href="https://doi.org/10.1137/20M1340848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose that a linear bounded operator $B$ on a Hilbert space exhibits at least linear GMRES convergence, i.e., there exists $M_B&lt;1$ such that the GMRES residuals fulfill $\|r_k\|\leq M_B\|r_{k-1}\|$ for every initial residual $r_0$ and step $k\in\mathbb{N}$. We prove that GMRES with a compactly perturbed operator $A=B+C$ admits the bound $\|r_k\|/\|r_0\|\leq\prod_{j=1}^k\bigl(M_B+(1+M_B)\,\|A^{-1}\|\,\sigma_j(C)\bigr)$, i.e., the singular values $\sigma_j(C)$ control the departure from the bound for the unperturbed problem. This result can be seen as an extension of [I. Moret, SIAM J. Numer. Anal., 34 (1997), pp. 513--516], where only the case $B=\lambda I$ is considered. In this special case $M_B=0$ convergence is superlinear.},
  archive      = {J_SIMAX},
  author       = {Jan Blechta},
  doi          = {10.1137/20M1340848},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {436-447},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Stability of linear GMRES convergence with respect to compact perturbations},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrices with tunable infinity-norm condition number and no
need for pivoting in LU factorization. <em>SIMAX</em>, <em>42</em>(1),
417–435. (<a href="https://doi.org/10.1137/20M1357238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a two-parameter family of nonsymmetric dense $n\times n$ matrices $A(\alpha,\beta)$ for which LU factorization without pivoting is numerically stable, and we show how to choose $\alpha$ and $\beta$ to achieve any value of the $\infty$-norm condition number. The matrix $A(\alpha,\beta)$ can be formed from a simple formula in $O(n^2)$ flops. The matrix is suitable for use in the HPL-AI Mixed-Precision Benchmark, which requires an extreme scale test matrix (dimension $n&gt;10^7$) that has a controlled condition number and can be safely used in LU factorization without pivoting. It is also of interest as a general-purpose test matrix.},
  archive      = {J_SIMAX},
  author       = {Massimiliano Fasi and Nicholas J. Higham},
  doi          = {10.1137/20M1357238},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {417-435},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Matrices with tunable infinity-norm condition number and no need for pivoting in LU factorization},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lower memory oblivious (tensor) subspace embeddings with
fewer random bits: Modewise methods for least squares. <em>SIMAX</em>,
<em>42</em>(1), 376–416. (<a
href="https://doi.org/10.1137/19M1308116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper new general modewise Johnson--Lindenstrauss (JL) subspace embeddings are proposed that can be both generated much faster and stored more easily than traditional JL embeddings when working with extremely large vectors and/or tensors. Corresponding embedding results are then proven for two different types of low-dimensional (tensor) subspaces. The first of these new subspace embedding results produces improved space complexity bounds for embeddings of rank-$r$ tensors whose CP decompositions are contained in the span of a fixed (but unknown) set of $r$ rank-$1$ basis tensors. In the traditional vector setting this first result yields new and very general near-optimal oblivious subspace embedding constructions that require fewer random bits to generate than standard JL embeddings when embedding subspaces of $\mathbb{C}^N$ spanned by basis vectors with special Kronecker structure. The second result proven herein provides new fast JL embeddings of arbitrary $r$-dimensional subspaces $\mathcal{S} \subset \mathbb{C}^N$ which also require fewer random bits (and so are easier to store, i.e., require less space) than standard fast JL embedding methods in order to achieve small $\epsilon$-distortions. These new oblivious subspace embedding results work by (i) effectively folding any given vector in $\mathcal{S}$ into a (not necessarily low-rank) tensor, and then (ii) embedding the resulting tensor into $\mathbb{C}^m$ for $m \leq C r \log^c(N) / \epsilon^2$. Applications related to compression and fast compressed least squares solution methods are also considered, including those used for fitting low-rank CP decompositions, and the proposed JL embedding results are shown to work well numerically in both settings.},
  archive      = {J_SIMAX},
  author       = {Mark A. Iwen and Deanna Needell and Elizaveta Rebrova and Ali Zare},
  doi          = {10.1137/19M1308116},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {376-416},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Lower memory oblivious (Tensor) subspace embeddings with fewer random bits: Modewise methods for least squares},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perturbations of CUR decompositions. <em>SIMAX</em>,
<em>42</em>(1), 351–375. (<a
href="https://doi.org/10.1137/19M128394X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CUR decomposition is a factorization of a low-rank matrix obtained by selecting certain column and row submatrices of it. We perform a thorough investigation of what happens to such decompositions in the presence of noise. Since CUR decompositions are nonuniquely formed, we investigate several variants and give perturbation estimates for each in terms of the magnitude of the noise matrix in a broad class of norms which includes all Schatten $p$-norms. The estimates given here are qualitative and illustrate how the choice of columns and rows affects the quality of the approximation, and additionally we obtain new state-of-the-art bounds for some variants of CUR approximations.},
  archive      = {J_SIMAX},
  author       = {Keaton Hamm and Longxiu Huang},
  doi          = {10.1137/19M128394X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {351-375},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Perturbations of CUR decompositions},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Best low-rank approximations and kolmogorov <span
class="math inline"><em>n</em></span>-widths. <em>SIMAX</em>,
<em>42</em>(1), 330–350. (<a
href="https://doi.org/10.1137/20M1355720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We relate the problem of best low-rank approximation in the spectral norm for a matrix $A$ to Kolmogorov $n$-widths and corresponding optimal spaces. We characterize all the optimal spaces for the image of the Euclidean unit ball under $A$, and we show that any orthonormal basis in an $n$-dimensional optimal space generates a best rank-$n$ approximation to $A$. We also present a simple and explicit construction to obtain a sequence of optimal $n$-dimensional spaces once an initial optimal space is known. This results in a variety of solutions to the best low-rank approximation problem and provides alternatives to the truncated singular value decomposition. This variety can be exploited to obtain best low-rank approximations with problem-oriented properties.},
  archive      = {J_SIMAX},
  author       = {Michael S. Floater and Carla Manni and Espen Sande and Hendrik Speleers},
  doi          = {10.1137/20M1355720},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {330-350},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Best low-rank approximations and kolmogorov $n$-widths},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Triple decomposition and tensor recovery of third order
tensors. <em>SIMAX</em>, <em>42</em>(1), 299–329. (<a
href="https://doi.org/10.1137/20M1323266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the Tucker decomposition, in this paper we introduce a new tensor decomposition for third order tensors, which decomposes a third order tensor to three third order factor tensors. Each factor tensor has two low dimensions. We call such a decomposition the triple decomposition, and the corresponding rank the triple rank. The triple rank of a third order tensor is not greater than the middle value of the Tucker rank. The number of parameters in the bilevel form of standard triple decomposition is less than the number of parameters of Tucker decomposition in substantial cases. The theoretical discovery is confirmed numerically. Numerical tests show that third order tensor data from practical applications such as internet traffic and image are of low triple ranks. A tensor recovery method based on low rank triple decomposition is proposed. Its convergence and convergence rate are established. Numerical experiments confirm the efficiency of this method.},
  archive      = {J_SIMAX},
  author       = {Liqun Qi and Yannan Chen and Mayank Bakshi and Xinzhen Zhang},
  doi          = {10.1137/20M1323266},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {299-329},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Triple decomposition and tensor recovery of third order tensors},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the compressibility of tensors. <em>SIMAX</em>,
<em>42</em>(1), 275–298. (<a
href="https://doi.org/10.1137/20M1316639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors are often compressed by expressing them in data-sparse tensor formats, where storage costs in such formats are less than those in the original structure. In this paper, we develop three methodologies that bound the compressibility of a tensor: (1) Algebraic structure, (2) smoothness, and (3) displacement structure. For each methodology, we derive bounds on storage costs in various low rank tensor formats that partially explain the abundance of compressible tensors in applied mathematics. For example, using displacement structure, we show that the solution tensor $\mathcal{X} \in \mathbb{C}^{n \times n \times n}$ of a discretized Poisson equation $-\nabla^2 u =1$ on $[-1,1]^3$ with zero Dirichlet conditions can be approximated to a relative accuracy of $0&lt;\epsilon&lt;1$ in the Frobenius norm by a tensor in tensor-train format with $\mathcal{O}(n (\log n)^2 (\log(1/\epsilon))^2)$ degrees of freedom. The constructive bound also allows us to design a spectral algorithm that solves this equation with $\mathcal{O}(n (\log n)^3 (\log(1/\epsilon))^3)$ complexity.},
  archive      = {J_SIMAX},
  author       = {Tianyi Shi and Alex Townsend},
  doi          = {10.1137/20M1316639},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {275-298},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On the compressibility of tensors},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence analysis of direct minimization and
self-consistent iterations. <em>SIMAX</em>, <em>42</em>(1), 243–274. (<a
href="https://doi.org/10.1137/20M1332864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the numerical solution of subspace optimization problems, consisting of minimizing a smooth functional over the set of orthogonal projectors of fixed rank. Such problems are encountered in particular in electronic structure calculation (Hartree--Fock and Kohn--Sham density functional theory models). We compare from a numerical analysis perspective two simple representatives, the damped self-consistent field (SCF) iterations and the gradient descent algorithm, of the two classes of methods competing in the field: SCF and direct minimization methods. We derive asymptotic rates of convergence for these algorithms and analyze their dependence on the spectral gap and other properties of the problem. Our theoretical results are complemented by numerical simulations on a variety of examples, from toy models with tunable parameters to realistic Kohn--Sham computations. We also provide an example of chaotic behavior of the simple SCF iterations for a nonquadratic functional.},
  archive      = {J_SIMAX},
  author       = {Éric Cancès and Gaspard Kemlin and Antoine Levitt},
  doi          = {10.1137/20M1332864},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {243-274},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Convergence analysis of direct minimization and self-consistent iterations},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locating conical degeneracies in the spectra of parametric
self-adjoint matrices. <em>SIMAX</em>, <em>42</em>(1), 224–242. (<a
href="https://doi.org/10.1137/20M134174X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simple iterative scheme is proposed for locating the parameter values for which a two-parameter family of real symmetric matrices has a double eigenvalue. The convergence is proved to be quadratic. An extension of the scheme to complex Hermitian matrices (with three parameters) and to the location of triple eigenvalues (five parameters for real symmetric matrices) is also described. Algorithm convergence is illustrated in several examples: a real symmetric family, a complex Hermitian family, a family of matrices with an “avoided crossing” (no convergence), and a five-parameter family of real symmetric matrices with a triple eigenvalue.},
  archive      = {J_SIMAX},
  author       = {Gregory Berkolaiko and Advait Parulekar},
  doi          = {10.1137/20M134174X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {224-242},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Locating conical degeneracies in the spectra of parametric self-adjoint matrices},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Norm and trace estimation with random rank-one vectors.
<em>SIMAX</em>, <em>42</em>(1), 202–223. (<a
href="https://doi.org/10.1137/20M1331718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A few matrix-vector multiplications with random vectors are often sufficient to obtain reasonably good estimates for the norm of a general matrix or the trace of a symmetric positive semi-definite matrix. Several such probabilistic estimators have been proposed and analyzed for standard Gaussian and Rademacher random vectors. In this work, we consider the use of rank-one random vectors, that is, Kronecker products of (smaller) Gaussian or Rademacher vectors. It is not only cheaper to sample such vectors but it can sometimes also be much cheaper to multiply a matrix with a rank-one vector instead of a general vector. In this work, theoretical and numerical evidence is given that the use of rank-one instead of unstructured random vectors still leads to good estimates. In particular, it is shown that our rank-one estimators multiplied with a modest constant constitute, with high probability, upper bounds of the quantity of interest. Partial results are provided for the case of lower bounds. The application of our techniques to condition number estimation for matrix functions is illustrated.},
  archive      = {J_SIMAX},
  author       = {Zvonimir Bujanovic and Daniel Kressner},
  doi          = {10.1137/20M1331718},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {202-223},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Norm and trace estimation with random rank-one vectors},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random matrices generating large growth in LU factorization
with pivoting. <em>SIMAX</em>, <em>42</em>(1), 185–201. (<a
href="https://doi.org/10.1137/20M1338149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We identify a class of random, dense, $n\times n$ matrices for which LU factorization with any form of pivoting produces a growth factor typically of size at least $n/(4 \log n)$ for large $n$. The condition number of the matrices can be arbitrarily chosen, and large growth also happens for the transpose. Previously, no matrices with all these properties were known. The matrices can be generated by the MATLAB function gallery(&#39;randsvd&#39;,...), and they are formed as the product of two random orthogonal matrices from the Haar distribution with a diagonal matrix having only one diagonal entry different from 1, which lies between 0 and 1 (the “one small singular value” case). Our explanation for the large growth uses the fact that the maximum absolute value of any element of a Haar distributed orthogonal matrix tends to be relatively small for large $n$. We verify the behavior numerically and find that for partial pivoting the actual growth is significantly larger than the lower bound and much larger than the growth observed for random matrices with elements from the uniform [0,1] or standard normal distributions. We show more generally that a rank-1 perturbation to an orthogonal matrix producing large growth for any form of pivoting also generates large growth under reasonable assumptions. Finally, we demonstrate that GMRES-based iterative refinement can provide stable solutions to $Ax = b$ when large growth occurs in low precision LU factors, even when standard iterative refinement cannot.},
  archive      = {J_SIMAX},
  author       = {Desmond J. Higham and Nicholas J. Higham and Srikara Pranesh},
  doi          = {10.1137/20M1338149},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {185-201},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Random matrices generating large growth in LU factorization with pivoting},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast approximation of the gauss–newton hessian matrix for
the multilayer perceptron. <em>SIMAX</em>, <em>42</em>(1), 165–184. (<a
href="https://doi.org/10.1137/19M129961X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a fast algorithm for entrywise evaluation of the Gauss--Newton Hessian (GNH) matrix for the fully connected feed-forward neural network. The algorithm has a precomputation step and a sampling step. While it generally requires $\mathcal{O}(Nn)$ work to compute an entry (and the entire column) in the GNH matrix for a neural network with $N$ parameters and $n$ data points, our fast sampling algorithm reduces the cost to $\mathcal{O}(n+d/\epsilon^2)$ work, where $d$ is the output dimension of the network and $\epsilon$ is a prescribed accuracy (independent of $N$). One application of our algorithm is constructing the hierarchical-matrix ($\mathcal{H}$-matrix) approximation of the GNH matrix for solving linear systems and eigenvalue problems. It generally requires $\mathcal{O}(N^2)$ memory and $\mathcal{O}(N^3)$ work to store and factorize the GNH matrix, respectively. The $\mathcal{H}$-matrix approximation requires only $\mathcal{O}(N r_o)$ memory footprint and $\mathcal{O}(N r_o^2)$ work to be factorized, where $r_o \ll N$ is the maximum rank of off-diagonal blocks in the GNH matrix. We demonstrate the performance of our fast algorithm and the $\mathcal{H}$-matrix approximation on classification and autoencoder neural networks. (A corrected version is attached.)},
  archive      = {J_SIMAX},
  author       = {Chao Chen and Severin Reiz and Chenhan D. Yu and Hans-Joachim Bungartz and George Biros},
  doi          = {10.1137/19M129961X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {165-184},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast approximation of the gauss--newton hessian matrix for the multilayer perceptron},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uniqueness of nonnegative matrix factorizations by rigidity
theory. <em>SIMAX</em>, <em>42</em>(1), 134–164. (<a
href="https://doi.org/10.1137/19M1279472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorizations are often encountered in data mining applications where they are used to explain datasets by a small number of parts. For many of these applications it is desirable that there exists a unique nonnegative matrix factorization up to trivial modifications given by scalings and permutations. This means that model parameters are uniquely identifiable from the data. Rigidity theory of bar and joint frameworks is a field that studies uniqueness of point configurations given some of the pairwise distances. The goal of this paper is to use ideas from rigidity theory to study uniqueness of nonnegative matrix factorizations in the case when nonnegative rank of a matrix is equal to its rank. We characterize infinitesimally rigid nonnegative factorizations, prove that a nonnegative factorization is infinitesimally rigid if and only if it is locally rigid and a certain matrix achieves its maximal possible Kruskal rank, and show that locally rigid nonnegative factorizations can be extended to globally rigid nonnegative factorizations. These results give so far the strongest necessary condition for the uniqueness of a nonnegative factorization. We also explore connections between rigidity of nonnegative factorizations and boundaries of the set of matrices of fixed nonnegative rank. Finally we extend these results from nonnegative factorizations to completely positive factorizations.},
  archive      = {J_SIMAX},
  author       = {Robert Krone and Kaie Kubjas},
  doi          = {10.1137/19M1279472},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {134-164},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Uniqueness of nonnegative matrix factorizations by rigidity theory},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Componentwise perturbation analysis of the schur
decomposition of a matrix. <em>SIMAX</em>, <em>42</em>(1), 108–133. (<a
href="https://doi.org/10.1137/20M1330774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a unified scheme for perturbation analysis of the Schur decomposition $A = UTU^{{H}}$ of an $n$th order matrix $A$ which allows one to obtain new local (asymptotic) componentwise perturbation bounds for the corresponding unitary transformation matrix $U$ and upper triangular matrix $T$. This scheme involves $n(n-1)/2$ basic perturbation parameters which determine all componentwise bounds for the elements of $U$, the eigenvalues, the invariant subspaces, and the superdiagonal elements of $T$. New sensitivity estimates and condition numbers of eigenvalues, invariant subspaces, and superdiagonal elements are derived which produce theoretically the same results but are computationally alternative to the well-known local perturbation bounds. These estimates are based on computing the inverse of a block lower triangular matrix of order $n(n-1)/2$, which is obtained from the Schur form, and do not involve eigenvectors. Since the computation of the inverse may be done efficiently by parallel algorithms, the implementation of new estimates can be advantageous in comparison with the usage of classical estimates.},
  archive      = {J_SIMAX},
  author       = {Petko H. Petkov},
  doi          = {10.1137/20M1330774},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {108-133},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Componentwise perturbation analysis of the schur decomposition of a matrix},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of limited-memory krylov methods for stieltjes
functions of hermitian matrices. <em>SIMAX</em>, <em>42</em>(1), 83–107.
(<a href="https://doi.org/10.1137/20M1351072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a limited amount of memory and a target accuracy, we propose and compare several polynomial Krylov methods for the approximation of $f(A){b}$, the action of a Stieltjes matrix function of a large Hermitian matrix on a vector. Using new error bounds and estimates, as well as existing results, we derive predictions of the practical performance of the methods and rank them accordingly. As byproducts, we derive new results on inexact Krylov iterations for matrix functions in order to allow for a fair comparison of rational Krylov methods with polynomial inner solves.},
  archive      = {J_SIMAX},
  author       = {Stefan Güttel and Marcel Schweitzer},
  doi          = {10.1137/20M1351072},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {83-107},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A comparison of limited-memory krylov methods for stieltjes functions of hermitian matrices},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low rank pure quaternion approximation for pure quaternion
matrices. <em>SIMAX</em>, <em>42</em>(1), 58–82. (<a
href="https://doi.org/10.1137/19M1307329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quaternion matrices are employed successfully in many color image processing applications. In particular, a pure quaternion matrix can be used to represent red, green, and blue channels of color images. A low-rank approximation for a pure quaternion matrix can be obtained by using the quaternion singular value decomposition. However, this approximation is not optimal in the sense that the resulting low-rank approximation matrix may not be pure quaternion, i.e., the low-rank matrix contains a real component which is not useful for the representation of a color image. The main contribution of this paper is to find an optimal rank-$r$ pure quaternion matrix approximation for a pure quaternion matrix (a color image). Our idea is to use a projection on a low-rank quaternion matrix manifold and a projection on a quaternion matrix with zero real component, and develop an alternating projections algorithm to find such optimal low-rank pure quaternion matrix approximation. The convergence of the projection algorithm can be established by showing that the low-rank quaternion matrix manifold and the zero real component quaternion matrix manifold has a nontrivial intersection point. Numerical examples on synthetic pure quaternion matrices and color images are presented to illustrate the projection algorithm can find optimal low-rank pure quaternion approximation for pure quaternion matrices or color images.},
  archive      = {J_SIMAX},
  author       = {Guangjing Song and Weiyang Ding and Michael K. Ng},
  doi          = {10.1137/19M1307329},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {58-82},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Low rank pure quaternion approximation for pure quaternion matrices},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral properties of kernel matrices in the flat limit.
<em>SIMAX</em>, <em>42</em>(1), 17–57. (<a
href="https://doi.org/10.1137/19M129677X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel matrices are of central importance to many applied fields. In this manuscript, we focus on spectral properties of kernel matrices in the so-called “flat limit,” which occurs when points are close together relative to the scale of the kernel. We establish asymptotic expressions for the determinants of the kernel matrices, which we then leverage to obtain asymptotic expressions for the main terms of the eigenvalues. Analyticity of the eigenprojectors yields expressions for limiting eigenvectors, which are strongly tied to discrete orthogonal polynomials. Both smooth and finitely smooth kernels are covered, with stronger results available in the finite smoothness case.},
  archive      = {J_SIMAX},
  author       = {Simon Barthelmé and Konstantin Usevich},
  doi          = {10.1137/19M129677X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {17-57},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Spectral properties of kernel matrices in the flat limit},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured backward errors for eigenvalues of linear
port-hamiltonian descriptor systems. <em>SIMAX</em>, <em>42</em>(1),
1–16. (<a href="https://doi.org/10.1137/20M1344184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When computing the eigenstructure of matrix pencils associated with the passivity analysis of perturbed port-Hamiltonian descriptor system using a structured generalized eigenvalue method, one should make sure that the computed spectrum satisfies the symmetries that corresponds to this structure and the underlying physical system. We perform a backward error analysis and show that for matrix pencils associated with port-Hamiltonian descriptor systems and a given computed eigenstructure with the correct symmetry structure there always exists a nearby port-Hamiltonian descriptor system with exactly that eigenstructure. We also derive bounds for how near this system is and show that the stability radius of the system plays a role in that bound.},
  archive      = {J_SIMAX},
  author       = {Volker Mehrmann and Paul Van Dooren},
  doi          = {10.1137/20M1344184},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {1-16},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structured backward errors for eigenvalues of linear port-hamiltonian descriptor systems},
  volume       = {42},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
