<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods---52">SIMODS - 52</h2>
<ul>
<li><details>
<summary>
(2021). Interpretable approximation of high-dimensional data.
<em>SIMODS</em>, <em>3</em>(4), 1301–1323. (<a
href="https://doi.org/10.1137/21M1407707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we apply the previously introduced approximation method based on the analysis of variance (ANOVA) decomposition and Grouped Transformations to synthetic and real data. The advantage of this method is the interpretability of the approximation, i.e., the ability to rank the importance of the attribute interactions or the variable couplings. Moreover, we are able to generate an attribute ranking to identify unimportant variables and reduce the dimensionality of the problem. We compare the method to other approaches on publicly available benchmark datasets.},
  archive      = {J_SIMODS},
  author       = {Daniel Potts and Michael Schmischke},
  doi          = {10.1137/21M1407707},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1301-1323},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Interpretable approximation of high-dimensional data},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the bias, risk, and consistency of sample means in
multi-armed bandits. <em>SIMODS</em>, <em>3</em>(4), 1278–1300. (<a
href="https://doi.org/10.1137/20M1361249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sample mean is among the most well-studied estimators in statistics, having many desirable properties such as unbiasedness and consistency. However, when analyzing data collected using a multi-armed bandit (MAB) experiment, the sample mean is biased and much remains to be understood about its properties. For example, when is it consistent, how large is its bias, and can we bound its mean squared error? This paper delivers a thorough and systematic treatment of the bias, risk, and consistency of MAB sample means. Specifically, we identify four distinct sources of selection bias (sampling, stopping, choosing, and rewinding) and analyze them both separately and together. We further demonstrate that a new notion of effective sample size can be used to bound the risk of the sample mean under suitable loss functions. We present several carefully designed examples to provide intuition on the different sources of selection bias we study. Our treatment is nonparametric and algorithm-agnostic, meaning that it is not tied to a specific algorithm or goal. In a nutshell, our proofs combine variational representations of information-theoretic divergences with new martingale concentration inequalities.},
  archive      = {J_SIMODS},
  author       = {Jaehyeok Shin and Aaditya Ramdas and Alessandro Rinaldo},
  doi          = {10.1137/20M1361249},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1278-1300},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On the bias, risk, and consistency of sample means in multi-armed bandits},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the effectiveness of richardson extrapolation in data
science. <em>SIMODS</em>, <em>3</em>(4), 1251–1277. (<a
href="https://doi.org/10.1137/21M1397349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Richardson extrapolation is a classical technique from numerical analysis that can improve the approximation error of an estimation method by combining linearly several estimates obtained from different values of one of its hyperparameters without the need to know in details the inner structure of the original estimation method. The main goal of this paper is to study when Richardson extrapolation can be used within data science beyond the existing applications to step-size adaptations in stochastic gradient descent. We identify two situations where Richardson interpolation can be useful: (1) when the hyperparameter is the number of iterations of an existing iterative optimization algorithm with applications to averaged gradient descent and Frank--Wolfe algorithms (where we obtain asymptotically rates of $O(1/k^2)$ on polytopes, where $k$ is the number of iterations) and (2) when it is a regularization parameter with applications to Nesterov smoothing techniques for minimizing nonsmooth functions (where we obtain asymptotically rates close to $O(1/k^2)$ for nonsmooth functions) and kernel ridge regression. In all these cases, we show that extrapolation techniques come with no significant loss in performance but with sometimes strong gains, and we provide theoretical justifications based on asymptotic developments for such gains, as well as empirical illustrations on classical problems from machine learning.},
  archive      = {J_SIMODS},
  author       = {Francis Bach},
  doi          = {10.1137/21M1397349},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1251-1277},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On the effectiveness of richardson extrapolation in data science},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimal algorithm for strict circular seriation.
<em>SIMODS</em>, <em>3</em>(4), 1223–1250. (<a
href="https://doi.org/10.1137/21M139356X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of circular seriation, where we are given a matrix of pairwise dissimilarities between $n$ objects and the goal is to find a circular order of the objects in a manner that is consistent with their dissimilarity. This problem is a generalization of the classical linear seriation problem, where the goal is to find a linear order and for which optimal ${\cal O}(n^2)$ algorithms are known. Our contributions can be summarized as follows. First, we introduce circular Robinson matrices as the natural class of dissimilarity matrices for the circular seriation problem. Second, for the case of strict circular Robinson dissimilarity, matrices we provide an optimal ${\cal O}(n^2)$ algorithm for the circular seriation problem. Finally, we propose a statistical model to analyze the well-posedness of the circular seriation problem for large $n$. In particular, we establish ${\cal O}(\log(n)/n)$ rates on the distance between any circular ordering found by solving the circular seriation problem to the underlying order of the model in the Kendall-tau metric.},
  archive      = {J_SIMODS},
  author       = {Santiago Armstrong and Cristóbal Guzmán and Carlos A. Sing Long},
  doi          = {10.1137/21M139356X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1223-1250},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {An optimal algorithm for strict circular seriation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MR-GAN: Manifold regularized generative adversarial networks
for scientific data. <em>SIMODS</em>, <em>3</em>(4), 1197–1222. (<a
href="https://doi.org/10.1137/20M1344299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing interest in applying generative adversarial networks (GANs) in complex scientific applications, training GANs on scientific data remains a challenging problem from both theoretical and practical standpoints. One reason for this is that the generator is unable to accurately capture the underlying complex manifold structure of the real scientific data using only gradients from the discriminator. In this paper, we address this challenge using a novel approach that exploits the unique geometry of the scientific data to improve the quality of the generated data. Specifically, we improve the training of the GAN using an additional term referred to as a manifold regularizer which encourages the generator to respect the unique geometry of the scientific data manifold and generate high quality data. We theoretically prove that the addition of this regularization term leads to improved performance for different classes of GANs including deep convolutional GAN and Wasserstein GAN. Finally, we carry out performance comparisons on diverse datasets: synthetic data (Gaussian mixture), natural image data (celebrity face images (CelebA)), and scientific experimental data (scanning electron microscopy images of organic crystalline materials). In most of these applications, we find that the proposed manifold regularization-based approach helps in avoiding mode collapse, produces stable training, and leads to significant gains in terms of geometry score compared to its unregularized counterparts.},
  archive      = {J_SIMODS},
  author       = {Qunwei Li and Bhavya Kailkhura and Rushil Anirudh and Jize Zhang and Yi Zhou and Yingbin Liang and T. Yong-Jin Han and Pramod K. Varshney},
  doi          = {10.1137/20M1344299},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1197-1222},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {MR-GAN: Manifold regularized generative adversarial networks for scientific data},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mean-field controls with q-learning for cooperative MARL:
Convergence and complexity analysis. <em>SIMODS</em>, <em>3</em>(4),
1168–1196. (<a href="https://doi.org/10.1137/20M1360700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent reinforcement learning (MARL), despite its popularity and empirical success, suffers from the curse of dimensionality. This paper builds the mathematical framework to approximate cooperative MARL by a mean-field control (MFC) approach and shows that the approximation error is of $\mathcal{O}(\frac{1}{\sqrt{N}})$. By establishing an appropriate form of the dynamic programming principle for both the value function and the Q function, it proposes a model-free kernel-based Q-learning algorithm (MFC-K-Q), which is shown to have a linear convergence rate for the MFC problem, the first of its kind in the MARL literature. It further establishes that the convergence rate and the sample complexity of MFC-K-Q are independent of the number of agents $N$, which provides an $\mathcal{O}(\frac{1}{\sqrt{N}})$ approximation to the MARL problem with $N$ agents in the learning environment. Empirical studies for the network traffic congestion problem demonstrate that MFC-K-Q outperforms existing MARL algorithms when $N$ is large, for instance, when $N&gt;50$.},
  archive      = {J_SIMODS},
  author       = {Haotian Gu and Xin Guo and Xiaoli Wei and Renyuan Xu},
  doi          = {10.1137/20M1360700},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1168-1196},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Mean-field controls with Q-learning for cooperative MARL: Convergence and complexity analysis},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence of recursive stochastic algorithms using
wasserstein divergence. <em>SIMODS</em>, <em>3</em>(4), 1141–1167. (<a
href="https://doi.org/10.1137/21M1389808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a unified framework, based on iterated random operator theory, to analyze the convergence of constant stepsize recursive stochastic algorithms (RSAs). RSAs use randomization to efficiently compute expectations, and so their iterates form a stochastic process. The key idea of our analysis is to lift the RSA into an appropriate higher-dimensional space and then express it as an equivalent Markov chain. Instead of determining the convergence of this Markov chain (which may not converge under constant stepsize), we study the convergence of the distribution of this Markov chain. To study this, we define a new notion of Wasserstein divergence. We show that if the distribution of the iterates in the Markov chain satisfy a contraction property with respect to the Wasserstein divergence, then the Markov chain admits an invariant distribution. We show that convergence of a large family of constant stepsize RSAs can be understood using this framework, and we provide several detailed examples.},
  archive      = {J_SIMODS},
  author       = {Abhishek Gupta and William B. Haskell},
  doi          = {10.1137/21M1389808},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1141-1167},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Convergence of recursive stochastic algorithms using wasserstein divergence},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deterministic tensor completion with hypergraph expanders.
<em>SIMODS</em>, <em>3</em>(4), 1117–1140. (<a
href="https://doi.org/10.1137/20M1379745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a novel analysis of low-rank tensor completion based on hypergraph expanders. As a proxy for rank, we minimize the max-quasinorm of the tensor, which generalizes the max-norm for matrices. Our analysis is deterministic and shows that the number of samples required to approximately recover an order-$t$ tensor with at most $n$ entries per dimension is linear in $n$, under the assumption that the rank and order of the tensor are $O(1)$. As steps in our proof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform regular hypergraph model and prove several new properties about the tensor max-quasinorm. To the best of our knowledge, this is the first deterministic analysis of tensor completion. We develop a practical algorithm that solves a relaxed version of the max-quasinorm minimization problem, and we demonstrate its efficacy with numerical experiments.},
  archive      = {J_SIMODS},
  author       = {Kameron Decker Harris and Yizhe Zhu},
  doi          = {10.1137/20M1379745},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1117-1140},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Deterministic tensor completion with hypergraph expanders},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational representations and neural network estimation of
rényi divergences. <em>SIMODS</em>, <em>3</em>(4), 1093–1116. (<a
href="https://doi.org/10.1137/20M1368926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a new variational formula for the Rényi family of divergences, $R_\alpha(Q\|P)$, between probability measures $Q$ and $P$. Our result generalizes the classical Donsker--Varadhan variational formula for the Kullback--Leibler divergence. We further show that this Rényi variational formula holds over a range of function spaces; this leads to a formula for the optimizer under very weak assumptions and is also key in our development of a consistency theory for Rényi divergence estimators. By applying this theory to neural network estimators, we show that if a neural network family satisfies one of several strengthened versions of the universal approximation property, then the corresponding Rényi divergence estimator is consistent. In contrast to density estimator based methods, our estimators involve only expectations under $Q$ and $P$ and hence are more effective in high dimensional systems. We illustrate this via several numerical examples of neural network estimation in systems of up to 5,000 dimensions.},
  archive      = {J_SIMODS},
  author       = {Jeremiah Birrell and Paul Dupuis and Markos A. Katsoulakis and Luc Rey-Bellet and Jie Wang},
  doi          = {10.1137/20M1368926},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1093-1116},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Variational representations and neural network estimation of rényi divergences},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communication-efficient distributed eigenspace estimation.
<em>SIMODS</em>, <em>3</em>(4), 1067–1092. (<a
href="https://doi.org/10.1137/20M1364862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed computing is a standard way to scale up machine learning and data science algorithms to process large amounts of data. In such settings, avoiding communication amongst machines is paramount for achieving high performance. Rather than distribute the computation of existing algorithms, a common practice for avoiding communication is to compute local solutions or parameter estimates on each machine and then combine the results; in many convex optimization problems, even simple averaging of local solutions can work well. However, these schemes do not work when the local solutions are not unique. Spectral methods are a collection of such problems, where solutions are orthonormal bases of the leading invariant subspace of an associated data matrix. These solutions are only unique up to rotation and reflections. Here, we develop a communication-efficient distributed algorithm for computing the leading invariant subspace of a data matrix. Our algorithm uses a novel alignment scheme that minimizes the Procrustean distance between local solutions and a reference solution and only requires a single round of communication. For the important case of principal component analysis (PCA), we show that our algorithm achieves a similar error rate to that of a centralized estimator. We present numerical experiments demonstrating the efficacy of our proposed algorithm for distributed PCA as well as other problems where solutions exhibit rotational symmetry, such as node embeddings for graph data and spectral initialization for quadratic sensing.},
  archive      = {J_SIMODS},
  author       = {Vasileios Charisopoulos and Austin R. Benson and Anil Damle},
  doi          = {10.1137/20M1364862},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1067-1092},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Communication-efficient distributed eigenspace estimation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Train like a (var)pro: Efficient training of neural networks
with variable projection. <em>SIMODS</em>, <em>3</em>(4), 1041–1066. (<a
href="https://doi.org/10.1137/20M1359511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved state-of-the-art performance across a variety of traditional machine learning tasks, e.g., speech recognition, image classification, and segmentation. The ability of DNNs to efficiently approximate high-dimensional functions has also motivated their use in scientific applications, e.g., to solve partial differential equations and to generate surrogate models. In this paper, we consider the supervised training of DNNs, which arises in many of the above applications. We focus on the central problem of optimizing the weights of the given DNN such that it accurately approximates the relation between observed input and target data. Devising effective solvers for this optimization problem is notoriously challenging due to the large number of weights, nonconvexity, data sparsity, and nontrivial choice of hyperparameters. To solve the optimization problem more efficiently, we propose the use of variable projection (VarPro), a method originally designed for separable nonlinear least-squares problems. Our main contribution is the Gauss--Newton VarPro method (GNvpro) that extends the reach of the VarPro idea to nonquadratic objective functions, most notably cross-entropy loss functions arising in classification. These extensions make GNvpro applicable to all training problems that involve a DNN whose last layer is an affine mapping, which is common in many state-of-the-art architectures. In our four numerical experiments from surrogate modeling, segmentation, and classification, GNvpro solves the optimization problem more efficiently than commonly used stochastic gradient descent (SGD) schemes. Also, GNvpro finds solutions that generalize well, and in all but one example better than well-tuned SGD methods, to unseen data points.},
  archive      = {J_SIMODS},
  author       = {Elizabeth Newman and Lars Ruthotto and Joseph Hart and Bart van Bloemen Waanders},
  doi          = {10.1137/20M1359511},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1041-1066},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Train like a (Var)Pro: Efficient training of neural networks with variable projection},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Is temporal difference learning optimal? An
instance-dependent analysis. <em>SIMODS</em>, <em>3</em>(4), 1013–1040.
(<a href="https://doi.org/10.1137/20M1331524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of policy evaluation in discounted, tabular Markov decision processes and provide instance-dependent guarantees on the $\ell_\infty$-error under a generative model. We establish both asymptotic and nonasymptotic versions of local minimax lower bounds for policy evaluation, thereby providing an instance-dependent baseline by which to compare algorithms. Theory-inspired simulations show that the widely used temporal difference (TD) algorithm is strictly suboptimal when evaluated in a nonasymptotic setting, even when combined with Polyak--Ruppert iterate averaging. We remedy this issue by introducing and analyzing variance-reduced forms of stochastic approximation, showing that they achieve nonasymptotic, instance-dependent optimality up to logarithmic factors.},
  archive      = {J_SIMODS},
  author       = {Koulik Khamaru and Ashwin Pananjady and Feng Ruan and Martin J. Wainwright and Michael I. Jordan},
  doi          = {10.1137/20M1331524},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1013-1040},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Is temporal difference learning optimal? an instance-dependent analysis},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix denoising for weighted loss functions and
heterogeneous signals. <em>SIMODS</em>, <em>3</em>(3), 987–1012. (<a
href="https://doi.org/10.1137/20M1319577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating a low-rank matrix from a noisy observed matrix. Previous work has shown that the optimal method depends crucially on the choice of loss function. In this paper, we use a family of weighted loss functions, which arise naturally for problems such as submatrix denoising, denoising with heteroscedastic noise, and denoising with missing data. However, weighted loss functions are challenging to analyze because they are not orthogonally invariant. We derive optimal spectral denoisers for these weighted loss functions. By combining different weights, we then use these optimal denoisers to construct a new denoiser that exploits heterogeneity in the signal matrix to boost estimation with unweighted loss.},
  archive      = {J_SIMODS},
  author       = {William E. Leeb},
  doi          = {10.1137/20M1319577},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {987-1012},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Matrix denoising for weighted loss functions and heterogeneous signals},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On stochastic gradient langevin dynamics with dependent data
streams: The fully nonconvex case. <em>SIMODS</em>, <em>3</em>(3),
959–986. (<a href="https://doi.org/10.1137/20M1355392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of sampling from a target distribution, which is not necessarily log-concave, in the context of empirical risk minimization and stochastic optimization as presented in [M. Raginsky, A. Rakhlin, and M. Telgarsky, Proc. Mach. Learn. Res., 65 (2017), pp. 1674--1703]. Non-asymptotic results are established in the $L^1$-Wasserstein distance for the behavior of stochastic gradient Langevin dynamics algorithms. We allow gradient estimates based on dependent data streams. Our convergence estimates are sharper and uniform in the number of iterations, in contrast to those in previous studies.},
  archive      = {J_SIMODS},
  author       = {Ngoc Huy Chau and Éric Moulines and Miklós Rásonyi and Sotirios Sabanis and Ying Zhang},
  doi          = {10.1137/20M1355392},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {959-986},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On stochastic gradient langevin dynamics with dependent data streams: The fully nonconvex case},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicit deep learning. <em>SIMODS</em>, <em>3</em>(3),
930–958. (<a href="https://doi.org/10.1137/20M1358517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks. Such rules are based on the solution of a fixed-point equation involving a single vector of hidden features, which is thus only implicitly defined. The implicit framework greatly simplifies the notation of deep learning, and opens up many new possibilities in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization.},
  archive      = {J_SIMODS},
  author       = {Laurent El Ghaoui and Fangda Gu and Bertrand Travacca and Armin Askari and Alicia Tsai},
  doi          = {10.1137/20M1358517},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {930-958},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Implicit deep learning},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The trimmed lasso: Sparse recovery guarantees and practical
optimization by the generalized soft-min penalty. <em>SIMODS</em>,
<em>3</em>(3), 900–929. (<a
href="https://doi.org/10.1137/20M1330634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach to solve the sparse approximation or best subset selection problem, namely find a $k$-sparse vector ${x}\in\mathbb{R}^d$ that minimizes the $\ell_2$ residual $\lVert A{x}-{y} \rVert_2$. We consider a regularized approach, whereby this residual is penalized by the nonconvex ${\it trimmed lasso}$, defined as the $\ell_1$-norm of ${x}$ excluding its $k$ largest-magnitude entries. We prove that the trimmed lasso has several appealing theoretical properties, and in particular derive sparse recovery guarantees assuming successful optimization of the penalized objective. Next, we show empirically that directly optimizing this objective can be quite challenging. Instead, we propose a surrogate for the trimmed lasso, called the ${\it generalized soft-min}$. This penalty smoothly interpolates between the classical lasso and the trimmed lasso, while taking into account all possible $k$-sparse patterns. The generalized soft-min penalty involves summation over $\binom{d}{k}$ terms, yet we derive a polynomial-time algorithm to compute it. This, in turn, yields a practical method for the original sparse approximation problem. Via simulations, we demonstrate its competitive performance compared to current state of the art.},
  archive      = {J_SIMODS},
  author       = {Tal Amir and Ronen Basri and Boaz Nadler},
  doi          = {10.1137/20M1330634},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {900-929},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The trimmed lasso: Sparse recovery guarantees and practical optimization by the generalized soft-min penalty},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The signature kernel is the solution of a goursat PDE.
<em>SIMODS</em>, <em>3</em>(3), 873–899. (<a
href="https://doi.org/10.1137/20M1366794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been an increased interest in the development of kernel methods for learning with sequential data. The signature kernel is a learning tool with the potential to handle irregularly sampled, multivariate time series. In [F. J. Király and H. Oberhauser, J. Mach. Learn. Res., 20 (2019), 31] the authors introduced a kernel trick for the truncated version of this kernel avoiding the exponential complexity that would have been involved in a direct computation. Here we show that for continuously differentiable paths, the signature kernel solves a hyperbolic PDE and recognize the connection with a well-known class of differential equations known in the literature as Goursat problems. This Goursat PDE only depends on the increments of the input sequences, does not require the explicit computation of signatures, and can be solved efficiently using state-of-the-art hyperbolic PDE numerical solvers, giving a kernel trick for the untruncated signature kernel, with the same raw complexity as the method from Király and Oberhauser, but with the advantage that the PDE numerical scheme is well suited for GPU parallelization, which effectively reduces the complexity by a full order of magnitude in the length of the input sequences. In addition, we extend the previous analysis to the space of geometric rough paths and establish, using classical results from rough path theory, that the rough version of the signature kernel solves a rough integral equation analogous to the aforementioned Goursat problem. Finally, we empirically demonstrate the effectiveness of this PDE kernel as a machine learning tool in various data science applications dealing with sequential data. We make the library \tt sigkernel publicly available at https://github.com/crispitagorico/sigkernel.},
  archive      = {J_SIMODS},
  author       = {Cristopher Salvi and Thomas Cass and James Foster and Terry Lyons and Weixin Yang},
  doi          = {10.1137/20M1366794},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {873-899},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The signature kernel is the solution of a goursat PDE},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate spectral gaps for markov chain mixing times in
high dimensions. <em>SIMODS</em>, <em>3</em>(3), 854–872. (<a
href="https://doi.org/10.1137/19M1283082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a concept of approximate spectral gap to analyze the mixing time of reversible Markov chain Monte Carlo (MCMC) algorithms for which the usual spectral gap is degenerate or almost degenerate. We use the idea to analyze an MCMC algorithm to sample from mixtures of densities. As an application we study the mixing time of a Gibbs sampler for variable selection in linear regression models. We show that, properly tuned, the algorithm has a mixing time that grows at most polynomially with the dimension. Our results also suggest that the mixing time improves when the posterior distribution contracts towards the true model and the initial distribution is well chosen.},
  archive      = {J_SIMODS},
  author       = {Yves F. Atchadé},
  doi          = {10.1137/19M1283082},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {854-872},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximate spectral gaps for markov chain mixing times in high dimensions},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FANOK: Knockoffs in linear time. <em>SIMODS</em>,
<em>3</em>(3), 833–853. (<a
href="https://doi.org/10.1137/20M1363698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a series of algorithms that efficiently implement Gaussian model-X knockoffs to control the false discovery rate on large-scale feature selection problems. Identifying the knockoff distribution requires solving a large-scale semidefinite program for which we derive several efficient methods. One handles generic covariance matrices and has a complexity scaling as $\mathcal{O}(p^3)$, where $p$ is the ambient dimension, while another assumes a rank-$k$ factor model on the covariance matrix to reduce this complexity bound to $\mathcal{O}(pk^2)$. We review an efficient procedure to estimate factor models and show that under a factor model assumption, we can sample knockoff covariates with complexity linear in the dimension. We test our methods on problems with $p$ as large as 500 000.},
  archive      = {J_SIMODS},
  author       = {Armin Askari and Quentin Rebjock and Alexandre d&#39;Aspremont and Laurent El Ghaoui},
  doi          = {10.1137/20M1363698},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {833-853},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {FANOK: Knockoffs in linear time},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximation properties of ridge functions and extreme
learning machines. <em>SIMODS</em>, <em>3</em>(3), 815–832. (<a
href="https://doi.org/10.1137/20M1356348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a compact set $D\subset\mathbb{R}^{m}$ we consider the problem of approximating a function $f$ over $D$ by sums of ridge functions ${x}\mapsto\varphi({w}^{T}{x})$ with ${w}$ in a given set $\mathcal{W}$. While such sums are dense in $C(D)$ if $\mathcal{W}=\mathbb{R}^{m}$, to understand the effectiveness of these approximations we consider finite $\mathcal{W}$ and estimate the infimum of $\left\Vert f-g\right\Vert _{\infty}$ over $g\in V_{\mathcal{W}}:={span}\left{ \,{x}\mapsto\varphi({w}^{T}{x})\mid\varphi\in C(\mathbb{R}),\,{w}\in\mathcal{W}\,\right} $ in terms of the Lipschitz constant of $f$. In particular, we show lower bounds for the worst case approximation of functions of Lipschitz constant one by considering unapproximable functions $f$ ($\left\Vert f-g\right\Vert _{\infty}\geq\left\Vert f\right\Vert _{\infty}$ for any $g\in V_{\mathcal{W}}$) of Lipschitz constant one. Accurate approximations appear to require $\left|\mathcal{W}\right|=\Omega(m^{2})$ for $D$ a unit hypercube in $\mathbb{R}^{m}$: if $\left|\mathcal{W}\right|\leq\frac{1}{2}m(m-1)$ and $m\geq2$, then $\sup_{f}\inf_{g\in V_{\mathcal{W}}}\left\Vert f-g\right\Vert _{\infty}\geq1/\sqrt{m-1}$, where $f$ ranges over Lipschitz functions of Lipschitz constant one. Similar results hold for sums of products of pairs of ridge functions with weight vectors in $\mathcal{W}$.},
  archive      = {J_SIMODS},
  author       = {Palle Jorgensen and David E. Stewart},
  doi          = {10.1137/20M1356348},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {815-832},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximation properties of ridge functions and extreme learning machines},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-efficient structured convex optimization via extreme
point sampling. <em>SIMODS</em>, <em>3</em>(3), 787–814. (<a
href="https://doi.org/10.1137/20M1358037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory is a key computational bottleneck when solving large-scale convex optimization problems, such as semidefinite programs (SDPs). In this paper, we focus on the regime in which storing an $n\times n$ matrix decision variable is prohibitive. To solve SDPs in this regime, we develop a randomized algorithm that returns a random vector whose covariance matrix is near-feasible and near-optimal for the SDP. We show how to develop such an algorithm by modifying the Frank--Wolfe algorithm to systematically replace the matrix iterates with random vectors. As an application of this approach, we show how to implement the Goemans--Williamson approximation algorithm for MaxCut using $\mathcal{O}(n)$ memory in addition to the memory required to store the problem instance. We then extend our approach to deal with a broader range of structured convex optimization problems, replacing decision variables with random extreme points of the feasible region.},
  archive      = {J_SIMODS},
  author       = {Nimita Shinde and Vishnu Narayanan and James Saunderson},
  doi          = {10.1137/20M1358037},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {787-814},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Memory-efficient structured convex optimization via extreme point sampling},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised learning for aggregated multilayer graphs
using diffuse interface methods and fast matrix-vector products.
<em>SIMODS</em>, <em>3</em>(2), 758–785. (<a
href="https://doi.org/10.1137/20M1352028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We generalize a graph-based multiclass semi-supervised classification technique based on diffuse interface methods to multilayer graphs. Besides the treatment of various applications with an inherent multilayer structure, we present a very flexible approach that interprets high-dimensional data in a low-dimensional multilayer graph representation. Highly efficient numerical methods involving the spectral decomposition of the corresponding differential graph operators as well as fast matrix-vector products based on the nonequispaced fast Fourier transform enable the rapid treatment of large and high-dimensional data sets. We perform various numerical tests putting a special focus on image segmentation. In particular, we test the performance of our method on data sets with up to 10 million nodes per layer as well as up to 104 dimensions, resulting in graphs with up to 52 layers. While all presented numerical experiments can be run on an average laptop computer, the linear dependence per iteration step of the runtime on the network size in all stages of our algorithm makes it scalable to even larger and higher-dimensional problems.},
  archive      = {J_SIMODS},
  author       = {Kai Bergermann and Martin Stoll and Toni Volkmer},
  doi          = {10.1137/20M1352028},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {758-785},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Semi-supervised learning for aggregated multilayer graphs using diffuse interface methods and fast matrix-vector products},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of the neighborhood pattern similarity measure for
the role extraction problem. <em>SIMODS</em>, <em>3</em>(2), 736–757.
(<a href="https://doi.org/10.1137/20M1358785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we analyze an indirect approach, called the Neighborhood Pattern Similarity approach, to solve the so-called role extraction problem of a large-scale graph. The method is based on the preliminary construction of a node similarity matrix, which allows in a second stage to group together, with an appropriate clustering technique, the nodes that are assigned to have the same role. The analysis builds on the notion of ideal graphs where all nodes with the same role are also structurally equivalent.},
  archive      = {J_SIMODS},
  author       = {Melissa Marchand and Kyle Gallivan and Wen Huang and Paul Van Dooren},
  doi          = {10.1137/20M1358785},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {736-757},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Analysis of the neighborhood pattern similarity measure for the role extraction problem},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed learning with sparse communications by
identification. <em>SIMODS</em>, <em>3</em>(2), 715–735. (<a
href="https://doi.org/10.1137/20M1347772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed optimization for large-scale learning, a major performance limitation stems from the communications between the different entities. To the extent that computations are performed by workers on local data while a coordinator machine coordinates their updates to minimize a global loss, we present an asynchronous optimization algorithm that efficiently reduces the communications between the coordinator and workers. This reduction comes from a random sparsification of the local updates. We show that this algorithm converges linearly in the strongly convex case and also identifies optimal strongly sparse solutions. We further exploit this identification to propose an automatic dimension reduction, aptly sparsifying all exchanges between coordinator and workers.},
  archive      = {J_SIMODS},
  author       = {Dmitry Grishchenko and Franck Iutzeler and Jérôme Malick and Massih-Reza Amini},
  doi          = {10.1137/20M1347772},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {715-735},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Distributed learning with sparse communications by identification},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Test data reuse for the evaluation of continuously evolving
classification algorithms using the area under the receiver operating
characteristic curve. <em>SIMODS</em>, <em>3</em>(2), 692–714. (<a
href="https://doi.org/10.1137/20M1333110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance evaluation of continuously evolving machine learning algorithms presents new challenges, especially for high-risk application domains such as medicine. In principle, to obtain performance measures that generalize to a target population, a new independent test dataset randomly drawn from the target population should be used each time a new performance evaluation is required. However, test datasets of sufficient quality are often hard to acquire, and it is tempting to utilize a previously used test dataset for a new performance evaluation. With extensive experiments on simulated and real data we illustrate how such a “naive” approach to test data reuse can inadvertently result in overfitting the algorithm to the test data, resulting in a generalization loss and overly optimistic conclusions about the algorithm performance. We investigate the use of a modified version of the reusable holdout mechanism of Dwork et al. [Science, 349 (2015), pp. 636--638], which allows for repeated reuse of the same test dataset. We extend their approach to the use of AUC, the area under the receiver operating characteristic curve, as the reported performance metric. Theoretical guarantees for our method are proven to hold in extremely data-rich scenarios. However, our empirical results indicate promising performance of the proposed technique even on small data. With extensive simulation studies and experiments on real medical imaging data we show that our procedure indeed substantially reduces the problem of overfitting to the test data, even when the test dataset is small, at the cost of a mild additional uncertainty on the reported test performance.},
  archive      = {J_SIMODS},
  author       = {Alexej Gossmann and Aria Pezeshk and Yu-Ping Wang and Berkman Sahiner},
  doi          = {10.1137/20M1333110},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {692-714},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Test data reuse for the evaluation of continuously evolving classification algorithms using the area under the receiver operating characteristic curve},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global minima of overparameterized neural networks.
<em>SIMODS</em>, <em>3</em>(2), 676–691. (<a
href="https://doi.org/10.1137/19M1308943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore some mathematical features of the loss landscape of overparameterized neural networks. A priori, one might imagine that the loss function looks like a typical function from $\mathbb{R}^d$ to $\mathbb{R}$, in particular, that it has discrete global minima. In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function. If a neural net has $d$ parameters and is trained on $n$ data points $(x_i, y_i) \in \mathbb{R}^s \times \mathbb{R}^r$, with $d&gt;r n$, we show that the locus $M$ of global minima of $L$ is usually not discrete but rather an $(d- rn)$-dimensional submanifold of $\mathbb{R}^d$. In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very-high-dimensional submanifold of $\mathbb{R}^d$.},
  archive      = {J_SIMODS},
  author       = {Yaim Cooper},
  doi          = {10.1137/19M1308943},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {676-691},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Global minima of overparameterized neural networks},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonbacktracking eigenvalues under node removal: X-centrality
and targeted immunization. <em>SIMODS</em>, <em>3</em>(2), 656–675. (<a
href="https://doi.org/10.1137/20M1352132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonbacktracking matrix and its eigenvalues have many applications in network science and graph mining, such as node and edge centrality, community detection, length spectrum theory, graph distance, and epidemic and percolation thresholds. In network epidemiology, the reciprocal of the largest eigenvalue of the nonbacktracking matrix is a good approximation for the epidemic threshold of certain network dynamics. In this work, we develop techniques that identify which nodes have the largest impact on this leading eigenvalue. We do so by studying the behavior of the spectrum of the nonbacktracking matrix after a node is removed from the graph. From this analysis we derive two new centrality measures: $X$-degree and X-nonbacktracking centrality. We perform extensive experimentation with targeted immunization strategies derived from these two centrality measures. Our spectral analysis and centrality measures can be broadly applied, and will be of interest to both theorists and practitioners alike.},
  archive      = {J_SIMODS},
  author       = {Leo Torres and Kevin S. Chan and Hanghang Tong and Tina Eliassi-Rad},
  doi          = {10.1137/20M1352132},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {656-675},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonbacktracking eigenvalues under node removal: X-centrality and targeted immunization},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The gap between theory and practice in function
approximation with deep neural networks. <em>SIMODS</em>, <em>3</em>(2),
624–655. (<a href="https://doi.org/10.1137/20M131309X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) is transforming whole industries as complicated decision-making processes are being automated by deep neural networks (DNNs) trained on real-world data. Driven in part by a rapidly expanding literature on DNN approximation theory showing that DNNs can approximate a rich variety of functions, these tools are increasingly being considered for problems in scientific computing. Yet, unlike more traditional algorithms in this field, relatively little is known about DNNs in relation to the principles of numerical analysis, namely, stability, accuracy, computational efficiency, and sample complexity. In this paper we first introduce a computational framework for examining DNNs in practice, and then use it to study their empirical performance with regard to these issues. We examine the performance of DNNs of different widths and depths on a variety of test functions in various dimensions, including smooth and piecewise smooth functions. We also compare DL against best-in-class methods for smooth function approximation based on compressed sensing. Our main conclusion from these experiments is that there is a crucial gap between the approximation theory of DNNs and their practical performance, with trained DNNs performing relatively poorly on functions for which there are strong approximation results (e.g., smooth functions) yet performing well in comparison to best-in-class methods for other functions. To analyze this gap further, we then provide some theoretical insights. We establish a practical existence theorem, which asserts the existence of a DNN architecture and training procedure that offers the same performance as compressed sensing. This result establishes a key theoretical benchmark. It demonstrates that the gap can be closed, albeit via a DNN approximation strategy which is guaranteed to perform as well as, but no better than, current best-in-class schemes. Nevertheless, it demonstrates the promise of practical DNN approximation by highlighting the potential for developing better schemes through the careful design of DNN architectures and training strategies.},
  archive      = {J_SIMODS},
  author       = {Ben Adcock and Nick Dexter},
  doi          = {10.1137/20M131309X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {624-655},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The gap between theory and practice in function approximation with deep neural networks},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simplex-structured matrix factorization: Sparsity-based
identifiability and provably correct algorithms. <em>SIMODS</em>,
<em>3</em>(2), 593–623. (<a
href="https://doi.org/10.1137/20M1354982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide novel algorithms with identifiability guarantees for simplex-structured matrix factorization (SSMF), a generalization of nonnegative matrix factorization. Current state-of-the-art algorithms that provide identifiability results for SSMF rely on the sufficiently scattered condition (SSC), which requires the data points to be well spread within the convex hull of the basis vectors. The conditions under which our proposed algorithms recover the unique decomposition are in most cases much weaker than the SSC. We only require having $d$ points on each facet of the convex hull of the basis vectors whose dimension is $d-1$. The key idea is based on extracting facets containing the largest number of points. We illustrate the effectiveness of our approach on synthetic data sets and hyperspectral images, showing that it outperforms state-of-the-art SSMF algorithms, as it is able to handle higher noise levels, rank deficient matrices, outliers, and input data that highly violates the SSC.},
  archive      = {J_SIMODS},
  author       = {Maryam Abdolali and Nicolas Gillis},
  doi          = {10.1137/20M1354982},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {593-623},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Simplex-structured matrix factorization: Sparsity-based identifiability and provably correct algorithms},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normal-bundle bootstrap. <em>SIMODS</em>, <em>3</em>(2),
573–592. (<a href="https://doi.org/10.1137/20M1356002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic models of data sets often exhibit salient geometric structure. Such a phenomenon is summed up in the manifold distribution hypothesis and can be exploited in probabilistic learning. Here we present normal-bundle bootstrap (NBB), a method that generates new data which preserve the geometric structure of a given data set. Inspired by algorithms for manifold learning and concepts in differential geometry, our method decomposes the underlying probability measure into a marginalized measure on a learned data manifold and conditional measures on the normal spaces. The algorithm estimates the data manifold as a density ridge and constructs new data by bootstrapping projection vectors and adding them to the ridge. We apply our method to the inference of density ridge and related statistics, and to data augmentation to reduce overfitting.},
  archive      = {J_SIMODS},
  author       = {Ruda Zhang and Roger Ghanem},
  doi          = {10.1137/20M1356002},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {573-592},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Normal-bundle bootstrap},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binary component decomposition part i: The
positive-semidefinite case. <em>SIMODS</em>, <em>3</em>(2), 544–572. (<a
href="https://doi.org/10.1137/19M1278612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of decomposing a low-rank positive-semidefinite matrix into symmetric factors with binary entries, either ${\pm 1}$ or ${0,1}$. This research answers fundamental questions about the existence and uniqueness of these decompositions. It also leads to tractable factorization algorithms that succeed under a mild deterministic condition. A companion paper addresses the related problem of decomposing a low-rank rectangular matrix into a binary factor and an unconstrained factor.},
  archive      = {J_SIMODS},
  author       = {Richard Kueng and Joel A. Tropp},
  doi          = {10.1137/19M1278612},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {544-572},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Binary component decomposition part i: The positive-semidefinite case},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient linearly convergent regularized proximal point
algorithm for fused multiple graphical lasso problems. <em>SIMODS</em>,
<em>3</em>(2), 524–543. (<a
href="https://doi.org/10.1137/20M1344160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, analyzing data from different classes or over a temporal grid has attracted a great deal of interest. As a result, various multiple graphical models for learning a collection of graphical models simultaneously have been derived by introducing sparsity in graphs and similarity across multiple graphs. This paper focuses on the fused multiple graphical Lasso model, which encourages not only shared pattern of sparsity but also shared values of edges across different graphs. For solving this model, we develop an efficient regularized proximal point algorithm, where the subproblem in each iteration of the algorithm is solved by a superlinearly convergent semismooth Newton method. To implement the semismooth Newton method, we derive an explicit expression for the generalized Jacobian of the proximal mapping of the fused multiple graphical Lasso regularizer. Unlike those widely used first order methods, our approach has heavily exploited the underlying second order information through the semismooth Newton method. This not only can accelerate the convergence of the algorithm but also can improve its robustness. The efficiency and robustness of our proposed algorithm are demonstrated by comparing it with some state-of-the-art methods on both synthetic and real data sets.},
  archive      = {J_SIMODS},
  author       = {Ning Zhang and Yangjing Zhang and Defeng Sun and Kim-Chuan Toh},
  doi          = {10.1137/20M1344160},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {524-543},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {An efficient linearly convergent regularized proximal point algorithm for fused multiple graphical lasso problems},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-reference alignment in high dimensions: Sample
complexity and phase transition. <em>SIMODS</em>, <em>3</em>(2),
494–523. (<a href="https://doi.org/10.1137/20M1354994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-reference alignment entails estimating a signal in $\mathbb{R}^L$ from its circularly shifted and noisy copies. This problem has been studied thoroughly in recent years, focusing on the finite-dimensional setting (fixed $L$). Motivated by single-particle cryo-electron microscopy, we analyze the sample complexity of the problem in the high-dimensional regime $L\to\infty$. Our analysis uncovers a phase transition phenomenon governed by the parameter $\alpha = L/(\sigma^2\log L)$, where $\sigma^2$ is the variance of the noise. When $\alpha&gt;2$, the impact of the unknown circular shifts on the sample complexity is minor. Namely, the number of measurements required to achieve a desired accuracy $\varepsilon$ approaches $\sigma^2/\varepsilon$ for small $\varepsilon$; this is the sample complexity of estimating a signal in additive white Gaussian noise, which does not involve shifts. In sharp contrast, when $\alpha\leq 2$, the problem is significantly harder, and the sample complexity grows substantially more quickly with $\sigma^2$.},
  archive      = {J_SIMODS},
  author       = {Elad Romanov and Tamir Bendory and Or Ordentlich},
  doi          = {10.1137/20M1354994},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {494-523},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Multi-reference alignment in high dimensions: Sample complexity and phase transition},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A metric on directed graphs and markov chains based on
hitting probabilities. <em>SIMODS</em>, <em>3</em>(2), 467–493. (<a
href="https://doi.org/10.1137/20M1348315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shortest-path, commute time, and diffusion distances on undirected graphs have been widely employed in applications such as dimensionality reduction, link prediction, and trip planning. Increasingly, there is interest in using asymmetric structure of data derived from Markov chains and directed graphs, but few metrics are specifically adapted to this task. We introduce a metric on the state space of any ergodic, finite-state, time-homogeneous Markov chain and, in particular, on any Markov chain derived from a directed graph. Our construction is based on hitting probabilities, with nearness in the metric space related to the transfer of random walkers from one node to another at stationarity. Notably, our metric is insensitive to shortest and average walk distances, thus giving new information compared to existing metrics. We use possible degeneracies in the metric to develop an interesting structural theory of directed graphs and explore a related quotienting procedure. Our metric can be computed in $O(n^3)$ time, where $n$ is the number of states, and in examples we scale up to $n=10,000$ nodes and $\approx 38M$ edges on a desktop computer. In several examples, we explore the nature of the metric, compare it to alternative methods, and demonstrate its utility for weak recovery of community structure in dense graphs, visualization, structure recovering, dynamics exploration, and multiscale cluster detection.},
  archive      = {J_SIMODS},
  author       = {Zachary M. Boyd and Nicolas Fraiman and Jeremy Marzuola and Peter J. Mucha and Braxton Osting and Jonathan Weare},
  doi          = {10.1137/20M1348315},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {467-493},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A metric on directed graphs and markov chains based on hitting probabilities},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank <span class="math inline">2<em>r</em></span> iterative
least squares: Efficient recovery of ill-conditioned low rank matrices
from few entries. <em>SIMODS</em>, <em>3</em>(1), 439–465. (<a
href="https://doi.org/10.1137/20M1315294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new, simple, and computationally efficient iterative method for low rank matrix completion. Our method is inspired by the class of factorization-type iterative algorithms, but substantially differs from them in the way the problem is cast. Precisely, given a target rank $r$, instead of optimizing on the manifold of rank $r$ matrices, we allow our interim estimated matrix to have a specific overparametrized rank $2r$ structure. Our algorithm, denoted R2RILS, for rank $2r$ iterative least squares, thus has low memory requirements, and at each iteration it solves a computationally cheap sparse least squares problem. We motivate our algorithm by its theoretical analysis for the simplified case of a rank 1 matrix. Empirically, R2RILS is able to recover ill-conditioned low rank matrices from very few observations---near the information limit---and it is stable to additive noise.},
  archive      = {J_SIMODS},
  author       = {Jonathan Bauch and Boaz Nadler and Pini Zilber},
  doi          = {10.1137/20M1315294},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {439-465},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Rank $2r$ iterative least squares: Efficient recovery of ill-conditioned low rank matrices from few entries},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalization error of minimum weighted norm and kernel
interpolation. <em>SIMODS</em>, <em>3</em>(1), 414–438. (<a
href="https://doi.org/10.1137/20M1359912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the generalization error of functions that interpolate prescribed data points and are selected by minimizing a weighted norm. Under natural and general conditions, we prove that both the interpolants and their generalization errors converge as the number of parameters grows, and the limiting interpolant belongs to a reproducing kernel Hilbert space. This rigorously establishes an implicit bias of minimum weighted norm interpolation and explains why norm minimization may either benefit or suffer from over-parameterization. As special cases of this theory, we study interpolation by trigonometric polynomials and spherical harmonics. Our approach is from a deterministic and approximation theory viewpoint, as opposed to a statistical or random matrix one.},
  archive      = {J_SIMODS},
  author       = {Weilin Li},
  doi          = {10.1137/20M1359912},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {414-438},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Generalization error of minimum weighted norm and kernel interpolation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Doubly stochastic normalization of the gaussian kernel is
robust to heteroskedastic noise. <em>SIMODS</em>, <em>3</em>(1),
388–413. (<a href="https://doi.org/10.1137/20M1342124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental step in many data-analysis techniques is the construction of an affinity matrix describing similarities between data points. When the data points reside in Euclidean space, a widespread approach is to form an affinity matrix by the Gaussian kernel with pairwise distances, and to follow with a certain normalization (e.g., the row-stochastic normalization or its symmetric variant). We demonstrate that the doubly stochastic normalization of the Gaussian kernel with zero main diagonal (i.e., no self-loops) is robust to heteroskedastic noise. That is, the doubly stochastic normalization is advantageous in that it automatically accounts for observations with different noise variances. Specifically, we prove that in a suitable high-dimensional setting where heteroskedastic noise does not concentrate too much in any particular direction in space, the resulting (doubly stochastic) noisy affinity matrix converges to its clean counterpart with rate $m^{-1/2}$, where $m$ is the ambient dimension. We demonstrate this result numerically and show that, in contrast, the popular row-stochastic and symmetric normalizations behave unfavorably under heteroskedastic noise. Furthermore, we provide examples of simulated and experimental single-cell RNA sequence data with intrinsic heteroskedasticity, where the advantage of the doubly stochastic normalization for exploratory analysis is evident.},
  archive      = {J_SIMODS},
  author       = {Boris Landa and Ronald R. Coifman and Yuval Kluger},
  doi          = {10.1137/20M1342124},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {388-413},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Doubly stochastic normalization of the gaussian kernel is robust to heteroskedastic noise},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A performance guarantee for spectral clustering.
<em>SIMODS</em>, <em>3</em>(1), 369–387. (<a
href="https://doi.org/10.1137/20M1352193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-step spectral clustering method, which consists of the Laplacian eigenmap and a rounding step, is a widely used method for graph partitioning. It can be seen as a natural relaxation to the NP-hard minimum ratio cut problem. In this paper, we study the following central question: When is spectral clustering able to find the global solution to the minimum ratio cut problem? First, we provide a condition that naturally depends on the intra- and intercluster connectivities of a given partition under which we may certify that this partition is the solution to the minimum ratio cut problem. Then, we develop a deterministic two-to-infinity norm perturbation bound for the invariant subspace of the graph Laplacian that corresponds to the $k$ smallest eigenvalues. Finally, by combining these two results we give a condition under which spectral clustering is guaranteed to output the global solution to the minimum ratio cut problem, which serves as a performance guarantee for spectral clustering.},
  archive      = {J_SIMODS},
  author       = {March Boedihardjo and Shaofeng Deng and Thomas Strohmer},
  doi          = {10.1137/20M1352193},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {369-387},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A performance guarantee for spectral clustering},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Greed works: An improved analysis of sampling
kaczmarz–motzkin. <em>SIMODS</em>, <em>3</em>(1), 342–368. (<a
href="https://doi.org/10.1137/19M1307044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic iterative algorithms have gained recent interest in machine learning and signal processing for solving large-scale systems of equations, $A{x}={b}$. One such example is the randomized Kaczmarz (RK) algorithm, which acts only on single rows of the matrix $A$ at a time. While RK randomly selects a row of $A$ to work with, Motzkin&#39;s Method (MM) employs a greedy row selection. Connections between the two algorithms resulted in the Sampling Kaczmarz--Motzkin (SKM) algorithm, which samples a random subset of $\beta$ rows of $A$ and then greedily selects the best row of the subset. Despite their variable computational costs, all three algorithms have been proven to have the same theoretical upper bound on the convergence rate. In this work, an improved analysis of the range of random (RK) to greedy (MM) methods is presented. This analysis improves upon previous known convergence bounds for SKM, capturing the benefit of partially greedy selection schemes. This work also further generalizes previous known results, removing the theoretical assumptions that $\beta$ must be fixed at every iteration and that $A$ must have normalized rows.},
  archive      = {J_SIMODS},
  author       = {Jamie Haddock and Anna Ma},
  doi          = {10.1137/19M1307044},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {342-368},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Greed works: An improved analysis of sampling kaczmarz--motzkin},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transport model for feature extraction. <em>SIMODS</em>,
<em>3</em>(1), 321–341. (<a
href="https://doi.org/10.1137/19M1296926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new feature extraction method for noisy datasets with intricate spatio-temporal structures that is based on the concept of transport operators on graphs. The proposed approach generalizes and extends the many existing data representation methodologies built upon diffusion processes to a new domain where dynamical systems play a key role. The main advantage of this approach comes from the ability to exploit different relationships than those arising in the context of, e.g., graph Laplacians. Fundamental properties of the transport operators are proved. We demonstrate the flexibility of the method by introducing several diverse examples of transformations. We close the paper with a series of computational experiments and applications to the problem of image clustering and classification of hyperspectral data, to illustrate the practical implications of our algorithm and its ability to quantify new aspects of relationships within complicated datasets.},
  archive      = {J_SIMODS},
  author       = {Wojciech Czaja and Dong Dong and Pierre-Emmanuel Jabin and Franck Olivier Ndjakou Njeunje},
  doi          = {10.1137/19M1296926},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {321-341},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Transport model for feature extraction},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time performance of distributed temporal-difference
learning with linear function approximation. <em>SIMODS</em>,
<em>3</em>(1), 298–320. (<a
href="https://doi.org/10.1137/20M1311971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the policy evaluation problem in multi-agent reinforcement learning, modeled by a Markov decision process. In this problem, the agents operate in a common environment under a fixed control policy, working together to discover the value (global discounted accumulative reward) associated with each environmental state. Over a series of time steps, the agents act, get rewarded, update their local estimate of the value function, and then communicate with their neighbors. The local update at each agent can be interpreted as a distributed variant of the popular temporal-difference learning methods TD$(\lambda)$. Our main contribution is to provide a finite-time analysis on the performance of this distributed TD$(\lambda)$ algorithm for both constant and time-varying step sizes. The key idea in our analysis is to use the geometric mixing time $\tau$ of the underlying Markov chain, that is, although the “noise” in our algorithm is Markovian, its dependence is very weak at samples spaced out at every $\tau$. We provide an explicit upper bound on the convergence rate of the proposed method as a function of the network topology, the discount factor, the constant $\lambda$, and the mixing time $\tau$. Our results also provide a mathematical explanation for observations that have appeared previously in the literature about the choice of $\lambda$. Our upper bound illustrates the trade-off between approximation accuracy and convergence speed implicit in the choice of $\lambda$. When $\lambda=1$, the solution will correspond to the best possible approximation of the value function, while choosing $\lambda = 0$ leads to faster convergence when the noise in the algorithm has large variance.},
  archive      = {J_SIMODS},
  author       = {Thinh T. Doan and Siva Theja Maguluri and Justin Romberg},
  doi          = {10.1137/20M1311971},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {298-320},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Finite-time performance of distributed temporal-difference learning with linear function approximation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diversity sampling is an implicit regularization for kernel
methods. <em>SIMODS</em>, <em>3</em>(1), 280–297. (<a
href="https://doi.org/10.1137/20M1320031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel methods have achieved very good performance on large scale regression and classification problems by using the Nyström method and preconditioning techniques. The Nyström approximation---based on a subset of landmarks---gives a low rank approximation of the kernel matrix, and is known to provide a form of implicit regularization. We further elaborate on the impact of sampling diverse landmarks for constructing the Nyström approximation in supervised as well as unsupervised kernel methods. By using Determinantal Point Processes (DPP) for sampling, we obtain additional theoretical results concerning the interplay between diversity and regularization. Empirically, we demonstrate the advantages of training kernel methods based on subsets made of diverse points. In particular, if the dataset has a dense bulk and a sparser tail, we show that Nyström kernel regression with diverse landmarks increases the accuracy of the regression in sparser regions of the dataset, with respect to a uniform landmark sampling. A greedy heuristic is also proposed to select diverse samples of significant size within large datasets when exact DPP sampling is not practically feasible.},
  archive      = {J_SIMODS},
  author       = {Michael Fanuel and Joachim Schreurs and Johan Suykens},
  doi          = {10.1137/20M1320031},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {280-297},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Diversity sampling is an implicit regularization for kernel methods},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor methods for nonlinear matrix completion.
<em>SIMODS</em>, <em>3</em>(1), 253–279. (<a
href="https://doi.org/10.1137/20M1323448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the low-rank matrix completion (LRMC) problem, the low-rank assumption means that the columns (or rows) of the matrix to be completed are points on a low-dimensional linear algebraic variety. This paper extends this thinking to cases where the columns are points on a low-dimensional nonlinear algebraic variety, a problem we call low algebraic dimension matrix completion (LADMC). Matrices whose columns belong to a union of subspaces are an important special case. We propose an LADMC algorithm that leverages existing LRMC methods on a tensorized representation of the data. For example, a second-order tensorized representation is formed by taking the Kronecker product of each column with itself, and we consider higher-order tensorizations as well. This approach will succeed in many cases where traditional LRMC is guaranteed to fail because the data are low-rank in the tensorized representation but are not in the original representation. We also provide a formal mathematical justification for the success of our method. In particular, we give bounds on the rank of these data in the tensorized representation, and we prove sampling requirements to guarantee uniqueness of the solution. We also provide experimental results showing that the new approach outperforms existing state-of-the-art methods for matrix completion under a union-of-subspaces model.},
  archive      = {J_SIMODS},
  author       = {Greg Ongie and Daniel Pimentel-Alarcón and Laura Balzano and Rebecca Willett and Robert D. Nowak},
  doi          = {10.1137/20M1323448},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {253-279},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Tensor methods for nonlinear matrix completion},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error bounds for dynamical spectral estimation.
<em>SIMODS</em>, <em>3</em>(1), 225–252. (<a
href="https://doi.org/10.1137/20M1335984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamical spectral estimation is a well-established numerical approach for estimating eigenvalues and eigenfunctions of the Markov transition operator from trajectory data. Although the approach has been widely applied in biomolecular simulations, its error properties remain poorly understood. Here we analyze the error of a dynamical spectral estimation method called “the variational approach to conformational dynamics&quot; (VAC). We bound the approximation error and estimation error for VAC estimates. Our analysis establishes VAC&#39;s convergence properties and suggests new strategies for tuning VAC to improve accuracy.},
  archive      = {J_SIMODS},
  author       = {Robert J. Webber and Erik H. Thiede and Douglas Dow and Aaron R. Dinner and Jonathan Weare},
  doi          = {10.1137/20M1335984},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {225-252},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Error bounds for dynamical spectral estimation},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multikernel regression with sparsity constraint.
<em>SIMODS</em>, <em>3</em>(1), 201–224. (<a
href="https://doi.org/10.1137/20M1318882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide a Banach-space formulation of supervised learning with generalized total- variation (gTV) regularization. We identify the class of kernel functions that are admissible in this framework. Then, we propose a variation of supervised learning in a continuous-domain hybrid search space with gTV regularization. We show that the solution admits a multikernel expansion with adaptive positions. In this representation, the number of active kernels is upper-bounded by the number of data points while the gTV regularization imposes an $\ell_1$ penalty on the kernel coefficients. Finally, we illustrate numerically the outcome of our theory.},
  archive      = {J_SIMODS},
  author       = {Shayan Aziznejad and Michael Unser},
  doi          = {10.1137/20M1318882},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {201-224},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Multikernel regression with sparsity constraint},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable semidefinite programming. <em>SIMODS</em>,
<em>3</em>(1), 171–200. (<a
href="https://doi.org/10.1137/19M1305045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semidefinite programming (SDP) is a powerful framework from convex optimization that has striking potential for data science applications. This paper develops a provably correct randomized algorithm for solving large, weakly constrained SDP problems by economizing on the storage and arithmetic costs. Numerical evidence shows that the method is effective for a range of applications, including relaxations of \sf MaxCut, abstract phase retrieval, and quadratic assignment. Running on a laptop equivalent, the algorithm can handle SDP instances where the matrix variable has over $10^{14}$ entries.},
  archive      = {J_SIMODS},
  author       = {Alp Yurtsever and Joel A. Tropp and Olivier Fercoq and Madeleine Udell and Volkan Cevher},
  doi          = {10.1137/19M1305045},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {171-200},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Scalable semidefinite programming},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diffusion state distances: Multitemporal analysis, fast
algorithms, and applications to biological networks. <em>SIMODS</em>,
<em>3</em>(1), 142–170. (<a
href="https://doi.org/10.1137/20M1324089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-dependent metrics are powerful tools for learning the underlying structure of high-dimensional data. This article further develops and analyzes a data-dependent metric known as diffusion state distance (DSD), which compares points using a data-driven diffusion process. Unlike related diffusion methods, DSDs incorporate information across time scales, which allows for the intrinsic data structure to be inferred in a parameter-free manner. This article develops a theory for DSD based on the multitemporal emergence of mesoscopic equilibria in the underlying diffusion process. New algorithms for denoising and dimension reduction with DSD are also proposed and analyzed. These approaches are based on a weighted spectral decomposition of the underlying diffusion process, and experiments on synthetic datasets and real biological networks illustrate the efficacy of the proposed algorithms in terms of both speed and accuracy. Throughout, comparisons with related methods are made in order to illustrate the distinct advantages of DSD for datasets exhibiting multiscale structure.},
  archive      = {J_SIMODS},
  author       = {Lenore Cowen and Kapil Devkota and Xiaozhe Hu and James M. Murphy and Kaiyi Wu},
  doi          = {10.1137/20M1324089},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {142-170},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Diffusion state distances: Multitemporal analysis, fast algorithms, and applications to biological networks},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral neighbor joining for reconstruction of latent tree
models. <em>SIMODS</em>, <em>3</em>(1), 113–141. (<a
href="https://doi.org/10.1137/20M1365715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common assumption in multiple scientific applications is that the distribution of observed data can be modeled by a latent tree graphical model. An important example is phylogenetics, where the tree models the evolutionary lineages of a set of observed organisms. Given a set of independent realizations of the random variables at the leaves of the tree, a key challenge is to infer the underlying tree topology. In this work we develop spectral neighbor joining (SNJ), a novel method to recover the structure of latent tree graphical models. Given a matrix that contains a measure of similarity between all pairs of observed variables, SNJ computes a spectral measure of cohesion between groups of observed variables. We prove that SNJ is consistent and derive a sufficient condition for correct tree recovery from an estimated similarity matrix. Combining this condition with a concentration of measure result on the similarity matrix, we bound the number of samples required to recover the tree with high probability. We illustrate via extensive simulations that in comparison to several other reconstruction methods, SNJ requires fewer samples to accurately recover trees with a large number of leaves or long edges.},
  archive      = {J_SIMODS},
  author       = {Ariel Jaffe and Noah Amsel and Yariv Aizenbud and Boaz Nadler and Joseph T. Chang and Yuval Kluger},
  doi          = {10.1137/20M1365715},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {113-141},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Spectral neighbor joining for reconstruction of latent tree models},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The convex mixture distribution: Granger causality for
categorical time series. <em>SIMODS</em>, <em>3</em>(1), 83–112. (<a
href="https://doi.org/10.1137/20M133097X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework for learning Granger causality networks for multivariate categorical time series based on the mixture transition distribution (MTD) model. Traditionally, MTD is plagued by a nonconvex objective, nonidentifiability, and the presence of local optima. To circumvent these problems, we recast inference in the MTD as a convex problem. The new formulation facilitates the application of MTD to high-dimensional multivariate time series. As a baseline, we also formulate a multinomial logistic transition distribution (mLTD) model. While it is a straightforward extension of autoregressive Bernoulli generalized linear models, it has not been previously applied to the analysis of multivariate categorical time series. We establish identifiability conditions of the MTD model and compare them to those for mLTD. We further devise novel and efficient optimization algorithms for MTD based on our proposed convex formulation and compare the MTD and mLTD in both simulated and real data experiments. Finally, we establish consistency of the convex MTD in high dimensions. Our approach simultaneously provides a comparison of methods for network inference in categorical time series and opens the door to modern, regularized inference with the MTD model.},
  archive      = {J_SIMODS},
  author       = {Alex Tank and Xiudi Li and Emily B. Fox and Ali Shojaie},
  doi          = {10.1137/20M133097X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {83-112},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The convex mixture distribution: Granger causality for categorical time series},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum entropy methods for texture synthesis: Theory and
practice. <em>SIMODS</em>, <em>3</em>(1), 52–82. (<a
href="https://doi.org/10.1137/19M1307731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen the rise of convolutional neural network techniques in exemplar-based image synthesis. These methods often rely on the minimization of some variational formulation on the image space for which the minimizers are assumed to be the solutions of the synthesis problem. In this paper we investigate, both theoretically and experimentally, another framework to deal with this problem using an alternate sampling/minimization scheme. First, we use results from information geometry to assess that our method yields a probability measure which has maximum entropy under some constraints in expectation. Then, we turn to the analysis of our method, and we show, using recent results from the Markov chain literature, that its error can be explicitly bounded with constants which depend polynomially on the dimension even in the nonconvex setting. This includes the case where the constraints are defined via a differentiable neural network. Finally, we present an extensive experimental study of the model, including a comparison with state-of-the-art methods.},
  archive      = {J_SIMODS},
  author       = {Valentin De Bortoli and Agnès Desolneux and Alain Durmus and Bruno Galerne and Arthur Leclaire},
  doi          = {10.1137/19M1307731},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {52-82},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Maximum entropy methods for texture synthesis: Theory and practice},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Admissibility of solution estimators for stochastic
optimization. <em>SIMODS</em>, <em>3</em>(1), 31–51. (<a
href="https://doi.org/10.1137/19M1291546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We look at stochastic optimization problems through the lens of statistical decision theory. In particular, we address admissibility, in the statistical decision theory sense, of the natural sample average estimator for stochastic optimization problems (which is also known as the empirical risk minimization rule in learning literature). It is well known that for some simple stochastic optimization problems, the sample average estimator may not be admissible. This is known as Stein&#39;s paradox in the statistics literature. We show in this paper that for optimizing stochastic linear functions over compact sets, the sample average estimator is admissible. We also study problems with convex quadratic objectives subject to box constraints. Stein&#39;s paradox holds when there are no constraints and the dimension of the problem is at least three. We show that in the presence of box constraints, admissibility is recovered for dimensions 3 and 4.},
  archive      = {J_SIMODS},
  author       = {Amitabh Basu and Tu Nguyen and Ao Sun},
  doi          = {10.1137/19M1291546},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {31-51},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Admissibility of solution estimators for stochastic optimization},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistency of archetypal analysis. <em>SIMODS</em>,
<em>3</em>(1), 1–30. (<a
href="https://doi.org/10.1137/20M1331792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Archetypal analysis is an unsupervised learning method that uses a convex polytope to summarize multivariate data. For fixed $k$, the method finds a convex polytope with $k$ vertices, called archetype points, such that the polytope is contained in the convex hull of the data and the mean squared distance between the data and the polytope is minimal. In this paper, we prove a consistency result that shows if the data is independently sampled from a probability measure with bounded support, then the archetype points converge to a solution of the continuum version of the problem, of which we identify and establish several properties. We also obtain the convergence rate of the optimal objective values under appropriate assumptions on the distribution. If the data is independently sampled from a distribution with unbounded support, we also prove a consistency result for a modified method that penalizes the dispersion of the archetype points. Our analysis is supported by detailed computational experiments of the archetype points for data sampled from the uniform distribution in a disk, the normal distribution, an annular distribution, and a Gaussian mixture model.},
  archive      = {J_SIMODS},
  author       = {Braxton Osting and Dong Wang and Yiming Xu and Dominique Zosso},
  doi          = {10.1137/20M1331792},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {1-30},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Consistency of archetypal analysis},
  volume       = {3},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
