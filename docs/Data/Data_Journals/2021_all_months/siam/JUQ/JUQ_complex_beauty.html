<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq---55">JUQ - 55</h2>
<ul>
<li><details>
<summary>
(2021). A trade-off between explorations and repetitions for
estimators of two global sensitivity indices in stochastic models
induced by probability measures. <em>JUQ</em>, <em>9</em>(4), 1673–1713.
(<a href="https://doi.org/10.1137/19M1272706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sobol sensitivity indices assess how the output of a given mathematical model is sensitive to its inputs. If the model is stochastic, then it cannot be represented as a function of the inputs, thus raising questions about how to do a sensitivity analysis in those models. Practitioners have been using an approach that exploits the availability of methods for deterministic models. For each input, the stochastic model is repeated and the outputs are averaged. These averages are seen as if they came from a deterministic model and hence Sobol&#39;s method can be used. We show that the estimator so obtained is asymptotically biased if the number of repetitions goes to infinity too slowly. With limited computational resources, the number of repetitions of the stochastic model and the number of explorations of the input space cannot be large together and hence some balance must be found. We find the pair of numbers that minimizes a bound on some rank-based error criterion, penalizing bad rankings of the inputs&#39; sensitivities. Also, under minimal distributional assumptions, we derive a functional relationship between the output, the input, and some random noise; the Sobol--Hoeffding decomposition can be applied to it to define a new sensitivity index, which asymptotically is estimated without bias even though the number of repetitions remains fixed. The theory is illustrated on numerical experiments.},
  archive      = {J_JUQ},
  author       = {Gildas Mazo},
  doi          = {10.1137/19M1272706},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1673-1713},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A trade-off between explorations and repetitions for estimators of two global sensitivity indices in stochastic models induced by probability measures},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic gradients for fast calibration of differential
equation models. <em>JUQ</em>, <em>9</em>(4), 1643–1672. (<a
href="https://doi.org/10.1137/20M1364424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration of large-scale differential equation models to observational or experimental data is a widespread challenge throughout applied sciences and engineering. A crucial bottleneck in state-of-the-art calibration methods is the calculation of local sensitivities, i.e., derivatives of the loss function with respect to the estimated parameters, which often necessitates several numerical solves of the underlying system of partial or ordinary differential equations. In this paper we present a new probabilistic approach to computing local sensitivities. The proposed method has several advantages over classical methods. First, it operates within a constrained computational budget and provides a probabilistic quantification of uncertainty incurred in the sensitivities from this constraint. Second, information from previous sensitivity estimates can be recycled in subsequent computations, reducing the overall computational effort for iterative gradient-based calibration methods. The methodology presented is applied to two challenging test problems and compared against classical methods.},
  archive      = {J_JUQ},
  author       = {Jonathan Cockayne and Andrew Duncan},
  doi          = {10.1137/20M1364424},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1643-1672},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Probabilistic gradients for fast calibration of differential equation models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linked gaussian process emulation for systems of computer
models using matérn kernels and adaptive design. <em>JUQ</em>,
<em>9</em>(4), 1615–1642. (<a
href="https://doi.org/10.1137/20M1323771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art linked Gaussian process offers a way to build analytical emulators for systems of computer models. We generalize the closed form expressions for the linked Gaussian process under the squared exponential kernel to a class of Matérn kernels that are essential in advanced applications. An iterative procedure to construct linked Gaussian processes as surrogate models for any feed-forward systems of computer models is presented and illustrated on a feed-back coupled satellite system. We also introduce an adaptive design algorithm that could increase the approximation accuracy of linked Gaussian process surrogates with reduced computational costs on running expensive computer systems, by allocating runs and refining emulators of individual submodels based on their heterogeneous functional complexity.},
  archive      = {J_JUQ},
  author       = {Deyu Ming and Serge Guillas},
  doi          = {10.1137/20M1323771},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1615-1642},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Linked gaussian process emulation for systems of computer models using matérn kernels and adaptive design},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reproducing kernel hilbert spaces, polynomials, and the
classical moment problem. <em>JUQ</em>, <em>9</em>(4), 1589–1614. (<a
href="https://doi.org/10.1137/21M1394965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that polynomials do not belong to the reproducing kernel Hilbert space of infinitely differentiable translation-invariant kernels whose spectral measures have moments corresponding to a determinate moment problem. Our proof is based on relating this question to the problem of best linear estimation in continuous time one-parameter regression models with a stationary error process defined by the kernel. In particular, we show that the existence of a sequence of estimators with variances converging to 0 implies that the regression function cannot be an element of the reproducing kernel Hilbert space. This question is then related to the determinacy of the Hamburger moment problem for the spectral measure corresponding to the kernel. In the literature it was observed that a nonvanishing constant function does not belong to the reproducing kernel Hilbert space associated with the Gaussian kernel. Our results provide a unifying view of this phenomenon and show that the mentioned result can be extended for arbitrary polynomials and a broad class of translation-invariant kernels.},
  archive      = {J_JUQ},
  author       = {Holger Dette and Anatoly A. Zhigljavsky},
  doi          = {10.1137/21M1394965},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1589-1614},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Reproducing kernel hilbert spaces, polynomials, and the classical moment problem},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the saturation phenomenon of stochastic gradient descent
for linear inverse problems. <em>JUQ</em>, <em>9</em>(4), 1553–1588. (<a
href="https://doi.org/10.1137/20M1374456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) is a promising method for solving large-scale inverse problems due to its excellent scalability with respect to data size. The current mathematical theory in the lens of regularization theory predicts that SGD with a polynomially decaying stepsize schedule may suffer from an undesirable saturation phenomenon; i.e., the convergence rate does not further improve with the solution regularity index when it is beyond a certain range. In this work, we present a refined convergence rate analysis of SGD and prove that saturation actually does not occur if the initial stepsize of the schedule is sufficiently small. Several numerical experiments are provided to complement the analysis.},
  archive      = {J_JUQ},
  author       = {Bangti Jin and Zehui Zhou and Jun Zou},
  doi          = {10.1137/20M1374456},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1553-1588},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the saturation phenomenon of stochastic gradient descent for linear inverse problems},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A nonparametric bayesian framework for uncertainty
quantification in stochastic simulation. <em>JUQ</em>, <em>9</em>(4),
1527–1552. (<a href="https://doi.org/10.1137/20M1345517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When we use simulation to assess the performance of stochastic systems, the input models used to drive simulation experiments are often estimated from finite real-world data. There exist both input model and simulation estimation uncertainties in the system performance estimates. Without strong prior information on the input models and the system mean response surface, in this paper, we propose a Bayesian nonparametric framework to quantify the impact from both sources of uncertainty. Specifically, since the real-world data often represent the variability caused by various latent sources of uncertainty, Dirichlet process mixture (DPM) based nonparametric input models are introduced to model a mixture of heterogeneous distributions, which can faithfully capture the important features of real-world data, such as multimodality and skewness. Bayesian posteriors of flexible input models characterize the input model estimation uncertainty, which automatically accounts for both model selection and parameter value uncertainty. Then input model estimation uncertainty is propagated to outputs by using direct simulation. Thus, under very general conditions, our framework delivers an empirical credible interval accounting for both input and simulation uncertainties. A variance decomposition is further developed to quantify the relative contributions from both sources of uncertainty. Our approach is supported by rigorous theoretical and empirical study.},
  archive      = {J_JUQ},
  author       = {Wei Xie and Cheng Li and Yuefeng Wu and Pu Zhang},
  doi          = {10.1137/20M1345517},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1527-1552},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A nonparametric bayesian framework for uncertainty quantification in stochastic simulation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stability of gibbs posteriors from the wasserstein loss for
bayesian full waveform inversion. <em>JUQ</em>, <em>9</em>(4),
1499–1526. (<a href="https://doi.org/10.1137/20M1334218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the Wasserstein loss function has been proven to be effective when applied to deterministic full-waveform inversion (FWI) problems. We consider the application of this loss function in Bayesian FWI so that the uncertainty can be captured in the solution. Other loss functions that are commonly used in practice are also considered for comparison. Existence and stability of the resulting Gibbs posteriors are shown on function space under weak assumptions on the prior and model. In particular, the distribution arising from the Wasserstein loss is shown to be quite stable with respect to high-frequency noise in the data. We then illustrate the difference between the resulting distributions numerically, using Laplace approximations to estimate the unknown velocity field and uncertainty associated with the estimates.},
  archive      = {J_JUQ},
  author       = {Matthew M. Dunlop and Yunan Yang},
  doi          = {10.1137/20M1334218},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1499-1526},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stability of gibbs posteriors from the wasserstein loss for bayesian full waveform inversion},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty quantification for markov random fields.
<em>JUQ</em>, <em>9</em>(4), 1457–1498. (<a
href="https://doi.org/10.1137/20M1374614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an information-based uncertainty quantification method for general Markov random fields (MRFs). MRFs are structured, probabilistic graphical models over undirected graphs and provide a fundamental unifying modeling tool for statistical mechanics, probabilistic machine learning, and artificial intelligence. Typically, MRFs are complex and high-dimensional with nodes and edges (connections) built in a modular fashion from simpler, low-dimensional probabilistic models and their local connections; in turn, this modularity allows one to incorporate available data to MRFs and efficiently simulate them by leveraging their graph-theoretic structure. Learning graphical models from data and/or constructing them from physical modeling and constraints necessarily involves uncertainties inherited from data, modeling choices, or numerical approximations. These uncertainties in the MRF can be manifested either in the graph structure or the probability distribution functions and necessarily will propagate in predictions for quantities of interest. Here we quantify such uncertainties using tight, information-based bounds on the predictions of quantities of interest; these bounds take advantage of the graphical structure of MRFs and are capable of handling the inherent high dimensionality of such graphical models. We demonstrate our methods in MRFs for medical diagnostics and statistical mechanics models. In the latter, we develop uncertainty quantification bounds for finite-size effects and phase diagrams, which constitute two of the typical prediction goals of statistical mechanics modeling.},
  archive      = {J_JUQ},
  author       = {Panagiota Birmpa and Markos A. Katsoulakis},
  doi          = {10.1137/20M1374614},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1457-1498},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification for markov random fields},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instances of computational optimal recovery: Dealing with
observation errors. <em>JUQ</em>, <em>9</em>(4), 1438–1456. (<a
href="https://doi.org/10.1137/20M1328476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When attempting to recover functions from observational data, one naturally seeks to do so in an optimal manner with respect to some modeling assumption. With a focus put on the worst-case setting, this is the standard goal of optimal recovery. The distinctive twists here are the consideration of inaccurate data through some boundedness models and the emphasis on computational realization. Several scenarios are unraveled through the efficient constructions of optimal recovery maps: local optimality under linearly or semidefinitely describable models, global optimality for the estimation of linear functionals under approximability models, and global near-optimality under approximability models in the space of continuous functions.},
  archive      = {J_JUQ},
  author       = {Mahmood Ettehad and Simon Foucart},
  doi          = {10.1137/20M1328476},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1438-1456},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Instances of computational optimal recovery: Dealing with observation errors},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing shapley effects for sensitivity analysis.
<em>JUQ</em>, <em>9</em>(4), 1411–1437. (<a
href="https://doi.org/10.1137/19M1304738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapley effects are attracting increasing attention as sensitivity measures. When the value function is the conditional variance, they account for the individual and higher order effects of a model input. They are also well defined under model input dependence. However, one of the issues associated with their use is computational cost. We present new algorithms that offer major improvements for the computation of Shapley effects, reducing computational burden by several orders of magnitude (from $k! \cdot k$ to $2^k$, where $k$ is the number of inputs) with respect to currently available implementations. These algorithms work in the presence of input dependencies. With these new algorithms, one may estimate all generalized (Shapley--Owen) effects for interactions.},
  archive      = {J_JUQ},
  author       = {Elmar Plischke and Giovanni Rabitti and Emanuele Borgonovo},
  doi          = {10.1137/19M1304738},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1411-1437},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Computing shapley effects for sensitivity analysis},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Taylor approximation for chance constrained optimization
problems governed by partial differential equations with
high-dimensional random parameters. <em>JUQ</em>, <em>9</em>(4),
1381–1410. (<a href="https://doi.org/10.1137/20M1381381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast and scalable optimization method to solve chance or probabilistic constrained optimization problems governed by partial differential equations (PDEs) with high-dimensional random parameters. To address the critical computational challenges of expensive PDE solution and high-dimensional uncertainty, we construct surrogates of the constraint function by Taylor approximation, which relies on efficient computation of the derivatives, low-rank approximation of the Hessian, and a randomized algorithm for eigenvalue decomposition. To tackle the difficulty of the nondifferentiability of the inequality chance constraint, we use a smooth approximation of the discontinuous indicator function involved in the chance constraint, and we apply a penalty method to transform the inequality constrained optimization problem to an unconstrained one. Moreover, we design a gradient-based optimization scheme that gradually increases smoothing and penalty parameters to achieve convergence, for which we present an efficient computation of the gradient of the approximate cost functional by the Taylor approximation. Based on numerical experiments for a problem in optimal groundwater management, we demonstrate the accuracy of the Taylor approximation, its ability to greatly accelerate constraint evaluations, the convergence of the continuation optimization scheme, and the scalability of the proposed method in terms of the number of PDE solves with increasing random parameter dimension from one thousand to hundreds of thousands.},
  archive      = {J_JUQ},
  author       = {Peng Chen and Omar Ghattas},
  doi          = {10.1137/20M1381381},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1381-1410},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Taylor approximation for chance constrained optimization problems governed by partial differential equations with high-dimensional random parameters},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emulation of stochastic simulators using generalized lambda
models. <em>JUQ</em>, <em>9</em>(4), 1345–1380. (<a
href="https://doi.org/10.1137/20M1337302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic simulators are ubiquitous in many fields of applied sciences and engineering. In the context of uncertainty quantification and optimization, a large number of simulations is usually necessary, which becomes intractable for high-fidelity models. Thus surrogate models of stochastic simulators have been intensively investigated in the last decade. In this paper, we present a novel approach to surrogating the response distribution of a stochastic simulator which uses generalized lambda distributions, whose parameters are represented by polynomial chaos expansions of the model inputs. As opposed to most existing approaches, this new method does not require replicated runs of the simulator at each point of the experimental design. We propose a new fitting procedure which combines maximum conditional likelihood estimation with (modified) feasible generalized least-squares. We compare our method with state-of-the-art nonparametric kernel estimation on four different applications stemming from mathematical finance and epidemiology. Its performance is illustrated in terms of the accuracy of both the mean/variance of the stochastic simulator and the response distribution. As the proposed approach can also be used with experimental designs containing replications, we carry out a comparison on two of the examples, showing that replications do not necessarily help to get a better overall accuracy and may even worsen the results (at a fixed total number of runs of the simulator).},
  archive      = {J_JUQ},
  author       = {Xujia Zhu and Bruno Sudret},
  doi          = {10.1137/20M1337302},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1345-1380},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Emulation of stochastic simulators using generalized lambda models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GAN-based priors for quantifying uncertainty in supervised
learning. <em>JUQ</em>, <em>9</em>(3), 1314–1343. (<a
href="https://doi.org/10.1137/20M1354210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and deep learning algorithms are frequently used in critical tasks where their output is used in high-stakes downstream applications. In these cases it is important to quantify the uncertainty in the predictions of these algorithms. Motivated by this, we present a novel learning-based Bayesian inference approach for quantifying uncertainty in a prediction. Our approach uses samples drawn from the joint distribution of measurements and predictions to train a generative adversarial network (GAN). Thereafter, given a noisy measurement, we use the generator component of the GAN as a prior in a Bayesian update. By reformulating the resulting high-dimensional posterior sampling problem to the low-dimensional latent space of the GAN, we are able to perform efficient Markov Chain Monte Carlo (MCMC) updates. We apply this approach to image classification and image inpainting problems in computer vision and to forward and inverse uncertainty quantification tasks arising in computational physics, and demonstrate how the ability to quantify uncertainty can be used to (a) detect samples that lie outside the distribution of the training samples, (b) quantify the confidence in the prediction, and (c) determine the subsequent measurement within an active learning strategy.},
  archive      = {J_JUQ},
  author       = {Dhruv V. Patel and Assad A. Oberai},
  doi          = {10.1137/20M1354210},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1314-1343},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {GAN-based priors for quantifying uncertainty in supervised learning},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lagrangian uncertainty quantification and information
inequalities for stochastic flows. <em>JUQ</em>, <em>9</em>(3),
1242–1313. (<a href="https://doi.org/10.1137/19M1263133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a systematic information-theoretic framework for quantification and mitigation of error in probabilistic Lagrangian (i.e., path-based) predictions which are obtained from dynamical systems generated by uncertain (Eulerian) vector fields. This work is motivated by the desire to improve Lagrangian predictions in complex dynamical systems based either on analytically simplified or data-driven models. We derive a hierarchy of general information bounds on uncertainty in estimates of statistical observables $\mathbb{E}^{\nu}[f]$, evaluated on trajectories of the approximating dynamical system, relative to the “true&quot; observables $\mathbb{E}^{\mu}[f]$ in terms of certain $\varphi$-divergences, $\mathcal{D}_{\varphi}(\mu\|\nu)$, which quantify discrepancies between probability measures $\mu$ associated with the original dynamics and their approximations $\nu$. We then derive two distinct bounds on $\mathcal{D}_{\varphi}(\mu\|\nu)$ itself in terms of the Eulerian fields.},
  archive      = {J_JUQ},
  author       = {Michal Branicki and Kenneth Uda},
  doi          = {10.1137/19M1263133},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1242-1313},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Lagrangian uncertainty quantification and information inequalities for stochastic flows},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Importance sampling for pathwise sensitivity of stochastic
chaotic systems. <em>JUQ</em>, <em>9</em>(3), 1217–1241. (<a
href="https://doi.org/10.1137/20M1352454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new pathwise sensitivity estimator for chaotic SDEs. By introducing a spring term between the original and perturbated SDEs, we derive a new estimator by importance sampling. The variance of the new estimator increases only linearly in time T compared with the exponential increase of the standard pathwise estimator. We compare our estimator with the Malliavin estimator and extend both of them to the multilevel Monte Carlo method, which further improves the computational efficiency. Finally, we also consider using this estimator for the SDE with small volatility to approximate the sensitivities of the invariant measure of chaotic ODEs. Furthermore, Richardson--Romberg extrapolation on the volatility parameter gives a more accurate and efficient estimator. Numerical experiments support our analysis.},
  archive      = {J_JUQ},
  author       = {Wei Fang and Mike B. Giles},
  doi          = {10.1137/20M1352454},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1217-1241},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Importance sampling for pathwise sensitivity of stochastic chaotic systems},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-level a posteriori error estimation for adaptive
multilevel stochastic galerkin finite element method. <em>JUQ</em>,
<em>9</em>(3), 1184–1216. (<a
href="https://doi.org/10.1137/20M1342586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper considers a class of parametric elliptic partial differential equations (PDEs), where the coefficients and the right-hand side function depend on infinitely many (uncertain) parameters. We introduce a two-level a posteriori estimator to control the energy error in multilevel stochastic Galerkin approximations for this class of PDE problems. We prove that the two-level estimator always provides a lower bound for the unknown approximation error, while the upper bound is equivalent to a saturation assumption. We propose and empirically compare three adaptive algorithms, where the structure of the estimator is exploited to perform spatial refinement as well as parametric enrichment. The paper also discusses implementation aspects of computing multilevel stochastic Galerkin approximations.},
  archive      = {J_JUQ},
  author       = {Alex Bespalov and Dirk Praetorius and Michele Ruggeri},
  doi          = {10.1137/20M1342586},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1184-1216},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Two-level a posteriori error estimation for adaptive multilevel stochastic galerkin finite element method},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying truncation-related uncertainties in unsteady
fluid dynamics reduced order models. <em>JUQ</em>, <em>9</em>(3),
1152–1183. (<a href="https://doi.org/10.1137/19M1354819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new method to quantify the uncertainty introduced by the drastic dimensionality reduction commonly practiced in the field of computational fluid dynamics, the ultimate goal being to simulate accurate priors for real-time data assimilation. Our key ingredient is a stochastic Navier--Stokes closure mechanism that arises by assuming random unresolved flow components. This decomposition is carried out through Galerkin projection with a proper orthogonal decomposition (POD-Galerkin) basis. The residual velocity fields, model structure, and evolution of coefficients of the reduced order&#39;s solutions are used to compute the resulting multiplicative and additive noise&#39;s correlations. The low computational cost of these consistent correlation estimators makes them applicable to the study of complex fluid flows. This stochastic POD-reduced order model (POD-ROM) is applied to 2-dimensional and 3-dimensional direct numerical simulations of wake flows at Reynolds 100 and 300, respectively, with uncertainty quantification and forecasting outside the learning interval being the main focus. The proposed stochastic POD-ROM approach is shown to stabilize the unstable temporal coefficients and to maintain their variability under control, while exhibiting an impressively accurate predictive capability.},
  archive      = {J_JUQ},
  author       = {Valentin Resseguier and Agustin M. Picard and Etienne Memin and Bertrand Chapron},
  doi          = {10.1137/19M1354819},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1152-1183},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quantifying truncation-related uncertainties in unsteady fluid dynamics reduced order models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian linear approximation for the estimation of the
shapley effects. <em>JUQ</em>, <em>9</em>(3), 1132–1151. (<a
href="https://doi.org/10.1137/20M1342884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the estimation of sensitivity indices called “Shapley effects.” These sensitivity indices enable one to handle dependent input variables. The Shapley effects are generally difficult to estimate, but they are easily computable in the Gaussian linear framework. The aim of this work is to use the values of the Shapley effects in an approximated Gaussian linear framework as estimators of the true Shapley effects corresponding to a nonlinear model. First, we consider Gaussian input variables with small variances. We provide rates of convergence of the estimated Shapley effects to the true Shapley effects. Then, we focus on the case where the inputs are given by a non-Gaussian empirical mean. We prove that, under some mild assumptions, when the number of terms in the empirical mean increases, the difference between the true Shapley effects and the estimated Shapley effects given by the Gaussian linear approximation converges to 0. Our theoretical results are supported by numerical studies, showing that the Gaussian linear approximation is accurate and enables one to decrease the computational time significantly.},
  archive      = {J_JUQ},
  author       = {Baptiste Broto and François Bachoc and Marine Depecker and Jean-Marc Martinez},
  doi          = {10.1137/20M1342884},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1132-1151},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Gaussian linear approximation for the estimation of the shapley effects},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A higher order unscented transform. <em>JUQ</em>,
<em>9</em>(3), 1094–1131. (<a
href="https://doi.org/10.1137/20M135546X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new approach for estimating the expected values of nonlinear functions applied to multivariate random variables with arbitrary distributions. Rather than assuming a particular distribution, we assume that we are only given the first four moments of the distribution. The goal is to efficiently represent the distribution using a small number of quadrature nodes which are called $\sigma$-points. What we mean by this is choosing nodes and weights in order to match the specified moments of the distribution. The classical scaled unscented transform (SUT) matches the mean and covariance of a distribution. In this paper, we introduce the higher order unscented transform (HOUT), which also matches any given skewness and kurtosis tensors. It turns out that the key to matching the higher moments is the tensor CANDECOMP/PARAFAC (CP) decomposition. While the minimal CP decomposition is NP-complete, we present a practical algorithm for computing a nonminimal CP decomposition and prove convergence in linear time. We then show how to combine the CP decompositions of the moments in order to form the $\sigma$-points and weights of the HOUT. By passing the $\sigma$-points through a nonlinear function and applying our quadrature rule we can estimate the moments of the output distribution. We prove that the HOUT is exact on arbitrary polynomials up to fourth order and derive error bounds in terms of the regularity of the function and the decay of the probability. Finally, we numerically compare the HOUT to the SUT on nonlinear functions applied to non-Gaussian random variables including an application to forecasting and uncertainty quantification for chaotic dynamics.},
  archive      = {J_JUQ},
  author       = {Deanna C. Easley and Tyrus Berry},
  doi          = {10.1137/20M135546X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1094-1131},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A higher order unscented transform},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Post hoc uncertainty quantification for remote sensing
observing systems. <em>JUQ</em>, <em>9</em>(3), 1064–1093. (<a
href="https://doi.org/10.1137/19M1304283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article sets forth a practical methodology for uncertainty quantification of physical state estimates derived from remote sensing observing systems. Remote sensing instruments observe parts of the electromagnetic spectrum and use computational algorithms to infer the underlying true physical states. In current practice, many sources of uncertainty are not accounted for in this process, leading to underestimates of uncertainties on quantities of interest. We propose a procedure that combines Monte Carlo simulation experiments with statistical modeling to approximate distributions of unknown true states given point estimates of those states. Our method is carried out post hoc, that is, after the operational processing step. We demonstrate the procedure using four months of data from NASA&#39;s Orbiting Carbon Observatory-2 mission and compare it to validation measurements from the Total Column Carbon Observing Network.},
  archive      = {J_JUQ},
  author       = {Amy Braverman and Jonathan Hobbs and Joaquim Teixeira and Michael Gunson},
  doi          = {10.1137/19M1304283},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1064-1093},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Post hoc uncertainty quantification for remote sensing observing systems},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semiautomatic method for history matching using sequential
monte carlo. <em>JUQ</em>, <em>9</em>(3), 1034–1063. (<a
href="https://doi.org/10.1137/19M1286694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the history matching method is to locate nonimplausible regions of the parameter space of complex deterministic or stochastic models by matching model outputs with data. It does this via a series of waves where at each wave an emulator is fitted to a small number of training samples. An implausibility measure is defined which takes into account the closeness of simulated and observed outputs as well as emulator uncertainty. As the waves progress, the emulator becomes more accurate so that training samples are more concentrated on promising regions of the space and poorer parts of the space are rejected with more confidence. While history matching has proved to be useful, existing implementations are not fully automated, and some ad hoc choices are made during the process, which involves user intervention and is time consuming. This occurs especially when the nonimplausible region becomes small and it is difficult to sample this space uniformly to generate new training points. In this article we develop a sequential Monte Carlo (SMC) algorithm for implementing history matching that is semiautomated. Our novel SMC approach reveals that the history matching method yields a nonimplausible region that can be multimodal, highly irregular, and very difficult to sample uniformly. Our SMC approach offers a much more reliable sampling of the nonimplausible space, which requires additional computation compared to other approaches used in the literature.},
  archive      = {J_JUQ},
  author       = {Christopher Drovandi and David J. Nott and Daniel E. Pagendam},
  doi          = {10.1137/19M1286694},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1034-1063},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A semiautomatic method for history matching using sequential monte carlo},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Numerical approximation of optimal convergence for
fractional elliptic equations with additive fractional gaussian noise.
<em>JUQ</em>, <em>9</em>(3), 1013–1033. (<a
href="https://doi.org/10.1137/20M1374286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study numerical approximation for one-dimensional stochastic elliptic equations with integral fractional Laplacian and the additive Gaussian noise of power-law: $1/f^\beta$ noise and fractional Brownian noise. We present an optimal convergence of our method using spectral expansions of noises. We first establish the well-posedness of a corresponding deterministic problem and show the stability of solutions for the rough data via negative norms in weighted Sobolev spaces. We also analyze the regularity of the noise and approximation properties of their finite truncations. Next, we show the optimal error estimates of our method for a wide range of parameters in the order of fractional operator and the fractional Gaussian noise. Finally, we present several numerical examples to illustrate the mean-square convergence orders and verify our optimal convergence rates.},
  archive      = {J_JUQ},
  author       = {Zhaopeng Hao and Zhongqiang Zhang},
  doi          = {10.1137/20M1374286},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1013-1033},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Numerical approximation of optimal convergence for fractional elliptic equations with additive fractional gaussian noise},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PDE-constrained optimal control problems with uncertain
parameters using SAGA. <em>JUQ</em>, <em>9</em>(3), 979–1012. (<a
href="https://doi.org/10.1137/18M1224076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an optimal control problem (OCP) for a partial differential equation (PDE) with random coefficients. The optimal control function is a deterministic, distributed forcing term that minimizes an expected quadratic regularized loss functional. For the numerical approximation of this PDE-constrained OCP, we replace the expectation in the objective functional by a suitable quadrature formula and, eventually, discretize the PDE by a Galerkin method. To practically solve such an approximate OCP, we propose an importance sampling version the SAGA algorithm [A. Defazio, F. Bach, and S. Lacoste-Julien, Advances in Neural Information Processing Systems 27, Curran Associates, Red Hook, NY, 2014, pp. 1646--1654], a type of stochastic gradient algorithm with a fixed-length memory term, which computes at each iteration the gradient of the loss functional in only one quadrature point, randomly chosen from a possibly nonuniform distribution. We provide a full error and complexity analysis of the proposed numerical scheme. In particular we compare the complexity of the generalized SAGA algorithm with importance sampling, with that of the stochastic gradient and the conjugate gradient (CG) algorithms, applied to the same discretized OCP. We show that SAGA converges exponentially in the number of iterations as for a CG algorithm and has a similar asymptotic computational complexity, in terms of computational cost versus accuracy. Moreover, it features good preasymptotic properties, as shown by our numerical experiments, which makes it appealing in a limited budget context.},
  archive      = {J_JUQ},
  author       = {Matthieu Martin and Fabio Nobile},
  doi          = {10.1137/18M1224076},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {979-1012},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {PDE-constrained optimal control problems with uncertain parameters using SAGA},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic analysis of multilevel best linear unbiased
estimators. <em>JUQ</em>, <em>9</em>(3), 953–978. (<a
href="https://doi.org/10.1137/20M1321607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the computational complexity and variance of multilevel best linear unbiased estimators introduced in [D. Schaden and E. Ullmann, SIAM/ASA J. Uncertain. Quantif., 8 (2020), pp. 601--635]. We specialize the results in this work to PDE-based models that are parameterized by a discretization quantity, e.g., the finite element mesh size. In particular, we investigate the asymptotic complexity of the so-called sample allocation optimal best linear unbiased estimators (SAOBs). These estimators have the smallest variance given a fixed computational budget. However, SAOBs are defined implicitly by solving an optimization problem and are difficult to analyze. Alternatively, we study a class of auxiliary estimators based on the Richardson extrapolation of the parametric model family. This allows us to provide an upper bound for the complexity of the SAOBs, showing that their complexity is optimal within a certain class of linear unbiased estimators. Moreover, the complexity of the SAOBs is not larger than the complexity of multilevel Monte Carlo. The theoretical results are illustrated by numerical experiments with an elliptic PDE.},
  archive      = {J_JUQ},
  author       = {Daniel Schaden and Elisabeth Ullmann},
  doi          = {10.1137/20M1321607},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {953-978},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Asymptotic analysis of multilevel best linear unbiased estimators},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A convex optimization framework for the inverse problem of
identifying a random parameter in a stochastic partial differential
equation. <em>JUQ</em>, <em>9</em>(2), 922–952. (<a
href="https://doi.org/10.1137/20M1323953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of this work is to study the inverse problem of identifying a stochastic parameter in partial differential equations with random data. In the framework of stochastic Sobolev spaces, we prove the Lipschitz continuity and the differentiability of the parameter-to-solution map and provide a new derivative characterization. We introduce a new energy-norm based modified output least-squares (OLS) objective functional and prove its smoothness and convexity. For stable inversion, we develop a regularization framework and prove an existence result for the regularized stochastic optimization problem. We also consider the OLS based stochastic optimization problem and provide an adjoint approach to compute the derivative of the OLS-functional. In the finite-dimensional noise setting, we give a parameterization of the inverse problem. We develop a computational framework by using the stochastic Galerkin discretization scheme and derive explicit discrete formulas for the considered objective functionals and their gradient. We provide detailed computational results to illustrate the feasibility and efficacy of the developed inversion framework. Encouraging numerical results demonstrate some of the advantages of the new framework over the existing approaches.},
  archive      = {J_JUQ},
  author       = {Baasansuren Jadamba and Akhtar A. Khan and Miguel Sama and Hans-Jorg Starkloff and Christiane Tammer},
  doi          = {10.1137/20M1323953},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {922-952},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A convex optimization framework for the inverse problem of identifying a random parameter in a stochastic partial differential equation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global sensitivity analysis and wasserstein spaces.
<em>JUQ</em>, <em>9</em>(2), 880–921. (<a
href="https://doi.org/10.1137/20M1354957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensitivity indices are commonly used to quantify the relative influence of any specific group of input variables on the output of a computer code. In this paper, we focus both on computer codes, the output of which is a cumulative distribution function, and on stochastic computer codes. We propose a way to perform a global sensitivity analysis for these kinds of computer codes. In the first setting, we define two indices: the first one is based on Wasserstein Fréchet means, while the second one is based on the Hoeffding decomposition of the indicators of Wasserstein balls. Further, when dealing with the stochastic computer codes, we define an “ideal version” of the stochastic computer code that fits into the framework of the first setting. Finally, we deduce a procedure to realize a second level global sensitivity analysis, namely, when one is interested in the sensitivity related to the input distributions rather than in the sensitivity related to the inputs themselves. Several numerical studies are proposed as illustrations of the different settings.},
  archive      = {J_JUQ},
  author       = {Jean-Claude Fort and Thierry Klein and Agnès Lagnoux},
  doi          = {10.1137/20M1354957},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {880-921},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Global sensitivity analysis and wasserstein spaces},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear parabolic problems in random moving domains.
<em>JUQ</em>, <em>9</em>(2), 848–879. (<a
href="https://doi.org/10.1137/19M1284889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider linear parabolic equations on a random noncylindrical domain. Utilizing the domain mapping method, we write the problem as a partial differential equation with random coefficients on a cylindrical deterministic domain. We assume we are given a deterministic initial domain and a random velocity field. Exploiting the deterministic results concerning equations on noncylindrical domains, we state the necessary assumptions about the velocity field and, in addition, about the flow transformation that this field generates. In this paper we consider both cases, the uniformly bounded with respect to the sample and log-normal type transformation. In addition, we give an explicit example of a log-normal type transformation and prove that it does not satisfy the uniformly bounded condition. We define a general framework for considering linear parabolic problems on random non- cylindrical domains. As the first example, we consider the heat equation on a random tube domain and prove its well-posedness. Moreover, as the other example we consider the parabolic Stokes equation which illustrates the case when it is not enough just to study the plain-back transformation of the function, but instead to consider, for example, the Piola type transformation, in order to keep the divergence-free property.},
  archive      = {J_JUQ},
  author       = {Ana Djurdjevac},
  doi          = {10.1137/19M1284889},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {848-879},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Linear parabolic problems in random moving domains},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-entropy-based importance sampling with
failure-informed dimension reduction for rare event simulation.
<em>JUQ</em>, <em>9</em>(2), 818–847. (<a
href="https://doi.org/10.1137/20M1344585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of rare event or failure probabilities in high dimensions is of interest in many areas of science and technology. We consider problems where the rare event is expressed in terms of a computationally costly numerical model. Importance sampling with the cross-entropy method offers an efficient way to address such problems provided that a suitable parametric family of biasing densities is employed. Although some existing parametric distribution families are designed to perform efficiently in high dimensions, their applicability within the cross-entropy method is limited to problems with dimension of $\mathcal{O}(10^2)$. In this work, rather than directly building sampling densities in high dimensions, we focus on identifying the intrinsic low-dimensional structure of the rare event simulation problem. To this end, we exploit a connection between rare event simulation and Bayesian inverse problems. This allows us to adapt dimension reduction techniques from Bayesian inference to construct new, effectively low-dimensional, biasing distributions within the cross-entropy method. In particular, we employ the approach in [O. Zahm et al., preprint, arXiv:1807.03712v2, 2018], as it enables control of the error in the approximation of the optimal biasing distribution. We illustrate our method using two standard high-dimensional reliability benchmark problems and one structural mechanics application involving random fields.},
  archive      = {J_JUQ},
  author       = {Felipe Uribe and Iason Papaioannou and Youssef M. Marzouk and Daniel Straub},
  doi          = {10.1137/20M1344585},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {818-847},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Cross-entropy-based importance sampling with failure-informed dimension reduction for rare event simulation},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multifidelity approximate bayesian computation with
sequential monte carlo parameter sampling. <em>JUQ</em>, <em>9</em>(2),
788–817. (<a href="https://doi.org/10.1137/20M1316160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifidelity approximate Bayesian computation (MF-ABC) is a likelihood-free technique for parameter inference that exploits model approximations to significantly increase the speed of ABC algorithms [T. P. Prescott and R. E. Baker, SIAM/ASA J. Uncertain. Quantif., 8 (2020), pp. 114--138]. Previous work has considered MF-ABC only in the context of rejection sampling, which does not explore parameter space particularly efficiently. In this work, we integrate the multifidelity approach with the ABC sequential Monte Carlo (ABC-SMC) algorithm into a new MF-ABC-SMC algorithm. We show that the improvements generated by each of ABC-SMC and MF-ABC to the efficiency of generating Monte Carlo samples and estimates from the ABC posterior are amplified when the two techniques are used together.},
  archive      = {J_JUQ},
  author       = {Thomas P. Prescott and Ruth E. Baker},
  doi          = {10.1137/20M1316160},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {788-817},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multifidelity approximate bayesian computation with sequential monte carlo parameter sampling},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unbiased inference for discretely observed hidden markov
model diffusions. <em>JUQ</em>, <em>9</em>(2), 763–787. (<a
href="https://doi.org/10.1137/20M131549X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Bayesian inference method for diffusions observed discretely and with noise, which is free of discretization bias. Unlike existing unbiased inference methods, our method does not rely on exact simulation techniques. Instead, our method uses standard time-discretized approximations of diffusions, such as the Euler--Maruyama scheme. Our approach is based on particle marginal Metropolis--Hastings, a particle filter, randomized multilevel Monte Carlo, and an importance sampling type correction of approximate Markov chain Monte Carlo. The resulting estimator leads to inference without a bias from the time-discretization as the number of Markov chain iterations increases. We give convergence results and recommend allocations for algorithm inputs. Our method admits a straightforward parallelization and can be computationally efficient. The user-friendly approach is illustrated on three examples, where the underlying diffusion is an Ornstein--Uhlenbeck process, a geometric Brownian motion, and a $2d$ nonreversible Langevin equation.},
  archive      = {J_JUQ},
  author       = {Neil K. Chada and Jordan Franks and Ajay Jasra and Kody J. Law and Matti Vihola},
  doi          = {10.1137/20M131549X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {763-787},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Unbiased inference for discretely observed hidden markov model diffusions},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient computation of extreme excursion probabilities for
dynamical systems through rice’s formula. <em>JUQ</em>, <em>9</em>(2),
731–762. (<a href="https://doi.org/10.1137/19M1308177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel computational method for evaluating the extreme excursion probabilities arising from random initialization of nonlinear dynamical systems. The method uses a Markov chain Monte Carlo or a Laplace approximation approach to construct a biasing distribution that in turn is used in an importance sampling procedure to estimate the extreme excursion probabilities. The prior and likelihood of the biasing distribution are obtained by using Rice&#39;s formula from excursion probability theory. We use Gaussian mixture biasing distributions and approximate the non-Gaussian initial state distribution by the method of moments to circumvent the linearity and Gaussianity assumptions needed by excursion probability theory. The method requires the tangent linear model of the nonlinear dynamical system to be bounded. Additionally, our method works best when the uncertainty-induced trajectories of the nonlinear dynamical system are small perturbations around a mean linear trajectory and thus when the target system exhibits only mild nonlinearity. We demonstrate the effectiveness of this computational framework for nonlinear dynamical systems of up to 100 dimensions.},
  archive      = {J_JUQ},
  author       = {Vishwas Rao and Mihai Anitescu},
  doi          = {10.1137/19M1308177},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {731-762},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Efficient computation of extreme excursion probabilities for dynamical systems through rice&#39;s formula},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient estimation of the ANOVA mean dimension, with an
application to neural net classification. <em>JUQ</em>, <em>9</em>(2),
708–730. (<a href="https://doi.org/10.1137/20M1350236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean dimension of a black box function of $d$ variables is a convenient way to summarize the extent to which it is dominated by high or low order interactions. It is expressed in terms of $2^d-1$ variance components, but it can be written as the sum of $d$ Sobol&#39; indices that can be estimated by leave one out methods. We compare the variance of these leave one out methods: a Gibbs sampler called winding stairs, a radial sampler that changes each variable one at a time from a baseline, and a naive sampler that never reuses function evaluations and so costs about double the other methods. For an additive function the radial and winding stairs are most efficient. For a multiplicative function the naive method can easily be most efficient if the factors have high kurtosis. As an illustration we consider the mean dimension of a neural network classifier of digits from the MNIST data set. The classifier is a function of 784 pixels. For that problem, winding stairs is the best algorithm. We find that inputs to the final softmax layer have mean dimensions ranging from 1.35 to 2.0.},
  archive      = {J_JUQ},
  author       = {Christopher Hoyt and Art B. Owen},
  doi          = {10.1137/20M1350236},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {708-730},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Efficient estimation of the ANOVA mean dimension, with an application to neural net classification},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model error estimation using the expectation maximization
algorithm and a particle flow filter. <em>JUQ</em>, <em>9</em>(2),
681–707. (<a href="https://doi.org/10.1137/19M1297300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model error covariances play a central role in the performance of data assimilation methods applied to nonlinear state-space models. However, these covariances are largely unknown in most of the applications. A misspecification of the model error covariance has a strong impact on the computation of the posterior probability density function, leading to unreliable estimations and even to a total failure of the assimilation procedure. In this work, we propose the combination of the expectation maximization (EM) algorithm with an efficient particle filter to estimate the model error covariance using a batch of observations. Based on the EM algorithm principles, the proposed method encompasses two stages: the expectation stage, in which a particle filter is used with the present updated value of the model error covariance as given to find the probability density function that maximizes the likelihood, followed by a maximization stage, in which the expectation under the probability density function found in the expectation step is maximized as a function of the elements of the model error covariance. This novel algorithm here presented combines the EM algorithm with a fixed point algorithm and does not require a particle smoother to approximate the posterior densities. We demonstrate that the new method accurately and efficiently solves the linear model problem. Furthermore, for the chaotic nonlinear Lorenz-96 model the method is stable even for observation error covariance 10 times larger than the estimated model error covariance matrix and also is successful in moderately large dimensional situations where the dimension of the estimated matrix is 40 x 40.},
  archive      = {J_JUQ},
  author       = {María Magdalena Lucini and Peter Jan van Leeuwen and Manuel Pulido},
  doi          = {10.1137/19M1297300},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {681-707},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Model error estimation using the expectation maximization algorithm and a particle flow filter},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty quantification for the BGK model of the
boltzmann equation using multilevel variance reduced monte carlo
methods. <em>JUQ</em>, <em>9</em>(2), 650–680. (<a
href="https://doi.org/10.1137/20M1331846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a control variate multilevel Monte Carlo method for the kinetic Bhatnagar--Gross--Krook model of the Boltzmann equation subject to random inputs. The method combines a multilevel Monte Carlo technique with the computation of the optimal control variate multipliers derived from local or global variance minimization problems. Consistency and convergence analysis for the method equipped with a second-order positivity-preserving and asymptotic-preserving scheme in space and time is also performed. Various numerical examples confirm that the optimized multilevel Monte Carlo method outperforms the classical multilevel Monte Carlo method especially for problems with discontinuities.},
  archive      = {J_JUQ},
  author       = {Jingwei Hu and Lorenzo Pareschi and Yubo Wang},
  doi          = {10.1137/20M1331846},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {650-680},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification for the BGK model of the boltzmann equation using multilevel variance reduced monte carlo methods},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse polynomial chaos expansions: Literature survey and
benchmark. <em>JUQ</em>, <em>9</em>(2), 593–649. (<a
href="https://doi.org/10.1137/20M1315774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse polynomial chaos expansions (PCE) are a popular surrogate modelling method that takes advantage of the properties of PCE, the sparsity-of-effects principle, and powerful sparse regression solvers to approximate computer models with many input parameters, relying on only a few model evaluations. Within the last decade, a large number of algorithms for the computation of sparse PCE have been published in the applied math and engineering literature. We present an extensive review of the existing methods and develop a framework for classifying the algorithms. Furthermore, we conduct a unique benchmark on a selection of methods to identify which approaches work best in practical applications. Comparing their accuracy on several benchmark models of varying dimensionality and complexity, we find that the choice of sparse regression solver and sampling scheme for the computation of a sparse PCE surrogate can make a significant difference of up to several orders of magnitude in the resulting mean-squared error. Different methods seem to be superior in different regimes of model dimensionality and experimental design size.},
  archive      = {J_JUQ},
  author       = {Nora Lüthen and Stefano Marelli and Bruno Sudret},
  doi          = {10.1137/20M1315774},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {593-649},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sparse polynomial chaos expansions: Literature survey and benchmark},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Output-weighted optimal sampling for bayesian experimental
design and uncertainty quantification. <em>JUQ</em>, <em>9</em>(2),
564–592. (<a href="https://doi.org/10.1137/20M1347486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a class of acquisition functions for sample selection that lead to faster convergence in applications related to Bayesian experimental design and uncertainty quantification. The approach follows the paradigm of active learning, whereby existing samples of a black-box function are utilized to optimize the next most informative sample. The proposed method aims to take advantage of the fact that some input directions of the black-box function have a larger impact on the output than others, which is important especially for systems exhibiting rare and extreme events. The acquisition functions introduced in this work leverage the properties of the likelihood ratio, a quantity that acts as a probabilistic sampling weight and guides the active-learning algorithm toward regions of the input space that are deemed most relevant. We demonstrate the proposed approach in the uncertainty quantification of a hydrological system as well as the probabilistic quantification of rare events in dynamical systems and the identification of their precursors in up to 30 dimensions.},
  archive      = {J_JUQ},
  author       = {Antoine Blanchard and Themistoklis Sapsis},
  doi          = {10.1137/20M1347486},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {564-592},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Output-weighted optimal sampling for bayesian experimental design and uncertainty quantification},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lattice boltzmann method for stochastic convection-diffusion
equations. <em>JUQ</em>, <em>9</em>(2), 536–563. (<a
href="https://doi.org/10.1137/19M1270665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a lattice Boltzmann method (LBM) for stochastic convection-diffusion equations (CDEs). The stochastic Galerkin method is employed to transform the stochastic CDE into a system of deterministic CDEs and the LBM is then used to discretize the deterministic CDEs. The consistency of the method is shown with the Maxwell iteration. Thanks to the property that the diffusion coefficient matrix of the deterministic CDEs is positive definite, we prove the weighted $L^2$-stability of the LBM. With this stability, the convergence of the method can be directly established. Numerical experiments are conducted to verify the accuracy of the LBM and demonstrate its effectiveness for stochastic CDEs. The numerical results not only are in good agreement with those existing in the literature but also show the ability of the LBM for stochastic problems with complex boundaries.},
  archive      = {J_JUQ},
  author       = {Weifeng Zhao and Juntao Huang and Wen-An Yong},
  doi          = {10.1137/19M1270665},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {536-563},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Lattice boltzmann method for stochastic convection-diffusion equations},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variance reduction for dependent sequences with applications
to stochastic gradient MCMC. <em>JUQ</em>, <em>9</em>(2), 507–535. (<a
href="https://doi.org/10.1137/19M1301199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel and practical variance reduction approach for additive functionals of dependent sequences. Our approach combines the use of control variates with the minimization of an empirical variance estimate. We analyze finite sample properties of the proposed method and derive finite-time bounds of the excess asymptotic variance to zero. We apply our methodology to stochastic gradient Markov chain Monte Carlo (SGMCMC) methods for Bayesian inference on large data sets and combine it with existing variance reduction methods for SGMCMC. We present empirical results carried out on a number of benchmark examples showing that our variance reduction method achieves significant improvement as compared to state-of-the-art methods at the expense of a moderate increase of computational overhead.},
  archive      = {J_JUQ},
  author       = {Denis Belomestny and Leonid Iosipoi and Eric Moulines and Alexey Naumov and Sergey Samsonov},
  doi          = {10.1137/19M1301199},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {507-535},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Variance reduction for dependent sequences with applications to stochastic gradient MCMC},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust system design with limited experimental data and an
inexact simulation model. <em>JUQ</em>, <em>9</em>(2), 483–506. (<a
href="https://doi.org/10.1137/20M1316287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer simulations of physical systems are commonly used to improve system performance via optimizing the system design. We study the case when, in addition to the simulation, a limited amount of experimental data is collected from the real physical system. This article describes a method for selecting a conservative system design that is robust to uncertainty from the simulation parameters and simulation bias. The concept is that each potential system design is assigned a worst-case scenario in a data-driven feasible region. The conservative system design is then chosen as the best of the worst-cases. The method is shown to have good statistical properties. A case study is performed where a vehicle safety belt design is chosen to minimize the impact of vehicle crashes on a driver.},
  archive      = {J_JUQ},
  author       = {Wenbo Sun and Matthew Plumlee and Jingwen Hu and Jionghua (Judy) Jin},
  doi          = {10.1137/20M1316287},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {483-506},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Robust system design with limited experimental data and an inexact simulation model},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fokker–planck particle systems for bayesian inference:
Computational approaches. <em>JUQ</em>, <em>9</em>(2), 446–482. (<a
href="https://doi.org/10.1137/19M1303162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference can be embedded into an appropriately defined dynamics in the space of probability measures. In this paper, we take Brownian motion and its associated Fokker--Planck equation as a starting point for such embeddings and explore several interacting particle approximations. More specifically, we consider both deterministic and stochastic interacting particle systems and combine them with the idea of preconditioning by the empirical covariance matrix. In addition to leading to affine invariant formulations which asymptotically speed up convergence, preconditioning allows for gradient-free implementations in the spirit of the ensemble Kalman filter. While such gradient-free implementations have been demonstrated to work well for posterior measures that are nearly Gaussian, we extend their scope of applicability to multimodal measures by introducing localized gradient-free approximations. Numerical results demonstrate the effectiveness of the considered methodologies.},
  archive      = {J_JUQ},
  author       = {Sebastian Reich and Simon Weissmann},
  doi          = {10.1137/19M1303162},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {446-482},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Fokker--planck particle systems for bayesian inference: Computational approaches},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian spatial inversion and conjugate selection gaussian
prior models. <em>JUQ</em>, <em>9</em>(2), 420–445. (<a
href="https://doi.org/10.1137/19M1302995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study conjugate prior models in Bayesian spatial inversion. The spatial Kriging model may be phrased in a conjugate Bayesian inversion setting with a Gaussian prior model and a Gauss-linear likelihood function, resulting in a Gaussian posterior model. Spatial variables with unimodal, symmetric spatial histograms can be represented by this Kriging model. We generalize this Gaussian prior model by a selection mechanism, and this selection Gaussian prior model may represent multimodal, skewed, and/or peaked spatial variables. Also this selection Gaussian prior model is conjugate with respect to Gauss-linear likelihood functions. Hence the posterior model is selection Gaussian and analytically tractable. Efficient algorithms for simulation of and prediction in the selection Gaussian posterior model are defined. Model parameter inference in a maximum likelihood setting, which is simplified by the conjugate property, is also discussed. Moreover, we demonstrate that any conjugate prior model can be generalized by selection and still remain conjugate with respect to the actual likelihood function. Lastly, a seismic inversion case study is presented, and improvements of 20--40\% in prediction mean-square-error, relative to traditional Gaussian inversion, are found.},
  archive      = {J_JUQ},
  author       = {Henning Omre and Kjartan Rimstad},
  doi          = {10.1137/19M1302995},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {420-445},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bayesian spatial inversion and conjugate selection gaussian prior models},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilevel markov chain monte carlo for bayesian inversion
of parabolic partial differential equations under gaussian prior.
<em>JUQ</em>, <em>9</em>(2), 384–419. (<a
href="https://doi.org/10.1137/20M1354714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the convergence of a multilevel Markov chain Monte Carlo (MLMCMC) algorithm for the Bayesian estimation of solution functionals for linear, parabolic partial differential equations subject to a log-Gaussian uncertain diffusion coefficient. Precisely, our multilevel convergence analysis is for a time-independent, log-Gaussian diffusion coefficient and for observations which are assumed to be corrupted by additive, centered, Gaussian observation noise. The elliptic spatial part of the parabolic PDE is assumed to be neither uniformly coercive nor uniformly bounded in terms of the realizations of the unknown Gaussian random field. The pathwise, multilevel discretization in space and time is a standard, first order, Lagrangian simplicial finite element method in the spatial domain and a first order, implicit timestepping of backward Euler type ensuring good dissipation and unconditional stability, and resulting in first order convergence in terms of the spatial meshwidth and the timestep. The Markov chain Monte Carlo (MCMC) algorithms covered by our analysis comprise the standard independence sampler as well as various variants, such as pCN. We prove that the proposed MLMCMC algorithm delivers approximate Bayesian estimates of quantities of interest consistent to first order in the discretization parameter on the finest spatial/temporal discretization meshwidth and stepsize in overall work which scales essentially (i.e., up to terms which depend logarithmically on the discretization parameters) as that of one deterministic solve on the finest mesh. Our convergence analysis is based on the discretization-level dependent truncation of the increments, introduced first in [V. H. Hoang, J. H. Quek, and Ch. Schwab, Inverse Problems, 36 (2020), 035021] for the corresponding elliptic forward problems, which is an essential modification of the MLMCMC method developed for elliptic problems under uniform prior in [V. H. Hoang, Ch. Schwab, and A. M. Stuart, Inverse Problems, 29 (2013), 085010]. This modification is required to address measurability and integrability issues encountered in the Bayesian posterior density evaluated at consecutive discretization levels with respect to the Gaussian prior. Both independence and pCN samplers are analyzed in detail. Applicability of our analysis to other versions of MCMC is discussed.},
  archive      = {J_JUQ},
  author       = {Viet Ha Hoang and Jia Hao Quek and Christoph Schwab},
  doi          = {10.1137/20M1354714},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {384-419},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel markov chain monte carlo for bayesian inversion of parabolic partial differential equations under gaussian prior},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A quasi-monte carlo method for optimal control under
uncertainty. <em>JUQ</em>, <em>9</em>(2), 354–383. (<a
href="https://doi.org/10.1137/19M1294952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an optimal control problem under uncertainty, where the target function is the solution of an elliptic partial differential equation with random coefficients, steered by a control function. The robust formulation of the optimization problem is stated as a high-dimensional integration problem over the stochastic variables. It is well known that carrying out a high-dimensional numerical integration of this kind using a Monte Carlo method has a notoriously slow convergence rate; meanwhile, a faster rate of convergence can potentially be obtained by using sparse grid quadratures, but these lead to discretized systems that are nonconvex due to the involvement of negative quadrature weights. In this paper, we analyze instead the application of a quasi-Monte Carlo method, which retains the desirable convexity structure of the system and has a faster convergence rate compared to ordinary Monte Carlo methods. In particular, we show that under moderate assumptions on the decay of the input random field, the error rate obtained by using a specially designed, randomly shifted rank-1 lattice quadrature rule is essentially inversely proportional to the number of quadrature nodes. The overall discretization error of the problem, consisting of the dimension truncation error, finite element discretization error, and quasi-Monte Carlo quadrature error, is derived in detail. We assess the theoretical findings in numerical experiments.},
  archive      = {J_JUQ},
  author       = {Philipp A. Guth and Vesa Kaarnioja and Frances Y. Kuo and Claudia Schillings and Ian H. Sloan},
  doi          = {10.1137/19M1294952},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {354-383},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A quasi-monte carlo method for optimal control under uncertainty},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EzGP: Easy-to-interpret gaussian process models for computer
experiments with both quantitative and qualitative factors.
<em>JUQ</em>, <em>9</em>(2), 333–353. (<a
href="https://doi.org/10.1137/19M1288462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer experiments with both quantitative and qualitative inputs are commonly used in science and engineering applications. Constructing desirable emulators for such computer experiments remains a challenging problem. In this article, we propose an easy-to-interpret Gaussian process (EzGP) model for computer experiments to reflect the change of the computer model under the different level combinations of qualitative factors. The proposed modeling strategy, based on an additive Gaussian process, is flexible to address the heterogeneity of computer models involving multiple qualitative factors. We also develop two useful variants of the EzGP model to achieve computational efficiency for data with high dimensionality and large sizes. The merits of these models are illustrated by several numerical examples and a real data application.},
  archive      = {J_JUQ},
  author       = {Qian Xiao and Abhyuday Mandal and C. Devon Lin and Xinwei Deng},
  doi          = {10.1137/19M1288462},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {333-353},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {EzGP: Easy-to-interpret gaussian process models for computer experiments with both quantitative and qualitative factors},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of ordinary differential equation models with
discretization error quantification. <em>JUQ</em>, <em>9</em>(1),
302–331. (<a href="https://doi.org/10.1137/19M1278405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider parameter estimation of ordinary differential equation (ODE) models from noisy observations. For this problem, one conventional approach is to fit numerical solutions (e.g., Euler, Runge--Kutta) of ODEs to data. However, such a method does not account for the discretization error in numerical solutions and has limited estimation accuracy. In this study, we develop an estimation method that quantifies the discretization error based on data. The key idea is to model the discretization error as random variables and estimate their variance simultaneously with the ODE parameter. The proposed method has the form of iteratively reweighted least squares, where the discretization error variance is updated with the isotonic regression algorithm and the ODE parameter is updated by solving a weighted least squares problem using the adjoint system. Experimental results demonstrate that the proposed method attains robust estimation with at least comparable accuracy to the conventional method by successfully quantifying the reliability of the numerical solutions.},
  archive      = {J_JUQ},
  author       = {Takeru Matsuda and Yuto Miyatake},
  doi          = {10.1137/19M1278405},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {302-331},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Estimation of ordinary differential equation models with discretization error quantification},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Density estimation by randomized quasi-monte carlo.
<em>JUQ</em>, <em>9</em>(1), 280–301. (<a
href="https://doi.org/10.1137/19M1259213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the density of a random variable $X$ that can be sampled exactly by Monte Carlo (MC). We investigate the effectiveness of replacing MC by randomized quasi-MC (RQMC) or by stratified sampling over the unit cube to reduce the integrated variance (IV) and the mean integrated square error (MISE) for kernel density estimators. We show theoretically and empirically that the RQMC and stratified estimators can achieve substantial reductions of the IV and the MISE, and even faster convergence rates than MC in some situations, while leaving the bias unchanged. We also show that the variance bounds obtained via a traditional Koksma--Hlawka-type inequality for RQMC are much too loose to be useful when the dimension of the problem exceeds a few units. We describe an alternative way to estimate the IV, a good bandwidth, and the MISE, under RQMC or stratification, and we show empirically that in some situations, the MISE can be reduced significantly even in high-dimensional settings.},
  archive      = {J_JUQ},
  author       = {Amal Ben Abdellah and Pierre L&#39;Ecuyer and Art B. Owen and Florian Puchhammer},
  doi          = {10.1137/19M1259213},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {280-301},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Density estimation by randomized quasi-monte carlo},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential design of computer experiments for the
computation of bayesian model evidence. <em>JUQ</em>, <em>9</em>(1),
260–279. (<a href="https://doi.org/10.1137/20M1320432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of competing models of some phenomenon together with measurement data, Bayesian model selection (BMS) is a process of finding the model that is the best candidate for being the true data-generating process. BMS relies on the computation of Bayesian model evidence, which is defined as the marginal likelihood of the measurement data (i.e., the average likelihood over a model&#39;s parameter space). In this article, we introduce a new method for computing Bayesian model evidence. Our method consists of three key elements. First, all competing model functions are emulated by Gaussian processes. Model evaluations for training the Gaussian processes are chosen one by one in a sequential manner. Second, a model-time allocation strategy decides how many model evaluations are spent on each of the competing models. Third, a sequential sampling strategy selects design points in each model&#39;s parameter space. In numerical experiments, the method shows a speed-up of more than 1,000 compared to Monte Carlo estimation. While, in lower-dimensional cases, the use of Gaussian processes alone is very effective, in higher-dimensional cases, the model-time allocation strategy and the sampling strategy become more important as they focus the effort on the right model and in the right areas of the parameter domains.},
  archive      = {J_JUQ},
  author       = {Michael Sinsbeck and Emily Cooke and Wolfgang Nowak},
  doi          = {10.1137/20M1320432},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {260-279},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sequential design of computer experiments for the computation of bayesian model evidence},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Representing model discrepancy in bound-to-bound data
collaboration. <em>JUQ</em>, <em>9</em>(1), 231–259. (<a
href="https://doi.org/10.1137/19M1270185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the existing methodology in bound-to-bound data collaboration (B2BDC), an optimization-based deterministic uncertainty quantification (UQ) framework, to explicitly take into account model discrepancy. The discrepancy is represented as a linear combination of finite basis functions, and the feasible set is constructed according to a collection of modified model-data constraints. Formulas for making predictions are also modified to include the model discrepancy function. Prior information about the model discrepancy can be added to the framework as additional constraints. Dataset consistency, a central feature of B2BDC, is generalized based on the extended framework.},
  archive      = {J_JUQ},
  author       = {Wenyu Li and Arun Hegde and James Oreluk and Andrew Packard and Michael Frenklach},
  doi          = {10.1137/19M1270185},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {231-259},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Representing model discrepancy in bound-to-bound data collaboration},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Can we trust bayesian uncertainty quantification from
gaussian process priors with squared exponential covariance kernel?
<em>JUQ</em>, <em>9</em>(1), 185–230. (<a
href="https://doi.org/10.1137/19M1253010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the frequentist coverage properties of credible sets resulting from Gaussian process priors with squared exponential covariance kernel. First, we show that by selecting the scaling hyperparameter using the maximum marginal likelihood estimator in the (slightly modified) squared exponential covariance kernel, the corresponding $L_2$-credible sets will provide overconfident, misleading uncertainty statements for a large, representative subclass of the functional parameters in the context of the Gaussian white noise model. Then we show that by either blowing up the credible sets with a logarithmic factor or modifying the maximum marginal likelihood estimator with a logarithmic term, one can get reliable uncertainty statements and adaptive size of the credible sets under some additional restriction. Finally, we demonstrate in a numerical study that the derived negative and positive results extend beyond the Gaussian white noise model to the nonparametric regression and classification models for small sample sizes as well. The performance of the squared exponential covariance kernel is also compared to the Matérn covariance kernel.},
  archive      = {J_JUQ},
  author       = {Amine Hadji and Botond Szabó},
  doi          = {10.1137/19M1253010},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {185-230},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Can we trust bayesian uncertainty quantification from gaussian process priors with squared exponential covariance kernel?},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal design of large-scale bayesian linear inverse
problems under reducible model uncertainty: Good to know what you don’t
know. <em>JUQ</em>, <em>9</em>(1), 163–184. (<a
href="https://doi.org/10.1137/20M1347292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimal design of infinite-dimensional Bayesian linear inverse problems governed by partial differential equations that contain secondary reducible model uncertainties, in addition to the uncertainty in the inversion parameters. By reducible uncertainties we refer to parametric uncertainties that can be reduced through parameter inference. We seek experimental designs that minimize the posterior uncertainty in the primary parameters while accounting for the uncertainty in secondary parameters. We accomplish this by deriving a marginalized A-optimality criterion and developing an efficient computational approach for its optimization. We illustrate our approach for estimating an uncertain time-dependent source in a contaminant transport model with an uncertain initial state as secondary uncertainty. Our results indicate that accounting for additional model uncertainty in the experimental design process is crucial.},
  archive      = {J_JUQ},
  author       = {Alen Alexanderian and Noemi Petra and Georg Stadler and Isaac Sunseri},
  doi          = {10.1137/20M1347292},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {163-184},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Optimal design of large-scale bayesian linear inverse problems under reducible model uncertainty: Good to know what you don&#39;t know},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using coupling methods to estimate sample quality of
stochastic differential equations. <em>JUQ</em>, <em>9</em>(1), 135–162.
(<a href="https://doi.org/10.1137/20M1312009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A probabilistic approach to estimating sample qualities of stochastic differential equations is introduced in this paper. The aim is to provide a quantitative upper bound of the distance between the invariant probability measure of a stochastic differential equation and that of its numerical approximation. In order to extend estimates of finite time truncation error to infinite time, it is crucial to know the rate of contraction of the transition kernel of the SDE. We find that suitable numerical coupling methods can effectively estimate such rate of contraction, which gives the distance between two invariant probability measures. Our algorithms are tested with several low and high dimensional numerical examples.},
  archive      = {J_JUQ},
  author       = {Matthew Dobson and Yao Li and Jiayu Zhai},
  doi          = {10.1137/20M1312009},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {135-162},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Using coupling methods to estimate sample quality of stochastic differential equations},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quasi-monte carlo finite element analysis for wave
propagation in heterogeneous random media. <em>JUQ</em>, <em>9</em>(1),
106–134. (<a href="https://doi.org/10.1137/20M1334164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and analyze a quasi-Monte Carlo (QMC) algorithm for efficient simulation of wave propagation modeled by the Helmholtz equation in a bounded region in which the refractive index is random and spatially heterogenous. Our focus is on the case in which the region can contain multiple wavelengths. We bypass the usual sign-indefiniteness of the Helmholtz problem by switching to an alternative sign-definite formulation recently developed by Ganesh and Morgenstern Numer. Algorithms, 83 (2020), pp. 1441--1487. The price to pay is that the regularity analysis required for QMC methods becomes much more technical. Nevertheless we obtain a complete analysis with error comprising stochastic dimension truncation error, finite element error, and cubature error, with results comparable to those obtained for the diffusion problem.},
  archive      = {J_JUQ},
  author       = {M. Ganesh and Frances Y. Kuo and Ian H. Sloan},
  doi          = {10.1137/20M1334164},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {106-134},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quasi-monte carlo finite element analysis for wave propagation in heterogeneous random media},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilevel monte carlo finite difference methods for
fractional conservation laws with random data. <em>JUQ</em>,
<em>9</em>(1), 65–105. (<a
href="https://doi.org/10.1137/19M1279447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish a notion of random entropy solution for degenerate fractional conservation laws incorporating randomness in the initial data, convective flux, and diffusive flux. In order to quantify the solution uncertainty, we design a multilevel Monte Carlo finite difference method (MLMC-FDM) to approximate the ensemble average of the random entropy solutions. Furthermore, we analyze the convergence rates for MLMC-FDM and compare them with the convergence rates for the deterministic case. Additionally, we formulate error vs. work estimates for the multilevel estimator. Finally, we present several numerical experiments to demonstrate the efficiency of these schemes and validate the theoretical estimates obtained in this work.},
  archive      = {J_JUQ},
  author       = {Ujjwal Koley and Deep Ray and Tanmay Sarkar},
  doi          = {10.1137/19M1279447},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {65-105},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel monte carlo finite difference methods for fractional conservation laws with random data},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization-based markov chain monte carlo methods for
nonlinear hierarchical statistical inverse problems. <em>JUQ</em>,
<em>9</em>(1), 29–64. (<a
href="https://doi.org/10.1137/20M1318365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many hierarchical inverse problems, not only do we want to estimate high- or infinite-dimensional model parameters in the parameter-to-observable maps, but we also have to estimate hyperparameters that represent critical assumptions in the statistical and mathematical modeling processes. As a joint effect of high-dimensionality, nonlinear dependence, and nonconcave structures in the joint posterior distribution over model parameters and hyperparameters, solving inverse problems in the hierarchical Bayesian setting poses a significant computational challenge. In this work, we develop scalable optimization-based Markov chain Monte Carlo (MCMC) methods for solving hierarchical Bayesian inverse problems with nonlinear parameter-to-observable maps and a broader class of hyperparameters. Our algorithmic development is based on the recently developed scalable randomize-then-optimize (RTO) method [J. M. Bardsley et al., SIAM J. Sci. Comput., 42 (2016), pp. A1317--A1347] for exploring the high- or infinite-dimensional parameter space. We first extend the RTO machinery to the Poisson likelihood and discuss the implementation of RTO in the hierarchical setting. Then, by using RTO either as a proposal distribution in a Metropolis-within-Gibbs update or as a biasing distribution in the pseudomarginal MCMC [C. Andrieu and G. O. Roberts, Ann. Statist., 37 (2009), pp. 697--725], we present efficient sampling tools for hierarchical Bayesian inversion. In particular, the integration of RTO and the pseudomarginal MCMC has sampling performance robust to model parameter dimensions. Numerical examples in PDE-constrained inverse problems and positron emission tomography are used to demonstrate the performance of our methods.},
  archive      = {J_JUQ},
  author       = {Johnathan M. Bardsley and Tiangang Cui},
  doi          = {10.1137/20M1318365},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {29-64},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Optimization-based markov chain monte carlo methods for nonlinear hierarchical statistical inverse problems},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the asymptotical regularization for linear inverse
problems in presence of white noise. <em>JUQ</em>, <em>9</em>(1), 1–28.
(<a href="https://doi.org/10.1137/20M1330841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We interpret steady linear statistical inverse problems as artificial dynamic systems with white noise and introduce a stochastic differential equation system where the inverse of the ending time $T$ naturally plays the role of the squared noise level. The time-continuous framework then allows us to apply classical methods from data assimilation, namely, the Kalman--Bucy filter and 3DVAR, and to analyze their behavior as a regularization method for the original problem. Such treatment offers some connections to the famous asymptotical regularization method, which has not yet been analyzed in the context of random noise. We derive error bounds for both methods in terms of the mean-squared error under standard assumptions and discuss commonalities and differences between both approaches. If an additional tuning parameter $\alpha$ for the initial covariance is chosen appropriately in terms of the ending time $T$, one of the proposed methods gains order optimality. Our results extend theoretical findings in the discrete setting given in the recent paper by Iglesias et al. [Commun. Math. Sci., 15 (2017), pp. 1867--1895]. Numerical examples confirm our theoretical results.},
  archive      = {J_JUQ},
  author       = {Shuai Lu and Pingping Niu and Frank Werner},
  doi          = {10.1137/20M1330841},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {1-28},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the asymptotical regularization for linear inverse problems in presence of white noise},
  volume       = {9},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
