<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SICOMP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sicomp---65">SICOMP - 65</h2>
<ul>
<li><details>
<summary>
(2021). Derandomization beyond connectivity: Undirected laplacian
systems in nearly logarithmic space. <em>SICOMP</em>, <em>50</em>(6),
1892–1922. (<a href="https://doi.org/10.1137/20M134109X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a deterministic $O(\log n\cdot\log\log n)$-space algorithm for approximately solving linear systems given by Laplacians of undirected graphs, and consequently also approximating hitting times, commute times, and escape probabilities for undirected graphs. Previously, such systems were known to be solvable by randomized algorithms using $O(\log n)$ space [D. Doron, F. Le Gall, and A. Ta-Shma, Probabilistic logarithmic-space algorithms for Laplacian solvers, in APPROX/RANDOM 2017, LIPIcs. Leibniz Int. Proc. Inform. 81, Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, Germany, 2017, 41] and hence by deterministic algorithms using $O(\log^{3/2} n)$ space [M. Saks and S. Zhou, J. Comput. System Sci., 58 (1999), pp. 376--403]. Our algorithm combines ideas from time-efficient Laplacian solvers [D. A. Spielman and S.-H. Teng, Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems, in STOC 2004, ACM, New York, 2004, pp. 81--90; R. Peng and D. A. Spielman, An efficient parallel solver for SDD linear systems, in STOC 2014, ACM, New York, 2014, pp. 333--342] with ideas used to show that Undirected S-T Connectivity is in deterministic logspace [O. Reingold, J. ACM, 55 (2008); E. Rozenman and S. Vadhan, Derandomized squaring of graphs, in RANDOM 2005, Lecture Notes in Comput. Sci. 3624, Springer, Berlin, 2005, pp. 436--447].},
  archive      = {J_SICOMP},
  author       = {Jack Murtagh and Omer Reingold and Aaron Sidford and Salil Vadhan},
  doi          = {10.1137/20M134109X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1892-1922},
  shortjournal = {SIAM J. Comput.},
  title        = {Derandomization beyond connectivity: Undirected laplacian systems in nearly logarithmic space},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlocal games with noisy maximally entangled states are
decidable. <em>SICOMP</em>, <em>50</em>(6), 1800–1891. (<a
href="https://doi.org/10.1137/20M134592X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a special class of nonlocal games $(G,\psi)$, where $G$ is a two-player one-round game, and $\psi$ is a bipartite state independent of $G$. In the game $(G,\psi)$, the players are allowed to share arbitrarily many copies of $\psi$. The value of the game $(G,\psi)$, denoted by $\omega^*(G,\psi)$, is the supremum of the winning probability that the players can achieve with arbitrarily many copies of preshared states $\psi$. For a noisy maximally entangled state $\psi$, a two-player one-round game $G$ and an arbitrarily small precision $\epsilon&gt;0$, this paper proves an upper bound on the number of copies of $\psi$ for the players to win the game with a probability $\epsilon$ close to $\omega^*(G,\psi)$. A noisy maximally entangled state is a two-qudit state with both marginals being completely mixed states and the maximal correlation being less than $1$. In particular, it includes $(1-\epsilon)|\Psi_m\rangle\langle\Psi_m|+\epsilon\frac{\mathbbm{1}_m}{m}\otimes\frac{\mathbbm{1}_m}{m}$ for $\epsilon&gt;0$, where $|\Psi_m\rangle=\frac{1}{\sqrt{m}}\sum_{i=0}^{m-1}|m,m\rangle$ is an $m$-dimensional maximally entangled state. Hence, it is feasible to approximately compute $\omega^*(G,\psi)$ to an arbitrary precision. Recently, a breakthrough result by Ji et al. showed that it is undecidable to approximate the values of nonlocal games to a constant precision, when the players preshare arbitrarily many copies of perfect maximally entangled states, which implies that $\mathrm{MIP}^*=\mathrm{RE}$. In contrast, our result implies the hardness of approximating nonlocal games collapses when the preshared maximally entangled states are noisy. The paper develops a theory of Fourier analysis on matrix spaces by extending a number of techniques in Boolean analysis and Hermitian analysis to matrix spaces. We establish a series of new techniques, such as a quantum invariance principle and a hypercontractive inequality for random operators, which we believe have further applications. (A corrected version is attached.)},
  archive      = {J_SICOMP},
  author       = {Minglong Qin and Penghui Yao},
  doi          = {10.1137/20M134592X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1800-1891},
  shortjournal = {SIAM J. Comput.},
  title        = {Nonlocal games with noisy maximally entangled states are decidable},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A full dichotomy for <span
class="math inline">$\hol^{c}$</span>, inspired by quantum computation.
<em>SICOMP</em>, <em>50</em>(6), 1739–1799. (<a
href="https://doi.org/10.1137/20M1311557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holant problems are a family of counting problems parameterized by sets of algebraic-complex-valued constraint functions and defined on graphs. They arise from the theory of holographic algorithms, which was originally inspired by concepts from quantum computation. Here, we employ quantum information theory to explain existing results about holant problems in a concise way and to derive two new dichotomies: one for a new family of problems, which we call ${{Holant}}^+$, and, building on this, a full dichotomy for ${{Holant}}^c$. These two families of holant problems assume the availability of certain unary constraint functions---the two pinning functions in the case of ${{Holant}}^c$, and four functions in the case of ${{Holant}}^+$---and allow arbitrary sets of algebraic-complex valued constraint functions otherwise. The dichotomy for ${{Holant}}^+$ also applies when inputs are restricted to instances defined on planar graphs. In proving these complexity classifications, we derive an original result about entangled quantum states.},
  archive      = {J_SICOMP},
  author       = {Miriam Backens},
  doi          = {10.1137/20M1311557},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1739-1799},
  shortjournal = {SIAM J. Comput.},
  title        = {A full dichotomy for $\hol^{c}$, inspired by quantum computation},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Counting solutions to random CNF formulas. <em>SICOMP</em>,
<em>50</em>(6), 1701–1738. (<a
href="https://doi.org/10.1137/20M1351527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give the first efficient algorithm to approximately count the number of solutions in the random $k$-SAT model when the density of the formula scales exponentially with $k$. The best previous counting algorithm for the permissive version of the model was due to Montanari and Shah and was based on the correlation decay method, which works up to densities $(1+o_k(1))\frac{2\log k}{k}$, the Gibbs uniqueness threshold for the model. Instead, our algorithm harnesses a recent technique by Moitra to work for random formulas with much higher densities. The main challenge in our setting is to account for the presence of high-degree variables whose marginal distributions are hard to control and which cause significant correlations within the formula.},
  archive      = {J_SICOMP},
  author       = {Andreas Galanis and Leslie Ann Goldberg and Heng Guo and Kuan Yang},
  doi          = {10.1137/20M1351527},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1701-1738},
  shortjournal = {SIAM J. Comput.},
  title        = {Counting solutions to random CNF formulas},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Promise constraint satisfaction: Algebraic structure and a
symmetric boolean dichotomy. <em>SICOMP</em>, <em>50</em>(6), 1663–1700.
(<a href="https://doi.org/10.1137/19M128212X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A classic result due to Schaefer [Proceedings of STOC 78, ACM, 1978, pp. 216--226] classifies all constraint satisfaction problems (CSPs) over the Boolean domain as being either in ${P}$ or ${NP}$-hard. This paper considers a promise-problem variant of CSPs called PCSPs. A PCSP over a finite set of pairs of constraints $\Gamma$ consists of a pair $(\Psi_P, \Psi_Q)$ of CSPs with the same set of variables such that for every $(P, Q) \in \Gamma$, $P(x_{i_1}, \hdots, x_{i_k})$ is a clause of $\Psi_P$ if and only if $Q(x_{i_1}, \hdots, x_{i_k})$ is a clause of $\Psi_Q$. The promise problem ${PCSP}(\Gamma)$ is to distinguish, given $(\Psi_P, \Psi_Q)$, between the cases $\Psi_P$ is satisfiable and $\Psi_Q$ is unsatisfiable. Many problems such as approximate graph and hypergraph coloring as well as the $(2+\epsilon)$-SAT problem due to Austrin, Guruswami, and H\aastad [SIAM J. Comput., 46 (2017), pp. 1554--1573] can be placed in this framework. This paper is motivated by the pursuit of understanding the computational complexity of Boolean PCSPs, determining for which $\Gamma$ the associated PCSP is polynomial-time tractable or ${NP}$-hard. As our main result, we show that ${PCSP}(\Gamma)$ exhibits a dichotomy (it is either polynomial-time tractable or ${NP}$-hard) when the relations in $\Gamma$ are symmetric and allow for negations of variables. In particular, we show that every such polynomial-time tractable $\Gamma$ can be solved via either Gaussian elimination over $\mathbb F_2$ or a linear programming relaxation. We achieve our dichotomy theorem by extending the (weak) polymorphism framework of Austrin, Guruswami, and H\aastad which itself is a generalization of the algebraic approach used by polymorphisms to study CSPs. In both the algorithm and hardness portions of our proof, we incorporate new ideas and techniques not utilized in the CSP case.},
  archive      = {J_SICOMP},
  author       = {Joshua Brakensiek and Venkatesan Guruswami},
  doi          = {10.1137/19M128212X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1663-1700},
  shortjournal = {SIAM J. Comput.},
  title        = {Promise constraint satisfaction: Algebraic structure and a symmetric boolean dichotomy},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph pattern detection: Hardness for all induced patterns
and faster noninduced cycles. <em>SICOMP</em>, <em>50</em>(5),
1627–1662. (<a href="https://doi.org/10.1137/20M1335054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the pattern detection problem in graphs: given a constant size pattern graph $H$ and a host graph $G$, determine whether $G$ contains a subgraph isomorphic to $H$. We present the following new improved upper and lower bounds: We prove that if a pattern $H$ contains a $k$-clique subgraph, then detecting whether an $n$ node host graph contains a not necessarily induced copy of $H$ requires at least the time for detecting whether an $n$ node graph contains a $k$-clique. The previous result of this nature required that $H$ contains a $k$-clique which is disjoint from all other $k$-cliques of $H$. We show that if the famous Hadwiger conjecture from graph theory is true, then detecting whether an $n$ node host graph contains a not necessarily induced copy of a pattern with chromatic number $t$ requires at least the time for detecting whether an $n$ node graph contains a $t$-clique. This implies that (1) under Hadwiger&#39;s conjecture for every $k$-node pattern $H$, finding an induced copy of $H$ requires at least the time of $\sqrt k$-clique detection and size $\omega(n^{\sqrt{k}/4})$ for any constant depth circuit, and (2) unconditionally, detecting an induced copy of a random $G(k,p)$ pattern with high probability requires at least the time of $\Theta(k/\log k)$-clique detection, and hence also at least size $n^{\Omega(k/\log k)}$ for circuits of constant depth. We show that for every $k$, there exists a $k$-node pattern that contains a $k-1$-clique and that can be detected as an induced subgraph in $n$ node graphs in the best known running time for $k-1$-clique detection. Previously such a result was only known for infinitely many $k$. Finally, we consider the case when the pattern is a directed cycle on $k$ nodes, and we would like to detect whether a directed $m$-edge graph $G$ contains a $k$-cycle as a not necessarily induced subgraph. We resolve a 14-year-old conjecture of [Yuster and Zwick, Proceedings of SODA, 2004, pp. 247--253] on the complexity of $k$-cycle detection by giving a tight analysis of their $k$-cycle algorithm. Our analysis improves the best bounds for $k$-cycle detection in directed graphs for all $k&gt;5$.},
  archive      = {J_SICOMP},
  author       = {Mina Dalirrooyfard and Thuy Duong Vuong and Virginia Vassilevska Williams},
  doi          = {10.1137/20M1335054},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1627-1662},
  shortjournal = {SIAM J. Comput.},
  title        = {Graph pattern detection: Hardness for all induced patterns and faster noninduced cycles},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple, deterministic, constant-round coloring in congested
clique and MPC. <em>SICOMP</em>, <em>50</em>(5), 1603–1626. (<a
href="https://doi.org/10.1137/20M1366502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We settle the complexity of the $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems in the \sf CONGESTED CLIQUE model by presenting a simple deterministic algorithm for both problems running in a constant number of rounds. This matches the complexity of the recent breakthrough randomized constant-round $(\Delta+1)$-list coloring algorithm due to Chang et al. [Proceedings of the 38th ACM Symposium on Principles of Distributed Computing, 2019] and significantly improves upon the state-of-the-art $O(\log \Delta)$-round deterministic $(\Delta+1)$-coloring bound of Parter [Proceedings of the 45th Annual International Colloquium on Automata, Languages and Programming]. A remarkable property of our algorithm is its simplicity. Whereas the state-of-the-art randomized algorithms for this problem are based on the quite involved local coloring algorithm of Chang, Li, and Pettie [Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, 2018], our algorithm can be described in just a few lines. At a high level, it applies a careful derandomization of a recursive procedure which partitions the nodes and their respective palettes into separate bins. We show that after $O(1)$ recursion steps, the remaining uncolored subgraph within each bin has linear size and thus can be solved locally by collecting it to a single node. This algorithm can also be implemented in the massively parallel computation (\sf MPC) model provided that each machine has linear (in ${\mathfrak{n}}$, the number of nodes in the input graph) space. We also show an extension of our algorithm to the \sf MPC regime, in which machines have sublinear space: we present the first deterministic $(\Delta+1)$-list coloring algorithm designed for sublinear-space \sf MPC, which runs in $O(\log \Delta + \log \log \mathfrak{n})$ rounds.},
  archive      = {J_SICOMP},
  author       = {Artur Czumaj and Peter Davies and Merav Parter},
  doi          = {10.1137/20M1366502},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1603-1626},
  shortjournal = {SIAM J. Comput.},
  title        = {Simple, deterministic, constant-round coloring in congested clique and MPC},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polynomial time approximation schemes for the traveling
repairman and other minimum latency problems. <em>SICOMP</em>,
<em>50</em>(5), 1580–1602. (<a
href="https://doi.org/10.1137/19M126918X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a polynomial time approximation scheme for the weighted traveling repairman problem (TRP) in the Euclidean plane, on trees, and on planar graphs. This improves upon the quasi-polynomial time approximation schemes for the unweighted TRP in the Euclidean plane and trees and on the 3.59-approximation for planar graphs. The algorithms are based on a new decomposition technique that reduces the approximation of weighted TRP to instances for which we may restrict ourselves to solutions that are the concatenation of only a constant number of traveling salesman problem paths. A similar reduction applies to many other problems with an average completion time objective. To illustrate the strength of this approach, we apply the same technique to the well-studied scheduling problem of minimizing total weighted completion time under precedence constraints, $1|prec|\sum w_{j}C_{j}$, and present a polynomial time approximation scheme for the case of interval order precedence constraints. This improves on the known 3/2-approximation for this problem.},
  archive      = {J_SICOMP},
  author       = {René Sitters},
  doi          = {10.1137/19M126918X},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1580-1602},
  shortjournal = {SIAM J. Comput.},
  title        = {Polynomial time approximation schemes for the traveling repairman and other minimum latency problems},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An algebraic approach to nonmalleability. <em>SICOMP</em>,
<em>50</em>(5), 1537–1579. (<a
href="https://doi.org/10.1137/16M1073595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In their seminal work on nonmalleable cryptography, Dolev, Dwork, and Naor showed how to construct a nonmalleable commitment with logarithmically-many “rounds&#39;&#39;/``slots,” the idea being that any adversary may successfully maul in some slots but would fail in at least one. Since then new ideas have been introduced, ultimately resulting in constant-round protocols based on any one-way function. Yet, in spite of this remarkable progress, each of the known constructions of nonmalleable commitments leaves something to be desired. In this paper we propose a new technique that allows us to construct a nonmalleable protocol with only a single slot and to improve in at least one aspect over each of the previously proposed protocols. Two direct byproducts of our new ideas are a four-round nonmalleable commitment and a four-round nonmalleable zero-knowledge argument, the latter matching the round-complexity of the best known zero-knowledge argument (without the nonmalleability requirement). The protocols are based on the existence of one-way functions and admit very efficient instantiations via standard homomorphic commitments and sigma protocols. Our analysis relies on algebraic reasoning, and makes use of error correcting codes in order to ensure that committers&#39; tags differ in many coordinates. One way of viewing our construction is as a method for combining many atomic subprotocols in a way that simultaneously amplifies soundness and nonmalleability, thus requiring much weaker guarantees to begin with, and resulting in a protocol which is much trimmer in complexity compared to the existing ones.},
  archive      = {J_SICOMP},
  author       = {Vipul Goyal and Silas Richelson and Alon Rosen and Margarita Vald},
  doi          = {10.1137/16M1073595},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1537-1579},
  shortjournal = {SIAM J. Comput.},
  title        = {An algebraic approach to nonmalleability},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Searchable symmetric encryption: Optimal locality in linear
space via two-dimensional balanced allocations. <em>SICOMP</em>,
<em>50</em>(5), 1501–1536. (<a
href="https://doi.org/10.1137/19M1303186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searchable symmetric encryption (SSE) enables a client to store a database on an untrusted server while supporting keyword search in a secure manner. Despite the rapidly increasing interest in SSE technology, experiments indicate that the performance of the known schemes scales badly to large databases. Somewhat surprisingly, this is not due to their usage of cryptographic tools, but rather due to their poor locality (where locality is defined as the number of noncontiguous memory locations the server accesses with each query). The only known schemes that do not suffer from poor locality suffer either from an impractical space overhead or from an impractical read efficiency (where read efficiency is defined as the ratio between the number of bits the server reads with each query and the actual size of the answer). We construct the first SSE schemes that simultaneously enjoy optimal locality, optimal space overhead, and nearly optimal read efficiency. Specifically, for a database of size $N$, under the modest assumption that no keyword appears in more than $N^{1 - 1/\log \log N}$ documents, we construct a scheme with read efficiency $\tilde{O}(\log \log N)$. This essentially matches the lower bound of Cash and Tessaro (EUROCRYPT &#39;14) showing that any SSE scheme must be suboptimal in either its locality, its space overhead, or its read efficiency. In addition, even without making any assumptions on the structure of the database, we construct a scheme with read efficiency $\tilde{O}(\log N)$. Our schemes are obtained via a two-dimensional generalization of the classic balanced allocations (``balls and bins&#39;&#39;) problem that we put forward. We construct nearly optimal two-dimensional balanced allocation schemes, and then combine their algorithmic structure with subtle cryptographic techniques.},
  archive      = {J_SICOMP},
  author       = {Gilad Asharov and Moni Naor and Gil Segev and Ido Shahaf},
  doi          = {10.1137/19M1303186},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {1501-1536},
  shortjournal = {SIAM J. Comput.},
  title        = {Searchable symmetric encryption: Optimal locality in linear space via two-dimensional balanced allocations},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fixed-depth size-hierarchy theorem for <span
class="math inline">AC<sup>0</sup>[⊕]</span> via the coin problem.
<em>SICOMP</em>, <em>50</em>(4), 1461–1499. (<a
href="https://doi.org/10.1137/19M1276467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we prove the first fixed-depth size-hierarchy theorem for uniform ${\mathrm{AC}}^0[\oplus]$. In particular, we show that for any fixed $d$ and integer parameter $k$, the class ${\mathcal{{C}}}_{d,k}$ of functions that have uniform ${\mathrm{AC}}^0[\oplus]$ formulas of depth $d$ and size $n^k$ form an infinite hierarchy. We show this by exhibiting the first class of functions that have uniform ${\mathrm{AC}}^0[\oplus]$ formulas of size $n^k$ but no ${\mathrm{AC}}^0[\oplus]$ formulas of size less than $n^{\varepsilon_0 k}$ for some absolute constant $\varepsilon_0 &gt; 0$. The uniform formulas are designed to solve the $\delta$-coin problem, which is the computational problem of distinguishing between coins that are heads with probability $(1+\delta)/2$ or $(1-\delta)/2,$ where $\delta$ is a parameter that is going to $0$. We study the complexity of this problem and make progress on both upper bound and lower bound fronts. Regarding Upper bounds, for any constant $d\geq 2$, we show that there are uniform monotone ${\mathrm{AC}}^0$ formulas (i.e., made up of AND and OR gates only) solving the $\delta$-coin problem that have depth $d$, size $\exp(O(d\cdot(1/\delta)^{1/(d-1)}))$, and sample complexity (i.e., number of inputs) ${\mathop{\mathrm{poly}}}(1/\delta).$ This matches previous upper bounds of O&#39;Donnell and Wimmer [ICALP 2007: Automata, Languages and Programming, Lecture Notes in Comput. Sci. 4596, Springer, New York, 2007, pp. 195--206] and Amano [ICALP 2009: Automata, Languages and Programming, Lecture Notes in Comput. Sci. 5555, Springer, New York, 2009, pp. 59--70] in terms of size (which is optimal), while improving the sample complexity from $\exp(O(d\cdot(1/\delta)^{1/(d-1)}))$ to ${\mathop{\mathrm{poly}}}(1/\delta)$. The improved sample complexity is crucial for proving the size-hierarchy theorem. Regarding Lower bounds, we show that the preceding upper bounds are nearly tight (in terms of size) even for the significantly stronger model of ${\mathrm{AC}}^0[\oplus]$ formulas (which are also allowed NOT and Parity gates): formally, we show that any ${\mathrm{AC}}^0[\oplus]$ formula solving the $\delta$-coin problem must have size $\exp(\Omega(d\cdot(1/\delta)^{1/(d-1)})).$ This strengthens a result of Shaltiel and Viola [SIAM J. Comput., 39 (2010), pp. 3122--3154], who prove an $\exp(\Omega((1/\delta)^{1/(d+2)}))$ lower bound for ${\mathrm{AC}}^0[\oplus]$ circuits, and a result of Cohen, Ganor, and Raz [APPROX-RANDOM, LIPIcs. Leibniz Int. Proc. Inform. 28, Schloss Dagstuhl, Leibniz-Zentrum fuer Informatik, Wadern, 2014, pp. 618--629], who show an $\exp(\Omega((1/\delta)^{1/(d-1)}))$ lower bound for ${\mathrm{AC}}^0$ circuits. The upper bound is a derandomization involving a use of Janson&#39;s inequality and an extension of classical polynomial-based combinatorial designs. For the lower bound, we prove an optimal (up to a constant factor) degree lower bound for multivariate polynomials over ${\mathbb{F}}_2$ solving the $\delta$-coin problem, which may be of independent interest.},
  archive      = {J_SICOMP},
  author       = {Nutan Limaye and Karteek Sreenivasaiah and Srikanth Srinivasan and Utkarsh Tripathi and S. Venkitesh},
  doi          = {10.1137/19M1276467},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1461-1499},
  shortjournal = {SIAM J. Comput.},
  title        = {A fixed-depth size-hierarchy theorem for $\mathrm{AC}^0[\oplus]$ via the coin problem},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error thresholds for arbitrary pauli noise. <em>SICOMP</em>,
<em>50</em>(4), 1410–1460. (<a
href="https://doi.org/10.1137/20M1337375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The error threshold of a one-parameter family of quantum channels is defined as the largest noise level such that the quantum capacity of the channel remains positive. This in turn guarantees the existence of a quantum error correction code for noise modeled by that channel. Discretizing the single-qubit errors leads to the important family of Pauli quantum channels; curiously, multipartite entangled states can increase the threshold of these channels beyond the so-called hashing bound, an effect termed superadditivity of coherent information. In this work, we divide the simplex of Pauli channels into one-parameter families and compute numerical lower bounds on their error thresholds. We find substantial increases of error thresholds relative to the hashing bound for large regions in the Pauli simplex corresponding to biased noise, which is a realistic noise model in promising quantum computing architectures. The error thresholds are computed on the family of graph states, a special type of stabilizer state. In order to determine the coherent information of a graph state, we devise an algorithm that exploits the symmetries of the underlying graph, resulting in a substantial computational speed-up. This algorithm uses tools from computational group theory and allows us to consider symmetric graph states on a large number of vertices. Our algorithm works particularly well for repetition codes and concatenated repetition codes (or cat codes), for which our results provide the first comprehensive study of superadditivity for arbitrary Pauli channels. In addition, we identify a novel family of quantum codes based on tree graphs. The error thresholds of these tree graph states outperform repetition and cat codes in large regions of the Pauli simplex, and hence form a new code family with desirable error correction properties.},
  archive      = {J_SICOMP},
  author       = {Johannes Bausch and Felix Leditzky},
  doi          = {10.1137/20M1337375},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1410-1460},
  shortjournal = {SIAM J. Comput.},
  title        = {Error thresholds for arbitrary pauli noise},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A proof of the algebraic tractability conjecture for
monotone monadic SNP. <em>SICOMP</em>, <em>50</em>(4), 1359–1409. (<a
href="https://doi.org/10.1137/19M128466X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The logic MMSNP is a restricted fragment of existential second-order logic which can express many interesting queries in graph theory and finite model theory. The logic was introduced by Feder and Vardi, who showed that every MMSNP sentence is computationally equivalent to a finite-domain constraint satisfaction problem (CSP); the involved probabilistic reductions were derandomized by Kun using explicit constructions of expander structures. We present a new proof of the reduction to finite-domain CSPs that does not rely on the results of Kun. The new universal-algebraic proof allows us to obtain a stronger statement and to verify the more general Bodirsky--Pinsker dichotomy conjecture for CSPs in MMSNP. Our approach uses the fact that every MMSNP sentence describes a finite union of CSPs for countably infinite $\omega$-categorical structures; moreover, by a recent result of Hubička and Nešetřil, these structures can be expanded to homogeneous structures with finite relational signature and the Ramsey property.},
  archive      = {J_SICOMP},
  author       = {Manuel Bodirsky and Florent Madelaine and Antoine Mottet},
  doi          = {10.1137/19M128466X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1359-1409},
  shortjournal = {SIAM J. Comput.},
  title        = {A proof of the algebraic tractability conjecture for monotone monadic SNP},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A little charity guarantees almost envy-freeness.
<em>SICOMP</em>, <em>50</em>(4), 1336–1358. (<a
href="https://doi.org/10.1137/20M1359134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fair division of indivisible goods is a very well-studied problem. The goal of this problem is to distribute $m$ goods to $n$ agents in a “fair” manner, where every agent has a valuation for each subset of goods. We assume monotone valuations. Envy-freeness is the most extensively studied notion of fairness. However, envy-free allocations do not always exist when goods are indivisible. The notion of fairness we consider here is “envy-freeness up to any good,” EFX, where no agent envies another agent after the removal of any single good from the other agent&#39;s bundle. It is not known if such an allocation always exists. We show there is always a partition of the set of goods into $n+1$ subsets $(X_1,\ldots,X_n,P)$, where for $i \in [n]$, $X_i$ is the bundle allocated to agent $i$ and the set $P$ is unallocated (or donated to charity) such that we have (1) envy-freeness up to any good, (2) no agent values $P$ higher than her own bundle, and (3) fewer than $n$ goods go to charity, i.e., $|P| &lt; n$ (typically $m \gg n$). Our proof is constructive and leads to a pseudopolynomial time algorithm to find such an allocation. When agents have additive valuations and $|{P}|$ is large (i.e., when $|P|$ is close to $n$), our allocation also has a good maximin share (MMS) guarantee. Moreover, a minor variant of our algorithm also shows the existence of an allocation that is 4/7 groupwise maximin share (GMMS): this is a notion of fairness stronger than MMS. This improves upon the current best bound of 1/2 known for an approximate GMMS allocation. (Very recently and independently, Amanatidis, Ntokos, and Markakis [Theoret. Comput. Sci., 841 (2020), pp. 94--109], also showed the existence of a 4/7-GMMS allocation.)},
  archive      = {J_SICOMP},
  author       = {Bhaskar Ray Chaudhury and Telikepalli Kavitha and Kurt Mehlhorn and Alkmini Sgouritsa},
  doi          = {10.1137/20M1359134},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1336-1358},
  shortjournal = {SIAM J. Comput.},
  title        = {A little charity guarantees almost envy-freeness},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tight bounds for the subspace sketch problem with
applications. <em>SICOMP</em>, <em>50</em>(4), 1287–1335. (<a
href="https://doi.org/10.1137/20M1311831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the subspace sketch problem one is given an $n \times d$ matrix $A$ with $O(\log(nd))$ bit entries, and would like to compress it in an arbitrary way to build a small space data structure $Q_p$, so that for any given $x \in \mathbb{R}^d$, with probability at least 2/3, one has $Q_p(x) = (1 \pm \varepsilon) \|Ax\|_p$, where $p \geq 0$ and the randomness is over the construction of $Q_p$. The central question is, how many bits are necessary to store $Q_p$? This problem has applications to the communication of approximating the number of nonzeros in a matrix product, the size of coresets in projective clustering, the memory of streaming algorithms for regression in the row-update model, and embedding subspaces of $L_p$ in functional analysis. A major open question is the dependence on the approximation factor $\varepsilon$. We show if $p \geq 0$ is not a positive even integer and $d = \Omega(\log(1/\varepsilon))$, then $\widetilde{\Omega}(\varepsilon^{-2} d)$ bits are necessary. On the other hand, if $p$ is a positive even integer, then there is an upper bound of $O(d^p \log(nd))$ bits independent of $\varepsilon$. Our results are optimal up to logarithmic factors. As corollaries of our main lower bound, we obtain new lower bounds for a wide range of applications, including the above, which in many cases are optimal.},
  archive      = {J_SICOMP},
  author       = {Yi Li and Ruosong Wang and David P. Woodruff},
  doi          = {10.1137/20M1311831},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1287-1335},
  shortjournal = {SIAM J. Comput.},
  title        = {Tight bounds for the subspace sketch problem with applications},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving CSPs using weak local consistency. <em>SICOMP</em>,
<em>50</em>(4), 1263–1286. (<a
href="https://doi.org/10.1137/18M117577X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characterization of all the constraint satisfaction problems solvable by local consistency checking (also known as CSPs of bounded width) was proposed by Feder and Vardi [SIAM J. Comput., 28 (1998), pp. 57--104]. It was confirmed by two independent proofs by Bulatov [Bounded Relational Width, manuscript, 2009] and Barto and Kozik [L. Barto and M. Kozik, 50th Annual IEEE Symposium on Foundations of Computer Science, 2009, pp. 595--603], [L. Barto and M. Kozik, J. ACM, 61 (2014), 3]. Later Barto [J. Logic Comput., 26 (2014), pp. 923--943] proved a collapse of the hierarchy of local consistency notions by showing that (2,3) minimality solves all the CSPs of bounded width. In this paper we present a new consistency notion, jpq consistency, which also solves all the CSPs of bounded width. Our notion is strictly weaker than (2,3) consistency, (2,3) minimality, path consistency, and singleton arc consistency (SAC). This last fact allows us to answer the question of Chen, Dalmau, and Grußien [J. Logic Comput., 23 (2013), pp. 87--108] by confirming that SAC solves all the CSPs of bounded width. Moreover, as known algorithms work faster for SAC, the result implies that CSPs of bounded width can be, in practice, solved more efficiently. The definition of jpq consistency is closely related to a consistency condition obtained from the rounding of an SDP relaxation of a CSP instance. In fact, the main result of this paper is used by Dalmau et al. [Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, Philadelphia, ACM, New York, 2017, pp. 340--357] to show that CSPs with near unanimity polymorphisms admit robust approximation algorithms with polynomial loss. Finally, an algebraic characterization of some term conditions satisfied in algebras associated with templates of bounded width, first proved by Brady, is a direct consequence of our result.},
  archive      = {J_SICOMP},
  author       = {Marcin Kozik},
  doi          = {10.1137/18M117577X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1263-1286},
  shortjournal = {SIAM J. Comput.},
  title        = {Solving CSPs using weak local consistency},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse MDS matrices over small fields: A proof of the GM-MDS
conjecture. <em>SICOMP</em>, <em>50</em>(4), 1248–1262. (<a
href="https://doi.org/10.1137/20M1323345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A $k \times n$ matrix over a field is called an MDS (maximum distance separable) matrix if it satisfies the following property: Any $k$ columns of it are linearly independent. Equivalently, its rows span an MDS code. A question arising in coding theory is what zero patterns MDS matrices can have. There is a natural combinatorial condition, called the rectangle condition, which is necessary over any field, and sufficient over exponentially large fields, concretely of size ${n-1 \choose k-1}$. The GM-MDS conjecture of Dau, Song, and Yuen [On the existence of MDS codes over small fields with constrained generator matrices, in 2014 IEEE International Symposium on Information Theory (ISIT), pp. 1787--1791] speculated that whenever the rectangle condition holds, there exist algebraic constructions over much smaller fields of size $n+k-1$, and gave an algebraic conjecture that implies this. In this work, we prove this algebraic conjecture. In an independent and parallel work, Yildiz and Hassibi [Optimum linear codes with support constraints over small fields, in 2018 IEEE Information Theory Workshop (ITW), pp. 1--5] found an alternative proof for the algebraic conjecture.},
  archive      = {J_SICOMP},
  author       = {Shachar Lovett},
  doi          = {10.1137/20M1323345},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1248-1262},
  shortjournal = {SIAM J. Comput.},
  title        = {Sparse MDS matrices over small fields: A proof of the GM-MDS conjecture},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive planar point location. <em>SICOMP</em>,
<em>50</em>(4), 1200–1247. (<a
href="https://doi.org/10.1137/18M1218194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present self-adjusting data structures for answering point location queries in convex and connected subdivisions. Let $n$ be the number of vertices in a convex or connected subdivision. Our structures use $O(n)$ space. For any convex subdivision $S$, our method processes any online query sequence $\sigma$ in $O({OPT} + n)$ time, where OPT is the minimum time required by any linear decision tree for answering point location queries in $S$ to process $\sigma$. For connected subdivisions, the processing time is $O({OPT} + n + |\sigma|\log(\log^* n))$. In both cases, the time bound includes the $O(n)$ preprocessing time.},
  archive      = {J_SICOMP},
  author       = {Siu-Wing Cheng and Man-Kit Lau},
  doi          = {10.1137/18M1218194},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1200-1247},
  shortjournal = {SIAM J. Comput.},
  title        = {Adaptive planar point location},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward tight approximation bounds for graph diameter and
eccentricities. <em>SICOMP</em>, <em>50</em>(4), 1155–1199. (<a
href="https://doi.org/10.1137/18M1226737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the most important graph parameters is the diameter, the largest distance between any two vertices. There are no known very efficient algorithms for computing the diameter exactly. Thus, much research has been devoted to how fast this parameter can be approximated. Chechik et al. [Proceedings of SODA 2014, Portland, OR, 2014, pp. 1041--1052] showed that the diameter can be approximated within a multiplicative factor of 3/2 in $\tilde{O}(m^{3/2})$ time. Furthermore, Roditty and Vassilevska W. [Proceedings of STOC &#39;13, New York, ACM, 2013, pp. 515--524] showed that unless the strong exponential time hypothesis (SETH) fails, no $O(n^{2-{\varepsilon}})$ time algorithm can achieve an approximation factor better than 3/2 in sparse graphs. Thus the above algorithm is essentially optimal for sparse graphs for approximation factors less than 3/2. It was, however, completely plausible that a 3/2-approximation is possible in linear time. In this work we conditionally rule out such a possibility by showing that unless SETH fails no $O(m^{3/2-{\varepsilon}})$ time algorithm can achieve an approximation factor better than 5/3. Another fundamental set of graph parameters is the eccentricities. The eccentricity of a vertex $v$ is the distance between $v$ and the farthest vertex from $v$. Chechik et al. [Proceedings of SODA 2014, Portland, OR, 2014, pp. 1041--1052] showed that the eccentricities of all vertices can be approximated within a factor of $5/3$ in $\tilde{O}(m^{3/2})$ time and Abboud, Vassilevska W., and Wang [Proceedings of SODA 2016, Arlington, VA, 2016, pp. 377--391] showed that no $O(n^{2-{\varepsilon}})$ algorithm can achieve better than 5/3 approximation in sparse graphs. We show that the runtime of the 5/3 approximation algorithm is also optimal by proving that under SETH, there is no $O(m^{3/2-{\varepsilon}})$ algorithm that achieves a better than 9/5 approximation. We also show that no near-linear time algorithm can achieve a better than 2 approximation for the eccentricities. This is the first lower bound in fine-grained complexity that addresses near-linear time computation. We show that our lower bound for near-linear time algorithms is essentially tight by giving an algorithm that approximates eccentricities within a $2+\delta$ factor in $\tilde{O}(m/\delta)$ time for any $0&lt;\delta&lt;1$. This beats all eccentricity algorithms in Cairo, Grossi, and Rizzi [Proceedings of SODA 2016, Arlington, VA, 2016, pp. 363--376] and is the first constant factor approximation for eccentricities in directed graphs. To establish the above lower bounds we study the $S$-$T$ diameter problem: Given a graph and two subsets $S$ and $T$ of vertices, output the largest distance between a vertex in $S$ and a vertex in $T$. We give new algorithms and show tight lower bounds that serve as a starting point for all other hardness results. Our lower bounds apply only to sparse graphs. We show that for dense graphs, there are near-linear time algorithms for $S$-$T$ diameter, diameter, and eccentricities, with almost the same approximation guarantees as their $\tilde{O}(m^{3/2})$ counterparts, improving upon the best known algorithms for dense graphs.},
  archive      = {J_SICOMP},
  author       = {Arturs Backurs and Liam Roditty and Gilad Segal and Virginia Vassilevska Williams and Nicole Wein},
  doi          = {10.1137/18M1226737},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {1155-1199},
  shortjournal = {SIAM J. Comput.},
  title        = {Toward tight approximation bounds for graph diameter and eccentricities},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Breaking the logarithmic barrier for truthful combinatorial
auctions with submodular bidders. <em>SICOMP</em>, <em>50</em>(3),
STOC16-1-17. (<a href="https://doi.org/10.1137/16M1088594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a central problem in algorithmic mechanism design: constructing truthful mechanisms for welfare maximization in combinatorial auctions with submodular bidders. Dobzinski, Nisan, and Schapira provided the first mechanism that guarantees a nontrivial approximation ratio of $O(\log^2 m)$ [STOC&#39;06, ACM, New York, 2006, pp. 644--652], where $m$ is the number of items. This approximation ratio was subsequently improved to $O(\log m\log \log m)$ [S. Dobzinski, APPROX&#39;07, Springer, Berlin, 2007, pp. 89--103] and then to $O(\log m)$ [P. Krysta and B. Vöcking, ICALP&#39;12, Springer, Heidelberg, 2012, pp. 636--647]. In this paper we develop the first mechanism that breaks the logarithmic barrier. Specifically, the mechanism provides an approximation ratio of $O(\sqrt {\log m})$. Similarly to previous constructions, our mechanism uses polynomially many value and demand queries and, in fact, provides the same approximation ratio for the larger class of XOS (also known as fractionally subadditive) valuations. We also develop a computationally efficient implementation of the mechanism for combinatorial auctions with budget additive bidders. Although, in general, computing a demand query is NP-hard for budget additive valuations, we observe that the specific form of demand queries that our mechanism uses can be efficiently computed when bidders are budget additive.},
  archive      = {J_SICOMP},
  author       = {Shahar Dobzinski},
  doi          = {10.1137/16M1088594},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-1-17},
  shortjournal = {SIAM J. Comput.},
  title        = {Breaking the logarithmic barrier for truthful combinatorial auctions with submodular bidders},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tight bounds for single-pass streaming complexity of the set
cover problem. <em>SICOMP</em>, <em>50</em>(3), STOC16-341-376. (<a
href="https://doi.org/10.1137/16M1095482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We resolve the space complexity of single-pass streaming algorithms for approximating the classic set cover problem. For finding an $\alpha$-approximate set cover (for any $\alpha=o(\sqrt{n}/\log n)$) using a single-pass streaming algorithm, we show that $\Theta(mn/\alpha)$ space is both sufficient and necessary (up to an $O(\log n)$ factor); here $m$ denotes the number of sets and $n$ denotes the size of the universe. This provides a strong negative answer to the open question posed by Har-Peled et al. [Towards tight bounds for the streaming set cover problem, in Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS &#39;16), pp. 371--383] regarding the possibility of having a single-pass algorithm with a small approximation factor that uses sublinear space. We further study the problem of estimating the size of a minimum set cover (as opposed to finding the actual sets) and establish that an additional factor of $\alpha$ savings in the space is achievable in this case and is the best possible. In other words, we show that $\Theta(mn/\alpha^2)$ space is both sufficient and necessary (up to logarithmic factors) for estimating the size of a minimum set cover to within a factor of $\alpha$. Our algorithm, in fact, works for the more general problem of estimating the optimal value of a covering integer program. On the other hand, our lower bound holds even for set cover instances, where the sets are presented in a random order.},
  archive      = {J_SICOMP},
  author       = {Sepehr Assadi and Sanjeev Khanna and Yang Li},
  doi          = {10.1137/16M1095482},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-341-376},
  shortjournal = {SIAM J. Comput.},
  title        = {Tight bounds for single-pass streaming complexity of the set cover problem},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-source dispersers for polylogarithmic entropy and
improved ramsey graphs. <em>SICOMP</em>, <em>50</em>(3), STOC16-30-67.
(<a href="https://doi.org/10.1137/16M1096219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In his 1947 paper that inaugurated the probabilistic method, Erdös proved the existence of $(2+o(1))\log{n}$-Ramsey graphs on $n$ vertices. Matching Erdös&#39;s result with a constructive proof is considered a central problem in combinatorics and has gained significant attention in the literature. The state-of-the-art result was obtained in the celebrated paper by Barak et al. [Ann. of Math. (2), 176 (2012), pp. 1483--1543], who constructed a $2^{2^{(\log\log{n})^{1-\alpha}}}$-Ramsey graph for some universal constant $\alpha &gt; 0$. In this work, we significantly improve the result of Barak et al. and construct $2^{(\log\log{n})^c}$-Ramsey graphs, for some universal constant $c$. In the language of theoretical computer science, this resolves the problem of explicitly constructing dispersers for two $n$-bit sources with entropy ${{polylog}}(n)$. In fact, our disperser is a zero-error disperser that outputs a constant fraction of the entropy. Previously, such dispersers could only support entropy $\Omega(n)$.},
  archive      = {J_SICOMP},
  author       = {Gil Cohen},
  doi          = {10.1137/16M1096219},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-30-67},
  shortjournal = {SIAM J. Comput.},
  title        = {Two-source dispersers for polylogarithmic entropy and improved ramsey graphs},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential separation of communication and external
information. <em>SICOMP</em>, <em>50</em>(3), STOC16-236-254. (<a
href="https://doi.org/10.1137/16M1096293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show an exponential gap between communication complexity and external information complexity by analyzing a communication task suggested as a candidate by Braverman [A Hard-to-Compress Interactive Task?, in Proceedings of the 51th Annual Allerton Conference on Communication, Control, and Computing, IEEE, 2013]. Previously, only a separation of communication complexity and internal information complexity was known. More precisely, we obtain an explicit example of a search problem with external information complexity at most $O(k)$, with respect to any input distribution, and distributional communication complexity at least $2^k$, with respect to some input distribution. In particular, this shows that a communication protocol cannot always be compressed to its external information. By a result of Braverman [SIAM J. Comput., 44 (2015), pp. 1698--1739], our gap is the largest possible. Moreover, since the upper bound of $O(k)$ on the external information complexity of the problem is obtained with respect to any input distribution, our result implies an exponential gap between communication complexity and information complexity (both internal and external) in the nondistributional setting of Braverman [SIAM J. Comput., 44 (2015), pp. 1698--1739]. In this setting, no gap was previously known, even for internal information complexity.},
  archive      = {J_SICOMP},
  author       = {Anat Ganor and Gillat Kol and Ran Raz},
  doi          = {10.1137/16M1096293},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-236-254},
  shortjournal = {SIAM J. Comput.},
  title        = {Exponential separation of communication and external information},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constant-round interactive proofs for delegating
computation. <em>SICOMP</em>, <em>50</em>(3), STOC16-255-340. (<a
href="https://doi.org/10.1137/16M1096773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The celebrated ${\sf IP}={\sf PSPACE}$ theorem [Lund, Fortnow, Karloff, and Nisan, J. ACM, 39 (1992), pp. 859--868; Shamir, J. ACM, 39 (1992), pp. 869--877] allows an all-powerful but untrusted prover to convince a polynomial-time verifier of the validity of extremely complicated statements (as long as they can be evaluated using polynomial space). The interactive proof system designed for this purpose requires a polynomial number of communication rounds and an exponential-time (polynomial-space complete) prover. In this paper, we study the power of more efficient interactive proof systems. Our main result is that for every statement that can be evaluated in polynomial time and bounded-polynomial space there exists an interactive proof that satisfies the following strict efficiency requirements: (1) the honest prover runs in polynomial time, (2) the verifier is almost linear time (and under some conditions even sublinear), and (3) the interaction consists of only a constant number of communication rounds. Prior to this work, very little was known about the power of efficient, constant-round interactive proofs (rather than arguments). This result represents significant progress on the round complexity of interactive proofs (even if we ignore the running time of the honest prover) and on the expressive power of interactive proofs with polynomial-time honest prover (even if we ignore the round complexity). This result has several applications, and in particular it can be used for verifiable delegation of computation. Our construction leverages several new notions of interactive proofs, which may be of independent interest. One of these notions is that of unambiguous interactive proofs where the prover has a unique successful strategy. Another notion is that of probabilistically checkable interactive proofs ($\mathsf{PCIP}$s), where the verifier only reads a few bits of the transcript in checking the proof (this could be viewed as an interactive extension of $\mathsf{PCIP}$s). An equivalent notion to $\mathsf{PCIP}$s, called interactive oracle proofs, was recently introduced in an independent work of Ben-Sasson, Chiesa, and Sponcer [Proceedings of TCC, 2016, pp. 31--60].},
  archive      = {J_SICOMP},
  author       = {Omer Reingold and Guy N. Rothblum and Ron D. Rothblum},
  doi          = {10.1137/16M1096773},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-255-340},
  shortjournal = {SIAM J. Comput.},
  title        = {Constant-round interactive proofs for delegating computation},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tight space bound for consensus. <em>SICOMP</em>,
<em>50</em>(3), STOC16-18-29. (<a
href="https://doi.org/10.1137/16M1096785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the consensus problem, there are $n$ processes that each has a private input value. Each nonfaulty process must output a single value such that no two processes output different values and the output is the input value of some process. There are many consensus protocols for systems where the processes may only communicate by reading and writing to shared registers. Of particular interest are protocols that have progress guarantees such as randomized wait-freedom or obstruction-freedom. In 1992, it was proved that such protocols must use $\Omega(\sqrt{n})$ registers. In 2015, this was improved to $\Omega(n)$ registers in the anonymous setting, where processes do not have identifiers. We prove that every randomized wait-free or obstruction-free protocol for solving consensus among $n$ processes must use at least $n-1$ registers.},
  archive      = {J_SICOMP},
  author       = {Leqi Zhu},
  doi          = {10.1137/16M1096785},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-18-29},
  shortjournal = {SIAM J. Comput.},
  title        = {A tight space bound for consensus},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A polynomial lower bound for testing monotonicity.
<em>SICOMP</em>, <em>50</em>(3), STOC16-406-433. (<a
href="https://doi.org/10.1137/16M1097006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that every algorithm for testing $n$-variate Boolean functions for monotonicity must have query complexity $\tilde{\Omega}(n^{1/4})$. All previous lower bounds for this problem were designed for nonadaptive algorithms and, as a result, the best previous lower bound for general (possibly adaptive) monotonicity testers was only $\Omega(\log n)$. Combined with the query complexity of the nonadaptive monotonicity tester of Khot, Minzer, and Safra (FOCS 2015), our lower bound shows that adaptivity can result in at most a quadratic reduction in the query complexity for testing monotonicity. By contrast, we show that there is an exponential gap between the query complexity of adaptive and nonadaptive algorithms for testing regular linear threshold functions (LTFs) for monotonicity. Chen, De, Servedio, and Tan (STOC 2015) recently showed that nonadaptive algorithms require almost $\Omega(n^{1/2})$ queries for this task. We introduce a new adaptive monotonicity testing algorithm which has query complexity $O(\log n)$ when the input is a regular LTF.},
  archive      = {J_SICOMP},
  author       = {Aleksandrs Belovs and Eric Blais},
  doi          = {10.1137/16M1097006},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-406-433},
  shortjournal = {SIAM J. Comput.},
  title        = {A polynomial lower bound for testing monotonicity},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deterministic almost-tight distributed algorithm for
approximating single-source shortest paths. <em>SICOMP</em>,
<em>50</em>(3), STOC16-98-137. (<a
href="https://doi.org/10.1137/16M1097808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deterministic $(1+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the \sf CONGEST model); here $n$ is the number of nodes in the network, $D$ is its (hop) diameter, and edge weights are positive integers from 1 to $\operatorname{poly}(n)$. This is the first nontrivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized $(1+o(1))$-approximation $\tilde{O}(\sqrt{n}D^{1/4}+D)$-time algorithm of Nanongkai [in Proceedings of STOC, 2014, pp. 565--573] by a factor of as large as $n^{1/8}$, and (ii) the $O(\epsilon^{-1}\log\epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir&#39;s $\tilde{O}(n^{1/2+\epsilon}+D)$-time algorithm [in Proceedings of STOC, 2013, pp. 381--390] within the same running time. (Throughout, we use $\tilde{O}(\cdot)$ to hide polylogarithmic factors in $n$.) Our running time matches the known time lower bound of $\Omega(\sqrt{n/\log n}+D)$ [M. Elkin, SIAM J. Comput., 36 (2006), pp. 433--456], thus essentially settling the status of this problem which was raised at least a decade ago [M. Elkin, SIGACT News, 35 (2004), pp. 40--57]. It also implies a $(2+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network&#39;s weighted diameter which almost matches the lower bound by Holzer and Pinsker [in Proceedings of OPODIS, 2015, Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, Germany, 2016, 6]. In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the “hitting set argument” commonly used for shortest paths computation in various settings, and (ii) a simple, deterministic construction of an $(n^{o(1)},o(1))$-hop set of size $n^{1+o(1)}$. We combine these techniques with many distributed algorithmic techniques, some of which are from problems that are not directly related to shortest paths, e.g., ruling sets [A. V. Goldberg, S. A. Plotkin, and G. E. Shannon, SIAM J. Discrete Math., 1 (1988), pp. 434--446], source detection [C. Lenzen and D. Peleg, in Proceedings of PODC, 2013, pp. 375--382], and partial distance estimation [C. Lenzen and B. Patt-Shamir, in Proceedings of PODC, 2015, pp. 153--162]. Our hop set construction also leads to single-source shortest paths algorithms in two other settings: (i) a $(1+o(1))$-approximation $n^{o(1)}$-time algorithm on congested cliques, and (ii) a $(1+o(1))$-approximation $n^{o(1)}$-pass $n^{1+o(1)}$-space streaming algorithm. The first result answers an open problem in [D. Nanongkai, in Proceedings of STOC, 2014, pp. 565--573]. The second result partially answers an open problem raised by McGregor in 2006 [List of Open Problems in Sublinear Algorithms: Problem 14].},
  archive      = {J_SICOMP},
  author       = {Monika Henzinger and Sebastian Krinninger and Danupon Nanongkai},
  doi          = {10.1137/16M1097808},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-98-137},
  shortjournal = {SIAM J. Comput.},
  title        = {A deterministic almost-tight distributed algorithm for approximating single-source shortest paths},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bipartite perfect matching is in quasi-NC. <em>SICOMP</em>,
<em>50</em>(3), STOC16-218-235. (<a
href="https://doi.org/10.1137/16M1097870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the bipartite perfect matching problem is in quasi-$\mathsf{NC}^2$. That is, it has uniform circuits of quasi-polynomial size $n^{O(\log n)}$, and $O(\log^2 n)$ depth. Previously, only an exponential upper bound was known on the size of such circuits with poly-logarithmic depth. We obtain our result by an almost complete derandomization of the famous Isolation Lemma when applied to yield an efficient randomized parallel algorithm for the bipartite perfect matching problem.},
  archive      = {J_SICOMP},
  author       = {Stephen Fenner and Rohit Gurjar and Thomas Thierauf},
  doi          = {10.1137/16M1097870},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-218-235},
  shortjournal = {SIAM J. Comput.},
  title        = {Bipartite perfect matching is in quasi-NC},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithmic bayesian persuasion. <em>SICOMP</em>,
<em>50</em>(3), STOC16-68-97. (<a
href="https://doi.org/10.1137/16M1098334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persuasion, defined as the act of exploiting an informational advantage in order to influence the decisions of others, is ubiquitous. Indeed, persuasive communication has been estimated to account for almost a third of all economic activity in the U.S. This paper examines persuasion through a computational lens, focusing on what is perhaps the most basic and fundamental model in this space: the celebrated Bayesian persuasion model of Kamenica and Gentzkow [Am. Econ. Rev., 101 (2011), pp. 2590--2615]. Here there are two players, a sender and a receiver. The receiver must take one of a number of actions with an a priori unknown payoff, and the sender has access to additional information regarding the payoffs of the various actions for both players. The sender can commit to revealing a noisy signal regarding the realization of the payoffs of various actions, and would like to do so to maximize her own payoff in expectation assuming that the receiver rationally acts to maximize his own payoff. When the payoffs of various actions follow a joint distribution (the common prior), the sender&#39;s problem is nontrivial, and its computational complexity depends on the representation of this prior. We examine the sender&#39;s optimization task in three of the most natural input models for this problem, and essentially pin down its computational complexity in each. When the payoff distributions of the different actions are independently and identically distributed (i.i.d.) and given explicitly, we exhibit a polynomial-time (exact) algorithmic solution, and a “simple” $(1-1/e)$-approximation algorithm. Our optimal scheme for the i.i.d. setting involves an analogy to auction theory, and makes use of Border&#39;s characterization of the space of reduced-forms for single-item auctions. When action payoffs are independent but nonidentical with marginal distributions given explicitly, we show that it is \#P-hard to compute the optimal expected sender utility. In doing so, we rule out a generalized Border&#39;s theorem, in the sense of Gopalan, Nisan, and Roughgarden [Public projects, boolean functions, and the borders of Border&#39;s theorem, in Proceedings of the Sixteenth ACM Conference on Economics and Computation, EC &#39;15, ACM, New York, 2015, p. 395], for this setting. Finally, we consider a general (possibly correlated) joint distribution of action payoffs presented by a black box sampling oracle, and exhibit a fully polynomial-time approximation scheme (FPTAS) with a bicriteria guarantee. Our FPTAS is based on Monte Carlo sampling, and its analysis relies on the principle of deferred decisions. Moreover, we show that this result is the best possible in the black-box model for information-theoretic reasons.},
  archive      = {J_SICOMP},
  author       = {Shaddin Dughmi and Haifeng Xu},
  doi          = {10.1137/16M1098334},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-68-97},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithmic bayesian persuasion},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lift-and-round to improve weighted completion time on
unrelated machines. <em>SICOMP</em>, <em>50</em>(3), STOC16-138-159. (<a
href="https://doi.org/10.1137/16M1099583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of scheduling jobs on unrelated machines so as to minimize the sum of weighted completion times. Our main result is a $(\nicefrac{3}{2}-c)$-approximation algorithm for some fixed $c&gt;0$, improving upon the long-standing bound of $\nicefrac{3}{2}$. To do this, we first introduce a new lift-and-project-based SDP relaxation for the problem. This is necessary, as the previous convex programming relaxations have an integrality gap of $\nicefrac{3}{2}$. Second, we give a new general bipartite-rounding procedure that produces an assignment with certain strong negative correlation properties.},
  archive      = {J_SICOMP},
  author       = {Nikhil Bansal and Aravind Srinivasan and Ola Svensson},
  doi          = {10.1137/16M1099583},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-138-159},
  shortjournal = {SIAM J. Comput.},
  title        = {Lift-and-round to improve weighted completion time on unrelated machines},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A duality-based unified approach to bayesian mechanism
design. <em>SICOMP</em>, <em>50</em>(3), STOC16-160-200. (<a
href="https://doi.org/10.1137/16M1100113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a unified view of many recent developments in Bayesian mechanism design, including the black-box reductions of Cai, Daskalakis, and Weinberg [in Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science, 2013], simple auctions for additive buyers [S. Hart and N. Nisan, in Proceedings of the 13th ACM Conference on Electronic Commerce, 2012], and posted-price mechanisms for unit-demand buyers [S. Chawla, J. D. Hartline, and R. D. Kleinberg, in Proceedings of the 8th ACM Conference on Electronic Commerce, 2007, pp. 243--251]. Additionally, we show that viewing these three previously disjoint lines of work through the same lens leads to new developments as well. First, we provide a duality framework for Bayesian mechanism design, which naturally accommodates multiple agents and arbitrary objectives/feasibility constraints. Using this, we prove that either a posted-price mechanism or the Vickrey--Clarke--Groves auction with per-bidder entry fees achieves a constant factor of the optimal revenue achievable by a Bayesian Incentive Compatible mechanism whenever buyers are unit-demand or additive, unifying previous breakthroughs of Chawla et al. [in Proceedings of the 42nd ACM Symposium on Theory of Computing, 2010] and Yao [in Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, 2015, pp. 92--109], and improving both approximation ratios (from 30 to 24 and 69 to 8, respectively). Finally, we show that this view also leads to improved structural characterizations in the framework of Cai, Daskalakis, and Weinberg.},
  archive      = {J_SICOMP},
  author       = {Yang Cai and Nikhil R. Devanur and S. Matthew Weinberg},
  doi          = {10.1137/16M1100113},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-160-200},
  shortjournal = {SIAM J. Comput.},
  title        = {A duality-based unified approach to bayesian mechanism design},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithmic stability for adaptive data analysis.
<em>SICOMP</em>, <em>50</em>(3), STOC16-377-405. (<a
href="https://doi.org/10.1137/16M1103646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptivity is an important feature of data analysis---the choice of questions to ask about a dataset often depends on previous interactions with the same dataset. However, statistical validity is typically studied in a nonadaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. [Proceedings of STOC, ACM, 2015, pp.117--126] and Hardt and Ullman [Proceedings of FOCS, IEEE, 2014, pp. 454--463] initiated the formal study of this problem and gave the first upper and lower bounds on the achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution ${P}$ and a set of $n$ independent samples ${x}$ is drawn from ${P}$. We seek an algorithm that, given ${x}$ as input, accurately answers a sequence of adaptively chosen “queries” about the unknown distribution ${P}$. How many samples $n$ must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions toward resolving this question: 1. We give upper bounds on the number of samples $n$ that are needed to answer statistical queries. The bounds improve and simplify the work of Dwork et al. and have been applied in subsequent work by those authors [Science, 349 (2015), pp. 636--638; Proceedings of NIPS, 2015, pp. 2350--2358]. 2. We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and an important class of optimization queries (alternatively, risk minimization queries). As in Dwork et al., our algorithms are based on a connection with algorithmic stability in the form of differential privacy. We extend their work by giving a quantitatively optimal, more general, and simpler proof of their main theorem that stable algorithms of the kind guaranteed by differential privacy imply low generalization error. We also show that weaker stability guarantees such as bounded Kullback--Leibler divergence and total variation distance lead to correspondingly weaker generalization guarantees.},
  archive      = {J_SICOMP},
  author       = {Raef Bassily and Kobbi Nissim and Adam Smith and Thomas Steinke and Uri Stemmer and Jonathan Ullman},
  doi          = {10.1137/16M1103646},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-377-405},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithmic stability for adaptive data analysis},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A (1+epsilon)-approximation for makespan scheduling with
precedence constraints using LP hierarchies. <em>SICOMP</em>,
<em>50</em>(3), STOC16-201-217. (<a
href="https://doi.org/10.1137/16M1105049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a classical problem in scheduling, one has $n$ unit size jobs with a precedence order and the goal is to find a schedule of those jobs on $m$ identical machines as to minimize the makespan. It is one of the remaining four open problems from the book of Garey and Johnson whether or not this problem is $\mathbf{NP}$-hard for $m=3$. We prove that for any fixed $\varepsilon$ and $m$, an LP-hierarchy lift of the time-indexed LP with a slightly super poly-logarithmic number of $r = (\log(n))^{\Theta(\log \log n)}$ rounds provides a $(1 + \varepsilon)$-approximation. For example, Sherali--Adams suffices as hierarchy. This implies an algorithm that yields a $(1+\varepsilon)$-approximation in time $n^{O(r)}$. The previously best approximation algorithms guarantee a $2 - \frac{7}{3m+1}$-approximation in polynomial time for $m \geq 4$ and $\frac{4}{3}$ for $m=3$. Our algorithm is based on a recursive scheduling approach where in each step we reduce the correlation in form of long chains. Our method adds to the rather short list of examples where hierarchies are actually useful to obtain better approximation algorithms.},
  archive      = {J_SICOMP},
  author       = {Elaine Levey and Thomas Rothvo},
  doi          = {10.1137/16M1105049},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-201-217},
  shortjournal = {SIAM J. Comput.},
  title        = {A (1+epsilon)-approximation for makespan scheduling with precedence constraints using LP hierarchies},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special section on the 48th annual ACM symposium on theory
of computing (STOC 2016). <em>SICOMP</em>, <em>50</em>(3), STOC16-i-iii.
(<a href="https://doi.org/10.1137/21N974881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SICOMP contains 14 specially selected papers from the 48th Annual ACM Symposium on Theory of Computing (STOC 2016), held June 18--June 21, 2016, in Cambridge, Massachusetts. The papers here were chosen to represent both the excellence and the broad range of the STOC program. The papers have been revised and extended by the authors and subjected to the standard thorough reviewing process of SICOMP. The program committee members were Alexandr Andoni, Sanjeev Arora, Allison Bishop, Avrim Blum, Keren Censor-Hillel, Timothy Chan, Chandra Chekuri, Jing Chen, Zeev Dvir, Fabrizio Grandoni, Parikshit Gopalan, Kasper Green Larsen, Huijia (Rachel) Lin, Konstantin Makarychev, Yishay Mansour (chair), Jakob Nordström, Debmalya Panigrahi, Prasad Raghavendra, Sofya Raskhodnikova, R Ravi, Mario Szegedy, Êva Tardos, Salil Vadhan, Avi Wigderson, and Ronald de Wolf. We briefly describe here the papers that appear in this special issue. In “Breaking the Logarithmic Barrier for Truthful Combinatorial Auctions with Submodular Bidders,” Shahar Dobzinski provides the first truthful mechanism for welfare maximization in combinatorial auctions with submodular bidders whose approximation ratio is $O(\sqrt{\log m})$. Previously the best ratio was $O(\log m)$. In “A Tight Space Bound for Consensus,” Leqi Zhu proves that every randomized wait-free (or obstruction-free) consensus protocol for $n$ processes must use at least $n-1$ registers. Previously, this bound was known only in the anonymous setting, while for the general case only a $\sqrt{n}$ bound was known. In “Two-Source Dispersers for Polylogarithmic Entropy and Improved Ramsey Graphs,” Gil Cohen constructs a $2^{(\log\log n)^c}$-Ramsey graph for some universal constant $c$, a significant improvement in this direction. In the language of theoretical computer science, this resolves the problem of explicitly constructing dispersers for two $n$-bit sources with entropy ${polylog}(n)$. Previously, such dispersers could only support entropy $\Omega(n)$. In “Algorithmic Bayesian Persuasion,” Shaddin Dughmi and Haifeng Xu examines Bayesian persuasion through a computational lens for the first time. When the payoff distributions are i.i.d. across actions, the authors provide a polynomial-time optimal solution and a “simple” $(1-1/e)$-approximation. For independent but nonidentical distributions, \#P-hardness is proved. For the general case with a black-box sampling oracle, an FPTAS is provided and shown to be the best possible under the black-box model. In “A Deterministic Almost-Tight Distributed Algorithm for Approximating Single-Source Shortest Paths,” Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai present a deterministic $(1 + o(1))$-approximation algorithm for solving the single-source shortest paths problem on distributed weighted networks in $O(n^{1/2+o(1)} + D^{1+o(1)})$ rounds, where $n$ is the number of nodes and $D$ is the diameter of the network. This improves upon previous results in being deterministic and completing in less time or in obtaining a smaller approximation factor. Moreover, it is almost tight due to a known lower bound. In “Lift-and-Round to Improve Weighted Completion Time on Unrelated Machines,” Nikhil Bansal, Aravind Srinivasan, and Ola Svensson improve, by a small but fixed constant, the long-standing approximation factor of $3/2$ for the problem of scheduling jobs on unrelated machines so as to minimize the sum of weighted completion times. In “A Duality-Based Unified Approach to Bayesian Mechanism Design,” Yang Cai, Nikhil Devanur, and Seth Matthew Weinberg provide a duality-based unified framework for designing simple and approximately optimal auctions. Using this framework, the authors prove that either a posted-price mechanism or the Vickrey--Clarke--Groves auction with per-bidder entry fees achieves a constant-factor of the optimal revenue achievable by a Bayesian Incentive Compatible mechanism whenever buyers are unit-demand or additive, unifying previous breakthroughs of Chawla et al. and Yao, and improving both approximation ratios. In “A $(1+\varepsilon)$-Approximation for Makespan Scheduling with Precedence Constraints using LP Hierarchies,” Elaine Levey and Thomas Rothvoss consider the problem of scheduling $n$ unit size jobs with a precedence order on $m$ identical machines as to minimize the makespan. They prove that for any fixed $\epsilon$ and $m$, an LP-hierarchy lift of the time-indexed LP with a slightly super poly-logarithmic number of $r = (\log n)^{\Theta(\log \log n)}$ rounds provides a $(1 + \epsilon)$-approximation. The previous best approximation algorithms for this problem guarantee a $(2 - 7/(3m+1))$-approximation in polynomial time for $m \ge 4$ and $4/3$ for $m=3$. In “Bipartite Perfect Matching Is in Quasi-${{NC}}$,” Stephen Fenner, Rohit Gurjar, and Thomas Thierauf show that the bipartite perfect matching problem is in quasi-${{NC}}^2$. That is, it has uniform circuits of quasi-polynomial size $n^{O(\log n)}$, and $O(\log^2 n)$ depth. Previously, only an exponential upper bound was known on the size of such circuits with poly-logarithmic depth. In “Exponential Separation of Communication and External Information,” Anat Ganor, Gillat Kol, and Ran Raz prove the first gap, an exponential gap, between external information complexity and communication complexity of a communication task. Previously such a separation was known only for the internal information vs communication complexity. This result has implication to the question of compressing communication protocols to the amount of information they reveal about the inputs. In “Constant-Round Interactive Proofs for Delegating Computation,” Omer Reingold, Guy Rothblum, and Ron Rothblum design efficient, constant-round interactive proofs. They show that for any statement that can be evaluated in polynomial time and space $S$, there exists a constant-round interactive protocol where the prover has polynomial runtime and the verifier has a runtime of about $n+\poly(S)$. Prior to this work, very little was known about the power of constant-round protocol. This result is a major step for the grand challenge of verifiable delegation of computation. In “Tight Bounds for Single-Pass Streaming Complexity of the Set Cover Problem,” Sepehr Assadi, Sanjeev Khanna, and Yang Li resolve the space complexity of single-pass streaming algorithms for approximating the classic set cover problem. For finding an $\alpha$-approximate set cover (for any $\alpha= o(\sqrt{n})$) using a single-pass streaming algorithm, they show that $\Theta(mn/\alpha)$ space is both sufficient and necessary (up to an $O(\log n)$ factor), where $m$ denotes number of the sets and $n$ denotes size of the universe. They further study the problem of estimating the size of a minimum set cover (as opposed to finding the actual sets) and achieve an additional saving of a factor of $\alpha$ in the space complexity, which is also the best possible. In “A Polynomial Lower Bound for Testing Monotonicity,” Aleksandrs Belovs and Eric Blais show a polynomial lower bound on query complexity for adaptive testers of monotonicity of an $n$-variate Boolean function. Prior to this work, similar lower bounds were known only for the nonadaptive testers, and proving similar bounds for adaptive testers has been a major challenge. In “Algorithmic Stability for Adaptive Data Analysis,” Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman take a solid step forward in the area of adaptive data analysis by establishing a clean, tight connection between the notion of differential privacy (max-KL stability) and design of adaptive queries. This connection improves a number of bounds that were known prior to this paper, and generalizes to handle more “data analysis&quot; settings. We thank the authors, the program committee members, and the reviewers for STOC 2016 for their hard work, and we especially thank the SICOMP reviewers for their work in evaluating submitted papers.},
  archive      = {J_SICOMP},
  author       = {Alexandr Andoni and Keren Censor-Hillel and Jing Chen and Debmalya Panigrahi},
  doi          = {10.1137/21N974881},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {STOC16-i-iii},
  shortjournal = {SIAM J. Comput.},
  title        = {Special section on the 48th annual ACM symposium on theory of computing (STOC 2016)},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrigendum: LDFS-based certifying algorithm for the minimum
path cover problem on cocomparability graphs. <em>SICOMP</em>,
<em>50</em>(3), 1148–1153. (<a
href="https://doi.org/10.1137/20M1327835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This corrigendum corrects errors found by Jérémie Dusart in the proof of correctness of the algorithms in [D. G. Corneil, B. Dalton, and M. Habib, SIAM J. Comput., 42 (2013), pp. 792--807]; there are no changes in the algorithms themselves.},
  archive      = {J_SICOMP},
  author       = {Jérémie Dusart and Derek G. Corneil and Michel Habib},
  doi          = {10.1137/20M1327835},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {1148-1153},
  shortjournal = {SIAM J. Comput.},
  title        = {Corrigendum: LDFS-based certifying algorithm for the minimum path cover problem on cocomparability graphs},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed spanner approximation. <em>SICOMP</em>,
<em>50</em>(3), 1103–1147. (<a
href="https://doi.org/10.1137/20M1312630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the fundamental network design problem of constructing approximate minimum spanners. Our contributions are for the distributed setting, providing both algorithmic and hardness results. Our main hardness result shows that an $\alpha$-approximation for the minimum directed $k$-spanner problem for $k \geq 5$ requires $\Omega(n /\sqrt{\alpha}\log{n})$ rounds using deterministic algorithms or $\Omega(\sqrt{n }/\sqrt{\alpha}\log{n})$ rounds using randomized ones, in the Congest model of distributed computing. Combined with the constant-round $O(n^{\epsilon})$-approximation algorithm in the Local model of [L. Barenboim, M. Elkin, and C. Gavoille, Theoret. Comput. Sci., 751 (2016), pp. 2--23], as well as a polylog-round $(1+\epsilon)$-approximation algorithm in the Local model that we show here, our lower bounds for the Congest model imply a strict separation between the Local and Congest models. Notably, to the best of our knowledge, this is the first separation between these models for a local approximation problem. Similarly, a separation between the directed and undirected cases is implied. We also prove hardness results for weighted $k$-spanners and for unweighted undirected $k$-spanners for $k \geq 4$ in the Congest model. In addition, we show lower bounds for the minimum weighted 2-spanner problem in the Congest and Local models. On the algorithmic side, apart from the aforementioned $(1+\epsilon)$-approximation algorithm for minimum $k$-spanners, our main contribution is a new distributed construction of minimum 2-spanners that uses only polynomial local computations. Our algorithm has a guaranteed approximation ratio of $O(\log(m/n))$ for a graph with $n$ vertices and $m$ edges, which matches the best known ratio for polynomial-time sequential algorithms [G. Kortsarz and D. Peleg, J. Algorithms, 17 (1994), pp. 222--236], and is tight if we restrict ourselves to polynomial local computations. An algorithm with this approximation factor was not previously known for the distributed setting. The number of rounds required for our algorithm is $O(\log{n}\log{\Delta})$ with high probability, where $\Delta$ is the maximum degree in the graph. Our approach allows us to extend our algorithm to work also for the directed, weighted, and client-server variants of the problem. It also provides a Congest algorithm for the minimum dominating set problem, with a guaranteed $O(\log{\Delta})$ approximation ratio.},
  archive      = {J_SICOMP},
  author       = {Keren Censor-Hillel and Michal Dory},
  doi          = {10.1137/20M1312630},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {1103-1147},
  shortjournal = {SIAM J. Comput.},
  title        = {Distributed spanner approximation},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral analysis of matrix scaling and operator scaling.
<em>SICOMP</em>, <em>50</em>(3), 1034–1102. (<a
href="https://doi.org/10.1137/20M1315981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a spectral analysis of a continuous scaling algorithm for matrix scaling and operator scaling. The main result is that if the input matrix or operator has a spectral gap, then a natural gradient flow has linear convergence. This implies that a simple gradient descent algorithm also has linear convergence under the same assumption. The spectral gap condition for operator scaling is closely related to the notion of quantum expander studied in quantum information theory. The spectral analysis also provides bounds on some important quantities of the scaling problems, such as the condition number of the scaling solution and the capacity of the matrix and operator. These results can be used in various applications of scaling problems, including matrix scaling on expander graphs, permanent lower bounds on random matrices, the Paulsen problem on random frames, and Brascamp--Lieb constants on random operators. In some applications, the inputs of interest satisfy the spectral condition and we prove significantly stronger bounds than the worst case bounds.},
  archive      = {J_SICOMP},
  author       = {Tsz Chiu Kwok and Lap Chi Lau and Akshay Ramachandran},
  doi          = {10.1137/20M1315981},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {1034-1102},
  shortjournal = {SIAM J. Comput.},
  title        = {Spectral analysis of matrix scaling and operator scaling},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NC algorithms for computing a perfect matching and a maximum
flow in one-crossing-minor-free graphs. <em>SICOMP</em>, <em>50</em>(3),
1014–1033. (<a href="https://doi.org/10.1137/19M1256221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1988, Vazirani gave an NC algorithm for computing the number of perfect matchings in $K_{3,3}$-minor-free graphs by building on Kasteleyn&#39;s scheme for planar graphs, and stated that this “opens up the possibility of obtaining an NC algorithm for finding a perfect matching in $K_{3,3}$-free graphs.” In this paper, we finally settle this 30-year-old open problem. Building on recent NC algorithms for planar and bounded-genus perfect matching by Anari and Vazirani and later by Sankowski, we obtain NC algorithms for perfect matching in any minor-closed graph family that forbids a one-crossing graph. This family includes several well-studied graph families including the $K_{3,3}$-minor-free graphs and $K_5$-minor-free graphs. Graphs in these families not only have unbounded genus, but can have genus as high as $O(n)$. Our method applies as well to several other problems related to perfect matching. In particular, we obtain NC algorithms for the following problems in any family of graphs (or networks) with a one-crossing forbidden minor: (1) Determining whether a given graph has a perfect matching and, if so, finding one. (2) Finding a minimum-weight perfect matching in the graph, assuming that the edge weights are polynomially bounded. (3) Finding a maximum $st$-flow in the network, with arbitrary capacities. The main new idea enabling our results is the definition and use of matching-mimicking networks, small replacement networks that behave the same with respect to matching problems involving a fixed set of terminals, as the larger network they replace.},
  archive      = {J_SICOMP},
  author       = {David Eppstein and Vijay V. Vazirani},
  doi          = {10.1137/19M1256221},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {1014-1033},
  shortjournal = {SIAM J. Comput.},
  title        = {NC algorithms for computing a perfect matching and a maximum flow in one-crossing-minor-free graphs},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantum hardness of learning shallow classical circuits.
<em>SICOMP</em>, <em>50</em>(3), 972–1013. (<a
href="https://doi.org/10.1137/20M1344202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the quantum learnability of constant-depth classical circuits under the uniform distribution and in the distribution-independent framework of probably approximately correct (PAC) learning. In order to attain our results, we establish connections between quantum learning and quantum-secure cryptosystems. We then achieve the following results. 1. Hardness of PAC learning ${AC}^0$ and ${TC}^0$ under the uniform distribution. Our first result concerns the concept class ${TC}^0$ (resp., ${AC}^0$), the class of constant-depth, polynomial-sized circuits with unbounded fan-in majority gates (resp., ${AND}, {OR}, {NOT}$ gates). We show the following: if there exists no quantum (quasi-)polynomial-time algorithm to solve the ring-learning with errors (${RLWE}$) problem, then there exists no (quasi-)polynomial-time quantum learning algorithm for ${TC}^0$; and if there exists no $2^{O(d^{1/\eta})}$-time quantum algorithm to solve ${RLWE}$ with dimension $d = O(polylog n)$ (for every constant $\eta &gt; 2$), then there exists no $O(n^{ \log^{\nu} n} )$-time quantum learning algorithm for $poly(n)$-sized ${AC}^0$ circuits (for a constant $\nu&gt;0$), matching the classical upper bound of Linial, Mansour and Nisan [J. ACM, 40 (1993), pp. 607--620], where the learning algorithms are under the uniform distribution (even with access to quantum membership queries). The main technique in these results uses an explicit family of pseudorandom functions that are believed to be quantum-secure to construct concept classes that are hard to learn quantumly under the uniform distribution. 2. Hardness of learning ${TC}^0_2$ in the PAC setting. Our second result shows that if there exists no quantum polynomial-time algorithm for the ${LWE}$ problem, then there exists no polynomial-time quantum-PAC learning algorithm for the class ${TC}^0_2$, i.e., depth-2 ${TC}^0$ circuits. The main technique in this result is to establish a connection between the quantum security of public-key encryption schemes and the learnability of a concept class that consists of decryption functions of the cryptosystem. Our results show that quantum resources do not give an exponential improvement to learning constant-depth polynomial-sized neural networks. This also gives a strong (conditional) negative answer to one of the “Ten Semi-Grand Challenges for Quantum Computing Theory&quot; raised by Aaronson https://www.scottaaronson.com/writings/qchallenge.html, 2005.},
  archive      = {J_SICOMP},
  author       = {Srinivasan Arunachalam and Alex Bredariol Grilo and Aarthi Sundaram},
  doi          = {10.1137/20M1344202},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {972-1013},
  shortjournal = {SIAM J. Comput.},
  title        = {Quantum hardness of learning shallow classical circuits},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From independent sets and vertex colorings to isotropic
spaces and isotropic decompositions: Another bridge between graphs and
alternating matrix spaces. <em>SICOMP</em>, <em>50</em>(3), 924–971. (<a
href="https://doi.org/10.1137/19M1299128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 1970s, Lovász built a bridge between graphs and alternating matrix spaces, in the context of perfect matchings [Proceedings of FCT, 1979, pp. 565--574]. A similar connection between bipartite graphs and matrix spaces plays a key role in the recent resolutions of the noncommutative rank problem [A. Garg et al., Proceedings of FOCS, 2016, pp. 109--117; G. Ivanyos, Y. Qiao, and K. V. Subrahmanyam, Comput. Complexity, 26 (2017), pp. 717--763]. In this paper, we lay the foundation for another bridge between graphs and alternating matrix spaces, in the context of independent sets and vertex colorings. The corresponding structures in alternating matrix spaces are isotropic spaces and isotropic decompositions, both useful structures in group theory and manifold theory. We first show that the maximum independent set problem and the vertex $c$-coloring problem reduce to the maximum isotropic space problem and the isotropic $c$-decomposition problem, respectively. Next, we show that several topics and results about independent sets and vertex colorings have natural correspondences for isotropic spaces and decompositions. These include algorithmic problems, such as the maximum independent set problem for bipartite graphs, and exact exponential-time algorithms for the chromatic number, as well as mathematical questions, such as the number of maximal independent sets, and the relation between the maximum degree and the chromatic number. These connections lead to new interactions between graph theory and algebra. Some results have concrete applications to group theory and manifold theory, and we initiate a variant of these structures in the context of quantum information theory. Finally, we propose several open questions for further exploration.},
  archive      = {J_SICOMP},
  author       = {Xiaohui Bei and Shiteng Chen and Ji Guan and Youming Qiao and Xiaoming Sun},
  doi          = {10.1137/19M1299128},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {924-971},
  shortjournal = {SIAM J. Comput.},
  title        = {From independent sets and vertex colorings to isotropic spaces and isotropic decompositions: Another bridge between graphs and alternating matrix spaces},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metrical task systems on trees via mirror descent and unfair
gluing. <em>SICOMP</em>, <em>50</em>(3), 909–923. (<a
href="https://doi.org/10.1137/19M1237879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider metrical task systems on tree metrics and present an $O(\mathrm{depth} \times \log n)$-competitive randomized algorithm based on the mirror descent framework introduced in our prior work on the $k$-server problem. For the special case of hierarchically separated trees (HSTs), we use mirror descent to refine the standard approach based on gluing unfair metrical task systems. This yields an $O(\log n)$-competitive algorithm for HSTs, thus removing an extraneous $\log\log n$ in the bound of Fiat and Mendel (2003). Combined with well-known HST embedding theorems, this also gives an $O((\log n)^2)$-competitive randomized algorithm for every $n$-point metric space.},
  archive      = {J_SICOMP},
  author       = {Sébastien Bubeck and Michael B. Cohen and James R. Lee and Yin Tat Lee},
  doi          = {10.1137/19M1237879},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {909-923},
  shortjournal = {SIAM J. Comput.},
  title        = {Metrical task systems on trees via mirror descent and unfair gluing},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to use indistinguishability obfuscation: Deniable
encryption, and more. <em>SICOMP</em>, <em>50</em>(3), 857–908. (<a
href="https://doi.org/10.1137/15M1030108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new technique, that we call punctured programs, to apply indistinguishability obfuscation towards cryptographic problems. We use this technique to carry out a systematic study of the applicability of indistinguishability obfuscation to a variety of cryptographic goals. Along the way, we resolve the 16-year-old open question of deniable encryption, posed by Canetti et al. in 1997: In deniable encryption, a sender who is forced to reveal to an adversary both her message and the randomness she used for encrypting it should be able to convincingly provide “fake” randomness that can explain any alternative message that she would like to pretend that she sent. We resolve this question by giving the first construction of deniable encryption that does not require any preplanning by the party that must later issue a denial. In addition, we show the generality of our punctured programs technique by also constructing a variety of core cryptographic objects from indistinguishability obfuscation and one-way functions (or close variants). In particular we obtain public-key encryption, short “hash-and-sign” selectively secure signatures, chosen-ciphertext secure public-key encryption, noninteractive zero knowledge arguments and injective trapdoor functions. These results suggest the possibility of indistinguishability obfuscation becoming a “central hub” for cryptography.},
  archive      = {J_SICOMP},
  author       = {Amit Sahai and Brent Waters},
  doi          = {10.1137/15M1030108},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {857-908},
  shortjournal = {SIAM J. Comput.},
  title        = {How to use indistinguishability obfuscation: Deniable encryption, and more},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Near-optimal approximate shortest paths and transshipment in
distributed and streaming models. <em>SICOMP</em>, <em>50</em>(3),
815–856. (<a href="https://doi.org/10.1137/19M1286955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for solving the transshipment problem---also known as uncapacitated minimum cost flow---up to a multiplicative error of $ 1 + \varepsilon $ in undirected graphs with nonnegative edge weights using a tailored gradient descent algorithm. Using $\widetilde{O}(\cdot)$ to hide polylogarithmic factors in $n$ (the number of nodes in the graph), our gradient descent algorithm takes $ \widetilde O(\varepsilon^{-2}) $ iterations, and in each iteration it solves an instance of the transshipment problem up to a multiplicative error of $ \polylog n $. In particular, this allows us to perform a single iteration by computing a solution on a sparse spanner of logarithmic stretch. Using a randomized rounding scheme, we can further extend the method to finding approximate solutions for the single-source shortest paths (SSSP) problem. As a consequence, we improve upon prior works by obtaining the following results: (1) Broadcast CONGEST model: $ (1 + \varepsilon) $-approximate SSSP using $ \widetilde{O} ((\sqrt{n} + D) \varepsilon^{-3}) $ rounds, where $ D $ is the (hop) diameter of the network. (2) Broadcast Congested Clique model: $ (1 + \varepsilon) $-approximate transshipment and SSSP using $ \widetilde{O} (\varepsilon^{-2}) $ rounds. (3) Multipass Streaming model: $ (1 + \varepsilon) $-approximate transshipment and SSSP using $ \widetilde{O} (n) $ space and $ \widetilde{O} (\varepsilon^{-2}) $ passes. The previously fastest SSSP algorithms for these models leverage sparse hop sets. We bypass the hop set construction; computing a spanner is sufficient with our method. The above bounds assume nonnegative edge weights that are polynomially bounded in $n$; for general nonnegative weights, there is an additional multiplicative overhead equal to the logarithm of the maximum ratio between nonzero weights. Our algorithms can also handle asymmetric costs for traversing edges in opposite directions. In this case, we obtain an additional multiplicative dependence of the maximum ratio between the two costs on some edge.},
  archive      = {J_SICOMP},
  author       = {Ruben Becker and Sebastian Forster and Andreas Karrenbauer and Christoph Lenzen},
  doi          = {10.1137/19M1286955},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {815-856},
  shortjournal = {SIAM J. Comput.},
  title        = {Near-optimal approximate shortest paths and transshipment in distributed and streaming models},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the power of relaxed local decoding algorithms.
<em>SICOMP</em>, <em>50</em>(2), 788–813. (<a
href="https://doi.org/10.1137/19M1307834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A locally decodable code (LDC) $C \colon {0,1}^k \to {0,1}^n$ is an error correcting code wherein individual bits of the message can be recovered by only querying a few bits of a noisy codeword. LDCs found a myriad of applications both in theory and in practice, ranging from probabilistically checkable proofs to distributed storage. However, despite nearly two decades of extensive study, the best known constructions of $O(1)$-query LDCs have superpolynomial blocklength. The notion of relaxed LDCs is a natural relaxation of LDCs, which aims to bypass the foregoing barrier by requiring local decoding of nearly all individual message bits, yet allowing decoding failure (but not error) on the rest. State of the art constructions of $O(1)$-query relaxed LDCs achieve blocklength $n = O\left(k^{1+ \gamma}\right)$ for an arbitrarily small constant $\gamma$. We prove a lower bound which shows that $O(1)$-query relaxed LDCs cannot achieve blocklength $n = k^{1+ o(1)}$. This resolves an open problem raised by Goldreich in 2004.},
  archive      = {J_SICOMP},
  author       = {Tom Gur and Oded Lachish},
  doi          = {10.1137/19M1307834},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {788-813},
  shortjournal = {SIAM J. Comput.},
  title        = {On the power of relaxed local decoding algorithms},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient algorithm for generalized polynomial partitioning
and its applications. <em>SICOMP</em>, <em>50</em>(2), 760–787. (<a
href="https://doi.org/10.1137/19M1268550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2015, Guth proved that if $\EuScript{S}$ is a collection of $n$ $g$-dimensional semialgebraic sets in ${\mathbb{R}}^d$ and if $D\geq 1$ is an integer, then there is a $d$-variate polynomial $P$ of degree at most $D$ so that each connected component of $\mathbb{R}^d\setminus Z(P)$ intersects $O(n/D^{d-g})$ sets from $\EuScript{S}$. Such a polynomial is called a generalized partitioning polynomial. We present a randomized algorithm that computes such polynomials efficiently---the expected running time of our algorithm is linear in $\lvert \EuScript{S}\rvert$. Our approach exploits the technique of quantifier elimination combined with that of $\eps$-samples. We also present an extension of our construction to multilevel polynomial partitioning for semialgebraic sets in $\mathbb{R}^d$. We present five applications of our result. The first is a data structure for answering point-enclosure queries among a family of semialgebraic sets in $\mathbb{R}^d$ in $O(\log n)$ time, with storage complexity and expected preprocessing time of $O(n^{d+\eps})$. The second is a data structure for answering range-searching queries with semialgebraic ranges in $\mathbb{R}^d$ in $O(\log n)$ time, with $O(n^{t+\eps})$ storage and expected preprocessing time, where $t &gt; 0$ is an integer that depends on $d$ and the description complexity of the ranges. The third is a data structure for answering vertical ray-shooting queries among semialgebraic sets in $\mathbb{R}^{d}$ in $O(\log^2 n)$ time, with $O(n^{d+\eps})$ storage and expected preprocessing time. The fourth is an efficient algorithm for cutting algebraic curves in $\mathbb{R}^2$ into pseudosegments. The fifth application is for eliminating depth cycles among triangles in $\mathbb{R}^3$, where we show a nearly optimal algorithm to cut $n$ pairwise disjoint nonvertical triangles in ${\mathbb{R}}^3$ into pieces that form a depth order.},
  archive      = {J_SICOMP},
  author       = {Pankaj K. Agarwal and Boris Aronov and Esther Ezra and Joshua Zahl},
  doi          = {10.1137/19M1268550},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {760-787},
  shortjournal = {SIAM J. Comput.},
  title        = {Efficient algorithm for generalized polynomial partitioning and its applications},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective wireless scheduling via hypergraph sketches.
<em>SICOMP</em>, <em>50</em>(2), 718–759. (<a
href="https://doi.org/10.1137/19M1275085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An overarching issue in resource management of wireless networks is assessing their capacity: How much communication can be achieved in a network, utilizing all the tools available: power control, scheduling, routing, channel assignment, and rate adjustment? We propose the first framework for approximation algorithms in the physical model of wireless interference that addresses these questions in full in interference-limited networks. The approximations obtained are at most doubly logarithmic in the link length and rate diversity. Where previous bounds are known, this gives an exponential improvement or better. The key insight is the discovery that a properly chosen power assignment infers a form of locality, or tolerance to the interference of both shorter and longer communication links. This allows us to simplify the complex interference relationship of the physical model into a new form of conflict graphs, at a small cost. We also show that the approximation obtained is provably the best possible for any conflict graph formulation.},
  archive      = {J_SICOMP},
  author       = {Magnús M. Halldórsson and Tigran Tonoyan},
  doi          = {10.1137/19M1275085},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {718-759},
  shortjournal = {SIAM J. Comput.},
  title        = {Effective wireless scheduling via hypergraph sketches},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The communication complexity of set intersection and
multiple equality testing. <em>SICOMP</em>, <em>50</em>(2), 674–717. (<a
href="https://doi.org/10.1137/20M1326040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we explore fundamental problems in randomized communication complexity such as computing SetIntersection on sets of size $k$ and EqualityTesting between vectors of length $k$. Sağlam and Tardos [Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science, 2013, pp. 678--687] and Brody et al. [Algorithmica, 76 (2016), pp. 796--845] showed that for these types of problems, one can achieve optimal communication volume of $O(k)$ bits, with a randomized protocol that takes $O(\log^* k)$ rounds. They also proved that this is one point along the optimal round-communication trade-off curve. Aside from rounds and communication volume, there is a third parameter of interest, namely the error probability $p_{{err}}$, which we write $2^{-E}$. It is straightforward to show that protocols for SetIntersection or EqualityTesting need to send at least $\Omega(k + E)$ bits, regardless of the number of rounds. Is it possible to simultaneously achieve optimality in all three parameters, namely $O(k + E)$ communication and $O(\log^* k)$ rounds? In this paper we prove that there is no universally optimal algorithm, and we complement the existing round-communication trade-offs [M. Sağlam and G. Tardos, Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science, 2013, pp. 678--687; J. Brody et al., Algorithmica, 76 (2016), pp. 796--845] with a new trade-off between rounds, communication, and probability of error. In particular, any protocol for solving multiple EqualityTesting in $r$ rounds with failure probability $p_{{err}} = 2^{-E}$ has communication volume $\Omega(Ek^{1/r})$. We present several algorithms for multiple EqualityTesting (and its variants) that match or nearly match our lower bound and the lower bound of [M. Sağlam and G. Tardos, Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science, 2013, pp. 678--687; J. Brody et al., Algorithmica, 76 (2016), pp. 796--845]. Lower bounds on EqualityTesting extend to SetIntersection for every $r, k,$ and $p_{{err}}$ (which is trivial); in the reverse direction, we prove that upper bounds on EqualityTesting for $r, k, p_{{err}}$ imply similar upper bounds on SetIntersection with parameters $r+1, k,$ and $p_{{err}}$. Our original motivation for considering $p_{{err}}$ as an independent parameter came from the problem of enumerating triangles in distributed (${CONGEST}$) networks having maximum degree $\Delta$. We prove that this problem can be solved in $O(\Delta/\log n + \log\log \Delta)$ time with high probability $1-1/{poly}(n)$. This beats the trivial (deterministic) $O(\Delta)$-time algorithm and is superior to the $\tilde{O}(n^{1/3})$ algorithm of [Y. Chang, S. Pettie, and H. Zhang, Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms, 2019, pp. 821--840; Y. Chang and T. Saranurak, Proceedings of the ACM Symposium on Principles of Distributed Computing, 2019, pp. 66--73] when $\Delta=\tilde{O}(n^{1/3})$.},
  archive      = {J_SICOMP},
  author       = {Dawei Huang and Seth Pettie and Yixiang Zhang and Zhijun Zhang},
  doi          = {10.1137/20M1326040},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {674-717},
  shortjournal = {SIAM J. Comput.},
  title        = {The communication complexity of set intersection and multiple equality testing},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New results on linear size distance preservers.
<em>SICOMP</em>, <em>50</em>(2), 662–673. (<a
href="https://doi.org/10.1137/19M123662X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given $p$ node pairs in an $n$-node graph, a distance preserver is a sparse subgraph that agrees with the original graph on all of the given pairwise distances. We prove the following bounds on the number of edges needed for a distance preserver: 1. Any $p$ node pairs in a directed weighted graph have a distance preserver on $O(n + n^{2/3} p)$ edges. 2. Any $p = \Omega(\frac{n^2}{{\tt RS}(n)})$ node pairs in an undirected unweighted graph have a distance preserver on $O(p)$ edges, where ${\tt RS}(n)$ is the Ruzsa--Szemerédi function from combinatorial graph theory. 3. As a lower bound, there are examples where one needs $\omega(\sigma^2)$ edges to preserve all pairwise distances within a subset of $\sigma = o(n^{2/3})$ nodes in an undirected weighted graph. If we additionally require that the graph is unweighted, then the range of this lower bound falls slightly to $\sigma \le n^{2/3 - o(1)}$.},
  archive      = {J_SICOMP},
  author       = {Greg Bodwin},
  doi          = {10.1137/19M123662X},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {662-673},
  shortjournal = {SIAM J. Comput.},
  title        = {New results on linear size distance preservers},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Isolating a vertex via lattices: Polytopes with totally
unimodular faces. <em>SICOMP</em>, <em>50</em>(2), 636–661. (<a
href="https://doi.org/10.1137/19M1290802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a geometric approach toward derandomizing the isolation lemma of Mulmuley, Vazirani, and Vazirani. We construct a quasi-polynomial family of weights that isolate a vertex in any 0/1-polytope for which each face spans an affine space defined by a totally unimodular matrix. These polytopes are also called box-totally dual integral or principally box-integer. This includes the polytopes given by totally unimodular constraints and generalizes the recent derandomization of the isolation lemma for bipartite perfect matching and matroid intersection. We prove our result by associating a lattice to each face of the polytope and showing that if there is a totally unimodular kernel matrix for this lattice, then the number of vectors of length within 3/2 of the shortest vector in it is polynomially bounded. The proof of this latter geometric fact is combinatorial and follows from a polynomial bound on the number of circuits of size within 3/2 of the shortest circuit in a regular matroid. This is the technical core of the paper and relies on a variant of Seymour&#39;s decomposition theorem for regular matroids. It generalizes an influential result by Karger on the number of minimum cuts in a graph to regular matroids.},
  archive      = {J_SICOMP},
  author       = {Rohit Gurjar and Thomas Thierauf and Nisheeth K. Vishnoi},
  doi          = {10.1137/19M1290802},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {636-661},
  shortjournal = {SIAM J. Comput.},
  title        = {Isolating a vertex via lattices: Polytopes with totally unimodular faces},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An <span
class="math inline"><em>O</em>(<em>n</em>log <em>n</em>)</span>-time
algorithm for the <span class="math inline"><em>k</em></span>-center
problem in trees. <em>SICOMP</em>, <em>50</em>(2), 602–635. (<a
href="https://doi.org/10.1137/18M1196522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a classical $k$-center problem in trees. Let $T$ be a tree of $n$ vertices such that every vertex has a nonnegative weight. The problem is to find $k$ centers on the edges of $T$ such that the maximum weighted distance from all vertices to their closest centers is minimized. Megiddo and Tamir [SIAM J. Comput., 12 (1983), pp. 751--758] gave an algorithm that can solve the problem in $O(n\log^2 n)$ time by using Cole&#39;s parametric search. Since then it has been open for over three decades whether the problem can be solved in $O(n\log n)$ time. In this paper, we present an $O(n\log n)$ time algorithm for the problem and thus settle the open problem affirmatively.},
  archive      = {J_SICOMP},
  author       = {Haitao Wang and Jingru Zhang},
  doi          = {10.1137/18M1196522},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {602-635},
  shortjournal = {SIAM J. Comput.},
  title        = {An $O(n\log n)$-time algorithm for the $k$-center problem in trees},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Algorithms for weighted matching generalizations II:
F-factors and the special case of shortest paths. <em>SICOMP</em>,
<em>50</em>(2), 555–601. (<a
href="https://doi.org/10.1137/16M1106225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an undirected graph or multigraph $G=(V,E)$ and a function $f:V\to \mathbb{Z_+}$, an $f$-factor is a subgraph whose degree function is $f$. For integral edge weights of maximum magnitude $W$ our algorithm finds a maximum weight $f$-factor in time $\tilde{O}(Wf(V)^{\omega})$, where $f(V)=\sum_{v\in V} f(v)$ and $\omega$ is the exponent of matrix multiplication. The algorithm is randomized and has two versions. For worst-case time the algorithm is correct with high probability. For expected time the algorithm is Las Vegas. The algorithm is based on a detailed analysis of the structure of the optimum blossoms. A special case gives a representation for single-source shortest-paths in conservative undirected graphs, generalizing the standard shortest-path tree to a “tree of cycles”. The representation can be constructed by a randomized algorithm with the same time bound as above, or deterministically by an algorithm for maximum weight matching, achieving time $O(n(m + n \log n))$ or $O(\sqrt{n }\ m \log (nW))$.},
  archive      = {J_SICOMP},
  author       = {Harold N. Gabow and Piotr Sankowski},
  doi          = {10.1137/16M1106225},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {555-601},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithms for weighted matching generalizations II: F-factors and the special case of shortest paths},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Voronoi diagrams on planar graphs, and computing the
diameter in deterministic <span
class="math inline"><em>Õ</em>(<em>n</em><sup>5/3</sup>)</span> time.
<em>SICOMP</em>, <em>50</em>(2), 509–554. (<a
href="https://doi.org/10.1137/18M1193402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an explicit and efficient construction of additively weighted Voronoi diagrams on planar graphs. Let $G$ be a planar graph with $n$ vertices and $b$ sites that lie on a constant number of faces. We show how to preprocess $G$ in $\tilde O(nb^2)$ time so that one can compute any additively weighted Voronoi diagram for these sites in $\tilde O(b)$ time. We use this construction to compute the diameter of a directed planar graph with real arc lengths in $\tilde{O}(n^{5/3})$ time. This improves the recent breakthrough result of Cabello [SODA 2017, SIAM, Philadelphia, 2017, pp. 2143--2152], both by improving the running time (from $\tilde{O}(n^{11/6})$), and by providing a deterministic algorithm. It is in fact the first truly subquadratic deterministic algorithm for this problem. Our use of Voronoi diagrams to compute the diameter follows that of Cabello, but he used abstract Voronoi diagrams, which makes his diameter algorithm more involved, more expensive, and randomized. As in Cabello&#39;s work, our algorithm can compute, for every vertex $v$, both the farthest vertex from $v$ (i.e., the eccentricity of $v$), and the sum of distances from $v$ to all other vertices. Hence, our algorithm can also compute the radius, median, and Wiener index (sum of all pairwise distances) of a planar graph within the same time bounds. Our construction of Voronoi diagrams for planar graphs is of independent interest.},
  archive      = {J_SICOMP},
  author       = {Paweł Gawrychowski and Haim Kaplan and Shay Mozes and Micha Sharir and Oren Weimann},
  doi          = {10.1137/18M1193402},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {509-554},
  shortjournal = {SIAM J. Comput.},
  title        = {Voronoi diagrams on planar graphs, and computing the diameter in deterministic $\tilde{O}(n^{5/3})$ time},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained complexity of the graph homomorphism problem
for bounded-treewidth graphs. <em>SICOMP</em>, <em>50</em>(2), 487–508.
(<a href="https://doi.org/10.1137/20M1320146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a fixed graph $H$, by Hom($H$) we denote the computational problem which asks whether a given graph $G$ admits a homomorphism to $H$, i.e., an edge-preserving mapping from $V(G)$ to $V(H)$. As Hom($K_k$) is equivalent to $k$-Coloring, graph homomorphisms can be seen as generalizations of colorings. It is known that Hom($H$) is polynomial-time solvable if $H$ is bipartite or has a vertex with a loop, and NP-complete otherwise [Hell and Nešetřil, J. Comb. Theory Ser. B, 48 (1990), pp. 92--110]. In this paper we are interested in the complexity of the problem, parameterized by the treewidth of the input graph $G$. If $G$ has $n$ vertices and is given along with its tree decomposition of width ${tw}(G)$, then the problem can be solved in time $|V(H)|^{{tw}(G)} \cdot n^{\mathcal{O}(1)}$, using a straightforward dynamic programming. We explore whether this bound can be improved. We show that if $H$ is a projective core, then the existence of such a faster algorithm is unlikely: assuming the Strong Exponential Time Hypothesis, the Hom($H$) problem cannot be solved in time $(|V(H)|-\epsilon)^{{tw}(G)} \cdot n^{\mathcal{O}(1)}$, for any $\epsilon &gt; 0$. This result provides a full complexity characterization for a large class of graphs $H$, as almost all graphs are projective cores. We also notice that the naive algorithm can be improved for some graphs $H$ and show a complexity classification for all graphs $H$, assuming two conjectures from algebraic graph theory. In particular, there are no known graphs $H$ which are not covered by our result.},
  archive      = {J_SICOMP},
  author       = {Karolina Okrasa and Paweł Rzaͅżewski},
  doi          = {10.1137/20M1320146},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {487-508},
  shortjournal = {SIAM J. Comput.},
  title        = {Fine-grained complexity of the graph homomorphism problem for bounded-treewidth graphs},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Algorithms for weighted matching generalizations i:
Bipartite graphs, b-matching, and unweighted f-factors. <em>SICOMP</em>,
<em>50</em>(2), 440–486. (<a
href="https://doi.org/10.1137/16M1106195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $G=(V,E)$ be a weighted graph or multigraph, with $f$ or $b$ a function assigning a nonnegative integer to each vertex. An $f$-factor is a subgraph whose degree function is $f$; a perfect $b$-matching is a $b$-factor in the graph formed from $G$ by adding an unlimited number of copies of each edge. This two-part paper culminates in an efficient algebraic algorithm to find a maximum $f$-factor, i.e., $f$-factor with maximum weight. Along the way it presents simpler special cases of interest. Part II presents the maximum $f$-factor algorithm and the special case of shortest paths in conservative undirected graphs (negative edges allowed). Part I presents these results: An algebraic algorithm for maximum $b$-matching, i.e., maximum weight $b$-matching. It is almost identical to its special case $b\equiv 1$, ordinary weighted matching. The time is $O(Wb(V)^{\omega})$ for $W$ the maximum magnitude of an edge weight, $b(V)=\sum_{v\in V} b(v)$, and $\omega&lt;2.373$ the exponent of matrix multiplication. An algebraic algorithm to find an $f$-factor. The time is $O(f(V)^{\omega})$ for $f(V)=\sum_{v\in V} f(v)$. The specialization of the $f$-factor algorithm to bipartite graphs and its extension to maximum/minimum bipartite $f$-factors. This improves the known complexity bounds for vertex capacitated max-flow and min-cost max-flow on a subclass of graphs. Each algorithm is randomized and has two versions achieving the above time bound: For worst-case time the algorithm is correct with high probability. For expected time the algorithm is Las Vegas.},
  archive      = {J_SICOMP},
  author       = {Harold N. Gabow and Piotr Sankowski},
  doi          = {10.1137/16M1106195},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {440-486},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithms for weighted matching generalizations i: Bipartite graphs, b-matching, and unweighted f-factors},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perfect <span
class="math inline"><em>L</em><sub><em>p</em></sub></span> sampling in a
data stream. <em>SICOMP</em>, <em>50</em>(2), 382–439. (<a
href="https://doi.org/10.1137/18M1229912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we resolve the one-pass space complexity of perfect $L_p$ sampling for $p \in (0,2)$ in a stream. Given a stream of updates (insertions and deletions) to the coordinates of an underlying vector $f \in \mathbb{R}^n$, a perfect $L_p$ sampler must output an index $i$ with probability $|f_i|^p/\|f\|_p^p$ and is allowed to fail with some probability $\delta$. So far, for $p &gt; 0$ no algorithm has been shown to solve the problem exactly using ${poly}( \log n)$-bits of space. In 2010, Monemizadeh and Woodruff introduced an approximate $L_p$ sampler which, given an approximation parameter $\nu$, outputs $i$ with probability $(1 \pm \nu)|f_i|^p /\|f\|_p^p$, using space polynomial in $\nu^{-1}$ and $\log(n)$. The space complexity was later reduced by Jowhari, Sağlam, and Tardos to roughly $O(\nu^{-p} \log^2 n \log \delta^{-1})$ for $p \in (0,2)$, which matches the general $p\geq 0$ lower bound of $\Omega(\log^2 n \log \delta^{-1})$ in terms of $n$ and $\delta$, but is loose in terms of $\nu$. Given these nearly tight bounds, it is perhaps surprising that no lower bound exists in terms of $\nu$---not even a bound of $\Omega(\nu^{-1})$ is known. In this paper, we explain this phenomenon by demonstrating the existence of an $O(\log^2 n \log \delta^{-1})$-bit perfect $L_p$ sampler for $p \in (0,2)$. This shows that $\nu$ need not factor into the space of an $L_p$ sampler, which closes the complexity of the problem for this range of $p$. For $p=2$, our bound is $O(\log^3 n \log \delta^{-1})$-bits, which matches the prior best known upper bound of $O(\nu^{-2}\log^3n \log \delta^{-1})$, but has no dependence on $\nu$. Note that there is still a $\log n$ gap between our upper bound and the lower bound for $p=2$, the ution of which we leave as an open problem. For $p&lt;2$, our bound holds in the random oracle model, matching the lower bounds in that model. However, we show that our algorithm can be derandomized with only a $O((\log \log n)^2)$ blow-up in the space (and no blow-up for $p=2$). Our derandomization technique is quite general, and can be used to derandomize a large class of linear sketches, including the more accurate count-sketch variant of Minton and Price [Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, Philadelphia, 2014, pp. 669--686], resolving an open question in that paper. Finally, we show that a $(1\pm\epsilon)$ relative error estimate of the frequency $f_i$ of the sampled index $i$ can be obtained using an additional $O(\epsilon^{-p} \log n)$-bits of space for $p &lt; 2$, and $O( \epsilon^{-2} \log^2 n)$ bits for $p=2$, which was possible before only by running the prior algorithms with $\nu = \epsilon$.},
  archive      = {J_SICOMP},
  author       = {Rajesh Jayaram and David Woodruff},
  doi          = {10.1137/18M1229912},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {382-439},
  shortjournal = {SIAM J. Comput.},
  title        = {Perfect $L_p$ sampling in a data stream},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic sampling from graphical models. <em>SICOMP</em>,
<em>50</em>(2), 350–381. (<a
href="https://doi.org/10.1137/20M1315099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of sampling from a graphical model when the model itself is changing dynamically with time. This problem derives its interest from a variety of inference, learning, and sampling settings in machine learning, computer vision, statistical physics, and theoretical computer science. While the problem of sampling from a static graphical model has received considerable attention, theoretical works for its dynamic variants have been largely lacking. The main contribution of this paper is an algorithm that can sample dynamically from a broad class of graphical models over discrete random variables. Our algorithm is parallel and Las Vegas: it knows when to stop, and it outputs samples from the exact distribution. We also provide sufficient conditions under which this algorithm runs in time proportional to the size of the update on general graphical models as well as well-studied specific spin systems. In particular we obtain, for the Ising model (ferromagnetic or antiferromagnetic) and for the hardcore model the first dynamic sampling algorithms that can handle both edge and vertex updates (addition, deletion, and change of functions). The algorithms for both these models are efficient within regimes that are close to the respective uniqueness regimes, beyond which, even for the static and approximate sampling, no local algorithms were known or the problem itself is intractable. Our dynamic sampling algorithm relies on a local resampling algorithm and a new “equilibrium&quot; property that is shown to be satisfied by our algorithm at each step and enables us to prove its correctness. This equilibrium property is robust enough to guarantee the correctness of our algorithm, helps us improve bounds on fast convergence on specific models, and should be of independent interest.},
  archive      = {J_SICOMP},
  author       = {Weiming Feng and Nisheeth K. Vishnoi and Yitong Yin},
  doi          = {10.1137/20M1315099},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {350-381},
  shortjournal = {SIAM J. Comput.},
  title        = {Dynamic sampling from graphical models},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). List-decoding with double samplers. <em>SICOMP</em>,
<em>50</em>(2), 301–349. (<a
href="https://doi.org/10.1137/19M1276650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We strengthen the notion of double samplers, first introduced by Dinur and Kaufman [``High dimensional expanders imply agreement expanders,” in Proc. 58th IEEE Symp. on Foundations of Comp. Science, IEEE, 2017, pp. 974--985], which are samplers with additional combinatorial properties, and whose existence we prove using high-dimensional expanders. The ABNNR code construction [N. Alon et al., IEEE Trans. Inform. Theory, 38 (1992), pp. 509--516] achieves large distance by starting with a base code $C$ with moderate distance, and then amplifying the distance using a sampler. We show that if the sampler is part of a larger double sampler, then the construction has an efficient list-decoding algorithm. Our algorithm works even if the ABNNR construction is not applied to a base code $C$ but rather to any string. In this case the resulting code is approximate-list-decodable, i.e., the output list contains an approximation to the original input. Our list-decoding algorithm works as follows: It uses a local voting scheme from which it constructs a unique games constraint graph. The constraint graph is an expander, so we can solve unique games efficiently. These solutions are the output of the list-decoder. This is a novel use of a unique games algorithm as a subroutine in a decoding procedure, as opposed to the more common situation in which unique games are used for demonstrating hardness results. Double samplers and high-dimensional expanders are akin to pseudorandom objects in their utility, but they greatly exceed random objects in their combinatorial properties. We believe that these objects hold significant potential for coding theoretic constructions and view this work as demonstrating the power of double samplers in this context.},
  archive      = {J_SICOMP},
  author       = {Irit Dinur and Prahladh Harsha and Tali Kaufman and Inbal Livni Navon and Amnon Ta-Shma},
  doi          = {10.1137/19M1276650},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {301-349},
  shortjournal = {SIAM J. Comput.},
  title        = {List-decoding with double samplers},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online contention resolution schemes with applications to
bayesian selection problems. <em>SICOMP</em>, <em>50</em>(2), 255–300.
(<a href="https://doi.org/10.1137/18M1226130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new rounding technique designed for online optimization problems, which is related to contention resolution schemes, a technique initially introduced in the context of submodular function maximization. Our rounding technique, which we call online contention resolution schemes (OCRSs), is applicable to many online selection problems, including Bayesian online selection, oblivious posted pricing mechanisms, and stochastic probing models. It allows for handling a wide set of constraints and shares many strong properties of offline contention resolution schemes. In particular, OCRSs for different constraint families can be combined to obtain an OCRS for their intersection. Moreover, we can approximately maximize submodular functions in the online settings we consider. We thus get a broadly applicable framework for several online selection problems, which improves on previous approaches in terms of the types of constraints that can be handled, the objective functions that can be dealt with, and the assumptions on the strength of the adversary. Furthermore, we resolve two open problems from the literature; namely, we present the first constant-factor constrained oblivious posted price mechanism for matroid constraints and the first constant-factor algorithm for weighted stochastic probing with deadlines.},
  archive      = {J_SICOMP},
  author       = {Moran Feldman and Ola Svensson and Rico Zenklusen},
  doi          = {10.1137/18M1226130},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {255-300},
  shortjournal = {SIAM J. Comput.},
  title        = {Online contention resolution schemes with applications to bayesian selection problems},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The complexity of contracts. <em>SICOMP</em>,
<em>50</em>(1), 211–254. (<a
href="https://doi.org/10.1137/20M132153X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study of computing (near-)optimal contracts in succinctly representable principal-agent settings. Here optimality means maximizing the principal&#39;s expected payoff over all incentive-compatible contracts---known in economics as “second-best” solutions. We also study a natural relaxation to approximately incentive-compatible contracts. We focus on principal-agent settings with succinctly described (and exponentially large) outcome spaces. We show that the computational complexity of computing a near-optimal contract depends fundamentally on the number of agent actions. For settings with a constant number of actions, we present a fully polynomial-time approximation scheme (FPTAS) for the separation oracle of the dual of the problem of minimizing the principal&#39;s payment to the agent, and we use this subroutine to efficiently compute a $\delta$-incentive-compatible ($\delta$-IC) contract whose expected payoff matches or surpasses that of the optimal IC contract. With an arbitrary number of actions, we prove that the problem is hard to approximate within any constant $c$. This inapproximability result holds even for $\delta$-IC contracts where $\delta$ is a sufficiently rapidly-decaying function of $c$. On the positive side, we show that simple linear $\delta$-IC contracts with constant $\delta$ are sufficient to achieve a constant-factor approximation of the “first-best” (full-welfare-extracting) solution, and that such a contract can be computed in polynomial time.},
  archive      = {J_SICOMP},
  author       = {P. Dütting and T. Roughgarden and I. Talgam-Cohen},
  doi          = {10.1137/20M132153X},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {211-254},
  shortjournal = {SIAM J. Comput.},
  title        = {The complexity of contracts},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Query-to-communication lifting using low-discrepancy
gadgets. <em>SICOMP</em>, <em>50</em>(1), 171–210. (<a
href="https://doi.org/10.1137/19M1310153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifting theorems are theorems that relate the query complexity of a function $f:{0,1}^{n}\to {0,1}$ to the communication complexity of the composed function $f\circ g^{n}$ for some “gadget” $g:{ 0,1}^{b}\times {0,1}^{b}\to {0,1}$. Such theorems allow transferring lower bounds from query complexity to the communication complexity, and have seen numerous applications in recent years. In addition, such theorems can be viewed as a strong generalization of a direct-sum theorem for the gadget $g$. We prove a new lifting theorem that works for all gadgets $g$ that have logarithmic length and exponentially-small discrepancy, for both deterministic and randomized communication complexity. Thus, we significantly increase the range of gadgets for which such lifting theorems hold. Our result has two main motivations: first, allowing a larger variety of gadgets may support more applications. In particular, our work is the first to prove a randomized lifting theorem for logarithmic-size gadgets, thus improving some applications of the theorem. Second, our result can be seen as a strong generalization of a direct-sum theorem for functions with low discrepancy.},
  archive      = {J_SICOMP},
  author       = {Arkadev Chattopadhyay and Yuval Filmus and Sajin Koroth and Or Meir and Toniann Pitassi},
  doi          = {10.1137/19M1310153},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {171-210},
  shortjournal = {SIAM J. Comput.},
  title        = {Query-to-communication lifting using low-discrepancy gadgets},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum rectilinear convex subsets. <em>SICOMP</em>,
<em>50</em>(1), 145–170. (<a
href="https://doi.org/10.1137/19M1303010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $P$łabelpage1 be a set of $n$ points in the plane. We consider a variation of the classical Erdös--Szekeres problem, presenting efficient algorithms with $O(n^3)$ running time and $O(n^2)$ space complexity that compute (1) a subset $S$ of $P$ such that the boundary of the rectilinear convex hull of $S$ has the maximum number of points from $P$, (2) a subset $S$ of $P$ such that the boundary of the rectilinear convex hull of $S$ has the maximum number of points from $P$ and its interior contains no element of $P$, (3) a subset $S$ of $P$ such that the rectilinear convex hull of $S$ has maximum area and its interior contains no element of $P$, and (4) when each point of $P$ is assigned a weight, positive or negative, a subset $S$ of $P$ that maximizes the total weight of the points in the rectilinear convex hull of $S$. We also revisit the problems of computing a maximum area orthoconvex polygon and computing a maximum area staircase polygon, amidst a point set in a rectangular domain. We obtain new and simpler algorithms to solve both problems with the same complexity as in the state of the art.},
  archive      = {J_SICOMP},
  author       = {Hernán González-Aguilar and David Orden and Pablo Pérez-Lantero and David Rappaport and Carlos Seara and Javier Tejel and Jorge Urrutia},
  doi          = {10.1137/19M1303010},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {145-170},
  shortjournal = {SIAM J. Comput.},
  title        = {Maximum rectilinear convex subsets},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure versus hardness through the obfuscation lens.
<em>SICOMP</em>, <em>50</em>(1), 98–144. (<a
href="https://doi.org/10.1137/17M1136559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of modern cryptography, starting from public-key encryption and going beyond, is based on the hardness of structured (mostly algebraic) problems like factoring, discrete log, or finding short lattice vectors. While structure is perhaps what enables advanced applications, it also puts the hardness of these problems in question. In particular, this structure often puts them in low (and so-called structured) complexity classes such as $\mathsf{NP}\cap \mathsf{coNP}$ or statistical zero-knowledge ($\mathsf{SZK}$). Is this structure really necessary? For some cryptographic primitives, such as one-way permutations and homomorphic encryption, we know that the answer is yes---they imply hard problems in $\mathsf{NP}\cap \mathsf{coNP}$ and $\mathsf{SZK}$, respectively. In contrast, one-way functions do not imply such hard problems, at least not by black-box reductions. Yet, for many basic primitives such as public-key encryption, oblivious transfer, and functional encryption, we do not have any answer. We show that the above primitives, and many others, do not imply hard problems in $\mathsf{NP}\cap\mathsf{coNP}$ or $\mathsf{SZK}$ via black-box reductions. In fact, we first show that even the very powerful notion of indistinguishability obfuscation (IO) does not imply such hard problems, and then deduce the same for a large class of primitives that can be constructed from IO.},
  archive      = {J_SICOMP},
  author       = {Nir Bitansky and Akshay Degwekar and Vinod Vaikuntanathan},
  doi          = {10.1137/17M1136559},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {98-144},
  shortjournal = {SIAM J. Comput.},
  title        = {Structure versus hardness through the obfuscation lens},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perfect secure computation in two rounds. <em>SICOMP</em>,
<em>50</em>(1), 68–97. (<a
href="https://doi.org/10.1137/19M1272044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that any multiparty functionality can be evaluated using a 2-round protocol with perfect correctness and perfect semihonest security, provided that the majority of parties are honest. This settles the round complexity of information-theoretic semihonest multiparty computation, resolving a longstanding open question [Y. Ishai and E. Kushilevitz, Randomizing polynomials: A new representation with applications to round-efficient secure computation, in Proceedings of the 41st Annual Symposium on Foundations of Computer Science FOCS 2000, IEEE Computer Society, 2000, pp. 294--304]. The protocol is efficient for ${NC}^1$ functionalities. Furthermore, given black-box access to a one-way function, the protocol can be made efficient for any polynomial functionality, at the cost of only guaranteeing computational security. Our results are based on a new notion of multiparty randomized encoding which extends and relaxes the standard notion of randomized encoding of functions [Y. Ishai and E. Kushilevitz, Randomizing polynomials: A new representation with applications to round-efficient secure computation, in Proceedings of the 41st Annual Symposium on Foundations of Computer Science FOCS 2000, IEEE Computer Society, 2000, pp. 294--304]. The property of a multiparty randomized encoding (MPRE) is that if the functionality $g$ is an encoding of the functionality $f$, then for any (permitted) coalition of players, their respective outputs and inputs in $g$ allow them to simulate their respective inputs and outputs in $f$, without learning anything else, including the other outputs of $f$. We further introduce a new notion of effective degree, and show that the round complexity of a functionality $f$ is characterized by the degree of its MPRE. We construct degree-2 MPREs for general functionalities in several settings under different assumptions, and use these constructions to obtain 2-round protocols. Our constructions also give rise to new protocols in the client-server model with optimal round complexity.},
  archive      = {J_SICOMP},
  author       = {Benny Applebaum and Zvika Brakerski and Rotem Tsabary},
  doi          = {10.1137/19M1272044},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {68-97},
  shortjournal = {SIAM J. Comput.},
  title        = {Perfect secure computation in two rounds},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional disclosure of secrets: Amplification, closure,
amortization, lower-bounds, and separations. <em>SICOMP</em>,
<em>50</em>(1), 32–67. (<a
href="https://doi.org/10.1137/18M1217097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the conditional disclosure of secrets (CDS) problem [Gertner et al., J. Comput. System Sci., 60 (2000), pp. 592--629] Alice and Bob, who hold inputs $x$ and $y$, respectively, wish to release a common secret $s$ to Carol (who knows both $x$ and $y$) if and only if the input $(x,y)$ satisfies some predefined predicate $f$. Alice and Bob are allowed to send a single message to Carol which may depend on their inputs and some joint randomness and the goal is to minimize the communication complexity while providing information-theoretic security. In this work, we initiate the study of CDS manipulation techniques and derive the following positive and negative results: (Closure) A CDS for $f$ can be turned into a CDS for its complement $\bar{f}$ with only a minor blow-up in complexity. More generally, for a (possibly nonmonotone) predicate $h$, we obtain a CDS for $h(f_1,\ldots,f_m)$ whose cost is essentially linear in the formula size of $h$ and polynomial in the CDS complexity of $f_i$. (Amplification) It is possible to reduce the privacy and correctness error of a CDS from constant to $2^{-k}$ with a multiplicative overhead of $O(k)$. Moreover, this overhead can be amortized over $k$-bit secrets. (Amortization) Every predicate $f$ over $n$-bit inputs admits a CDS for multibit secrets whose amortized communication complexity per secret bit grows linearly with the input length $n$ for sufficiently long secrets. In contrast, the best known upper-bound for single-bit secrets is exponential in $n$. (Lower-bounds) There exists a (nonexplicit) predicate $f$ over $n$-bit inputs for which any perfect (single-bit) CDS requires communication of at least $\Omega(n)$. This is an exponential improvement over the previously known $\Omega(\log n)$ lower-bound. (Separations) There exists an (explicit) predicate whose CDS complexity is exponentially smaller than its randomized communication complexity. This matches a lower-bound of Gay, Kerenidis, and Wee [Advances in Cryptology, Lecture Notes in Comput. Sci. 9216, Springer, New York, 2015, pp. 485--502] and, combined with another result of theirs, yields an exponential separation between the communication complexity of linear CDS and non-linear CDS. This is the first provable gap between the communication complexity of linear CDS (which captures most known protocols) and nonlinear CDS.},
  archive      = {J_SICOMP},
  author       = {Benny Applebaum and Barak Arkis and Pavel Raykov and Prashant Nalini Vasudevan},
  doi          = {10.1137/18M1217097},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {32-67},
  shortjournal = {SIAM J. Comput.},
  title        = {Conditional disclosure of secrets: Amplification, closure, amortization, lower-bounds, and separations},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A near-linear approximation scheme for multicuts of embedded
graphs with a fixed number of terminals. <em>SICOMP</em>,
<em>50</em>(1), 1–31. (<a
href="https://doi.org/10.1137/18M1183297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an undirected edge-weighted graph $G$ and a set $R$ of pairs of vertices called pairs of terminals, a multicut is a set of edges such that removing these edges from $G$ disconnects each pair in $R$. We provide an algorithm computing a $(1+\varepsilon)$-approximation of the minimum multicut of a graph $G$ in time $(g+t)^{(O(g+t)^3)}\cdot(1/\varepsilon)^{O(g+t)} \cdot n \log n$, where $g$ is the genus of $G$ and $t$ is the number of terminals. This is tight in several aspects, as the minimum multicut problem is both APX-hard and W[1]-hard (parameterized by the number of terminals), even on planar graphs (equivalently, when $g=0$). Our result, in the field of fixed-parameter approximation algorithms, mostly relies on concepts borrowed from computational topology of graphs on surfaces. In particular, we use and extend various recent techniques concerning homotopy, homology, and covering spaces. Interestingly, such topological techniques seem necessary even for the planar case. We also exploit classical ideas stemming from approximation schemes for planar graphs and low-dimensional geometric inputs. A key insight toward our result is a novel characterization of a minimum multicut as the union of some Steiner trees in the universal cover of the surface in which $G$ is embedded.},
  archive      = {J_SICOMP},
  author       = {Vincent Cohen-Addad and Éric Colin de Verdière and Arnaud de Mesmay},
  doi          = {10.1137/18M1183297},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {1-31},
  shortjournal = {SIAM J. Comput.},
  title        = {A near-linear approximation scheme for multicuts of embedded graphs with a fixed number of terminals},
  volume       = {50},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
