<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip---298">IETIP - 298</h2>
<ul>
<li><details>
<summary>
(2021). CA-PMG: Channel attention and progressive multi-granularity
training network for fine-grained visual classification. <em>IETIP</em>,
<em>15</em>(14), 3718–3727. (<a
href="https://doi.org/10.1049/ipr2.12238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification is challenging due to the inherently subtle intra-class object variations. To solve this issue, a novel framework named channel attention and progressive multi-granularity training network, is proposed. It first exploits meaningful feature maps through the channel attention module and captures multi-granularity features by the progressive multi-granularity training module. For each feature map, the channel attention module is proposed to explore channel-wise correlation. This allows the model to re-weight the channels of the feature map according to the impact of their semantic information on performance. Furthermore, the progressive multi-granularity training module is introduced to fuse features cross multi-granularity. And the fused features pay more attention to the subtle differences between images. The model can be trained efficiently in an end-to-end manner without bounding box or part annotations. Finally, comprehensive experiments are conducted to show that the method achieves state-of-the-art performances on the CUB-200-2011, Stanford Cars, and FGVC-Aircraft datasets. Ablation studies demonstrate the effectiveness of each part in our module.},
  archive      = {J_IETIP},
  author       = {Peipei Zhao and Qiguang Miao and Hang Yao and Xiangzeng Liu and Ruyi Liu and Maoguo Gong},
  doi          = {10.1049/ipr2.12238},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3718-3727},
  shortjournal = {IET Image Process.},
  title        = {CA-PMG: Channel attention and progressive multi-granularity training network for fine-grained visual classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image encryption algorithm for crowd data based on a new
hyperchaotic system and bernstein polynomial. <em>IETIP</em>,
<em>15</em>(14), 3698–3717. (<a
href="https://doi.org/10.1049/ipr2.12237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new two-dimensional chaotic system in the form of a cascade structure is designed, which is derived from the Chebyshev system and the infinite collapse system. Performance analysis including trajectory, Lyapunov exponent and approximate entropy indicate that it has a larger chaotic range, better ergodicity and more complex chaotic behaviour than those of advanced two-dimensional chaotic system recently proposed. Moreover, to protect the security of the crowd image data, the newly designed two-dimensional chaotic system is utilized to propose a visually meaningful image cryptosystem combined with singular value decomposition and Bernstein polynomial. First, the plain image is compressed by singular value decomposition, and then encrypted to the noise-like cipher image by scrambling and diffusion algorithm. Later, the steganographic image is obtained by randomly embedding the cipher image into a carrier image in spatial domain through the Bernstein polynomial-based embedding method, thereby realizing the double security of image information and image appearance. Besides, the visual quality of the steganographic image can be improved by the adjustment factor according to different carrier images during the embedding process. Ultimately, security analyses indicate that it has higher encryption efficiency (2 Mbps) and the visual quality of steganography image can reach 39 dB.},
  archive      = {J_IETIP},
  author       = {Donghua Jiang and Lidong Liu and Xingyuan Wang and Xianwei Rong},
  doi          = {10.1049/ipr2.12237},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3698-3717},
  shortjournal = {IET Image Process.},
  title        = {Image encryption algorithm for crowd data based on a new hyperchaotic system and bernstein polynomial},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive colour restoration and detail retention for image
enhancement. <em>IETIP</em>, <em>15</em>(14), 3685–3697. (<a
href="https://doi.org/10.1049/ipr2.12223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision-based crowd understanding and analysis technology has been widely used in public safety due to the rapid growth of population and the frequent occurrence of various accidents. Improving imaging quality is the key to improve the performance of crowd analysis, density estimation, target recognition, segmentation, and detection in computer vision tasks. Due to the complex imaging environment such as fog and low illumination, some images taken in outdoor environment often have the problems of colour distortion, lack of details, and the poor imaging quality, which affect the subsequent visual tasks. To improve the imaging quality and visual effect, an adaptive colour restoration and detail retention-based method is proposed for image enhancement. First, to overcome the problem of colour distortion caused by low illumination and fog, a multi-channel fusion based adaptive image colour restoration method is proposed. To make the enhancement result more consistent with human observation, the detail retention-based method is applied to enhance the details. Experimental results demonstrate that the authors&#39; results are effective and outperform the compared methods both in visual and objective evaluations.},
  archive      = {J_IETIP},
  author       = {Kangjian He and Dapeng Tao and Dan Xu},
  doi          = {10.1049/ipr2.12223},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3685-3697},
  shortjournal = {IET Image Process.},
  title        = {Adaptive colour restoration and detail retention for image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modal semantic correlation learning by bi-CNN network.
<em>IETIP</em>, <em>15</em>(14), 3674–3684. (<a
href="https://doi.org/10.1049/ipr2.12176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross modal retrieval can retrieve images through a text query and vice versa. In recent years, cross modal retrieval has attracted extensive attention. The purpose of most now available cross modal retrieval methods is to find a common subspace and maximize the different modal correlation. To generate specific representations consistent with cross modal tasks, this paper proposes a novel cross modal retrieval framework, which integrates feature learning and latent space embedding. In detail, we proposed a deep CNN and a shallow CNN to extract the feature of the samples. The deep CNN is used to extract the representation of images, and the shallow CNN uses a multi-dimensional kernel to extract multi-level semantic representation of text. Meanwhile, we enhance the semantic manifold by constructing cross modal ranking and within-modal discriminant loss to improve the division of semantic representation. Moreover, the most representative samples are selected by using online sampling strategy, so that the approach can be implemented on a large-scale data. This approach not only increases the discriminative ability among different categories, but also maximizes the relativity between different modalities. Experiments on three real word datasets show that the proposed method is superior to the popular methods.},
  archive      = {J_IETIP},
  author       = {Chaoyi Wang and Liang Li and Chenggang Yan and Zhan Wang and Yaoqi Sun and Jiyong Zhang},
  doi          = {10.1049/ipr2.12176},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3674-3684},
  shortjournal = {IET Image Process.},
  title        = {Cross-modal semantic correlation learning by bi-CNN network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple object tracking based on multi-task learning with
strip attention. <em>IETIP</em>, <em>15</em>(14), 3661–3673. (<a
href="https://doi.org/10.1049/ipr2.12327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking (MOT) framework based on bifurcate strategy was usually challenged by data association of different model path, which work for object localisation and appearance embedding independently. By incorporating the re-identification (re-ID) as appearance embedding model, more recent studies on task combination of a single network have made a great progress in tracking performance. Unfortunately, the contributive improvement from re-ID model is hard to balance the accuracy and efficiency for the whole framework. For more effective enhancement of the overall tracking performance, a real-time detection needs to be taken into consideration with other auxiliary means for MOT modelling. Therefore, in this study, a one-shot multiple object tracking is proposed based on multi-task learning to obtain satisfactory performance in both speed and robustness. With updated re-training strategy for the backbone model of detection, a D2LA network is proposed to achieve more characteristic fine-grained feature extraction in branching task of pedestrian recognition. Additionally, a strip attention module is also introduced to further strengthen the feature discriminative capability of the tracking framework in occlusion. Experiments on the 2DMOT15, MOT16, MOT17, and MOT20 benchmark data sets have shown a superior performance in comparison to other state-of-the-art tracking approaches.},
  archive      = {J_IETIP},
  author       = {Yaoye Song and Peng Zhang and Wei Huang and Yufei Zha and Tao You and Yanning Zhang},
  doi          = {10.1049/ipr2.12327},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3661-3673},
  shortjournal = {IET Image Process.},
  title        = {Multiple object tracking based on multi-task learning with strip attention},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive learning of graph encoder for accelerating
pedestrian trajectory prediction training. <em>IETIP</em>,
<em>15</em>(14), 3645–3660. (<a
href="https://doi.org/10.1049/ipr2.12185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of pedestrian trajectory prediction, the hybrid structures of temporal feature extractor or spatial feature extractor have paved the way for the precise prediction model, and they are in larger and larger scale. Learning of specific feature encoding model not only influenced by the structure of the network, but also by the learning manners such as supervised learning and unsupervised learning. Previous works concentrated on more comprehensive encoders and more delicate designs of feature extractors. However, the mutual influence factors from the neighbour pedestrians associate with the distance to the centre pedestrian seldomly noticed. Most of the existed feature extractors in prediction models trained in the way of supervised learning other than unsupervised manners caused the problem that the extracted features are always handcrafted without the natural distinction of obscure situations. The graph contrastive accelerating encoder is proposed, which accelerates the pedestrian trajectory prediction training process of the state of the art method of spatio-temporal graph transformer networks. Employing the unsupervised contrastive learning process and the graph of neighbours representing distance affection of nearest and farthest pedestrian to the centre pedestrian, the graph contrastive accelerating encoder significantly shrinked the training time. Holding the final performance on to state of the art level, the proposed method let the lowest pedestrian trajectory prediction error show up in the obviously earlier training steps.},
  archive      = {J_IETIP},
  author       = {Zonggui Yao and Jun Yu and Jiajun Ding},
  doi          = {10.1049/ipr2.12185},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3645-3660},
  shortjournal = {IET Image Process.},
  title        = {Contrastive learning of graph encoder for accelerating pedestrian trajectory prediction training},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label learning based target detecting from multi-frame
data. <em>IETIP</em>, <em>15</em>(14), 3638–3644. (<a
href="https://doi.org/10.1049/ipr2.12271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of target detecting, lots of progress have been made in recent years. Owing to the progress of multiple frames time series data, or video satellites, target detecting from space-borne satellite videos has been available. However, detecting a slightly moving target from space-borne videos is still a difficult task, because of the low resolution and illumination variation influence. This paper considers target detecting from time series data as multi-label problem as there are several different kinds of background objects and targets of interest. To some extent the background of time series data is comparative invariant, using background analysis method to extract the target from the background is promising. This paper proposes a novel target detecting algorithm based on multi-label learning and Gaussian background description model aiming at extracting slowly moving target. To further enhance performances, multi-frame fusion and post processing method was utilized to catch the slight difference due to movement. Experimental results on real world datasets indicate that the proposed method outperforms some state-of-the-art algorithms.},
  archive      = {J_IETIP},
  author       = {Mengqing Mei and Fazhi He},
  doi          = {10.1049/ipr2.12271},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3638-3644},
  shortjournal = {IET Image Process.},
  title        = {Multi-label learning based target detecting from multi-frame data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time automatic helmet detection of motorcyclists in
urban traffic using improved YOLOv5 detector. <em>IETIP</em>,
<em>15</em>(14), 3623–3637. (<a
href="https://doi.org/10.1049/ipr2.12295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traffic accidents, motorcycle accidents are the main cause of casualties, especially in developing countries. The main cause of fatal injuries in motorcycle accidents is that motorcycle riders or passengers do not wear helmets. In this paper, an automatic helmet detection of motorcyclists method based on deep learning is presented. The method consists of two steps. The first step uses the improved YOLOv5 detector to detect motorcycles (including motorcyclists) from video surveillance. The second step takes the motorcycles detected in the previous step as input and continues to use the improved YOLOv5 detector to detect whether the motorcyclists wear helmets. The improvement of the YOLOv5 detector includes the fusion of triplet attention and the use of soft-NMS instead of NMS. A new motorcycle helmet dataset (HFUT-MH) is being proposed, which is larger and more comprehensive than the existing dataset derived from multiple traffic monitoring in Chinese cities. Finally, the proposed method is verified by experiments and compared with other state-of-the-art methods. Our method achieves mAP of 97.7%, F1-score of 92.7% and frames per second (FPS) of 63, which outperforms other state-of-the-art detection methods.},
  archive      = {J_IETIP},
  author       = {Wei Jia and Shiquan Xu and Zhen Liang and Yang Zhao and Hai Min and Shujie Li and Ye Yu},
  doi          = {10.1049/ipr2.12295},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3623-3637},
  shortjournal = {IET Image Process.},
  title        = {Real-time automatic helmet detection of motorcyclists in urban traffic using improved YOLOv5 detector},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MFNet-LE: Multilevel fusion network with laplacian embedding
for face presentation attacks detection. <em>IETIP</em>,
<em>15</em>(14), 3608–3622. (<a
href="https://doi.org/10.1049/ipr2.12308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face detection is playing a pivotal role for crowd counting and abnormal events detection. However, it is vulnerable to face presentation attacks by printed photos, videos, and 3D masks of real human faces. Although numerous detection techniques based on deep learning have been employed to address the problem of face presentation attacks, there are still several weaknesses in these approaches, such as high algorithm complexity and a lack of detection ability. To overcome these weaknesses, a method based on a multilevel fusion network with Laplacian embedding (MFNet-LE) for the detection of face presentation attacks is proposed. First, a shallow network that contains just three layers was developed, which makes the model faster. Then, an optimised multilevel fusion strategy was developed to combine the input with the output of all previous layers to improve the detection ability of the method. Finally, a Laplacian embedding algorithm is introduced to maintain the inter-class discrimination and penalise the intra-class distance. Under the joint supervision of Laplacian loss and softmax loss, the proposed approach can obtain more discriminative features, which enhance the accuracy of attack detection. Experiments were conducted with three public databases for face presentation attacks: CASIA FASD, Idiap Replay Attack database and MSU USSA. The results demonstrate that the MFNet-LE model can outperform the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Sijie Niu and Xiaofeng Qu and Junting Chen and Xizhan Gao and Tingwei Wang and Jiwen Dong},
  doi          = {10.1049/ipr2.12308},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3608-3622},
  shortjournal = {IET Image Process.},
  title        = {MFNet-LE: Multilevel fusion network with laplacian embedding for face presentation attacks detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Part-level attention networks for cross-domain person
re-identification. <em>IETIP</em>, <em>15</em>(14), 3599–3607. (<a
href="https://doi.org/10.1049/ipr2.12292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) is in significant demand for intelligent security and single or multiple-target tracking. However, there are issues in the person Re-ID tasks, such as sharp decline in cross-data sets detection accuracy, poor generalization and cross-domain ability of the model. This work mainly studies the generalization and adaptation of cross-domain person Re-ID models. Different from most existing methods for cross-domain Re-ID tasks, the authors use diversified spatial semantic feature in pixel-level learning in the target domain to improve the generality and adaptability of the model. In the case that no information of the target domain is used during the model training, the trained model is directly tested on the data set of the target domain. It has proven effective to add the attention cascade module into the backbone network combining with the part-level branch. The authors conducted extensive experiments based on the three data sets of Market-1501, DukeMTMC-ReID and MSMT17, resulting in both single-domain and cross-domain tests with an average improvement of Rank1 and mAP values of about 10% compared with Baseline through the authors&#39; proposed method named Part-Level Attention Network.},
  archive      = {J_IETIP},
  author       = {Qun Zhao and Nisuo Du and Zhi Ouyang and Ning Kang and Ziyan Liu and Xu Wang and Qing He and Yiling Xu and Shichun Ge and Jingkuan Song},
  doi          = {10.1049/ipr2.12292},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3599-3607},
  shortjournal = {IET Image Process.},
  title        = {Part-level attention networks for cross-domain person re-identification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-dimensional weighted cross-attention network in
crowded scenes. <em>IETIP</em>, <em>15</em>(14), 3585–3598. (<a
href="https://doi.org/10.1049/ipr2.12298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human detection in crowded scenes is one of the research components of crowd safety problem analysis, such as emergency warning and security monitoring platforms. Although the existing anchor-free methods have fast inference speed, they are not suitable for object detection in crowded scenes due to the model&#39;s inability to predict the well-fined object detection bounding boxes. This work proposes an end-to-end anchor-free network, Multi-dimensional Weighted Cross-Attention Network (MANet), which can perform real-time human detection in crowded scenes. Specifically, the Double-flow Weighted Feature Cascade Module (DW-FCM) is used in the extractor to highlight the contribution of features at different levels. The Triplet Cross Attention Module (TCAM) is used in the detector head to enhance the association dependence of multi-dimension features, further strengthening human boundary features&#39; discrimination ability at a fine-grained level. Moreover, the strategy of Adaptively Opposite Thrust Mapping (AOTM) ground-truth annotation is proposed to achieve bias correction of erroneous mappings and reduce the iterations of useless learning of the network. These strategies effectively alleviate the defect that the existing anchor-free network cannot correctly distinguish and locate the individual human in crowded scenes. Compared with the anchor-based detection method, there is no need to set anchor parameters manually, and the detection speed can satisfy the real-time application. Finally, through extensive comparative experiments on CrowdHuman and WIDER FACE datasets, the results demonstrate that the improved strategy achieves the state-of-the-art result in the anchor-free methods.},
  archive      = {J_IETIP},
  author       = {Yefan Xie and Jiangbin Zheng and Xuan Hou and Irfan Raza Naqvi and Yue Xi and Nailiang Kuang},
  doi          = {10.1049/ipr2.12298},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3585-3598},
  shortjournal = {IET Image Process.},
  title        = {Multi-dimensional weighted cross-attention network in crowded scenes},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view intrinsic low-rank representation for robust face
recognition and clustering. <em>IETIP</em>, <em>15</em>(14), 3573–3584.
(<a href="https://doi.org/10.1049/ipr2.12232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, subspace-based multi-view face recognition has attracted increasing attention and many related methods have been proposed. However, the most existing methods ignore the specific local structure of different views. This drawback can cause these methods&#39; discriminating ability to degrade when many noisy samples exist in data. To tackle this problem, a multi-view low-rank representation method is proposed, which exploits both intrinsic relationships and specific local structures of different views simultaneously. It is achieved by hierarchical Bayesian methods that constrain the low-rank representation of each view so that it matches a linear combination of an intrinsic representation matrix and a specific representation matrix to obtain common and specific characteristics of different views. The intrinsic representation matrix holds the consensus information between views, and the specific representation matrices indicate the diversity among views. Furthermore, the model injects a clustering structure into the low-rank representation. This approach allows for adaptive adjustment of the clustering structure while pursuing the optimization of the low-rank representation. Hence, the model can well capture both the relationship between data and the clustering structure explicitly. Extensive experiments on several datasets demonstrated the effectiveness of the proposed method compared to similar state-of-the-art methods in classification and clustering.},
  archive      = {J_IETIP},
  author       = {Zhi-yang Wang and Stanley Ebhohimhen Abhadiomhen and Zhi-feng Liu and Xiang-jun Shen and Wen-yun Gao and Shu-ying Li},
  doi          = {10.1049/ipr2.12232},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3573-3584},
  shortjournal = {IET Image Process.},
  title        = {Multi-view intrinsic low-rank representation for robust face recognition and clustering},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel face recognition method based on fusion of LBP and
HOG. <em>IETIP</em>, <em>15</em>(14), 3559–3572. (<a
href="https://doi.org/10.1049/ipr2.12192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the hot topics in the field of computer vision research, face recognition technology has received significant attention due to its potentiality for a wide range of applications in government as well as commercial purposes. In practical applications, although several existing face recognition methods have achieved good performances in specific scenes, they easily suffer from a sharp decline in recognition rate if affected by different conditions of light, expression, posture and occlusion. Among many factors, influences of complex illuminations on face recognition are particularly significant. To further improve the performance of the existing local binary pattern (LBP) operator, neighbourhood weighted average LBP (NWALBP) is first proposed for fully considering the strong correlations between pixel pairs in the neighbourhood, which extends the traditional LBP uni-layer neighbourhood template window to the bi-layer neighbourhood template window and calculates the weighted average of bi-layer neighbourhood pixels in each direction. Then, inspired by center symmetric LBP (CS-LBP), centre symmetric NWALBP (CS-NWALBP) is further proposed, which can effectively reduce computation complexity by only comparing the weighted average values of the neighbourhood pixels that are symmetric about the centre pixel. Finally, by combining the merit of histogram of oriented gradient (HOG), a feature fusion algorithm named CS-NWALBP+HOG is suggested. Several experiments have eventually demonstrated that our proposed algorithms have more robust performance under complex illumination conditions if compared with many other latest algorithms.},
  archive      = {J_IETIP},
  author       = {Ting Chen and Tao Gao and Shuying Li and Xi Zhang and Jinpei Cao and Dachun Yao and Yh Li},
  doi          = {10.1049/ipr2.12192},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3559-3572},
  shortjournal = {IET Image Process.},
  title        = {A novel face recognition method based on fusion of LBP and HOG},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd estimation using key-point matching with support
vector regression. <em>IETIP</em>, <em>15</em>(14), 3551–3558. (<a
href="https://doi.org/10.1049/ipr2.12300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crowd behaviour understanding and density estimation are some of the fast-growing fields in video surveillance. There are many techniques (detection and regression) that are used as the method of crowd analysis and estimation. In the present approach, SVR (support vector regression) is used as the basic analysis technique and the novel key-point matching with SURF (speedup robust feature) is used as the feature extractor for moving objects in the video. The traditional linear regression methods used mainly key-point as one of the statistical features instead of matching with consecutive frames, but we used the magnitude of the optical flow for foreground object extraction instead of inter-frame difference. The combination of the optical flow of foreground objects and key-point matching generates new features apart from conventional features such as areas and corners. In this new approach, key-point pairing with linear regression is tested with the PETS2009 dataset, and performance is compared with the existing approaches.},
  archive      = {J_IETIP},
  author       = {E.M.C.L Ekanayake and Yunqi Lei},
  doi          = {10.1049/ipr2.12300},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3551-3558},
  shortjournal = {IET Image Process.},
  title        = {Crowd estimation using key-point matching with support vector regression},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn from object counting: Crowd counting with
meta-learning. <em>IETIP</em>, <em>15</em>(14), 3543–3550. (<a
href="https://doi.org/10.1049/ipr2.12241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of crowd counting is to learn a counter that can estimate the number of people in a single image. So far, most of the proposed work evaluates the crowd density by fitting the constructed density map corresponding to the sample. The performance of those algorithms depends on a large amount of carefully prepared data. However, a significant problem with crowd data sets is the difficulty of labeling. To address such a situation, utilizing object counting data in few-shot scenes is considered and an efficient algorithm to extract the meta-information is proposed, thus improving the accuracy and convergence rate of the crowd counting tasks. Specifically, the counting network is trained with only object counting tasks constructed on different domains during the meta-training phase. Then, the meta-counter is testing on crowd counting tasks in the meta-testing stage. Experimentally, it is demonstrated that the above way improves the converge rate and accuracy of crowd counting tasks on three crowd counting datasets when meta-training on ten-type object counting tasks.},
  archive      = {J_IETIP},
  author       = {Changtong Zan and Baodi Liu and Weili Guan and Kai Zhang and Weifeng Liu},
  doi          = {10.1049/ipr2.12241},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3543-3550},
  shortjournal = {IET Image Process.},
  title        = {Learn from object counting: Crowd counting with meta-learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level features extraction network with gating
mechanism for crowd counting. <em>IETIP</em>, <em>15</em>(14),
3534–3542. (<a href="https://doi.org/10.1049/ipr2.12304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is still a practical and challenging problem owing to scale variations and information loss. Most existing methods based on the straightforward fusion of different features from a deep neural network seem to eliminate this limitation. However, these features are difficult to be fused since they often differ significantly in modality and dimensionality. Unlike previous works, a multi-level features extraction network with gating mechanism for crowd counting is proposed. Specifically, a multi-channel gated unit to adaptively extract features in different levels of the network is proposed, which can avoid interference from confusing information. To fully aggregate features via multi-level fusion, multi-level features extraction scheme is presented. The multi-level features extraction network learns to fuse features from multiple levels and reduce false predictions. Extensive experiments and evaluations clearly illustrate that the proposed approach achieves state-of-the-art counting performance against other methods on four mainstream crowd counting benchmarks.},
  archive      = {J_IETIP},
  author       = {Xin Zeng and Qiang Guo and Haoran Duan and Yunpeng Wu},
  doi          = {10.1049/ipr2.12304},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3534-3542},
  shortjournal = {IET Image Process.},
  title        = {Multi-level features extraction network with gating mechanism for crowd counting},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MFP-net: Multi-scale feature pyramid network for crowd
counting. <em>IETIP</em>, <em>15</em>(14), 3522–3533. (<a
href="https://doi.org/10.1049/ipr2.12230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has been widely used for dense crowd counting, it still faces two challenges. Firstly, the popular network models are sensitive to scale variance of human head, human occlusions, and complex background due to repeated utilization of vanilla convolution kernels. Secondly, the vanilla feature fusion often depends on summation or concatenation, which ignores the correlation of different features leading to information redundancy and low robustness to background noise. To address these issues, a multi-scale feature pyramid network (MFP-Net) for dense crowd counting is proposed in this paper. The proposed MFP-Net makes two contributions. Firstly, the feature pyramid fusion module is designed that adopts rich convolutions with different depths and scales, not only to expand the receptive field, but also to improve the inference speed of models by using parallel group convolution. Secondly, a feature attention-aware module is added in the feature fusion stage. The module can achieve local and global information fusion by capturing the importance of the spatial and channel domains to improve model robustness. The proposed MFP-Net is evaluated on five publicly available datasets, and experiments show that the MFP-Net not only provides better crowd counting results than comparative models, but also requires fewer parameters.},
  archive      = {J_IETIP},
  author       = {Tao Lei and Dong Zhang and Risheng Wang and Shuying Li and Weijiang Zhang and Asoke K. Nandi},
  doi          = {10.1049/ipr2.12230},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3522-3533},
  shortjournal = {IET Image Process.},
  title        = {MFP-net: Multi-scale feature pyramid network for crowd counting},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MSR-FAN: Multi-scale residual feature-aware network for
crowd counting. <em>IETIP</em>, <em>15</em>(14), 3512–3521. (<a
href="https://doi.org/10.1049/ipr2.12175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting aims to count the number of people in crowded scenes, which is important to the security systems, traffic control and so on. The existing methods typically using local features cannot properly handle the perspective distortion and the varying scales in congested scene images, and henceforth perform wrong people counting. To alleviate this issue, this study proposes a multi-scale residual feature-aware network (MSR-FAN) that combines multi-scale features using multiple receptive field sizes and learns the feature-aware information on each image. The MSR-FAN is trained end-to-end to generate high-quality density map and evaluate the crowd number. The method consists of three parts. To handle the perspective changes problem, the first part, the direction-based feature-enhanced network, is designed to encode the perspective information in four directions based on the initial image feature. The second part, the proposed multi-scale residual block module, gets the global information to handle the represent the regional feature better. This module explores features of different scales as well as reinforce the global feature. The third part, the feature-aware block, is designed to extract the feature hidden in the different channels. Experiment results based on benchmark datasets show that the proposed approach outperforms the existing state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Haoyu Zhao and Weidong Min and Xin Wei and Qi Wang and Qiyan Fu and Zitai Wei},
  doi          = {10.1049/ipr2.12175},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3512-3521},
  shortjournal = {IET Image Process.},
  title        = {MSR-FAN: Multi-scale residual feature-aware network for crowd counting},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning method for video-based action recognition.
<em>IETIP</em>, <em>15</em>(14), 3498–3511. (<a
href="https://doi.org/10.1049/ipr2.12303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a deep learning method for video-based action recognition is proposed. On the one hand, boundary compensation on the basis of a deep neural network is performed to achieve action proposal. Boundary compensation considering non-maximum suppression according to sliding window priority is applied to remove redundant windows. To accurately detect boundaries, a boundary compensation network is established with multiple networks to process different numbers of segments. On the other hand, action recognition based on the resultant action proposals is performed. To further utilise boundary compensation, three methods are introduced for key frame selection. Optical flow and RGB features are combined via a channel fusion to realise feature representation. A two-stream network with a spatiotemporal structure is adopted for action recognition. The proposed method is evaluated on three public datasets. The experimental results demonstrate that the proposed method achieves a superior performance to that of state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Guanwen Zhang and Yukun Rao and Changhao Wang and Wei Zhou and Xiangyang Ji},
  doi          = {10.1049/ipr2.12303},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3498-3511},
  shortjournal = {IET Image Process.},
  title        = {A deep learning method for video-based action recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent label mining for group activity recognition in
basketball videos. <em>IETIP</em>, <em>15</em>(14), 3487–3497. (<a
href="https://doi.org/10.1049/ipr2.12265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion information has been widely exploited for group activity recognition in sports video. However, in order to model and extract the various motion information between the adjacent frames, existing algorithms only use the coarse video-level labels as supervision cues. This may lead to the ambiguity of extracted features and the omission of changing rules of motion patterns that are also important sports video recognition. In this paper, a latent label mining strategy for group activity recognition in basketball videos is proposed. The authors&#39; novel strategy allows them to obtain the latent labels set for marking different frames in an unsupervised way, and build the frame-level and video-level representations with two separate levels of supervision signal. Firstly, the latent labels of motion patterns are digged using the unsupervised hierarchical clustering technique. The generated latent labels are then taken as the frame-level supervision signal to train a deep CNN for the frame-level features extraction. Lastly, the frame-level features are fed into an LSTM network to build the spatio-temporal representation for group activity recognition. Experimental results on the public NCAA dataset demonstrate that the proposed algorithm achieves state-of-the-art performance.},
  archive      = {J_IETIP},
  author       = {Lifang Wu and Zeyu Li and Ye Xiang and Meng Jian and Jialie Shen},
  doi          = {10.1049/ipr2.12265},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3487-3497},
  shortjournal = {IET Image Process.},
  title        = {Latent label mining for group activity recognition in basketball videos},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd activity recognition in live video streaming via
3D-ResNet and region graph convolution network. <em>IETIP</em>,
<em>15</em>(14), 3476–3486. (<a
href="https://doi.org/10.1049/ipr2.12239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the era of we-media, live video industry has shown an explosive growth trend. For large-scale live video streaming, especially those containing crowd events that may cause great social impact, how to identify and supervise the crowd activity in live video streaming effectively is of great value to push the healthy development of live video industry. The existing crowd activity recognition mainly uses visual information, rarely fully exploiting and utilizing the correlation or external knowledge between crowd content. Therefore, a crowd activity recognition method in live video streaming is proposed by 3D-ResNet and regional graph convolution network (ReGCN). (1) After extracting deep spatiotemporal features from live video streaming with 3D-ResNet, the region proposals are generated by region proposal network. (2) A weakly supervised ReGCN is constructed by making region proposals as graph nodes and their correlations as edges. (3) Crowd activity in live video streaming is recognised by combining the output of ReGCN, the deep spatiotemporal features and the crowd motion intensity as external knowledge. Four experiments are conducted on the public collective activity extended dataset and a real-world dataset BJUT-CAD. The competitive results demonstrate that our method can effectively recognise crowd activity in live video streaming.},
  archive      = {J_IETIP},
  author       = {Junpeng Kang and Jing Zhang and Wensheng Li and Li Zhuo},
  doi          = {10.1049/ipr2.12239},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3476-3486},
  shortjournal = {IET Image Process.},
  title        = {Crowd activity recognition in live video streaming via 3D-ResNet and region graph convolution network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Behaviour detection in crowded classroom scenes via
enhancing features robust to scale and perspective variations.
<em>IETIP</em>, <em>15</em>(14), 3466–3475. (<a
href="https://doi.org/10.1049/ipr2.12318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting human behaviours in images of crowded classroom scenes is a challenging task, due to the large variations of humans in scale and pose perspective. In this paper, two modules are proposed to tackle these two variations. First, an attention-based RoI (region-of-interest) extractor is designed to handle scale variation. Feature fusion and attention mechanism are used to improve the RoI feature with more local and global information. Second, a transformation-based detection head is introduced to handle perspective variation. The spatial transformation is adopted to extract consistent representation under various perspectives. Moreover, since there is a lack of proper datasets for human behaviour detection in classroom scenes, a new dataset is created, namely CLBD. The experiments on the proposed dataset demonstrate that the modules obtain significant improvements of performance over the state-of-the-art detectors.},
  archive      = {J_IETIP},
  author       = {Mingyu Liu and Fanman Meng and Qingbo Wu and Linfeng Xu and Qianghua Liao},
  doi          = {10.1049/ipr2.12318},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3466-3475},
  shortjournal = {IET Image Process.},
  title        = {Behaviour detection in crowded classroom scenes via enhancing features robust to scale and perspective variations},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anomaly detection in video sequences: A benchmark and
computational model. <em>IETIP</em>, <em>15</em>(14), 3454–3465. (<a
href="https://doi.org/10.1049/ipr2.12258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection has attracted considerable search attention. However, existing anomaly detection databases encounter two major problems. Firstly, they are limited in scale. Secondly, training sets contain only video-level labels indicating the existence of an abnormal event during the full video while lacking annotations of precise time durations. To tackle these problems, we contribute a new L arge-scale A nomaly D etection ( LAD ) database as the benchmark for anomaly detection in video sequences, which is featured in two aspects. 1) It contains 2000 video sequences including normal and abnormal video clips with 14 anomaly categories including crash, fire, violence etc . with large scene varieties, making it the largest anomaly analysis database to date. 2) It provides the annotation data, including video-level labels (abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal video frame) to facilitate anomaly detection. Leveraging the above benefits from the LAD database, we further formulate anomaly detection as a fully supervised learning problem and propose a multi-task deep neural network to solve it. We firstly obtain the local spatiotemporal contextual feature by using an Inflated 3D convolutional (I3D) network. Then we construct a recurrent convolutional neural network fed the local spatiotemporal contextual feature to extract the spatiotemporal contextual feature. With the global spatiotemporal contextual feature, the anomaly type and score can be computed simultaneously by a multi-task neural network. Experimental results show that the proposed method outperforms the state-of-the-art anomaly detection methods on our database and other public databases of anomaly detection. Supplementary materials are available at http://sim.jxufe.cn/JDMKL/ymfang/anomaly-detection.html .},
  archive      = {J_IETIP},
  author       = {Boyang Wan and Wenhui Jiang and Yuming Fang and Zhiyuan Luo and Guanqun Ding},
  doi          = {10.1049/ipr2.12258},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3454-3465},
  shortjournal = {IET Image Process.},
  title        = {Anomaly detection in video sequences: A benchmark and computational model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep social force network for anomaly event detection.
<em>IETIP</em>, <em>15</em>(14), 3441–3453. (<a
href="https://doi.org/10.1049/ipr2.12299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly event detection is vital in surveillance video analysis. However, how to learn the discriminative motion in the crowd scene is still not tackled. Here, a deep social force network by exploiting both social force extracting and deep motion coding is proposed. Given a grid of particles with velocity provided by the optical flow, the interaction force in the crowd scene is investigated and a social force module is embedded in a deep network. A deep motion convolution was further designed with a 3D (DMC-3D) module. The DMC-3D not only eliminates the noise motion in the crowd scene with a spatial encoder–decoder but also learns the 3D feature with a spatio-temporal encoder. The deep social force coding is modelled with multiple features, in which each feature can describe specific anomaly motion. The experiments on UCF-Crime and ShanghaiTech datasets demonstrate that our method can predict the temporal localization of anomaly events and outperform the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Xingming Yang and Zhiming Wang and Kewei Wu and Zhao Xie and Jinkui Hou},
  doi          = {10.1049/ipr2.12299},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3441-3453},
  shortjournal = {IET Image Process.},
  title        = {Deep social force network for anomaly event detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-view 3D human pose estimation without camera parameters
for action recognition. <em>IETIP</em>, <em>15</em>(14), 3433–3440. (<a
href="https://doi.org/10.1049/ipr2.12277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of 3D human pose estimation is to estimate the 3D coordinates of key points of the human body directly from images. Although multi-view based methods have better performance and higher precision of coordinate estimation than a single-view based, they need to know the camera parameters. In order to effectively avoid the restriction of this constraint and improve the generalizability of the model, a dual-view single-person 3D pose estimation method without camera parameters is proposed. This method first uses the 2D pose estimation network HR-net to estimate the 2D joint point coordinates from two images with different views, and then inputs them into the 3D regression network to generate the final 3D joint point coordinates. In order to make the 3D regression network fully learn the spatial structure relationship of the human body and the transformation projection relationship between different views, a self-supervised training method is designed based on a 3D human pose orthogonal projection model to generate the virtual views. In the pose estimation experiments on the Human3.6 dataset, this method achieves a significantly improved estimation error of 34.5 mm. Furthermore, an action recognition based on the human poses extracted by the proposed method is conducted, and an accuracy of 83.19% is obtained.},
  archive      = {J_IETIP},
  author       = {Long Liu and Le Yang and Wanjun Chen and Xin Gao},
  doi          = {10.1049/ipr2.12277},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3433-3440},
  shortjournal = {IET Image Process.},
  title        = {Dual-view 3D human pose estimation without camera parameters for action recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class structure-aware adversarial loss for cross-domain
human action recognition. <em>IETIP</em>, <em>15</em>(14), 3425–3432.
(<a href="https://doi.org/10.1049/ipr2.12309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain action recognition is a challenging vision task due to the domain shift and the absence of labeled data in the target domain. With only labelled source domain and unlabelled target domain data during training, some existing methods rely on an adversarial framework to align the features from different domains to a common latent space. However, the existing adversarial-based approaches have a major limitation of only attempting to perform the alignment from a holistic view, ignoring the underlying coherence of class structure across domains. A class structure-aware adversarial loss (CSCAL) is presented to address this issue. The CSCAL incorporates the category information into the adversarial learning branch to capture the fine-grained alignment of each class, effectively avoiding the false mixup of samples from different categories in the embedding space. Experiments on HMDB51, UCF101 and Olympic Sports datasets show significant improvement compared to the baseline. Code and trained model can be found at https://github.com/bregmangh/CSCAL .},
  archive      = {J_IETIP},
  author       = {Wanjun Chen and Long Liu and Guangfeng Lin and Yajun Chen and Jing Wang},
  doi          = {10.1049/ipr2.12309},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3425-3432},
  shortjournal = {IET Image Process.},
  title        = {Class structure-aware adversarial loss for cross-domain human action recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human behaviour recognition with mid-level representations
for crowd understanding and analysis. <em>IETIP</em>, <em>15</em>(14),
3414–3424. (<a href="https://doi.org/10.1049/ipr2.12147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd understanding and analysis have received increasing attention for couples of decades, and development of human behaviour recognition strongly supports the application of crowd understanding and analysis. Human behaviour recognition usually seeks to automatically analyse ongoing movements and actions in different camera views by using various machine learning methodologies in unknown video clips or image sequences. Compared to other data modalities such as documents and images, processing video data demands much higher computational and storage resources. The idea of using middle level semantic concepts to represent human actions from videos is explored and it is argued that these semantic attributes enable the construction of more descriptive methods for human action recognition. The mid-level attributes, initialized by a cluster processing, are built upon low level features and fully utilize the discrepancies in different action classes, which can capture the importance of each attribute for each action class. In this way, the representation is constructed to be semantically rich and capable of highly discriminative performance even paired with simple linear classifiers. The method is verified on three challenging datasets (KTH, UCF50 and HMDB51), and the experimental results demonstrate that our method achieves better results than the baseline methods on human action recognition.},
  archive      = {J_IETIP},
  author       = {Bangyong Sun and Nianzeng Yuan and Shuying Li and Siyuan Wu and Nan Wang},
  doi          = {10.1049/ipr2.12147},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3414-3424},
  shortjournal = {IET Image Process.},
  title        = {Human behaviour recognition with mid-level representations for crowd understanding and analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd understanding and analysis. <em>IETIP</em>,
<em>15</em>(14), 3411–3413. (<a
href="https://doi.org/10.1049/ipr2.12379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  author       = {Qi Wang and Bo Liu and Jianzhe Lin},
  doi          = {10.1049/ipr2.12379},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3411-3413},
  shortjournal = {IET Image Process.},
  title        = {Crowd understanding and analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Erratum. <em>IETIP</em>, <em>15</em>(13), 3410. (<a
href="https://doi.org/10.1049/ipr2.12312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/ipr2.12312},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3410},
  shortjournal = {IET Image Process.},
  title        = {Erratum},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of material on a tray in automatic assembly line
based on convolutional neural network. <em>IETIP</em>, <em>15</em>(13),
3400–3409. (<a href="https://doi.org/10.1049/ipr2.12302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of detecting materials inside a tray in an automated production line, it is necessary to detect not only the known materials and blank space in the designated area, but also the unknown materials misplaced inside the tray. However, the supervised detection algorithm based on deep learning can only detect the known and blank material areas. Therefore, this paper proposed a phased material detection. The first stage is to detect the tray and then identify the material area in the second stage. In order to improve the tray detection accuracy during the first stage under the condition of a high intersection ratio, an improved YOLOv5s tray detection method is proposed. The structure of YOLOv5s is improved using the SENet. This paper proposes to use the rich geometric information of the shallow network and the high-level semantic information to integrate the bypass features. MAP@0.5:0.95 of the improved model increased from 95.7% to 96.6% and MAP@0.95 from 78.5% to 90.8% . The challenge of detecting unknown wrong materials on a tray can be resolved through the recognition of material area segmentation images processed by using the improved pre-detection algorithm, together with the relative position reference between the material area and the tray. The experimental results showed that the improved method proposed meets the industrial detection requirements with an overall recognition accuracy of 91% within a 250 ms detection interval.},
  archive      = {J_IETIP},
  author       = {Dunli Hu and Yuting Zhang and Li Xufeng and Xiaoping Zhang},
  doi          = {10.1049/ipr2.12302},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3400-3409},
  shortjournal = {IET Image Process.},
  title        = {Detection of material on a tray in automatic assembly line based on convolutional neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic optimization of hessian determinant image pyramid
for memory-efficient and high performance keypoint detection in SURF.
<em>IETIP</em>, <em>15</em>(13), 3392–3399. (<a
href="https://doi.org/10.1049/ipr2.12336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing demand for scale-invariant and fast object recognition, speeded up robust features (SURF) have emerged and become a widely used feature extraction algorithm in computer vision. Nevertheless, SURF still requires high memory usage and heavy computations caused by the keypoint detection procedure that produces huge image pyramid composed of many hessian determinant (HD) data for supporting the scale-invariance. Therefore, in this paper, dynamic optimization schemes of the HD image pyramid are proposed for completely removing the redundancies of the keypoint detection with keeping the original functionality of SURF without any loss. The proposed approach has shown to reduce memory usage by 45–51%, the execution time by 37–50% and the power consumption by 18–42% when compared with the keypoint detection in the original SURF algorithm.},
  archive      = {J_IETIP},
  author       = {Eunhee Cho and Yoonjin Kim},
  doi          = {10.1049/ipr2.12336},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3392-3399},
  shortjournal = {IET Image Process.},
  title        = {Dynamic optimization of hessian determinant image pyramid for memory-efficient and high performance keypoint detection in SURF},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Change detection with cross enhancement of high- and
low-level change-related features. <em>IETIP</em>, <em>15</em>(13),
3380–3391. (<a href="https://doi.org/10.1049/ipr2.12334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) is a fundamental yet challenging problem, which aims at detecting changed object in two observations. Recent CD methods are designed based on the off-the-shelf semantic segmentation network architectures, which is not optimal for extracting and using change-related features. In this paper, a novel CD network architecture is proposed, including change-related feature extraction, cross feature enhancement, and multi-level supervision. Absolute difference of the features of different convolutional layers is first computed from a Unet-like network for two observations. The features are partitioned into high- and low-level features according to their functionalities. Then the high- and low-level features are recurrently refined by cross feature enhancement to increase the representational ability of the features. The network learns change-related features with multi-level supervisions. The final CD result can be obtained by fusing multiple predictions. Experimental results on three CD benchmark datasets indicate the superiority of the authors&#39; method when compared with six state-of-the-art deep learning-based CD methods.},
  archive      = {J_IETIP},
  author       = {Rui Huang and Yan Xing and Mo Zhou and Ruofei Wang},
  doi          = {10.1049/ipr2.12334},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3380-3391},
  shortjournal = {IET Image Process.},
  title        = {Change detection with cross enhancement of high- and low-level change-related features},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prior-guided multiscale network for single-image dehazing.
<em>IETIP</em>, <em>15</em>(13), 3368–3379. (<a
href="https://doi.org/10.1049/ipr2.12333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image dehazing is an important problem because it is a key prerequisite for most high-level computer vision tasks. Traditional prior-based methods adopt priors generated from clear images to restrain the atmospheric scattering model and then recover haze-free images. However, these prior-based methods always encounter over-enhancement, such as halos and colour distortion. To solve this problem, many works use a convolutional neural network to retrieve original images. However, without priors as guidance, these learning-based methods dehaze effectively in synthetic datasets but perform poorly in real scenes. Hence, in this paper, we propose a prior-guided multiscale network for single-image dehazing named PGMNet. Specifically, prior-based methods are adopted to acquire dehazed images of the training dataset in advance and then send these dehazed images to a parameter-shared encoder to form multiscale features. During the decoding process, these multiscale features are adopted to guide the prior-guided multiscale network to recover more image details. Moreover, considering that these prior-based dehazed images usually contain some over-enhanced regions, a spatial attention guided feature aggregation module and squeeze-and-excitation module are adopted to alleviate colour distortion. The proposed PGMNet takes the advantage of prior-based methods in real haze removal and provides superior performance compared with the state-of-the-art methods on both synthetic and real-world datasets.},
  archive      = {J_IETIP},
  author       = {Nian Wang and Zhigao Cui and Yanzhao Su and Chuan He and Yunwei Lan and Aihua Li},
  doi          = {10.1049/ipr2.12333},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3368-3379},
  shortjournal = {IET Image Process.},
  title        = {Prior-guided multiscale network for single-image dehazing},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weibull-distribution-based hybrid total variation method
for speckle reduction in ultrasound images. <em>IETIP</em>,
<em>15</em>(13), 3347–3367. (<a
href="https://doi.org/10.1049/ipr2.12332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speckle reduction is still an intractable task in ultrasound imaging field. Ultrasound speckle is usually described as multiplicative noise with its statistics following a Rayleigh or Gaussian distribution. To employ these two distributions effectively, the authors attempt to describe ultrasound speckle using a Weibull distribution, because it can include the Rayleigh distribution as a special case and also approximate a Gaussian distribution by varying its shape and scale parameters. The authors’ contribution in this paper is to propose a Weibull-distribution-based hybrid total variation (WHTV) method to reduce ultrasound speckle. The WHTV energy functional is convex and consists of a new data fidelity term and a new regularization term. The former is derived from the multiplicative Weibull model of ultrasound speckle based on the maximum likelihood criterion. The latter is a new edge-weighted combination of the first- and second-order total variation, with the advantage of preserving edges while alleviating the staircase effects. The minimization of the WHTV energy functional is implemented by the split Bregman algorithm. Experimental results on synthetic and real ultrasound images have demonstrated not only that the Weibull distribution is a better fitting model for the statistics of ultrasound speckle than other distributions such as Rayleigh, Gaussian, Gamma, and Nakagami, but also that the proposed WHTV method can achieve better despeckling performance than several state-of-the-art variational methods.},
  archive      = {J_IETIP},
  author       = {Wenchao Cui and Liangzhi Shao and Guoqiang Gong and Ke Lu and Shuifa Sun and Yirong Wu and Yiyuan Zhou},
  doi          = {10.1049/ipr2.12332},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3347-3367},
  shortjournal = {IET Image Process.},
  title        = {A weibull-distribution-based hybrid total variation method for speckle reduction in ultrasound images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HCISNet: Higher-capacity invisible image steganographic
network. <em>IETIP</em>, <em>15</em>(13), 3332–3346. (<a
href="https://doi.org/10.1049/ipr2.12329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information hiding as a crucial method for multimedia security and privacy protection, has received a substantial amount of attention. However, the most existing methods focus on the resistance of steganalysis and the robustness of information, resulting in low capacity. In this paper, to further increase the capacity of information hiding, a high-capacity adversarial image steganography model with end-to-end manner, termed as HCISNet is proposed. First, an enhanced Dense Atrous Spatial Pyramid Pooling module is presented to learn more semantic information in cover image, which can adaptively embed more message in redundant regions with rich textures. Then, a modified discriminator network with lightweight residual block is employed to assist the encoder network with optimal high-capacity solution. Meanwhile, the multiple objective function with the perceptual loss and several training tricks are developed, which can improve the visual and perceptual consistency of the original and the steganographic image, further enhancing the capacity. Finally, experiments and analysis are conducted on three public datasets, where this model has better imperceptibility, higher security and greater capacity. Under same experimental conditions, the cover image can embed 5.68 bits per pixel (BPP), far exceeding the previous highest value of 4.4 BPP in state-of-the-art methods in the authors&#39; knowledge.},
  archive      = {J_IETIP},
  author       = {Yafeng Li and Ju Liu and Xiaoxi Liu and Xuejing Wang and Xuesong Gao and Yuyi Zhang},
  doi          = {10.1049/ipr2.12329},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3332-3346},
  shortjournal = {IET Image Process.},
  title        = {HCISNet: Higher-capacity invisible image steganographic network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). No-reference image quality assessment based on multiscale
feature representation. <em>IETIP</em>, <em>15</em>(13), 3318–3331. (<a
href="https://doi.org/10.1049/ipr2.12328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The no-reference image quality assessment (NR-IQA) method can evaluate the distortions in an image without the reference image. However, due to the diversity of the image contents and distortion types, it is hard for the existing NR-IQA algorithms to obtain competitive performance on both synthetically and authentically distorted images. To address the problem, a multiscale feature representation-based NR-IQA method that performs well for both synthetic and authentic distortions is proposed. This model consists of two parts: The feature extraction part and the feature fusion part. First, part of the Res2Net-50 network is chosen as the feature extraction part due to its high ability in increasing the range of receptive fields. Then, the feature fusion part consisting of a novel residual block and two fully connected layers is designed to fuse the extracted features and realize the quality score mapping. After a series of stepwise optimization experiments, the most competitive network architecture consisting of the feature extraction part and the feature fusion part is obtained. Comprehensive experiments on the LIVE, TID2013, CSIQ, KADID-10k, KonIQ-10K, and LIVE challenge databases demonstrate that the proposed method can work powerfully on both the synthetic and authentic distortions and also has a strong generalization ability.},
  archive      = {J_IETIP},
  author       = {Junhui Li and Shuang Qiao and Chenyi Zhao and Tian Zhang},
  doi          = {10.1049/ipr2.12328},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3318-3331},
  shortjournal = {IET Image Process.},
  title        = {No-reference image quality assessment based on multiscale feature representation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo-siamese residual atrous pyramid network for
multi-focus image fusion. <em>IETIP</em>, <em>15</em>(13), 3304–3317.
(<a href="https://doi.org/10.1049/ipr2.12326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth of field is one of the critical reasons to limit the richness of image information. Usually, in a scene with multiple targets, when the distance between each target and the lens is different, the clear scene image can be get within a certain distance range. This situation restricts the further image processing, such as semantic segmentation, object recognition and 3D reconstruction. Multi-focus image fusion uses two or more images focused on different targets to fuse scene information, which can solve this problem to a great extent. In general, two or more multi-focus images can cover almost all near/far targets. The fusion of more than two multi-focus images can be accomplished by cascading the fusion results of the previous two images and the next image to be processed many times. Therefore, the paper focus on the fusion of two multi-focus images. Inspired by this, new Pseudo-Siamese neural network with several residual atrous convolution pyramids with multi-level perception ability to perceive the multi-level features and consistency relations of multi-focus image pairs is proposed, and multi-layer residual blocks are used to fuse the extracted features. In this process, the residual of the groundtruth and the generated image will be learned. Finally, a fully focused image without blur will be generated. After several ablation experiments and comparison experiments with other methods, the results show that the performance of the method proposed in this paper is state-of-the-art, and overall better than other methods, which are advanced.},
  archive      = {J_IETIP},
  author       = {Limai Jiang and Hui Fan and Jinjiang Li and Changhe Tu},
  doi          = {10.1049/ipr2.12326},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3304-3317},
  shortjournal = {IET Image Process.},
  title        = {Pseudo-siamese residual atrous pyramid network for multi-focus image fusion},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundary augment: A data augment method to defend poison
attack. <em>IETIP</em>, <em>15</em>(13), 3292–3303. (<a
href="https://doi.org/10.1049/ipr2.12325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Deep Neural Networks(DNNs) have been applied in many fields such as computer vision and natural language processing. Many third-party cloud training platforms have been built to facilitate many individual users or small enterprises for training their models, for example, Colab(google) or AWS cloud platform. For these cloud platforms, there exist many potentially fatal risks, including poison attacks. At the same time, as for federated learning, poison attack is also a severe threat to which. In this paper, a novel method to defend against poison attacks by estimating the distribution of poison data and retraining the backdoor model with a few training data is introduced. The estimated distribution under the manifold DeepFool algorithm fits the poison data well, which can be used to search the manifold boundary of the poisoned data and the clean. Unlike empirical defense methods, the authors&#39; approach is attack-agnostic, which means that the approach is robust for the various attack methods. Also, it is proven that the adversarial training approach is a practical approach to defend against the poison attack. The authors&#39; approach is tested on the datasets MNIST , CIFAR-10 , GTSRB and ImageNet . The accuracy of the retrained model decreases slightly, but the ASR drops drastically, which proves that our approach has a powerful generalization to defend against the most poison attacks.},
  archive      = {J_IETIP},
  author       = {Xuan Chen and YueNa Ma and ShiWei Lu and Yu Yao},
  doi          = {10.1049/ipr2.12325},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3292-3303},
  shortjournal = {IET Image Process.},
  title        = {Boundary augment: A data augment method to defend poison attack},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPI: Multi-receptive and parallel integration for salient
object detection. <em>IETIP</em>, <em>15</em>(13), 3281–3291. (<a
href="https://doi.org/10.1049/ipr2.12324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic representation of deep features is essential for image context understanding, and effective fusion of features with different semantic representations can significantly improve the model&#39;s performance on salient object detection. This paper proposes a novel method called multi-receptive and parallel integration, for salient object detection. Firstly, a multi-receptive enhancement module is designed to effectively expand the receptive fields of features from different layers and generate features with different receptive fields. Multi-receptive enhancement module can enhance the semantic representation and improve the model&#39;s perception of the image context, which enables the model to locate the salient object accurately. Secondly, in order to reduce the reuse of redundant information in the complex top-down fusion method and weaken the differences between semantic features, a relatively simple but effective parallel fusion strategy is proposed. It allows multi-scale features to better interact with each other, thus improving the overall performance of the model. Experimental results on multiple datasets demonstrate that the proposed method outperforms state-of-the-art methods under different evaluation metrics.},
  archive      = {J_IETIP},
  author       = {Han Sun and Jun Cen and Ningzhong Liu and Dong Liang and Huiyu Zhou},
  doi          = {10.1049/ipr2.12324},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3281-3291},
  shortjournal = {IET Image Process.},
  title        = {MPI: Multi-receptive and parallel integration for salient object detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subcutaneous sweat pore estimation from optical coherence
tomography. <em>IETIP</em>, <em>15</em>(13), 3267–3280. (<a
href="https://doi.org/10.1049/ipr2.12322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract Sweat pore, one of the level 3 features of fingerprint, has attracted much attention in fingerprint recognition. Traditional sweat pores on surface fingerprint are unclear or blurred when fingers are stained or damaged. Subcutaneous sweat pores, as cross section of the sweat glands, are resistant to external interferences. With 3D fingertip information measured by optical coherence tomography (OCT), the subcutaneous sweat pore estimation from OCT volume data is investigated. First, an adaptive subcutaneous pore image reconstruction method is proposed. It utilizes the skin surface and viable epidermis junction as reference and realizes depth-adaptive pore image reconstruction. Second, a dilated U-Net combining the U-Net with dilated convolution is proposed for subcutaneous sweat pore extraction, which can prevent information loss of sweat pores caused by downsampling. To the best knowledge, it is the first time that subcutaneous sweat pore extraction is investigated and proposed. Experiments on subcutaneous pore image reconstruction and sweat pore extraction are both conducted. The qualitative and quantitative results show that the proposed adaptive method performs better in subcutaneous pore image reconstruction compared with the fix-depth method, and the dilated U-Net outperforms other methods on subcutaneous sweat pore extraction.},
  archive      = {J_IETIP},
  author       = {Baojin Ding and Haixia Wang and Peng Chen and Yilong Zhang and Ronghua Liang and Yipeng Liu},
  doi          = {10.1049/ipr2.12322},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3267-3280},
  shortjournal = {IET Image Process.},
  title        = {Subcutaneous sweat pore estimation from optical coherence tomography},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical guided network for low-light image enhancement.
<em>IETIP</em>, <em>15</em>(13), 3254–3266. (<a
href="https://doi.org/10.1049/ipr2.12321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to insufficient illumination in low-light conditions, the brightness and contrast of the captured images are low, which affect the processing of other computer vision tasks. Low-light enhancement is a challenging task that requires simultaneous processing of colour, brightness, contrast, artefacts and noise. To solve this problem, the authors apply the deep residual network to the low-light enhancement task, and propose a hierarchical guided low-light enhancement network. The key of this method is recombined hierarchical guided features through the feature aggregation module to realize low-light enhancement. The network is based on the U-Net network, and then hierarchically guided with the input pyramid branch in the encoding and decoding network. The input pyramid structure realizes multi-level receptive fields and generates a hierarchical representation. The encoding and decoding structure concatenates the hierarchical features of the input pyramid and generates a set of hierarchical features. Finally, the feature aggregation module is used to fuse different features to achieve low-light enhancement tasks. The effectiveness of the components is proved through ablation experiments. In addition, the authors are also evaluating on different data sets, and the experimental results show that the method proposed is superior to other methods in subjective and objective evaluation.},
  archive      = {J_IETIP},
  author       = {Xiaomei Feng and Jinjiang Li and Hui Fan},
  doi          = {10.1049/ipr2.12321},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3254-3266},
  shortjournal = {IET Image Process.},
  title        = {Hierarchical guided network for low-light image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LR-RoadNet: A long-range context-aware neural network for
road extraction via high-resolution remote sensing images.
<em>IETIP</em>, <em>15</em>(13), 3239–3253. (<a
href="https://doi.org/10.1049/ipr2.12320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road extraction from high-resolution remote sensing images (HRSIs) has great importance in various practical applications. However, most existing road extraction methods have considerable limitation in capturing long-range shape feature of road, and thus, they are ineffective in extracting road region under complex scenes. To address this issue, a novel model called long-range context-aware road extraction neural network (LR-RoadNet) is proposed. LR-RoadNet takes advantage of strip pooling to capture long-range context from horizontal and vertical directions, aiming to improve continuity and completeness of road extraction results. Specifically, the LR-RoadNet consists of two parts: strip residual module (SRM) and strip pyramid pooling module (SPPM). The SRM is built based on residual unit, in which the strip pooling is employed to learn general and long-range road feature from input image. Then, the SPPM is used to obtain long-range feature from multiple scales by multiple parallel strip pooling operations. More importantly, a structural similarity (SS) loss function is introduced to further explore road structure for optimizing LR-RoadNet. The experimental results show that the proposed method achieves great improvement than other state-of-the-art methods on three challenging datasets, Cheng-Roads, Zimbabwe-Roads and Mass-Roads.},
  archive      = {J_IETIP},
  author       = {Panle Li and Zhihui Tian and Xiaohui He and Mengjia Qiao and Xijie Cheng and Dingjun Song and Mingyang Chen and Jiamian Li and Tao Zhou and Xiaoyu Guo and Zhiqiang Li and Daidong Li and Zihao Ding and Runchuan Li},
  doi          = {10.1049/ipr2.12320},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3239-3253},
  shortjournal = {IET Image Process.},
  title        = {LR-RoadNet: A long-range context-aware neural network for road extraction via high-resolution remote sensing images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A night low-illumination image enhancement model based on
small probability area filtering and lossless mapping enhancement.
<em>IETIP</em>, <em>15</em>(13), 3221–3238. (<a
href="https://doi.org/10.1049/ipr2.12319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel night-time image enhancement approach was proposed in this paper to address the problems of low contrast and poor details of low-illumination images captured at night. To begin with, the luminance component V was extracted that was irrelevant to the colour information of the image upon converting the image to the HSV space from the RGB space. Then, by converting the luminance component V of the image into the probability space, the image was divided into a small-probability grey-scale area and a normal area based on the theory of probability. Moreover, pixels were transferred from the small probability area to the normal area of the image according to the nearest attribution principle that was established. Lastly, the contrast enhancement of the image was realized thanks to lossless mapping functions without losing the number of grey levels of the image. As can be observed from experimental results, the proposed method is superior to the most advanced algorithm in visual quality and quantitative measurement.},
  archive      = {J_IETIP},
  author       = {Lei He and Wei Long and Shouxin Liu and Yanyan Li and Wei Ding},
  doi          = {10.1049/ipr2.12319},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3221-3238},
  shortjournal = {IET Image Process.},
  title        = {A night low-illumination image enhancement model based on small probability area filtering and lossless mapping enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-exposure image fusion based on feature evaluation with
adaptive factor. <em>IETIP</em>, <em>15</em>(13), 3211–3220. (<a
href="https://doi.org/10.1049/ipr2.12317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors present a new multi-exposure images fusion method based on feature evaluation with adaptive factor. It is noticed the existing multi-exposure fusion algorithm is not well adapted to the input images, which are overall bright or dark, the fused image quality is not pretty good, and the details are not preserved completely. So an adaptive factor to adapt the intensity of input images is presented here. First, the exposure assessment weight, texture change weight, and colour intensity weight are calculated by a sliding window. Finally, the images are fused by using a pyramid to avoid the seams. Twenty exposure input images of different scenes are selected, the subjective and objective aspects are analysed and compared with several existing multi-exposure image fusion methods. The experimental results show that the proposed method can retain more details and obtain satisfactory visual effects on static scenes.},
  archive      = {J_IETIP},
  author       = {Li Huang and Zhengping Li and Chao Xu and Bo Feng},
  doi          = {10.1049/ipr2.12317},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3211-3220},
  shortjournal = {IET Image Process.},
  title        = {Multi-exposure image fusion based on feature evaluation with adaptive factor},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient detection and robust tracking of spermatozoa in
microscopic video. <em>IETIP</em>, <em>15</em>(13), 3200–3210. (<a
href="https://doi.org/10.1049/ipr2.12316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sperm concentration and motility are generally analysed only in the discrete state in microscopic videos. As for sperm nonspecific aggregation areas, it brings difficulties to accurate sperm detection. In this paper, an algorithm for nonspecific aggregates automatic segmentation, detection and tracking of sperm is proposed. A grid model commensurate with the size of a sperm head is created to segment nonspecific aggregation areas. Multi-scale edge function and new energy functional are designed based on the level set method to realize sperm head segmentation. In the sperm tracking stage, we improve the weight condition and the standard of trust flow quantization based on graph theory method, and simplify the sperm tracking to the vertex matching between two frames to solve the matching failure problem of adjacent frames with small space distance. The proposed method achieves accurate segmentation of sperm non-specific aggregation regions, which outperforms the level set methods of LBF and SBGFR. At the same time, our method can real-time calculation of sperm concentration and motility during sperm tracking. It is compared with four state-of-the-art algorithms, and it gives lower tracking error rates, which has potential applications in male fertility field.},
  archive      = {J_IETIP},
  author       = {Ronghua Zhu and Yansong Cui and Enyu Hou and Jianming Huang},
  doi          = {10.1049/ipr2.12316},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3200-3210},
  shortjournal = {IET Image Process.},
  title        = {Efficient detection and robust tracking of spermatozoa in microscopic video},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-teaching based pseudo label refinery for cross-domain
object detection. <em>IETIP</em>, <em>15</em>(13), 3189–3199. (<a
href="https://doi.org/10.1049/ipr2.12315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is one of the main tasks in computer vision and has made great progress in recent years. However, the performance of target detectors is significantly dropped by the differences between existing datasets and application scenarios, leading to the so-called domain shift problem. To address such an issue, a novel co-teaching based pseudo label refinery framework for cross-domain object detection is developed, which cooperates with two models to select data from target domain for each other. This strategy can effectively purify the predicted pseudo labels and resist noisy labels. Specifically, the framework consists of two encoders (i.e. structure encoder and global encoder), two classifiers and one discriminator, in which structure encoder is used to extract structural features that are not disturbed by colour, and the global encoder is used to extract the complete discriminant features. The two encoders are each followed by a classifier. In training, the structure and global encoder with labelled source samples are first trained, so that it has the initial recognition ability. Then the samples assigned are used with pseudo labels by the classifier following the structure encoder to fine-tune the global encoder which pre-trained on the labelled source domain and obtain the refined labels for the target data. With the refined labels, the structure encoder is further optimised on the target domain. During this process, the proposal is to cross use the two classifiers to promote the mutual transfer of complementary capabilities of the two encoders. Moreover, a novel residual channel attention block (RCA) embedded with salient features is designed to pay more attention to the target regions. Extensive experiments demonstrate that the developed framework can generate clean labels for unlabelled target data and boost the performance of cross domain object detection. The code is available at http://www.msp-lab.cn:1436/msp/cbplr-master .},
  archive      = {J_IETIP},
  author       = {Kunpeng Wang and Jingxiang Cai and Juan Yao and Peng Liu and Zhiqin Zhu},
  doi          = {10.1049/ipr2.12315},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3189-3199},
  shortjournal = {IET Image Process.},
  title        = {Co-teaching based pseudo label refinery for cross-domain object detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AFF-UNIT: Adaptive feature fusion for unsupervised
image-to-image translation. <em>IETIP</em>, <em>15</em>(13), 3172–3188.
(<a href="https://doi.org/10.1049/ipr2.12314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image-to-image translation is to generate images closer to the target domain style while preserving the significant features of the original image. This paper contends an adaptive feature fusion method for unsupervised image translation. The proposed architecture, termed as AFF-UNIT, is based on a compact network structure to further improve the quality of generated images. First of all, a feature extraction module based on an adaptive feature fusion method is proposed, which combines low-level fine-grained information and high-level semantic information to obtain feature maps with richer information. At the same time, a feature-similarity loss is proposed to guide the feature extraction module to extract features that are more conducive to improving the translation result. In addition, AFF-UNIT reuses the feature extraction module in the generator and discriminator to simplify the framework. Extensive experiments on five popular benchmarks demonstrate the superior performance of AFF-UNIT over state-of-the-art methods in terms of FID, KID, IS, and also human preference. Comprehensive ablation studies are also carried out to isolate the validity of each proposed component.},
  archive      = {J_IETIP},
  author       = {Yuqiang Li and Haochen Meng and Hong Lin and Chun Liu},
  doi          = {10.1049/ipr2.12314},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3172-3188},
  shortjournal = {IET Image Process.},
  title        = {AFF-UNIT: Adaptive feature fusion for unsupervised image-to-image translation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting texture characteristics and spatial correlations
for robustness metric of data hiding with noisy transmission.
<em>IETIP</em>, <em>15</em>(13), 3160–3171. (<a
href="https://doi.org/10.1049/ipr2.12313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data hiding aims to embed a secret message into a digital object such as image by slightly modifying the object content without arousing noticeable artefacts. The resultant object containing hidden information will be sent to a desired receiver via some insecure channels, e.g. images transmitted through noisy channel, social networks are vulnerable to unknown pollution or compression by a third party, which may lead the transmitted objects to be attacked such that the reconstructed message has a significant error rate. It therefore requires us to use robust embedding strategies for data hiding to realise reliable message retrieval. To this end, in this paper, a metric model to estimate the robustness of data hiding for noisy transmission based on the statistical characteristics of cover and embedding operation is presented, the former is mainly reflected by spatial frequency and texture feature, and the latter embedding operation is mainly reflected by embedding modification. The goal is to ensure that both statistical characteristics and embedding operation can be used to maximise the embedding robustness. To the best knowledge, it is the first time to estimate robustness before data hiding by a special metric model. Experimental results show that, by combining the proposed metric model in three classical data hiding methods, i.e. BPS, DE and QIM, the robustness can be significantly improved, which demonstrates its superiority and applicability.},
  archive      = {J_IETIP},
  author       = {Yanli Chen and Hongxia Wang and Hanzhou Wu and Yonghui Zhou and Limengnan Zhou and Yi Chen},
  doi          = {10.1049/ipr2.12313},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3160-3171},
  shortjournal = {IET Image Process.},
  title        = {Exploiting texture characteristics and spatial correlations for robustness metric of data hiding with noisy transmission},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Moving target inverse synthetic aperture radar image
resolution enhancement based on two-dimensional block sparse signal
reconstruction. <em>IETIP</em>, <em>15</em>(13), 3153–3159. (<a
href="https://doi.org/10.1049/ipr2.12310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve the high-resolution inverse synthetic aperture radar (ISAR) imaging of moving targets, the range Doppler method and compressed sensing technique can be used. However, those methods are generally faced with the problem of migration through range cells and basis mismatch problems, and the imaging results can be improved. To solve these problems and improve the image quality, the problem of ISAR imaging is termed as a block-sparse signal recovery problem by utilizing the block-sparse structure of the ISAR images. A localized low-rank promoting (LLP) method is introduced and extended to the complex case for the recovery of range compressed block-sparse signals. The sparse recovery problem is solved by minimizing another function, which is the surrogate function that can be solved more effectively. Based on the LLP method, the coefficients of the range compressed echo signal are reconstructed and some 2 × 2 matrices can be obtained. And then the log-determinant function is introduced to find the low rankness solutions for these matrices. Then the LLP method is also used in the cross-range domain to reconstruct the ISAR image. Experimental results show that the proposed method can recover better focused and higher quality ISAR images compared with the traditional methods.},
  archive      = {J_IETIP},
  author       = {Xingyu He and Ningning Tong and Tao Liu},
  doi          = {10.1049/ipr2.12310},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3153-3159},
  shortjournal = {IET Image Process.},
  title        = {Moving target inverse synthetic aperture radar image resolution enhancement based on two-dimensional block sparse signal reconstruction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient region growing based on gaussian pyramid.
<em>IETIP</em>, <em>15</em>(13), 3142–3152. (<a
href="https://doi.org/10.1049/ipr2.12307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For region growing image segmentation, seed selection and image noise are two major concerns causing negative segmentation performance. This paper proposes a regional growth algorithm based on the Gaussian pyramid (GPRG), which automatically selects seed points and optimizes the growth path. To achieve automatic seed selection, Gaussian difference pyramid generated by the Gaussian pyramid is required. The growing seed of the salient target in an image is acquired by searching for extremums in each image of the Gaussian difference pyramid. For growing path optimization, edge changing direction is updated according to different Gaussian difference pyramid images with sizes from small to large. The edge of region growing is determined by the edge of the salient target in an image. Segmentation experiments show that this algorithm is able to automatically select seeds and accurately segment the salient target in an image with and without image noise. Performances in terms of segmentation accuracy (SA), mean intersection-over-union (mIoU) and E-measure are superior compared with other methods.},
  archive      = {J_IETIP},
  author       = {Jianjun Jiao and Xiaopeng Wang and Jungping Zhang and Qingsheng Wang},
  doi          = {10.1049/ipr2.12307},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3142-3152},
  shortjournal = {IET Image Process.},
  title        = {Salient region growing based on gaussian pyramid},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In-orbit geometric calibration approach and positioning
accuracy analysis for the gaofen-7 laser footprint camera.
<em>IETIP</em>, <em>15</em>(13), 3130–3141. (<a
href="https://doi.org/10.1049/ipr2.12306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaofen-7 (GF-7) satellite is China&#39;s first submeter high-resolution optical stereo-mapping satellite. The satellite is equipped with two area-array laser footprint cameras to capture the laser spots emitted by laser altimeters. To accurately identify and locate the geographical positions of laser spots, a stepwise geometric calibration method with ground laboratory information for high-resolution area-array camera is proposed in this paper. It includes the internal and external calibration. First, the initial values of the camera&#39;s internal parameters are determined based on the laboratory calibration results. Then, based on a high-precision reference ortho-image and a digital elevation model, the ground control points (GCPs) for multi-scene footprint images are obtained by image matching. Finally, the external and internal calibration methods for the laser footprint camera are implemented successively. The experimental results indicate that this method can restore geometric imaging relationships with high precision. After the internal and external calibration, the systematic residual error of the image point is eliminated, the remaining residual achieves 0.38 pixel, and the accuracy of direct positioning on the ground is better than 5.0 m, and the elevation accuracy is within 1.5 m.},
  archive      = {J_IETIP},
  author       = {Junfeng Xie and Chaofeng Ren and Huihui Jiao and Jianping Pan},
  doi          = {10.1049/ipr2.12306},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3130-3141},
  shortjournal = {IET Image Process.},
  title        = {In-orbit geometric calibration approach and positioning accuracy analysis for the gaofen-7 laser footprint camera},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative domain adaptation for chest x-ray image analysis.
<em>IETIP</em>, <em>15</em>(13), 3118–3129. (<a
href="https://doi.org/10.1049/ipr2.12305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray images taken under different conditions follow different distributions, preventing the models trained on a domain from generalising well on the other domain. In this paper, a generative domain adaptation (GDA) method is proposed to address this issue and facilitate the learning process for downstream analysis. GDA adapts different domains to a virtual common one where images are aligned at the appearance level. To this end, a domain shared generator is used to transform the input images and two competitive discriminators are used to adversarially supervise the transforming process. The domain discriminator drives the generator to narrow the domain gap while the fidelity discriminator forces the generator to keep the inherent information. Moreover, a specific classification or detection network is attached to the generator to supervise it in a task-oriented manner. Experiment results on a large-scale dataset containing 46k chest X-ray images demonstrate that GDA outperforms representative domain adaptation methods by a large margin for both disease classification and lesion detection as well as provides useful transformed images to assist experts for diagnosis.},
  archive      = {J_IETIP},
  author       = {Baocai Yin and Wenchao Liu and Zhonghua Fu and Jing Zhang and Cong Liu and Zengfu Wang},
  doi          = {10.1049/ipr2.12305},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3118-3129},
  shortjournal = {IET Image Process.},
  title        = {Generative domain adaptation for chest X-ray image analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised generative adversarial network for single
image deraining. <em>IETIP</em>, <em>15</em>(13), 3105–3117. (<a
href="https://doi.org/10.1049/ipr2.12301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the basis of image processing, single image deraining has always been a significant and challenging issue. Due to the lack of real rainy images and corresponding clean images, most deraining networks are trained by synthetic datasets, which makes the output images unsatisfactory in real applications. Besides, note that a heavy rainfall is typically accompanied with some fog. Although some deraining networks have been proposed to remove the rain streaks in the rainy images, the output images may still be blurred due to the accompanied fog. In this paper, these problems existing in single image deraining is comprehensively considered, and propose a Cycle-Derain network based on an unsupervised attention-guided mechanism. Specifically, the Cycle-Derain network takes advantage of generative adversarial networks with two mappings and the cycle consistency loss to train both unpaired rainy images and rain-free images. Moreover, it introduces an unsupervised attention-guided mechanism and exploits the loop-search positioning algorithm to deal with the details of rain and fog in images. Extensive experiments have been carried out, and the results show that the proposed Cycle-Derain network is preferable compared with other deraining networks, especially in term of rainy image restoration.},
  archive      = {J_IETIP},
  author       = {Zhiying Song and Yuting Guo and Zifan Ma and Ruocong Tang and Linfeng Liu},
  doi          = {10.1049/ipr2.12301},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3105-3117},
  shortjournal = {IET Image Process.},
  title        = {An unsupervised generative adversarial network for single image deraining},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate and efficient vehicle detection framework based on
SSD algorithm. <em>IETIP</em>, <em>15</em>(13), 3094–3104. (<a
href="https://doi.org/10.1049/ipr2.12297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection plays an important role in intelligent transportation systems and security. Using the original Single Shot MultiBox Detector (SSD) directly for vehicle detection, lacks accuracy and stability. Moreover, most of the state-of-the-art methods need cost a lot of time to inference. Vehicle detection is often used in complex traffic environments. Therefore, faster detection speed and higher detection accuracy are required. This study is aimed at developing a trade-off between accuracy and speed vehicle detection framework based on the SSD algorithm. To improve the multi-scale detection performance of SSD, semantic information, detailed features and receptive fields are combined to propose the feature pyramid enhancement strategy (FPES). On the other hand, the cascade detection mechanism is proposed to strengthen the positioning capability of SSD and an adaptive threshold acquisition method for object detection module (ODM) stage to improve model accuracy. Finally, a more efficient convolutional network is deployed through network slimming. Experimental results demonstrate that the proposed framework achieves state-of-the-art performance on UA-DETRAC and Udacity benchmarks. Interestingly, the inference time is the lowest for the proposed method than the state-of-the-art methods, promising its application for fast and effective vehicle detection.},
  archive      = {J_IETIP},
  author       = {Min Zhao and Yuan Zhong and Dihua Sun and Yuhao Chen},
  doi          = {10.1049/ipr2.12297},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3094-3104},
  shortjournal = {IET Image Process.},
  title        = {Accurate and efficient vehicle detection framework based on SSD algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Road extraction from high resolution remote sensing image
via a deep residual and pyramid pooling network. <em>IETIP</em>,
<em>15</em>(13), 3080–3093. (<a
href="https://doi.org/10.1049/ipr2.12296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The road extraction from high resolution remote sensing image is of great importance in a variety of applications. Recently, the abundant deep convolutional neural networks are proposed for road extraction task. However, the existing approaches lack suitable strategy to utilize multiple views road features for road extraction, which fails to extract road with smooth appearance and accurate boundary under complex scenes. To address this problem, the authors propose a novel deep residual and pyramid pooling network (DRPPNet) for extracting road regions from high resolution remote sensing image. The DRPPNet consists of three parts: deep residual network (DResNet), pyramid pooling module (PPM) and deep decoder (DD). Specially, the DResNet uses several residual blocks to extract deep road features from input images, which can enhance learning ability of DRPPNet and avoid gradient vanish. Then, PPM is proposed to fuse road features from multiple views and it aims to address disadvantage of single view feature. Finally, the DD is used to recover size of feature maps to input size. Extensive experiments on two challenging road datasets demonstrate that proposed method outperforms the state-of-the-art methods greatly on performance of road extraction task.},
  archive      = {J_IETIP},
  author       = {Yibo Han and Pu Han and Manlei Jia},
  doi          = {10.1049/ipr2.12296},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3080-3093},
  shortjournal = {IET Image Process.},
  title        = {Road extraction from high resolution remote sensing image via a deep residual and pyramid pooling network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). YOLOFig detection model development using deep learning.
<em>IETIP</em>, <em>15</em>(13), 3071–3079. (<a
href="https://doi.org/10.1049/ipr2.12293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of fruit, including accuracy and speed is of great significance for robotic harvesting. Nevertheless, attributes such as illumination variation, occlusion, and so on have made fruit detection a challenging task. A robust YOLOFig detection model was proposed to solve detection challenges and to improve detection accuracy and speed. The YOLOFig detection model incorporated Leaky activated ResNet43 backbone with a new 2,3,4,3,2 residual block arrangement, spatial pyramid pooling network (SPPNet), feature pyramid network (FPN), complete (CIoU) loss, and distance DIoU−NMS to improve the fruit detection performance. The obtained average precision (AP) and speed (frames per second or fps) respectively under 2,3,4,3,2 residual block arranged backbone for YOLOv3b is 78.6% and 69.8 fps, YOLOv4b is 87.6% and 57.1 fps, and YOLOFig is 89.3% and 96.8 fps; under 1,2,8,8,4 residual block arranged backbone for YOLOv3 is 77.1% and 56.3 fps, YOLOv4 is 87.1% and 52.5 fps, and YOLOResNet70 is 87.3% and 79 fps; and under 3,4,6,3 residual block arranged backbone for YOLOResNet50 is 85.4% and 77.1 fps. An indication that the new residual block arranged backbone of 2,3,4,3,2 outperformed 1,2,8,8,4 on an average AP of 1.33% and detection speed of 15.2%. Finally, the compared results showed that the YOLOFig detection model performed better than other models at the same level of residual block arrangement. It can better generalize and is highly suitable for real-time harvesting robots.},
  archive      = {J_IETIP},
  author       = {Olarewaju Mubashiru Lawal and Huamin Zhao},
  doi          = {10.1049/ipr2.12293},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3071-3079},
  shortjournal = {IET Image Process.},
  title        = {YOLOFig detection model development using deep learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector co-occurrence morphological edge detection for colour
image. <em>IETIP</em>, <em>15</em>(13), 3063–3070. (<a
href="https://doi.org/10.1049/ipr2.12290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morphological edge detection is a principal component in pattern recognition and machine vision. Traditional edge detection operators only take pixel mutual into consideration. However, the edges are influenced not only by pixel mutual but also by the boundary characteristics. Here, the vector co-occurrence morphological edge detection operator is proposed, which takes the pixel and boundary information both into consideration. The vector co-occurrence algorithm is exploited to resist the influence of the noise points and detect the edges from the colour image rather than the grey image. And, we lead to define a precise definition of the manner of sorting high-dimensional data for the colour image. The experiment results always illustrate the advancement and practicability of our methods against the baseline method. In terms of experiments, the BSDS500 dataset is introduced to compare and analyse with other algorithms. Based on the standard benchmark index evaluation in the BSDS500 dataset, the ODS and AP of various algorithms are compared and analysed.},
  archive      = {J_IETIP},
  author       = {Ying Lu and Chunming He and Yu-Feng Yu and Guoxia Xu and Hu Zhu and Lizhen Deng},
  doi          = {10.1049/ipr2.12290},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3063-3070},
  shortjournal = {IET Image Process.},
  title        = {Vector co-occurrence morphological edge detection for colour image},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parallel multi-block alternating direction method of
multipliers for tensor completion. <em>IETIP</em>, <em>15</em>(13),
3053–3062. (<a href="https://doi.org/10.1049/ipr2.12289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an algorithm for the tensor completion problem of estimating multi-linear data under the limitation of observation rate. Many tensor completion methods are based on nuclear norm minimization, they may fail to achieve the global solution for solving nuclear norm minimization in tensor completion problem with high missing ratio. To tackle this issue, an adaptive tensor completion method based on parallel multi-block alternating direction method of multipliers (ADMM) algorithm is proposed, it can derive the model from the initial estimate and compute the next estimate from the current solution. The parallel multi-block ADMM with global convergence is adopted to solve the dual problem, which greatly improves the processing power and reliability of the algorithm.},
  archive      = {J_IETIP},
  author       = {Hu Zhu and Zhongyang Wang and Taiyu Yan and Yu-Feng Yu and Lizhen Deng and Bing-Kun Bao},
  doi          = {10.1049/ipr2.12289},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3053-3062},
  shortjournal = {IET Image Process.},
  title        = {A parallel multi-block alternating direction method of multipliers for tensor completion},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integration of gradient guidance and edge enhancement into
super-resolution for small object detection in aerial images.
<em>IETIP</em>, <em>15</em>(13), 3037–3052. (<a
href="https://doi.org/10.1049/ipr2.12288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small objects are difficult because of their poor-quality appearance and small size, and such issues are especially pronounced for aerial images of great importance. To address the small object detection (SOD) problem, a united architecture that tries to upsample small objects into super-resolved versions, achieving characteristics similar to those large objects and thus resulting in more discriminative detection is used. For this purpose, a new end-to-end multi-task generative adversarial network (GAN) is proposed. In the architecture, the generator is a super-resolution (SR) network, and the discriminator is a multi-task network. In the generator, a gradient guide and an edge-enhancement strategy are introduced to alleviate structural distortions. In the discriminator, a faster region-based convolutional neural network (FRCNN) is incorporated for the task of object detection. Specifically, the discriminator outputs a distribution scalar to measure the realness. Then, each super-resolved image passes through the discriminator with a realness distribution, classification scores, and bounding box regression offsets. Furthermore, the losses of the detection task are backpropagated into the generator during training rather than being optimized independently. Extensive experiments on the challenging cars overhead with context dataset (COWC), detectIon in optical remote sensing images (DIOR), vision meets drones (VisDrone), and dataset for object detection in aerial images (DOTA) demonstrate the effectiveness of the proposed method in reconstructing structures while generating natural super-resolved images and show the superiority of the proposed method in detecting small objects over state-of-the-art detectors.},
  archive      = {J_IETIP},
  author       = {Jinzhen Mu and Shuang Li and Zongming Liu and Yan Zhou},
  doi          = {10.1049/ipr2.12288},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3037-3052},
  shortjournal = {IET Image Process.},
  title        = {Integration of gradient guidance and edge enhancement into super-resolution for small object detection in aerial images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank nonnegative sparse representation and local
preservation-based matrix regression for supervised image feature
selection. <em>IETIP</em>, <em>15</em>(13), 3021–3036. (<a
href="https://doi.org/10.1049/ipr2.12281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix regression has attracted much attention due to directly select some meaningful features from matrix data. However, most existing matrix regressions do not consider the global and local structure of the matrix data simultaneously. To this end, we propose a low-rank nonnegative sparse representation and local preserving matrix regression (LNSRLP-MR) model for image feature selection. Here, the loss function is defined by the left and right regression matrices. To capture the global structure and discriminative information of the training images and reduce the effect of heterogeneous data and noises, we impose the low-rank constraint on the self-representation error matrix and the nonnegative sparse constraint on the coefficient vector. The graph matrix can be learned adaptively through representation coefficients, so that accurate local structure information in samples can be revealed. Feature selection is performed by obtained row sparse transformation matrix. An optimization procedure and its performance are also present. Experimental results on several image datasets show that compared with the-state-of-the-art method, the average classification accuracy of the proposed method is improved by at least 1.2% and up to 3.3%. For images with noise or occlusion, the accuracy is improved significantly, up to 4%, which indicates that this method has strong robustness.},
  archive      = {J_IETIP},
  author       = {Xingyu Zhu and Xiuhong Chen},
  doi          = {10.1049/ipr2.12281},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3021-3036},
  shortjournal = {IET Image Process.},
  title        = {Low-rank nonnegative sparse representation and local preservation-based matrix regression for supervised image feature selection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Erratum: Secure high capacity tetris-based scheme for data
hiding. <em>IETIP</em>, <em>15</em>(12), 3020. (<a
href="https://doi.org/10.1049/ipr2.12291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/ipr2.12291},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3020},
  shortjournal = {IET Image Process.},
  title        = {Erratum: Secure high capacity tetris-based scheme for data hiding},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-light image enhancement based on exponential retinex
variational model. <em>IETIP</em>, <em>15</em>(12), 3003–3019. (<a
href="https://doi.org/10.1049/ipr2.12287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of residual noise, low contrast, and limited detail information caused by low-light images, this paper proposes a new Retinex variational model. According to Retinex theory, it is necessary to estimate the illumination and reflectance components decomposed from the original image. In order to better maintain the edge information, texture richness, and prevent artefacts, the exponential forms of local variation deviation and total variation are used as illumination prior and reflectance prior, respectively, and mixed norms are used to constrain them, so as to deal with the illumination information and texture details of the image more effectively, and then use the bright channel prior to improve the colour reproduction sense of the original image, thereby constructing the objective function, and finally using the alternating iterative optimization method to find the optimal solution to the proposed model. Experiments show that compared with other existing image enhancement methods, the method proposed here improves the contrast of the image, overcomes the phenomenon of halo artefacts and colour distortion, is more consistent with human vision, and produces better results in terms of quantitative performance.},
  archive      = {J_IETIP},
  author       = {Xinyu Chen and Jinjiang Li and Zhen Hua},
  doi          = {10.1049/ipr2.12287},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3003-3019},
  shortjournal = {IET Image Process.},
  title        = {Low-light image enhancement based on exponential retinex variational model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor dimensionality reduction via mode product and HSIC.
<em>IETIP</em>, <em>15</em>(12), 2986–3002. (<a
href="https://doi.org/10.1049/ipr2.12285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor dimensionality reduction (TDR) is a hot research topic in machine learning, which learns data representations by preserving the original data structure while avoiding convert samples into vectors and solving the problem of the curse of dimensionality of tensor data. In the work, a novel TDR approach based on mode product and Hilbert–Schmidt Independence criterion (HSIC) is proposed. The contributions of authors&#39; work is described as following: (1) HSIC measures the statistical correlation of two random variables. However, instead of measuring the statistical correlation of two random variables directly, HSIC first transforms the two random variables into two reproducing kernel Hilbert spaces (RKHSs), and then measures the statistical correlation of transformed random variables by using Hilbert–Schmidt operators between the two RKHSs. The exploitation of RKHS increases the flexibility and applicability of HSIC. Although HSIC is widely used in machine learning, the authors have not seen its application to dimensionality reduction (DR)(except for authors&#39; previous work). (2) A novel HSIC-based TDR approach is proposed, which first applies HSIC to capture statistical information of tensor data set for DR. The authors give the mathematical derivation of HSIC for tensor data and establish a framework of TDR based on HSIC, named HSIC-TDR for short, which aims to improve the DR results of tensor by exploring and preserving the statistical information of original data set. (3) Furthermore, to solve the out-of-sample problem, the authors learn an explicit expression between the dimensionality-reduced tensors and the higher-dimensional tensors by introducing mode product to HSIC-TDR. The experimental results between the proposed method and other state-of-the-art algorithm on various datasets demonstrate the well performance of the proposed method.},
  archive      = {J_IETIP},
  author       = {Guo Niu and Zhengming Ma},
  doi          = {10.1049/ipr2.12285},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2986-3002},
  shortjournal = {IET Image Process.},
  title        = {Tensor dimensionality reduction via mode product and HSIC},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High energy flash x-ray image restoration using region
extrema and kernel optimization. <em>IETIP</em>, <em>15</em>(12),
2970–2985. (<a href="https://doi.org/10.1049/ipr2.12284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of high energy flash X-ray images is crucial to the high-precision diagnosis of object density. High energy flash X-ray radiography is susceptible to the system blur, which usually causes the poor quality of static images. In response to this, a novel restoration algorithm using region extrema and kernel optimization (REKO) is presented. Based on the observation that the region extrema distribution of blurred high energy flash X-ray images deviates from opposite ends of image grey domain, the sparseness-inducing prior for regularizing image region extrema is applied to construct the restoration model. Considering the sparse characteristics of blur kernels, the sparseness-inducing regularization is incorporated to constrain blur kernels in the restoration model. The non-convex and non-linear objective function is gradually minimized through energy alternating minimization and dually linear approximation. Furthermore, a continuity enforced kernel optimization algorithm is proposed to estimate more accurate blur kernels. The discontinuous kernel elements are suppressed by extracting the main structure of blur kernels and constructing kernel continuity function in cross windows. Experimental results demonstrate that our algorithm can more accurately estimate blur kernels and achieve restoration results with sharper edges on high energy flash X-ray images.},
  archive      = {J_IETIP},
  author       = {Xiaolin Wang and Qingwu Li and Jinxin Xu},
  doi          = {10.1049/ipr2.12284},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2970-2985},
  shortjournal = {IET Image Process.},
  title        = {High energy flash X-ray image restoration using region extrema and kernel optimization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PaI-net: A modified u-net of reducing semantic gap for
surgical instrument segmentation. <em>IETIP</em>, <em>15</em>(12),
2959–2969. (<a href="https://doi.org/10.1049/ipr2.12283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking the instruments in a surgical scene is an essential task in minimally invasive surgery. However, due to the unpredictability of scenes, automatically segmenting the instruments is very challenging. In this paper, a novel method named parallel inception network (PaI-Net) is proposed, in which an attention parallel module (APM) and an output fusion module (OFM) are integrated with U-Net to improve the segmentation ability. Specially, APM utilizes multi-scale convolution kernels and global average pooling operations to extract semantic information and global context information of different scales, while OFM combines the feature maps of the decoder part to aggregate the abundant boundary information of shallow layers and the rich semantic information of deep layers together, which achieve a significant improvement in generating segmentation masks. Finally, the evaluation of proposed method on robotic instruments segmentation task from Medical Image Computing and Computer Assisted Intervention Society (MICCAI) and retinal image segmentation task from International Symposium on Biomedical Imaging (ISBI) show that our model has achieved advanced performance on multi-scale semantic segmentation and is superior to the current state-of-the-art models.},
  archive      = {J_IETIP},
  author       = {Xiaoyan Wang and Luyao Wang and Xingyu Zhong and Cong Bai and Xiaojie Huang and Ruiyi Zhao and Ming Xia},
  doi          = {10.1049/ipr2.12283},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2959-2969},
  shortjournal = {IET Image Process.},
  title        = {PaI-net: A modified U-net of reducing semantic gap for surgical instrument segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pain expression assessment based on a locality and identity
aware network. <em>IETIP</em>, <em>15</em>(12), 2948–2958. (<a
href="https://doi.org/10.1049/ipr2.12282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical medicine, the pain feeling is a significant indicator for the medical condition of patients. Of late, automatic pain assessment methods have received more and more interests. Many researchers proposed corresponding methods and achieved impressive results. However, they always ignore the locality and individual differences of painful expression. Therefore, a locality and identity aware network (LIAN) for pain assessment is presented here. Concretely, for the locality characteristic, a locality aware module consisting of a two-branch structure, feature and attention branches, is presented. The former learns pain features by a deep network, while the latter guides the pain features to focus on the discriminational regions of pain. As for the individual differences, an identity aware module with a multi-task method is proposed to represent identity-related information to achieve identity-invariant pain assessment. Extensive experiments on public databases show the superiority of LIAN in pain assessment.},
  archive      = {J_IETIP},
  author       = {Xuwu Xin and Xiaowu Li and Shengfu Yang and Xiaoyan Lin and Xin Zheng},
  doi          = {10.1049/ipr2.12282},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2948-2958},
  shortjournal = {IET Image Process.},
  title        = {Pain expression assessment based on a locality and identity aware network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging attention-based visual clue extraction for image
classification. <em>IETIP</em>, <em>15</em>(12), 2937–2947. (<a
href="https://doi.org/10.1049/ipr2.12280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based approaches have made considerable progress in image classification tasks, but most of the approaches lack interpretability, especially in revealing the decisive information causing the categorization of images. This paper seeks to answer the question of what clues encode the discriminative visual information between image categories and can help improve the classification performance. To this end, an attention-based clue extraction network (ACENet) is introduced to mine the decisive local visual information for image classification. ACENet constructs a clue-attention mechanism, that is global-local attention, between the image and visual clue proposals extracted from it and then introduces a contrastive loss defined over the achieved discrete attention distribution to increase the discriminability of clue proposals. The loss encourages considerable attention to be devoted to discriminative clue proposals, that is those similar within the same category and dissimilar across categories. The experimental results for the Negative Web Image (NWI) dataset and the public ImageNet2012 dataset demonstrate that ACENet can extract true clues to improve the image classification performance and outperforms the baselines and the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yunbo Cui and Youtian Du and Xue Wang and Hang Wang and Chang Su},
  doi          = {10.1049/ipr2.12280},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2937-2947},
  shortjournal = {IET Image Process.},
  title        = {Leveraging attention-based visual clue extraction for image classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multistage reaction-diffusion equation network for image
super-resolution. <em>IETIP</em>, <em>15</em>(12), 2926–2936. (<a
href="https://doi.org/10.1049/ipr2.12279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based models have progressed considerably in single-image super-resolution. A high-resolution pattern generation task is performed at the end of convolution neural networks (CNNs) with some convolution-based operations in these models. However, this process may be difficult because all the work is done through the remarkable learning ability of CNN without any specific learning target. Reaction-diffusion equation (RDE) is a mechanism involved in the pattern generation process that can serve as a guide for super-resolution. It is proposed to embed RDE into a super-resolution network by designing a reaction-diffusion process block (RDPB) in this study. The proposed RDPB uses Euler method for iteratively solving one particular RDE, which is determined by the parameter generated through CNN. Accordingly, this module guides and leads the CNN in generating patterns for image super-resolution. Moreover, a multistage framework is constructed to guide each network module further. On the basis of these two designs, the multistage reaction-diffusion equation network is proposed for image super-resolution. Experimental results demonstrated that the proposed model can obtain findings consistent with the conclusions of state-of-the-art methods with a relatively shallow structure and small model size.},
  archive      = {J_IETIP},
  author       = {Xiaofeng Pu and Zengmao Wang},
  doi          = {10.1049/ipr2.12279},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2926-2936},
  shortjournal = {IET Image Process.},
  title        = {Multistage reaction-diffusion equation network for image super-resolution},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infrared imaging enhancement through local window-based
saliency extraction with spatial weight. <em>IETIP</em>,
<em>15</em>(12), 2910–2925. (<a
href="https://doi.org/10.1049/ipr2.12276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared image enhancement is an effective way to solve contrast reduction or details degradation in infrared imagery. An infrared enhancement approach based on local saliency extraction is proposed here. First, saliency maps are extracted within a local window by combining spatial weight. Second, with the change of the window size, potential targets and details in different sizes can be extracted. Considering window sizes as the scales, the saliency maps are obtained and infrared images are enhanced at different scales, and finally, multi-scale fusion is used to achieve the enhancement. Eight popular infrared enhancement approaches are introduced for comparison. Subjective qualitative observation experiments show that our strategy based on local saliency analysis and multi-scale fusion can well extract potential targets and areas of varying size from the source images with different sizes, to obtain a good enhancement effect. Meanwhile, we introduce six objective evaluation methods to measure the results, and the evaluation data to prove the effectiveness of the proposed algorithm. The experiments also indicate the real-time processing capability of the proposed method. The proposed method is finally well applied in the hardware system and shows its good performance.},
  archive      = {J_IETIP},
  author       = {Tong Li and Jufeng Zhao and Xiaohui Wu and Haifeng Mao and Guangmang Cui},
  doi          = {10.1049/ipr2.12276},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2910-2925},
  shortjournal = {IET Image Process.},
  title        = {Infrared imaging enhancement through local window-based saliency extraction with spatial weight},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge feature enhancement approach using hilbert transform of
cauchy distribution and its applications. <em>IETIP</em>,
<em>15</em>(12), 2891–2909. (<a
href="https://doi.org/10.1049/ipr2.12275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on past work showing that the Hilbert transform can be used for edge feature enhancement, a new edge feature enhancement method is developed using a two-dimensional (2D) isotropic Hilbert transform of the Cauchy distribution. First, both the shape of the Hilbert kernel and the Hilbert transform of edge feature models (step and delta) and various simulated signals result in edge feature enhancement, the properties of which are derived and confirmed. Second, Cauchy distribution as low-pass filter is introduced in Hilbert transform to get new edge feature enhancement operator, and its efficiency under various criteria is comprehensively discussed. Third, a 2D isotropic extension is presented using a circularly symmetric window function. Finally, two experiments, including edge detection and image segmentation, are performed to validate the proposed edge feature enhancement method. The experimental results of the method applied to the Berkeley Segmentation Dataset and remote sensing images demonstrate that the new method is effective for edge detection and image segmentation.},
  archive      = {J_IETIP},
  author       = {Ke Wang and Gustavo K. Rohde and Jian Xiao},
  doi          = {10.1049/ipr2.12275},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2891-2909},
  shortjournal = {IET Image Process.},
  title        = {Edge feature enhancement approach using hilbert transform of cauchy distribution and its applications},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous digital zooming using generative adversarial
networks for dual camera system. <em>IETIP</em>, <em>15</em>(12),
2880–2890. (<a href="https://doi.org/10.1049/ipr2.12274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a generative adversarial network (GAN) with patch match algorithm to realize a high-quality digital zooming using two camera modules with different focal lengths. In dual camera system, shorter focal length module produces the wide-view image with the low resolution. On the other hand, the longer focal length module produces the tele-view image via optical zooming. The long-focal image contains more details than short-focal image and can be used to guide short-focal image to reconstruct high frequency part. Firstly, a feature extraction block (FEB) is advanced to extract feature of long-focal image and short focal-image to reconstruct a wide-view image with different resolutions. Next, a patch match algorithm is integrated into convolution neural networks (CNN) to fuse information of long-focal with short-focal image and generate a new fused image. Finally, the fused image and short-focal image are merged with a feature fusion block (FFB) to predict high-resolution images. In addition, generative adversarial networks are used for filtering information integrated by previous network and output the zoomed image. Extensive experiments on benchmark datasets show that our algorithm achieves favorable performance against state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yifan Yang and Qi Li and Yongyi Yu and Zhuang He and Huajun Feng and Zhihai Xu and Yueting Chen},
  doi          = {10.1049/ipr2.12274},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2880-2890},
  shortjournal = {IET Image Process.},
  title        = {Continuous digital zooming using generative adversarial networks for dual camera system},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of hand-wrist maturity level based on
similarity matching. <em>IETIP</em>, <em>15</em>(12), 2866–2879. (<a
href="https://doi.org/10.1049/ipr2.12273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Judging the maturity level of each hand-wrist reference bone is the core issue in bone age assessment. Relying on the superiority of convolutional neural networks in feature representation, deep learning is widely studied for the automatic bone age assessment. However, an efficient but complex deep learning network requests a large dataset with bone-maturity-level labels for training, restricting its large-scale application in bone maturity classification. For this reason, we transform the bone-maturity-level classification problem into the similarity matching problem. Also, we propose a general structure based on Siamese network by merging two inputs into a two-channel input and introducing a dual attention mechanism, to create an Attentional Two-Channel Network (ATC-Net). This paper takes the intermediate phalanges III as an example to assess the performance of the similarity matching method and the ATC-Net. Experiments show that our method can perform better on small datasets, which effectively makes up for the data shortage problem. The ATC-Net used for classification significantly reduces the evaluation time compared with other classical networks. It reduces the time of assessing one sample by about 49% as compared to VGG-16. And more importantly, it achieves the highest classification accuracy of 92.74% among all investigated networks.},
  archive      = {J_IETIP},
  author       = {Keji Mao and Lijian Chen and Minhao Wang and Ruiji Xu and Xiaomin Zhao},
  doi          = {10.1049/ipr2.12273},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2866-2879},
  shortjournal = {IET Image Process.},
  title        = {Classification of hand-wrist maturity level based on similarity matching},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BFGS method based variable projection approach for image
restoration. <em>IETIP</em>, <em>15</em>(12), 2854–2865. (<a
href="https://doi.org/10.1049/ipr2.12270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a variable projection approach based on the BFGS (Broyden–Fletcher–Goldfarb–Shanno) method for image reconstruction problems is proposed, which is an alternative to the common alternating minimisation scheme. The image restoration is expressed as a nonlinear least-squares problem with reduced parameter space. To improve the efficiency of the algorithm, the BFGS method is proposed to be used to optimise the reduced objective function. The large-scale problem considered in this paper is projected on to a small Krylov subspace using Lanczos bidiagonalisation. The regularisation parameter is selected by a weighted generalised cross validation criterion. Numerical examples demonstrate the efficiency and effectiveness of the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Qiong-Ying Chen and Yun-Zhi Huang and Min Gan and C. L. Philip Chen and Guang-Yong Chen},
  doi          = {10.1049/ipr2.12270},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2854-2865},
  shortjournal = {IET Image Process.},
  title        = {BFGS method based variable projection approach for image restoration},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face hallucination based on cluster consistent dictionary
learning. <em>IETIP</em>, <em>15</em>(12), 2841–2853. (<a
href="https://doi.org/10.1049/ipr2.12269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face hallucination is a super-resolution technique specially designed to reconstruct high-resolution faces from low-resolution faces. Most state-of-the-art algorithms leverage position-patch prior knowledge of human faces to better super-resolve face images. However, most of them assume the training face dataset is sufficiently large, well cropped or aligned. This paper, proposes a novel example-based face hallucination method, based on cluster consistent dictionary learning with the assumption that human faces have similar facial structures. In this method, the paired face image patches are firstly labelled as face areas including eyes, nose, mouth and other parts, as well as non-face areas without requiring the training face images cropped and aligned. Then, the training patches are clustered according their labels and textures. The cluster consistent dictionary is learned to represent the low-resolution patches and the high-resolution patches. Finally, the high-resolution patches of the input low-resolution face image can be efficiently generated by using the adjusted anchored neighbourhood regression. As utilizing the labelled facial parts prior knowledge, the proposed method represents more details in the reconstruction. Experimental results demonstrate that the authors&#39; algorithm outperforms many state-of-the-art techniques for face hallucination under different datasets.},
  archive      = {J_IETIP},
  author       = {Minqi Li and Xiangjian He and Kin-Man Lam and Kaibing Zhang and Junfeng Jing},
  doi          = {10.1049/ipr2.12269},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2841-2853},
  shortjournal = {IET Image Process.},
  title        = {Face hallucination based on cluster consistent dictionary learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image resolution and contrast enhancement with optimal
brightness compensation using wavelet transforms and particle swarm
optimization. <em>IETIP</em>, <em>15</em>(12), 2833–2840. (<a
href="https://doi.org/10.1049/ipr2.12268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the image resolution and contrast along with uniform brightness distribution over the entire image helps to retrieve the vital realistic information necessary for human perception and interpretation. A new procedure to improve these parameters is implemented and tested. Initially, the image is super resolute using discrete wavelet transform (DWT), stationary wavelet transform (SWT) image decomposition and bicubic interpolation. The resolute image is used for contrast enhancement by using SWT and contrast limited adaptive histogram equalization (CLAHE) approach. The optimized brightness compensation technique, which uses particle swarm optimization (PSO), is applied to the resolution and contrast-enhanced image to obtain uniform brightness distribution over the entire image. The proposed approach is tested on three data sets of images and it is found that the visual results, as well as the resolution and contrast levels of the tested images, are significantly superior to subjective and objective results.},
  archive      = {J_IETIP},
  author       = {V Hyma Lakshmi Tirumani and Madhu Tenneti and Ch. Srikavya K and Sarat Kumar Kotamraju},
  doi          = {10.1049/ipr2.12268},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2833-2840},
  shortjournal = {IET Image Process.},
  title        = {Image resolution and contrast enhancement with optimal brightness compensation using wavelet transforms and particle swarm optimization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SFCN: Symmetric feature comparison network for detecting
ischemic stroke lesions on CT images. <em>IETIP</em>, <em>15</em>(12),
2818–2832. (<a href="https://doi.org/10.1049/ipr2.12267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  author       = {Long Zhang and Chuang Zhu and YueWei Wu and Yang Yang and Yihao Luo and Ruoning Song and Lian Liu and Jie Yang},
  doi          = {10.1049/ipr2.12267},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2818-2832},
  shortjournal = {IET Image Process.},
  title        = {SFCN: Symmetric feature comparison network for detecting ischemic stroke lesions on CT images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fuzzy spectral clustering algorithm for hyperspectral
image classification. <em>IETIP</em>, <em>15</em>(12), 2810–2817. (<a
href="https://doi.org/10.1049/ipr2.12266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is an unsupervised clustering algorithm, and is widely used in the field of pattern recognition and computer vision due to its good clustering performance. However, the traditional spectral clustering algorithm is not suitable for large-scale data classification, such as hyperspectral remote sensing image, because of its high computational complexity, and it is difficult to characterize the inherent uncertainty of the hyperspectral remote sensing image. This paper uses fuzzy anchors to process hyperspectral image classification and proposes a novel spectral clustering algorithm based on fuzzy similarity measure. The proposed algorithm utilizes the fuzzy similarity measure to obtain the similarity between the data points and the anchors, and then gets the similarity matrix. Finally, spectral clustering is performed on the similarity matrix to compute the classification results. The experimental results on the hyperspectral remote sensing image data sets have demonstrated the effectiveness of the proposed algorithm, and the introduction of fuzzy similarity measure gives rise to a more robust similarity matrix. Compared with existing methods, the proposed algorithm has a better classification result on the hyperspectral remote sensing image, and the kappa coefficient obtained by the proposed algorithm is 2% higher than the traditional algorithms.},
  archive      = {J_IETIP},
  author       = {Kang Li and Jindong Xu and Tianyu Zhao and Zhaowei Liu},
  doi          = {10.1049/ipr2.12266},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2810-2817},
  shortjournal = {IET Image Process.},
  title        = {A fuzzy spectral clustering algorithm for hyperspectral image classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Candidate box fusion based approach to adjust position of
the candidate box for object detection. <em>IETIP</em>, <em>15</em>(12),
2799–2809. (<a href="https://doi.org/10.1049/ipr2.12264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of object detection has been applied to all aspects in our lives. Although object detection methods based on deep learning have been widely used in various fields, there are still some overlooked problems in the candidate box selection stage. The detection results of traditional candidate box selection methods can only select a relatively optimal maximum candidate box. If the maximum candidate box is still not accurate enough, this type of methods will not be able to do adjust it. To solve this problem, an object detection method based on the multiple candidate box fusion is proposed. The method can not only retain the maximum candidate box and delete the non-maximum candidate box, but also adjust the position of the maximum candidate box again. Thereby a more accurate maximum candidate box can be obtained. In order to verify the generalization ability of the method, the candidate box fusion method is combined with the two object detection frameworks: faster R-CNN model and YOLOv3 model. The results of these experiments prove that the proposed method can achieve higher detection accuracy and complete the object detection task more effectively.},
  archive      = {J_IETIP},
  author       = {Jie Cao and Wei Ren and Hong Zhang and Zuohan Chen},
  doi          = {10.1049/ipr2.12264},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2799-2809},
  shortjournal = {IET Image Process.},
  title        = {Candidate box fusion based approach to adjust position of the candidate box for object detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical image encryption scheme based on self-verification
matrix. <em>IETIP</em>, <em>15</em>(12), 2787–2798. (<a
href="https://doi.org/10.1049/ipr2.12263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the shortcomings of existing medical image encryption algorithms, including a lack of anti-tampering methods and security, this report presents an anti-tampering encryption algorithm for medical images that is based on a self-verification matrix. First, chaotic coordinates generated by chaos are used to traverse all pixels in a plain image to generate a two-dimensional matrix (a self-verification matrix) with positioning information. The accurate location of illegally altered image pixels can be detected using the self-verification matrix. To improve the security of the self-authentication matrix, DNA coding is also applied to the self-authentication matrix, and the plain image is also diffused statically to destroy the pixel distribution. Next, the scrambled image and self-verification matrix are mixed and cross-scrambled. Finally, the fused image is diffused dynamically to improve the security of the encrypted image. Experimental simulation and performance analysis show that the algorithm achieves good encryption effectiveness, provides strong anti-tampering capabilities, and can accurately locate at least 4 pixels.},
  archive      = {J_IETIP},
  author       = {Zhenlong Man and Jinqing Li and Xiaoqiang Di},
  doi          = {10.1049/ipr2.12263},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2787-2798},
  shortjournal = {IET Image Process.},
  title        = {Medical image encryption scheme based on self-verification matrix},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustly correlated key-medical image for DNA-chaos based
encryption. <em>IETIP</em>, <em>15</em>(12), 2770–2786. (<a
href="https://doi.org/10.1049/ipr2.12261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images include confidential and sensitive information about patients. Hence, ensuring the security of these images is a crucial requirement. This paper proposes an efficient and secure medical image encryption-decryption scheme based on deoxyribonucleic acid (DNA), one-dimensional chaotic maps (tent and logistic maps), and hash functions (SHA-256 and MD5). The first part of the proposed scheme is the key generation based on the hash functions of the image and its metadata. The key then is highly related and intensely sensitive to the original image. The second part is the rotation and permutation of the first two MSB bit-plans of the medical image to reduce its black background that produces redundant DNA encoded sequences. The third part is the DNA encoding-decoding using dynamically chosen DNA rules for every 2-bit pixel value through the logistic map. Meanwhile, the confusion-diffusion is performed using the tent map and XOR operation. Simulation results and security analysis prove the good encryption effects of the proposed scheme compared to the state-of-art methods with a correlation of 6.66617e-7 and a very large key space of 2 624 . Furthermore, the proposed system has a strong ability to resist various common attacks such as chosen/known-plaintext attacks and cropping/noise attacks.},
  archive      = {J_IETIP},
  author       = {Ichraf Aouissaoui and Toufik Bakir and Anis Sakly},
  doi          = {10.1049/ipr2.12261},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2770-2786},
  shortjournal = {IET Image Process.},
  title        = {Robustly correlated key-medical image for DNA-chaos based encryption},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gait recognition based on sparse linear subspace.
<em>IETIP</em>, <em>15</em>(12), 2761–2769. (<a
href="https://doi.org/10.1049/ipr2.12260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition has broad application prospects in intelligent security monitoring. However, due to the variability of human walking states and the complexity of external conditions during sample collection, gait recognition is still facing many challenges. Among them, gait recognition algorithms based on shallow learning are hard to achieve the correct recognition rate required by many applications, while the amount of gait training data cannot meet the needs of model training based on deep learning. To solve the above problem, this paper presents a novel gait recognition scheme based on sparse linear subspace. First, frame-by-frame gait energy images (ffGEIs) are extracted as primary gait features and sparse linear subspace technology is used to represent them for dimension reduction. Second, a new gait classification algorithm based on support vector machine is presented, which adopts Gaussian radial basis function (RBF) kernels to achieve cross-view gait recognition. Finally, the proposed gait recognition approach is evaluated on two open-accessed gait databases to demonstrate its performance.},
  archive      = {J_IETIP},
  author       = {Junqin Wen and Xiuhui Wang},
  doi          = {10.1049/ipr2.12260},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2761-2769},
  shortjournal = {IET Image Process.},
  title        = {Gait recognition based on sparse linear subspace},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel weighted total variation model for image denoising.
<em>IETIP</em>, <em>15</em>(12), 2749–2760. (<a
href="https://doi.org/10.1049/ipr2.12259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a very important problem in image processing field. In order to improve denoising effects and meanwhile keep image structures, a novel weighted total variation (WTV) model is proposed in this paper. The WTV model consists of data fidelity and ℓ 1 norm based regularisation terms. In the WTV model, a weight function w in exponential form is incorporated into the regularisation term, which only depends on the given image itself without extra parameters. The nonlinearly monotone formulation of helps to increase gaps between lower and higher frequencies of images, which is effective to highlight edges and keep textures. For solving the proposed model, the alternating direction method of multipliers is explored and the according convergence is analysed. Compared experiments of TV, HOTV, ATV and models are conducted and the results show the effectiveness and efficiency of the proposed model.},
  archive      = {J_IETIP},
  author       = {Meng-Meng Li and Bing-Zhao Li},
  doi          = {10.1049/ipr2.12259},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2749-2760},
  shortjournal = {IET Image Process.},
  title        = {A novel weighted total variation model for image denoising},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detail texture detection based on yolov4-tiny combined with
attention mechanism and bicubic interpolation. <em>IETIP</em>,
<em>15</em>(12), 2736–2748. (<a
href="https://doi.org/10.1049/ipr2.12228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aero-engine blades crack detection is one of the important tasks in daily ground maintenance, crack is a kind of texture feature, due to the random distribution, irregular shape and vague characteristics, which is still a challenging task to realize automatic detection in working environment. A detection model based on the Yolov4-tiny is proposed that is universal and focuses more on the characteristics of cracks, and it is implemented in embedded device. First, in order to distinguish the cracks and noises, an improved attention module is introduced into the backbone of Yolov4-tiny to enhance the model&#39;s capability to focus on crack areas; second, in order to improve the effect of multi-scale feature fusion, the bicubic interpolation is implemented in upsampling module; finally, in order to solve the redundant detection results of bounding-boxes in crack areas, the optimized non-maximum suppression method is proposed to make the detection results better corresponding to the groundTruth. The robustness of proposed detection model was demonstrated by evaluating varying lighting and noise images. The average precision on integrated datasets is 81.6%, which outperforms the original Yolov4-tiny by an increase of 12.3%.},
  archive      = {J_IETIP},
  author       = {Tian Hui and YueLei Xu and Rasol Jarhinbek},
  doi          = {10.1049/ipr2.12228},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2736-2748},
  shortjournal = {IET Image Process.},
  title        = {Detail texture detection based on yolov4-tiny combined with attention mechanism and bicubic interpolation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic license plate recognition on microprocessors and
custom computing platforms: A review. <em>IETIP</em>, <em>15</em>(12),
2717–2735. (<a href="https://doi.org/10.1049/ipr2.12262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic license plate recognition (ALPR) is the process of extracting and recognizing character information within a localized license plate region. Typically, ALPR involves three steps; image capture, image procession and plate recognition. The performance of an ALPR is largely dependent on the quality of the captured image, which is determined by factors such as environmental variation, camera quality and occlusion. Image procession and plate recognition step involves image processing techniques that extract and recognizes license plate and characters, respectively. ALPR systems could be realized on microprocessors (software-based) or custom computing platforms (hardware-based). Drawbacks such as portability, power consumption and computational speed limit software-based ALPR for real-time deployment. Custom platforms for ALPR consume less power and achieve high processing speed for real-time capability. However, limited computing resources available within a custom chip make it difficult to implement State-of-the-Art computationally intensive algorithms. Thus, very few literatures discussed ALPR techniques on custom computing platforms. This paper presents a comprehensive review of algorithms and architectures of ALPR on microprocessors and custom computing platforms. Design approaches, performance, gaps, suggestions and trends are discussed.},
  archive      = {J_IETIP},
  author       = {Princewill Akpojotor and Adebayo Adetunmbi and Boniface Alese and Ayodeji Oluwatope},
  doi          = {10.1049/ipr2.12262},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2717-2735},
  shortjournal = {IET Image Process.},
  title        = {Automatic license plate recognition on microprocessors and custom computing platforms: A review},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Texture and exposure awareness based refill for HDRI
reconstruction of saturated and occluded areas. <em>IETIP</em>,
<em>15</em>(11), 2705–2716. (<a
href="https://doi.org/10.1049/ipr2.12257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dynamic-range image (HDRI) displays scenes as vivid as the real scenes. HDRI can be reconstructed by fusing a set of bracketed-exposure low-dynamic-range images (LDRI). For the reconstruction, many works succeed in removing the ghost artefacts caused by moving objects. The critical issue is reconstructing the areas which are saturated due to bad exposure and occluded due to motion with no ghost artefacts. To overcome this issue, this paper proposes texture and exposure awareness based refill. The proposed work first locates the saturated and occluded areas existing in input image set, then refills background textures or patches containing rough exposure and colour information into located areas. Proposed work can be integrated with multiple existing ghost removal works to improve the reconstruction result. Experimental results show that proposed work removes the ghost artefacts caused by saturated and occluded areas in subjective evaluation. For the objective evaluation, the proposed work improves the HDR-VDP-2 evaluation result for multiple conventional works by 1.33% on average.},
  archive      = {J_IETIP},
  author       = {Jianming Zhou and Yipeng Deng and Qin Liu and Takeshi Ikenaga},
  doi          = {10.1049/ipr2.12257},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2705-2716},
  shortjournal = {IET Image Process.},
  title        = {Texture and exposure awareness based refill for HDRI reconstruction of saturated and occluded areas},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised person re-identification approach based on
cross-view distribution alignment. <em>IETIP</em>, <em>15</em>(11),
2693–2704. (<a href="https://doi.org/10.1049/ipr2.12256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised clustering is a kind of popular solution for unsupervised person re-identification (re-ID). However, due to the influence of cross-view differences, the results of clustering labels are not accurate. To solve this problem, an unsupervised re ID method based on cross-view distributed alignment (CV-DA) to reduce the influence of unsupervised cross-view is proposed. Specifically, based on a popular unsupervised clustering method, density clustering DBSCAN is used to obtain pseudo labels. By calculating the similarity scores of images in the target domain and the source domain, the similarity distribution of different camera views is obtained and is aligned with the distribution with the consistency constraint of pseudo labels. The cross-view distribution alignment constraint is used to guide the clustering process to obtain a more reliable pseudo label. The comprehensive comparative experiments are done in two public datasets, i.e. Market-1501 and DukeMTMC-reID. The comparative results show that the proposed method outperforms several state-of-the-art approaches with mAP reaching 52.6% and rank1 71.1%. In order to prove the effectiveness of the proposed CV-DA, the proposed constraint is added into two advanced re-ID methods. The experimental results demonstrate that the mAP and rank increase by 0.5–2% after using the cross-view distribution alignment constraint comparing with that of the associated original methods without using CV-DA.},
  archive      = {J_IETIP},
  author       = {Xibin Jia and Xing Wang and Qing Mi},
  doi          = {10.1049/ipr2.12256},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2693-2704},
  shortjournal = {IET Image Process.},
  title        = {An unsupervised person re-identification approach based on cross-view distribution alignment},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved algorithm using weighted guided coefficient and
union self-adaptive image enhancement for single image haze removal.
<em>IETIP</em>, <em>15</em>(11), 2680–2692. (<a
href="https://doi.org/10.1049/ipr2.12255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visibility of outdoor images is usually significantly degraded by haze. Existing dehazing algorithms, such as dark channel prior (DCP) and colour attenuation prior (CAP), have made great progress and are highly effective. However, they all suffer from the problems of dark distortion and detailed information loss. This paper proposes an improved algorithm for single-image haze removal based on dark channel prior with weighted guided coefficient and union self-adaptive image enhancement. First, a weighted guided coefficient method with sampling based on guided image filtering is proposed to refine the transmission map efficiently. Second, the k -means clustering method is adopted to calibrate the original image into bright and non-bright colour areas and form a transmission constraint matrix. The constraint matrix is then marked by connected-component labelling, and small bright regions are eliminated to form an atmospheric light constraint matrix, which can suppress the halo effect and optimize the atmospheric light. Finally, an adaptive linear contrast enhancement algorithm with a union score is proposed to optimize restored images. Experimental results demonstrate that the proposed algorithm can overcome the problems of image distortion and detailed information loss and is more efficient than conventional dehazing algorithms.},
  archive      = {J_IETIP},
  author       = {Guangbin Zhou and Lifeng He and Yong Qi and Meimei Yang and Xiao Zhao and Yuyan Chao},
  doi          = {10.1049/ipr2.12255},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2680-2692},
  shortjournal = {IET Image Process.},
  title        = {An improved algorithm using weighted guided coefficient and union self-adaptive image enhancement for single image haze removal},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HNSF log-demons: Diffeomorphic demons registration using
hierarchical neighbourhood spectral features. <em>IETIP</em>,
<em>15</em>(11), 2666–2679. (<a
href="https://doi.org/10.1049/ipr2.12254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many biomedical applications require accurate non-rigid image registration that can cope with complex deformations. However, popular diffeomorphic Demons registration algorithms suffer from difficulties for complex and serious distortions since they only use image greyscale and gradient information. To address these difficulties, a new diffeomorphic Demons registration algorithm is proposed using hierarchical neighbourhood spectral features namely HNSF Log-Demons in this paper. In view of three important properties of hierarchical neighbourhood spectral features based on line graph such as rotation invariance, invariance of linear changes of brightness, and robustness to noise, the hierarchical neighbourhood spectral features of a reference image and a moving image is first extracted and these novel spectral features are incorporated into the energy function of the diffeomorphic registration framework to improve the capability of capturing complex distortions. Secondly, the Nystr m approximation based on random singular value decomposition is employed to effectively enhance the computational efficiency of HNSF Log-Demons. Finally, the hybrid multi-resolution strategy based on wavelet decomposition in the registration process is utilised to further improve the registration accuracy and efficiency. Experimental results show that the proposed HNSF Log-Demons not only effectively ensures the generation of smooth and reversible deformation field, but also achieves better performance than state-of-the-art algorithms.},
  archive      = {J_IETIP},
  author       = {Xiaogang Du and Dongxin Gu and Tao Lei and Song Wang and Xuejun Zhang and Hongying Meng},
  doi          = {10.1049/ipr2.12254},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2666-2679},
  shortjournal = {IET Image Process.},
  title        = {HNSF log-demons: Diffeomorphic demons registration using hierarchical neighbourhood spectral features},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral recovery-guided hyperspectral super-resolution
using transfer learning. <em>IETIP</em>, <em>15</em>(11), 2656–2665. (<a
href="https://doi.org/10.1049/ipr2.12253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single hyperspectral image (HSI) super-resolution (SR) has attracted researcher&#39;s attention; however, most existing methods directly model the mapping between low- and high-resolution images from an external training dataset, which requires large memory and computing resources. Moreover, there are few such available training datasets in real cases, which prevent deep-learning-based methods from further improving performance. Here, a novel single HSI SR method based on transfer learning is proposed. The proposed method is composed of two stages: spectral down-sampled image SR reconstruction based on transfer learning and HSI reconstruction via spectral recovery module. Instead of directly applying the learned knowledge from the colour image domain to HSI SR, the spectral down-sampled image is fed into a spatial SR model to obtain a high-resolution image, which acts as a bridge between the colour image and HSI. The spectral recovery network is used to restore the HSI from the bridge image. In addition, pre-training and collaborative fine-tuning are proposed to promote the performance of SR and spectral recovery. Experiments on two public HSI datasets show that the proposed method achieves promising SR performance with a small paired HSI dataset.},
  archive      = {J_IETIP},
  author       = {Shaolei Zhang and Guangyuan Fu and Hongqiao Wang and Yuqing Zhao},
  doi          = {10.1049/ipr2.12253},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2656-2665},
  shortjournal = {IET Image Process.},
  title        = {Spectral recovery-guided hyperspectral super-resolution using transfer learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reversible data hiding for encrypted image based on adaptive
prediction error coding. <em>IETIP</em>, <em>15</em>(11), 2643–2655. (<a
href="https://doi.org/10.1049/ipr2.12252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding (RDH) is a useful technique of data security. Embedding capacity is one of the most important performance of RDH for encrypted image. Many existing RDH algorithms for encrypted image do not reach desirable embedding capacity yet. To address this problem, a new RDH algorithm is proposed for encrypted image based on adaptive prediction error coding. The proposed RDH algorithm uses a block-based encryption scheme to preserve spatial correlation of original image in the encrypted domain and exploits a novel technique called adaptive prediction error coding to vacate room for data embedding. A key contribution of the proposed RDH algorithm is the adaptive prediction error coding. It can efficiently vacate room from encrypted image block by adaptively coding prediction errors according to block content and thus contributes to a large embedding capacity. Many experiments on benchmark image databases are done to validate performance of the proposed RDH algorithm. The results show that the average embedding rates on the open databases of UCID, BOSSBase and BOWS-2 are 1.7081, 2.4437 and 2.3083 bpp, respectively. Comparison results illustrate that the proposed RDH algorithm outperforms some state-of-the-art RDH algorithms in embedding capacity.},
  archive      = {J_IETIP},
  author       = {Zhenjun Tang and Mingyuan Pang and Chunqiang Yu and Guijin Fan and Xianquan Zhang},
  doi          = {10.1049/ipr2.12252},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2643-2655},
  shortjournal = {IET Image Process.},
  title        = {Reversible data hiding for encrypted image based on adaptive prediction error coding},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smart pansharpening approach using kernel-based image
filtering. <em>IETIP</em>, <em>15</em>(11), 2629–2642. (<a
href="https://doi.org/10.1049/ipr2.12251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image fusion plays important roles in numerous applications, including monitoring, metrology, and agriculture. Image fusion gathers essential information from several image sources and consolidates them into a single image called a fused image. The fused image involves relevant data, and it is more informative than any other images extracted from one source. This study proposed a pansharpening technique based on image filtering utilising a bilateral filter to generate high-frequency details from panchromatic image. The various types of side window guided filters are employed to enhance the multispectral band from panchromatic image and then used these filters to adjust spatial data misfortune that happens when images are combined. Experimental results demonstrated that the proposed method provides consistent results concise with reported by the previous research in terms of subjective and objective assessments on remote sensing data.},
  archive      = {J_IETIP},
  author       = {Ahmad AL Smadi and Shuyuan Yang and Atif Mehmood and Ahed Abugabah and Min Wang and Muzaffar Bashir},
  doi          = {10.1049/ipr2.12251},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2629-2642},
  shortjournal = {IET Image Process.},
  title        = {Smart pansharpening approach using kernel-based image filtering},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discriminative self-attention cycle GAN for face
super-resolution and recognition. <em>IETIP</em>, <em>15</em>(11),
2614–2628. (<a href="https://doi.org/10.1049/ipr2.12250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image captured via surveillance videos in an open environment is usually of low quality, which seriously affects the visual quality and recognition accuracy. Most image super-resolution methods adopt paired high-quality and its interpolated low-resolution version to train the super-resolution network. It is difficult to achieve contented visual quality and restoring discriminative features in real scenarios. A discriminative self-attention cycle generative adversarial network is proposed for real-world face image super-resolution. Based on the cycle GAN framework, unpaired samples are adopted to train a degradation network and a reconstruction network simultaneously. A self-attention mechanism is employed to capture the contextual information for details restoring. A Siamese face recognition network is introduced to provide a constraint on identify consistency. In addition, an asymmetric perceptual loss is introduced to handle the imbalance between the degradation model and the reconstruction model. Experimental results show that the observation model achieved more realistic low-quality face images, and the super-resolved face images have shown better subjective quality and higher face recognition performance.},
  archive      = {J_IETIP},
  author       = {Xiaoguang Li and Ning Dong and Jianglu Huang and Li Zhuo and Jiafeng Li},
  doi          = {10.1049/ipr2.12250},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2614-2628},
  shortjournal = {IET Image Process.},
  title        = {A discriminative self-attention cycle GAN for face super-resolution and recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-class COVID-19 segmentation network with pyramid
attention and edge loss in CT images. <em>IETIP</em>, <em>15</em>(11),
2604–2613. (<a href="https://doi.org/10.1049/ipr2.12249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the end of 2019, a novel coronavirus COVID-19 broke out. Due to its high contagiousness, more than 74 million people have been infected worldwide. Automatic segmentation of the COVID-19 lesion area in CT images is an effective auxiliary medical technology which can quantitatively diagnose and judge the severity of the disease. In this paper, a multi-class COVID-19 CT image segmentation network is proposed, which includes a pyramid attention module to extract multi-scale contextual attention information, and a residual convolution module to improve the discriminative ability of the network. A wavelet edge loss function is also proposed to extract edge features of the lesion area to improve the segmentation accuracy. For the experiment, a dataset of 4369 CT slices is constructed, including three symptoms: ground glass opacities, interstitial infiltrates, and lung consolidation. The dice similarity coefficients of three symptoms of the model achieve 0.7704, 0.7900, 0.8241 respectively. The performance of the proposed network on public dataset COVID-SemiSeg is also evaluated. The results demonstrate that this model outperforms other state-of-the-art methods and can be a powerful tool to assist in the diagnosis of positive infection cases, and promote the development of intelligent technology in the medical field.},
  archive      = {J_IETIP},
  author       = {Fuli Yu and Yu Zhu and Xiangxiang Qin and Ying Xin and Dawei Yang and Tao Xu},
  doi          = {10.1049/ipr2.12249},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2604-2613},
  shortjournal = {IET Image Process.},
  title        = {A multi-class COVID-19 segmentation network with pyramid attention and edge loss in CT images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Underwater image enhancement based on colour correction and
fusion. <em>IETIP</em>, <em>15</em>(11), 2591–2603. (<a
href="https://doi.org/10.1049/ipr2.12247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image processing has always been a very challenging problem. Under the influence of environmental factors, underwater images are prone to some problems, such as colour cast, low visibility, and few edge details. Here, an image enhancement algorithm is proposed to improve image degradation mainly caused by the absorption of light. First, colour compensation and white balance algorithm are used to restore the natural appearance of the image. Then the improved dark channel prior (DCP) is used to improve the visibility and avoid blocking artifacts which appear in traditional DCP. Unsharp masking (USM) is applied to enhance the texture features of the DCP image. Finally, wavelet fusion is used to fuse the DCP image and DCP+USM image. The fusion algorithm not only further improves the visibility and texture features, but also reduces the noise of DCP+USM. Compared with other methods, quantitative analysis results show that the enhanced images have higher visibility, more details and edge information.},
  archive      = {J_IETIP},
  author       = {Daqi Zhu and Zhiqiang Liu and Youmin Zhang},
  doi          = {10.1049/ipr2.12247},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2591-2603},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement based on colour correction and fusion},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical image steganographic algorithm via modified LSB
method and chaotic map. <em>IETIP</em>, <em>15</em>(11), 2580–2590. (<a
href="https://doi.org/10.1049/ipr2.12246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods of hiding information in an image are existing now. The least significant bit is the famous method used in steganographic algorithms. Medical image steganography is a technique used to make the transmission of these images secure so that the decision of the Specialist physician based on these images is not affected. In this paper, medical image steganographic algorithm using modified least significant bit and chaotic map is proposed. The main problem is that the selection of embedding pixels within the host image is not protected enough in most existing methods. So, the author used two-dimensional piecewise smooth chaotic map to select the positions of these pixels randomly. On the other hand, all bits in the secret medical image are transmitted without losing any bit. To do that, the secret medical image is encrypted using one-dimensional piecewise chaotic map (Tent map). Then, the steganographic algorithm is used to hide the bits of the encrypted secret medical image. The bits of each embedded pixel are shuffled before the embedding process randomly. After that, the stego image is created. The host image and stego image are analysed with the peak signal-to-noise ratio, the mean square error, histogram test, image quality measure and relative entropy test. The stego image displays acceptable result when comparing with the host image. Also, the chi-square attack test is performed and the stego image can resist it. The proposed algorithm can assist the sending of medical images via communication media.},
  archive      = {J_IETIP},
  author       = {A. A. Karawia},
  doi          = {10.1049/ipr2.12246},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2580-2590},
  shortjournal = {IET Image Process.},
  title        = {Medical image steganographic algorithm via modified LSB method and chaotic map},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thigh muscle segmentation using a hybrid FRFCM-based
multi-atlas method and morphology-based interpolation algorithm.
<em>IETIP</em>, <em>15</em>(11), 2572–2579. (<a
href="https://doi.org/10.1049/ipr2.12245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volume of lower extremity muscles is affected by some diseases. Quantification of thigh muscles in medical images can lead to an easier investigation of these diseases. Most of the previous works in thigh muscle segmentation are based on models and atlases that require manually segmented datasets in 3D. As manual segmentation of these muscles is a time-consuming task, in this work, only one initial slice is segmented by a new hybrid FRFCM-based multi-atlas method and other slices are segmented based on this slice. In the proposed method, after noise reduction, the muscle region is extracted from other tissues by the FRFCM method. Then, an initial slice of each dataset is segmented by a multi-atlas method. The segmented muscles in the initial slice are used to segment muscles in the other slices of each dataset. The proposed method was evaluated with 20 CT datasets. The average DSC, Precision, and Sensitivity of the method for individual muscle segmentation were , and , respectively. The quantitative and intuitive results of the proposed method show the better results of this method in comparison to other state-of-the-art thigh muscle segmentation techniques.},
  archive      = {J_IETIP},
  author       = {Malihe Molaie and Reza Aghaeizadeh Zoroofi},
  doi          = {10.1049/ipr2.12245},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2572-2579},
  shortjournal = {IET Image Process.},
  title        = {Thigh muscle segmentation using a hybrid FRFCM-based multi-atlas method and morphology-based interpolation algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blood vessel and background separation for retinal image
quality assessment. <em>IETIP</em>, <em>15</em>(11), 2559–2571. (<a
href="https://doi.org/10.1049/ipr2.12244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal image analysis has become an intuitive and standard aided diagnostic technique for eye diseases. The good image quality is essential support for doctors to provide timely and accurate disease diagnosis. This paper proposes an end-to-end learning based method for evaluating the retinal image quality. First, blood vessels of the input image are segmented by U-Net, and the fundus image is divided into two parts: blood vessels and background. Then, we design a dual branch network module which extracts global features that influence the image quality and suppress the interference of blood vessels and local textures to achieve better performance. The proposed module can be embedded in various advanced network structures. The experimental results show the more efficient convergence rate for the network with the module. The best network accuracy rate is 85.83%, the AUC is 0.9296, and the F1-score is 0.7967 on the collected local dataset. Additionally, the model generalization is tested on the public DRIMDB dataset. The accuracy, AUC, and F1-score reach 97.89%, 0.9978, and 0.9688, respectively. Compared with the state-of-the-art networks, the performance of the proposed method is proven to be accurate and effective for retinal image quality assessment.},
  archive      = {J_IETIP},
  author       = {Yi-Peng Liu and Yajun Lv and Zhanqing Li and Jing Li and Yan Liu and Peng Chen and Ronghua Liang},
  doi          = {10.1049/ipr2.12244},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2559-2571},
  shortjournal = {IET Image Process.},
  title        = {Blood vessel and background separation for retinal image quality assessment},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SeqFace: Learning discriminative features by using face
sequences. <em>IETIP</em>, <em>15</em>(11), 2548–2558. (<a
href="https://doi.org/10.1049/ipr2.12243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have greatly improved the Face Recognition (FR) performance in recent years. Almost all CNNs in FR are trained on the carefully labeled datasets containing plenty of identities. However, such high-quality datasets are very expensive to collect, which restricts many researchers to achieve state-of-the-art performance. In this paper, a framework, called SeqFace, for learning discriminative face features is proposed. Besides a traditional identity training dataset, the designed SeqFace can train CNNs by using an additional dataset which includes a large number of face sequences collected from videos. Moreover, the label smoothing regularization (LSR) and a new proposed discriminative sequence agent (DSA) loss are employed to enhance the discrimination power of deep face features via making full use of the sequence data. Only with a single ResNet model, the method achieves very competitive performance on several face recognition benchmarks, including LFW, YTF, CFP, AgeDB, and MegaFace. The code and model are publicly available at the website https://github.com/huangyangyu/SeqFace .},
  archive      = {J_IETIP},
  author       = {Wei Hu and Yangyu Huang and Fan Zhang and Ruirui Li and Hengchao Li},
  doi          = {10.1049/ipr2.12243},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2548-2558},
  shortjournal = {IET Image Process.},
  title        = {SeqFace: Learning discriminative features by using face sequences},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic image segmentation based on label propagation.
<em>IETIP</em>, <em>15</em>(11), 2532–2547. (<a
href="https://doi.org/10.1049/ipr2.12242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an automatic approach for the segmentation of coloured natural scene images based on graphs and the propagation of labels originally designed for communities detection in complex networks. Images are initially pre-segmented with super-pixels, followed by feature extraction using colour information of each super-pixels. The resulting graph consists of vertices which represent super-pixels, whereas the edge weights are a measure of similarity between super-pixels. The resulting segmentation corresponds to the propagation of labels among the vertices. In this article, three strategies for propagating labels have been formulated: (i) iterative propagation (ILP), (ii) recursive propagation (RLP) and (iii) a weighted recursive propagation (WRLP). The experiments have shown that the proposed methods, when compared to other state-of-the-art methods, produce better results in terms of segmentation quality and processing time.},
  archive      = {J_IETIP},
  author       = {Ivar Vargas Belizario and Oscar Cuadros Linares and João do Espirito Santo Batista Neto},
  doi          = {10.1049/ipr2.12242},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2532-2547},
  shortjournal = {IET Image Process.},
  title        = {Automatic image segmentation based on label propagation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Part-MOT: A multi-object tracking method with instance
part-based embedding. <em>IETIP</em>, <em>15</em>(11), 2521–2531. (<a
href="https://doi.org/10.1049/ipr2.12240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part-MOT, a one-stage anchor-free architecture which unifies the object identification representation and detection in one task for visual object tracking is presented. For object representation, a position relevant feature is obtained using the center-ness information, which takes advantage of the anchor-free ideal to encode the feature map as the instance-aware embedding. To adapt to the object&#39;s movement, the clustering-based method to get the global instance feature is introduced. This enables this approach more robust to make better tracking decisions. Part-MOT achieves the state-of-the-art performance on public datasets, with especially strong results for object deformation and movement changes.},
  archive      = {J_IETIP},
  author       = {Xiaohu Liu and Yichuang Luo and Keding Yan and Jianfei Chen and Zhiyong Lei},
  doi          = {10.1049/ipr2.12240},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2521-2531},
  shortjournal = {IET Image Process.},
  title        = {Part-MOT: A multi-object tracking method with instance part-based embedding},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remote sensing image super-resolution based on convolutional
blind denoising adaptive dense connection. <em>IETIP</em>,
<em>15</em>(11), 2508–2520. (<a
href="https://doi.org/10.1049/ipr2.12236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current super-resolution (SR) deep network is mainly applied to the common image and pays little attention to the image with noise. The remote sensing image contains much noise, so that the SR reconstruction effect is not satisfactory. Therefore, a convolution blind denoising adaptive dense connection SR (CBD-ADCSR) network for the remote sensing image is proposed in this paper. The whole model is divided into a convolution blind denoising (CBD) network for denoising and an ADCSR network for reconstruction. Firstly, the components of the network are given in detail and are analysed. Secondly, a data set making method is designed combining motion blur, defocusing blur and Gaussian noise, which is used to generate low-resolution image data sets with complex degradation for the model training. Finally, through the detailed comparative experiment, it is proved that the reconstruction effect of the CBD-ADCSR model is better than that of the most state-of-the-art algorithms in objective criteria. In addition, compared with the original ADCSR network, CBD-ADCSR has a stronger ability for noise suppression.},
  archive      = {J_IETIP},
  author       = {Xin Yang and Tangxin Xie and Yingqing Guo and Dake Zhou},
  doi          = {10.1049/ipr2.12236},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2508-2520},
  shortjournal = {IET Image Process.},
  title        = {Remote sensing image super-resolution based on convolutional blind denoising adaptive dense connection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and efficient lossless encoder in image compression
with low computation and low memory. <em>IETIP</em>, <em>15</em>(11),
2494–2507. (<a href="https://doi.org/10.1049/ipr2.12235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on frequency-based lossless new encoding technique with low computation and low memory scheme. This encoding technique doesn&#39;t require, any table similar to Huffman and Golomb Rice encoder, and doesn&#39;t take high computation time like an arithmetic encoder. For better comparative results, nearly 200 standard images were tested with the proposed encoder and their results were compared with the standard encoders. Based on the analysis of results reported in the experimental part, the proposed scheme in Discrete Cosine Transformation (DCT) achieves a high Compression Ratio (CR) of 97.68 %, Bits Per Pixel (BPP) of 0.19, Peak Signal To Noise Ratio (PSNR) of 27.62 dB for pepper image, CR of 98.65%, BPP of 0.11 and PSNR of 29.24 dB for house image, CR of 98.58 %, BPP of 0.11, PSNR of 30.59 dB for pepper image, CR of 98.37%, BPP of 0.13, PSNR of 31.37dB for house image in Discrete Wavelet Transformation (DWT) than compared to other encoders. It consumes less computation time to attain superior CR with a better quality of reconstructed picture than DCT based Huffman, Golomb- Rice (GR) coder and Low Complexity, Low memory Entropy Coder (LLEC), DWT based Arithmetic, GR code, LLEC.},
  archive      = {J_IETIP},
  author       = {N. Karthikeyan and N. M. Saravanakumar and M. Sivakumar},
  doi          = {10.1049/ipr2.12235},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2494-2507},
  shortjournal = {IET Image Process.},
  title        = {Fast and efficient lossless encoder in image compression with low computation and low memory},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep forgery discriminator via image degradation analysis.
<em>IETIP</em>, <em>15</em>(11), 2478–2493. (<a
href="https://doi.org/10.1049/ipr2.12234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network-based deep generative model is widely applied in creating hyper-realistic face-swapping images and videos. However, its malicious use has posed a great threat to online contents, thus making detecting the authenticity of images and videos a tricky task. Most of the existing detection methods are only suitable for one type of forgery and only work for low-quality tampered images, restricting their applications. This paper concerns the construction of a novel discriminator with better comprehensive capabilities. Through analysis of the visual characteristics of manipulated images from the perspective of image quality, it is revealed that the synthesized face does have different degrees of quality degradation compared to the source content. Therefore, several kinds of image quality-related handicraft features are extracted, including texture, sharpness, frequency domain features, and deep features, to unveil the inconsistent information and modification traces in the fake faces. In this way, a 1065-dimensional vector of each image is obtained through multi-feature fusion, and it is then fed into RF to train a targeted binary classification detector. Extensive experiments have shown that the proposed scheme is superior to the previous methods in recognition accuracy on multiple manipulation databases including the Celeb-DF database with better visual quality.},
  archive      = {J_IETIP},
  author       = {Miaomiao Yu and Jun Zhang and Shuohao Li and Jun Lei and Fenglei Wang and Hao Zhou},
  doi          = {10.1049/ipr2.12234},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2478-2493},
  shortjournal = {IET Image Process.},
  title        = {Deep forgery discriminator via image degradation analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image defogging based on amended dark channel prior and
4-directional l1 regularisation. <em>IETIP</em>, <em>15</em>(11),
2454–2477. (<a href="https://doi.org/10.1049/ipr2.12233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dark channel prior (DCP) algorithm has been widely used in the field of image defogging because of its simple theory and clear restoration result. However, the DCP algorithm has significant limitations. This study clarifies the relationship between halo artfacts and the size of the dark channel patch of the DCP algorithm and analyses the reason why the colour of close-range white objects appears distorted in the restored images. An amended DCP method is then proposed to solve these problems, utilising a locally variable weighted 4-directional L 1 regularisation and a corresponding parallel algorithm to optimise the transmission. A deep neural network, 4DL 1 R-net, is then trained to further enhance the processing speed. Extensive experiments demonstrate that this method is effective. The proposed method can obtain clear details, maintain the natural clarity of images, and achieve significant improvements over state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yuliang Yang and Wei Long and Yanyan Li and Xiaoqiu Shi and Lin Gao},
  doi          = {10.1049/ipr2.12233},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2454-2477},
  shortjournal = {IET Image Process.},
  title        = {Image defogging based on amended dark channel prior and 4-directional l1 regularisation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic detection of safety helmet wearing based on head
region location. <em>IETIP</em>, <em>15</em>(11), 2441–2453. (<a
href="https://doi.org/10.1049/ipr2.12231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of difficult and low precision in the detection of safety helmet wearing in the complex pose of construction worker, a detection method of safety helmet wearing based on pose estimation is proposed. In the pose estimation model of OpenPose, the residual network optimized feature extraction is introduced to obtain the skeletal point information of the construction worker, and then the pose of the construction worker is estimated based on the skeletal point information, three-point localization method is proposed for the front and back pose, and skin colour detection method is proposed for the side pose, and then to determine the head region. The YOLO v4 is used to detect the safety helmet region, and then the construction worker&#39;s safety helmet wearing is judged according to whether the head region intersects the safety helmet region or not. Experimental results show that the detection accuracy of the method is higher than other methods, and the adaptability to the environment is stronger.},
  archive      = {J_IETIP},
  author       = {Yuwan Gu and Yusheng Wang and Lin Shi and Ning Li and Lihua Zhuang and Shoukun Xu},
  doi          = {10.1049/ipr2.12231},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2441-2453},
  shortjournal = {IET Image Process.},
  title        = {Automatic detection of safety helmet wearing based on head region location},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weight-based colour constancy using contrast stretching.
<em>IETIP</em>, <em>15</em>(11), 2424–2440. (<a
href="https://doi.org/10.1049/ipr2.12229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main issues in colour image processing is changing objects&#39; colour due to colour of illumination source. Colour constancy methods tend to modify overall image colour as if it was captured under natural light illumination. Without colour constancy, the colour would be an unreliable cue to object identity. Till now, many methods in colour constancy domain are presented. They are in two categories; statistical methods and learning-based methods. This paper presents a new statistical weighted algorithm for illuminant estimation. Weights are adjusted to highlight two key factors in the image for illuminant estimation, that is contrast and brightness. The focus was on the convex part of the contrast stretching function to create the weights. Moreover, a novel partitioning mechanism in the colour domain that leads to improvement in efficiency is proposed. The proposed algorithm is evaluated on two benchmark linear image databases according to two evaluation metrics. The experimental results showed that it is competitive to the statistical state of the art methods. In addition to its low computational cost, it has the advantage of improving the efficiency of statistics-based algorithms for dark images and images with low brightness contrast. Moreover, it is robust to camera change types.},
  archive      = {J_IETIP},
  author       = {Zeinab Abedini and Mansour Jamzad},
  doi          = {10.1049/ipr2.12229},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2424-2440},
  shortjournal = {IET Image Process.},
  title        = {Weight-based colour constancy using contrast stretching},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised many-to-many image-to-image translation across
multiple domains. <em>IETIP</em>, <em>15</em>(11), 2412–2423. (<a
href="https://doi.org/10.1049/ipr2.12227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-domain image-to-image translation aims to synthesize images among multiple domains without labelled data, which is more general and complicated than one-to-one image mapping. However, existing methods mainly focus on reducing the large costs of modelling and do not pay enough attention to the quality of generated images. In some target domains, their translation results may not be expected or even cause the model collapse. To improve the image quality, an effective many-to-many mapping framework for unsupervised multi-domain image-to-image translation is proposed. There are two key aspects to the proposed method. The first is a many-to-many architecture with only one domain-shared encoder and several domain-specialized decoders to effectively and simultaneously translate images across multiple domains. The second is two proposed constraints extended from one-to-one mappings to further help improve the generation. All the evaluations demonstrate that the proposed framework is superior to existing methods and provides an effective solution for multi-domain image-to-image translation.},
  archive      = {J_IETIP},
  author       = {Ye Lin and Keren Fu and Shenggui Ling and Peng Cheng},
  doi          = {10.1049/ipr2.12227},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2412-2423},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised many-to-many image-to-image translation across multiple domains},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pedestrian re-identification based on attribute mining and
reasoning. <em>IETIP</em>, <em>15</em>(11), 2399–2411. (<a
href="https://doi.org/10.1049/ipr2.12225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-level semantic information extracted from the pedestrian attribute feature is an important element for pedestrian recognition. Pedestrian attribute recognition plays an important role in both intelligent video surveillance and pedestrian re-identification promoting the convenience of searching and performance of model. This paper tries finding a practical method to improve the performance of the pedestrian re-identification by combining pedestrian attributes and identities. The multi-task learning method combines pedestrian recognition and attribute information in a direct way that considers the correlation between pedestrian attributes and identities but ignores the principle and degree of such correlation. To solve this problem, a new pedestrian recognition framework based on attribute mining and reasoning is proposed in this paper. To enhance the expression ability of attribute features, it designs spatial channel attention module (SCAM) based on attention mechanism to extract features from every attribute. SCAM can not only locate the attributes on the feature map, but also effectively mine channel features with a higher degree of association with attributes. In addition, both spatial attention model and channel attention model are integrated by multiple groups of parallel branches, which further improve the network performance. Finally, using the semantic reasoning and information transmission function of graph convolutional network, the relationship between attribute features and pedestrian features can be mined. Besides, pedestrian features with stronger expression ability can also be obtained. Experiment work is conducted in two databases, DukeMTMC-reID and Market-1501, which are commonly used in pedestrian recognition tasks. On the Market-1501 dataset, the final effect of the algorithm model CMC-1 can reach 94.74%, and mAP can reach 87.02%; on the DukeMTMC-reID dataset, CMC-1 can reach 87.03%, and mAP can reach 77.11%. The results show that our method is at the top of the existing pedestrian recognition methods.},
  archive      = {J_IETIP},
  author       = {Chao Li and Xiaoyu Yang and Kangning Yin and Yifan Chang and Zhiguo Wang and Guangqiang Yin},
  doi          = {10.1049/ipr2.12225},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2399-2411},
  shortjournal = {IET Image Process.},
  title        = {Pedestrian re-identification based on attribute mining and reasoning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shadow detection from images using fuzzy logic and
PCPerturNet. <em>IETIP</em>, <em>15</em>(10), 2384–2397. (<a
href="https://doi.org/10.1049/ipr2.12221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection is a challenging and essential task for interpreting the scene. Regardless of encouraging findings from current Deep Learning (DL) approaches used for shadow detection, the methods are also dealing with inconsistent situations where the visual representation of non-shadow and shadow regions is equivalent. In this article, a DL based approach is introduced for image pixel-level shadow detection. The proposed CNN-based approach, pattern conserver convolutional neural network (PCPerturNet) profits from a new design where shadow features are defined utilizing an effective skip-connection mapping arrangement. To make PCPerturNet robust from the change in brightness and contrast, several perturbed instances are generated by using a fuzzy-logic based method to train the system. Also, five types of augmentations are applied to images during training to make the system robust from the change in scale, orientation and flip. PCPerturNet derives and conserves shadow patterns in manifold layers and uses those layers progressively in several units to produce the shadow mask. The output of the proposed method is tested on two freely accessible databases and one self-created database where the accuracy rate obtained is 96.4%, 96.8%, and 89.4% which indicates that the proposed method outperforms the other shadow detection approaches used in the literature.},
  archive      = {J_IETIP},
  author       = {Jyotismita Chaki},
  doi          = {10.1049/ipr2.12221},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2384-2397},
  shortjournal = {IET Image Process.},
  title        = {Shadow detection from images using fuzzy logic and PCPerturNet},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph wavelet transform for image texture classification.
<em>IETIP</em>, <em>15</em>(10), 2372–2383. (<a
href="https://doi.org/10.1049/ipr2.12220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph is a data structure that can represent complex relationships among data. Graph signal processing, unlike traditional signal processing, explicitly considers the structure and relationship among the signal samples. Graph wavelet transform can provide a multiscale analysis for the graph signal. It is well known that texture is a region property in an image, which is characterized with the intensity and relationship among pixels. In this context of the graph signal processing framework, an image texture can be considered as the signal on the graph. Therefore, a texture classification method based on graph wavelet transform is proposed. Specifically, image textures are decomposed into multiscale components by using two-channel graph wavelet filter banks. Then the local singular value decomposition is applied to each subband. In order to improve the noise-resistant ability, the maximum, mean and median values of the local singular values of graph-wavelet transformation coefficients are extracted. Finally, the Weibull distributions are used to model those extracted values to describe the image textures. The experiments on the benchmark texture datasets are conducted to demonstrate the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Yu-Long Qiao and Yue Zhao and Chun-Yan Song and Kai-Ge Zhang and Xue-Zhi Xiang},
  doi          = {10.1049/ipr2.12220},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2372-2383},
  shortjournal = {IET Image Process.},
  title        = {Graph wavelet transform for image texture classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain tumour cell segmentation and detection using deep
learning networks. <em>IETIP</em>, <em>15</em>(10), 2363–2371. (<a
href="https://doi.org/10.1049/ipr2.12219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical science is a challenging area for various problems associated with health care and there always exists scope for continuous medical research. The major challenges in medical imaging are in the region of lesion, segmentation and classification of tumours in the brain. Several technical challenge exists in the classification due to the variation in the tumour size, shape, texture information and location. There is a need for automatic identification of high-grade glioma (HGG) and lower-grade glioma (LGG). The management and grade of brain tumour depend on the depth of the tumour. Due to its irregular features, manual segmentation involves longer time and also increases the misclassification rate. Inspired by these issues, this paper introduces two automatic deep learning networks called U-Net-based deep convolution network and U-Net with dense network. The proposed method is evaluated in our own brain tumour image database consisting of 300 high-grade brain tumour cases and 200 normal cases. To improve the overall efficiency of the network, data augmentation is applied in both training and validation. The proposed U-Net-based Dense Convolutional Network (DenseNet) architecture is compared with the performance of U-Net architecture and concluded that the proposed DenseNet produces a higher dice value. The validation results have revealed that our proposed method can have better segmentation efficiency. Also, the performance of the proposed DenseNet achieved better results compared with the state-of-the-art algorithms. Validation of the test images proves that segmented output classification of tumour risk and the normal region produces a sensitivity of 88.7%, Jaccard index of 0.839, dice score value of 0.911, F1 score of 0.906 and specificity of 100% using U-Net-based DenseNet architecture.},
  archive      = {J_IETIP},
  author       = {Sanjeevirayar Bagyaraj and Rajendran Tamilselvi and Parisa Beham Mohamed Gani and Devanathan Sabarinathan},
  doi          = {10.1049/ipr2.12219},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2363-2371},
  shortjournal = {IET Image Process.},
  title        = {Brain tumour cell segmentation and detection using deep learning networks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Alzheimer’s disease diagnosis based on the visual attention
model and equal-distance ring shape context features. <em>IETIP</em>,
<em>15</em>(10), 2351–2362. (<a
href="https://doi.org/10.1049/ipr2.12218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is an irreversible neurodegenerative disease caused by rapid degeneration of brain cells. More and more researchers focus on effective and accurate methods for the diagnosis of AD. In this paper, a method to identify AD by extracting equal-distant ring shape context features from saliency map of structural magnetic resonance imaging (sMRI) is proposed. The experimental results on the thin-layer MR images of the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) dataset showed that our method helped improve the performance of identifying brain diseases. Specifically, the classification accuracy of 94.83% for AD versus CN, 98.31% for AD versus MCI and 85.77% for MCI versus CN, respectively. At the same time, experiments on Open Access Series of Imaging Studies dataset and clinically collected thick-layer MR images verify the classification performance of the method. The results show that this method may have higher application value in clinical application, with classification accuracies of 96.56% and 98.18% for AD versus CN, respectively. Compared with the methods based on gray matter (GM) density, cortical thickness and hippocampal volume, our method achieved higher accuracy of AD (or MCI) and CN classification.},
  archive      = {J_IETIP},
  author       = {Huan Lao and Xuejun Zhang and Yanyan Tang and Chan Liang},
  doi          = {10.1049/ipr2.12218},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2351-2362},
  shortjournal = {IET Image Process.},
  title        = {Alzheimer&#39;s disease diagnosis based on the visual attention model and equal-distance ring shape context features},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blocking artefacts reduction based on a ripple matrix
permutation image of high-frequency images in the wavelet domain.
<em>IETIP</em>, <em>15</em>(10), 2342–2350. (<a
href="https://doi.org/10.1049/ipr2.12217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block compressed sensing (BCS) approaches based on matrix permutations effectively reduce blocking artefacts in the high-quality reconstruction of images. To further reduce the blocking artefacts, the paper proposes a novel method for their processing in the wavelet domain based on the ripple matrix permutation (RMP) BCS approach. The method makes use of the energy distribution characteristics of images in the wavelet domain. The low-frequency images contain the basic information. The high-frequency images contain the edge textures information of the image, and these images are extremely sparse. Then, the proposed method performs matrix permutations only on the high-frequency images. This avoids the obvious energy differences among the blocks. The method can better balance the textures among blocks; in turn, the blocking artefacts are reduced. The approach involves performing a wavelet decomposition on the image. Then, the transformed high-frequency image is subjected to a RMP to achieve textures balancing. Finally, compressed sensing processing is performed on the permuted high-frequency image. As a result, the balancing effect becomes more significant, and the low-frequency part of the image remains unchanged, the differences among the blocks are reduced. Simulation results demonstrate that the high-frequency part of the image wavelet domain is texture balanced. When the image is reconstructed after the compressed sensing step, the image quality is significantly improved.},
  archive      = {J_IETIP},
  author       = {Xiuli Du and Jinting Liu and Wei Zhang and Ya&#39;na Lv},
  doi          = {10.1049/ipr2.12217},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2342-2350},
  shortjournal = {IET Image Process.},
  title        = {Blocking artefacts reduction based on a ripple matrix permutation image of high-frequency images in the wavelet domain},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning-free handwritten word spotting method for
historical handwritten documents. <em>IETIP</em>, <em>15</em>(10),
2332–2341. (<a href="https://doi.org/10.1049/ipr2.12216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word spotting on degraded and noisy historical documents can become a challenging task considering the computational time and memory usage required to scan the entire document image. This paper proposes a new effective technique for multi-language word spotting using a two different feature extraction techniques, Histogram of Oriented Gradients (HOG) and Speeded Up Robust Features (SURF) features. First, regions of interest (ROIs) are extracted using a cross-correlation measure, and the extracted ROIs are re-ranked using feature extraction and matching methods. The algorithm handles two types of scenarios: Segmentation-based and segmentation-free. It also facilitates the search for words that occur once as well as multiple times in the image. Evaluations were conducted on the George Washington and HADARA datasets using a standard evaluation method. The proposed methodology shows improved performance over contemporary technologies currently being used in the word spotting research field.},
  archive      = {J_IETIP},
  author       = {Hanadi Hassen Mohammed and Nandhini Subramanian and Somaya Al-Madeed},
  doi          = {10.1049/ipr2.12216},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2332-2341},
  shortjournal = {IET Image Process.},
  title        = {Learning-free handwritten word spotting method for historical handwritten documents},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient way to use MS-CLEAN associated with shannon’s
entropy. <em>IETIP</em>, <em>15</em>(10), 2319–2331. (<a
href="https://doi.org/10.1049/ipr2.12215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article covers deconvolution methods in the context of radio astronomical images. A new formulation is proposed to deal with negative brightness, deconvoluting separately the positive and negative brightness of the sky. The positive brightness is physically possible, but negative brightness is a degradation product. At the same time, the paper presents Shannon&#39;s entropy&#39;s behaviour in the context of the Multi-Scale CLEAN (MS-CLEAN) algorithm, defining the measured brightness as information in the scope of Shannon&#39;s entropy. The knowledge acquired is used in an example of information monitoring at scales, which automatically reduces the search space of MS-CLEAN, and reduces the computational cost. The proposed algorithm, called Relevant Component Multi-Scale CLEAN (RC-CLEAN), can be up to 4 times faster than the classic MS-CLEAN without prejudice to the identification of structures and noise reduction. Here, Structural Similarity Index ( SSIM ) and Peak Signal to Noise Ratio ( PSNR ) used to quantify the results, respectively, showed the same quality for the SSIM and gains of up to 11 dB for the PSNR . RC-CLEAN also shows a result similar to that obtained by the standard software of large astronomical laboratories using real data.},
  archive      = {J_IETIP},
  author       = {Ramon G. Campos and Evandro O. T. Salles},
  doi          = {10.1049/ipr2.12215},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2319-2331},
  shortjournal = {IET Image Process.},
  title        = {An efficient way to use MS-CLEAN associated with shannon&#39;s entropy},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale ensemble of convolutional neural networks for
skin lesion classification. <em>IETIP</em>, <em>15</em>(10), 2309–2318.
(<a href="https://doi.org/10.1049/ipr2.12214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection and treatment of skin cancer can considerably reduce the patient mortality rates. Convolutional neural network (CNN) has been widely applied in the field of computer aided diagnosis. However, for skin lesions, the inconsistent size of lesion regions in dermatoscope images hinders the convolutional neural network precise discrimination. To solve this problem, multiscale ensemble of convolutional neural networks called MECNN is proposed, which involves three branches with different lesion scales as the model input. The first branch locates the lesion region outline by identifying the largest local response point. Then, MECNN reduces the search area of the lesion region and divides the outline into two scales used as the input for the other two branches. A global loss function is defined to control the learning objectives of the three branches and MECNN fuses the branches output as the final classification result. The proposed model is evaluated on the public HAM10000 dataset and achieves a higher classification accuracy than the comparative state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yi-Peng Liu and Ziming Wang and Zhanqing Li and Jing Li and Ting Li and Peng Chen and Ronghua Liang},
  doi          = {10.1049/ipr2.12214},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2309-2318},
  shortjournal = {IET Image Process.},
  title        = {Multiscale ensemble of convolutional neural networks for skin lesion classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient target detection in hyperspectral image based on
visual attention. <em>IETIP</em>, <em>15</em>(10), 2301–2308. (<a
href="https://doi.org/10.1049/ipr2.12197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient target detection in hyperspectral image is a significant task in image segmentation, target tracking, image classification and so on. Many existing saliency detection algorithms for hyperspectral image detection cannot present the boundary of the salient target well and the description of the target is not enough. A method based on visual attention to detect the salient target of hyperspectral image is proposed in this paper. In this method, frequency-tuned (FT) salient detection model is combined with spectral salient to detect target in hyperspectral image. FT model is used to get target with clear border, and spectral information is made full use of to improve the accuracy of target detection. Firstly, FT is used to detect saliency of hyperspectral image and the saliency map is generated. Then, spectral information of the hyperspectral image is measured by similarity, and the spectral saliency is obtained by calculating spectral angle distance between the spectral vectors. Finally, the FT&#39;s saliency map and the spectral saliency map are combined to form the final saliency target maps. Experimental results show that our method is superior to other methods in saliency target detection of hyperspectral image, and the precision-recall curve and F-measure are better as well.},
  archive      = {J_IETIP},
  author       = {Minghua Zhao and Liqin Yue and Jing Hu and Shuangli Du and Peng Li and Li Wang},
  doi          = {10.1049/ipr2.12197},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2301-2308},
  shortjournal = {IET Image Process.},
  title        = {Salient target detection in hyperspectral image based on visual attention},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Occluded pedestrian detection combined with semantic
features. <em>IETIP</em>, <em>15</em>(10), 2292–2300. (<a
href="https://doi.org/10.1049/ipr2.12196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of pedestrian detection is to identify the location and size of pedestrians in images or videos. However, occlusions are very common in real-life scenarios, which make pedestrian detection more difficult. In order to solve the occlusion problem in pedestrian detection, a semantic feature enhancement module to acquire more informative and richer semantic features is proposed. The detector enhances semantic features by fusing feature maps of different layers, and detects pedestrians based on their locations and scales. The Experiments performed on Caltech and CityPersons datasets show that the algorithm achieves superior performance for detecting occluded pedestrians, especially heavily occluded ones. 30.6% and 47.9% log-average missing rates are achieved in the heavily occluded subsets of Caltech and Cityperons, respectively. Moreover, this method is robust to the detection of heavily occluded pedestrians, and the module can be easily used by other detection frameworks.},
  archive      = {J_IETIP},
  author       = {Binjie Ruan and Chongyang Zhang},
  doi          = {10.1049/ipr2.12196},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2292-2300},
  shortjournal = {IET Image Process.},
  title        = {Occluded pedestrian detection combined with semantic features},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Jointly human semantic parsing and attribute recognition
with feature pyramid structure in EfficientNets. <em>IETIP</em>,
<em>15</em>(10), 2281–2291. (<a
href="https://doi.org/10.1049/ipr2.12195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian attributes recognition is an important issue in computer vision and has a special role in the field of video surveillance. The previous methods presented to solve this issue are mainly based on multi-label end-to-end deep neural networks. These methods neglect to apply attributes for defining local feature areas and they suffer from the problems of the bounding box presence. A new framework for jointly human semantic parsing and pedestrian attribute recognition to achieve effective attribute recognition is proposed. By extracting human parts via semantic parsing, both semantic and spatial information can be explored with eliminating of background. The framework also uses multi-scale features to employ rich details and contextual information through proposed attribute recognition-bidirectional feature pyramid network. For baseline network that has a significant impact on the performance, EfficientNet-B3 is selected as a baseline network from The EfficientNet family which provides an appropriate trade-off between the three factors of CNNs scaling (depth/width/resolution). Finally, the proposed framework is tested on datasets PETA, RAP and PA-100k. Experimental results show that our method has superior performance in both mean accuracy and instance-based metrics compared to state-of-the-art results.},
  archive      = {J_IETIP},
  author       = {Mahnaz Moghaddam and Mostafa Charmi and Hossein Hassanpoor},
  doi          = {10.1049/ipr2.12195},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2281-2291},
  shortjournal = {IET Image Process.},
  title        = {Jointly human semantic parsing and attribute recognition with feature pyramid structure in EfficientNets},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diagnosing skin cancer via c-means segmentation with
enhanced fuzzy optimization. <em>IETIP</em>, <em>15</em>(10), 2266–2280.
(<a href="https://doi.org/10.1049/ipr2.12194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of cancer decreases the death rate as well, but mostly the disorder symptoms are unpredictable. Numerous skin cancer detection techniques are available in the dice, yet the effectiveness remains unachieved. This paper aims to introduce a skin cancer detection technique that characterizes the nature of cancer: normal, benign or malignant. The proposed technique includes three stages like Segmentation, Feature Extraction, and Classification. Here, the Fuzzy C-means Clustering (FCM) is used to segment the given input image. Then, the features are mined from the segmented image using Local Vector Pattern (LVP) and Local Binary Pattern (LBP). Subsequently, the Fuzzy classifier is used to do the classification process that gets the extracted features (LVP+LBP) as the input. The classifier outputs the nature of the image. As the primary contribution of this work, the limits of membership functions in the Fuzzy classifier are optimally selected by a new improved Rider Optimization Algorithm (ROA) termed as Distance Oriented ROA (DOROA). The performance of the proposed DOROA model is compared over other conventional models in terms of accuracy, sensitivity, specificity, precision, Negative Predictive Value (NPV), F1-score and Matthews correlation coefficient (MCC), False positive rate (FPR), False Negative Rate (FNR), and False Discovery Rate (FDR).},
  archive      = {J_IETIP},
  author       = {Nagayalanka Durgarao and Ghanta Sudhavani},
  doi          = {10.1049/ipr2.12194},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2266-2280},
  shortjournal = {IET Image Process.},
  title        = {Diagnosing skin cancer via C-means segmentation with enhanced fuzzy optimization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-sensitivity synchronous image encryption based on
improved one-dimensional compound sine map. <em>IETIP</em>,
<em>15</em>(10), 2247–2265. (<a
href="https://doi.org/10.1049/ipr2.12193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An improved one-dimensional compound sine map is introduced. The evaluation of this chaotic system shows that it has high sensitivity and random chaotic behaviour. Based on this chaotic system, a new fast image encryption scheme is proposed. Through the cross-processing of multiple chaotic sequences, two high-sensitivity pseudo-random sequences are generated, and the generated high-sensitivity random sequence is used for synchronization cross-processing in four directions. Using the symmetry of the image, the image is divided into four directions, and the processing of each direction is different based on the high-sensitivity sequence and the chaotic value. Through one traversal, the entire image is processed four times differently. The encryption process is uncontrollable and invisible depending on the chaotic sequence. Simulation test results show that the algorithm has good encryption results, is sensitive to the initial secret key and provides satisfactory security capabilities compared with other algorithms.},
  archive      = {J_IETIP},
  author       = {Xingyuan Wang and Maozhen Zhang},
  doi          = {10.1049/ipr2.12193},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2247-2265},
  shortjournal = {IET Image Process.},
  title        = {High-sensitivity synchronous image encryption based on improved one-dimensional compound sine map},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral clustering based on high-frequency texture
components for face datasets. <em>IETIP</em>, <em>15</em>(10),
2240–2246. (<a href="https://doi.org/10.1049/ipr2.12191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is one of the most widely used technologies for clustering tasks, which represents data as a weighted graph, and aims to find an appropriate way to cut the graph apart in order to categorize the raw data. The pivotal step of spectral clustering is to find out the accurate information to estimate the relationship of pairwise data, based on which a graph can be constructed. According to the cognition that different faces are distinguished by the edge contour which can be represented by high-frequency texture components, a novel spectral clustering algorithm via high-frequency signal of human face, named high-frequency spectral clustering (HFSC) is proposed. In HFSC, first the local high-frequency texture components are extracted from samples. Then the relationship of pairwise samples can be estimated with the degree of correlation, which is produced from the image high-frequency information. The graph will be set up with the correlation information. Subsequently the graph cut will be implemented to achieve the final clustering results. Experimental results show that this algorithm outperforms the state-of-the-art clustering methods on several datasets.},
  archive      = {J_IETIP},
  author       = {Zexiao Liang and Shaozhi Guo and Dakang Liu and Jianzhong Li},
  doi          = {10.1049/ipr2.12191},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2240-2246},
  shortjournal = {IET Image Process.},
  title        = {Spectral clustering based on high-frequency texture components for face datasets},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Looking around in the neighbourhood: Location estimation of
outdoor urban images. <em>IETIP</em>, <em>15</em>(10), 2227–2239. (<a
href="https://doi.org/10.1049/ipr2.12190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual geolocalisation has remained as a challenge in the research community: Given a query image, and a geo-tagged reference database, the goal is to derive a location estimate for the query image. We propose an approach to tackling the geolocalisation problem in a four-step manner. Essentially, our approach focuses on re-ranking the candidate images after image retrieval, by considering the visual similarity of the candidate and its neighbouring images, to the query image. By introducing the neighbouring images, the visual information of a candidate location has been enriched. The evaluation has been conducted on three street view datasets, where our approach outperforms three baseline approaches, in terms of location estimation accuracy on two datasets. We provide discussions related to, firstly, whether using deep features for image retrieval helps improve location estimation accuracy, and the effectiveness of geographical neighbourhoods; secondly, using different deep architectures for feature extraction, and its impact on estimation accuracy; thirdly, investigating if our approach consistently outperforms the classic 1-NN approach, on two datasets with significant difference in visual elements.},
  archive      = {J_IETIP},
  author       = {Jie Huang and Haozhi Huang},
  doi          = {10.1049/ipr2.12190},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2227-2239},
  shortjournal = {IET Image Process.},
  title        = {Looking around in the neighbourhood: Location estimation of outdoor urban images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A motion parameters estimating method based on deep learning
for visual blurred object tracking. <em>IETIP</em>, <em>15</em>(10),
2213–2226. (<a href="https://doi.org/10.1049/ipr2.12189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking the specific object in the blurred scenes is one of the challenging problems in computer vision and image processing. The accuracy and performance of trackers within the blur frames usually demonstrate a severe decrease. Accordingly, this problem needs to be corrected for better tracking results. Thus, this study seeks to present the best solution. To this end, a novel deep learning approach is proposed for object tracking in the presence of motion blur and fast motion. The hidden information in the blurring kernel is useful for tracking a specific blurred object through a series of consecutive frames. In this study, this matter is evaluated from a new perspective to solve the problems of blurred object tracking and objects with highly fast motions using a convolutional neural network (CNN) and a particle filter. Therefore, the proposed framework has two phases. First, the kernel leading to blurring is estimated by CNN, and then by a particle filter and the probability distribution of motion information that has been achieved by the kernel estimation the object is tracked. The results demonstrate that the suggested method can enhance the accuracy of tracking compared with the state-of-the-art, especially when the amount of blur is high.},
  archive      = {J_IETIP},
  author       = {Iman Iraei and Karim Faez},
  doi          = {10.1049/ipr2.12189},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2213-2226},
  shortjournal = {IET Image Process.},
  title        = {A motion parameters estimating method based on deep learning for visual blurred object tracking},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rotational invariant fractional derivative filters for lung
tissue classification. <em>IETIP</em>, <em>15</em>(10), 2202–2212. (<a
href="https://doi.org/10.1049/ipr2.12188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new and powerful rotation invariant fractional derivative convolution neural network model is proposed for the classification of five categories of interstitial lung diseases. Fractional derivative convolution neural network model employs fractional derivative filters for the texture enhancement of the lung tissue patches instead of the raw image, is given as input directly to the convolution neural network. These FD filters are rotation invariant which solves the problem of rotation invariance of lung tissue patterns caused by pose variations of the patient during CT scanning. Also, the problem of the poor performance of most classifiers such as, support vector machine and K-nearest neighbours caused by an imbalanced dataset is solved, by oversampling the minority categories emphysema and ground glass patches, and under-sampling the majority category, micronodules patches. The experimental results are executed on the publicly available interstitial lung disease database which shows the fractional derivative convolution neural network model performs better than the state-of-art with average F-score and accuracy noted as 93.32% and 93.33% respectively.},
  archive      = {J_IETIP},
  author       = {V. N. Sukanya Doddavarapu and Giri Babu Kande and B. Prabhakar Rao},
  doi          = {10.1049/ipr2.12188},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2202-2212},
  shortjournal = {IET Image Process.},
  title        = {Rotational invariant fractional derivative filters for lung tissue classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving crowd counting with scale-aware convolutional
neural network. <em>IETIP</em>, <em>15</em>(10), 2192–2201. (<a
href="https://doi.org/10.1049/ipr2.12187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale variations may cause a serious problem in crowd counting. In recent years, most methods for this problem use convolutional neural networks with a fixed scale for encoding and decoding image features. The scale of the convolutional layer is usually manually adjusted and may have to deal with image features on unfitted scales. In this paper, a method called scale-aware convolutional neural network(SCNet) is proposed, which adds a scale selection mechanism to the dilated convolutional operation. Shared weight multi-branch is used to deal with features on different scales, and an attention mechanism is introduced to determine the weights of the branches that fit the scale. Experimental results demonstrate that the proposed SCNet outperforms most existing methods.},
  archive      = {J_IETIP},
  author       = {Qingge Ji and Hang Chen and Di Bao},
  doi          = {10.1049/ipr2.12187},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2192-2201},
  shortjournal = {IET Image Process.},
  title        = {Improving crowd counting with scale-aware convolutional neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pre-processing technique to decrease inspection time in
glass tube production lines. <em>IETIP</em>, <em>15</em>(10), 2179–2191.
(<a href="https://doi.org/10.1049/ipr2.12186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In case of glass tube for pharmaceutical applications, high-quality defect detection is made via inspection systems based on computer vision. The processing must guarantee real-time inspection and meet increasing rate and quality requirements. Defect detection in glass tubes is complicated by aspects that hamper the efficiency of state-of-the-art techniques. This paper presents a pre-processing algorithm which excludes portions of the image where defects are surely absent. The approach decreases the time for defect detection and classification phases (any detection algorithm can be applied), as they are applied only in high-probability candidate sub-image. We derive a methodology to get robust values of algorithm&#39;s parameters during production. The algorithm relies on detrended standard deviation and double threshold hysteresis, which solve issues related to the misalignment between illuminator and acquisition camera, and enable a robust detection despite rotation, vibration, and irregularities of tubes. We consider Canny, MAGDDA, and Niblack algorithms. The solution keeps the detection quality of such algorithms and reaches a 4.69× throughput gain. It represents a methodology to obtain defect detection in time-constrained environments through a software-only approach, and can be exploited in parallel/accelerated solutions and in contexts where a linear camera is utilized on both flat and uneven surfaces.},
  archive      = {J_IETIP},
  author       = {Gabriele Antonio De Vitis and Pierfrancesco Foglia and Cosimo Antonio Prete},
  doi          = {10.1049/ipr2.12186},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2179-2191},
  shortjournal = {IET Image Process.},
  title        = {A pre-processing technique to decrease inspection time in glass tube production lines},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improve conditional adversarial domain adaptation using
self-training. <em>IETIP</em>, <em>15</em>(10), 2169–2178. (<a
href="https://doi.org/10.1049/ipr2.12184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation for image classification is one of the most fundamental transfer learning tasks and a promising solution to overcome the annotation burden. Existing deep adversarial domain adaptation approaches imply minimax optimization algorithms, matching the global features across domains. However, the information conveyed in unlabelled target samples is not fully exploited. Here, adversarial learning and self-training are unified in an objective function, where the neural network parameters and the pseudo-labels of target samples are jointly optimized. The model&#39;s predictions on unlabelled samples are leveraged to pseudo-label target samples. The training procedure consists of two alternating steps. The first one is to train the network, while the second is to generate pseudo-labels, and the loop continues. The proposed method achieves mean accuracy improvements of 2% on Office-31 , 0.7% on ImageCLEF-DA , 1.8% on Office-Home , and 1.2% on Digits than the baseline, which is superior to most state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Zi Wang and Xiaoliang Sun and Ang Su and Gang Wang and Yang Li and Qifeng Yu},
  doi          = {10.1049/ipr2.12184},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2169-2178},
  shortjournal = {IET Image Process.},
  title        = {Improve conditional adversarial domain adaptation using self-training},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recognizing apple leaf diseases using a novel parallel
real-time processing framework based on MASK RCNN and transfer learning:
An application for smart agriculture. <em>IETIP</em>, <em>15</em>(10),
2157–2168. (<a href="https://doi.org/10.1049/ipr2.12183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective recognition of fruit leaf diseases has a substantial impact on agro-based economies. Several fruit diseases exist that badly impact the yield and quality of fruits. A naked-eye inspection of an infected region is a difficult and tedious process; therefore, it is required to have an automated system for accurate recognition of the disease. It is widely understood that low contrast images affect identification and classification accuracy. Here a parallel framework for real-time apple leaf disease identification and classification is proposed. Initially, a hybrid contrast stretching method to increase the visual impact of an image is proposed and then the MASK RCNN is configured to detect the infected regions. In parallel, the enhanced images are utilized for training a pre-trained CNN model for features extraction. The Kapur&#39;s entropy along MSVM (EaMSVM) approach-based selection method is developed to select strong features for the final classification. The Plant Village dataset is employed for the experimental process and achieve the best accuracy of 96.6% on the ensemble subspace discriminant analysis (ESDA) classifier. A comparison with the previous techniques illustrates the superiority of the proposed framework.},
  archive      = {J_IETIP},
  author       = {Zia ur Rehman and Muhammad Attique Khan and Fawad Ahmed and Robertas Damaševičius and Syed Rameez Naqvi and Wasif Nisar and Kashif Javed},
  doi          = {10.1049/ipr2.12183},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2157-2168},
  shortjournal = {IET Image Process.},
  title        = {Recognizing apple leaf diseases using a novel parallel real-time processing framework based on MASK RCNN and transfer learning: An application for smart agriculture},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). U-FPNDet: A one-shot traffic object detector based on
u-shaped feature pyramid module. <em>IETIP</em>, <em>15</em>(10),
2146–2156. (<a href="https://doi.org/10.1049/ipr2.12182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of automatic driving, identifying vehicles and pedestrians is the starting point of other automatic driving techniques. Using the information collected by the camera to detect traffic targets is particularly important. The main bottleneck of traffic object detection is due to the same category of targets, which may have different scales. For example, the pixel-level of cars may range from 30 to 300 px, which will cause instability of positioning and classification. In this paper, a multi-dimension feature pyramid is constructed in order to solve the multi-scale problem. The feature pyramid is built by developing a U-shaped module and using a cascade-method. In order to verify the effectiveness of the U-shaped module, we also designed a new one-shot detector U-FPNDet. The model first extracts the basic feature map by using the basic network and constructs the multi-dimension feature pyramid. Next, a pyramid pooling module is used to get more context information from the scene. Finally, the detection network is run on each level of the pyramid to obtain the final result by NMS. By using this method, a state-of-the-art performance is achieved on both detection and classification on commonly used benchmarks.},
  archive      = {J_IETIP},
  author       = {Xiao Ke and Jianping Li},
  doi          = {10.1049/ipr2.12182},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2146-2156},
  shortjournal = {IET Image Process.},
  title        = {U-FPNDet: A one-shot traffic object detector based on U-shaped feature pyramid module},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-tree filter for cost aggregation in stereo matching.
<em>IETIP</em>, <em>15</em>(10), 2135–2145. (<a
href="https://doi.org/10.1049/ipr2.12149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The desired solution to many labelling problems in computer vision is a spatially smooth result where label changes are aligned with the edges of the guidance image. It can be obtained traditionally by smoothing the label costs using edge-aware filters. However, local filters incorporate the information in a local support region to obtain locally-optimal and non-local tree-based filters, which often overuse piece-wise constant assumptions. In this paper, we propose a spatial-tree filter for cost aggregation. The tree model incorporates the spatial affinity into the tree structure. The tree distance between two pixels on our spatial tree is an approximated geodesic distance, which acts as a pixel similarity metric. The filtering process was implemented by recursively techniques in four directions: Top-to-bottom, left-to-right, and vice-versa. Thus, the complexity of our approach is linear to the number of image pixels. Extensive experiments demonstrate the effectiveness and efficiency of our spatial-tree filter in image smoothing and stereo matching. Our method performs better than the existing tree-based non-local method in cost aggregation.},
  archive      = {J_IETIP},
  author       = {Yusheng Jin and Hong Zhao and Penghui Bu},
  doi          = {10.1049/ipr2.12149},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2135-2145},
  shortjournal = {IET Image Process.},
  title        = {Spatial-tree filter for cost aggregation in stereo matching},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cloud detection of GF-7 satellite laser footprint image.
<em>IETIP</em>, <em>15</em>(10), 2127–2134. (<a
href="https://doi.org/10.1049/ipr2.12141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In November 2019, the GaoFen-7(GF-7) satellite was equipped with China&#39;s first laser altimeter with full waveform recording capability, which obtains high-precision long-range three-dimensional coordinates. The influence of clouds is noticeable for laser transmission, and a footprint camera is used to determine laser pointing and to image the ground. However, the cloud inevitably appears in the laser footprint image. In this study, the authors propose a cloud detection scheme for footprint images based on deep learning. First, an adaptive pooling model is proposed according to the characteristics of the cloud region. Next, model fusion was performed based on the SegNet and U-Net training results. Finally, test time augmentation was used to enhance the data and to improve cloud detection accuracy. The experimental results show that the fusion result of the model was approximately 5% better than that of the traditional cloud detection algorithm, which improved the shortcomings of the traditional algorithm, such as poor detection effect for thin clouds and complex underlying cloud surfaces. The related conclusions have certain reference significance for GF-7 data processing and related research on footprint images.},
  archive      = {J_IETIP},
  author       = {Jiaqi Yao and Xinming Tang and Guoyuan Li and Jinquan Guo and Jiyi Chen and Xiongdan Yang and Bo Ai},
  doi          = {10.1049/ipr2.12141},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2127-2134},
  shortjournal = {IET Image Process.},
  title        = {Cloud detection of GF-7 satellite laser footprint image},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optimized YOLO-based object detection model for crop
harvesting system. <em>IETIP</em>, <em>15</em>(9), 2112–2125. (<a
href="https://doi.org/10.1049/ipr2.12181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of automated crop harvesting system based on machine vision may improve productivity and optimize the operational cost. The scope of this study is to obtain visual information at the plantation which is crucial in developing an intelligent automated crop harvesting system. This paper aims to develop an automatic detection system with high accuracy performance, low computational cost and lightweight model. Considering the advantages of YOLOv3 tiny, an optimized YOLOv3 tiny network namely YOLO-P is proposed to detect and localize three objects at palm oil plantation which include fresh fruit bunch, grabber and palm tree under various environment conditions. The proposed YOLO-P model incorporated lightweight backbone based on densely connected neural network, multi-scale detection architecture and optimized anchor box size. The experimental results demonstrated that the proposed YOLO-P model achieved good mean average precision and F1 score of 98.68% and 0.97 respectively. Besides, the proposed model performed faster training process and generated lightweight model of 76 MB. The proposed model was also tested to identify fresh fruit bunch of various maturities with accuracy of 98.91%. The comprehensive experimental results show that the proposed YOLO-P model can effectively perform robust and accurate detection at the palm oil plantation.},
  archive      = {J_IETIP},
  author       = {Mohamad Haniff Junos and Anis Salwa Mohd Khairuddin and Subbiah Thannirmalai and Mahidzal Dahari},
  doi          = {10.1049/ipr2.12181},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2112-2125},
  shortjournal = {IET Image Process.},
  title        = {An optimized YOLO-based object detection model for crop harvesting system},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scale space radon transform. <em>IETIP</em>, <em>15</em>(9),
2097–2111. (<a href="https://doi.org/10.1049/ipr2.12180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extension of Radon transform by using a measure function capturing the user need is proposed. The new transform, called scale space Radon transform, is devoted to the case where the embedded shape in the image is not filiform. A case study is brought on a straight line and an ellipse where the SSRT behaviour in the scale space and in the presence of noise is deeply analyzed. In order to show the effectiveness of the proposed transform, the experiments have been carried out, first, on linear and elliptical structures generated synthetically subjected to strong altering conditions such blur and noise and then on structures images issued from real-world applications such as road traffic, satellite imagery and weld X-ray imaging. Comparisons in terms of detection accuracy and computational time with well-known transforms and recent work dedicated to this purpose are conducted, where the proposed transform shows an outstanding performance in detecting the above-mentioned structures and targeting accurately their spatial locations even in low-quality images.},
  archive      = {J_IETIP},
  author       = {Djemel Ziou and Nafaa Nacereddine and Aicha Baya Goumeidane},
  doi          = {10.1049/ipr2.12180},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2097-2111},
  shortjournal = {IET Image Process.},
  title        = {Scale space radon transform},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic feature selection and classification framework
for analyzing ultrasound kidney images using dragonfly algorithm and
random forest classifier. <em>IETIP</em>, <em>15</em>(9), 2080–2096. (<a
href="https://doi.org/10.1049/ipr2.12179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical imaging, the automatic diagnosis of kidney carcinoma has become more difficult because it is not easy to detect by physicians. Pre-processing is the first identification method to enhance image quality, remove noise and unwanted components from the backdrop of the kidneys image. The pre-processing method is essential and significant for the proposed algorithm. The objective of this analysis is to recognize and classify kidney disturbances with an ultrasound scan by providing a number of substantial content description parameters. The ultrasound pictures are prepared to protect the interest pixels before extracting the feature. A series of quantitative features were synthesized of each images, the principal component analysis was conducted for minimizing the number of features to produce set of wavelet-based multi-scale features. Dragonfly algorithm (DFA) was executed in this method. In the proposed work, the design and training of a random decision forest classifier and selected features are implemented. The classification of e-health information using ideal characteristics is used by the RF classifier. The proposed technique is activated in MATLAB/simulink work site and the experimental results show that the peak accuracy of the proposed technique is 95.6% using GWO-FFBN techniques compared to other existing techniques.},
  archive      = {J_IETIP},
  author       = {C Venkata Narasimhulu},
  doi          = {10.1049/ipr2.12179},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2080-2096},
  shortjournal = {IET Image Process.},
  title        = {An automatic feature selection and classification framework for analyzing ultrasound kidney images using dragonfly algorithm and random forest classifier},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust sperm cell tracking algorithm using uneven lighting
image fixing and improved branch and bound algorithm. <em>IETIP</em>,
<em>15</em>(9), 2068–2079. (<a
href="https://doi.org/10.1049/ipr2.12178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate and robust sperm cells tracking algorithm that is able to detect and track sperm cells in videos with high accuracy and efficiency is presented. It is fast enough to process approximately 30 frames per second. It can find the correct path and measure motility parameters for each sperm. It can also adapt with different types of images coming from different cameras and bad recording conditions. Specifically, a new way is offered to optimize uneven lighting images to improve sperm cells detection which gives us the ability to get more accurate tracking results. The shape of each detected object is used to specify collided sperms and utilized dynamic gates which become bigger and smaller according to the sperm cell&#39;s speed. For assigning tracks to the detected sperm cells positions an improved version of branch and bound algorithm which is faster than the normal one is offered. This sperm cells tracking algorithm outperforms many of the previous algorithms as it has lower error rate in both sperm detection and tracking. It is compared with six other algorithms, and it gives lower tracking error rates. This method will allow doctors and researchers to obtain sperm motility data instantly and accurately.},
  archive      = {J_IETIP},
  author       = {Ahmad Alhaj Alabdulla and Abdulsamet Haşıloğlu and Emrah Hicazi Aksu},
  doi          = {10.1049/ipr2.12178},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2068-2079},
  shortjournal = {IET Image Process.},
  title        = {A robust sperm cell tracking algorithm using uneven lighting image fixing and improved branch and bound algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A method of lining seam elimination with angle adaptation
and rectangular mark for road tunnel concrete lining images.
<em>IETIP</em>, <em>15</em>(9), 2056–2067. (<a
href="https://doi.org/10.1049/ipr2.12177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road Tunnels are an important part of the current road transportation infrastructure. As the main form of tunnel lining diseases, cracks are easy to interact with other areas, which seriously affects the safe operation of the tunnel. Due to the similarity of brightness and linearity between surface cracks and lining cracks, the existing crack detection algorithms can not extract cracks accurately and quickly. An algorithm of lining seam crack elimination with rectangular mark is proposed here. First, the line segments in the image are detected by the Line Segment Detector algorithm based on the coarse percolation detection of the crack. Second, the distribution directions are calculated, and cracks from the lining seams are distinguished by the adaptive threshold judgment method. Third, by using the distribution characteristics of pixels, the line segments are extended to form rectangular marks perpendicular to the direction of lining seams. Finally, the marking information is used to remove the lining joints and obtain the real surface cracks of tunnel lining. Experimental results show that the algorithm can quickly and effectively remove any shape distribution of lining seam. The algorithm fills in the blank of concrete tunnel lining surface crack detection technology.},
  archive      = {J_IETIP},
  author       = {Zhong Qu and Yu-Lu Zhong and Ling Liu},
  doi          = {10.1049/ipr2.12177},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2056-2067},
  shortjournal = {IET Image Process.},
  title        = {A method of lining seam elimination with angle adaptation and rectangular mark for road tunnel concrete lining images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An image encryption algorithm with a plaintext-related
quantisation scheme. <em>IETIP</em>, <em>15</em>(9), 2039–2055. (<a
href="https://doi.org/10.1049/ipr2.12174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes an image encryption algorithm that utilises a plaintext-related quantisation scheme. Various plaintext-related approaches from other algorithms are presented and their properties are briefly discussed. Main advantage of the proposed solution is the achievement of a similar behaviour like that of more complex approaches with a plaintext-related technique used in a rather simple step such as quantisation. This design should result in a favourable computational complexity of the whole algorithm. The properties of the proposal are evaluated by a number of commonly used numerical parameters. Also, the statistical properties of a pseudo-random sequence that is quantised according to the plain image pixel intensities are investigated by tests from NIST 800-22 suite. Obtained results are compared to values reported in related works and they imply that the proposed solution produces encrypted images with comparable statistical properties but authors&#39; design is faster and more efficient.},
  archive      = {J_IETIP},
  author       = {Jakub Oravec and Ľuboš Ovseník and Ján Papaj},
  doi          = {10.1049/ipr2.12174},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2039-2055},
  shortjournal = {IET Image Process.},
  title        = {An image encryption algorithm with a plaintext-related quantisation scheme},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AMBCR: Low-light image enhancement via attention guided
multi-branch construction and retinex theory. <em>IETIP</em>,
<em>15</em>(9), 2020–2038. (<a
href="https://doi.org/10.1049/ipr2.12173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to different lighting environments and equipment limitations, low-light images have high noise, low contrast and unobvious colours. The main purpose of low-light image enhancement is to preserve the details and suppress noise as much as possible while improving the contrast of the image. Here, different networks are first combined to construct a multi-branch module for features extraction, and use the module and Retinex theory to extract the reflection map of the image. Then an attention mechanism is introduced into the multi-branch construction to balance the feature weight of each branch, and get the final result by the reconstruction module. The Retinex theory is used to calculate the L 1 loss and the gradient loss for the intermediate feature map of the entire model to train our framework. The entire process is completed in an end-to-end-way, which avoids the hand-crafted reconstruction rules and reduces the workload. What&#39;s more, a large number of experiments demonstrate that the proposed framework performs better results than state-of-the-art algorithms in both quantitative and qualitative evaluations of image enhancement.},
  archive      = {J_IETIP},
  author       = {Miao Li and Dongming Zhou and Rencan Nie and Shidong Xie and Yanyu Liu},
  doi          = {10.1049/ipr2.12173},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2020-2038},
  shortjournal = {IET Image Process.},
  title        = {AMBCR: Low-light image enhancement via attention guided multi-branch construction and retinex theory},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DuGAN: An effective framework for underwater image
enhancement. <em>IETIP</em>, <em>15</em>(9), 2010–2019. (<a
href="https://doi.org/10.1049/ipr2.12172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement is an important low-level vision task with much attention of community. Clear underwater images are helpful for underwater operations. However, raw underwater images often suffer from different types of distortions caused by the underwater environment. To solve these problems, this paper proposes an end-to-end dual generative adversarial network (DuGAN) for underwater image enhancement. The images processed by existing methods are taken as training samples for reference, and they are segmented into clear parts and unclear parts. Two discriminators are used to complete adversarial training toward different areas of images with different training strategies, respectively. The proposed method is able to output more pleasing images than reference images benefit by this framework. Meanwhile, to ensure the authenticity of the enhanced images, content loss, adversarial loss, and style loss are combined as loss function of our framework. This framework is easy to use, and the subjective and objective experiments show that excellent results are achieved compared to those methods mentioned in the literature.},
  archive      = {J_IETIP},
  author       = {Huiqing Zhang and Luyu Sun and Lifang Wu and Ke Gu},
  doi          = {10.1049/ipr2.12172},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2010-2019},
  shortjournal = {IET Image Process.},
  title        = {DuGAN: An effective framework for underwater image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lemon-YOLO: An efficient object detection method for lemons
in the natural environment. <em>IETIP</em>, <em>15</em>(9), 1998–2009.
(<a href="https://doi.org/10.1049/ipr2.12171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient Intelligent detection is a key technology in automatic harvesting robots. However, citrus detection is still a challenging task because of varying illumination, random occlusion and colour similarity between fruits and leaves in natural conditions. In this paper, a detection method called Lemon-YOLO (L-YOLO) is proposed to improve the accuracy and real-time performance of lemon detection in the natural environment. The SE_ResGNet34 network is designed to replace DarkNet53 network in YOLOv3 algorithm as a new backbone of feature extraction. It can enhance the propagation of features, and needs less parameter, which helps to achieve higher accuracy and speed. Moreover, the SE_ResNet module is added to the detection block, to improve the quality of representations produced from the network by strengthening the convolutional features of channels. The experimental results show that the proposed L-YOLO has an average accuracy(AP) of 96.28% and a detection speed of 106 frames per second (FPS) on the lemon test set, which is 5.68% and 28 FPS higher than the YOLOv3, respectively. The results indicate that the L-YOLO method has superior detection performance. It can recognize and locate lemons in the natural environment more efficiently, providing technical support for the machine&#39;s picking lemon and other fruits.},
  archive      = {J_IETIP},
  author       = {Guojin Li and Xiaojie Huang and Jiaoyan Ai and Zeren Yi and Wei Xie},
  doi          = {10.1049/ipr2.12171},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1998-2009},
  shortjournal = {IET Image Process.},
  title        = {Lemon-YOLO: An efficient object detection method for lemons in the natural environment},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remote sensing target tracking in satellite videos based on
a variable-angle-adaptive siamese network. <em>IETIP</em>,
<em>15</em>(9), 1987–1997. (<a
href="https://doi.org/10.1049/ipr2.12170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing target tracking in satellite videos plays a key role in various fields. However, due to the complex backgrounds of satellite video sequences and many rotation changes of highly dynamic targets, typical target tracking methods for natural scenes cannot be used directly for such tasks, and their robustness and accuracy are difficult to guarantee. To address these problems, an algorithm is proposed for remote sensing target tracking in satellite videos based on a variable-angle-adaptive Siamese network (VAASN). Specifically, the method is based on the fully convolutional Siamese network (Siamese-FC). First, for the feature extraction stage, to reduce the impact of complex backgrounds, we present a new multifrequency feature representation method and introduce the octave convolution (OctConv) into the AlexNet architecture to adapt to the new feature representation. Then, for the tracking stage, to adapt to changes in target rotation, a variable-angle-adaptive module that uses a fast text detector with a single deep neural network (TextBoxes++) is introduced to extract angle information from the template frame and detection frames and performs angle consistency update operations on the detection frames. Finally, qualitative and quantitative experiments using satellite datasets show that the proposed method can improve tracking accuracy while achieving high efficiency.},
  archive      = {J_IETIP},
  author       = {Fukun Bi and Jiayi Sun and Jianhong Han and Yanping Wang and Mingming Bian},
  doi          = {10.1049/ipr2.12170},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1987-1997},
  shortjournal = {IET Image Process.},
  title        = {Remote sensing target tracking in satellite videos based on a variable-angle-adaptive siamese network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards accurate classification of skin cancer from
dermatology images. <em>IETIP</em>, <em>15</em>(9), 1971–1986. (<a
href="https://doi.org/10.1049/ipr2.12166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is the most well-known disease found in the individuals who are exposed to the Sun&#39;s ultra-violet (UV) radiations. It is identified when skin tissues on the epidermis grow in an uncontrolled manner and appears to be of different colour than the normal skin tissues. This paper focuses on predicting the class of dermascopic images as benign and malignant. A new feature extraction method has been proposed to carry out this work which can extract relevant features from image texture. Local and gradient information from and directions of images has been utilized for feature extraction. After that images are classified using machine learning algorithms by using those extracted features. The efficacy of the proposed feature extraction method has been proved by conducting several experiments on the publicly available image dataset 2016 International Skin Imaging Collaboration (ISIC 2016). The classification results obtained by the method are also compared with state-of-the-art feature extraction methods which show that it performs better than others. The evaluation criteria used to obtain the results are accuracy, true positive rate (TPR) and false positive rate (FPR) where TPR and FPR are used for generating receiver operating characteristic curves.},
  archive      = {J_IETIP},
  author       = {Anjali Gautam and Balasubramanian Raman},
  doi          = {10.1049/ipr2.12166},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1971-1986},
  shortjournal = {IET Image Process.},
  title        = {Towards accurate classification of skin cancer from dermatology images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised salient object detection via double object
proposals guidance. <em>IETIP</em>, <em>15</em>(9), 1957–1970. (<a
href="https://doi.org/10.1049/ipr2.12164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weakly supervised methods for salient object detection are attractive, since they greatly release the burden of annotating time-consuming pixel-wise masks. However, the image-level annotations utilized by current weakly supervised salient object detection models are too weak to provide sufficient supervision for this dense prediction task. To this end, a weakly supervised salient object detection method is proposed via double object proposals guidance, which is generated under the supervision of double bounding boxes annotations. With the double object proposals, the authors&#39; method is capable of capturing both accurate but incomplete salient foreground and background information, which contributes to generating saliency maps with uniformly highlighted saliency regions and effectively suppressed background. In addition, an unsupervised salient object segmentation method is proposed, taking advantage of the non-parametric statistical active contour model (NSACM), for segmenting salient objects with complete and compact boundaries. Experiments on five benchmark datasets show that the authors&#39; weakly supervised salient object detection approach consistently outperforms other weakly supervised and unsupervised methods by a considerable margin, and even has comparable performance to the fully supervised ones.},
  archive      = {J_IETIP},
  author       = {Zhiheng Zhou and Yongfan Guo and Ming Dai and Junchu Huang and Xiangwei Li},
  doi          = {10.1049/ipr2.12164},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1957-1970},
  shortjournal = {IET Image Process.},
  title        = {Weakly supervised salient object detection via double object proposals guidance},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized deep learning model for mango grading: Hybridizing
lion plus firefly algorithm. <em>IETIP</em>, <em>15</em>(9), 1940–1956.
(<a href="https://doi.org/10.1049/ipr2.12163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper intends to present an automated mango grading system under four stages (1) pre-processing, (2) feature extraction, (3) optimal feature selection and (4) classification. Initially, the input image is subjected to the pre-processing phase, where the reading, sizing, noise removal and segmentation process happens. Subsequently, the features are extracted from the pre-processed image. To make the system more effective, from the extracted features, the optimal features are selected using a new hybrid optimization algorithm termed the lion assisted firefly algorithm (LA-FF), which is the combination of LA and FF, respectively. Then, the optimal features are given for the classification process, where the optimized deep convolutional neural network (CNN) is deployed. As a major contribution, the configuration of CNN is fine-tuned via selecting the optimal count of convolutional layers. This obviously enhances the classification accuracy in grading system. For fine-tuning the convolutional layers in the deep CNN, the LA-FF algorithm is used so that the classifier is optimized. The grading is evaluated on the basis of healthydiseased, ripeunripe and bigmediumvery big cases with respect to type I and type II measures and the performance of the proposed grading model is compared over the other state-of-the-art models.},
  archive      = {J_IETIP},
  author       = {Mukesh Kumar Tripathi and Dhananjay D. Maktedar},
  doi          = {10.1049/ipr2.12163},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1940-1956},
  shortjournal = {IET Image Process.},
  title        = {Optimized deep learning model for mango grading: Hybridizing lion plus firefly algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image restoration model using jaya-bat optimization-enabled
noise prediction map. <em>IETIP</em>, <em>15</em>(9), 1926–1939. (<a
href="https://doi.org/10.1049/ipr2.12162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration approaches are introduced to restore the latent clear images from the degraded images. However, the performance of the existing approaches remains an open problem, which may leads to the further development of advanced image restoration techniques. Therefore, an effective image restoration method is developed for restoring the input image from various noises, like impulse noise and random noise. The generation of pixel map, identification of noisy pixel, and the enhancement of pixel are the three major phases involved in the proposed method. Initially, the noisy pixel map generation is performed from the input image, and then the noisy pixels are identified based on deep convolutional neural network, which is trained by the proposed Jaya-Bat algorithm. The Jaya-Bat algorithm is developed by combing the Jaya optimization algorithm and Bat algorithm. Once the noisy pixels are identified, the pixel enhancement is done using the neuro fuzzy system. The experiment is carried out using Statlog (landsat satellite) dataset, and the developed method achieves the maximal peak signal to noise ratio of 51.03 dB, maximal structural similarity index of 0.848 for the image with random noise, and the maximal second derivative like measure of enhancement 62.96 dB with impulse noise, respectively.},
  archive      = {J_IETIP},
  author       = {Premnath S. P and J. Arokia Renjit},
  doi          = {10.1049/ipr2.12162},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1926-1939},
  shortjournal = {IET Image Process.},
  title        = {Image restoration model using jaya-bat optimization-enabled noise prediction map},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CSIS: Compressed sensing-based enhanced-embedding capacity
image steganography scheme. <em>IETIP</em>, <em>15</em>(9), 1909–1925.
(<a href="https://doi.org/10.1049/ipr2.12161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography plays a vital role in securing secret data by embedding it in the cover images. Usually, these images are communicated in a compressed format. Existing techniques achieve this but have low embedding capacity. Hence, the goal here is to enhance the embedding capacity while preserving the visual quality of the stego-image. It is also intended to ensure that the scheme is resistant to steganalysis attacks. This paper proposes a compressed sensing image steganography (CSIS) scheme to achieve these goals. In CSIS, the cover image is sparsified block-wise, linear measurements are obtained, and then permissible measurements are selected. Next, the secret data is encrypted, and 2 bits of this encrypted data are embedded into each permissible measurement. For the reconstruction of the stego-image, ADMM and LASSO are used for the resultant optimization problem. Experiments are performed on several standard greyscale images and a colour image. Higher embedding capacity, 1.53 times more compared to the most recent scheme, is achieved. An average of 37.92 dB PSNR value, and average values close to 1 for both the mean SSIM index and the NCC coefficients are obtained, which is considered good. These metrics show that CSIS substantially outperforms existing similar steganography schemes.},
  archive      = {J_IETIP},
  author       = {Rohit Agrawal and Kapil Ahuja},
  doi          = {10.1049/ipr2.12161},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1909-1925},
  shortjournal = {IET Image Process.},
  title        = {CSIS: Compressed sensing-based enhanced-embedding capacity image steganography scheme},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forgery localization in images based on joint statistics of
image blocks with neighbouring blocks. <em>IETIP</em>, <em>15</em>(9),
1893–1908. (<a href="https://doi.org/10.1049/ipr2.12160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flawless image forensic analysis necessitates precise identification of tampering regions in digital images along with the determination of state of an image (original or forged). Most of the efforts towards localization of forgeries involve localization of information-changing forgeries with less focus towards localization of information-preserving forgeries. This paper proposes a new information-preserving forgery localization method to localize 10 different tampering types exploiting the fact that joint statistics of locally forged regions and neighbouring original regions is disturbed. The proposed 18-dimensional detector is trained using ensemble classifier for fine-grain identification of forged regions in post-JPEG compressed images and results are compared with two recent state-of-the-art detectors. The results demonstrate the effectiveness of the proposed detector with significant performance gain over existing detectors, particularly for localization in images that are compressed with low quality factors of compression.},
  archive      = {J_IETIP},
  author       = {Divya Singhal and Abhinav Gupta},
  doi          = {10.1049/ipr2.12160},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1893-1908},
  shortjournal = {IET Image Process.},
  title        = {Forgery localization in images based on joint statistics of image blocks with neighbouring blocks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Agile reactive navigation for a non-holonomic mobile robot
using a pixel processor array. <em>IETIP</em>, <em>15</em>(9),
1883–1892. (<a href="https://doi.org/10.1049/ipr2.12158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an agile reactive navigation strategy for driving a non-holonomic ground vehicle around a pre-set course of gates in a cluttered environment using a low-cost processor array sensor. This enables machine vision tasks to be performed directly upon the sensor&#39;s image plane, rather than using a separate general-purpose computer. The authors demonstrate a small ground vehicle running through or avoiding multiple gates at high speed using minimal computational resources. To achieve this, target tracking algorithms are developed for the Pixel Processing Array and captured images are then processed directly on the vision sensor acquiring target information for controlling the ground vehicle. The algorithm can run at up to 2000 fps outdoors and 200 fps at indoor illumination levels. Conducting image processing at the sensor level avoids the bottleneck of image transfer encountered in conventional sensors. The real-time performance of on-board image processing and robustness is validated through experiments. Experimental results demonstrate the algorithm&#39;s ability to enable a ground vehicle to navigate at an average speed of 2.20 m/s for passing through multiple gates and 3.88 m/s for a ‘slalom’ task in an environment featuring significant visual clutter.},
  archive      = {J_IETIP},
  author       = {Yanan Liu and Laurie Bose and Colin Greatwood and Jianing Chen and Rui Fan and Thomas Richardson and Stephen J. Carey and Piotr Dudek and Walterio Mayol-Cuevas},
  doi          = {10.1049/ipr2.12158},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1883-1892},
  shortjournal = {IET Image Process.},
  title        = {Agile reactive navigation for a non-holonomic mobile robot using a pixel processor array},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online variational inference on finite multivariate beta
mixture models for medical applications. <em>IETIP</em>, <em>15</em>(9),
1869–1882. (<a href="https://doi.org/10.1049/ipr2.12154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances led to the generation of large scale complex data. Thus, extraction and retrieval of information to automatically discover latent pattern have been largely studied in the various domains of science and technology. Consequently, machine learning experienced tremendous development and various statistical approaches have been suggested. In particular, data clustering has received a lot of attention. Finite mixture models have been revealed to be one of the flexible and popular approaches in data clustering. Considering mixture models, three crucial aspects should be addressed. The first issue is choosing a distribution which is flexible enough to fit the data. In this paper, a model based on multivariate Beta distributions is proposed. The two other challenges in mixture models are estimation of model&#39;s parameters and model complexity. To tackle these challenges, variational inference techniques demonstrated considerable robustness. In this paper, two methods are studied, namely, batch and online variational inferences and the models are evaluated on four medical applications including image segmentation of colorectal cancer, multi-class colon tissue analysis, digital imaging in skin lesion diagnosis and computer aid detection of Malaria.},
  archive      = {J_IETIP},
  author       = {Narges Manouchehri and Meeta Kalra and Nizar Bouguila},
  doi          = {10.1049/ipr2.12154},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1869-1882},
  shortjournal = {IET Image Process.},
  title        = {Online variational inference on finite multivariate beta mixture models for medical applications},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of approaches investigated for right ventricular
segmentation using short-axis cardiac MRI. <em>IETIP</em>,
<em>15</em>(9), 1845–1868. (<a
href="https://doi.org/10.1049/ipr2.12165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The right ventricular assessment is crucial to heart disease diagnosis. Unfortunately, its segmentation is quite challenging due to its intricate shape, ill-defined thin edges, large variability among patients, and pathologies. Besides, it is a very laborious and time-consuming task to be done manually. Therefore, automated segmentation techniques are very suitable to reduce the strain on the expert. Here, it is attempted to review the taxonomy of the current RV segmentation approaches adopted to handle the afore-mentioned issues. Enhanced by our expert&#39;s interpretation, the results of over forty research papers were evaluated based on several metrics such as the dice metric and the Hausdorff distance. Synthetic tables and charts were also used to discuss the reviewed approaches. The following study shows that none of the existing methods has proved accurate enough to meet all the RV challenging issues. Many misestimated results were reported for several cases. Eventually, global guidance is outlined, which supports combining different methods to enhance the expected results during the MRI short-axis slice processing.},
  archive      = {J_IETIP},
  author       = {Asma Ammari and Ramzi Mahmoudi and Badii Hmida and Rachida Saouli and Mohamed Hédi Bedoui},
  doi          = {10.1049/ipr2.12165},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1845-1868},
  shortjournal = {IET Image Process.},
  title        = {A review of approaches investigated for right ventricular segmentation using short-axis cardiac MRI},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse representation for face recognition: A review paper.
<em>IETIP</em>, <em>15</em>(9), 1825–1844. (<a
href="https://doi.org/10.1049/ipr2.12155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing use of surveillance cameras, face recognition is being studied by many researchers for security purposes. Although high accuracy has been achieved for frontal faces, the existing methods have shown poor performance for occluded and corrupt images. Recently, sparse representation based classification (SRC) has shown the state-of-the-art result in face recognition on corrupt and occluded face images. Several researchers have developed extended SRC methods in the last decade. This paper mainly focuses on SRC and its extended methods of face recognition. SRC methods have been compared on the basis of five issues of face recognition such as linear variation, non-linear variation, undersampled, pose variation, and low resolution. Detailed analysis of SRC methods for issues of face recognition have been discussed based on experimental results and execution time. Finally, the limitation of SRC methods have been listed to help the researchers to extend the work of existing methods to resolve the unsolved issues.},
  archive      = {J_IETIP},
  author       = {Jitendra Madarkar and Poonam Sharma and Rimjhim Padam Singh},
  doi          = {10.1049/ipr2.12155},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1825-1844},
  shortjournal = {IET Image Process.},
  title        = {Sparse representation for face recognition: A review paper},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 disease severity assessment using CNN model.
<em>IETIP</em>, <em>15</em>(8), 1814–1824. (<a
href="https://doi.org/10.1049/ipr2.12153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the highly infectious nature of the novel coronavirus (COVID-19) disease, excessive number of patients waits in the line for chest X-ray examination, which overloads the clinicians and radiologists and negatively affects the patient&#39;s treatment, prognosis and control of the pandemic. Now that the clinical facilities such as the intensive care units and the mechanical ventilators are very limited in the face of this highly contagious disease, it becomes quite important to classify the patients according to their severity levels. This paper presents a novel implementation of convolutional neural network (CNN) approach for COVID-19 disease severity classification (assessment). An automated CNN model is designed and proposed to divide COVID-19 patients into four severity classes as mild, moderate, severe, and critical with an average accuracy of 95.52% using chest X-ray images as input. Experimental results on a sufficiently large number of chest X-ray images demonstrate the effectiveness of CNN model produced with the proposed framework. To the best of the author&#39;s knowledge, this is the first COVID-19 disease severity assessment study with four stages (mild vs. moderate vs. severe vs. critical) using a sufficiently large number of X-ray images dataset and CNN whose almost all hyper-parameters are automatically tuned by the grid search optimiser.},
  archive      = {J_IETIP},
  author       = {Emrah Irmak},
  doi          = {10.1049/ipr2.12153},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1814-1824},
  shortjournal = {IET Image Process.},
  title        = {COVID-19 disease severity assessment using CNN model},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A keypoint-based object detection method with wide dual-path
backbone network and attention modules. <em>IETIP</em>, <em>15</em>(8),
1800–1813. (<a href="https://doi.org/10.1049/ipr2.12152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keypoint-based object detection is one of the most efficient and speedy methods at present, yet its performance is often worse than the anchor-based method. Without prior settings in the keypoint-based method, the huge search space of the keypoints results in the high recall but low precision. In this paper, the wide dual-path backbone network is introduced as a feature extractor to extract richer original information, which has fewer parameters and better classification performance. Then, the attention fusion module is designed to effectively fuse the dual-path with the consideration of the respective advantages of the residual-path and the densely connected path. In order to provide more accurate pixel-level information for keypoint prediction, the upsample dual-attention module is proposed to recover the spatial size of the feature map, which integrates multi-scale of channel-wise and spatial attention. Compared with other state-of-the-art detectors, this method has achieved accuracy-efficiency results with fewer parameters, lower FLOPs, and smaller model size. Experimental results show that the proposed wide dual-path backbone network has achieved 4.98% top1-error on the CIFAR-10 classification dataset. On the PASCAL VOC object detection dataset, this model has achieved an accuracy-efficiency tradeoff result of 78.3% mAP at the speed of 41 FPS.},
  archive      = {J_IETIP},
  author       = {Zhong Qu and Run Zhang and Kang-hua Bao},
  doi          = {10.1049/ipr2.12152},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1800-1813},
  shortjournal = {IET Image Process.},
  title        = {A keypoint-based object detection method with wide dual-path backbone network and attention modules},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pyramid non-local enhanced residual dense network for
single image de-raining. <em>IETIP</em>, <em>15</em>(8), 1786–1799. (<a
href="https://doi.org/10.1049/ipr2.12151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image de-raining based on convolutional neural network (CNN) has made considerable progress in recent years. However, usually the de-rained result has dark artifacts and image textures tend to be over-smoothed. In this paper, a pyramid non-local enhanced residual dense network is proposed to reduce such distortion. Firstly, the down-sampled images are input into the Laplacian pyramid, which can extract the overall and partial texture clues, and subsequently a set of images of different scales are produced. Secondly, these images are fed into a non-local enhanced residual dense block, which can not only capture long-distance dependencies of feature maps, but also fully utilizes the hierarchical features in every dense block, leading to high accuracy of rain streaks extraction and better preservation of image edge detail. Finally, the de-rained image is gradually restored by Gaussian reconstruction pyramid. Experimental results on both synthetic data and real-world data show that the artifacts distortion is obviously reduced by the proposed network. And the quality of de-rained image is significantly improved compared with the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Minghua Zhao and Hengrui Fan and Shuangli DU and Li Wang and Peng Li and Jing Hu},
  doi          = {10.1049/ipr2.12151},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1786-1799},
  shortjournal = {IET Image Process.},
  title        = {A pyramid non-local enhanced residual dense network for single image de-raining},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning adaptive spatial–temporal regularized correlation
filters for visual tracking. <em>IETIP</em>, <em>15</em>(8), 1773–1785.
(<a href="https://doi.org/10.1049/ipr2.12150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been many visual tracking methods based on correlation filters. These methods mainly enhance the tracking performances by considering the information of background, space, or time in the appearance model. This paper proposes an effective tracking method, named adaptive spatial–temporal regularized correlation filter (ASTRCF) tracker, based on the popular adaptive spatially regularized correlation filter (ASRCF) tracker. That is, the continuity of object&#39;s motion in the process of tracking is considered by introducing a temporal-regularized term in the appearance model of ASRCF tracker. Furthermore, its solution is inferred by applying the alternating direction method of multipliers. The proposed appearance model contains a background-awareness term, a spatially regularized term, an adaptive-weight term, and a temporal-regularized term. Therefore, it can not only keep the good performances of ASRCF tracker, such as learning the background information and the spatial information adaptively to enhance the discriminating ability, but also take advantage of the relation of correlation filters in the last frame and the current frame for addressing the complex cases, such as occlusion, and fast motion. Extensive experimental results on various challenging databases show that the proposed ASTRCF tracker achieves better tracking performances than some state-of-the-art trackers.},
  archive      = {J_IETIP},
  author       = {Jianwei Zhao and Yangxiao Li and Zhenghua Zhou},
  doi          = {10.1049/ipr2.12150},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1773-1785},
  shortjournal = {IET Image Process.},
  title        = {Learning adaptive spatial–temporal regularized correlation filters for visual tracking},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A low-light image enhancement method based on bright channel
prior and maximum colour channel. <em>IETIP</em>, <em>15</em>(8),
1759–1772. (<a href="https://doi.org/10.1049/ipr2.12148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement algorithms have been introduced to improve the visual quality of low-light images that may degrade the performance of many computer vision and multimedia systems designed for high-quality images. However, the existing bright channel prior and maximum colour channel enhancement algorithms introduce halo artifacts and colour distortions while enhancing the images. To overcome these limitations, in this paper, an effective fusion-based low-light image enhancement algorithm is proposed. In the proposed algorithm, the illumination of the low-light image is estimated from both the bright and maximum colour channels to overcome the halo artifacts and colour distortion problems. Further, an effective refinement method is utilized to improve the sharpness of the initial enhanced image representing the scene reflectance. Experiment results show that the proposed algorithm outperforms the state-of-the-art algorithms qualitatively and quantitatively. Moreover, the proposed algorithm reduces the halo artifacts and colour distortion and enhances the details while preserving the naturalness.},
  archive      = {J_IETIP},
  author       = {Ghada Sandoub and Randa Atta and Hesham Arafat Ali and Rabab Farouk Abdel-Kader},
  doi          = {10.1049/ipr2.12148},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1759-1772},
  shortjournal = {IET Image Process.},
  title        = {A low-light image enhancement method based on bright channel prior and maximum colour channel},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing an efficient and adaptive learning model for 3D
object generation. <em>IETIP</em>, <em>15</em>(8), 1745–1758. (<a
href="https://doi.org/10.1049/ipr2.12146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying representation learning and generative modelling has been at the core of the 3D learning domain. By leveraging the generative adversarial networks and convolutional neural networks for point-cloud representations, we propose a novel framework, which can directly generate 3D objects represented by point clouds. The novelties of the proposed method are threefold. First, the generative adversarial networks are applied to 3D object generation in the point-cloud space, where the model learns object representation from point clouds independently. In this work, we propose a 3D spatial transformer network, and integrate it into a generation model, whose ability for extracting and reconstructing features for 3D objects can be improved. Second, a point-wise approach is developed to reduce the computational complexity of the proposed network. Third, an evaluation system is proposed to measure the performance of our model by employing various categories and methods, and the error, considered as the difference between synthesized objects and raw objects are quantitatively compared, is less than 2.8%. Extensive experiments on benchmark dataset show that this method has a strong ability to generate 3D objects in the point-cloud space, and the synthesized objects have slight differences with man-made 3D objects.},
  archive      = {J_IETIP},
  author       = {Jiwei Hu and Wupeng Deng and Quan Liu and Kin-Man Lam and Ping Lou},
  doi          = {10.1049/ipr2.12146},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1745-1758},
  shortjournal = {IET Image Process.},
  title        = {Constructing an efficient and adaptive learning model for 3D object generation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature pyramid u-net for retinal vessel segmentation.
<em>IETIP</em>, <em>15</em>(8), 1733–1744. (<a
href="https://doi.org/10.1049/ipr2.12142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The retinal vessel is the only microvascular network that can be directly and non-invasively observed in humans. Cardiovascular and cerebrovascular diseases, such as diabetes, hypertension, can lead to structural changes of the retinal microvascular network. Therefore, it is of great significance to study effective retinal vessel segmentation methods and assist doctors in early diagnoses with quantitative results for vascular networks. In this study, we propose a novel convolutional neural network named feature pyramid U-Net (FPU-Net) that extracts multiscale representations by constructing two feature pyramids both on the encoder and the decoder of U-Net. In this representation, objects features with different size like micro-vessels and pathology will be fused for better vessel segmentation. The experimental results show that compared with state-of-the-art methods, FPU-Net is superior in terms of accuracy, sensitivity, F1-score, and area under the curve and capable of stronger domain generalisation across different datasets.},
  archive      = {J_IETIP},
  author       = {Yi-Peng Liu and Xue Rui and Zhanqing Li and Dongxu Zeng and Jing Li and Peng Chen and Ronghua Liang},
  doi          = {10.1049/ipr2.12142},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1733-1744},
  shortjournal = {IET Image Process.},
  title        = {Feature pyramid U-net for retinal vessel segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient local stereo matching method based on an
adaptive exponentially weighted moving average filter in SLIC space.
<em>IETIP</em>, <em>15</em>(8), 1722–1732. (<a
href="https://doi.org/10.1049/ipr2.12140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly obtaining accurate dense disparity maps has been the focus of stereo matching research. At present, approaches that achieve superior disparity maps require a large amount of computation, which is not suitable for practical applications. To address this issue, this paper proposes an efficient local matching method based on an adaptive exponentially weighted moving average filter and simple linear iterative clustering segmentation algorithm. First, an effective matching cost is introduced to adaptively integrate absolute intensity difference with Census transform, which is robust against texture free and luminance variate. Following this, during the cost aggregation, the exponentially weighted moving average filter and the SLIC segmentation are combined to handle the problems of computing consumption and adaptive expansion of the cost aggregation window. Finally, the dense disparity map is obtained by a winner-takes-all approach and disparity refinement. To demonstrate its efficiency and validity, the method is quantitatively tested and compared to existing approaches on the Middlebury benchmark. The results show that it has a non-occlusion accuracy of 90.66% and an average runtime of 7.01 s on the 2014 Middlebury dataset. Compared with existing competitive methods, the proposed method achieves superior matching results with a lower time cost.},
  archive      = {J_IETIP},
  author       = {Shan Yang and Xinyue Lei and Zhenfeng Liu and Guorong Sui},
  doi          = {10.1049/ipr2.12140},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1722-1732},
  shortjournal = {IET Image Process.},
  title        = {An efficient local stereo matching method based on an adaptive exponentially weighted moving average filter in SLIC space},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient recurrent attention network for remote sensing
scene classification. <em>IETIP</em>, <em>15</em>(8), 1712–1721. (<a
href="https://doi.org/10.1049/ipr2.12139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene classification for remote sensing is a popular topic, and many recent convolutional neural networks (CNNs)-based methods have shown the great model capacity and learning ability of highly discriminative features. Given a large number of training data, CNN can extract extensive features and learn to predict a remote sensing image. However, for supervised learning tasks, deep models often rely on a large number of labelled remote sensing images, which are difficult to pre-process. Thus, training a lightweight deep learning model is essential. Easy-classified and hard samples may also cause an imbalance of training set and lead the model to overwhelm the loss function. Accordingly, a novel Efficient Recurrent Attention Network (ERANet) for remote sensing scene classification is proposed. Different from traditional deep learning methods, Efficientnet-B0 is introduced as a lightweight backbone for the ARCNet framework, replacing the original one. By applying the modified efficient backbone, the low Floating Point Operations (FLOPs) and parameter numbers of the proposed ERANet are maintained. The significance of focal loss is determined and applied to address the sample imbalance problem and yield a desirable performance. Extensive experiments on several challenging remote sensing scene classification data sets prove the efficiency of the proposed ERANet.},
  archive      = {J_IETIP},
  author       = {Le Liang and Guoli Wang},
  doi          = {10.1049/ipr2.12139},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1712-1721},
  shortjournal = {IET Image Process.},
  title        = {Efficient recurrent attention network for remote sensing scene classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel denoising algorithm for medical images based on the
non-convex non-local similar adaptive regularization. <em>IETIP</em>,
<em>15</em>(8), 1702–1711. (<a
href="https://doi.org/10.1049/ipr2.12138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation is a powerful statistical image modelling technique and has been successfully applied to image denoising. For a given patch, a non-convex non-local similarity adaptive method is adopted for sparse representation of images. First, it uses the autoregressive model to perform dictionary learning from sample patch datasets. Second, the sparse representation of an image introduces non-convex non-local self-similarity as the regularization term. In order to make better use of the sparse regularization method for image denoising, the parameters used in this study are estimated using adaptive methods. This model is more efficient and accurate, Compared with K-means singular value decomposition (KSVD) algorithm, a generalized K-means clustering method, total variation of population sparsity (GSTV) algorithm, adaptive sparse domain selection (ASDS) algorithm, forward denoising convolutional neural network (DnCNNs), a fast and flexible Convolutional Neural Network image denoising method (FFNNet) and operator-splitting algorithm to minimize the Euler elastica functional (OSEEF). Image noise-reduction experiments confirmed that using the adaptive regularization method, the results in peak signal to noise ratio (PSNR) and visual opinion are better than other algorithms.},
  archive      = {J_IETIP},
  author       = {Lin Tian and Jiaqing Miao and Xiaobing Zhou and Chao Wang},
  doi          = {10.1049/ipr2.12138},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1702-1711},
  shortjournal = {IET Image Process.},
  title        = {A novel denoising algorithm for medical images based on the non-convex non-local similar adaptive regularization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical image fusion and noise suppression with
fractional-order total variation and multi-scale decomposition.
<em>IETIP</em>, <em>15</em>(8), 1688–1701. (<a
href="https://doi.org/10.1049/ipr2.12137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusion and noise suppression of medical images are becoming increasingly difficult to be ignored in image processing, and this technique provides abundant information for the clinical diagnosis and treatment. This paper proposes a medical image fusion and noise suppression model in pixel level. This model decomposes the original image into a noiseless base layer, a large-scale noiseless detail layer and a small-scale detail layer which contains details and noise information. The fractional-order derivative and saliency detection are used to construct the weight functions to fuse the base layers. The proposed total variation model combines the fractional-order derivative to fuse the small-scale detail layers. The mathematical properties and time complexity of the total variation model are also analysed. And choose-max method is used to fuse the large-scale detail medical layers simply. Our approach is based on fractional-order derivative, which enables keep more information and decrease blocky effects more effectively compared with the integer-order derivative. To verify the validity, the proposed method is compared with some fusion methods in the subjective and objective aspects. Experiments show that the proposed model fuses the source information fully and decreases noise cleanly.},
  archive      = {J_IETIP},
  author       = {Xuefeng Zhang and Hui Yan},
  doi          = {10.1049/ipr2.12137},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1688-1701},
  shortjournal = {IET Image Process.},
  title        = {Medical image fusion and noise suppression with fractional-order total variation and multi-scale decomposition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A plexus-convolutional neural network framework for fast
remote sensing image super-resolution in wavelet domain. <em>IETIP</em>,
<em>15</em>(8), 1679–1687. (<a
href="https://doi.org/10.1049/ipr2.12136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite image processing has been widely used in recent years in a number of applications such as land classification, Identification transfer, resource exploration, super-resolution image, etc. Due to the orbital location, revision time, quick view angle limitations, and weather impact, the satellite images are challenging to manage. There are many types of resolution, such as spatial, spectral, and temporal. Still, in our case, we concentrated on spatial image resolution to super resolve the images from low-resolution images. For remote sensing image super-resolution fast wavelet-based super-resolution (FWSR), we propose a novel, fast wavelet-based plexus framework that performs super-resolution convolutional neural network (SRCNN)-like extraction of features based on three hidden layers. First, wavelet sub-band images are combined into a pre-defined full-scale data training factor, including approximation and interchangeable stand-alone units (frequency sub-bands). Second, to speed up image recovery, mapping the sub-band image of the wavelet is then measured using its approximate image. Third, the added sub-pixel layer at the end of the network model is intended to reproduce image quality using a plexus framework. The approximation sub-band images obtained after discrete wavelet transform wavelet decomposition are used as input rather than the original image because of their high-frequency data and preserved characteristics. Five current super-resolution neural network approaches are compared with the proposed technique and tested on three pubic satellite image datasets and two benchmark datasets. The experimental findings are well compared qualitatively and quantitatively.},
  archive      = {J_IETIP},
  author       = {Farah Deeba and Yuanchun Zhou and Fayaz Ali Dharejo and Muhammad Ashfaq Khan and Bhagwan Das and Xuezhi Wang and Yi Du},
  doi          = {10.1049/ipr2.12136},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1679-1687},
  shortjournal = {IET Image Process.},
  title        = {A plexus-convolutional neural network framework for fast remote sensing image super-resolution in wavelet domain},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based video object segmentation algorithm.
<em>IETIP</em>, <em>15</em>(8), 1668–1678. (<a
href="https://doi.org/10.1049/ipr2.12135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the segmentation performance on videos with large object motion or deformation, a novel scheme is proposed which has two branches. In one branch, the attention mechanism is first utilized to highlight objects-related features. Then, to well consider the temporal coherence of videos, Conv3D is integrated to capture short-term temporal features, and the designed attention residual convolutional long–short-term memory is adopted to capture the long–short-term temporal information of objects under the interference of redundant video frames. Meanwhile, considering the negative effect of background motion, in another branch, the optical flow-based prediction model is introduced to predict objects regions in subsequent video frames with the annotated initial frame. At last, based on the fused results of two branches, the global thresholds and noising area clean method are employed to obtain segmented objects. The experiments on DAVIS2016 and CDnet2014 exhibit the competitive performance of the proposed scheme.},
  archive      = {J_IETIP},
  author       = {Ying Cao and Lijuan Sun and Chong Han and Jian Guo},
  doi          = {10.1049/ipr2.12135},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1668-1678},
  shortjournal = {IET Image Process.},
  title        = {Attention-based video object segmentation algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video super-resolution with non-local alignment network.
<em>IETIP</em>, <em>15</em>(8), 1655–1667. (<a
href="https://doi.org/10.1049/ipr2.12134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution (VSR) aims at recovering high-resolution frames from their low-resolution counterparts. Over the past few years, deep neural networks have dominated the video super-resolution task because of its strong non-linear representational ability. To exploit temporal correlations, most deep neural networks have to face two challenges: (1) how to align consecutive frames containing motions, occlusions and blurring, and establish accurate temporal correspondences, (2) how to effectively fuse aligned frames and balance their contributions. In this work, a novel video super-resolution network, named NLVSR, is proposed to solve above problems in an efficient and effective manner. For alignment, a temporal-spatial non-local operation is employed to align each frame to the reference frame. Compared with existing alignment approaches, the proposed temporal-spatial non-local operation is able to integrate the global information of each frame by a weighted sum, leading to a better performance in alignment. For fusion, an attention-based progressive fusion framework was designed to integrate aligned frames gradually. To penalize the points with low-quality in aligned features, an attention mechanism was employed for a robust reconstruction. Experimental results demonstrate the superiority of the proposed network in terms of quantitative and qualitative evaluation, and surpasses other state-of-the-art methods by 0.33 dB at least.},
  archive      = {J_IETIP},
  author       = {Chao Zhou and Can Chen and Fei Ding and Dengyin Zhang},
  doi          = {10.1049/ipr2.12134},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1655-1667},
  shortjournal = {IET Image Process.},
  title        = {Video super-resolution with non-local alignment network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual-attention v-network for pulmonary lobe segmentation
in CT scans. <em>IETIP</em>, <em>15</em>(8), 1644–1654. (<a
href="https://doi.org/10.1049/ipr2.12133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliable and automatic segmentation of pulmonary lobes in computed tomography scans is an important pre-condition for the diagnosis, assessment, and treatment of lung diseases. However, due to the incomplete lobar structures and morphological changes caused by diseases, the lobe segmentation still encounters great challenges. Recently, convolution neural network has exerted a tremendous impact on medical image analysis. Nevertheless, the basic convolution operations mainly obtain local features that are insufficient for accurate lobe segmentation. The idea that the global features are equally crucial especially when lesions appear is considered. Here, a dual-attention V-network named DAV-Net for pulmonary lobe segmentation is proposed. First, a novel dual-attention module to capture global contextual information and model the semantic dependencies in spatial and channel dimensions is introduced. Second, a progressive output scheme is used to avoid the vanishing gradient phenomenon and obtain relatively effective features in hidden layers. Finally, an improved combo loss is devised to address input and output lobe imbalance problem during training and inference. In the evaluation using the LUNA16 dataset and our in-house dataset, the proposed DAV-Net obtains Dice similarity coefficients of 0.947 and 0.934, respectively; these values are superior to those obtained by existing methods.},
  archive      = {J_IETIP},
  author       = {Shaohua Zheng and Weiyu Nie and Lin Pan and Bin Zheng and Zhiqiang Shen and Liqin Huang and Chenhao Pei and Yuhang She and Liuqing Chen},
  doi          = {10.1049/ipr2.12133},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1644-1654},
  shortjournal = {IET Image Process.},
  title        = {A dual-attention V-network for pulmonary lobe segmentation in CT scans},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A full-reference stereoscopic image quality assessment index
based on stable aggregation of monocular and binocular visual features.
<em>IETIP</em>, <em>15</em>(8), 1629–1643. (<a
href="https://doi.org/10.1049/ipr2.12132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stereoscopic image quality assessment, human visual system has been universally taken into account to detect perceptual characteristics. A novel full-reference stereoscopic image assessment metric by considering both monocular and binocular visual features of human visual system is proposed. In particular, a new region segmentation algorithm is firstly proposed to divide 3D images into occluded and non-occluded regions. The just noticeable difference model is employed on the occluded regions to formulate the monocular vision, while the binocular just noticeable difference model is applied to the non-occluded regions to reveal the binocular vision of the human visual system. In the proposed region segmentation, disparity information and Euclidean distance between stereo pairs are both adopted to solve the unstable segmentation problem of traditional methods. A new pooling strategy based on global edge features is then presented to aggregate the just noticeable difference and binocular just noticeable difference evaluation maps. In addition, some local image features as supplementary of just noticeable difference to describe visual characteristics of the human visual system are also extracted. Finally, an overall quality score is calculated based on the above-mentioned features to measure the visual quality of distorted stereo pairs. Experimental results show that the proposed metric achieves high consistency with the human visual system, and outperforms state-of-the-art algorithms on stereoscopic image quality assessment.},
  archive      = {J_IETIP},
  author       = {Jianwei Si and Huan Yang and Baoxiang Huang and Zhenkuan Pan and Honglei Su},
  doi          = {10.1049/ipr2.12132},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1629-1643},
  shortjournal = {IET Image Process.},
  title        = {A full-reference stereoscopic image quality assessment index based on stable aggregation of monocular and binocular visual features},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral face recognition with a spatial information
fusion for local dynamic texture patterns and collaborative
representation classifier. <em>IETIP</em>, <em>15</em>(8), 1617–1628.
(<a href="https://doi.org/10.1049/ipr2.12131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral face recognition provides improved classification rates due to its abundant information in the face cubes of every subject in hyperspectral face databases. However, while offering excellent opportunities, it also brings new challenges, such as low signal-to-noise ratio, interband misalignment, and high data dimensionality. Based on these ad hoc problems, literature has already proposed some optimisation methods including dimensionality reduction, image denoising, and alignment to perform face recognition, yet lacking comprehensive evaluation. This paper proposes a novel hyperspectral face recognition algorithm that is based on spatial information fusion for feature extraction (histogram of local dynamic texture patterns) and collaborative representation classifier for classification. Meanwhile, the algorithm is applied to three popular hyperspectral face databases, Carnegie Mellon University (CMU)-hyperspectral face database (HSFD), University of Western Australia (UWA)-HSFD, and Hong Kong Polytechnic University (PolyU)-HSFD databases. Experimental results demonstrate that CMU-HSFD and UWA-HSFD databases achieve very competitive classification results. PolyU-HSFD database also achieves rather good classification rates. The best recognition results are 98.5% ± 0.95, 96.6% ± 0.98, and 94.0% ± 2.86 for CMU-HSFD, UWA-HSFD and PolyU-HSFD, respectively. It demonstrates experimentally that this algorithm can be used to recognise faces. Moreover, we compared eight existing state-of-the-art face recognition techniques with our proposed method in performing hyperspectral face recognition. In this research, we formulate hyperspectral face recognition as an image-set classification problem and evaluate the performances compared with other kinds of algorithms. Comparisons with the eight existing hyperspectral face recognition techniques on three standard datasets show that the proposed algorithm outperforms most other state-of-the-art algorithms, indicating that it is a promising approach for hyperspectral face recognition.},
  archive      = {J_IETIP},
  author       = {Min Hao and Guangyuan Liu and Desheng Xie},
  doi          = {10.1049/ipr2.12131},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1617-1628},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral face recognition with a spatial information fusion for local dynamic texture patterns and collaborative representation classifier},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bilateral attention network for semantic segmentation.
<em>IETIP</em>, <em>15</em>(8), 1607–1616. (<a
href="https://doi.org/10.1049/ipr2.12129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing network feature representation capabilities and reducing the loss of image details have become the focus of semantic segmentation task. This work proposes the bilateral attention network for semantic segmentation. The authors embed two attention modules in the encoder and decoder structures . Specifically, high-level features of the encoder structure integrate all channel maps through dense channel relationships learned by the channel correlation coefficient attention module. The positively correlated channels promote each other, and the negatively correlated channels suppress each other. In the decoder structure, low-level features selectively emphasize the edge detail information in the feature map through the position attention module. The feature expression of semantic segmentation is improved by feature fusion of the two attention modules to obtain more accurate segmentation results . Finally, to verify the effectiveness of the model, the authors conduct experiments on the PASCAL VOC 2012 and Cityscapes scene analysis benchmark data sets and achieve a mean intersection-over-union of 74.92% and 66.63%, respectively.},
  archive      = {J_IETIP},
  author       = {Dongli Wang and Nanjun Li and Yan Zhou and Jinzhen Mu},
  doi          = {10.1049/ipr2.12129},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1607-1616},
  shortjournal = {IET Image Process.},
  title        = {Bilateral attention network for semantic segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Colour image segmentation based on a convex k-means
approach. <em>IETIP</em>, <em>15</em>(8), 1596–1606. (<a
href="https://doi.org/10.1049/ipr2.12128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a fundamental and challenging task in image processing and computer vision. The colour image segmentation is attracting more attention as the colour image provides more information than the grey image. A variational model based on a convex K-means approach to segment colour images is proposed. The proposed variational method uses a combination of l 1 and l 2 regularizers to maintain edge information of objects in images while overcoming the staircase effect. Meanwhile, our one-stage strategy is an improved version based on the smoothing and thresholding strategy, which contributes to improving the accuracy of segmentation. The proposed method performs the following steps. First, the colour set which can be determined by human or the K-means method is specified. Second, a variational model to obtain the most appropriate colour for each pixel from the colour set via convex relaxation and lifting is used. The Chambolle–Pock algorithm and simplex projection are applied to solve the variational model effectively. Experimental results and comparison analysis demonstrate the effectiveness and robustness of the method.},
  archive      = {J_IETIP},
  author       = {Tingting Wu and Xiaoyu Gu and Jinbo Shao and Ruoxuan Zhou and Zhi Li},
  doi          = {10.1049/ipr2.12128},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1596-1606},
  shortjournal = {IET Image Process.},
  title        = {Colour image segmentation based on a convex K-means approach},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative multi-scale residual network for deblurring.
<em>IETIP</em>, <em>15</em>(8), 1583–1595. (<a
href="https://doi.org/10.1049/ipr2.12127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dynamic scene deblurring, recent neural network–based methods have been very successful. But with the improvement of deep deblurring performance, network structure and learning become more complicated. Compared with large-scale network parameters and complex network structures, an iterative multi-scale residual network to achieve a more effective parameter sharing scheme is proposed. In each iterative unit, fast multi-scale residual blocks to replace superimposed convolutional layers or classic residual blocks are used. On the basis of preventing model overfitting, the receptive field of the network is increased. At the same time, the gated recurrent unit is introduced to connect modules of different stages. The model does not rely on the estimation of the blur kernel and directly generates sharp images in an end-to-end manner. The experimental structure on the benchmark dataset and real-world images showed that this method has better quality than the existing methods in terms of large-scale blur and subjective perception effects, both in quantitative and qualitative terms.},
  archive      = {J_IETIP},
  author       = {Tianlin Zhang and Jinjiang Li and Zhen Hua},
  doi          = {10.1049/ipr2.12127},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1583-1595},
  shortjournal = {IET Image Process.},
  title        = {Iterative multi-scale residual network for deblurring},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localised edge-region-based active contour for medical image
segmentation. <em>IETIP</em>, <em>15</em>(7), 1567–1582. (<a
href="https://doi.org/10.1049/ipr2.12126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting the region of interest (ROI) from medical images is a fundamental but challenging task due to the illumination change and imaging devices. Although many models based on the local region-based active contour model (LR-ACM) are proposed to deal with intensity inhomogeneity, it is still difficult for the global energy-based ACM with local image information to accurately extract the ROI from medical images. To solve this problem, this study proposes a novel localised ACM by constructing the gradient information based on the probability scores from the fuzzy k- nearest neighbour classifier. Different from the traditional LR-ACMs, our model is based on local rather than global image statistics, where a probability score-based edge detector is directly used for gradient information. The energy functional consists of localised region energy and an edge energy. By introducing a local characteristics function, the localised region energy with the probability-score-based edge information is formulated, which can make the evolution curve stop on the exact boundaries of ROI. The edge energy including the regularisation and the penalty terms is used to avoid the reinitialisation process and smooth the evolution curve during evolution. To maintain the signed distance property of the evolution curve, a novel potential function in the penalty term is designed, which can consistently control the diffusion direction of the evolution curve. Experiments on the medical images including the cardiac magnetic resonance imaging and the 3DIRCADb databases demonstrate that the proposed model is more robust and accurate to extract the ROI than the popular localised and region-based ACMs. The code is available at: https://github.com/HuaxiangLiu/Localized_ERAC/ .},
  archive      = {J_IETIP},
  author       = {Hua-Xiang Liu and Jiang-Xiong Fang and Zi-Jian Zhang and Yong-Cheng Lin},
  doi          = {10.1049/ipr2.12126},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1567-1582},
  shortjournal = {IET Image Process.},
  title        = {Localised edge-region-based active contour for medical image segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient framework for deep learning-based light-defect
image enhancement. <em>IETIP</em>, <em>15</em>(7), 1553–1566. (<a
href="https://doi.org/10.1049/ipr2.12125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enhancement of light-defect images such as extremely low-light, low-light and dim-light has always been a research hotspot. Most of the existing methods are excellent in specific illuminations, and there is much room for improvement in processing light-defect images with different illuminations. Therefore, this study proposes an efficient framework based on deep learning to enhance various light-defect images. The proposed framework estimates the reflectance component and illumination component. Next, we propose a generator guided by an attention mechanism in the reflectance part to repair the light-defect in the dark. In addition, we design a colour loss function for the problem of colour distortion in the enhanced images. Finally, the illumination map of the light-defect images is adjusted adaptively. Extensive experiments are conducted to demonstrate that our method can not only deal with the images with different illuminations but also enhance the images with clearer details and richer colours. At the same time, we prove its superiority by comparing it with state-of-the-art methods under both visual quality comparison and quantitative comparison of various datasets and real-world images.},
  archive      = {J_IETIP},
  author       = {Chengxu Ma and Daihui Li and Shangyou Zeng and Junbo Zhao and Hongyang Chen},
  doi          = {10.1049/ipr2.12125},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1553-1566},
  shortjournal = {IET Image Process.},
  title        = {An efficient framework for deep learning-based light-defect image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative adversarial network for low-light image
enhancement. <em>IETIP</em>, <em>15</em>(7), 1542–1552. (<a
href="https://doi.org/10.1049/ipr2.12124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement is rapidly gaining research attention due to the increasing demands of extreme visual tasks in various applications. Although numerous methods exist to enhance image qualities in low light, it is still undetermined how to trade-off between the human observation and computer vision processing. In this work, an effective generative adversarial network structure is proposed comprising both the densely residual block (DRB) and the enhancing block (EB) for low-light image enhancement. Specifically, the proposed end-to-end image enhancement method, consisting of a generator and a discriminator, is trained using the hyper loss function. The DRB adopts the residual and dense skip connections to connect and enhance the features extracted from different depths in the network while the EB receives unique multi-scale features to ensure feature diversity. Additionally, increasing the feature sizes allows the discriminator to further distinguish between fake and real images from the patch levels. The merits of the loss function are also studied to recover both contextual and local details. Extensive experimental results show that our method is capable of dealing with extremely low-light scenes and the realistic feature generator outperforms several state-of-the-art methods in a number of qualitative and quantitative evaluation tests.},
  archive      = {J_IETIP},
  author       = {Fei Li and Jiangbin Zheng and Yuan-fang Zhang},
  doi          = {10.1049/ipr2.12124},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1542-1552},
  shortjournal = {IET Image Process.},
  title        = {Generative adversarial network for low-light image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Level set method with retinex-corrected saliency embedded
for image segmentation. <em>IETIP</em>, <em>15</em>(7), 1530–1541. (<a
href="https://doi.org/10.1049/ipr2.12123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It can be a very challenging task when using level set method segmenting natural images with high intensity inhomogeneity and complex background scenes. A new synthesis level set method for robust image segmentation based on the combination of Retinex-corrected saliency region information and edge information is proposed in this work. First, the Retinex theory is introduced to correct the saliency information extraction. Second, the Retinex-corrected saliency information is embedded into the level set method due to its advantageous quality which makes a foreground object stand out relative to the backgrounds. Combined with the edge information, the boundary of segmentation will be more precise and smooth. Experiments indicate that the proposed segmentation algorithm is efficient, fast, reliable, and robust.},
  archive      = {J_IETIP},
  author       = {Dongmei Liu and Faliang Chang and Huaxiang Zhang and Li Liu},
  doi          = {10.1049/ipr2.12123},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1530-1541},
  shortjournal = {IET Image Process.},
  title        = {Level set method with retinex-corrected saliency embedded for image segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An exclusive-disjunction-based detection of
neovascularisation using multi-scale CNN. <em>IETIP</em>,
<em>15</em>(7), 1518–1529. (<a
href="https://doi.org/10.1049/ipr2.12122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an exclusive-disjunction-based detection of neovascularisation (NV), which is the formation of new blood vessels on the retinal surfaces, is presented. These vessels, being thin and fragile, get ruptured easily leading to permanent blindness. The proposed algorithm consists of two stages. In the first stage, the retinal images are classified into non-NV and NV using multi-scale convolutional neural network, while in the second stage, 13 relevant features are extracted from the vascular map of NV images to achieve the pixel locations of new blood vessels using a directional matched filter along with the Difference of Laplacian of Gaussian operator followed by an exclusive disjunction function with adaptive thresholding of the vascular map. At the same time, the pixel locations of optic disc (OD) are detected using intensity distribution and variations on the retinal images. Finally, the pixel locations of both new blood vessels and OD are compared for classification. If the pixel locations of new blood vessels fall inside the OD, they are labelled as NV on OD, else they are labelled as NV elsewhere. The proposed algorithm has achieved an accuracy of 99.5%, specificity of 97.5%, sensitivity of 98.9%, and area under the curve of 94.2% when tested on 155 non-NV and 115 NV images.},
  archive      = {J_IETIP},
  author       = {Geetha Pavani P and Birendra Biswal and M V S Sairam and Pradyut Kumar Biswal},
  doi          = {10.1049/ipr2.12122},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1518-1529},
  shortjournal = {IET Image Process.},
  title        = {An exclusive-disjunction-based detection of neovascularisation using multi-scale CNN},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error feedback denoising network. <em>IETIP</em>,
<em>15</em>(7), 1508–1517. (<a
href="https://doi.org/10.1049/ipr2.12121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks have been successfully used for image denoising due to their favourable performance. This paper examines the error feedback mechanism to image denoising and propose an error feedback denoising network. Specifically, we use the down-and-up projection sequence to estimate the noise feature. By the residual connection, the clean structures are removed from the noise features. The essential difference between the proposed network and other existing feedback networks is the projection sequence. Our error feedback projection sequence is down-and-up, which is more suitable for image denoising than the existing up-and-down order. Moreover, we design a compression block to improve the expression ability of the general 1 1 convolutional compression layer. The advantage of our well-designed down-and-up block is that the network parameters are fewer than other feedback networks and the receptive field is enlarged. We have implemented our error feedback denoising network on denoising and JPEG image deblocking. Extensive experiments verify the effectiveness of our down-and-up block and demonstrate that our error feedback denoising network is comparable with the state-of-the-art. The code will be open source. The source codes for reproducing the results can be found at: https://github.com/Houruizhi/EFDN.},
  archive      = {J_IETIP},
  author       = {Ruizhi Hou and Fang Li},
  doi          = {10.1049/ipr2.12121},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1508-1517},
  shortjournal = {IET Image Process.},
  title        = {Error feedback denoising network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image stitching method by multi-feature constrained
alignment and colour adjustment. <em>IETIP</em>, <em>15</em>(7),
1499–1507. (<a href="https://doi.org/10.1049/ipr2.12120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image alignment and colour consistency are two challenging tasks for image stitching. Traditional point correspondence methods are difficult to achieve good alignments due to their insufficiency and unreliability. The results are prone to errors and distortions. On the other hand, the problem of colour inconsistency in overlapping area between image pairs is still difficult to solve, especially when the illumination difference between images is large. To solve these problems, the authors integrate point features and line features into a warping model through a designed energy function. Line features will provide geometric constraints for image stitching, and remedy the defect of point correspondences in low-textured image stitching. A global colour consistency optimization method with colour mapping via a histogram extreme point-matching algorithm is proposed. The colour characteristic of reference images will be transferred to the others to achieve a global colour consistency. The proposed method is evaluated on a series of images, and compared with other methods. The experiments demonstrate that the proposed method provides convincing stitching results and achieves satisfied colour consistency results.},
  archive      = {J_IETIP},
  author       = {Xingsheng Yuan and Yongbin Zheng and Wei Zhao and Jiongming Su and Jianzhai Wu},
  doi          = {10.1049/ipr2.12120},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1499-1507},
  shortjournal = {IET Image Process.},
  title        = {Image stitching method by multi-feature constrained alignment and colour adjustment},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised automated retinal vessel segmentation based on
radon line detector and morphological reconstruction. <em>IETIP</em>,
<em>15</em>(7), 1484–1498. (<a
href="https://doi.org/10.1049/ipr2.12119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal blood vessel segmentation and analysis is critical for the computer-aided diagnosis of different diseases such as diabetic retinopathy. This study presents an automated unsupervised method for segmenting the retinal vasculature based on hybrid methods. The algorithm initially applies a preprocessing step using morphological operators to enhance the vessel tree structure against a non-uniform image background. The main processing applies the Radon transform to overlapping windows, followed by vessel validation, vessel refinement and vessel reconstruction to achieve the final segmentation. The method was tested on three publicly available datasets and a local database comprising a total of 188 images. Segmentation performance was evaluated using three measures: accuracy, receiver operating characteristic (ROC) analysis, and the structural similarity index. ROC analysis resulted in area under curve values of 97.39%, 97.01%, and 97.12%, for the DRIVE, STARE, and CHASE-DB1, respectively. Also, the results of accuracy were 0.9688, 0.9646, and 0.9475 for the same datasets. Finally, the average values of structural similarity index were computed for all four datasets, with average values of 0.9650 (DRIVE), 0.9641 (STARE), and 0.9625 (CHASE-DB1). These results compare with the best published results to date, exceeding their performance for several of the datasets; similar performance is found using accuracy.},
  archive      = {J_IETIP},
  author       = {Meysam Tavakoli and Alireza Mehdizadeh and Reza Pourreza Shahri and Jamshid Dehmeshki},
  doi          = {10.1049/ipr2.12119},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1484-1498},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised automated retinal vessel segmentation based on radon line detector and morphological reconstruction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid feature descriptor with jaya optimised least
squares SVM for facial expression recognition. <em>IETIP</em>,
<em>15</em>(7), 1471–1483. (<a
href="https://doi.org/10.1049/ipr2.12118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition has been a long-standing problem in the field of computer vision. This paper proposes a new simple scheme for effective recognition of facial expressions based on a hybrid feature descriptor and an improved classifier. Inspired by the success of stationary wavelet transform in many computer vision tasks, stationary wavelet transform is first employed on the pre-processed face image. The pyramid of histograms of orientation gradient features is then computed from the low-frequency stationary wavelet transform coefficients to capture more prominent details from facial images. The key idea of this hybrid feature descriptor is to exploit both spatial and frequency domain features which at the same time are robust against illumination and noise. The relevant features are subsequently determined using linear discriminant analysis. A new least squares support vector machine parameter tuning strategy is proposed using a contemporary optimisation technique called Jaya optimisation for classification of facial expressions. Experimental evaluations are performed on Japanese female facial expression and the Extended Cohn–Kanade (CK+) datasets, and the results based on 5-fold stratified cross-validation test confirm the superiority of the proposed method over state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Nikunja Bihari Kar and Deepak Ranjan Nayak and Korra Sathya Babu and Yu-Dong Zhang},
  doi          = {10.1049/ipr2.12118},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1471-1483},
  shortjournal = {IET Image Process.},
  title        = {A hybrid feature descriptor with jaya optimised least squares SVM for facial expression recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Window-aware guided image filtering via local entropy.
<em>IETIP</em>, <em>15</em>(7), 1459–1470. (<a
href="https://doi.org/10.1049/ipr2.12117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided image filtering is one of the widely used techniques in computer vision. However, it commonly leads to over-smoothed edges and a distorted appearance when tackling intricate texture patterns and complex noise. In this paper, a window-aware image filtering framework based on the bilateral filter guided by the local entropy is presented. The key idea of the authors&#39; proposed approach is to design a novel guidance input and a non-box filtering window. Specifically, using the Gaussian spatial kernel and the local entropy, a GEF that can maintain image feature details and yield a robust guidance input for BF is constructed. Meanwhile, based on an intensity-similar strategy, the local non-box filtering window is designed for the further preservation of edge structures. The authors&#39; approach not only inherits the advantages of bilateral filter i.e. simplicity, parallelisation and easiness of programming, but also is more powerful than bilateral filter and its variants. In addition, the guided entropy filter and the non-box window can also be transplanted to other local filters and can effectively improve the filtering effects. The qualitative and quantitative experimental results demonstrate that the authors&#39; approach has good performance in image denoising, texture (or background) smoothing, edge extraction and other applications in image processing.},
  archive      = {J_IETIP},
  author       = {Chong Liu and Cui Yang and Jun Wang},
  doi          = {10.1049/ipr2.12117},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1459-1470},
  shortjournal = {IET Image Process.},
  title        = {Window-aware guided image filtering via local entropy},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A level-set method for inhomogeneous image segmentation with
application to breast thermography images. <em>IETIP</em>,
<em>15</em>(7), 1439–1458. (<a
href="https://doi.org/10.1049/ipr2.12116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various level-set methods have been suggested for segmenting images with intensity inhomogeneity as local region-based models. The challenge in these methods is segmenting the inhomogeneous images with smooth edges. These methods cannot properly segment regions with smooth edges in inhomogeneous images. This paper presents a new local region-based active contour model called local self-weighted active contour model. In the proposed method, a novel different weighting technique is applied. In this model, the weight of each neighbour pixel in the energy function is set by a function of its intensity and not its geometrical distance regarding the central pixel as previous methods. Considering this, the presented approach can segment regions with smooth edges in the presence of inhomogeneity as breast thermography images. The experimental results of applying the model on heterogeneous images containing synthetic images and medical images, especially breast thermography images, are compared with well-known local level-set methods which show the perfect capability of the model. The segmentation results were evaluated using the F-score, accuracy, precision and recall criteria. The results show values of 0.8, 0.62, 0.73 and 0.82 for the average accuracy, F-score, precision and recall criteria on the segmentation of breast thermography images, respectively.},
  archive      = {J_IETIP},
  author       = {Asma Shamsi Koshki and M.R. Ahmadzadeh and M. Zekri and S. Sadri and E. Mahmoudzadeh},
  doi          = {10.1049/ipr2.12116},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1439-1458},
  shortjournal = {IET Image Process.},
  title        = {A level-set method for inhomogeneous image segmentation with application to breast thermography images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive level set method based on joint estimation
dealing with intensity inhomogeneity. <em>IETIP</em>, <em>15</em>(7),
1424–1438. (<a href="https://doi.org/10.1049/ipr2.12115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic object segmentation has been a challenging task due to intensity inhomogeneity. The traditional way is to eliminate the intensity inhomogeneity, which causes the object to lose useful intensity information. The authors propose an adaptive level set method for the segmentation of intensity inhomogeneous images. Firstly, global and local features are utilised to collaboratively estimate the image, which devotes to compensating for intensity inhomogeneity. The local estimation retains detailed spatial information, and the global estimation mainly contains the regional information of the partitioned object. Then, during the construction of the energy functional, joint estimation is introduced to create the external energy. To acquire the precise location of the boundary, a weighting factor indicated by the gradient is introduced into the internal energy. Finally, after the numerical calculation of the energy functional by additive operator splitting algorithm, this method achieves the desired performance in terms of accuracy and robustness. Experimental results verify this method outperforms the comparative methods and can be applied to many real-world scenarios.},
  archive      = {J_IETIP},
  author       = {Jiang Zhu and Yan Zeng and Jianqi Li and Shujuan Tian and Haolin Liu},
  doi          = {10.1049/ipr2.12115},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1424-1438},
  shortjournal = {IET Image Process.},
  title        = {An adaptive level set method based on joint estimation dealing with intensity inhomogeneity},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fusion-based simultaneous estimation of reflectance and
illumination for low-light image enhancement. <em>IETIP</em>,
<em>15</em>(7), 1410–1423. (<a
href="https://doi.org/10.1049/ipr2.12114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement is a challenging field in image processing. Retinex-based methods perform well for low-light images. However, reflectance and illumination estimation is an ill-posed problem. This paper presents a new framework for the simultaneous estimation of reflectance and illumination for low-light image enhancement. The algorithm estimates multiple instances of illumination and reflectance and blends them to estimate the final components. The proposed approach uses multi-scale fusion for illumination estimation and naive fusion for reflectance estimation. Extensive experimentation and analysis with a large set of low-light images validates the performance of the proposed approach. The comparison shows the superiority of the proposed approach over most of the existing low-light image enhancement methods. The proposed method provides colour constancy in low-light image enhancement and preserves the naturalness of the image.},
  archive      = {J_IETIP},
  author       = {Anil Singh Parihar and Kavinder Singh and Hrithik Rohilla and Gul Asnani},
  doi          = {10.1049/ipr2.12114},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1410-1423},
  shortjournal = {IET Image Process.},
  title        = {Fusion-based simultaneous estimation of reflectance and illumination for low-light image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep spectral unmixing framework via 3D denoising
convolutional autoencoder. <em>IETIP</em>, <em>15</em>(7), 1399–1409.
(<a href="https://doi.org/10.1049/ipr2.12113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral unmixing is an important technique which attempts to acquire pure spectra of distinct substances (endmembers) and estimate fractional abundances from highly mixed pixels. This paper proposed a novel deep network-based framework for unmixing problem. It contains two parts: a three-dimensional convolutional autoencoder for hyperspectral denoising (denoising 3D CAE) which aims to recover data from highly noised input imagery through an unsupervised manner, and a restrictive non-negative sparse autoencoder which extracts endmembers and abundances from the scene simultaneously. The proposed denoising 3D CAE network integrates 3D operations in each layer, which allows manipulating volumetric representation of the image data directly and facilitates hierarchical exploration for latent information. Being trained with corrupted hyperspectral image data, the denoising 3D CAE network has strong capacity of capturing the principle and robust local features in spatial and spectral domains efficiently, and it shows superior performance for image recovery with high noise disturbance. Moreover, a part-based non-negative autoencoder is concatenated, and the -norm penalty is imposed for sparsity enhancement of the solution. Comparative experiments are conducted both on synthetic and real-world hyperspectral data, which demonstrate the applicability and effectiveness of the proposed unmixing framework.},
  archive      = {J_IETIP},
  author       = {Peiyuan Jia and Miao Zhang and Yi Shen},
  doi          = {10.1049/ipr2.12113},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1399-1409},
  shortjournal = {IET Image Process.},
  title        = {Deep spectral unmixing framework via 3D denoising convolutional autoencoder},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed poisson gaussian noise reduction in fluorescence
microscopy images using modified structure of wavelet transform.
<em>IETIP</em>, <em>15</em>(7), 1383–1398. (<a
href="https://doi.org/10.1049/ipr2.12112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fluorescence microscopy is an important investigation tool of discoveries in the field of biological sciences where the imaging phenomena are limited by the noise. This paper introduces the integration of biorthogonal wavelet filters along with mixed Poisson-Gaussian unbiased risk estimate (MPGURE) based subband adaptive thresholding function for the restoration of low photon count microscopy images. The proposed algorithm consists of four steps. In the first step, variance stabilization transform along with a multi-scale Wiener filtering approach is used to filter out the noise and blurring effect. In the second step, deconvolved images are further decomposed by the biorthogonal wavelet filters. The modified wavelet subband structure is used for the identification of noisy and noise-free subbands. In the next stage, different noisy coefficients are thresholded using the MPGURE-based thresholding operation. The different thresholded images are combined along with different optimum coefficients. Finally, inverse variance stabilization transformation is applied to obtain the final restored output. Performance of the proposed algorithm is tested on 14 different benchmark image data sets with performance evaluation measures like signal-to-noise ratio, peak signal-to-noise ratio, mean structural similarity index measur, and correlation coefficient. Simulation results of the proposed algorithm claim better results than other state-of-the-art techniques.},
  archive      = {J_IETIP},
  author       = {Tushar Rasal and Thangaraj Veerakumar and Badri Narayan Subudhi and Sankaralingam Esakkirajan},
  doi          = {10.1049/ipr2.12112},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1383-1398},
  shortjournal = {IET Image Process.},
  title        = {Mixed poisson gaussian noise reduction in fluorescence microscopy images using modified structure of wavelet transform},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artefact-free image stitching via a better normed
seam-cutting energy function. <em>IETIP</em>, <em>15</em>(6), 1371–1381.
(<a href="https://doi.org/10.1049/ipr2.12111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image stitching, as the important field of computer graphics and vision, has received much attention in recent years. Image stitching techniques are generally decomposed into two phases: image alignment, which aligns target images with the reference images; and image composition, which fixes ghosting and visual artefacts. This work aims to propose a new strategy for the seam-cutting method which provides visually appealing result. Seam-cutting is one of the most influential methods in image composition, which can relieve artefacts and produce plausible results. However, it is observed that the state-of-the-art seam-cutting approaches usually lead to undesirable seams in some challenging scenes. Here, the authors put forward a novel seam-cutting method by defining a new energy function. This method uses power of norm as a colour difference which can magnify the weight of colour distinction to avoid undesirable seams and artefacts. The proposed method can be easily implemented. The test images are collected from the public available challenging datasets and taken by ourselves. Experiments demonstrate that the proposed method can create comparable or even better stitching results compared to other state-of-the-art seam-cutting approaches.},
  archive      = {J_IETIP},
  author       = {Xiangyan Qiu and Qiaoliang Li},
  doi          = {10.1049/ipr2.12111},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1371-1381},
  shortjournal = {IET Image Process.},
  title        = {Artefact-free image stitching via a better normed seam-cutting energy function},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards accurate coronary artery calcium segmentation with
multi-scale attention mechanism. <em>IETIP</em>, <em>15</em>(6),
1359–1370. (<a href="https://doi.org/10.1049/ipr2.12110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery calcium is a strong and independent marker of atherosclerosis and cardiovascular disease. Typically, the accurate segmentation of computed tomography images of the chest is an important prerequisite and basis for coronary artery calcium identification and analysis. However, this is very challenging in practice because the boundaries of coronary artery calcium, the small lesions with large shape variation, are very blurry, resulting in poor performance in existing studies. To tackle this challenge, we present a novel Attention-based Multi-Scale Network called AMSN, which can process information through both the main and boundary branches in parallel. Key to our AMSN is a new non-local multi-scale context encoder module, which is mainly composed of the multi-scale attention mechanism and local global long short-term memory module. By aggregating the multi-scale context information, i.e. high-resolution low-level and low-resolution high-level features, the model&#39;s feature representative capability and deployment ability are improved effectively. Besides, we introduce a new boundary preserving loss, which can consider the boundary information of all coronary artery calcium together and establish links for the segmentation of different coronary artery calcium simultaneously. Extensive experiments demonstrate our AMSN enables reliable accurate coronary artery calcium segmentation for assisted cardiovascular disease diagnosis clinically.},
  archive      = {J_IETIP},
  author       = {Yang Ning and Yunfeng Zhang and Xuemei Li and Caiming Zhang},
  doi          = {10.1049/ipr2.12110},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1359-1370},
  shortjournal = {IET Image Process.},
  title        = {Towards accurate coronary artery calcium segmentation with multi-scale attention mechanism},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Paralleled attention modules and adaptive focal loss for
siamese visual tracking. <em>IETIP</em>, <em>15</em>(6), 1345–1358. (<a
href="https://doi.org/10.1049/ipr2.12109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Siamese-based trackers have drawn amounts of attention in visual tracking field because of their excellent performance on different tracking benchmarks. However, most Siamese-based trackers encounter difficulties under circumstances such as similar objects interference and background clutters. Besides, there exists an extreme foreground–background data imbalance that weakens the performance during training but few loss functions pay attention to it. The authors intend to address the issues mentioned above by introducing a module named paralleled spatial and channel attention (PSCA) and adaptive focal loss (AFL). Firstly, paralleled spatial and channel attention is proposed to enhance the extracted features and eliminate the noise information from both spatial and channel aspects. Secondly, adaptive focal loss is proposed as the loss function to make the model focus on hard samples that contribute more to training process. Finally, paralleled spatial and channel attention and modified ResNet are combined for extracting more powerful features. Experimental results show that the authors&#39; method achieves outstanding performance in multiple benchmarks while keeping a beyond-real-time frame rate.},
  archive      = {J_IETIP},
  author       = {Yuyao Zhao and Min Jiang and Jun Kong and Sha Li},
  doi          = {10.1049/ipr2.12109},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1345-1358},
  shortjournal = {IET Image Process.},
  title        = {Paralleled attention modules and adaptive focal loss for siamese visual tracking},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selective focus saliency model driven by object
class-awareness. <em>IETIP</em>, <em>15</em>(6), 1332–1344. (<a
href="https://doi.org/10.1049/ipr2.12108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current many salient object detection (SOD) models only focus on highlighting visual conspicuous region but fail to make saliency detection for specific targets. In this paper, a selective focus saliency model driven by object class-awareness (SF-OCA) to run saliency detection is proposed. The framework consists of a visual saliency detection flow, a segmentation-classification flow, and a class-awareness selection module. It combines bottom-up visual perception with a top-down task-driven manner, which is capable of detecting specific category salient targets and eliminating the interference from other saliency areas, providing a new idea for saliency detection. Experimental results show that the method achieves comparable performance with state-of-the-art models on four public saliency datasets. In addition, a new dataset was also built to test the proposed framework for the selective focus saliency detection. Compared with other SOD methods, the method not only highlights visual saliency regions but can choose more important or more noteworthy targets in a class-awareness manner. The method also shows better robustness under a variety of conditions including multi-targets, small targets and complex background.},
  archive      = {J_IETIP},
  author       = {Danpei Zhao and Bo Yuan and Zhenwei Shi and Zhiguo Jiang},
  doi          = {10.1049/ipr2.12108},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1332-1344},
  shortjournal = {IET Image Process.},
  title        = {Selective focus saliency model driven by object class-awareness},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-resolution optical-to-SAR image registration using
mutual information and SPSA optimisation. <em>IETIP</em>,
<em>15</em>(6), 1319–1331. (<a
href="https://doi.org/10.1049/ipr2.12107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-resolution optical and synthetic aperture radar (SAR) images are widely used in many remote sensing application areas such as image fusion and change detection where image registration is a fundamental step. The latest high-resolution optical and SAR satellites and airborne systems provide geometrically corrected images which do not contain global deformations. Though the images do not have global differences, still, registration differences exist between these optical and SAR images. These registration differences should be minimised through an automatic registration method before using the images for the aforementioned applications. However, an automatic optical-to-SAR image registration is a challenging task due to the presence of significant nonlinear intensity differences as well as local geometric distortions between the images. In order to solve these problems, an automatic optical-to-SAR image registration method is proposed which can effectively handle the registration differences between the globally corrected high-resolution images. In the proposed method, initially, a coarse registration is performed by using a discrete simultaneous perturbation stochastic approximation (SPSA) optimisation. Then, a smooth continuous SPSA optimisation is utilised for the fine registration of the images. Experiments are performed on six sets of high-resolution optical-SAR image pairs and the results show the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Sourabh Paul and Umesh C. Pati},
  doi          = {10.1049/ipr2.12107},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1319-1331},
  shortjournal = {IET Image Process.},
  title        = {High-resolution optical-to-SAR image registration using mutual information and SPSA optimisation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmentation of osteosarcoma in MRI images by k-means
clustering, chan-vese segmentation, and iterative gaussian filtering.
<em>IETIP</em>, <em>15</em>(6), 1310–1318. (<a
href="https://doi.org/10.1049/ipr2.12106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike other types of tumours, automated osteosarcoma segmentation in magnetic resonance images (MRI) is a challenging task due to its different and unique intensity and texture. This paper presents a technique for segmenting osteosarcoma in MRI images using a combination of image processing techniques which include K-means clustering, Chan-Vese segmentation, iterative Gaussian filtering, and Canny edge detection. In addition, the proposed technique involves iterative morphological operations and object counting. The technique was tested using 50 MRI scan images that contain osteosarcoma tumours. The proposed technique was able to segment the osteosarcoma regardless of the variations in their intensities, textures and locations. The performance of the technique was measured by calculating the values for precision, recall, specificity, Dice score coefficient, accuracy and the running time (RT) for all tested cases. The proposed technique achieved 95.96% precision, 86.15% recall, 99.51% specificity, 89.84% Dice score coefficient, 98.02% accuracy, and 191.62 s average running time. This technique can assist clinicians in making treatment plans for patients with osteosarcoma.},
  archive      = {J_IETIP},
  author       = {Mohamed Nasor and Walid Obaid},
  doi          = {10.1049/ipr2.12106},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1310-1318},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of osteosarcoma in MRI images by K-means clustering, chan-vese segmentation, and iterative gaussian filtering},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Keypoint based comprehensive copy-move forgery detection.
<em>IETIP</em>, <em>15</em>(6), 1298–1309. (<a
href="https://doi.org/10.1049/ipr2.12105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifying the authenticity of a digital image has been challenging problem. The simplest of the image tampering tricks is the copy-move forgery. In copy-move forgery copied portion of the image is pasted on another part of the same image. Geometrical transformations are used on the copied portions of the image before pasting it for the tampered image to look realistic and visually convincing. To make it more complex, other processing approaches may also be applied in the forged region for hiding traces of forgery. These processings are the scale, rotation, JPEG compression, and AWGN. In this paper, an approach based on features of the CenSurE keypoint detector and FREAK descriptor is proposed. This combination has novelty in itself as it has never been used for this purpose before to the best of authors&#39; literature studies. CenSurE detectors are fast and give stable and accurate output even in the case of rotated images, which we club with binary descriptor FREAK. Hierarchical clustering and Neighbourhood search is applied in such a way that it can locate and detect multiple copy-move forgeries. The authors are hopeful that the proposed approach may be used in real-time image authentication and copy-move forgery detection.},
  archive      = {J_IETIP},
  author       = {Anjali Diwan and Rajat Sharma and Anil K. Roy and Suman K. Mitra},
  doi          = {10.1049/ipr2.12105},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1298-1309},
  shortjournal = {IET Image Process.},
  title        = {Keypoint based comprehensive copy-move forgery detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAR image despeckling using deep CNN. <em>IETIP</em>,
<em>15</em>(6), 1285–1297. (<a
href="https://doi.org/10.1049/ipr2.12104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) images are contaminated with noise called speckle that is multiplicative in nature. The presence of speckles in SAR images makes it impossible to understand and interpret for extensive range of applications. However, certain characteristics of SAR and the ability to function irrespective of weather conditions, are making such images worth processing in order to be able to extract relevant information. A despeckling model is proposed that uses deeper convolutional neural networks, which was never used before, as far as authors are concerned, for diminishing speckle in noisy SAR images. Multiple skip connections from the ResNet model are also employed in authors&#39; proposed architecture. In order to maintain uniformity, a formula to be followed while applying skip connections is also derived. A hybrid loss function is developed to train the network more consistently to achieve the desired output. Experiments on simulated SAR images using the NWPU-RESISC benchmark are conducted and tested on real TerraSAR-X images and compared with the state-of-the-art techniques. Results show that the proposed method achieved considerable improvements compared to state-of-the-art methods with PSNRs 27.02, 24.60, and 22.01 for looks 10, 4, and 1, respectively.},
  archive      = {J_IETIP},
  author       = {Alicia Passah and Khwairakpam Amitab and Debdatta Kandar},
  doi          = {10.1049/ipr2.12104},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1285-1297},
  shortjournal = {IET Image Process.},
  title        = {SAR image despeckling using deep CNN},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vehicle license plate recognition for fog-haze environments.
<em>IETIP</em>, <em>15</em>(6), 1273–1284. (<a
href="https://doi.org/10.1049/ipr2.12103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technique of vehicle license plate recognition can recognize and count the vehicles automatically, and thus many applications regarding the vehicles are greatly facilitated. However, the recognitions of vehicle license plates are extremely difficult especially in some fog-haze environments because the fog and haze blur the boundaries and characters of license plates significantly, which makes the license plates hard to be detected or recognised. To this end, this paper proposes a vehicle License Plate Recognition method for Fog-Haze environments (LPRFH). In LPRFH, a dark channel prior algorithm based on the local estimation of atmospheric light value is applied to dehaze the blurred images preliminarily. Then, the images are further dehazed, and the license plate regions are detected through a Joint Further-dehazing and Region-extracting Model on basis of an object detection convolution neural network. Finally, the image super-resolution is accomplished with a convolution-enhanced super-resolution convolutional neural network, and hence the characters of license plates can be recognised successfully. Extensive experiments have been conducted, and the results indicate that LPRFH can recognise the license plates accurately even in some severe fog-haze environments.},
  archive      = {J_IETIP},
  author       = {Xianli Jin and Ruocong Tang and Linfeng Liu and Jiagao Wu},
  doi          = {10.1049/ipr2.12103},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1273-1284},
  shortjournal = {IET Image Process.},
  title        = {Vehicle license plate recognition for fog-haze environments},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new blind image denoising method based on asymmetric
generative adversarial network. <em>IETIP</em>, <em>15</em>(6),
1260–1272. (<a href="https://doi.org/10.1049/ipr2.12102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a classical topic in computer vision. In recent years, with the development of deep learning, image denoising methods based on discriminative learning have received more attention. In this paper, a new blind image denoising method based on the asymmetric generative adversarial network (ID-AGAN) is proposed. In the new method, the adversarial learning is used to optimise the high-dimensional image information denoising, so as to balance the noise removal and detail retention. In order to overcome the unstability of the GAN training and improve the discriminative ability of the discriminating model, an image downsampling layer is added between the generating model and the discriminating model. Moreover, a multi-scale feature downsampling layer is utilised to extract the feature of the entire image and reducing the effect of noise on training images. Extensive experiments are conducted to verify the performance of the ID-AGAN algorithm. The results demonstrate that authors&#39; method has high performance and flexibility.},
  archive      = {J_IETIP},
  author       = {Yiming Wang and Dongxia Chang and Yao Zhao},
  doi          = {10.1049/ipr2.12102},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1260-1272},
  shortjournal = {IET Image Process.},
  title        = {A new blind image denoising method based on asymmetric generative adversarial network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA-based favourite skin colour restoration using improved
histogram equalization with variable enhancement degree and ensemble
extreme learning machine. <em>IETIP</em>, <em>15</em>(6), 1247–1259. (<a
href="https://doi.org/10.1049/ipr2.12101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents skin color enhancement based on favorite skin color to agree with user-defined favorite skin color using improved histogram equalization with variable enhancement degree (IHEwVED) and machine learning methods. The skin color to be adjusted in the input image is shifted to favorite skin color by using novel control parameters of the proposed IHEwVED method. Three different novel display device-dependent color image processing methods are introduced based on hsv and yiq color space to obtain the desired enhanced output images. A reduced convolutional neural network and the novel ensemble extreme learning machine (EELM) architectures are developed and implemented in a field-programmable gate array to test, synthesize, and validate the recognition capability of the user-defined favorite skin color. The less computational complex proposed IHEwVED-EELM method recognizes 45 to 50 favorite skin color per second of test images by consuming 0.035 second training time with training root mean square error (RMSE) of 0.0048 and testing RMSE of 0.01208. Finally, a stand-alone favorite skin color restoration system is developed using the high-speed video processor NI-PXI-1031 based on the IHEwVED-EELM method in the Python-OpenCV environment. The laboratory experimental performances ascertain the real-time ability of the proposed favorite skin color restoration method.},
  archive      = {J_IETIP},
  author       = {Mrutyunjaya Sahani and Bhanja Kishor Swain and Pradipta Kishore Dash},
  doi          = {10.1049/ipr2.12101},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1247-1259},
  shortjournal = {IET Image Process.},
  title        = {FPGA-based favourite skin colour restoration using improved histogram equalization with variable enhancement degree and ensemble extreme learning machine},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive comprehensive particle swarm optimisation-based
functional-link neural network filtre model for denoising ultrasound
images. <em>IETIP</em>, <em>15</em>(6), 1232–1246. (<a
href="https://doi.org/10.1049/ipr2.12100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplicative speckle is a dominant type of noise that spoils the inherent features of the medical ultrasound (US) images. Apart from the speckle, impulse and Gaussian noises also appear in the US image due to the error encountered during the data transmission and transition of switching circuits and sensors. The noise not only deteriorates the visual quality of the US but also creates complications in the diagnosis. In this study, an adaptive comprehensive particle swarm optimisation-based functional-link neural network (ACPSO-FLNN) filtre has been proposed and implemented in filtering noisy US images in different noise conditions. The proposed filtre is compared with some state-of-the-art filtering techniques. Quantitative and qualitative measures such as training time, time complexity, convergence rate, and statistical test are included to study the performance of the proposed filtre. Furthermore, sensitivity, computational complexity, and order of the proposed filtre are also investigated. Friedman&#39;s test with 50 images is performed for statistical validation. The lower rank, that is, 6 and critical value of 21 × 10–4 of the proposed ACPSO-FLNN filtre validates its dominance over other filtres.},
  archive      = {J_IETIP},
  author       = {Manish Kumar and Sudhansu Kumar Mishra and Justin Joseph and Sunil Kumar Jangir and Dinesh Goyal},
  doi          = {10.1049/ipr2.12100},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1232-1246},
  shortjournal = {IET Image Process.},
  title        = {Adaptive comprehensive particle swarm optimisation-based functional-link neural network filtre model for denoising ultrasound images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd counting with segmentation attention convolutional
neural network. <em>IETIP</em>, <em>15</em>(6), 1221–1231. (<a
href="https://doi.org/10.1049/ipr2.12099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning occupies an undisputed dominance in crowd counting. This paper proposes a novel convolutional neural network architecture called SegCrowdNet. Despite the complex background in crowd scenes, the proposed SegCrowdNet still adaptively highlights the human head region and suppresses the non-head region by segmentation. With the guidance of an attention mechanism, the proposed SegCrowdNet pays more attention to the human head region and automatically encodes the highly refined density map. The crowd count can be obtained by integrating the density map. To adapt the variation of crowd counts, SegCrowdNet intelligently classifies the crowd count of each image into several groups. In addition, the multi-scale features are learned and extracted in the proposed SegCrowdNet to overcome the scale variations of the crowd. To verify the effectiveness of this proposed method, extensive experiments are conducted on four challenging datasets. The results demonstrate that the proposed SegCrowdNet achieves excellent performance compared with the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jiwei Chen and Zengfu Wang},
  doi          = {10.1049/ipr2.12099},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1221-1231},
  shortjournal = {IET Image Process.},
  title        = {Crowd counting with segmentation attention convolutional neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ground truth free retinal vessel segmentation by learning
from simple pixels. <em>IETIP</em>, <em>15</em>(6), 1210–1220. (<a
href="https://doi.org/10.1049/ipr2.12098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is fundamental for the automatic retinal image analysis and ocular disease screening. This paper aims to learn a ground truth free feature aggregation strategy for the vessel segmentation. Five vesselness maps modelling the vessels&#39;profile, appearance, and shape are first generated. Together, the histogram of the local binary pattern and the green colour are extracted. In each vesselness map, the pixels with large vesselness values are regarded as simple positive samples. The pixels with small vesselness values are regarded as simple negative samples, and the pixels with mediocre values are treated as difficult pixels. The simple positive samples and simple negative samples near the difficult pixels consist of the training dataset while the rest vesselness maps together with the local binary pattern histogram, and green colour channel are used as the features to learn a strong classifier. Then, without leveraging any ground truth, multiple kernel boosting is used to combine four support vector machine kernels to learn a strong vessel model for each image. Applying this learnt model to the pixels with mediocre values in the single vesselness map, their label will be determined. Totally, five strong vessel models are learnt. Finally, pixels with the majority supports from the strong vessel models are labelled as vessel pixels. The proposed method achieves accuracy of 94.83%, sensitivity of 72.59%, and specificity of 98.11% on DRIVE dataset, and accuracy of 95.51%, sensitivity of 78.09%, and specificity of 97.56% on STARE. It outperforms the state-of-the-art unsupervised methods and achieves comparable performances to the supervised methods.},
  archive      = {J_IETIP},
  author       = {Beiji Zou and Hongpu Fu and Zailiang Chen and Qing Liu},
  doi          = {10.1049/ipr2.12098},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1210-1220},
  shortjournal = {IET Image Process.},
  title        = {Ground truth free retinal vessel segmentation by learning from simple pixels},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain medical image fusion scheme based on shuffled
frog-leaping algorithm and adaptive pulse-coupled neural network.
<em>IETIP</em>, <em>15</em>(6), 1203–1209. (<a
href="https://doi.org/10.1049/ipr2.12092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of low contrast and blurred edge textures in medical image fusion, a new fusion scheme in non-subsampled contourlet transform (NSCT) domain is proposed to improve the quality of fused brain images which is based on pulse-coupled neural network (PCNN) and shuffled frog-leaping algorithm (SFLA). First, the source images are decomposed into low-frequency (LF) and high-frequency (HF) subbands using NSCT; if one of the source images is multicolour, then hue, saturation and brightness (HSI) transform is needed first. Second, different PCNN fusion rules are designed for LF and HF subbands according to their features, respectively. Parameters including decay time constants and amplification factors are optimised by SFLA. Finally, the fused image is reconstructed by inverse NSCT; and if necessary, an inverse HSI transform is needed. Visual and quantitative analysis of experimental results show that the fused image preserves more information of the source images, and the ability of edge retention is strong. The scheme has prominent advantages in mutual information and Q AB/F for multimodal brain images, including MRI-PET, MRI-SPECT, and CT-MRI, which proves that it can obtain better visual effect and have strong robustness as well as wide applications.},
  archive      = {J_IETIP},
  author       = {Yu Miao and Ning Chunyu and Xue Yazhuo},
  doi          = {10.1049/ipr2.12092},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1203-1209},
  shortjournal = {IET Image Process.},
  title        = {Brain medical image fusion scheme based on shuffled frog-leaping algorithm and adaptive pulse-coupled neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-light image enhancement based on retinex decomposition
and adaptive gamma correction. <em>IETIP</em>, <em>15</em>(5),
1189–1202. (<a href="https://doi.org/10.1049/ipr2.12097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images suffer from poor visibility and noise. In this paper, a low-light image enhancement method based on Retinex decomposition is proposed. A pyramid network is first utilized to extract multi-scale features to improve the quality of Retinex decomposition. Then the decomposed illumination is refined via an adaptive Gamma correction network to handle non-uniform illumination, while the decomposed reflectance is refined with a lightweight network. Finally, the enhanced image is obtained by element-wise multiplication between the refined illumination and reflectance components. Quantitative and qualitative experiments demonstrate the superiority of our method over state-of-the-art image enhancement methods.},
  archive      = {J_IETIP},
  author       = {Jingyu Yang and Yuwei Xu and Huanjing Yue and Zhongyu Jiang and Kun Li},
  doi          = {10.1049/ipr2.12097},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1189-1202},
  shortjournal = {IET Image Process.},
  title        = {Low-light image enhancement based on retinex decomposition and adaptive gamma correction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pre-training of gated convolution neural network for remote
sensing image super-resolution. <em>IETIP</em>, <em>15</em>(5),
1179–1188. (<a href="https://doi.org/10.1049/ipr2.12096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many very deep neural networks are proposed to obtain accurate super-resolution reconstruction of remote sensing images. However, the deeper the network for image SR is, the more difficult it is to train. The low-resolution inputs and features contain abundant low-frequency information and noise, which are treated equally as the high-frequency information to across the network. To solve these problems, a novel single-image super-resolution algorithm named pre-training of gated convolution neural network (PGCNN) is proposed for remote sensing images. The proposed PGCNN consists of several residual blocks with long skip connections. Each residual block contains an additional well-designed gated convolution unit, which provides different weights to high-frequency information and low-frequency information to control the transmission of information, making the main network focus on learning high-frequency information. Compared with several state-of-the-art methods, experimental results on the remote sensing datasets (SIRI-WHU, NWPU-RESISC45, RSSCN7 and UC-Merced-Land-Use) show that the proposed PGCNN has the accuracy and visual improvements.},
  archive      = {J_IETIP},
  author       = {Yali Peng and Xuning Wang and Junwei Zhang and Shigang Liu},
  doi          = {10.1049/ipr2.12096},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1179-1188},
  shortjournal = {IET Image Process.},
  title        = {Pre-training of gated convolution neural network for remote sensing image super-resolution},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segregation of meaningful strokes, a pre-requisite for self
co-articulation removal in isolated dynamic gestures. <em>IETIP</em>,
<em>15</em>(5), 1166–1178. (<a
href="https://doi.org/10.1049/ipr2.12095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture formation, a pre-processing step, has its importance when variations in patterns, scale, and speed come into play. Self co-articulations are intentional movements performed by an individual to complete a gesture, whose presence in the trajectory alters its original meaning. For recognition, most researchers have directly used the trajectory formed along with these self co-articulated strokes, with a few removing it using visible trait-like velocity. Usage of velocity has shortcomings as gesturing in air differs from gesturing over a solid surface; hence, we propose a gesture formation model, which incorporates global and local measures to remove these self co-articulations. The global measure uses Euclidean distance, instantaneous velocity, and polarity calculated from the complete gesture, while the local measure segments the gesture into stroke-level segments by using the minimum–maximum-polarity algorithm and applies the selective bypass rules. The proposed model, when experimented on gestures patterns with premeditated speed variation, has a mean error rate of 0.0069 and 7.40% self co-articulations;individuals’ natural gesticulation has a mean error rate of 0.0371 and 12.07% self co-articulations. Experimentation on each gesture of NITS hand gesture databases showed a relative improvement of 40% (accuracy 97%) over the existing baseline models.},
  archive      = {J_IETIP},
  author       = {Anish Monsley K. and Kuldeep Singh Yadav and Songhita Misra and Taimoor Khan and M. K. Bhuyan and Rabul Hussain Laskar},
  doi          = {10.1049/ipr2.12095},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1166-1178},
  shortjournal = {IET Image Process.},
  title        = {Segregation of meaningful strokes, a pre-requisite for self co-articulation removal in isolated dynamic gestures},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-speed target tracking algorithm for the
pulse-sequence-based image sensor. <em>IETIP</em>, <em>15</em>(5),
1157–1165. (<a href="https://doi.org/10.1049/ipr2.12094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a high-speed target tracking algorithm based on the pulse-sequence-based image sensor. The algorithm separates the target from the background through the statistical distribution of the pulses in a single frame of data. In order to achieve more accurate target tracking, the phenomenon of image lag in the process of capturing the high-speed moving target is studied. The positioning deviation is compensated by estimating the image lag length of the moving target. The algorithm is implemented with pipeline processing on field-programmable gate array (FPGA). The positioning accuracy of the algorithm is verified with input data from a pulse-sequence image sensor model. The verification results show that the maximum position deviation of the target at different speeds is 0.94 pixels. In the experiment of tracking a 40 m/s moving ball, this algorithm can provide real-time position information within 44 ns.},
  archive      = {J_IETIP},
  author       = {Jiangtao Xu and Xiangfeng Wang and Zhiyuan Gao and Kaiming Nie},
  doi          = {10.1049/ipr2.12094},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1157-1165},
  shortjournal = {IET Image Process.},
  title        = {High-speed target tracking algorithm for the pulse-sequence-based image sensor},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive enhanced affine transformation for non-rigid
registration of visible and infrared images. <em>IETIP</em>,
<em>15</em>(5), 1144–1156. (<a
href="https://doi.org/10.1049/ipr2.12093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid registration, performing well in all-weather and all-day/night conditions, directly determine the reliability of visible (VIS) and infrared (IR) image fusion. On account of non-planar scenes and differences between IR and VIS cameras, non-linear transformation models are more helpful to non-rigid image registration than the affine model. However, most of non-linear models usually used on non-rigid registration are constructed by control points at present. Aiming at the issue that the adaptiveness and generalization of the control-point-based models are limited, adaptive enhanced affine transformation (AEAT) is proposed for image registration, generalizing the affine model from linear to non-linear case. Firstly, Gaussian weighted shape context, measuring the structural similarity between multimodal images, is designed to extract putative matches from edge maps of IR and VIS images. Secondly, to implement global image registration, the optimal parameters of the AEAT modal are estimated from putative matches by a strategy of subsection optimization. Experiment results show that this approach is robust in different registration tasks and outperforms several competitive methods on registration precision and speed.},
  archive      = {J_IETIP},
  author       = {Chaobo Min and Yan Gu and Yingjie Li and Feng Yang},
  doi          = {10.1049/ipr2.12093},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1144-1156},
  shortjournal = {IET Image Process.},
  title        = {Adaptive enhanced affine transformation for non-rigid registration of visible and infrared images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse representation based intraframe and semi-intraframe
video coding schemes for low bitrates. <em>IETIP</em>, <em>15</em>(5),
1128–1143. (<a href="https://doi.org/10.1049/ipr2.12091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes some extensions of the successful sparse coding of still images to intraframe and semi-intraframe video coding. The presented frameworks apply the efficient K-singular value decomposition and recursive least squares dictionary learning methods for sparse representation of videos to study their coding performances. In the proposed semi-intraframe schemes, namely, SISC1 and SISC2, only frame-blocks with more than a threshold deviation from the blocks of the previous frame are transmitted/coded. This reduces the required bitrate and prevents the sparse coding of similar blocks, leading to more efficient video coding methods. The results show that the dictionary learning-based intraframe coding improves the rate-distortion performance of the conventional Motion-JPEG and Motion-JPEG2000 at low bitrates for more than about 3 and 0.5 dB of PSNR on average (for 0.2–1 bpp compression), respectively. The proposed methods outperform the basic dictionary learning-based coding, especially for slower changing videos, generally, with more than 3 dB superiority on average over the tested bitrates. These schemes even present superior performance than the HEVC in the intramode for the complex textured or cluttered scenes. The proposed SISC2 method also saves up to about 50% of the sparse coding computational cost by preventing the coding of more similar frame-blocks.},
  archive      = {J_IETIP},
  author       = {Maziar Irannejad and Homayoun Mahdavi-Nasab},
  doi          = {10.1049/ipr2.12091},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1128-1143},
  shortjournal = {IET Image Process.},
  title        = {Sparse representation based intraframe and semi-intraframe video coding schemes for low bitrates},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification of plant disease images via a
squeeze-and-excitation MobileNet model and twice transfer learning.
<em>IETIP</em>, <em>15</em>(5), 1115–1127. (<a
href="https://doi.org/10.1049/ipr2.12090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop diseases have a devastating effect on agricultural production, and serious diseases can lead to harvest failure entirely. Recent developments in deep learning have greatly improved the accuracy of image identification. In this study, we investigated the transfer learning of deep convolutional neural networks and modified the network structure to improve the learning capability of plant lesion characteristics. The MobileNet with squeeze-and-excitation (SE) block was selected in our approach. Integrating the merits of both, the pre-trained MobileNet and SE block were fused to form a new network, which we termed the SE-MobileNet, and was used to identify the plant diseases. In particular, the transfer learning was performed twice to obtain the optimum model. The first phase trained the model for the extended layers while the bottom convolution layers were frozen with the pre-trained weights on ImageNet; by loading the model trained in the first phase, the second phase retrained the model using the target dataset. The proposed procedure provides a significant increase in efficiency relative to other state-of-the-art methods. It reaches an average accuracy of 99.78% in the public dataset with clear backdrops. Even under multiple classes and heterogeneous background conditions, the average accuracy realises 99.33% for identifying the rice disease types. The experimental findings show the feasibility and effectiveness of the proposed procedure.},
  archive      = {J_IETIP},
  author       = {Junde Chen and Defu Zhang and Md Suzauddola and Yaser Ahangari Nanehkaran and Yuandong Sun},
  doi          = {10.1049/ipr2.12090},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1115-1127},
  shortjournal = {IET Image Process.},
  title        = {Identification of plant disease images via a squeeze-and-excitation MobileNet model and twice transfer learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face recognition based on adaptive margin and diversity
regularization constraints. <em>IETIP</em>, <em>15</em>(5), 1105–1114.
(<a href="https://doi.org/10.1049/ipr2.12089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a more robust facial feature can be learned by convolutional neural networks once introducing margins into loss functions. Those methods set a margin for each class manually to squeeze the intra-class variations within each class equally. However, the internal feature distributions of different persons in the real world are highly unbalanced, and the distance between different identities is not uniform either. As a result, applying the same margin on all classes might not lead to higher inter-class differences. To address this problem, this paper proposes an adaptive margin based on feature distribution to squeeze the feature interior spaces of different classes. Simultaneously, because the inter-class margin can adequately represent the distribution of different classes in the feature space, this paper proposes a novel diversity regularization method. The regularization weights of each class are dynamically set depending on their margins. This method proposed in this paper is intuitively interpretable and can be easily applied to other classification scenarios. Experiments on current existing benchmarks have demonstrated the superiority of our method over state-of-the-art competitors.},
  archive      = {J_IETIP},
  author       = {Zhemin Zhang and Xun Gong and Junzhou Chen},
  doi          = {10.1049/ipr2.12089},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1105-1114},
  shortjournal = {IET Image Process.},
  title        = {Face recognition based on adaptive margin and diversity regularization constraints},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extraction of compact boundary normalisation based geometric
descriptors for affine invariant shape retrieval. <em>IETIP</em>,
<em>15</em>(5), 1093–1104. (<a
href="https://doi.org/10.1049/ipr2.12088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape recognition and retrieval is a complex task on non-rigid objects and it can be effectively performed by using a set of compact shape descriptors. This paper presents a new technique for generating normalised contour points from shape silhouettes, which involves the identification of object contour from images and subsequently the object area normalisation (OAN) method is used to partition the object into equal part area segments with respect to shape centroid. Later, these contour points are used to derive six descriptors such as compact centroid distance (CCD), central angle (ANG), normalized points distance (NPD), centroid distance ratio (CDR), angular pattern descriptor (APD) and multi-triangle area representation (MTAR). These descriptors are a 1D shape feature vector which preserve contour and region information of the shapes. The performance of the proposed descriptors is evaluated on MPEG-7 Part-A, Part-B and multi-view curve dataset images. The present experiments are aimed to check proposed shape descriptor&#39;s robustness to affine invariance property and image retrieval performance. Comparative study has also been carried out for evaluating our approach with other state of the art approaches. The results show that image retrieval rate in OAN approach performs significantly better than that in other existing shape descriptors.},
  archive      = {J_IETIP},
  author       = {Arjun Paramarthalingam and Mirnalinee Thankanadar},
  doi          = {10.1049/ipr2.12088},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1093-1104},
  shortjournal = {IET Image Process.},
  title        = {Extraction of compact boundary normalisation based geometric descriptors for affine invariant shape retrieval},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral image classification using 3D 2D CNN.
<em>IETIP</em>, <em>15</em>(5), 1083–1092. (<a
href="https://doi.org/10.1049/ipr2.12087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have shown that deep-learning-derived methods based on convolutional neural network can achieve high performance in terms of accuracy when applied to computer vision task such as object detection, segmentation and classification particularly on hyperspectral image. However, the existing methods have long training times. To reduce the training time and increase the accuracy, this paper proposed a new 3D2D convolutional neural network combined for hyperspectral image classification. For this purpose, a 3D fast learning block (depthwise separable convolution block and a fast convolution block) followed by a 2D convolutional neural network was introduced to extract spectralspatial features. Four datasets were used for the experiment purpose and the results showed that the proposed method achieved excellent result on both small and large training data compared with existing methods. The proposed method increased the overall accuracies by 2% on UP and KSC datasets while significantly reducing the training time on IP and KSC datasets, respectively. The proposed method increased all accuracies for at least 6% on IP, KSC and UP datasets when compared to some state-of-the-art methods. Also, it reduced considerably the training and testing time on IP and KSC datasets when fast convolution block alone is involved.},
  archive      = {J_IETIP},
  author       = {Alou Diakite and Gui Jiangsheng and Fu Xiaping},
  doi          = {10.1049/ipr2.12087},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1083-1092},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral image classification using 3D 2D CNN},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light field editing in the gradient domain. <em>IETIP</em>,
<em>15</em>(5), 1072–1082. (<a
href="https://doi.org/10.1049/ipr2.12086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new method for light field applications such as content replacement and fusion in the gradient domain. This approach is inspired by successful gradient domain based image and video editing techniques. A necessary and important part of gradient-based solutions is recovering the signal of interest from artificially generated, and typically non-integrable, gradient data. As such, a new algorithm is developed to reconstruct a light field from a given gradient data set. In the algorithm, first, the 4D Haar wavelet decomposition of the light field is obtained from the given gradient data. Then, the light field is obtained from a wavelet synthesis step. This algorithm is intended as a building block for gradient-based light field editing methods, and as such, its performance is analysed on a set of benchmark light field data sets. The proposed reconstruction algorithm is an essential part in developing solutions for two light field problems: light field editing and light field fusion. Results show that processing light fields in the gradient domain offers significant advantages over processing in the intensity domain.},
  archive      = {J_IETIP},
  author       = {Ioana Sevcenco and Panajotis Agathoklis},
  doi          = {10.1049/ipr2.12086},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1072-1082},
  shortjournal = {IET Image Process.},
  title        = {Light field editing in the gradient domain},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meaningful ciphertext encryption algorithm based on bit
scrambling, discrete wavelet transform, and improved chaos.
<em>IETIP</em>, <em>15</em>(5), 1053–1071. (<a
href="https://doi.org/10.1049/ipr2.12085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a meaningful ciphertext encryption algorithm of good practicability and strong robustness. The plaintext undergoes pre-encryption of bit scrambling and secondary encryption of discrete wavelet transform (DWT) to obtain meaningful ciphertext. In the pre-encryption stage, only the higher three bit pages that are not all 0 or 1 are scrambled. The embedding method in the secondary encryption process is to add the hundreds and tens’ digit and ones’ digit of the middle ciphertext to the scrambled detail information obtained by DWT of the cover image. The information entropy of the intermediate ciphertext obtained by this method can reach 7.997, the correlation coefficient between the ciphertext and the cover image can reach 0.99, and the decrypted image of the ciphertext under high-intensity pollution can still be identified. It shows that the pre-encryption can conceal the plaintext information well, and the secondary encryption can eliminate the ciphertext texture characteristics while retaining the good robustness. Finally, the improved three one-dimensional chaos are used to generate random sequences, and the plaintext hash value is associated with the chaotic key. The improved chaos greatly improves the randomness of the sequence and broadens the range of parameters. The hash value SHA-256 of the plaintext is added to make the key stream adaptively change with the plaintext, which greatly improves the sensitivity of the plaintext and the ability to resist selected plaintext and ciphertext attacks. Experiments have also verified that the encryption algorithm can encrypt images of different types and shapes and has strong practicability, good bit scrambling effect, and high sensitivity to scrambling.},
  archive      = {J_IETIP},
  author       = {Shiwei Jing and Yuan Guo and Wei Chen},
  doi          = {10.1049/ipr2.12085},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1053-1071},
  shortjournal = {IET Image Process.},
  title        = {Meaningful ciphertext encryption algorithm based on bit scrambling, discrete wavelet transform, and improved chaos},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Security augmentation grounded on fresnel and arnold
transforms using hybrid chaotic structured phase mask. <em>IETIP</em>,
<em>15</em>(5), 1042–1052. (<a
href="https://doi.org/10.1049/ipr2.12084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel asymmetric double image encryption technique is introduced to protect the algorithm from the interlopers by using hybrid chaotic structured phase masks in Arnold and Fresnel transforms. Mostly random phase masks are used routinely in many algorithms which are not strong against many attacks. So, taking in consideration replacing of random phase masks with the new hybrid chaotic structured phase masks takes place in the algorithm. With the assistance of Fresnel zone plates, radial Hilbert mask and logistic map functions, the chaotic structured phase mask is accumulated and hybrid chaotic structured phase mask provides extra security to system. Arnold transform helps in pixel shuffling, and with the pixel reorganization, it also helps in image reaping and bordering. The propagation distance of the Fresnel transform and the constraints of the Arnold transform serve as keys for encryption and decryption. To check the sturdiness of hybrid chaotic structured phase mask, simulations have been done which are carried out by the support of MATLAB R2018a (9.4.0.813654).},
  archive      = {J_IETIP},
  author       = {Shivani Yadav and Hukum Singh},
  doi          = {10.1049/ipr2.12084},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1042-1052},
  shortjournal = {IET Image Process.},
  title        = {Security augmentation grounded on fresnel and arnold transforms using hybrid chaotic structured phase mask},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-focus image fusion evaluation based on jointly sparse
representation and atom focus measure. <em>IETIP</em>, <em>15</em>(5),
1032–1041. (<a href="https://doi.org/10.1049/ipr2.12083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion (MFIF) tries to combine images with different in-focus regions and get a composite image that is in focus everywhere. Although many new MFIF algorithms based on various new representation models have been proposed in recent years, the performance evaluation of MFIF is still a challenging issue. In this study, a novel MFIF objective evaluation metric based on jointly sparse representation and atom focus measure is proposed. It not only provides a more reliable alternative for MFIF quality measurement but also supplies a unique MFIF performance analysis method at the same time. In the measurement, the sources and their fusion results are decomposed jointly sparse with an over-complete learning dictionary to extract the atom remnants of the source images. Meanwhile, in order to emphasise the fusion effect of in-focus atoms, the sum-modified-Laplacian model is used to measure the atom focus degree. Then, the atom remnants weighted by their focus measures are used to measure MFIF quality. In the experiments, nine recently proposed fusion algorithms were tested to contrast the proposed metrics with other four widely used objective metrics. The experimental results demonstrated the rationality and accuracy of our method. Moreover, it was also proved quantificationally that the fusion degree of atoms is directly related to their in-focus degree, the high in-focus degree atoms usually have poor fusion effect.},
  archive      = {J_IETIP},
  author       = {Yanxiang Hu and Bo Zhang and Juntong Zhang and Qian Gao},
  doi          = {10.1049/ipr2.12083},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1032-1041},
  shortjournal = {IET Image Process.},
  title        = {Multi-focus image fusion evaluation based on jointly sparse representation and atom focus measure},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image dehazing based on bright channel prior model
and saliency analysis strategy. <em>IETIP</em>, <em>15</em>(5),
1023–1031. (<a href="https://doi.org/10.1049/ipr2.12082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze is a common atmospheric phenomenon that causes poor visibility in outdoor images, which greatly limits image application in later stages. Therefore, haze removal has become the first and most indispensable step when dealing with degraded images. In this paper, we propose a novel bright channel prior (BCP) model and a saliency analysis strategy for haze removal. First, we obtain a more robust and accurate atmospheric light by a superpixel-based dark channel method. Second, we utilize the dark channel prior (DCP) to handle dark regions in hazy images. However, the DCP often mistakes white regions for opaque haze and thus causes serious colour distortion and halo effects. To solve this problem, a new BCP is proposed to accurately estimate the transmission of bright regions in hazy images. Third, we fuse the DCP and BCP using a multiscale fusion strategy with Laplacian pyramid representation to gain the correct transmission information for both bright and dark regions. Finally, a novel saliency analysis strategy for transmission refinement is proposed, so that the texture details can remain present to the greatest extent in the restored images. The experimental results illustrate that our proposed method performs well in restoring images containing bright objects.},
  archive      = {J_IETIP},
  author       = {Libao Zhang and Shan Wang and Xiaohan Wang},
  doi          = {10.1049/ipr2.12082},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1023-1031},
  shortjournal = {IET Image Process.},
  title        = {Single image dehazing based on bright channel prior model and saliency analysis strategy},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved symbol reduction technique based huffman coder
for efficient entropy coding in the transform coders. <em>IETIP</em>,
<em>15</em>(4), 1008–1022. (<a
href="https://doi.org/10.1049/ipr2.12081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy coding is the essential block of transform coders that losslessly converts the quantized transform coefficients into the bit-stream suitable for transmission or storage. Usually, the entropy coders exhibit less compression capability than the lossy coding techniques. Hence, in the past decade, several efforts have been made to improve the compression capability of the entropy coding technique. Recently, a symbol reduction technique (SRT) based Huffman coder is developed to achieve higher compression than the existing entropy coders at similar complexity of the regular Huffman coder. However, the SRT-based Huffman coding is not popular for the real-time applications due to the improper negative symbol handling and the additional indexing issues, which restrict its compression gain at most 10–20% over the regular Huffman coder. Hence, in this paper, an improved SRT (ISRT) based Huffman coder is proposed to properly alleviate the deficiencies of the recent SRT-based Huffman coder and to achieve higher compression gains. The proposed entropy coder is extensively evaluated on the ground of compression gain and the time complexity. The results show that the proposed ISRT-based Huffman coder provides significant compression gain against the existing entropy coders with lower time consumptions.},
  archive      = {J_IETIP},
  author       = {Vikrant Singh Thakur and Kavita Thakur and Shubhrata Gupta},
  doi          = {10.1049/ipr2.12081},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1008-1022},
  shortjournal = {IET Image Process.},
  title        = {An improved symbol reduction technique based huffman coder for efficient entropy coding in the transform coders},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual-attention GAN for interior sketch colourisation.
<em>IETIP</em>, <em>15</em>(4), 997–1007. (<a
href="https://doi.org/10.1049/ipr2.12080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the professional field of interior designing, sketch colouring is often a time-consuming and vapidity task. The traditional neural network does not handle the semantic relationship of sketch lines well, and the colouring effect is unsatisfactory. This paper proposes visual-attention generative adversarial network (VAGAN), which enhances the processing effect of edge semantics, strengthens the network to line edge recognition ability, as well as reduces colour overflow and improved model colouring result. In addition, a two-stage training mode is used to simplify the training of rare samples. The simple line draft input into the trained VAGAN, output natural, realistic colour pictures. The experimental results show that, compared with the existing methods, the proposed method can better deal with the problem of sketch and generate stable and reliable images.},
  archive      = {J_IETIP},
  author       = {Xinrong Li and Hong Li and Chiyu Wang and Xun Hu and Wei Zhang},
  doi          = {10.1049/ipr2.12080},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {997-1007},
  shortjournal = {IET Image Process.},
  title        = {Visual-attention GAN for interior sketch colourisation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A content-dependent daltonization algorithm for colour
vision deficiencies based on lightness and chroma information.
<em>IETIP</em>, <em>15</em>(4), 983–996. (<a
href="https://doi.org/10.1049/ipr2.12079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To help CVD (colour vision deficiency) observers distinguish among different colours in digital images, this study proposes a content-dependent Daltonization algorithm based on lightness and chroma information. This improves the trade-offs in contrast, naturalness, and colour consistency deficiencies found in existing methods. Chroma remapping and brightness adjustments on image content are utilized to increase contrast and maximize the preservation of the original hue. In a quantitative study, the proposed method proved more useful for dichromats than did existing methods. The evaluation by research subjects showed that the method was superior to the current methods in balancing contrast, naturalness, and colour consistency.},
  archive      = {J_IETIP},
  author       = {Xuming Shen and Jianhua Feng and Xiandou Zhang},
  doi          = {10.1049/ipr2.12079},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {983-996},
  shortjournal = {IET Image Process.},
  title        = {A content-dependent daltonization algorithm for colour vision deficiencies based on lightness and chroma information},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised change detection method in SAR images based on
deep belief network using an improved fuzzy c-means clustering
algorithm. <em>IETIP</em>, <em>15</em>(4), 974–982. (<a
href="https://doi.org/10.1049/ipr2.12078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have recently displayed ground-breaking results for synthetic aperture radar image change detection problem. However, they still face the challenges of intrinsic noise and the difficulty of acquiring labeled data. To sort out these issues, we aim to develop a change detection approach specifically designed for analyzing synthetic aperture radar images based on Deep Belief Network as the deep architecture which includes unsupervised feature learning and supervised network fine-tuning. The deep neural networks can reach the final change maps directly from the two original images. A pre-classification based on Morphological Reconstruction and Membership Filtering is employed in order to minimize the effect of noise. Appropriate diversity samples are provided by a virtual sample generation method in order to mitigate overfitting raised by limited synthetic aperture radar data. Visual and quantitative analysis as well as comparisons with advanced algorithms show that our algorithm not only achieves better results but also requires less implementation time.},
  archive      = {J_IETIP},
  author       = {Sanae Attioui and Said Najah},
  doi          = {10.1049/ipr2.12078},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {974-982},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised change detection method in SAR images based on deep belief network using an improved fuzzy C-means clustering algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral image, video compression using sparse tucker
tensor decomposition. <em>IETIP</em>, <em>15</em>(4), 964–973. (<a
href="https://doi.org/10.1049/ipr2.12077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image and videos provide rich spectral information content, which facilitates accurate classification, unmixing, temporal change detection, and so on. However, with the rapid improvements in technology, the data size has increased many folds. To properly handle the enormous data volume, efficient methods are required to compress the data. This paper proposes a multi-way approach for compression of the hyperspectral image or video sequence. In this approach, a differential representation of the data is first obtained. In the case of hyperspectral images, the difference between consecutive bands is obtained and in case of videos, the difference between consecutive frames is computed. In the next step, a sparse Tucker tensor decomposition is performed and the sparse core tensor obtained. Finally, the core tensor and the corresponding factor matrices are truncated and the data encoded to obtain the compressed version for transmission. The compression method utilises the multi-way structure of the data and hence can be extended for hyperspectral videos. Experimental results on several real data imply that the proposed compression approach obtains better efficiency in terms of compression ratio, signal to noise ratio.},
  archive      = {J_IETIP},
  author       = {Samiran Das},
  doi          = {10.1049/ipr2.12077},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {964-973},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral image, video compression using sparse tucker tensor decomposition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense connection decoding network for crisp contour
detection. <em>IETIP</em>, <em>15</em>(4), 956–963. (<a
href="https://doi.org/10.1049/ipr2.12076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, contour detection algorithm has made obvious progress with the help of convolutional neural networks. The aim of this paper is to present a novel network connecting low- and high-resolution features to make the network achieving richer feature representation. First, VGG net is used as encoding part with outputting the features of different resolutions, and then the feature maps are combined in some specific resolution with up- or down-sample method. The combining process can be stack step-by-step. The proposed network makes the encoding part deeper to extract richer convolutional features. The experiments have shown that the proposed method improves the contour detection performances and outperform some existed convolutional neural networks based methods on BSDS500 and NYUD-V2 datasets.},
  archive      = {J_IETIP},
  author       = {Guili Xu and Chuan Lin and Yuehua Cheng},
  doi          = {10.1049/ipr2.12076},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {956-963},
  shortjournal = {IET Image Process.},
  title        = {Dense connection decoding network for crisp contour detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finger vein recognition algorithm under reduced field of
view. <em>IETIP</em>, <em>15</em>(4), 947–955. (<a
href="https://doi.org/10.1049/ipr2.12075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms for finger vein recognition methods are generally designed based on greyscale images containing vein distributions, but greyscale inhomogeneities and non-venous texture structures often adversely affect the recognition results. Besides, the performance of the algorithm when the field of view is reduced has not been studied. Therefore, the aim of this paper is to propose an algorithm based on binary image, so as to minimize the interference of non-venous factors in the identification process. We use the feature from accelerated segment test algorithm to detect feature points, and use gradient histogram to describe these feature points in a vectorized manner. In addition, we propose the concept of circular matching neighbourhood, and select the matching feature point pairs in this area. Then, the Euclidean distance and the number of correct matching pairs were comprehensively considered to increase the recognition rate of the algorithm. The algorithm was tested on the database of SDU-MLA, FV-USM and MMCBNU-6000. The results show that the algorithm has certain advantages before and after the field of view is reduced. Therefore, this paper not only provides a new idea for finger vein recognition, but also has practical application value for the miniaturization of finger vein acquisition devices.},
  archive      = {J_IETIP},
  author       = {Jian Jin and Si Di and Weijian Li and Xuetong Sun and Xudi Wang},
  doi          = {10.1049/ipr2.12075},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {947-955},
  shortjournal = {IET Image Process.},
  title        = {Finger vein recognition algorithm under reduced field of view},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimised CNN in conjunction with efficient pooling strategy
for the multi-classification of breast cancer. <em>IETIP</em>,
<em>15</em>(4), 936–946. (<a
href="https://doi.org/10.1049/ipr2.12074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tissue analysis using histopathological images is the most prevailing as well as a challenging task in the treatment of cancer. The clinical assessment of tissues becomes very tough as high variability in the magnification levels makes the situation worst for any pathologist to deal with the benign and malignant stages of cancer. One of the possible ways to address such a pathetic situation could be an advanced machine learning approach. Hence, a convolutional neural network (CNN) architecture is proposed to create an automated system for magnification independent multi-classification of breast cancer histopathological images. This automated system offers high productivity and consistency in diagnosing the eight different classes of breast cancer from a balanced BreakHis dataset. The system utilises an efficient training methodology to learn the discerning features from images of different magnification levels. Data augmentation techniques are also employed to overcome the problem of overfitting. Additionally, the performance of CNN architecture has been improved in a significant manner by adopting an appropriate pooling strategy and optimisation technique. Based on that, we have achieved an accuracy of 80.76%, 76.58%, 79.90%, and 74.21% at the magnification 40X, 100X, 200X, and 400X, respectively. The proposed model outperforms the handcrafted approaches with an average accuracy of 80.47% at 40X magnification level.},
  archive      = {J_IETIP},
  author       = {Shallu Sharma and Rajesh Mehra and Sumit Kumar},
  doi          = {10.1049/ipr2.12074},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {936-946},
  shortjournal = {IET Image Process.},
  title        = {Optimised CNN in conjunction with efficient pooling strategy for the multi-classification of breast cancer},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time long-term tracking with reliability assessment and
object recovery. <em>IETIP</em>, <em>15</em>(4), 918–935. (<a
href="https://doi.org/10.1049/ipr2.12072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many visual tracking algorithms based on discriminative correlation filters have been proposed and proved to be successful in short-term tracking. However, most algorithms do not handle long-term tracking well due to factors such as occlusion and deformation. The aim of this paper is to propose a long-term tracking method with reliability assessment and object recovery. First, the relationship between the overlap rate and the response value is extensively studied, and then from the perspective of the time axis, the tracking process is evaluated by the fluctuation trend of the continuous response value. In the object recovery mechanism, we propose to alternately use local search and global search to improve the efficiency of detection. To this end, a sliding window is designed for cyclic shifting in the local region to achieve dense sampling within the region of interest, and EdgeBox is used in the global search to achieve target detection. Further, flexible switching between local search and global search is achieved by the difference in displacement of the object. Extensive experimental results on several benchmark datasets demonstrate that the proposed long-term tracker can achieve state-of-the-art accuracy with real-time speed of about 27 frames per second.},
  archive      = {J_IETIP},
  author       = {Jun Liu and Ping Ye and Xingchen Zhang and Gang Xiao},
  doi          = {10.1049/ipr2.12072},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {918-935},
  shortjournal = {IET Image Process.},
  title        = {Real-time long-term tracking with reliability assessment and object recovery},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pixel pair–based encoding pattern for stereo matching via
an adaptively weighted cost. <em>IETIP</em>, <em>15</em>(4), 908–917.
(<a href="https://doi.org/10.1049/ipr2.12071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching, which is a key problem in computer vision, faces the challenge of radiometric distortions. Most of the existing stereo matching methods are based on simple matching cost algorithms and appear the problem of mismatch under radiometric distortions. It is necessary to improve the robustness and accuracy of matching cost algorithms. A novel encoding pattern is proposed for stereo matching. In the proposed encoding pattern, each of the matching windows in the grey image and gradient images is divided into several isoline-like sets with different radii. Then, pixel pairs are defined in the isoline-like sets. An encoding function is used to decide the relative order between the two pixels in each pixel pair. To apply the pattern for matching cost computation and enhance the matching accuracy, an adaptively weighted cost is designed that is related to the isoline-like sets. Experiments are conducted on the Middlebury and KITTI data sets to show the validity of the proposed method under severe radiometric distortions. Also, the comparisons with some widely used methods are made in the experiments to illustrate the advantage of the proposed method.},
  archive      = {J_IETIP},
  author       = {Yuli Fu and Kaimin Lai and Weixiang Chen and Youjun Xiang},
  doi          = {10.1049/ipr2.12071},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {908-917},
  shortjournal = {IET Image Process.},
  title        = {A pixel pair–based encoding pattern for stereo matching via an adaptively weighted cost},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel diagnostic map for computer-aided diagnosis of skin
cancer. <em>IETIP</em>, <em>15</em>(4), 897–907. (<a
href="https://doi.org/10.1049/ipr2.12070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In teledermoscopy, images are transmitted through a communication channel to the medical facility for medical consultation. This yields to bandwidth congestion and consumption of large storage size which impairs the transmission of the high-resolution dermoscopy image. This study proposes a novel technique by generating a diagnostic feature map, named diagonal compressive sensing (CS) features map. This proposed map is generated using the significant diagnostic features by aligning the feature vector in a diagonal map, which is then compressed using CS technique. Eventually, the recovered feature map at the receiver side is applied to the classification system for decision-making. In addition, physicians at the receiver side who were trained to read the feature maps can verify the classification decision, and then provide feedback to the patient. The results demonstrated that the proposed diagnostic map achieved less transmission time due to the small size of the feature map along with the compression process. Furthermore, the feature map has drastically improved the classification performance metrics, including the accuracy, which increased, for example, from 88.6% to 98% at 80% compression ratio compared to the traditional method on the compressed whole image.},
  archive      = {J_IETIP},
  author       = {Amira S. Ashour and Maram A. Wahba and Eman Elsaid Alaa and Yanhui Guo and Ahmed Refaat Hawas},
  doi          = {10.1049/ipr2.12070},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {897-907},
  shortjournal = {IET Image Process.},
  title        = {A novel diagnostic map for computer-aided diagnosis of skin cancer},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-image encryption algorithm based on image hash,
bit-plane decomposition and dynamic DNA coding. <em>IETIP</em>,
<em>15</em>(4), 885–896. (<a
href="https://doi.org/10.1049/ipr2.12069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems such as large data volumes, slow transmission rates and low security often occur when transmitting multiple digital images over the internet. In order to protect the content security of multiple images and improve the speed of internet transmission, a multi-image encryption algorithm based on image hash, bit-plane decomposition and dynamic DNA coding is proposed in this study. First, the proposed algorithm merges several grey images and image hashing algorithm is used to generate the initial key for chaotic mapping. Second, the random sequence generated by the improved three-dimensional chaotic map is used to decompose the bit plane of the merged image and the bit-plane matrix is substituted numerically. Finally, the random sequence generated by the four-dimensional hyperchaotic system is used to perform dynamic DNA coding and calculation on the image and the final multiple ciphertext images are generated by decomposition of the pixel matrix. The experimental results show that the proposed algorithm has large key space, strong key sensitivity, strong security and robustness, and can resist various conventional attacks such as statistical analysis, differential attack, exhaustive attack, cropping and noise attack.},
  archive      = {J_IETIP},
  author       = {Qiuyu Zhang and Jitian Han and Yutong Ye},
  doi          = {10.1049/ipr2.12069},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {885-896},
  shortjournal = {IET Image Process.},
  title        = {Multi-image encryption algorithm based on image hash, bit-plane decomposition and dynamic DNA coding},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A probabilistic collaborative dictionary learning-based
approach for face recognition. <em>IETIP</em>, <em>15</em>(4), 868–884.
(<a href="https://doi.org/10.1049/ipr2.12068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Sparse Representation based Classifier (SRC), a non-parametric model, can obtain an interesting result for pattern recognition , a reasonable interpretation has been lacked for its classification mechanism. What is more, the training samples are used as off-the-shelf dictionary directly in SRC, which can make the feature hidden in the training samples hard be extracted. At the same time, the complexity of the algorithm is increased because of too many atoms of the dictionary. The authors first explains in detail the classification mechanism of SRC from the view of probabilistic collaborative subspace and offer the process to improve the stability of the algorithm using the joint probability in the case of the multi-subspace. Then, the authors introduce the dictionary learning (DL) and Fisher criterion into the model to further enhance the discrimination of the coding coefficient. In order to ensure the convexity of the discrimination term and further enhance the discrimination, the authors add the L 21 -norm term into the Fisher discrimination term and offer the proof for its convexity. Finally, the experimental result on a series of benchmark databases, such as AR, Extended Yale B, LFW3D-hassner, LFW3D-sdm and LFW3D-Dlib, show that PCDDL outperforms existing classical classification models.},
  archive      = {J_IETIP},
  author       = {Shilin Lv and Jiuzhen Liang and Lan Di and Xia Yunfei and ZhenJie Hou},
  doi          = {10.1049/ipr2.12068},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {868-884},
  shortjournal = {IET Image Process.},
  title        = {A probabilistic collaborative dictionary learning-based approach for face recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive generative adversarial network for removing thin
cloud from a single remote sensing image. <em>IETIP</em>,
<em>15</em>(4), 856–867. (<a
href="https://doi.org/10.1049/ipr2.12067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land-surface observation is easily affected by the light transmission and scattering of semi-transparent clouds, high or low, resulting in blurring and reduced contrast of ground objects. To improve the visual appearance of remote sensing images, the authors present a deep learning method for thin cloud removal using a new attentive generative adversarial network without prior knowledge or assumptions, which copes with thin clouds that are unevenly distributed on different images and learns the attention map with weighted information about spatial features. Such a spatial attention model can endow each pixel with the global spatial context information. Consequently, the generative network focuses on the thin cloud regions to generate better local image restoration, and the discriminative network can evaluate the local consistency of the repaired regions. The experimental results show that this method is superior to state-of-the-art methods in recovering detailed texture information.},
  archive      = {J_IETIP},
  author       = {Hui Chen and Rong Chen and Nannan Li},
  doi          = {10.1049/ipr2.12067},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {856-867},
  shortjournal = {IET Image Process.},
  title        = {Attentive generative adversarial network for removing thin cloud from a single remote sensing image},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis on segmentation and biomarker-based approaches for
liver cancer detection: A survey. <em>IETIP</em>, <em>15</em>(4),
845–855. (<a href="https://doi.org/10.1049/ipr2.12073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver cancer is the major reason for death in this entire world. Manual detection of cancer tissue is found time consuming and difficult. Therefore, the development of an automatic detection approach with high accuracy for liver cancer is considered as the main aim of this work. The image processing approach can use the CAD for the classification of liver cancer in order to assist the physician in the decision-making process. An automated approach to effective classification of the liver tumour using effective features is conveyed in terms of the CAD system. Traditionally, radiologists delineate the liver and liver lesion on a slice-by-slice basis, which is time consuming and susceptible to inter- and intra-rater variations. Automatic methods for the segmentation of liver and liver tumours are, thus, highly demanded in clinical practice. A systematic review is being carried out to detect reported liver cancer from 2013 to 2020. Finally, a more precise technical direction is provided for all researchers in this review. Research gaps for earlier detection and its potential future aspects are also discussed.},
  archive      = {J_IETIP},
  author       = {P. Gunasekhar and S. Vijayalakshmi},
  doi          = {10.1049/ipr2.12073},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {845-855},
  shortjournal = {IET Image Process.},
  title        = {Analysis on segmentation and biomarker-based approaches for liver cancer detection: A survey},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A method of inpainting moles and acne on the high-resolution
face photos. <em>IETIP</em>, <em>15</em>(3), 833–844. (<a
href="https://doi.org/10.1049/ipr2.12066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile phones, more and more high-resolution photos are taken. The demand for high-resolution image inpainting is becoming increasingly urgent. In order to repair high-resolution face images automatically and quickly, this paper proposes an improved generative adversarial networks method. Firstly, we made a high-resolution dataset for training and testing, and abandoned the traditional 256 × 256 size data. Secondly, since the existing methods can only repair the mask with fixed size and shape on the image, when the global average pooling layer is used in the network, the improved network can repair the moles and acne with arbitrary sizes and shapes on the human face photos. Finally, in order to achieve optimal performance of the network, a mixed loss function is used in training. The experimental results prove that our method has not only achieved good results in qualitative results, but also achieved excellent results in quantitative results.},
  archive      = {J_IETIP},
  author       = {Xuewei Li and Xueming Li and Xianlin Zhang and Yang Liu and Jiayi Liang and Ziliang Guo and Keyu Zhai},
  doi          = {10.1049/ipr2.12066},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {833-844},
  shortjournal = {IET Image Process.},
  title        = {A method of inpainting moles and acne on the high-resolution face photos},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Delta tributary network—an efficient alternate approach for
bottleneck layers in CNN for plant disease classification.
<em>IETIP</em>, <em>15</em>(3), 818–832. (<a
href="https://doi.org/10.1049/ipr2.12065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, numerous research works have been proposed for diagnosing leaf diseases using state of the art convolutional neural networks. In this work, we propose a novel architecture called “Delta Tributary Network” that is built by stacking microarchitecture blocks called delta blocks specifically designed for leaf disease classification. These delta blocks utilize a novel channel split algorithm to reduce the number of channels given as input to 3 × 3 convolution layers. Unlike the existing bottleneck design which uses 1 × 1 convolution layers to decrease channel dimension space, Delta tributary network utilizes the a novel channel split algorithm to control the number of input channels togiven to 3 × 3 convolution layers, thereby preventing the linear stack of layers and henceforth avoiding over fitting and vanishing gradient problems. Delta tributary network when tested on plant village dataset gives an accuracy of 96% with just 0.3 million parameters on 133 MFLOP (Million Floating Point Operations) calculations. Further, delta tributary network tested on other bench mark datasets like CIFAR 10, CIFAR 100, MNIST and Fashion MNIST delivers higher accuracy than the other state of the art models with lesser trainable parameters, proving that delta blocks extract efficient potential features.},
  archive      = {J_IETIP},
  author       = {Suresh Gunasekaran and Kandasamy Gunavathi},
  doi          = {10.1049/ipr2.12065},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {818-832},
  shortjournal = {IET Image Process.},
  title        = {Delta tributary network—An efficient alternate approach for bottleneck layers in CNN for plant disease classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Noise robust intuitionistic fuzzy c-means clustering
algorithm incorporating local information. <em>IETIP</em>,
<em>15</em>(3), 805–817. (<a
href="https://doi.org/10.1049/ipr2.12064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain magnetic resonance image (MRI) is always contaminated by noise and has uncertainty on the boundary between different tissues. These characteristics bring challenges to the human brain image segmentation. To handle these limitations, many variants of standard fuzzy c-means (FCM) algorithm have been proposed. Some methods attempt to incorporate the local spatial information in the standard FCM algorithm. However, they can&#39;t solve the problem of data uncertainty very well. And some other methods can handle the problem of data uncertainty, but they are sensitive to noise since it doesn&#39;t incorporate any local spatial information. In this paper, we propose a noise robust intuitionistic fuzzy c-means (NR-IFCM) algorithm, which can handle noise and uncertainty problems simultaneously. In order to process the human brain MRI with noise better, we introduce a noise robust intuitionistic fuzzy set (NR-IFS) which is noise robust in this NR-IFCM algorithm. Meanwhile, in order to handle the data uncertainty, we also introduce a new intuitionistic fuzzy factor to this NR-IFCM algorithm which combine the local gray-level and the spatial information together. A large number of experimental results on human brain MRI validate the effectiveness and the superiority of our proposed NR-IFCM algorithm.},
  archive      = {J_IETIP},
  author       = {Zhenzhen Yang and Pengfei Xu and Yongpeng Yang and Bin Kang},
  doi          = {10.1049/ipr2.12064},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {805-817},
  shortjournal = {IET Image Process.},
  title        = {Noise robust intuitionistic fuzzy c-means clustering algorithm incorporating local information},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based saliency and ensembles of convolutional neural
networks for glaucoma detection. <em>IETIP</em>, <em>15</em>(3),
797–804. (<a href="https://doi.org/10.1049/ipr2.12063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma, after cataracts, is the second leading cause of worldwide vision loss. An ophthalmologist may use various tools and methods to diagnose a glaucomatous eye. Computer-aided methods involving deep convolutional neural networks also made it recently possible to detect glaucoma on fundus images. Previous studies traditionally trained a single convolutional neural network for automatic detection of glaucoma. In this study, a more advanced way of accurate automated glaucoma recognition is proposed. First, a graph-based saliency region detection is used to crop the optic disc and remove the redundant parts of the fundus images. Then, four methods are used to ensemble convolutional neural network models comprising up to three deep learning architectures for glaucoma classification. The detection performance of this model is better than a recent study that used the same dataset. It is also as good as, or better than, the results reported by other recent research in the literature on glaucoma detection.},
  archive      = {J_IETIP},
  author       = {Sertan Serte and Ali Serener},
  doi          = {10.1049/ipr2.12063},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {797-804},
  shortjournal = {IET Image Process.},
  title        = {Graph-based saliency and ensembles of convolutional neural networks for glaucoma detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust tracking algorithm for a human-following mobile
robot. <em>IETIP</em>, <em>15</em>(3), 786–796. (<a
href="https://doi.org/10.1049/ipr2.12062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of recognizing and tracking a specific human is considered a key technique for serving mobile robots. This paper proposes a real-time tracker that uses a stereo camera to track specific people in different environments. A tracker has been designed to detect the selected target from multiple people by use of block matching algorithm, human detection, and colour histogram comparison. Considering the similar colour of the objects, the predictor of the Kalman filter determines the position of the object. Finally, the mobile robot is moved according to the tracking results with a relative distance between the robot and the target. The effectiveness of the proposed method is demonstrated by experiments on many video sequences and real environments.},
  archive      = {J_IETIP},
  author       = {Tsung-Han Tsai and Chia-Hsiang Yao},
  doi          = {10.1049/ipr2.12062},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {786-796},
  shortjournal = {IET Image Process.},
  title        = {A robust tracking algorithm for a human-following mobile robot},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UDA-net: Densely attention network for underwater image
enhancement. <em>IETIP</em>, <em>15</em>(3), 774–785. (<a
href="https://doi.org/10.1049/ipr2.12061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imaging usually suffers from negative impacts due to the absorption and scattering effects in water. Underwater images thus have unfavourable visual quality to support the work in such environment. This paper addresses the problem of image improvement for single underwater image. The core idea lies in a new enhancement model based on deep learning architecture, in which a feature-level attention model is developed. This model is a multi-scale grid convolutional neural network that can facilitate fusing different types of information during representation learning. According to this information combination, a synergistic pooling mechanism is proposed to extract the channel-wise attention maps to derive the locally weighted features. Therefore, this model can adaptively focus on the feature regions corresponding to degraded patches in one underwater image and improve these patches consistently. Comprehensive experiments are conducted on benchmark and natural underwater images, and it can be demonstrated that this model is effective.},
  archive      = {J_IETIP},
  author       = {Yang Li and Rong Chen},
  doi          = {10.1049/ipr2.12061},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {774-785},
  shortjournal = {IET Image Process.},
  title        = {UDA-net: Densely attention network for underwater image enhancement},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Full-reference tone-mapped images quality assessment.
<em>IETIP</em>, <em>15</em>(3), 763–773. (<a
href="https://doi.org/10.1049/ipr2.12060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various tone mapping operators have been proposed to convert the high dynamic range images to low dynamic ranges to improve visualization on low dynamic range displays. This paper presents a full-reference objective quality assessment index to evaluate the perceived quality of tone-mapped images. The proposed method seamlessly employs the multi-scale structural fidelity, statistical naturalness, colourfulness and the multi-scale free energy of the image to create a similarity score between a produced low dynamic range image and its high dynamic range image. The extensive experiments on three publicly available datasets using Spearman&#39;s rank-order correlation coefficient, Kendall&#39;s rank-order correlation coefficient and receiver operating characteristics analyses indicate the proposed tone-mapped quality index is superior to recently proposed state-of-the-art objective quality indices.},
  archive      = {J_IETIP},
  author       = {Mohammad Reza Faraji},
  doi          = {10.1049/ipr2.12060},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {763-773},
  shortjournal = {IET Image Process.},
  title        = {Full-reference tone-mapped images quality assessment},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-style chinese art painting generation of flowers.
<em>IETIP</em>, <em>15</em>(3), 746–762. (<a
href="https://doi.org/10.1049/ipr2.12059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proposal and development of Generative Adversarial Networks, the great achievements in the field of image generation are made. Meanwhile, many works related to the generation of painting art have also been derived. However, due to the difficulty of data collection and the fundamental challenge from freehand expressions, the generation of traditional Chinese painting is still far from being perfect. This paper specialises in Chinese art painting generation of flowers, which is important and classic, by deep learning method. First, an unpaired flowers paintings data set containing three classic Chinese painting style: line drawing, meticulous, and ink is constructed. Then, based on the collected dataset, a Flower-Generative Adversarial Network framework to generate multi-style Chinese art painting of flowers is proposed. The Flower-Generative Adversarial Network, consisting of attention-guided generators and discriminators, transfers the style among line drawing, meticulous, and ink by an adversarial training way. Moreover, in order to solve the problem of artefact and blur in image generation by existing methods, a new loss function called Multi-Scale Structural Similarity to force the structure preservation is introduced. Extensive experiments show that the proposed Flower-Generative Adversarial Network framework can produce better and multi-style Chinese art painting of flowers than existing methods.},
  archive      = {J_IETIP},
  author       = {Feifei Fu and Jiancheng Lv and Chenwei Tang and Mao Li},
  doi          = {10.1049/ipr2.12059},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {746-762},
  shortjournal = {IET Image Process.},
  title        = {Multi-style chinese art painting generation of flowers},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stochastic approach for automated brain MRI segmentation.
<em>IETIP</em>, <em>15</em>(3), 735–745. (<a
href="https://doi.org/10.1049/ipr2.12058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an approach to segment lesions from brain magnetic resonance images in a fully automatic manner. The proposed idea leverages the strength of classical random walker algorithm and graph cut optimization technique in a single framework. We demonstrate that formulating a “prior” from a stochastic model can ameliorate the need of manually selected seed selection process in the random walker framework, thus making the algorithm fully automatic in a generic manner. By analytically solving a linear system of equations in the random walk process, initial labelling ∈ [0, 1] of each pixel in the image are computed to obtain the likelihood probability. These probabilities are then used to compute the likelihood for the data fidelity term in the energy function to compute the final segmentation, which is minimized by min cut max flow algorithm. Experimental results show the superiority of the proposed method over the state-of-the-art techniques on publicly available datasets.},
  archive      = {J_IETIP},
  author       = {Pubali Chatterjee and Kaushik Das Sharma and Amlan Chakrabarti},
  doi          = {10.1049/ipr2.12058},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {735-745},
  shortjournal = {IET Image Process.},
  title        = {A stochastic approach for automated brain MRI segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep residual deconvolutional networks for defocus blur
detection. <em>IETIP</em>, <em>15</em>(3), 724–734. (<a
href="https://doi.org/10.1049/ipr2.12057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate defocus blur detection has instigated wide research interest for the last few years. However, it is still a meaningful yet challenging machine vision task, and most methods rely on prior knowledge. Convolutional neural networks have proved the huge success for different tasks within the computer vision, and machine learning flew. A simple yet effective method of defocus blur detection was proposed in this paper, which by applying the deep residual convolutional encoder-decoder network. The aims of DRDN is to automatically generate pixel-level predictions for defocus blur images, and reconstruct output detection results of the same size as the input, which by performing several deconvolution operations at multiple scales through the transposed convolution, and skip connection. Afterwards, we used the slide window detection strategy and traversed the input image with a certain stride. Experiments on challenging benchmarks of defocus blur detection show that our algorithm achieved state-of-the-art performance, and powerfully balanced the detection accuracy, and detection time.},
  archive      = {J_IETIP},
  author       = {Kai Zeng and Yaonan Wang and Jianxu Mao and Xianen Zhou},
  doi          = {10.1049/ipr2.12057},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {724-734},
  shortjournal = {IET Image Process.},
  title        = {Deep residual deconvolutional networks for defocus blur detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense spatio-temporal stereo matching for intelligent
driving systems. <em>IETIP</em>, <em>15</em>(3), 715–723. (<a
href="https://doi.org/10.1049/ipr2.12056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of matching stereo images acquired by a stereo system mounted aboard an intelligent vehicle. The main idea behind the new method consists in involving temporal matching between a current stereo pair and its preceding one to achieve the spatial matching of the former stereo by involving the matching results obtained at the last frame. The proposed method is achieved in three main steps. First, an edge based disparity map is derived from the disparity map of the preceding frame, we call it the assisting disparity map (ADM). Second, for each scan-line, a set of local ranges and global ranges are deduced from the ADM to keep only potential matching candidates. Third, the matching is done on the basis of dynamic programming algorithm by involving in the resulting local and global ranges we get from the last step. The proposed approach has been tested on both real and synthetic stereo sequences and the results demonstrate its effectiveness.},
  archive      = {J_IETIP},
  author       = {Zakaria Kerkaou and Mohamed El Ansari and Lhoussaine Masmoudi and Redouan Lahmyed},
  doi          = {10.1049/ipr2.12056},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {715-723},
  shortjournal = {IET Image Process.},
  title        = {Dense spatio-temporal stereo matching for intelligent driving systems},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical JPEG image steganography method according to the
distortion reduction criterion based on an imperialist competitive
algorithm. <em>IETIP</em>, <em>15</em>(3), 705–714. (<a
href="https://doi.org/10.1049/ipr2.12055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most of the digital steganography methods provided for natural digital images, the embedding of the confidential message is based on the minimisation of the defined distortion functions. It is often done based on choosing the most optimal criterion of distortion. Although the distortion functions are designed innovatively, steganography algorithms will be optimal. In such approaches, embedding interactions are often overlooked. Unlike usual images that have areas with a variety of tissue features, there are many smooth areas in medical images that will make the changes more visible if they are manipulated. Therefore, this study presents an adaptive approach that comes from the interactions between the changes made during the embedding algorithm to reduce the probability of recognising the message embedded in medical images and reducing the distortion caused by embedding in a discrete cosine transform space and based on the imperialist competitive algorithm for joint photographic experts group images, especially in medical images due to the importance of information steganography in them. The results obtained show the high efficiency of the proposed algorithm in comparison with the state-of-the-art methods that are presented in this area.},
  archive      = {J_IETIP},
  author       = {Hamidreza Damghani and Farshid Babapour Mofrad and Leila Damghani},
  doi          = {10.1049/ipr2.12055},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {705-714},
  shortjournal = {IET Image Process.},
  title        = {Medical JPEG image steganography method according to the distortion reduction criterion based on an imperialist competitive algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative registration for multi-modality retinal fundus
photographs using directional vessel skeleton. <em>IETIP</em>,
<em>15</em>(3), 696–704. (<a
href="https://doi.org/10.1049/ipr2.12054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an automated registration method for multi-modality retinal fundus photographs based on the directional vessel skeleton. The main purpose is to register two retinal fundus photographs with different modalities of the same scanning region, which can provide multi-modality information for clinicians to diagnose retinal diseases or to make a treatment decision. The directional vessel skeleton of each fundus image is first detected by bias field correction and Gabor filter. The final registered fundus photographs are then obtained by the iterative affine registration between the detected directional vessel skeletons of two photographs. In this work, four kinds of fundus photographs in the macular regions of the patient with diseases, consisting of 20 optical coherence tomography fundus images, 20 colour fundus photographs, 20 fluorescein fundus angiography images and 20 indocyanine green angiography images, are utilised to quantitatively evaluate the proposed method. The root-mean-square errors show an advantageous performance in both registration success rate and accuracy.},
  archive      = {J_IETIP},
  author       = {Wenwen Kong and Pengxiao Zang and Sijie Niu and Dengwang Li},
  doi          = {10.1049/ipr2.12054},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {696-704},
  shortjournal = {IET Image Process.},
  title        = {Iterative registration for multi-modality retinal fundus photographs using directional vessel skeleton},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new blind image conversion complexity metric for
intelligent CMOS image sensors. <em>IETIP</em>, <em>15</em>(3), 683–695.
(<a href="https://doi.org/10.1049/ipr2.12053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many algorithms have been developed for complementary metal–oxide–semiconductor (CMOS) image sensors to speed up analogue-to-digital (A-to-D) conversion of captured images. However, there is no objective blind-image quality metric available to compare and quantify the quality and effectiveness of these speed-up algorithms. In this work, we developed a blind-image quality and complexity metric for this purpose. The proposed metric relies on counting the successive zeros in a code histogram. The proposed metric is called the conversion complexity metric (CCM). The CCM is designed to quantify how complex, and to predict how much time and power consuming a captured image is for A-to-D conversion, mainly by integrating (ramp) type A-to-D converter used in column-parallel architectures of a CMOS image sensor (CIS). The proposed metric, CCM, is tested for linearity, monotonicity, and sensitivity to many types of introduced distortion. The CCM is compared with other no-reference and full-reference image quality and complexity metrics. It accomplished, for brightness change distortion, 99% linearity and 316% sensitivity, providing a computationally efficient blind-image quality metric that no other metrics provide for CIS to intelligently adjust and optimise on-chip analogue and digital signal processing.},
  archive      = {J_IETIP},
  author       = {Mohamed R. Elmezayen and Suat U. Ay},
  doi          = {10.1049/ipr2.12053},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {683-695},
  shortjournal = {IET Image Process.},
  title        = {A new blind image conversion complexity metric for intelligent CMOS image sensors},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of a novel image moment computation in x-ray and
MRI image watermarking. <em>IETIP</em>, <em>15</em>(3), 666–682. (<a
href="https://doi.org/10.1049/ipr2.12052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal polar image moments which are defined over a unit disk, are used often in watermarking applications. One such image moment is polar harmonic Fourier transformation (PHFT). The quality of the extracted watermark depends on the accuracy of the computed image moments. Various methods have already been used for the computation of the moments with varying degrees of accuracy. This study proposes application of a novel image moments computation method which is a synergy of analytic, and numerical techniques, to image watermarking. The computed moment results in a robust watermark extraction. The PHFT have rotation, and scaling invariance property, which allows the watermark to resist various geometric attacks. In addition to the geometric attacks the watermark has the capability to resist various signal processing attacks. The performance of the proposed watermarking scheme is performed with the methods used in recently published articles. The experimental results show that the proposed method has the better performance compared to other techniques, and has better capability to resist various attacks.},
  archive      = {J_IETIP},
  author       = {Manoj K. Singh and Sanoj Kumar and Musrrat Ali and Deepika Saini},
  doi          = {10.1049/ipr2.12052},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {666-682},
  shortjournal = {IET Image Process.},
  title        = {Application of a novel image moment computation in X-ray and MRI image watermarking},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual branch convolutional neural network for copy move
forgery detection. <em>IETIP</em>, <em>15</em>(3), 656–665. (<a
href="https://doi.org/10.1049/ipr2.12051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of digital era has seen a rise in the cases of illegal copying, distribution and forging of images. Even the most secure data channels sometimes suffer to validate the integrity of images. Forgery of multimedia data is devastating in various important applications like defence and satellite. Increased illegal tampering of images has paved way for research in the area of digital forensics. Copy move forgery is one of the various tampering techniques which is used for manipulating an image&#39;s content. A deep learning–based passive Copy Move Forgery Detection algorithm is proposed that uses a novel dual branch convolutional neural network to classify images as original and forged. The dual branch convolutional neural network extracts multi-scale features by employing different kernel sizes in each branch. Fusion of extracted multi-scale features is then performed to achieve a good accuracy, precision and recall scores. Experiment analysis on MICC F-2000 dataset has been performed under two different kernel size combinations. Extensive result analysis and comparative analysis proves the efficacy of proposed architecture over existing architecture in terms of performance scores, computation time, and complexity.},
  archive      = {J_IETIP},
  author       = {Nidhi Goel and Samarjeet Kaur and Ruchika Bala},
  doi          = {10.1049/ipr2.12051},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {656-665},
  shortjournal = {IET Image Process.},
  title        = {Dual branch convolutional neural network for copy move forgery detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent learning with clique structures for prostate
sparse-view CT artifacts reduction. <em>IETIP</em>, <em>15</em>(3),
648–655. (<a href="https://doi.org/10.1049/ipr2.12048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks have achieved great success in streak artifacts reduction. However, there is no special method designed for the artifacts reduction of the prostate. To solve the problem, the artifacts reduction CliqueNet (ARCliqueNet) to reconstruct dense-view computed tomography images form sparse-view computed tomography images is proposed. In detail, first, the proposed ARCliqueNet extracts a set of feature maps from the prostate sparse-view CT image by Clique Block. Second, the feature maps are sent to ASPP with memory to be refined. Thenanother Clique Block is applied to the output of ASPP with memory and reconstruct the dense-view CT images. Later on, reconstructed dense-view CT images are used as new input of the original network. This process is repeated recurrently with memory delivering information between these recurrent stages. The final reconstructed dense-view CT images are the output of the last recurrent stage. Our proposed ARCliqueNet outperforms the SOTA (state-of-the-art) general artifacts reduction methods on the prostate dataset in terms of PSNR (peak signal-to-noise ratio) and SSIM (structural similarity). Therefore, we can draw the conclusion that Clique structures, ASPP with memory and recurrent learning are useful for prostate sparse-view CT Artifacts here.},
  archive      = {J_IETIP},
  author       = {Tiancheng Shen and Yibo Yang and Zhouchen Lin and Mingbin Zhang},
  doi          = {10.1049/ipr2.12048},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {648-655},
  shortjournal = {IET Image Process.},
  title        = {Recurrent learning with clique structures for prostate sparse-view CT artifacts reduction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional neural network for smoke and fire semantic
segmentation. <em>IETIP</em>, <em>15</em>(3), 634–647. (<a
href="https://doi.org/10.1049/ipr2.12046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, global warming has contributed to an increase in the number and intensity of wildfires destroying millions hectares of forest areas and causing many casualties each year. Firemen must therefore have the most effective means to prevent any wildfire from breaking out and to fight the blaze before being unable to contain and extinguish it. This article will present a new network architecture based on Convolutional Neural Network to detect and locate smoke and fire. This network generates fire and smoke masks in an RGB image by segmentation. The purpose of this work is to help firemen in assessing the extent of fire or monitor an incipient fire in real time with a camera embedded in a vehicle. To train this network, a database with the corresponding images and masks has been created. Such a database will allow to compare the performances of different networks. A comparison of this network with the best segmentation networks such as U-Net and Yuan networks has highlighted its efficiency in terms of location accuracy, reduction of false positive classifications such as clouds or haze. This architecture is also efficient in real time.},
  archive      = {J_IETIP},
  author       = {Sebastien Frizzi and Moez Bouchouicha and Jean-Marc Ginoux and Eric Moreau and Mounir Sayadi},
  doi          = {10.1049/ipr2.12046},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {634-647},
  shortjournal = {IET Image Process.},
  title        = {Convolutional neural network for smoke and fire semantic segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image denoising based on BCOLTA: Dataset and study.
<em>IETIP</em>, <em>15</em>(3), 624–633. (<a
href="https://doi.org/10.1049/ipr2.12039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot deburring is an effective method for improving the surface quality of the high-voltage copper contact. The first step of robot deburring is to acquire the burr images. We propose a new burr mathematical model and build a real burr image dataset for burr image denoising. In order to improve burr image denoising effects of the high-voltage copper contact, this study proposes an online burr image denoising algorithm, that is, block cosparsity overcomplete learning transform algorithm (BCOLTA). The penalty term and the condition number are affected by the burr parameter. The clustering and transform alternate minimisation algorithms are adopted to achieve lower computational cost and better denoising effect. In addition, BCOLTA also has a good adaptibility to inherent noise images, especially in Gaussian noise. Compared with other traditional and deep learning algorithms by no reference and full reference image quality assessment methods, BCOLTA has state-of-the-art denoising effects and computational complexity on dealing with burr images. This research will play an important role in the intelligent manufacturing field.},
  archive      = {J_IETIP},
  author       = {Lili Han and Shujuan Li and Xiuping Liu},
  doi          = {10.1049/ipr2.12039},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {624-633},
  shortjournal = {IET Image Process.},
  title        = {Image denoising based on BCOLTA: Dataset and study},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast level set image segmentation driven by a new region
descriptor. <em>IETIP</em>, <em>15</em>(3), 615–623. (<a
href="https://doi.org/10.1049/ipr2.12036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to deal with the intensities inhomogeneities and to overcome the effect of different types of noise in the image segmentation process, we have formulated a new level set function to implement a fast and robust active contour model. The proposed model was formulated by combining the SBGFRLS model and Legendre polynomials. With the aim of ensuring the segmentation accuracy and for dealing in the best way with the presence of noises and inhomogenous distribution of intensity, we define a local region descriptor for image intensities based on the Legendre polynomial. Instead of using the average intensity of the region, we regularised our level set function using a Gaussian filtering process. Experimental results on challenging images demonstrate the efficiency, robustness and the low cost and computational time of our model against the well-known active contour models.},
  archive      = {J_IETIP},
  author       = {Abdelkader Birane and Latifa Hamami},
  doi          = {10.1049/ipr2.12036},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {615-623},
  shortjournal = {IET Image Process.},
  title        = {A fast level set image segmentation driven by a new region descriptor},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An image denoising algorithm based on adaptive clustering
and singular value decomposition. <em>IETIP</em>, <em>15</em>(3),
598–614. (<a href="https://doi.org/10.1049/ipr2.12017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-similarity, a prior of natural images, has attracted much attention. The attribute means that low-rank group matrices can be constructed from similar image patches. For low-rank approximation denoising methods based on singular value decomposition (SVD) the ability to accurately construct group matrices with noise and handle singular values are keys. Here, combining image priors, a two-stage clustering method to adaptively construct group matrices is designed. The method is anti-noise, that is, when noise levels are high, these matrices are more accurate than that constructed by other algorithms. Then, according to the significance of singular values and singular vectors, singular vectors of the low-rank estimations are corrected so that the residual noise in the low-rank estimations is further suppressed. For back projection , the authors use the original noise level and the residual image to adaptively determine projection parameters and new noise levels . So, authors&#39; back projection can provide a good foundation for authors&#39; two-stage denoising methods, better remove noise and preserve image details. Experimental results show that compared with the existing state-of-the-art denoising algorithms, the proposed algorithm achieves competitive denoising performances in terms of quantitative metrics and preserving details. Especially with the increase of noise, the competitiveness of authors&#39; algorithms is gradually enhanced.},
  archive      = {J_IETIP},
  author       = {Ping Li and Hua Wang and Xuemei Li and Caiming Zhang},
  doi          = {10.1049/ipr2.12017},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {598-614},
  shortjournal = {IET Image Process.},
  title        = {An image denoising algorithm based on adaptive clustering and singular value decomposition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hard exudate segmentation in retinal image with attention
mechanism. <em>IETIP</em>, <em>15</em>(3), 587–597. (<a
href="https://doi.org/10.1049/ipr2.12007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is the main reason that causes preventable blindness. Hard exudate is one of the earliest signs of diabetic retinopathy. Precise detection of hard exudate is helpful for the early diagnosis of diabetic retinopathy. Fully convolutional network (FCN) shows great performance on hard exudate segmentation task. However, there are limitations for fully convolutional network to build long-range dependencies in different regions of the image. Convolution operator extract features in local area, segmentation results based on local features are likely to be wrong in some cases. Another channel attention method was proposed, and two different attention modules are used in the segmentation model. In this way, long-range dependencies across different image regions are built efficiently in different stages of feature extraction. In addition, a new loss function is designed to deal with the data imbalance problem in hard exudate segmentation task. The proposed method was evaluated by two public datasets, and the comparative experiments show the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Ze Si and Dongmei Fu and Yang Liu and Zhicheng Huang},
  doi          = {10.1049/ipr2.12007},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {587-597},
  shortjournal = {IET Image Process.},
  title        = {Hard exudate segmentation in retinal image with attention mechanism},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel reversible data hiding based on adaptive
block-partition and payload-allocation method. <em>IETIP</em>,
<em>15</em>(2), 571–585. (<a
href="https://doi.org/10.1049/ipr2.12050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reversible data hiding method based on partition is researched because it can effectively reduce shifting distortion in given embedding capacity. However, traditional partition is to divide an image into equal-sized blocks, which cannot be divided reasonably according to the content of the image. In order to achieve dynamic partition and effectively utilize the complexity of the image, this paper proposes a novel reversible data hiding based on adaptive block-partition and payload-allocation method. In this technique, instead of equal partition, adaptive block-partition is proposed to establish multiple histograms by dividing the cover image into several image blocks of different sizes dynamically, and the image blocks of different sizes are processed successively through multiple sorting and implement adaptive payload allocation according to complexity, then the data is embedded into two sides of prediction-error histograms to effectively reduce the shifting distortion. Experimental results show that the proposed method is superior to the state-of-the-art traditional fixed-sized blocking-based reversible data hiding methods.},
  archive      = {J_IETIP},
  author       = {Yang Yang and Genyan Huang and Tianrui Zou and Weiming Zhang},
  doi          = {10.1049/ipr2.12050},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {571-585},
  shortjournal = {IET Image Process.},
  title        = {A novel reversible data hiding based on adaptive block-partition and payload-allocation method},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infrared small target detection based on non-convex triple
tensor factorisation. <em>IETIP</em>, <em>15</em>(2), 556–570. (<a
href="https://doi.org/10.1049/ipr2.12049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present infrared target detection system, simultaneously achieving both the target detection performance, as well as, computing efficiency is considered as a big task. To, address the above said issue, a non-convex triple tensor factorisation is incorporated into the existing infrared patch tensor model in the proposed work. In the proposed model (triple tensor factorisation-infrared patch tensor), local prior information using linear structure tensor and corner strength is incorporated so that the strong clutters in the background can be easily suppressed and the target can be detected correctly. Finally, the method proposed, was solved by alternating direction method of the multiplier. Large number of experiments were conducted and the results presented in the experiments suggest that the method proposed has shown good performance for background suppression, as well as, for the target detection in the clutter environment, when compared with the other baseline methods.},
  archive      = {J_IETIP},
  author       = {Sur Singh Rawat and Sashi Kant Verma and Yatindra Kumar},
  doi          = {10.1049/ipr2.12049},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {556-570},
  shortjournal = {IET Image Process.},
  title        = {Infrared small target detection based on non-convex triple tensor factorisation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid BPSO-SVM for feature selection and classification
of ocular health. <em>IETIP</em>, <em>15</em>(2), 542–555. (<a
href="https://doi.org/10.1049/ipr2.12047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma and diabetic retinopathy are the most common eye diseases and the leading cause of blindness around the world. The prime objective of this study is to devise and develop an experimental computer-aided diagnosis system to provide an efficient way for assisting the ophthalmologist in early detection of ocular diseases such as glaucoma and diabetic retinopathy. The proposed technique follows three stages: Pre-processing, feature selection and classification. Initially, the fundus image is pre-processed to extract the green channel image, and the obtained green channel image is further enhanced using contrast limited adaptive histogram equalisation technique. Three different kinds of features: Clinical features, transform domain features and structural features are utilised to extract the relevant information from the enhanced fundus images. To avoid redundant information, an improved feature selection mechanism is used to select the optimum set of features from the extracted features. Subsequently, the selected features are used to train the support vector machine classifier for the classification of the retinal diseases with 10-fold cross-validation. The performance of the proposed method is assessed using eight different quantitative evaluation measures. The experimental results demonstrate the effectiveness of the proposed work over prior works for the early detection of ocular diseases.},
  archive      = {J_IETIP},
  author       = {B. Keerthiveena and S. Esakkirajan and Badri Narayan Subudhi and T. Veerakumar},
  doi          = {10.1049/ipr2.12047},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {542-555},
  shortjournal = {IET Image Process.},
  title        = {A hybrid BPSO-SVM for feature selection and classification of ocular health},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fractional-order generalized laguerre moments and moment
invariants for grey-scale image analysis. <em>IETIP</em>,
<em>15</em>(2), 523–541. (<a
href="https://doi.org/10.1049/ipr2.12044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, a new set of fractional-order moments, named fractional-order generalized Laguerre moments (FGLM), is introduced. These proposed moments are defined on the Cartesian coordinate system and their basis functions are represented by the fractional-order generalized Laguerre polynomials. Contrary to the classical Chebyshev, Legendre and Gegenbauer moments, which provide only global feature, our proposed FGLM have the ability to extract both global and local features. Moreover, a new set of rotation, scale and translation invariants of the FGLM, is derived and introduced for image classification and invariant pattern recognition. Just as important, we have presented a systematic parameter selection method for finding the optimal fractional parameter values with respect to pattern recognition applications. Finally, several recursive methods for reducing the computation time of our proposed invariants are also provided in this study. Therefore, to demonstrate the performance of the introduced fractional-order moments and moment invariants, a number of experimental analysis are performed in terms of global and local features extraction, robustness to noise, invariance to geometric deformations, object recognition and computational speed. The presented theoretical and experimental results clearly show that the proposed fractional-order moments and their corresponding invariants could be extremely useful in the field of image analysis.},
  archive      = {J_IETIP},
  author       = {Rachid Benouini and Imad Batioua and Khalid Zenkouar and Said Najah},
  doi          = {10.1049/ipr2.12044},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {523-541},
  shortjournal = {IET Image Process.},
  title        = {Fractional-order generalized laguerre moments and moment invariants for grey-scale image analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Digital image steganalysis: A survey on paradigm shift from
machine learning to deep learning based techniques. <em>IETIP</em>,
<em>15</em>(2), 504–522. (<a
href="https://doi.org/10.1049/ipr2.12043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography, a branch of data hiding techniques aims to hide confidential information within any digital media by obscuring the existence of hidden information. On the contrary, steganalysis aims to detect steganography. With the advent of powerful steganographic algorithms, the process of cracking them became very challenging. Traditional steganalysis following machine learning principle employs a two-step process, with first process extracting highly sophisticated features capable of discriminating hidden message from original data and second process classifying the input as innocent or guilty. In recent years, deep learning which has its roots in artificial neural networks emerged as a brilliant alternative for many computer vision tasks. A review of recent research works in deep learning based digital image steganalysis is presented here. The paradigm shift from machine learning approaches to employing more promising deep learning architectures, observed with the current research community and hence in literature has been presented here in chronological order. Deep learning can unify the two-step process into a single process by giving the ability for machine to learn end-to-end by itself. The use of Convolutional Neural Networks to perform steganalysis in spatial or transform or combination of both domains has effectively lowered the detection error rates.},
  archive      = {J_IETIP},
  author       = {Arivazhagan Selvaraj and Amrutha Ezhilarasan and Sylvia Lilly Jebarani Wellington and Ananthi Roy Sam},
  doi          = {10.1049/ipr2.12043},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {504-522},
  shortjournal = {IET Image Process.},
  title        = {Digital image steganalysis: A survey on paradigm shift from machine learning to deep learning based techniques},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new two-stage method for single image rain removal.
<em>IETIP</em>, <em>15</em>(2), 492–503. (<a
href="https://doi.org/10.1049/ipr2.12040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with video de-raining, single image de-raining is more technically difficult due to the lack of temporally redundant information. This paper proposes a new two-stage method for single image de-raining. In the first stage, the authors develop an effective two-step model to detect the rain streaks by taking pixel intensity, and direction of rain streaks as priors. In the second stage, the rain repair process is performed at the patch level. The authors first define a way to search for similar patches of each patch, and then group the similar patches together to form a matrix. Finally, a low-rank matrix completion technique is utilized to recover the rain-stained pixels based on the rain map obtained from the first stage. Compared with several state-of-the-art methods, authors&#39; proposed method is competitive in terms of the abilities of removing rain streaks, and preserving image details.},
  archive      = {J_IETIP},
  author       = {Jian Zhu and Peiyu Liu and Yu Luo and Jie Ling and Enhua Wu},
  doi          = {10.1049/ipr2.12040},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {492-503},
  shortjournal = {IET Image Process.},
  title        = {A new two-stage method for single image rain removal},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vehicle detection in aerial images based on lightweight deep
convolutional network. <em>IETIP</em>, <em>15</em>(2), 479–491. (<a
href="https://doi.org/10.1049/ipr2.12038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection in aerial images is an interesting and challenging task. Traditional methods are based on sliding-window search and handcrafted features, which limits the representation power and has heavy computational costs. Recent research have shown that deep-learning algorithms are widely used in the field of object detection. However, the deep-learning algorithms still face many difficulties and challenges in the object detection under the aerial scene. Meanwhile, the high computational cost of detection algorithms lead to low-detection efficiency. In this study, we build a fast and accurate lightweight detection framework for vehicle detection in aerial scenes. The proposed detection method improves the expressive ability of detection network and significantly reduces the amount of calculations in the model. Meanwhile, setting suitable anchor boxes according to the size of the object vehicles have been introduced in our model, which also effectively improves the performance of the detection. In addition, we have published a new aerial vehicle image dataset and verified the effectiveness of our method. In the Munich dataset and our dataset, our method achieves 85.8% and 91.2% of the mean average precision (mAP), and its detection time is 1.78 and 0.048 s on Nvidia Titan XP. Our results show that the proposed framework achieves significant improvement over several alternatives and state-of-the-art schemes with higher accuracy and less detection time.},
  archive      = {J_IETIP},
  author       = {Jiaquan Shen and Ningzhong Liu and Han Sun},
  doi          = {10.1049/ipr2.12038},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {479-491},
  shortjournal = {IET Image Process.},
  title        = {Vehicle detection in aerial images based on lightweight deep convolutional network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Facial expression recognition using a combination of
enhanced local binary pattern and pyramid histogram of oriented
gradients features extraction. <em>IETIP</em>, <em>15</em>(2), 468–478.
(<a href="https://doi.org/10.1049/ipr2.12037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic facial expression recognition, which has many applications such as drivers, patients, and criminals&#39; emotions recognition, is a challenging task. This is due to the variety of individuals and facial expression variability in different conditions, for instance, gender, race, colour and changing illumination. In addition, there are many regions in a face image such as forehead, mouth, eyes, eyebrows, nose, cheeks and chin, and extracting features of all these regions are expensive in terms of computational time. Each of the six basic emotions of anger, disgust, fear, happiness, sadness and surprise affect some regions more than the other regions. The goal of this study is to evaluate the performance of enhanced local binary pattern, pyramid histogram of oriented gradients feature-extraction algorithms and their combination in terms of recognition accuracy, feature vector length and computational time on one, two and three combined regions of a face image. Our experimental results show that the combination of both feature-extraction algorithms yields an average recognition accuracy of 95.33% using three regions, that is, the mouth, nose and eyes on Cohn–Kanade dataset. Besides, the mouth region is the most important part in terms of accuracy in comparison to eyes, nose and combination of both eyes and nose regions.},
  archive      = {J_IETIP},
  author       = {Maede Sharifnejad and Asadollah Shahbahrami and Alireza Akoushideh and Reza Zare Hassanpour},
  doi          = {10.1049/ipr2.12037},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {468-478},
  shortjournal = {IET Image Process.},
  title        = {Facial expression recognition using a combination of enhanced local binary pattern and pyramid histogram of oriented gradients features extraction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of breast mass in two-view mammograms via
deep learning. <em>IETIP</em>, <em>15</em>(2), 454–467. (<a
href="https://doi.org/10.1049/ipr2.12035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the second deadliest cancer among women. Mammography is an important method for physicians to diagnose breast cancer. The main purpose of this study is to use deep learning to automatically classify breast masses in mammograms into benign and malignant. This study proposes a two-view mammograms classification model consisting of convolutional neural network (CNN) and recurrent neural network (RNN), which is used to classify benign and malignant breast masses. The model is composed of two branch networks, and two modified ResNet are used to extract breast-mass features of mammograms from craniocaudal (CC) view and mediolateral oblique (MLO) view, respectively. In order to effectively utilise the spatial relationship of the two-view mammograms, gate recurrent unit (GRU) structures of RNN is used to fuse the features of the breast mass from the two-view. The digital database for screening mammography (DDSM) be used for training and testing our model. The experimental results show that the classification accuracy, recall and area under curve (AUC) of our method reach 0.947, 0.941 and 0.968, respectively. Compared with previous studies, our method has significantly improved the performance of benign and malignant classification.},
  archive      = {J_IETIP},
  author       = {Hua Li and Jing Niu and Dengao Li and Chen Zhang},
  doi          = {10.1049/ipr2.12035},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {454-467},
  shortjournal = {IET Image Process.},
  title        = {Classification of breast mass in two-view mammograms via deep learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Completely blind image quality assessment via contourlet
energy statistics. <em>IETIP</em>, <em>15</em>(2), 443–453. (<a
href="https://doi.org/10.1049/ipr2.12034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An aim of completely blind image quality assessment (BIQA) is to develop algorithms which can grade image quality without any prior knowledge of the images. Here, a new contourlet energy statistics based completely on blind opinion-unaware BIQA (OU-BIQA) method is proposed, which can predict the perceptual severity of a range of image distortion types without requiring any prior knowledge. According to the energy distribution of the contourlet sub-bands of natural images in log-domain, the lower-scale sub-band energy can be predicted by the corresponding higher-scale sub-band energies of distorted images. A quality model is then constructed by quantifying the difference between predicted energy and realistic energy. Meanwhile, an effective method for adjusting and compensating an undesired distortion is integrated into the quality model. Experimental results show that the proposed new method outperforms state-of-the-art OU-BIQA models on relevant portions of TID2013 database, and is competitive on the LIVE IQA database. Moreover, the proposed model is very fast, suggesting a real-time solution to high-performance BIQA.},
  archive      = {J_IETIP},
  author       = {Chaofeng Li and Tuxin Guan and Yuhui Zheng and Bo Jin and Xiaojun Wu and Alan Bovik},
  doi          = {10.1049/ipr2.12034},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {443-453},
  shortjournal = {IET Image Process.},
  title        = {Completely blind image quality assessment via contourlet energy statistics},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised approach for traffic motion patterns
extraction. <em>IETIP</em>, <em>15</em>(2), 428–442. (<a
href="https://doi.org/10.1049/ipr2.12033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic analysis, understanding typical activities, and identifying vehicle behaviour in crowded traffic scenes are fundamental and challenging tasks for traffic video surveillance. Some recent researches have been using machine learning approaches to extract meaningful patterns occurring in a traffic scene, for example, intersection. In this regard, we convert visual patterns and features to visual words using dense and sparse optical flow and learning traffic motion patterns with group sparse topical coding (GSTC) algorithm. In the first step of the proposed algorithm, the input traffic video is divided into non-overlapping clips. After that, motion vectors are extracted using dual TV-L1 as a dense optical flow and Lucas–Kanade as a sparse optical flow and converted to flow words. For learning traffic motion patterns, the GSTC algorithm, that is, a non-probabilistic topic model (TM) has been applied. These patterns represent priors on observable motion, which can be utilised to describe a scene and answer behaviour questions such as what are the motion patterns in a traffic scene and what is going on. The experimental results which have been obtained using a real dataset, QUML, show that the combination of the GSTC + dual TV-L1 extracts more traffic motion patterns in comparison with the GSTC + Lucas–Kanade and previous studies.},
  archive      = {J_IETIP},
  author       = {Amin Moradi and Asadollah Shahbahrami and Alireza Akoushideh},
  doi          = {10.1049/ipr2.12033},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {428-442},
  shortjournal = {IET Image Process.},
  title        = {An unsupervised approach for traffic motion patterns extraction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy based iterative matting technique for underwater
images. <em>IETIP</em>, <em>15</em>(2), 419–427. (<a
href="https://doi.org/10.1049/ipr2.12032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents an iterative matting technique for extraction of underwater objects from images. The technique adopts histogram division and stretching to obtain multiple images of different contrast levels that exhibit all image details. For each contrast image, alpha matte is produced and is further refined with every iteration. In the end, fuzzy weights are assigned to the alpha mattes obtained at different contrast levels that are combined using weighted average. The resultant alpha matte thus includes more accurate pixels from multiple alpha mattes and generates much refined matte image. The proposed technique is tested, visually and quantitatively, on a manual dataset containing 50 images. The less MSE shows that the proposed technique achieves noticeably higher accuracy as compared with contemporary image matting techniques.},
  archive      = {J_IETIP},
  author       = {Benish Amin and M Mohsin Riaz and Abdul Ghafoor},
  doi          = {10.1049/ipr2.12032},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {419-427},
  shortjournal = {IET Image Process.},
  title        = {Fuzzy based iterative matting technique for underwater images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Providing clear pruning threshold: A novel CNN pruning
method via l0 regularisation. <em>IETIP</em>, <em>15</em>(2), 405–418.
(<a href="https://doi.org/10.1049/ipr2.12030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning is a significant way to improve the practicability of convolution neural networks (CNNs) by removing the redundant structure of the network model. However, in most of the existing network pruning methods l 1 or l 2 regularisation is applied to parameter matrices and the manual selection of pruning threshold is difficult and labor-intensive. A novel CNNs network pruning method via l 0 regularisation is proposed, which adopts l 0 regularisation to expand the saliency gap between neurons. A half-quadratic splitting (HQS) based iterative algorithm is put forward to calculate the approximation solution of l 0 regularisation, which makes the joint optimisation problem of regularisation term and training loss function can be solved by various gradient-based algorithms. Meanwhile, a hyperparameters selection method is designed to make most of the hyperparameters in the algorithm can be determined by examining the pre-trained model. The results of experiments on MNIST, Fashion-MNIST and CIFAR100 show that the proposed method can provide a much clearer pruning threshold by widening the saliency gap, and achieve a similar or even better compression performance, compared with the state-of-the-art studies.},
  archive      = {J_IETIP},
  author       = {Guo Li and Gang Xu},
  doi          = {10.1049/ipr2.12030},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {405-418},
  shortjournal = {IET Image Process.},
  title        = {Providing clear pruning threshold: A novel CNN pruning method via l0 regularisation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On learning based compressed sensing for high resolution
image reconstruction. <em>IETIP</em>, <em>15</em>(2), 393–404. (<a
href="https://doi.org/10.1049/ipr2.12029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing (CS) or compressive sampling has shown an enormous potential to reconstruct a signal from its highly under-sampled observations. A high dimensional image processing system can adopt the CS paradigm to reduce the storage and the transmission burden. However, a large sensing system (matrix) is required to capture high dimensional images. The CS reconstruction algorithms demand heavy computational requirement due to the use of large sensing matrix. It sometimes becomes impractical to implement a sensing system of the desired size due to the limited access to the CPU memory. To address this issue, the present work proposes a deep learning based CS framework that uses a convolutional neural network to enable capturing of a high dimensional image by utilizing a multi-layer subsampling and filtering operations. The proposed approach uses another convolutional neural network that reconstructs the original image without depending on the sensing network at a significantly reduced computational cost. Extensive simulation results show that the proposed method reconstructs a high dimensional image with an improved PSNR value by dB, SSIM value by and FSIM value by compared to the other state-of-the-art methods in less than 1.4 seconds.},
  archive      = {J_IETIP},
  author       = {Sheikh Rafiul Islam and Santi P. Maity and Ajoy Kumar Ray},
  doi          = {10.1049/ipr2.12029},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {393-404},
  shortjournal = {IET Image Process.},
  title        = {On learning based compressed sensing for high resolution image reconstruction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A feature-optimized faster regional convolutional neural
network for complex background objects detection. <em>IETIP</em>,
<em>15</em>(2), 378–392. (<a
href="https://doi.org/10.1049/ipr2.12028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks are playing an increasingly important role in the field of object detection. However, the complex background of the detected image, the limited receptive field by the fixed geometry of the convolution kernel when building the model, and the positioning and pooling deviation from the region of interest are still important factors that affect the detection accuracy. In this paper, an improved algorithm is proposed for target detection based on Faster regional convolutional neural network. In the bounding box positioning phase, an improved interpolation algorithm-Newton&#39;s parabolic interpolation is proposed instead of bilinear interpolation, after ROI size normalized by extending a parallel branch of tensor to weaken the negative impact of complex background on prospects in the phase of feature extraction using neural network recently popular attention CBAM mechanism model and the deformable convolution. Without bells and whistles, a series of experiments show that our method has higher target detection accuracy on the datasets PASCAL VOC2007, VOC2012, COCO 2014 and DIOR. Hence, the method is effective for actual target recognition tasks in complex background environments. The authors hope that the method will contribute to future research. Code has been made available at: https://github.com/liumaozhen-lmz/Faster_R-CNN_Attention.git.},
  archive      = {J_IETIP},
  author       = {Kun Wang and Maozhen Liu},
  doi          = {10.1049/ipr2.12028},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {378-392},
  shortjournal = {IET Image Process.},
  title        = {A feature-optimized faster regional convolutional neural network for complex background objects detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interest point detection from multi-beam light detection and
ranging point cloud using unsupervised convolutional neural network.
<em>IETIP</em>, <em>15</em>(2), 369–377. (<a
href="https://doi.org/10.1049/ipr2.12027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest point detection plays an important role in many computer vision applications. This work is motivated by the light detection and ranging odometry task in autonomous driving. Existing methods are not capable of detecting enough interest points in unstructured scenarios where there are little constructions or trees around, and correspondingly light detection and ranging odometry will fail to continuous localisation. An interest point detector is proposed for detecting interest points from multi-beam light detection and ranging point cloud using unsupervised convolutional neural network. The point cloud is projected into a two-dimensional structured data according to the scanning geometry. Then the convolutional neural network filters trained in an unsupervised manner are used to generate a local feature map with the two-dimensional structured data as input. Finally, interest points are obtained by extracting the grids that have significant differences with their neighbour grids. Based on an odometry benchmark, the experiments show that the proposed interest point detector can capture more local details, which contributes to more than 16% error decrease in point cloud registration in highway scenes.},
  archive      = {J_IETIP},
  author       = {Deyu Yin and Qian Zhang and Jingbin Liu and Xinlian Liang and Yunsheng Wang and Shoubin Chen and Jyri Maanpää and Juha Hyyppä and Ruizhi Chen},
  doi          = {10.1049/ipr2.12027},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {369-377},
  shortjournal = {IET Image Process.},
  title        = {Interest point detection from multi-beam light detection and ranging point cloud using unsupervised convolutional neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The trade-off between accuracy and the complexity of
real-time background subtraction. <em>IETIP</em>, <em>15</em>(2),
350–368. (<a href="https://doi.org/10.1049/ipr2.12026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction used in object detection, tracking and action recognition is a typical method that separates foreground objects from the background. These applications require accuracy and a complexity reduction technique. Some approaches have been proposed to either increase accuracy or decrease complexity. However, the trade-off between increasing accuracy and reducing the complexity of background subtraction is a big challenge. To address this issue, a background subtraction-based real-time moving object-detection approach is proposed. The key contribution in authors&#39; proposal is to use a colour image and a novel colour-gradient blending fused image to achieve accurate background/foreground segmentation. The fused image is a combination of a gradient image and a colour image to correct illumination variations and preserve the edge information. Also, thresholds are adaptively selected based on the dynamic background behaviour to attain a more robust classification system. The proposed model based on real-time and complex videos from the CD-2012 and CD-2014 change detection data sets, and the CMD data set is evaluated. Experimental results indicate that authors&#39; method processes around 43 frames per second and requires six bytes of memory per pixel, which is noticeably more efficient and less complex than other background subtraction methods.},
  archive      = {J_IETIP},
  author       = {Md Alamgir Hossain and VanDung Nguyen and Eui-Nam Huh},
  doi          = {10.1049/ipr2.12026},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {350-368},
  shortjournal = {IET Image Process.},
  title        = {The trade-off between accuracy and the complexity of real-time background subtraction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Atom search-jaya-based deep recurrent neural network for
liver cancer detection. <em>IETIP</em>, <em>15</em>(2), 337–349. (<a
href="https://doi.org/10.1049/ipr2.12019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of liver cancer is the fundamental requirement of computer-aided diagnosis in the clinical sector. The traditional methods used in the liver detection process are not effective in accurately detecting the tumour region using a large-sized dataset. Moreover, segmenting the large intensity of the tumour region is a complex issue with the existing methods. To overcome these issues, an accurate and efficient liver cancer detection method named atom search-Jaya-based deep recurrent neural network is proposed in this research. The proposed method mimics the atomic motion based on the interaction forces and the constraint forces of the hybrid molecules. The optimal solution is revealed through the fitness measure, which in turn accepts the minimal error value as the optimal solution. The weights of the classifier are optimally updated based on the position of the atom with respect to the iterations. The proposed atom search-Jaya-based deep recurrent neural network attained significantly better performance in accurately detecting the tumor region using the exploration ability of atoms in the search space. The results obtained by the proposed model in terms of accuracy, specificity, sensitivity, and precision are 93.64%, 96%, 95%, and 94.88%, respectively, while considering the three features using 80% of training data.},
  archive      = {J_IETIP},
  author       = {Mariappan Navaneethakrishnan and Subbiah Vairamuthu and Govindaswamy Parthasarathy and Rajan Cristin},
  doi          = {10.1049/ipr2.12019},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {337-349},
  shortjournal = {IET Image Process.},
  title        = {Atom search-jaya-based deep recurrent neural network for liver cancer detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-level up-projection network for single image
super-resolution. <em>IETIP</em>, <em>15</em>(2), 325–336. (<a
href="https://doi.org/10.1049/ipr2.12014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most convolutional neural network-based single image super-resolution (SR) methods do not take full account of the hierarchical features of the original low-resolution (LR) images, including the intra-channel spatial feature information and the inter-channel feature information, which decreases the representational capacity of the network. A deep multi-level upprojection network (DMUN) is proposed to solve this problem. Local feature up-projection unit is adopted in DMUN to obtain high-resolution (HR) feature of different levels and then to reconstruct the SR image. Residual up-projection group in DMUN mines the hierarchical LR feature information and its corresponding HR residual information recursively. A residual recorrection mechanism is further introduced, which adopts HR residual information to re-correct HR features and enrich the details of the output image. Finally, the original residual block with spatial-and-channel attention mechanism is improved, which adaptively recalibrates features by considering the intra-channel spatial relationships and the inter-channel pixel-wise interdependencies simultaneously. Experiments on benchmark datasets show that DMUN achieves favourable performance against state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yan Shen and Liao Zhang and Yun Chen and Yi Xie and Zhongli Wang and Xiaotao Shao},
  doi          = {10.1049/ipr2.12014},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {325-336},
  shortjournal = {IET Image Process.},
  title        = {Deep multi-level up-projection network for single image super-resolution},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Review of automated segmentation approaches for knee images.
<em>IETIP</em>, <em>15</em>(2), 302–324. (<a
href="https://doi.org/10.1049/ipr2.12045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knee disorders are common among the human population. Knee osteoarthritis (OA) is the most widespread knee joint disorder, which may require surgical treatment. The detection and diagnosis of knee joint disorders from medical images demand enormous human effort and time. The development of a computer-aided diagnosis (CAD) system can notably minimise the burden of medical experts and remove the intra-observer and inter-observer variations. To achieve the goal, the highly challenging research problem of knee image segmentation has been frequently paid attention in past years, which can be efficiently applied in the development of the CAD system. Knee image segmentation is a challenging task owing to the image contrasts, intensity variations, shape irregularities, and the presence of thin cartilage structures. Therefore, this paper presents a literature review of automated segmentation approaches mainly focused on the segmentation of knee cartilage and bone, with respect to the underlying technical aspects, datasets used, and the performance reported. The paper also presents the growth from classical segmentation approaches towards the deep learning approaches in the knee image segmentation. Owing to the varying quality and complexity of different knee image datasets, this paper abstains from doing a rigorous comparative evaluation of image segmentation approaches.},
  archive      = {J_IETIP},
  author       = {Ridhma and Manvjeet Kaur and Sanjeev Sofat and Devendra K. Chouhan},
  doi          = {10.1049/ipr2.12045},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {302-324},
  shortjournal = {IET Image Process.},
  title        = {Review of automated segmentation approaches for knee images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning for occluded and multi-scale pedestrian
detection: A review. <em>IETIP</em>, <em>15</em>(2), 286–301. (<a
href="https://doi.org/10.1049/ipr2.12042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection, as a research hotspot in the field of computer vision, is widely used in many fields, such as automatic driving, video surveillance, robots and so on. In recent years, with the rapid development of deep learning, pedestrian detection technology has made unprecedented breakthroughs. However, it fails to saturate pedestrian detection research, and there are still many problems to be solved. This study reviews the current research status of pedestrian detection methods based on deep learning. In the first place, we summarised the research results of two stage and one stage pedestrian detection based on deep learning, also summarised and analysed the improvement methods. Meanwhile, we focused on the occlusion and multi-scale problems of pedestrian detection and discussed the corresponding solutions. At last, we induced the pedestrian detection datasets and evaluation methods and prospected the development trend of deep learning in pedestrian detection.},
  archive      = {J_IETIP},
  author       = {Yanqiu Xiao and Kun Zhou and Guangzhen Cui and Lianhui Jia and Zhanpeng Fang and Xianchao Yang and Qiongpei Xia},
  doi          = {10.1049/ipr2.12042},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {286-301},
  shortjournal = {IET Image Process.},
  title        = {Deep learning for occluded and multi-scale pedestrian detection: A review},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Underwater image restoration: A state-of-the-art review.
<em>IETIP</em>, <em>15</em>(2), 269–285. (<a
href="https://doi.org/10.1049/ipr2.12041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imaging is one of the hot areas of research and is receiving considerable attention from the research community due to the challenges involved. Underwater images are prone to various distortions like poor contrast and colour deviation. Light scattering and light absorption in water medium are the main reasons for degradation in subaquatic images. Scattering of visible-light energy reduces the sharpness of image whereas varying degrees of light attenuation travelling in water results in the colour change. Restoration of distorted underwater images is an ill-posed and challenging problem. Various techniques and methodologies are being used to process and restore underwater images. In this study, we present a state-of-the-art review of various conventional and computer vision-based algorithms and techniques, developed so far, to present a clearer view of the methods used for underwater image restoration. We discuss various conditions for which the schemes have been developed as well as highlight the quality assessment methods used to evaluate their performance. We compare various state-of-art schemes based on various subjective and objective indices and discuss future research directions in the field of underwater image restoration.},
  archive      = {J_IETIP},
  author       = {Sheezan Fayaz and Shabir A. Parah and G. J. Qureshi and Vijaya Kumar},
  doi          = {10.1049/ipr2.12041},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {269-285},
  shortjournal = {IET Image Process.},
  title        = {Underwater image restoration: A state-of-the-art review},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new recognition algorithm for high-voltage lines based on
improved LSD and convolutional neural networks. <em>IETIP</em>,
<em>15</em>(1), 260–268. (<a
href="https://doi.org/10.1049/ipr2.12031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of high-voltage transmission and artificial intelligence technology, unmanned line inspection has become the inevitable trend of current electric power inspection. A new recognition algorithm for high-voltage lines is proposed based on colour (Red, Green, Blue) RGB image to support the unmanned line inspection. Firstly, in order to solve the problem of missing weak edges in image edge detection, an improved Canny algorithm is proposed. Fourier transform Gaussian filter is introduced to enhance the high-frequency signal of the image, which makes the extracted edge information more complete. At the same time, an improved line segment detector (LSD) algorithm is developed to extract the high-voltage line. The complementary edge information of the three channels of the colour RGB image is analyzed, and the calculation formula of the horizontal line angle is improved, which greatly reduces the possibility of false detection and missed detection in the high-voltage line extraction. In addition, the convolution neural network (CNN) is used to accurately recognize the extracted high-voltage lines, which reduces the interference of non–high-voltage lines. Simulation results show that the proposed algorithm has high recognition accuracy and strong robustness in the complex environment.},
  archive      = {J_IETIP},
  author       = {Yanhong Luo and Xue Yu and Dongsheng Yang},
  doi          = {10.1049/ipr2.12031},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {260-268},
  shortjournal = {IET Image Process.},
  title        = {A new recognition algorithm for high-voltage lines based on improved LSD and convolutional neural networks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel global convolutional network for semantic image
segmentation. <em>IETIP</em>, <em>15</em>(1), 252–259. (<a
href="https://doi.org/10.1049/ipr2.12025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel convolutional neural network for fast semantic segmentation is presented. Deep convolutional neural networks have achieved great progress in the task of vision scene understanding. While the increase of the accuracy mainly depends on the increase of depth and width. This slows down large networks and consumes power. A fast and efficient convolutional neural network, PGCNet, aiming at segmenting high-resolution images with a high speed is introduced. Compared with the competitive methods, the generated model has high performance with fewer parameters and floating point operations. First, a lightweight general architecture pre-trained on ImageNet is relied on as the main encoder. Then, a novel lateral connection module to better transmit features from encoder to decoder. Third, a powerful method termed as PGCN block to extract features of each block in the encoder is proposed and an edge decoder is applied as a supervision for pixels on the edge of stuff and things during training. Experiments show that this method has great advantages. Based on the proposed PGCNet, 75.8% mean IoU is achieved on the cityscapes test set and 35.4 Hz on a standard Cityscapes image on GTX1080Ti.},
  archive      = {J_IETIP},
  author       = {Xing Bai and Jun Zhou},
  doi          = {10.1049/ipr2.12025},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {252-259},
  shortjournal = {IET Image Process.},
  title        = {Parallel global convolutional network for semantic image segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gender discrimination, age group classification and carried
object recognition from gait energy image using fusion of parallel
convolutional neural network. <em>IETIP</em>, <em>15</em>(1), 239–251.
(<a href="https://doi.org/10.1049/ipr2.12024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age and gender are the two key attributes for healthy social interactions, access control, intelligence marketing etc. Likewise, carried object recognition helps in identifying owner of the baggage being abandoned or the person littering in the public places. The above-mentioned surveillance task displays discriminative characteristics in gait. Primates can accomplish scene context understanding and reacting to different circumstances with varying reflexes with ease. Human beings achieve this by recollecting prior experiences and adapting to new situations quickly. Modelling the human behaviour, this research work has combined customized and learnable filters so that knowledge database can always be kept up to date, as well as, provides flexibility in learning new contexts. Thus, a specialized parallel deep convolutional neural network architecture with customized filters that extracts intrinsic characteristics and data driven learnable filters are fused to enhance the performance of single convolutional neural network is proposed. From the experimentation it is observed that, the learning is augmented when customized filters and learnable filters are fused together. Results show that the proposed system achieves better performance for CASIA B datAQ2abase and OU-ISIR gait database-large population dataset with age and real-life carried object.},
  archive      = {J_IETIP},
  author       = {Newlin Shebiah Russel and Arivazhagan Selvaraj},
  doi          = {10.1049/ipr2.12024},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {239-251},
  shortjournal = {IET Image Process.},
  title        = {Gender discrimination, age group classification and carried object recognition from gait energy image using fusion of parallel convolutional neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective weighted vector median filter for impulse noise
reduction based on minimizing the degree of aggregation. <em>IETIP</em>,
<em>15</em>(1), 228–238. (<a
href="https://doi.org/10.1049/ipr2.12023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Impulse noise is regarded as an outlier in the local window of an image. To detect noise, many proposed methods are based on aggregated distance, including spatially weighted aggregated distance, n nearest neighbour distance, local density, and angle-weighted quaternion aggregated distance. However, these methods ignore the weight of each pixel or have limited adaptability. This study introduces the concept of degree of aggregation and proposes a weighting method to obtain the weight vector of the pixels by minimizing the degree of aggregation. The weight vector obtained gives larger components on the signal pixels than on the noisy pixels. Then it is fused with the aggregated distance to form a weighted aggregated distance that can reasonably characterise the noise and signal. The weighted aggregated distance, along with an adaptive segmentation method, can effectively detect the noise. To further enhance the effect of noise detection and removal, an adaptive selection strategy is incorporated to reduce the noise density in the local window. At last, noisy pixels detected are replaced with the weighted channel combination optimization values. The experimental results exhibit the validity of the proposed method by showing better performance in terms of both objective criteria and visual effects.},
  archive      = {J_IETIP},
  author       = {Xiangxi Meng and Tongwei Lu and Feng Min and Tao Lu},
  doi          = {10.1049/ipr2.12023},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {228-238},
  shortjournal = {IET Image Process.},
  title        = {An effective weighted vector median filter for impulse noise reduction based on minimizing the degree of aggregation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double-weighted patch-based label fusion for MR brain image
segmentation. <em>IETIP</em>, <em>15</em>(1), 218–227. (<a
href="https://doi.org/10.1049/ipr2.12022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, a large number of label fusion methods have been introduced and applied to magnetic resonance brain image segmentation. This paper proposes a double-weighted label fusion method to improve the segmentation accuracy of magnetic resonance brain tissues. In weighted label fusion methods, different weights are utilised for different neighbouring pixels around the central test pixel. The weight of one specific neighbouring pixel is determined by the structural similarity between the patch of the atlas that centred in the neighbouring pixel and the patch of the image to be segmented that centred in the central test pixel, which is referred to as a patch-based label fusion scheme. This paper presents a double-weighted label fusion method to improve the segmentation accuracy of magnetic resonance brain tissues. The proposed label fusion method adds another new type of weights to the neighbouring pixels around the central test pixel and this new type of weights are calculated based on the atlas information itself. Segmentation experiments of different brain tissues show that our method can improve the segmentation performance.},
  archive      = {J_IETIP},
  author       = {Meng Yan and Huazhong Jin and Zhiqiang Zhao and Dahai Xia and Ning Pan},
  doi          = {10.1049/ipr2.12022},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {218-227},
  shortjournal = {IET Image Process.},
  title        = {Double-weighted patch-based label fusion for MR brain image segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast depth intra coding based on texture feature and
spatio-temporal correlation in 3D-HEVC. <em>IETIP</em>, <em>15</em>(1),
206–217. (<a href="https://doi.org/10.1049/ipr2.12021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the computation burden of the depth intra coding in 3D-HEVC, a complexity reduction scheme based on texture feature and spatio-temporal correlation is proposed. Firstly, a maximum splitting depth layer decision algorithm is proposed to reduce unnecessary splitting depth layer of the coding tree unit utilising the information of the previous encoded I frame in the same view. Secondly, a new texture complexity model is built by pixel-based statistical method combined with edge detection. Based on the proposed model, the coding unit block is divided into the smooth block, texture or edge block. On the coding unit level, an early termination of coding unit splitting algorithm for smooth blocks is proposed to filter out unnecessary coding blocks. Thirdly, on the predicting unit level, a fast candidate mode decision algorithm considering predicting unit&#39;s types and spatial correlation is proposed to decide the candidate mode list directly. Experimental results describe that the proposed algorithm reduces 53.8% depth intra coding time on average, with 0.43% BD-rate loss on synthesised views.},
  archive      = {J_IETIP},
  author       = {Tiansong Li and Li Yu and Hongkui Wang and Yamei Chen},
  doi          = {10.1049/ipr2.12021},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {206-217},
  shortjournal = {IET Image Process.},
  title        = {Fast depth intra coding based on texture feature and spatio-temporal correlation in 3D-HEVC},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object scale selection of hierarchical image segmentation
with deep seeds. <em>IETIP</em>, <em>15</em>(1), 191–205. (<a
href="https://doi.org/10.1049/ipr2.12020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical image segmentation is a prevalent technique in the literature for improving segmentation quality, where the segmentation result needs to be searched at different scales of the hierarchy to identify objects represented from various scales. In this paper, a novel framework for improving the quality of object segmentation is presented. To this end, the authors first select the optimal segments among several hierarchical scales of the input image using simple mid-level features and dynamic programming. Simultaneously, deep seeds are localised on the input image for the foreground and background classes using a deep classification network and a saliency network, respectively. Then, a graphical model is constructed as a set of nodes that jointly propagate information from deep seeds to unmarked regions to obtain the final object segmentation. Comprehensive experiments are performed on different datasets for popular hierarchical image segmentation algorithms. The experimental results show that the proposed framework can significantly improve the quality of object segmentation at low computational costs and without training any segmentation network.},
  archive      = {J_IETIP},
  author       = {Zaid Al-Huda and Bo Peng and Yan Yang and Riyadh Nazar Ali Algburi},
  doi          = {10.1049/ipr2.12020},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {191-205},
  shortjournal = {IET Image Process.},
  title        = {Object scale selection of hierarchical image segmentation with deep seeds},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An innovate approach for retinal blood vessel segmentation
using mixture of supervised and unsupervised methods. <em>IETIP</em>,
<em>15</em>(1), 180–190. (<a
href="https://doi.org/10.1049/ipr2.12018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of retinal blood vessels is a very important diagnostic procedure in ophthalmology. Segmenting blood vessels in the presence of pathological lesions is a major challenge. In this paper, an innovative approach to segment the retinal blood vessel in the presence of pathology is proposed. The method combines both supervised and unsupervised approaches in the retinal imaging context. Two innovative descriptors named local Haar pattern and modified speeded up robust features are also proposed. Experiments are conducted on three publicly available datasets named: DRIVE, STARE and CHASE DB1, and the proposed method has been compared against the state-of-the-art methods. The proposed method is found about 1% more accurate than the best performing supervised method and 2% more accurate than the state-of-the-art Nguyen et al.’s method.},
  archive      = {J_IETIP},
  author       = {Md. Abu Sayed and Sajib Saha and G. M. Atiqur Rahaman and Tanmai K. Ghosh and Yogesan Kanagasingam},
  doi          = {10.1049/ipr2.12018},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {180-190},
  shortjournal = {IET Image Process.},
  title        = {An innovate approach for retinal blood vessel segmentation using mixture of supervised and unsupervised methods},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalized quality assessment method for natural and
screen content images. <em>IETIP</em>, <em>15</em>(1), 166–179. (<a
href="https://doi.org/10.1049/ipr2.12016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A generalized objective quality assessment method is proposed for natural images and screen content images. Since natural images and screen content images have different statistical properties, the modelling of a generalized quality assessment method that works for both types of images is complicated because some properties of natural images and screen content images are conflicting to one another. The proposed method assesses the perceptual quality of an image based on edge magnitude and direction. In this method, an image is first separated into regions with high and low gradients. Gradient is used due to the small perceptual span of the human visual system for textual content. For high gradient regions, small kernel size of Prewitt operators is used to obtain the gradient magnitude and direction. Correspondingly, bigger kernel size of Prewitt operators is utilized for low gradient regions. Visual quality indices are computed from both regions and pooled to obtain the final quality index. From the performance comparison, it is shown that the proposed method could assess the perceived quality of natural images and screen content images with high accuracy.},
  archive      = {J_IETIP},
  author       = {Woei-Tan Loh and David B. L. Bong},
  doi          = {10.1049/ipr2.12016},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {166-179},
  shortjournal = {IET Image Process.},
  title        = {A generalized quality assessment method for natural and screen content images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAR image noise suppression of BEMD by the kernel principle
component analysis. <em>IETIP</em>, <em>15</em>(1), 155–165. (<a
href="https://doi.org/10.1049/ipr2.12015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of synthetic aperture radar image noise suppression by the bi-dimensional empirical mode decomposition (BEMD) algorithm, the edge effect is a key problem in the BEMD operation. To weaken this effect, an improved BEMD-kernel principal component analysis (BEMD-KPCA) method of image denoising is proposed in this study. Experimental results show that the BEMDKPCA algorithm has a good capability of improving edge effects in the BEMD decomposition process and satisfying the requirement of the reliable decomposition results. Compared with the traditional BEMD method, the proposed approach has a good effect on suppressing speckle noise. Additionally, the denoised image from the decomposed components of the IMFs processed by the BEMD-KPCA method sufficiently preserves the edge and detail information, confirming its high coherency with the original image.},
  archive      = {J_IETIP},
  author       = {Changjun Huang and Xinghua Zhou and Jiyuan Hu and Qingshan Zhou},
  doi          = {10.1049/ipr2.12015},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {155-165},
  shortjournal = {IET Image Process.},
  title        = {SAR image noise suppression of BEMD by the kernel principle component analysis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MSNet: A novel end-to-end single image dehazing network with
multiple inter-scale dense skip-connections. <em>IETIP</em>,
<em>15</em>(1), 143–154. (<a
href="https://doi.org/10.1049/ipr2.12013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dehazing is a challenging ill-posed image restoration task. Various prior-based and learning-based methods have been proposed. Among them, end-to-end deep models achieve great success on performance improvement. However, most of them are concentrated on feature learning within the same block scale in isolation, and cannot perform associated analysis well on feature characteristics of different scales. Inter-scale information reuse which is especially beneficial to image restoration is often neglected. Therefore, in this paper, a novel end-to-end network with multiple inter-scale dense skip-connections for image dehazing is proposed. Sufficient complementary information combination is considered through dense inter-scale skip-connections among encoder and decoder block layers. Besides avoiding gradient vanishing, a kind of bottleneck residual block is proposed to control the importance of local gradients at different scales over global learning process. Extensive comparisons and ablation studies on public dehazing datasets and real-world images have been conducted. The experiment results demonstrate that the proposed novel elements can ensure more stable training process and superior testing performance with great improvements on PSNR and SSIM. Authors&#39; haze-removal results consistently comply satisfactorily with real situations, having much higher definition and contrast without colour distortion than those from the state-of-the-art methods compared in this paper.},
  archive      = {J_IETIP},
  author       = {Qiaosi Yi and Aiwen Jiang and Xiaolin Deng and Changhong Liu},
  doi          = {10.1049/ipr2.12013},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {143-154},
  shortjournal = {IET Image Process.},
  title        = {MSNet: A novel end-to-end single image dehazing network with multiple inter-scale dense skip-connections},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature fusion quality assessment model for DASH video
streaming. <em>IETIP</em>, <em>15</em>(1), 127–142. (<a
href="https://doi.org/10.1049/ipr2.12012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Adaptive Streaming over HTTP (DASH) employs the flexible rate adaptation scheme to combat with time-varying channel conditions. In addition to compression impairment, DASH video streaming suffers from transmission impairment, such as rate switches and stalling events. Both of them severely degrade users&#39; Quality of Experience (QoE). Herein, an assessment model is established for DASH video streaming by directly jointing multiple QoE influential factors, which quantify impairments resulting from the compression and transmission. To demonstrate the influence of video content characteristics on users&#39; QoE, spatio-temporal content perceptual features are employed to represent the compression impairment. When reflecting temporal characteristics, a novel motion vector padding method is proposed to quantify the influence of intra macroblock on the human visual system. The proposed model is evaluated on a newly public Waterloo SQoE-III database, which is available for DASH video streaming. Experimental results demonstrate that the authors&#39; model outperforms the comparative models and owns the strong generalization ability to different video contents. Moreover, the proposed model is statistically superior to the existing models.},
  archive      = {J_IETIP},
  author       = {Hong Zhang and Fan Li and Zhisheng Yan},
  doi          = {10.1049/ipr2.12012},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {127-142},
  shortjournal = {IET Image Process.},
  title        = {Feature fusion quality assessment model for DASH video streaming},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual dense u-net for abnormal exposure restoration from
single images. <em>IETIP</em>, <em>15</em>(1), 115–126. (<a
href="https://doi.org/10.1049/ipr2.12011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital imaging devices sometimes capture images with abnormal exposure because of the complex lighting conditions and limited dynamic range of luminance. In this work, a new residual dense U-Net is proposed to predict the information that has been lost in saturated image areas, to enable abnormal exposure restoration from a single image. Full advantage of the multi-level features is taken from all the convolution layers in the restoration process. Specifically, the densely connected convolutional layers are used in a contracting encoder net to extract abundant local features. The transition layer and local residual learning after each dense block is then applied to adaptively learn more effectively from prior with present local features. Further, an expanding decoder net with dense layers is used and added with skip connections to preserve low-level information and existing details. Finally, multiple global residual learning is used to adaptively extract hierarchical features and help train the network. It is shown that such a network can be trained end-to-end from abnormal exposure images and outperform the prior best method on image enhancement. Experimental results show that the proposed model can greatly enhance the dynamic range of an abnormal exposure image.},
  archive      = {J_IETIP},
  author       = {Yue Que and Hyo Jong Lee},
  doi          = {10.1049/ipr2.12011},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {115-126},
  shortjournal = {IET Image Process.},
  title        = {Residual dense U-net for abnormal exposure restoration from single images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image denoising method based on variable exponential
fractional-integer-order total variation and tight frame sparse
regularization. <em>IETIP</em>, <em>15</em>(1), 101–114. (<a
href="https://doi.org/10.1049/ipr2.12010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a variational image restoration algorithm based on variable exponential fractional-order total variation (TV), variable exponential integer-order TV and tight frame sparse regularization. The energy functional of this variational problem is composed of four parts: a fractional-order TV regularization term with a variable exponent, an integer-order TV regularization term with a variable exponent, a data fidelity term and a tight frame regularization term. The variable exponent is a function of the gradient information of the image. The first three parts of the energy functional is the smooth part, and the last part is the non-smooth part. To minimize the smooth part, the optimization problem can be simply transformed into a gradient descent flow using the variational method. For the non-smooth part, we use the soft-thresholding method over the sparse framelet coefficients. As the combination of the fractional-order derivative and integer-order derivative, the variable exponent and the sparse regularisation, the proposed method can effectively remove noise of the image, protect the image boundary, and keep image texture details well. Experiments with simulated data and real data are provided to validate the effectiveness of proposed method. This method is robust to noise and is of some practical application value.},
  archive      = {J_IETIP},
  author       = {Yingmei Wang and Zhendong Wang},
  doi          = {10.1049/ipr2.12010},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {101-114},
  shortjournal = {IET Image Process.},
  title        = {Image denoising method based on variable exponential fractional-integer-order total variation and tight frame sparse regularization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end feature fusion siamese network for adaptive
visual tracking. <em>IETIP</em>, <em>15</em>(1), 91–100. (<a
href="https://doi.org/10.1049/ipr2.12009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to observations, different visual objects have different salient features in different scenarios. Even for the same object, its salient shape and appearance features may change greatly from time to time in a long-term tracking task. Motivated by them, an end-to-end feature fusion framework was proposed based on the Siamese network, named FF-Siam, which can effectively fuse different features for adaptive visual tracking. The framework consists of four layers. A feature extraction layer is designed to extract the different features of the target region and search region. The extracted features are then put into a weight generation layer to obtain the channel weights, which indicate the importance of different feature channels. Both features and the channel weights are utilised in a template generation layer to generate a discriminative template. Finally, the corresponding response maps created by the convolution of the search region features and the template are applied with a fusion layer to obtain the final response map for locating the target. Experimental results demonstrate that the proposed framework achieves state-of-the-art performance on the popular Temple-Colour, OTB50 and UAV123 benchmarks.},
  archive      = {J_IETIP},
  author       = {Dongyan Guo and Jun Wang and Weixuan Zhao and Ying Cui and Zhenhua Wang and Shengyong Chen},
  doi          = {10.1049/ipr2.12009},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {91-100},
  shortjournal = {IET Image Process.},
  title        = {End-to-end feature fusion siamese network for adaptive visual tracking},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KS-FQA: Keyframe selection based on face quality assessment
for efficient face recognition in video. <em>IETIP</em>, <em>15</em>(1),
77–90. (<a href="https://doi.org/10.1049/ipr2.12008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video is considered as one of the most useful and important forms of multimedia data, that is usually used in several applications. Despite its importance, video indexing and retrieval becomes a challenging task. In order to reduce the amount of data and keep only relevant frames, keyframe extraction becomes necessary in a content-based video retrieval (CBVR) system. In this paper, a keyframe extraction method is proposed based on the face image quality for video surveillance systems. Data is reduced by rejecting frames without faces. Then, face images are clustered by identity. After that, a set of candidate frames is selected to be proceeded. The face quality assessment is based on four metrics including pose estimation, sharpness, brightness and resolution, and the frame with the best face quality is considered as a keyframe. Experimental tests were carried on several datasets in order to prove the efficiency of authors&#39; method compared with state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Sahbi Bahroun and Rahma Abed and Ezzeddine Zagrouba},
  doi          = {10.1049/ipr2.12008},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {77-90},
  shortjournal = {IET Image Process.},
  title        = {KS-FQA: Keyframe selection based on face quality assessment for efficient face recognition in video},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of human activity detection based on an
intelligent regression model in video sequences. <em>IETIP</em>,
<em>15</em>(1), 65–76. (<a
href="https://doi.org/10.1049/ipr2.12006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most critical objective in security surveillance is abnormal event detection in public scenarios. A scheme is presented for detecting abnormal behaviours in the activities of human groups based on social behaviour analysis. This approach efficiently models group activities than some of the previous strategies that use independent local features. This paper presents a feature descriptor method to signify the movement by implementing the optical flow through covariance matrix coding. The multi-RoI (region of interest) covariance matrix has some frames or patches which could represent the movement in high accuracy. Normal samples are plentiful in public surveillance videos, while there are only a few abnormal samples. For that, the model of a hybridised optical flow covariance matrix is represented in this paper. Optical flow (OF) in the temporal domain is measured as a critical feature of video streams. The logistic regression method is used to detect abnormal activities in a crowded scene. Finally, the behaviours of human crowds can be predicted using benchmark datasets such as UMN, UCSD as well as BEHAVE. The obtained experimental results show that the proposed approach can effectively detect abnormal events from the abandoned environment of surveillance videos.},
  archive      = {J_IETIP},
  author       = {Natarajan Kumaran and Uyyala Srinivasulu Reddy},
  doi          = {10.1049/ipr2.12006},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {65-76},
  shortjournal = {IET Image Process.},
  title        = {Classification of human activity detection based on an intelligent regression model in video sequences},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient semantic segmentation method based on transfer
learning from object detection. <em>IETIP</em>, <em>15</em>(1), 57–64.
(<a href="https://doi.org/10.1049/ipr2.12005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, numerous semantic segmentation techniques were used to complex scenes such as urban streets. However, speed issues are not considered in most of these methods, and real-time methods do not mainly include enough accuracy. In this paper, an efficient semantic segmentation method is proposed, using the feature extractor of a real-time object detection model, Darknet53, as the backbone of DeepLabv3+. By the high accuracy of DeepLabv3+ structure and great efficiency of Darknet53, a mean intersection was obtained over union of 76.3% in Cityscapes test set, and fast inference speed simultaneously (0.178 s per frame on one GTX 1080Ti GPU). A huge imbalance of objects was noticed on Cityscapes dataset. To solve this problem, a Focal Loss like loss function was proposed to concentrate more on the hard difficult pixels. Moreover, an atrous convolution block was proposed to extract more high-level features. Based on the experimental results, it is proved that these changes contribute to a better result on the Cityscapes test set (77.8% mean Intersection over Union) and faster inference speed (0.171 s per frame). Authors&#39; model achieves state-of-art results on Cityscapes test set (79.1% mean Intersection over Union) after fine-tuning on Cityscapes coarsely annotated dataset.},
  archive      = {J_IETIP},
  author       = {Wei Yang and Jianlin Zhang and Zhongbi Chen and Zhiyong Xu},
  doi          = {10.1049/ipr2.12005},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {57-64},
  shortjournal = {IET Image Process.},
  title        = {An efficient semantic segmentation method based on transfer learning from object detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A remote-sensing image enhancement algorithm based on
patch-wise dark channel prior and histogram equalisation with colour
correction. <em>IETIP</em>, <em>15</em>(1), 47–56. (<a
href="https://doi.org/10.1049/ipr2.12004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object identification within an image captured during rough weather conditions (such as haze, fog) poses difficulty due to the reduction of an image. The rough weather conditions lead not only to the variation of the image&#39;s visual effect but also to the disadvantage of post-processing of an image. Furthermore, it causes inconvenience of all types of instruments that rely on optical imaging, such as satellite remote-sensing systems, aerial photo systems, outdoor monitoring systems, and object identification systems, respectively. Hence, the improvement and restorement of the visual effects and enhanced post-processing are needed. This research introduces a new image enhancement approach for image dehazing based on dark channel prior and piecewise linear transformation; also, the histogram equalisation technique, i.e. contrast limited adaptive histogram equalisation is applied. A dark channel prior is well known for its simplicity and productivity. In this work, the dark channel prior to a new angle is analysed in the first step, where average patch sizes are estimated for the computation of haze densities. Furthermore, the sky is approximated up to 5–10% of the hazy images, which has a good effect in removing the haze from the image. Using the dark channel, the proposed algorithm significantly boosted the effects of the dark images as well as reduced the influence of haze and noise. Eventually, for colour correction, the piecewise linear transformation technique is applied, which enhances the colour close to the original image. Experimental results demonstrate that the proposed method significantly improves the visibility of the algorithm on dark remote-sensing images as well as on hazy natural images.},
  archive      = {J_IETIP},
  author       = {Fayaz Ali Dharejo and Yuanchun Zhou and Farah Deeba and Munsif Ali Jatoi and Yi Du and Xuezhi Wang},
  doi          = {10.1049/ipr2.12004},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {47-56},
  shortjournal = {IET Image Process.},
  title        = {A remote-sensing image enhancement algorithm based on patch-wise dark channel prior and histogram equalisation with colour correction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Features’ value range approach to enhance the throughput of
texture classification. <em>IETIP</em>, <em>15</em>(1), 28–46. (<a
href="https://doi.org/10.1049/ipr2.12003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The definition of an image&#39;s category from a database with huge texture categories needs massive computation and time cost. Existing texture classification works focus on texture representation to improve the accuracy and efficiency of classification. This research wants to reduce the categories of the main classifier to decrease the comparison time of classification. To overcome computation time, a features&#39; value range (FR) approach to enhance the throughput of texture classification is proposed. The proposed approach decreases the number of candidate categories as a pre-classifier in a two-step serial classification. With the decrease in the number of candidates, the main classifier can work on a few categories to find the final category. Here, configuration parameters are defined and some criteria are proposed for evaluating the FR approach. The performance of the FR is evaluated in the presence of different levels of Gaussian noise. Finally, it is shown that using effective features (EF) and hardware implementation approaches can extend the applicability of the FR approach. Experimental results depicted that the throughput of the final decision increased up to 14.85× with considerable reliability.},
  archive      = {J_IETIP},
  author       = {Alireza Akoushideh and Babak Mazloom-Nezhad Maybodi and Asadollah Shahbahrami},
  doi          = {10.1049/ipr2.12003},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {28-46},
  shortjournal = {IET Image Process.},
  title        = {Features&#39; value range approach to enhance the throughput of texture classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face alignment based on fusion subspace and 3D fitting.
<em>IETIP</em>, <em>15</em>(1), 16–27. (<a
href="https://doi.org/10.1049/ipr2.12002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional face alignment approaches based on cascade regression have achieved satisfactory result on the frontal face, but for the face with large changes in posture and expression, a single initial shape will lead to the result falling into local optimum. In order to solve this problem, a two-stage cascade regression model for face alignment is proposed, which generates coarse initial shape from the aligned salient shape. The first stage is used to align the salient shape that contains some prominent landmarks. To enhance the robustness of authors&#39; method, the fusion subspace is used to divide the samples, and each subset trains cascade regression model separately. The alignment results of the first stage are used to generate the coarse initial shapes for the second stage through 3D fitting. The second stage is still based on cascade regression, which is used to further predict the full shape. The experimental results demonstrate the proposed method can achieve state-of-art performance, especially in unconstrained conditions with various poses.},
  archive      = {J_IETIP},
  author       = {Jiahui Zhang and Lan Di and Jiuzhen Liang},
  doi          = {10.1049/ipr2.12002},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {16-27},
  shortjournal = {IET Image Process.},
  title        = {Face alignment based on fusion subspace and 3D fitting},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN-based infrared dim small target detection algorithm
using target-oriented shallow-deep features and effective small anchor.
<em>IETIP</em>, <em>15</em>(1), 1–15. (<a
href="https://doi.org/10.1049/ipr2.12001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the extremely small size and low signal-to-clutter ratio, target detection in infrared images is still a considerable challenge. Specifically, it is very difficult to detect the point targets because there is no texture and shape information can be used. A target-oriented shallow-deep feature-based detection algorithm is proposed, opening up a promising direction for convolutional neural network-based infrared dim small target detection algorithms. To ensure that small target instances can be used correctly for networks, the effective small anchor is designed according to the shallow layer of ResNet50. To determine whether a detection result belongs to the target, the authors depend on whether the detection centre is included in the ground truth area, rather than on the Intersection Over Union overlap rate, which avoids misjudging the detection result. In this way, small targets can be trained and detected correctly through ResNet50. More importantly, the authors demonstrate that spatially finer shallow features are crucial for small target detection and that semantically stronger deep features are helpful for improving detection probability. Experimental results on simulation data sets and real data sets show that the proposed algorithm can detect the point target when the local signal-to-clutter ratio is approximately 1.3, displaying infinite advantage and great potentiality.},
  archive      = {J_IETIP},
  author       = {Jinming Du and Huanzhang Lu and Moufa Hu and Luping Zhang and Xinglin Shen},
  doi          = {10.1049/ipr2.12001},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {IET Image Process.},
  title        = {CNN-based infrared dim small target detection algorithm using target-oriented shallow-deep features and effective small anchor},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
