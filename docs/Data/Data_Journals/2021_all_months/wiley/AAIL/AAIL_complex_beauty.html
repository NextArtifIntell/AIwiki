<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAIL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aail---33">AAIL - 33</h2>
<ul>
<li><details>
<summary>
(2021). DARPA’s explainable AI (XAI) program: A retrospective.
<em>AAIL</em>, <em>2</em>(4), e61. (<a
href="https://doi.org/10.1002/ail2.61">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AAIL},
  author       = {David Gunning and Eric Vorm and Jennifer Yunyan Wang and Matt Turek},
  doi          = {10.1002/ail2.61},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e61},
  shortjournal = {Appl. AI Lett.},
  title        = {DARPA&#39;s explainable AI (XAI) program: A retrospective},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reframing explanation as an interactive medium: The EQUAS
(explainable QUestion answering system) project. <em>AAIL</em>,
<em>2</em>(4), e60. (<a href="https://doi.org/10.1002/ail2.60">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter is a retrospective analysis of our team&#39;s research for the Defense Advanced Research Projects Agency Explainable Artificial Intelligence project. Our initial approach was to use salience maps, English sentences, and lists of feature names to explain the behavior of deep-learning-based discriminative systems, with particular focus on visual question answering systems. We found that presenting static explanations along with answers led to limited positive effects. By exploring various combinations of machine and human explanation production and consumption, we evolved a notion of explanation as an interactive process that takes place usually between humans and artificial intelligence systems but sometimes within the software system. We realized that by interacting via explanations people could task and adapt machine learning (ML) agents. We added affordances for editing explanations and modified the ML system to act in accordance with the edits to produce an interpretable interface to the agent. Through this interface, editing an explanation can adapt a system&#39;s performance to new, modified purposes. This deep tasking, wherein the agent knows its objective and the explanation for that objective, will be critical to enable higher levels of autonomy.},
  archive      = {J_AAIL},
  author       = {William Ferguson and Dhruv Batra and Raymond Mooney and Devi Parikh and Antonio Torralba and David Bau and David Diller and Josh Fasching and Jaden Fiotto-Kaufman and Yash Goyal and Jeff Miller and Kerry Moffitt and Alex Montes de Oca and Ramprasaath R. Selvaraju and Ayush Shrivastava and Jialin Wu and Stefan Lee},
  doi          = {10.1002/ail2.60},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e60},
  shortjournal = {Appl. AI Lett.},
  title        = {Reframing explanation as an interactive medium: The EQUAS (Explainable QUestion answering system) project},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable activity recognition in videos: Lessons learned.
<em>AAIL</em>, <em>2</em>(4), e59. (<a
href="https://doi.org/10.1002/ail2.59">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the following activity recognition task: given a video, infer the set of activities being performed in the video and assign each frame to an activity. This task can be solved using modern deep learning architectures based on neural networks or conventional classifiers such as linear models and decision trees. While neural networks exhibit superior predictive performance as compared with decision trees and linear models, they are also uninterpretable and less explainable. We address this accuracy-explanability gap using a novel framework that feeds the output of a deep neural network to an interpretable, tractable probabilistic model called dynamic cutset networks, and performs joint reasoning over the two to answer questions. The neural network helps achieve high accuracy while dynamic cutset networks because of their polytime probabilistic reasoning capabilities make the system more explainable. We demonstrate the efficacy of our approach by using it to build three prototype systems that solve human-machine tasks having varying levels of difficulty using cooking videos as an accessible domain. We describe high-level technical details and key lessons learned in our human subjects evaluations of these systems.},
  archive      = {J_AAIL},
  author       = {Chiradeep Roy and Mahsan Nourani and Donald R. Honeycutt and Jeremy E. Block and Tahrima Rahman and Eric D. Ragan and Nicholas Ruozzi and Vibhav Gogate},
  doi          = {10.1002/ail2.59},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e59},
  shortjournal = {Appl. AI Lett.},
  title        = {Explainable activity recognition in videos: Lessons learned},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards structured NLP interpretation via graph explainers.
<em>AAIL</em>, <em>2</em>(4), e58. (<a
href="https://doi.org/10.1002/ail2.58">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) models have been increasingly deployed in real-world applications, and interpretation for textual data has also attracted dramatic attention recently. Most existing methods generate feature importance interpretation, which indicate the contribution of each word towards a specific model prediction. Text data typically possess highly structured characteristics and feature importance explanation cannot fully reveal the rich information contained in text. To bridge this gap, we propose to generate structured interpretations for textual data. Specifically, we pre-process the original text using dependency parsing, which could transform the text from sequences into graphs. Then graph neural networks (GNNs) are utilized to classify the transformed graphs. In particular, we explore two kinds of structured interpretation for pre-trained GNNs: edge-level interpretation and subgraph-level interpretation. Experimental results over three text datasets demonstrate that the structured interpretation can better reveal the structured knowledge encoded in the text. The experimental analysis further indicates that the proposed interpretations can faithfully reflect the decision-making process of the GNN model.},
  archive      = {J_AAIL},
  author       = {Hao Yuan and Fan Yang and Mengnan Du and Shuiwang Ji and Xia Hu},
  doi          = {10.1002/ail2.58},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e58},
  shortjournal = {Appl. AI Lett.},
  title        = {Towards structured NLP interpretation via graph explainers},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Objective criteria for explanations of machine learning
models. <em>AAIL</em>, <em>2</em>(4), e57. (<a
href="https://doi.org/10.1002/ail2.57">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective criteria to evaluate the performance of machine learning (ML) model explanations are a critical ingredient in bringing greater rigor to the field of explainable artificial intelligence. In this article, we survey three of our proposed criteria that each target different classes of explanations. In the first, targeted at real-valued feature importance explanations, we define a class of “infidelity” measures that capture how well the explanations match the ML models. We show that instances of such infidelity minimizing explanations correspond to many popular recently proposed explanations and, moreover, can be shown to satisfy well-known game-theoretic axiomatic properties. In the second, targeted to feature set explanations, we define a robustness analysis-based criterion and show that deriving explainable feature sets based on the robustness criterion yields more qualitatively impressive explanations. Lastly, for sample explanations, we provide a decomposition-based criterion that allows us to provide very scalable and compelling classes of sample-based explanations.},
  archive      = {J_AAIL},
  author       = {Chih-Kuan Yeh and Pradeep Ravikumar},
  doi          = {10.1002/ail2.57},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e57},
  shortjournal = {Appl. AI Lett.},
  title        = {Objective criteria for explanations of machine learning models},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward explainable and advisable model for self-driving
cars. <em>AAIL</em>, <em>2</em>(4), e56. (<a
href="https://doi.org/10.1002/ail2.56">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans learn to drive through both practice and theory, for example, by studying the rules, while most self-driving systems are limited to the former. Being able to incorporate human knowledge of typical causal driving behavior should benefit autonomous systems. We propose a new approach that learns vehicle control with the help of human advice. Specifically, our system learns to summarize its visual observations in natural language, predict an appropriate action response (eg, “I see a pedestrian crossing, so I stop”), and predict the controls, accordingly. Moreover, to enhance the interpretability of our system, we introduce a fine-grained attention mechanism that relies on semantic segmentation and object-centric RoI pooling. We show that our approach of training the autonomous system with human advice, grounded in a rich semantic representation, matches or outperforms prior work in terms of control prediction and explanation generation. Our approach also results in more interpretable visual explanations by visualizing object-centric attention maps. We evaluate our approach on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset.},
  archive      = {J_AAIL},
  author       = {Jinkyu Kim and Anna Rohrbach and Zeynep Akata and Suhong Moon and Teruhisa Misu and Yi-Ting Chen and Trevor Darrell and John Canny},
  doi          = {10.1002/ail2.56},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e56},
  shortjournal = {Appl. AI Lett.},
  title        = {Toward explainable and advisable model for self-driving cars},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating visual explanations with natural language.
<em>AAIL</em>, <em>2</em>(4), e55. (<a
href="https://doi.org/10.1002/ail2.55">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We generate natural language explanations for a fine-grained visual recognition task. Our explanations fulfill two criteria. First, explanations are class discriminative , meaning they mention attributes in an image which are important to identify a class. Second, explanations are image relevant , meaning they reflect the actual content of an image. Our system, composed of an explanation sampler and phrase-critic model, generates class discriminative and image relevant explanations. In addition, we demonstrate that our explanations can help humans decide whether to accept or reject an AI decision.},
  archive      = {J_AAIL},
  author       = {Lisa Anne Hendricks and Anna Rohrbach and Bernt Schiele and Trevor Darrell and Zeynep Akata},
  doi          = {10.1002/ail2.55},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e55},
  shortjournal = {Appl. AI Lett.},
  title        = {Generating visual explanations with natural language},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining autonomous drones: An XAI journey. <em>AAIL</em>,
<em>2</em>(4), e54. (<a href="https://doi.org/10.1002/ail2.54">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COGLE ( CO mmon G round L earning and E xplanation) is an explainable artificial intelligence (XAI) system where autonomous drones deliver supplies to field units in mountainous areas. The mission risks vary with topography, flight decisions, and mission goals. The missions engage a human plus AI team where users determine which of two AI-controlled drones is better for each mission. This article reports on the technical approach and findings of the project and reflects on challenges that complex combinatorial problems present for users, machine learning, user studies, and the context of use for XAI systems. COGLE creates explanations in multiple modalities. Narrative “What” explanations compare what each drone does on a mission and “Why” based on drone competencies determined from experiments using counterfactuals. Visual “Where” explanations highlight risks on maps to help users to interpret flight plans. One branch of the research studied whether the explanations helped users to predict drone performance. In this branch, a model induction user study showed that post-decision explanations had only a small effect in teaching users to determine by themselves which drone is better for a mission. Subsequent reflection suggests that supporting human plus AI decision making with pre-decision explanations is a better context for benefiting from explanations on combinatorial tasks.},
  archive      = {J_AAIL},
  author       = {Mark Stefik and Michael Youngblood and Peter Pirolli and Christian Lebiere and Robert Thomson and Robert Price and Lester D. Nelson and Robert Krivacic and Jacob Le and Konstantinos Mitsopoulos and Sterling Somers and Joel Schooler},
  doi          = {10.1002/ail2.54},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e54},
  shortjournal = {Appl. AI Lett.},
  title        = {Explaining autonomous drones: An XAI journey},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Methods and standards for research on explainable artificial
intelligence: Lessons from intelligent tutoring systems. <em>AAIL</em>,
<em>2</em>(4), e53. (<a href="https://doi.org/10.1002/ail2.53">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The DARPA Explainable Artificial Intelligence (AI) (XAI) Program focused on generating explanations for AI programs that use machine learning techniques. This article highlights progress during the DARPA Program (2017-2021) relative to research since the 1970s in the field of intelligent tutoring systems (ITSs). ITS researchers learned a great deal about explanation that is directly relevant to XAI. We suggest opportunities for future XAI research deriving from ITS methods, and consider the challenges shared by both ITS and XAI in using AI to assist people in solving difficult problems effectively and efficiently.},
  archive      = {J_AAIL},
  author       = {William J. Clancey and Robert R. Hoffman},
  doi          = {10.1002/ail2.53},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e53},
  shortjournal = {Appl. AI Lett.},
  title        = {Methods and standards for research on explainable artificial intelligence: Lessons from intelligent tutoring systems},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining robot policies. <em>AAIL</em>, <em>2</em>(4),
e52. (<a href="https://doi.org/10.1002/ail2.52">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to interact with a robot or make wise decisions about where and how to deploy it in the real world, humans need to have an accurate mental model of how the robot acts in different situations. We propose to improve users&#39; mental model of a robot by showing them examples of how the robot behaves in informative scenarios. We explore this in two settings. First, we show that when there are many possible environment states, users can more quickly understand the robot&#39;s policy if they are shown critical states where taking a particular action is important. Second, we show that when there is a distribution shift between training and test environment distributions, then it is more effective to show exploratory states that the robot does not visit naturally.},
  archive      = {J_AAIL},
  author       = {Olivia Watkins and Sandy Huang and Julius Frost and Kush Bhatia and Eric Weiner and Pieter Abbeel and Trevor Darrell and Bryan Plummer and Kate Saenko and Anca Dragan},
  doi          = {10.1002/ail2.52},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e52},
  shortjournal = {Appl. AI Lett.},
  title        = {Explaining robot policies},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating and evaluating explanations of attended and
error-inducing input regions for VQA models. <em>AAIL</em>,
<em>2</em>(4), e51. (<a href="https://doi.org/10.1002/ail2.51">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention maps, a popular heatmap-based explanation method for Visual Question Answering, are supposed to help users understand the model by highlighting portions of the image/question used by the model to infer answers. However, we see that users are often misled by current attention map visualizations that point to relevant regions despite the model producing an incorrect answer. Hence, we propose Error Maps that clarify the error by highlighting image regions where the model is prone to err. Error maps can indicate when a correctly attended region may be processed incorrectly leading to an incorrect answer, and hence, improve users&#39; understanding of those cases. To evaluate our new explanations, we further introduce a metric that simulates users&#39; interpretation of explanations to evaluate their potential helpfulness to understand model correctness. We finally conduct user studies to see that our new explanations help users understand model correctness better than baselines by an expected 30% and that our proxy helpfulness metrics correlate strongly ( ) with how well users can predict model correctness.},
  archive      = {J_AAIL},
  author       = {Arijit Ray and Michael Cogswell and Xiao Lin and Kamran Alipour and Ajay Divakaran and Yi Yao and Giedrius Burachas},
  doi          = {10.1002/ail2.51},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e51},
  shortjournal = {Appl. AI Lett.},
  title        = {Generating and evaluating explanations of attended and error-inducing input regions for VQA models},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How level of explanation detail affects human performance in
interpretable intelligent systems: A study on explainable fact checking.
<em>AAIL</em>, <em>2</em>(4), e49. (<a
href="https://doi.org/10.1002/ail2.49">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (XAI) systems aim to provide users with information to help them better understand computational models and reason about why outputs were generated. However, there are many different ways an XAI interface might present explanations, which makes designing an appropriate and effective interface an important and challenging task. Our work investigates how different types and amounts of explanatory information affect user ability to utilize explanations to understand system behavior and improve task performance. The presented research employs a system for detecting the truthfulness of news statements. In a controlled experiment, participants were tasked with using the system to assess news statements as well as to learn to predict the output of the AI. Our experiment compares various levels of explanatory information to contribute empirical data about how explanation detail can influence utility. The results show that more explanation information improves participant understanding of AI models, but the benefits come at the cost of time and attention needed to make sense of the explanation.},
  archive      = {J_AAIL},
  author       = {Rhema Linder and Sina Mohseni and Fan Yang and Shiva K. Pentyala and Eric D. Ragan and Xia Ben Hu},
  doi          = {10.1002/ail2.49},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e49},
  shortjournal = {Appl. AI Lett.},
  title        = {How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural response time analysis: Explainable artificial
intelligence using only a stopwatch. <em>AAIL</em>, <em>2</em>(4), e48.
(<a href="https://doi.org/10.1002/ail2.48">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How would you describe the features that a deep learning model composes if you were restricted to measuring observable behaviours? Explainable artificial intelligence (XAI) methods rely on privileged access to model architecture and parameters that is not always feasible for most users, practitioners and regulators. Inspired by cognitive psychology research on humans, we present a case for measuring response times (RTs) of a forward pass using only the system clock as a technique for XAI. Our method applies to the growing class of models that use input-adaptive dynamic inference and we also extend our approach to standard models that are converted to dynamic inference post hoc. The experimental logic is simple: If the researcher can contrive a stimulus set where variability among input features is tightly controlled, differences in RT for those inputs can be attributed to the way the model composes those features. First, we show that RT is sensitive to difficult, complex features by comparing RTs from ObjectNet and ImageNet. Next, we make specific a priori predictions about RT for abstract features present in the SCEGRAM data set, where object recognition in humans depends on complex intrascene object-object relationships. Finally, we show that RT profiles bear specificity for class identity and therefore the features that define classes. These results cast light on the model&#39;s feature space without opening the black box.},
  archive      = {J_AAIL},
  author       = {J. Eric T. Taylor and Shashank Shekhar and Graham W. Taylor},
  doi          = {10.1002/ail2.48},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e48},
  shortjournal = {Appl. AI Lett.},
  title        = {Neural response time analysis: Explainable artificial intelligence using only a stopwatch},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving users’ mental model with attention-directed
counterfactual edits. <em>AAIL</em>, <em>2</em>(4), e47. (<a
href="https://doi.org/10.1002/ail2.47">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of visual question answering (VQA), studies have shown improvement in users&#39; mental model of the VQA system when they are exposed to examples of how these systems answer certain image-question (IQ) pairs. In this work, we show that showing controlled counterfactual IQ examples are more effective at improving the mental model of users as compared to simply showing random examples. We compare a generative approach and a retrieval-based approach to show counterfactual examples. We use recent advances in generative adversarial networks to generate counterfactual images by deleting and inpainting certain regions of interest in the image. We then expose users to changes in the VQA system&#39;s answer on those altered images. To select the region of interest for inpainting, we experiment with using both human-annotated attention maps and a fully automatic method that uses the VQA system&#39;s attention values. Finally, we test the user&#39;s mental model by asking them to predict the model&#39;s performance on a test counterfactual image. We note an overall improvement in users&#39; accuracy to predict answer change when shown counterfactual explanations. While realistic retrieved counterfactuals obviously are the most effective at improving the mental model, we show that a generative approach can also be equally effective.},
  archive      = {J_AAIL},
  author       = {Kamran Alipour and Arijit Ray and Xiao Lin and Michael Cogswell and Jurgen P. Schulze and Yi Yao and Giedrius T. Burachas},
  doi          = {10.1002/ail2.47},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e47},
  shortjournal = {Appl. AI Lett.},
  title        = {Improving users&#39; mental model with attention-directed counterfactual edits},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From heatmaps to structured explanations of image
classifiers. <em>AAIL</em>, <em>2</em>(4), e46. (<a
href="https://doi.org/10.1002/ail2.46">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts with describing the explainable neural network (XNN), which attempts to extract and visualize several high-level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on a difficult fine-grained classification task of discriminating among different species of seagulls. Realizing that an important missing piece is a reliable heatmap visualization tool, we have developed integrated-gradient optimized saliency (I-GOS) and iGOS++ utilizing integrated gradients to avoid local optima in heatmap generation, which improved the performance across all resolutions. During the development of those visualizations, we realized that for a significant number of images, the classifier has multiple different paths to reach a confident prediction. This has led to our recent development of structured attention graphs, an approach that utilizes beam search to locate multiple coarse heatmaps for a single image, and compactly visualizes a set of heatmaps by capturing how different combinations of image regions impact the confidence of a classifier. Through the research process, we have learned much about insights in building deep network explanations, the existence and frequency of multiple explanations, and various tricks of the trade that make explanations work. In this paper, we attempt to share those insights and opinions with the readers with the hope that some of them will be informative for future researchers on explainable deep learning.},
  archive      = {J_AAIL},
  author       = {Li Fuxin and Zhongang Qi and Saeed Khorram and Vivswan Shitole and Prasad Tadepalli and Minsuk Kahng and Alan Fern},
  doi          = {10.1002/ail2.46},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e46},
  shortjournal = {Appl. AI Lett.},
  title        = {From heatmaps to structured explanations of image classifiers},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring and characterizing generalization in deep
reinforcement learning. <em>AAIL</em>, <em>2</em>(4), e45. (<a
href="https://doi.org/10.1002/ail2.45">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (RL) methods have achieved remarkable performance on challenging control tasks. Observations of the resulting behavior give the impression that the agent has constructed a generalized representation that supports insightful action decisions. We re-examine what is meant by generalization in RL, and propose several definitions based on an agent&#39;s performance in on-policy, off-policy, and unreachable states. We propose a set of practical methods for evaluating agents with these definitions of generalization. We demonstrate these techniques on a common benchmark task for deep RL, and we show that the learned networks make poor decisions for states that differ only slightly from on-policy states, even though those states are not selected adversarially. We focus our analyses on the deep Q-networks (DQNs) that kicked off the modern era of deep RL. Taken together, these results call into question the extent to which DQNs learn generalized representations, and suggest that more experimentation and analysis is necessary before claims of representation learning can be supported.},
  archive      = {J_AAIL},
  author       = {Sam Witty and Jun K. Lee and Emma Tosch and Akanksha Atrey and Kaleigh Clary and Michael L. Littman and David Jensen},
  doi          = {10.1002/ail2.45},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e45},
  shortjournal = {Appl. AI Lett.},
  title        = {Measuring and characterizing generalization in deep reinforcement learning},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remembering for the right reasons: Explanations reduce
catastrophic forgetting. <em>AAIL</em>, <em>2</em>(4), e44. (<a
href="https://doi.org/10.1002/ail2.44">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the evidence for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has “the right reasons” for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons .},
  archive      = {J_AAIL},
  author       = {Sayna Ebrahimi and Suzanne Petryk and Akash Gokul and William Gan and Joseph E. Gonzalez and Marcus Rohrbach and Trevor Darrell},
  doi          = {10.1002/ail2.44},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e44},
  shortjournal = {Appl. AI Lett.},
  title        = {Remembering for the right reasons: Explanations reduce catastrophic forgetting},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patching interpretable and-or-graph knowledge representation
using augmented reality. <em>AAIL</em>, <em>2</em>(4), e43. (<a
href="https://doi.org/10.1002/ail2.43">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel augmented reality (AR) interface to provide effective means to diagnose a robot&#39;s erroneous behaviors, endow it with new skills, and patch its knowledge structure represented by an And-Or-Graph (AOG). Specifically, an AOG representation of opening medicine bottles is learned from human demonstration and yields a hierarchical structure that captures the spatiotemporal compositional nature of the given task, which is highly interpretable for the users. Through a series of psychological experiments, we demonstrate that the explanations of a robotic system, inherited from and produced by the AOG, can better foster human trust compared to other forms of explanations. Moreover, by visualizing the knowledge structure and robot states, the AR interface allows human users to intuitively understand what the robot knows, supervise the robot&#39;s task planner, and interactively teach the robot with new actions. Together, users can quickly identify the reasons for failures and conveniently patch the current knowledge structure to prevent future errors. This capability demonstrates the interpretability of our knowledge representation and the new forms of interactions afforded by the proposed AR interface.},
  archive      = {J_AAIL},
  author       = {Hangxin Liu and Yixin Zhu and Song-Chun Zhu},
  doi          = {10.1002/ail2.43},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e43},
  shortjournal = {Appl. AI Lett.},
  title        = {Patching interpretable and-or-graph knowledge representation using augmented reality},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User-guided global explanations for deep image recognition:
A user study. <em>AAIL</em>, <em>2</em>(4), e42. (<a
href="https://doi.org/10.1002/ail2.42">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a user-guided approach for producing global explanations of deep networks for image recognition. The global explanations are produced with respect to a test data set and give the overall frequency of different “recognition reasons” across the data. Each reason corresponds to a small number of the most significant human-recognizable visual concepts used by the network. The key challenge is that the visual concepts cannot be predetermined and those concepts will often not correspond to existing vocabulary or have labeled data sets. We address this issue via an interactive-naming interface, which allows users to freely cluster significant image regions in the data into visually similar concepts. Our main contribution is a user study on two visual recognition tasks. The results show that the participants were able to produce a small number of visual concepts sufficient for explanation and that there was significant agreement among the concepts, and hence global explanations, produced by different participants.},
  archive      = {J_AAIL},
  author       = {Mandana Hamidi-Haines and Zhongang Qi and Alan Fern and Fuxin Li and Prasad Tadepalli},
  doi          = {10.1002/ail2.42},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e42},
  shortjournal = {Appl. AI Lett.},
  title        = {User-guided global explanations for deep image recognition: A user study},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable, interactive content-based image retrieval.
<em>AAIL</em>, <em>2</em>(4), e41. (<a
href="https://doi.org/10.1002/ail2.41">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the value of explanations in a human-in-the-loop (HITL) system is difficult. Previous methods either measure explanation-specific values that do not correspond to user tasks and needs or poll users on how useful they find the explanations to be. In this work, we quantify how much explanations help the user through a utility-based paradigm that measures change in task performance when using explanations vs not. Our chosen task is content-based image retrieval (CBIR), which has well-established baselines and performance metrics independent of explainability. We extend an existing HITL image retrieval system that incorporates user feedback with similarity-based saliency maps (SBSM) that indicate to the user which parts of the retrieved images are most similar to the query image. The system helps the user understand what it is paying attention to through saliency maps, and the user helps the system understand their goal through saliency-guided relevance feedback. Using the MS-COCO dataset, a standard object detection and segmentation dataset, we conducted extensive, crowd-sourced experiments validating that SBSM improves interactive image retrieval. Although the performance increase is modest in the general case, in more difficult cases such as cluttered scenes, using explanations yields an 6.5% increase in accuracy. To the best of our knowledge, this is the first large-scale user study showing that visual saliency map explanations improve performance on a real-world, interactive task. Our utility-based evaluation paradigm is general and potentially applicable to any task for which explainability can be incorporated.},
  archive      = {J_AAIL},
  author       = {Bhavan Vasu and Brian Hu and Bo Dong and Roddy Collins and Anthony Hoogs},
  doi          = {10.1002/ail2.41},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e41},
  shortjournal = {Appl. AI Lett.},
  title        = {Explainable, interactive content-based image retrieval},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). XAITK: The explainable AI toolkit. <em>AAIL</em>,
<em>2</em>(4), e40. (<a href="https://doi.org/10.1002/ail2.40">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in artificial intelligence (AI), driven mainly by deep neural networks, have yielded remarkable progress in fields, such as computer vision, natural language processing, and reinforcement learning. Despite these successes, the inability to predict how AI systems will behave “in the wild” impacts almost all stages of planning and deployment, including research and development, verification and validation, and user trust and acceptance. The field of explainable artificial intelligence (XAI) seeks to develop techniques enabling AI algorithms to generate explanations of their results; generally these are human-interpretable representations or visualizations that are meant to “explain” how the system produced its outputs. We introduce the Explainable AI Toolkit (XAITK), a DARPA-sponsored effort that builds on results from the 4-year DARPA XAI program. The XAITK has two goals: (a) to consolidate research results from DARPA XAI into a single publicly accessible repository; and (b) to identify operationally relevant capabilities developed on DARPA XAI and assist in their transition to interested partners. We first describe the XAITK website and associated capabilities. These place the research results from DARPA XAI in the wider context of general research in the field of XAI, and include performer contributions of code, data, publications, and reports. We then describe the XAITK analytics and autonomy software frameworks. These are Python-based frameworks focused on particular XAI domains, and designed to provide a single integration endpoint for multiple algorithm implementations from across DARPA XAI. Each framework generalizes APIs for system-level data and control while providing a plugin interface for existing and future algorithm implementations. The XAITK project can be followed at: https://xaitk.org .},
  archive      = {J_AAIL},
  author       = {Brian Hu and Paul Tunison and Bhavan Vasu and Nitesh Menon and Roddy Collins and Anthony Hoogs},
  doi          = {10.1002/ail2.40},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e40},
  shortjournal = {Appl. AI Lett.},
  title        = {XAITK: The explainable AI toolkit},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable neural computation via stack neural module
networks. <em>AAIL</em>, <em>2</em>(4), e39. (<a
href="https://doi.org/10.1002/ail2.39">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired subtask decomposition without relying on strong supervision. Our model allows linking different reasoning tasks through shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model&#39;s underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
  archive      = {J_AAIL},
  author       = {Ronghang Hu and Jacob Andreas and Trevor Darrell and Kate Saenko},
  doi          = {10.1002/ail2.39},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e39},
  shortjournal = {Appl. AI Lett.},
  title        = {Explainable neural computation via stack neural module networks},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Abstraction, validation, and generalization for explainable
artificial intelligence. <em>AAIL</em>, <em>2</em>(4), e37. (<a
href="https://doi.org/10.1002/ail2.37">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network architectures are achieving superhuman performance on an expanding range of tasks. To effectively and safely deploy these systems, their decision-making must be understandable to a wide range of stakeholders. Methods to explain artificial intelligence (AI) have been proposed to answer this challenge, but a lack of theory impedes the development of systematic abstractions, which are necessary for cumulative knowledge gains. We propose Bayesian Teaching as a framework for unifying explainable AI (XAI) by integrating machine learning and human learning. Bayesian Teaching formalizes explanation as a communication act of an explainer to shift the beliefs of an explainee. This formalization decomposes a wide range of XAI methods into four components: (a) the target inference, (b) the explanation, (c) the explainee model, and (d) the explainer model. The abstraction afforded by Bayesian Teaching to decompose XAI methods elucidates the invariances among them. The decomposition of XAI systems enables modular validation, as each of the first three components listed can be tested semi-independently. This decomposition also promotes generalization through recombination of components from different XAI systems, which facilitates the generation of novel variants. These new variants need not be evaluated one by one provided that each component has been validated, leading to an exponential decrease in development time. Finally, by making the goal of explanation explicit, Bayesian Teaching helps developers to assess how suitable an XAI system is for its intended real-world use case. Thus, Bayesian Teaching provides a theoretical framework that encourages systematic, scientific investigation of XAI.},
  archive      = {J_AAIL},
  author       = {Scott Cheng-Hsin Yang and Tomas Folke and Patrick Shafto},
  doi          = {10.1002/ail2.37},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e37},
  shortjournal = {Appl. AI Lett.},
  title        = {Abstraction, validation, and generalization for explainable artificial intelligence},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From “no clear winner” to an effective explainable
artificial intelligence process: An empirical journey. <em>AAIL</em>,
<em>2</em>(4), e36. (<a href="https://doi.org/10.1002/ail2.36">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“In what circumstances would you want this AI to make decisions on your behalf?” We have been investigating how to enable a user of an Artificial Intelligence-powered system to answer questions like this through a series of empirical studies, a group of which we summarize here. We began the series by (a) comparing four explanation configurations of saliency explanations and/or reward explanations. From this study we learned that, although some configurations had significant strengths, no one configuration was a clear “winner.” This result led us to hypothesize that one reason for the low success rates Explainable AI (XAI) research has in enabling users to create a coherent mental model is that the AI itself does not have a coherent model. This hypothesis led us to (b) build a model-based agent, to compare explaining it with explaining a model-free agent. Our results were encouraging, but we then realized that participants&#39; cognitive energy was being sapped by having to create not only a mental model, but also a process by which to create that mental model. This realization led us to (c) create such a process (which we term After-Action Review for AI or “AAR/AI”) for them, integrate it into the explanation environment, and compare participants&#39; success with AAR/AI scaffolding vs without it. Our AAR/AI studies&#39; results showed that AAR/AI participants were more effective assessing the AI than non-AAR/AI participants, with significantly better precision and significantly better recall at finding the AI&#39;s reasoning flaws.},
  archive      = {J_AAIL},
  author       = {Jonathan Dodge and Andrew Anderson and Roli Khanna and Jed Irvine and Rupika Dikkala and Kin-Ho Lam and Delyar Tabatabai and Anita Ruangrotsakun and Zeyad Shureih and Minsuk Kahng and Alan Fern and Margaret Burnett},
  doi          = {10.1002/ail2.36},
  journal      = {Applied AI Letters},
  month        = {12},
  number       = {4},
  pages        = {e36},
  shortjournal = {Appl. AI Lett.},
  title        = {From “no clear winner” to an effective explainable artificial intelligence process: An empirical journey},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A practical approach for applying machine learning in the
detection and classification of network devices used in building
management. <em>AAIL</em>, <em>2</em>(3), e35. (<a
href="https://doi.org/10.1002/ail2.35">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing deployment of smart buildings and infrastructure, supervisory control and data acquisition (SCADA) devices and the underlying IT network have become essential elements for the proper operations of these highly complex systems. Of course, with the increase in automation and the proliferation of SCADA devices, a corresponding increase in surface area of attack on critical infrastructure has increased. Understanding device behaviors in terms of known and understood or potentially qualified activities vs unknown and potentially nefarious activities in near-real time is a key component of any security solution. In this paper, we investigate the challenges with building robust machine learning models to identify unknowns purely from network traffic both inside and outside firewalls, starting with missing or inconsistent labels across sites, feature engineering and learning, temporal dependencies and analysis, and training data quality (including small sample sizes) for both shallow and deep learning methods. To demonstrate these challenges and the capabilities we have developed, we focus on Building Automation and Control networks (BACnet) from a private commercial building system. Our results show that “Model Zoo” built from binary classifiers based on each device or behavior combined with an ensemble classifier integrating information from all classifiers provides a reliable methodology to identify unknown devices as well as determining specific known devices when the device type is in the training set. The capability of the Model Zoo framework is shown to be directly linked to feature engineering and learning, and the dependency of the feature selection varies depending on both the binary and ensemble classifiers as well.},
  archive      = {J_AAIL},
  author       = {Maroun Touma and Shalisha Witherspoon and Shonda Witherspoon and Isabelle Crawford-Eng},
  doi          = {10.1002/ail2.35},
  journal      = {Applied AI Letters},
  month        = {9},
  number       = {3},
  pages        = {e35},
  shortjournal = {Appl. AI Lett.},
  title        = {A practical approach for applying machine learning in the detection and classification of network devices used in building management},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards an affordable magnetomyography instrumentation and
low model complexity approach for labour imminency prediction using a
novel multiresolution analysis. <em>AAIL</em>, <em>2</em>(3), e34. (<a
href="https://doi.org/10.1002/ail2.34">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to predict the onset of labour is seen to be an important tool in a clinical setting. Magnetomyography has shown promise in the area of labour imminency prediction, but its clinical application remains limited due to high resource consumption associated with its broad number of channels. In this study, five electrode channels, which account for 3.3% of the total, are used alongside a novel signal decomposition algorithm and low complexity classifiers (logistic regression and linear-SVM) to classify between labour imminency due within 0 to 48 hours and &gt;48 hours. The results suggest that the parsimonious representation comprising of five electrode channels and novel signal decomposition method alongside the candidate classifiers could allow for greater affordability and hence clinical viability of the magnetomyography-based prediction model, which carries a good degree of model interpretability. The results showed around a 20% increase on average for the novel decomposition method, alongside a reduced group of features across the various classification metrics considered for both the logistic regression and support vector machine.},
  archive      = {J_AAIL},
  author       = {Ejay Nsugbe and Ibrahim Sanusi},
  doi          = {10.1002/ail2.34},
  journal      = {Applied AI Letters},
  month        = {9},
  number       = {3},
  pages        = {e34},
  shortjournal = {Appl. AI Lett.},
  title        = {Towards an affordable magnetomyography instrumentation and low model complexity approach for labour imminency prediction using a novel multiresolution analysis},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adapting natural language processing for technical text.
<em>AAIL</em>, <em>2</em>(3), e33. (<a
href="https://doi.org/10.1002/ail2.33">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent dramatic successes, natural language processing (NLP) is not ready to address a variety of real-world problems. Its reliance on large standard corpora, a training and evaluation paradigm that favors the learning of shallow heuristics, and large computational resource requirements, makes domain-specific application of even the most successful NLP techniques difficult. This paper proposes technical language processing (TLP) which brings engineering principles and practices to NLP specifically for the purpose of extracting actionable information from language generated by experts in their technical tasks, systems, and processes. TLP envisages NLP as a socio-technical system rather than as an algorithmic pipeline. We describe how the TLP approach to meaning and generalization differs from that of NLP, how data quantity and quality can be addressed in engineering technical domains, and the potential risks of not adapting NLP for technical use cases. Engineering problems can benefit immensely from the inclusion of knowledge from unstructured data, currently unavailable due to issues with out of the box NLP packages. We illustrate the TLP approach by focusing on maintenance in industrial organizations as a case-study.},
  archive      = {J_AAIL},
  author       = {Alden Dima and Sarah Lukens and Melinda Hodkiewicz and Thurston Sexton and Michael P. Brundage},
  doi          = {10.1002/ail2.33},
  journal      = {Applied AI Letters},
  month        = {9},
  number       = {3},
  pages        = {e33},
  shortjournal = {Appl. AI Lett.},
  title        = {Adapting natural language processing for technical text},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep imputation on large-scale drug discovery data.
<em>AAIL</em>, <em>2</em>(3), e31. (<a
href="https://doi.org/10.1002/ail2.31">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More accurate predictions of the biological properties of chemical compounds would guide the selection and design of new compounds in drug discovery and help to address the enormous cost and low success-rate of pharmaceutical R&amp;D. However, this domain presents a significant challenge for AI methods due to the sparsity of compound data and the noise inherent in results from biological experiments. In this paper, we demonstrate how data imputation using deep learning provides substantial improvements over quantitative structure-activity relationship (QSAR) machine learning models that are widely applied in drug discovery. We present the largest-to-date successful application of deep-learning imputation to datasets which are comparable in size to the corporate data repository of a pharmaceutical company (678 994 compounds by 1166 endpoints). We demonstrate this improvement for three areas of practical application linked to distinct use cases; (a) target activity data compiled from a range of drug discovery projects, (b) a high value and heterogeneous dataset covering complex absorption, distribution, metabolism, and elimination properties, and (c) high throughput screening data, testing the algorithm&#39;s limits on early stage noisy and very sparse data. Achieving median coefficients of determination, R 2 , of 0.69, 0.36, and 0.43, respectively, across these applications, the deep learning imputation method offers an unambiguous improvement over random forest QSAR methods, which achieve median R 2 values of 0.28, 0.19, and 0.23, respectively. We also demonstrate that robust estimates of the uncertainties in the predicted values correlate strongly with the accuracies in prediction, enabling greater confidence in decision-making based on the imputed values.},
  archive      = {J_AAIL},
  author       = {Benedict W. J. Irwin and Thomas M. Whitehead and Scott Rowland and Samar Y. Mahmoud and Gareth J. Conduit and Matthew D. Segall},
  doi          = {10.1002/ail2.31},
  journal      = {Applied AI Letters},
  month        = {9},
  number       = {3},
  pages        = {e31},
  shortjournal = {Appl. AI Lett.},
  title        = {Deep imputation on large-scale drug discovery data},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrap aggregation accuracy gains in nearest-neighbor
nowcasting of swedish gross domestic product. <em>AAIL</em>,
<em>2</em>(2), e32. (<a href="https://doi.org/10.1002/ail2.32">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the accuracy gains that can be made by applying bootstrap aggregation to K nearest-neighbor nowcasting of Swedish gross domestic product. Using both a simulation-based approach and a theoretical approach, the results indicate that substantial nowcasting accuracy gains can be made using bootstrap aggregation when considering one neighbor in the nowcasting algorithm, that is, when K = 1. Furthermore, a comparison of the simulation-based and theoretical approaches to bootstrap aggregation indicates that using as few as 25 bootstrap replications can carry, in expectation, the simulation-based results close to the theoretical results.},
  archive      = {J_AAIL},
  author       = {Kristian Jönsson},
  doi          = {10.1002/ail2.32},
  journal      = {Applied AI Letters},
  month        = {6},
  number       = {2},
  pages        = {e32},
  shortjournal = {Appl. AI Lett.},
  title        = {Bootstrap aggregation accuracy gains in nearest-neighbor nowcasting of swedish gross domestic product},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Good practices for bayesian optimization of high dimensional
structured spaces. <em>AAIL</em>, <em>2</em>(2), e24. (<a
href="https://doi.org/10.1002/ail2.24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of structured but high dimensional data has opened new opportunities for optimization. One emerging and promising avenue is the exploration of unsupervised methods for projecting structured high dimensional data into low dimensional continuous representations, simplifying the optimization problem and enabling the application of traditional optimization methods. However, this line of research has been purely methodological with little connection to the needs of practitioners so far. In this article, we study the effect of different search space design choices for performing Bayesian optimization in high dimensional structured datasets. In particular, we analyses the influence of the dimensionality of the latent space, the role of the acquisition function and evaluate new methods to automatically define the optimization bounds in the latent space. Finally, based on experimental results using synthetic and real datasets, we provide recommendations for the practitioners.},
  archive      = {J_AAIL},
  author       = {Eero Siivola and Andrei Paleyes and Javier González and Aki Vehtari},
  doi          = {10.1002/ail2.24},
  journal      = {Applied AI Letters},
  month        = {6},
  number       = {2},
  pages        = {e24},
  shortjournal = {Appl. AI Lett.},
  title        = {Good practices for bayesian optimization of high dimensional structured spaces},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heritage connector: A machine learning framework for
building linked open data from museum collections. <em>AAIL</em>,
<em>2</em>(2), e23. (<a href="https://doi.org/10.1002/ail2.23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As with almost all data, museum collection catalogues are largely unstructured, variable in consistency and overwhelmingly composed of thin records. The form of these catalogues means that the potential for new forms of research, access and scholarly enquiry that range across multiple collections and related datasets remains dormant. In the project Heritage Connector: Transforming text into data to extract meaning and make connections , we are applying a battery of digital techniques to connect similar, identical and related objects within and across collections and other publications. In this article, we describe a framework to create a Linked Open Data knowledge graph from digital museum catalogues, perform record linkage to Wikidata, and add new entities to this graph from textual catalogue record descriptions (information retrieval). We focus on the use of machine learning to create these links at scale with a small amount of labelled data, and models which are small enough to run inference on datasets the size of museum collections on a mid-range laptop or a small cloud virtual machine. Our method for record linkage against Wikidata achieves 85%+ precision with the Science Museum Group (SMG) collection, and our method for information retrieval is shown to improve NER performance compared with pretrained models on the SMG collection with no labelled training data. We publish open-source software providing tools to perform these tasks.},
  archive      = {J_AAIL},
  author       = {Kalyan Dutia and John Stack},
  doi          = {10.1002/ail2.23},
  journal      = {Applied AI Letters},
  month        = {6},
  number       = {2},
  pages        = {e23},
  shortjournal = {Appl. AI Lett.},
  title        = {Heritage connector: A machine learning framework for building linked open data from museum collections},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical spline for time series prediction: An
application to naval ship engine failure rate. <em>AAIL</em>,
<em>2</em>(1), e22. (<a href="https://doi.org/10.1002/ail2.22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting equipment failure is important because it could improve availability and cut down the operating budget. Previous literature has attempted to model failure rate with bathtub-formed function, Weibull distribution, Bayesian network, or analytic hierarchy process. But these models perform well with a sufficient amount of data and could not incorporate the two salient characteristics: imbalanced category and sharing structure. Hierarchical model has the advantage of partial pooling. The proposed model is based on Bayesian hierarchical B-spline. Time series of the failure rate of 99 Republic of Korea Naval ships are modeled hierarchically, where each layer corresponds to ship engine, engine type, and engine archetype. As a result of the analysis, the suggested model predicted the failure rate of an entire lifetime accurately in multiple situational conditions, such as prior knowledge of the engine.},
  archive      = {J_AAIL},
  author       = {Hyunji Moon and Jinwoo Choi},
  doi          = {10.1002/ail2.22},
  journal      = {Applied AI Letters},
  month        = {3},
  number       = {1},
  pages        = {e22},
  shortjournal = {Appl. AI Lett.},
  title        = {Hierarchical spline for time series prediction: An application to naval ship engine failure rate},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cognitive analysis in sports: Supporting match analysis and
scouting through artificial intelligence. <em>AAIL</em>, <em>2</em>(1),
e21. (<a href="https://doi.org/10.1002/ail2.21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In elite sports, there is an opportunity to take advantage of rich and detailed datasets generated across multiple threads of the sporting business. Challenges currently exist due to time constraints to analyse the data, as well as the quantity and variety of data available to assess. Artificial Intelligence (AI) techniques can be a valuable asset in assisting decision makers in tackling such challenges, but deep AI skills are generally not held by those with rich experience in sporting domains. Here, we describe how certain commonly available AI services can be used to provide analytic assistance to sports experts in exploring, and gaining insights from, typical data sources. In particular, we focus on the use of Natural Language Processing and Conversational Interfaces to provide users with an intuitive and time-saving toolkit to explore their datasets and the conclusions arising from analytics performed on them. We show the benefit of presenting powerful AI and analytic techniques to domain experts, showing the potential for impact not only at the elite level of sports, where AI and analytic capabilities may be more available, but also at a more grass-roots level where there is generally little access to specialist resources. The work described in this paper was trialled with Leatherhead Football Club, a semi-professional team that, at the time, were based in the English 7th tier of football.},
  archive      = {J_AAIL},
  author       = {Joe Pavitt and Dave Braines and Richard Tomsett},
  doi          = {10.1002/ail2.21},
  journal      = {Applied AI Letters},
  month        = {3},
  number       = {1},
  pages        = {e21},
  shortjournal = {Appl. AI Lett.},
  title        = {Cognitive analysis in sports: Supporting match analysis and scouting through artificial intelligence},
  volume       = {2},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
