<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj---118">BIMJ - 118</h2>
<ul>
<li><details>
<summary>
(2021h). Cover picture: Biometrical journal 8’21. <em>BIMJ</em>,
<em>63</em>(8), NA. (<a
href="https://doi.org/10.1002/bimj.202170081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170081},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 8&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modern data science with r. Benjamin s. Baumer, daniel t.
Kaplan nicholas j. Horton (2021). CRC press, boca raton. 631 pages,
ISBN: 9780367191498. <em>BIMJ</em>, <em>63</em>(8), 1748–1749. (<a
href="https://doi.org/10.1002/bimj.202100258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Annika Hoyer},
  doi          = {10.1002/bimj.202100258},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1748-1749},
  shortjournal = {Bio. J.},
  title        = {Modern data science with r. benjamin s. baumer, daniel t. kaplan nicholas j. horton (2021). CRC press, boca raton. 631 pages, ISBN: 9780367191498},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical design and analysis of biological experiments.
H.-m. Kaltenbach (2021). Springer, cham. Springer series statistics for
biology and health. 269 pages, ISBN: 978-3-030-69640-5. <em>BIMJ</em>,
<em>63</em>(8), 1747. (<a
href="https://doi.org/10.1002/bimj.202100238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Ludwig A. Hothorn},
  doi          = {10.1002/bimj.202100238},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1747},
  shortjournal = {Bio. J.},
  title        = {Statistical design and analysis of biological experiments. h.-m. kaltenbach (2021). springer, cham. springer series statistics for biology and health. 269 pages, ISBN: 978-3-030-69640-5},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applied meta-analysis with r and stata. Ding-geng chen, karl
e. Peace (2021). Boca raton, FL, chapman and hall/CRC press, 2nd ed.,
456 pages. ISBN 9780367183837. <em>BIMJ</em>, <em>63</em>(8), 1745–1746.
(<a href="https://doi.org/10.1002/bimj.202100226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Richard D. Riley},
  doi          = {10.1002/bimj.202100226},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1745-1746},
  shortjournal = {Bio. J.},
  title        = {Applied meta-analysis with r and stata. ding-geng chen, karl e. peace (2021). boca raton, FL, chapman and Hall/CRC press, 2nd ed., 456 pages. ISBN 9780367183837},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive local false discovery rate procedures for highly
spiky data and their application RNA sequencing data of yeast SET4
deletion mutants. <em>BIMJ</em>, <em>63</em>(8), 1729–1744. (<a
href="https://doi.org/10.1002/bimj.202000256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chromatin dynamics are central to the regulation of gene expression and genome stability. In order to improve understanding of the factors regulating chromatin dynamics, the genes encoding these factors are deleted and the differential gene expression profiles are determined using approaches such as RNA sequencing. Here, we analyzed a gene expression dataset aimed at uncovering the function of the relatively uncharacterized chromatin regulator, Set4, in the model system Saccharomyces cerevisiae (budding yeast). The main theme of this paper focuses on identifying the highly differentially expressed genes in cells deleted for Set4 (referred to as Set4 mutant dataset) compared to the wild-type yeast cells. The Set4 mutant data produce a spiky distribution on the log-fold changes of their expressions, and it is reasonably assumed that genes which are not highly differentially expressed come from a mixture of two normal distributions. We propose an adaptive local false discovery rate (FDR) procedure, which estimates the null distribution of the log-fold changes empirically. We numerically show that, unlike existing approaches, our proposed method controls FDR at the aimed level (0.05) and also has competitive power in finding differentially expressed genes. Finally, we apply our procedure to analyzing the Set4 mutant dataset.},
  archive      = {J_BIMJ},
  author       = {Mark Louie Ramos and DoHwan Park and Johan Lim and Junyong Park and Khoa Tran and Eric Joshua Garcia and Erin Green},
  doi          = {10.1002/bimj.202000256},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1729-1744},
  shortjournal = {Bio. J.},
  title        = {Adaptive local false discovery rate procedures for highly spiky data and their application RNA sequencing data of yeast SET4 deletion mutants},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subclassification estimation of the weighted average
treatment effect. <em>BIMJ</em>, <em>63</em>(8), 1706–1728. (<a
href="https://doi.org/10.1002/bimj.202000310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting and subclassification are popular approaches using propensity scores (PSs) for estimation of causal effects. Weighting is appealing in that it gives consistent estimators for various causal estimands if appropriate weights are well defined and the PS model is correctly specified. Subclassification is known to be more robust to model misspecification than weighting, but its application to diverse causal estimands is limited. In this article, we propose generalized stratum weights to implement subclassification estimators for various causal estimands. These weights include stratum weights for the average treatment effect (ATE) of the overall population and those for the ATE of the treated as special cases. For this, we incorporate strata into the expression of the weighted average treatment effect (WATE). Particularly, we identify stratum weights for the ATE for the overlap population (ATO), for which the weighting estimator is known to be most efficient among the class of WATE estimators. We show that the identified stratum weights for ATO are equivalent to the optimal stratum weights, which are the inverse variances of the stratum-specific estimators. Simulation studies demonstrate that the proposed subclassification estimator for ATO is more robust to model misspecification than the weighting estimator for ATO. We also propose augmented subclassification estimators, which are shown to be less biased than the subclassification estimators when only the outcome model is correctly specified. The practical utility of the proposed methods is illustrated in a study of right heart catheterization.},
  archive      = {J_BIMJ},
  author       = {Byeong Yeob Choi},
  doi          = {10.1002/bimj.202000310},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1706-1728},
  shortjournal = {Bio. J.},
  title        = {Subclassification estimation of the weighted average treatment effect},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two sensitive characteristics and their overlap with two
questions per card. <em>BIMJ</em>, <em>63</em>(8), 1688–1705. (<a
href="https://doi.org/10.1002/bimj.202000395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we developed a new unique unrelated question randomized response model in which each card has two questions, either both questions on the sensitive characteristics or both questions on the two unrelated characteristics. The proposed model is unique in the sense this is the only way of asking two questions printed on each card that leads to protection of the privacy of the respondent. We first develop estimators of the prevalence of the two sensitive characteristics and of their overlap. Then we show that the resultant estimators are unbiased. Next we derive variance expressions for the developed estimators of the proportions. We also compute the relative efficiency and relative privacy protection of the proposed model with respect to its competitors. The variances of the proposed estimators are also verified by comparing them to the Cramer–Rao lower bounds of variance–covariance of the estimators. Estimators of conditional proportion, relative risk, and correlation coefficient are also discussed. Lastly, a real data application of the proposed model is considered, which shows the importance of the use of the proposed model in medical and social science studies.},
  archive      = {J_BIMJ},
  author       = {Tonghui Xu and Stephen A. Sedory and Sarjinder Singh},
  doi          = {10.1002/bimj.202000395},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1688-1705},
  shortjournal = {Bio. J.},
  title        = {Two sensitive characteristics and their overlap with two questions per card},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression models for order-of-addition experiments.
<em>BIMJ</em>, <em>63</em>(8), 1673–1687. (<a
href="https://doi.org/10.1002/bimj.202100048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of order-of-addition (OofA) experiments is to identify the best order in a sequence of m components in a system. Such experiments may be analyzed by various regression models, the most popular ones being based on pairwise ordering (PWO) factors or on component-position (CP) factors. This paper reviews these models and extensions and proposes a new class of models based on response surface (RS) regression using component position numbers as predictor variables. Using two published examples, it is shown that RS models can be quite competitive. In case of model uncertainty, we advocate the use of model averaging for analysis. The averaging idea leads naturally to a design approach based on a compound optimality criterion assigning weights to each candidate model.},
  archive      = {J_BIMJ},
  author       = {Hans-Peter Piepho and Emlyn R. Williams},
  doi          = {10.1002/bimj.202100048},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1673-1687},
  shortjournal = {Bio. J.},
  title        = {Regression models for order-of-addition experiments},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multilevel structural equation model for assessing a drug
effect on a patient-reported outcome measure in on-demand medication
data. <em>BIMJ</em>, <em>63</em>(8), 1652–1672. (<a
href="https://doi.org/10.1002/bimj.202100046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze data from a clinical trial investigating the effect of an on-demand drug for women with low sexual desire. These data consist of a varying number of measurements/events across patients of when the drug was taken, including data on a patient-reported outcome consisting of five items measuring an unobserved construct (latent variable). Traditionally, these data are aggregated prior to analysis by composing one sum score per event and averaging this sum score over all observed events. In this paper, we explain the drawbacks of this aggregating approach. One drawback is that these averages have different standard errors because the variance of the underlying events differs between patients and because the number of events per patient differs. Another drawback is the implicit assumption that all items have equal weight in relation to the latent variable being measured. We propose a multilevel structural equation model, treating the events (level 1) as nested observations within patients (level 2), as alternative analysis method to overcome these drawbacks. The model we apply includes a factor model measuring a latent variable at the level of the event and at the level of the patient. Then, in the same model, the latent variables are regressed on covariates to assess the drug effect. We discuss the inferences obtained about the efficacy of the on-demand drug using our proposed model. We further illustrate how to test for measurement invariance across grouping covariates and levels using the same model.},
  archive      = {J_BIMJ},
  author       = {Rob Kessels and Mirjam Moerbeek and Jos Bloemers and Peter G.M. van der Heijden},
  doi          = {10.1002/bimj.202100046},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1652-1672},
  shortjournal = {Bio. J.},
  title        = {A multilevel structural equation model for assessing a drug effect on a patient-reported outcome measure in on-demand medication data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the optimal population upper bound for scan
methods in retrospective disease surveillance. <em>BIMJ</em>,
<em>63</em>(8), 1633–1651. (<a
href="https://doi.org/10.1002/bimj.202000273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correctly and quickly identifying disease patterns and clusters is a vital aspect of public health and epidemiology so that disease outbreaks can be mitigated as effectively as possible. The circular scan method is one of the most commonly used methods for detecting disease outbreaks and clusters in retrospective and prospective disease surveillance. The circular scan method requires a population upper bound in order to construct the set of candidate zones to be scanned, which is usually set to 50% of the total population. The performance of the circular scan method is affected by the choice of the population upper bound, and choosing an upper bound different from the default value can improve the method&#39;s performance. Recently, the Gini coefficient based on the Lorenz curve, which was originally used in economics, was proposed to determine a better population upper bound. We present the elbow method, a new method for choosing the population upper bound, which seeks to address some of the limitations of the Gini-based method while improving the performance of the circular scan method over the default value. To evaluate the performance of the proposed approach, we evaluate the sensitivity and positive predictive value of the circular scan method for publicly-available benchmark data for the default value, the Gini coefficient method, and the elbow method.},
  archive      = {J_BIMJ},
  author       = {Mohammad Meysami and Joshua P. French and Ettie M. Lipner},
  doi          = {10.1002/bimj.202000273},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1633-1651},
  shortjournal = {Bio. J.},
  title        = {Estimating the optimal population upper bound for scan methods in retrospective disease surveillance},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A statistical model for the dynamics of COVID-19 infections
and their case detection ratio in 2020. <em>BIMJ</em>, <em>63</em>(8),
1623–1632. (<a href="https://doi.org/10.1002/bimj.202100125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case detection ratio of coronavirus disease 2019 (COVID-19) infections varies over time due to changing testing capacities, different testing strategies, and the evolving underlying number of infections itself. This note shows a way of quantifying these dynamics by jointly modeling the reported number of detected COVID-19 infections with nonfatal and fatal outcomes. The proposed methodology also allows to explore the temporal development of the actual number of infections, both detected and undetected, thereby shedding light on the infection dynamics. We exemplify our approach by analyzing German data from 2020, making only use of data available since the beginning of the pandemic. Our modeling approach can be used to quantify the effect of different testing strategies, visualize the dynamics in the case detection ratio over time, and obtain information about the underlying true infection numbers, thus enabling us to get a clearer picture of the course of the COVID-19 pandemic in 2020.},
  archive      = {J_BIMJ},
  author       = {Marc Schneble and Giacomo De Nicola and Göran Kauermann and Ursula Berger},
  doi          = {10.1002/bimj.202100125},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1623-1632},
  shortjournal = {Bio. J.},
  title        = {A statistical model for the dynamics of COVID-19 infections and their case detection ratio in 2020},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian variable selection for the cox regression model
with spatially varying coefficients with applications to louisiana
respiratory cancer data. <em>BIMJ</em>, <em>63</em>(8), 1607–1622. (<a
href="https://doi.org/10.1002/bimj.202000047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox regression model is a commonly used model in survival analysis. In public health studies, clinical data are often collected from medical service providers of different locations. There are large geographical variations in the covariate effects on survival rates from particular diseases. In this paper, we focus on the variable selection issue for the Cox regression model with spatially varying coefficients. We propose a Bayesian hierarchical model which incorporates a horseshoe prior for sparsity and a point mass mixture prior to determine whether a regression coefficient is spatially varying. An efficient two-stage computational method is used for posterior inference and variable selection. It essentially applies the existing method for maximizing the partial likelihood for the Cox model by site independently first and then applying an Markov chain Monte Carlo algorithm for variable selection based on results of the first stage. Extensive simulation studies are carried out to examine the empirical performance of the proposed method. Finally, we apply the proposed methodology to analyzing a real dataset on respiratory cancer in Louisiana from the Surveillance, Epidemiology, and End Results (SEER) program.},
  archive      = {J_BIMJ},
  author       = {Jinjian Mu and Qingyang Liu and Lynn Kuo and Guanyu Hu},
  doi          = {10.1002/bimj.202000047},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1607-1622},
  shortjournal = {Bio. J.},
  title        = {Bayesian variable selection for the cox regression model with spatially varying coefficients with applications to louisiana respiratory cancer data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust joint modelling of longitudinal and survival data:
Incorporating a time-varying degrees-of-freedom parameter.
<em>BIMJ</em>, <em>63</em>(8), 1587–1606. (<a
href="https://doi.org/10.1002/bimj.202000253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring of individual biomarkers has the potential of explaining the hazard of survival outcomes. In practice, these measurements are intermittently observed and are known to be subject to substantial measurement error. Joint modelling of longitudinal and survival data enables us to associate intermittently measured error-prone biomarkers with risks of survival outcomes and thus plays an important role in the analysis of medical data. Most of the joint models available in the literature have been built on the Gaussian assumption. This makes them sensitive to outliers. In this work, we study a range of robust models to address this issue. Of particular interest is the common occurrence in medical data that outliers can occur with different frequencies over time, for example, in the period when patients adjust to treatment changes. Motivated by the analysis of data gathered from patients with primary biliary cirrhosis, a new model with a time-varying robustness is introduced. Through both the motivating example and a simulation study, this research not only stresses the need to account for longitudinal outliers in the analysis of medical data and in joint modelling research but also highlights the bias and inefficiency from not properly estimating the degrees-of-freedom parameter. This work presents a number of methods in addition to the time-varying robustness, and each method can be fitted using the R package robjm .},
  archive      = {J_BIMJ},
  author       = {Lisa M. McFetridge and Özgür Asar and Jonas Wallin},
  doi          = {10.1002/bimj.202000253},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1587-1606},
  shortjournal = {Bio. J.},
  title        = {Robust joint modelling of longitudinal and survival data: Incorporating a time-varying degrees-of-freedom parameter},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A flexible joint model for multiple longitudinal biomarkers
and a time-to-event outcome: With applications to dynamic prediction
using highly correlated biomarkers. <em>BIMJ</em>, <em>63</em>(8),
1575–1586. (<a href="https://doi.org/10.1002/bimj.202000085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies it is common to collect data on multiple biomarkers during study follow-up for dynamic prediction of a time-to-event clinical outcome. The biomarkers are typically intermittently measured, missing at some event times, and may be subject to high biological variations, which cannot be readily used as time-dependent covariates in a standard time-to-event model. Moreover, they can be highly correlated if they are from in the same biological pathway. To address these issues, we propose a flexible joint model framework that models the multiple biomarkers with a shared latent reduced rank longitudinal principal component model and correlates the latent process to the event time by the Cox model for dynamic prediction of the event time. The proposed joint model for highly correlated biomarkers is more flexible than some existing methods since the latent trajectory shared by the multiple biomarkers does not require specification of a priori parametric time trend and is determined by data. We derive an expectation-maximization (EM) algorithm for parameter estimation, study large sample properties of the estimators, and adapt the developed method to make dynamic prediction of the time-to-event outcome. Bootstrap is used for standard error estimation and inference. The proposed method is evaluated using simulations and illustrated on a lung transplant data to predict chronic lung allograft dysfunction (CLAD) using chemokines measured in bronchoalveolar lavage fluid of the patients.},
  archive      = {J_BIMJ},
  author       = {Ning Li and Yi Liu and Shanpeng Li and Robert M. Elashoff and Gang Li},
  doi          = {10.1002/bimj.202000085},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1575-1586},
  shortjournal = {Bio. J.},
  title        = {A flexible joint model for multiple longitudinal biomarkers and a time-to-event outcome: With applications to dynamic prediction using highly correlated biomarkers},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensitivity and identification quantification by a relative
latent model complexity perturbation in bayesian meta-analysis.
<em>BIMJ</em>, <em>63</em>(8), 1555–1574. (<a
href="https://doi.org/10.1002/bimj.202000193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Bayesian meta-analysis expressed by a normal–normal hierarchical model (NNHM) has been widely used for combining evidence from multiple studies. Data provided for the NNHM are frequently based on a small number of studies and on uncertain within-study standard deviation values. Despite the widespread use of Bayesian NNHM, it has always been unclear to what extent the posterior inference is impacted by the heterogeneity prior (sensitivity ) and by the uncertainty in the within-study standard deviation values (identification ). Thus, to answer this question, we developed a unified method to simultaneously quantify both sensitivity and identification ( - ) for all model parameters in a Bayesian NNHM, based on derivatives of the Bhattacharyya coefficient with respect to relative latent model complexity (RLMC) perturbations. Three case studies exemplify the applicability of the method proposed: historical data for a conventional therapy, data from which one large study is first included and then excluded, and two subgroup meta-analyses specified by their randomization status. We analyzed six scenarios, crossing three RLMC targets with two heterogeneity priors (half-normal, half-Cauchy). The results show that - explicitly reveals which parameters are affected by the heterogeneity prior and by the uncertainty in the within-study standard deviation values. In addition, we compare the impact of both heterogeneity priors and quantify how - values are affected by omitting one large study and by the randomization status. Finally, the range of applicability of - is extended to Bayesian NtHM. A dedicated R package facilitates automatic - quantification in applied Bayesian meta-analyses.},
  archive      = {J_BIMJ},
  author       = {Małgorzata Roos and Sona Hunanyan and Haakon Bakka and Håvard Rue},
  doi          = {10.1002/bimj.202000193},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1555-1574},
  shortjournal = {Bio. J.},
  title        = {Sensitivity and identification quantification by a relative latent model complexity perturbation in bayesian meta-analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021h). Contents: Biometrical journal 8’21. <em>BIMJ</em>,
<em>63</em>(8), 1553–1554. (<a
href="https://doi.org/10.1002/bimj.202170084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170084},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1553-1554},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 8&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). Cover picture: Biometrical journal 7’21. <em>BIMJ</em>,
<em>63</em>(7), NA. (<a
href="https://doi.org/10.1002/bimj.202170071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170071},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 7&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction: Probability estimation with machine learning
methods for dichotomous and multicategory outcome: applications.
<em>BIMJ</em>, <em>63</em>(7), 1547. (<a
href="https://doi.org/10.1002/bimj.202170074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170074},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1547},
  shortjournal = {Bio. J.},
  title        = {Correction: probability estimation with machine learning methods for dichotomous and multicategory outcome: applications},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction: A product-limit estimator of the conditional
survival function when cure status is partially known. <em>BIMJ</em>,
<em>63</em>(7), 1544–1546. (<a
href="https://doi.org/10.1002/bimj.202100156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An error was detected in the derivation of the expression of the cumulative hazard function in a recently published paper by Safari, W. C., López-de-Ullibarri, I., and Jácome, M. A. (2021), A product-limit estimator of the conditional survival function when cure status is partially known. Biometrical Journal , 63 (5), 984–1005, https://doi.org/10.1002/bimj.202000173 . This short article aims to correct this error. There are some changes in the model notation in Section 2, the derivation of the expression of the cumulative hazard function in the Appendix, and the proofs of Lemmas 3 and 4 in the Supporting Information. Moreover, there is a small change in the generation of the values of the censoring variable in the simulation study. As a consequence, the simulation results in Section 4 are affected. A corrected version of these sections is given in the Supporting Information.},
  archive      = {J_BIMJ},
  author       = {Wende C. Safari and Ignacio López-de-Ullibarri and M. Amalia Jácome},
  doi          = {10.1002/bimj.202100156},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1544-1546},
  shortjournal = {Bio. J.},
  title        = {Correction: A product-limit estimator of the conditional survival function when cure status is partially known},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical models in toxicology. Mehdi razzaghi. Boca
raton, FL: Chapman and hall/CRC (2020). 288 pages. ISBN:
978-1-4987-7257-0. Https://doi.org/10.1201/9780429155185. <em>BIMJ</em>,
<em>63</em>(7), 1542–1543. (<a
href="https://doi.org/10.1002/bimj.202100214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Christian Ritz},
  doi          = {10.1002/bimj.202100214},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1542-1543},
  shortjournal = {Bio. J.},
  title        = {Statistical models in toxicology. mehdi razzaghi. boca raton, FL: chapman and Hall/CRC (2020). 288 pages. ISBN: 978-1-4987-7257-0. https://doi.org/10.1201/9780429155185},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulating longitudinal data from marginal structural models
using the additive hazard model. <em>BIMJ</em>, <em>63</em>(7),
1526–1541. (<a href="https://doi.org/10.1002/bimj.202000040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational longitudinal data on treatments and covariates are increasingly used to investigate treatment effects, but are often subject to time-dependent confounding. Marginal structural models (MSMs), estimated using inverse probability of treatment weighting or the g-formula, are popular for handling this problem. With increasing development of advanced causal inference methods, it is important to be able to assess their performance in different scenarios to guide their application. Simulation studies are a key tool for this, but their use to evaluate causal inference methods has been limited. This paper focuses on the use of simulations for evaluations involving MSMs in studies with a time-to-event outcome. In a simulation, it is important to be able to generate the data in such a way that the correct forms of any models to be fitted to those data are known. However, this is not straightforward in the longitudinal setting because it is natural for data to be generated in a sequential conditional manner, whereas MSMs involve fitting marginal rather than conditional hazard models. We provide general results that enable the form of the correctly specified MSM to be derived based on a conditional data generating procedure, and show how the results can be applied when the conditional hazard model is an Aalen additive hazard or Cox model. Using conditional additive hazard models is advantageous because they imply additive MSMs that can be fitted using standard software. We describe and illustrate a simulation algorithm. Our results will help researchers to effectively evaluate causal inference methods via simulation.},
  archive      = {J_BIMJ},
  author       = {Ruth H. Keogh and Shaun R. Seaman and Jon Michael Gran and Stijn Vansteelandt},
  doi          = {10.1002/bimj.202000040},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1526-1541},
  shortjournal = {Bio. J.},
  title        = {Simulating longitudinal data from marginal structural models using the additive hazard model},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New methods for the additive hazards model with the
informatively interval-censored failure time data. <em>BIMJ</em>,
<em>63</em>(7), 1507–1525. (<a
href="https://doi.org/10.1002/bimj.202000288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive hazards model is one of the most commonly used models for regression analysis of failure time data and many inference procedures have been developed for it under various situations. In particular, Wang et al. (2018a, Computational Statistics and Data Analysis , 125 , 1–9) discussed the situation where one observes informatively interval-censored data and proposed a likelihood estimation approach. However , it involves estimation of the unknown baseline cumulative hazard function and thus may be time-consuming . Corresponding to this, we propose two new procedures, an estimating equation-based one and an empirical likelihood-based one, and both do not need estimation of the cumulative hazard function and can be easily implemented. The asymptotic properties of the proposed methods are established and an extensive simulation study suggests that they work well in practical situations. An application is also provided.},
  archive      = {J_BIMJ},
  author       = {Bo Zhao and Shuying Wang and Chunjie Wang and Jianguo Sun},
  doi          = {10.1002/bimj.202000288},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1507-1525},
  shortjournal = {Bio. J.},
  title        = {New methods for the additive hazards model with the informatively interval-censored failure time data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A shared frailty model for multivariate longitudinal data on
adverse event of radiation therapy. <em>BIMJ</em>, <em>63</em>(7),
1493–1506. (<a href="https://doi.org/10.1002/bimj.202000237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oral mucositis is an inflammatory adverse event when treating head and neck cancer patients with radiation therapy (RT). The severity of its occurrence is believed to mainly depend on its site and the distribution of a cumulative radiation dose in the mouth area. The motivating study investigating differences in radiosensitivities (mucositis progression) at distinct sites where the severity of mucositis is assessed regularly at eight distinct sites on an ordinal scale results in multivariate longitudinal data and thus poses certain challenges. To deal with the multivariate longitudinal data in this particular setting, we take a time-to-event approach focusing on the first occurrence of severe mucositis at the distinct sites using the fact that the site-specific cumulative radiation dose thought to be the main driver of oral mucositis develops over time. Thereby, we may address multivariate longitudinal processes in a simpler and more compact fashion. In this article, to find out differences in mucositis progression at eight distinct sites we propose a shared frailty model for multivariate parallel processes within individuals. The shared frailty model directly incorporating ‘process indicators’ as covariates turns out to adequately explain the differences in the parallel processes (here, mucositis progressions at distinct sites) while taking individual effects into account. The parallel result with the one from the previous analysis based on the same data but conducted with an alternative statistical methodology shows adequacy of the proposed approach.},
  archive      = {J_BIMJ},
  author       = {Sung Won Kim and Martin Schumacher and Nicole H. Augustin},
  doi          = {10.1002/bimj.202000237},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1493-1506},
  shortjournal = {Bio. J.},
  title        = {A shared frailty model for multivariate longitudinal data on adverse event of radiation therapy},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive bayesian phase i clinical trial designs for
estimating the maximum tolerated doses for two drugs while fully
utilizing all toxicity information. <em>BIMJ</em>, <em>63</em>(7),
1476–1492. (<a href="https://doi.org/10.1002/bimj.202000142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combined treatments with multiple drugs are very common in the contemporary medicine, especially for medical oncology. Therefore, we developed a Bayesian adaptive Phase I clinical trial design entitled escalation with overdoing control using normalized equivalent toxicity score for estimating maximum tolerated dose (MTD) contour of two drug combination (EWOC-NETS-COM) used for oncology trials. The normalized equivalent toxicity score (NETS) as the primary endpoint of clinical trial is assumed to follow quasi-Bernoulli distribution and treated as quasi-continuous random variable in the logistic linear regression model which is used to describe the relationship between the doses of the two agents and the toxicity response. Four parameters in the dose–toxicity model were re-parameterized to parameters with explicit clinical meanings to describe the association between NETS and doses of two agents. Noninformative priors were used and Markov chain Monte Carlo was employed to update the posteriors of the four parameters in dose–toxicity model. Extensive simulations were conducted to evaluate the safety, trial efficiency, and MTD estimation accuracy of EWOC-NETS-COM under different scenarios, using the EWOC as reference. The results demonstrated that EWOC-NETS-COM not only efficiently estimates MTD contour of multiple drugs but also provides better trial efficiency by fully utilizing all toxicity information.},
  archive      = {J_BIMJ},
  author       = {Yuzi Zhang and Michael Kutner and Zhengjia Chen},
  doi          = {10.1002/bimj.202000142},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1476-1492},
  shortjournal = {Bio. J.},
  title        = {Adaptive bayesian phase i clinical trial designs for estimating the maximum tolerated doses for two drugs while fully utilizing all toxicity information},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient, doubly robust estimation of the effect of dose
switching for switchers in a randomized clinical trial. <em>BIMJ</em>,
<em>63</em>(7), 1464–1475. (<a
href="https://doi.org/10.1002/bimj.202000269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a clinical trial conducted by Janssen Pharmaceutica in which a flexible dosing regimen is compared to placebo, we evaluate how switchers in the treatment arm (i.e., patients who were switched to the higher dose) would have fared had they been kept on the low dose. This is done in order to understand whether flexible dosing is potentially beneficial for them. Simply comparing these patients&#39; responses with those of patients who stayed on the low dose does not likely entail a satisfactory evaluation because the latter patients are usually in a better health condition. Because the available information in the considered trial is too limited to enable a reliable adjustment, we will instead transport data from a fixed dosing trial that has been conducted concurrently on the same target, albeit not in an identical patient population. In particular, we propose an estimator that relies on an outcome model, a model for switching, and a propensity score model for the association between study and patient characteristics. The proposed estimator is asymptotically unbiased if either the outcome or the propensity score model is correctly specified, and efficient (under the semiparametric model where the randomization probabilities are known and independent of baseline covariates) when all models are correctly specified. The proposed method for transporting information from an external study is more broadly applicable in studies where a classical confounding adjustment is not possible due to near positivity violation (e.g., studies where switching takes place in a (near) deterministic manner). Monte Carlo simulations and application to the motivating study demonstrate adequate performance.},
  archive      = {J_BIMJ},
  author       = {Kelly Van Lancker and An Vandebosch and Stijn Vansteelandt},
  doi          = {10.1002/bimj.202000269},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1464-1475},
  shortjournal = {Bio. J.},
  title        = {Efficient, doubly robust estimation of the effect of dose switching for switchers in a randomized clinical trial},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximin design of cluster randomized trials with
heterogeneous costs and variances. <em>BIMJ</em>, <em>63</em>(7),
1444–1463. (<a href="https://doi.org/10.1002/bimj.202100019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials evaluate the effect of a treatment on persons nested within clusters, with clusters being randomly assigned to treatment. The optimal sample size at the cluster and person level depends on the study cost per cluster and per person, and the outcome variance at the cluster and the person level. The variances are unknown in the design stage and can differ between treatment arms. As a solution, this paper presents a Maximin design that maximizes the minimum relative efficiency (relative to the optimal design) over the variance parameter space, for trials with two treatment arms and a quantitative outcome. This maximin relative efficiency design (MMRED) is compared with a published Maximin design which maximizes the minimum efficiency (MMED). Both designs are also compared with the optimal designs for homogeneous costs and variances (balanced design) and heterogeneous costs and homogeneous variances (cost-conscious design), for a range of variances based upon three published trials. Whereas the MMED is balanced under high uncertainty about the treatment-to-control variance ratio, the MMRED then tends towards a balanced budget allocation between arms, leading to an unbalanced sample size allocation if costs are heterogeneous, similar to the cost-conscious design. Further, the MMRED corresponds to an optimal design for an intraclass correlation (ICC) in the lower half of the assumed ICC range (optimistic), whereas the MMED is the optimal design for the maximum ICC within the ICC range (pessimistic). Attention is given to the effect of the Welch–Satterthwaite degrees of freedom for treatment effect testing on the design efficiencies.},
  archive      = {J_BIMJ},
  author       = {Gerard J. P. van Breukelen and Math J. J. M. Candel},
  doi          = {10.1002/bimj.202100019},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1444-1463},
  shortjournal = {Bio. J.},
  title        = {Maximin design of cluster randomized trials with heterogeneous costs and variances},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assurance in vaccine efficacy clinical trial design based on
immunological responses. <em>BIMJ</em>, <em>63</em>(7), 1434–1443. (<a
href="https://doi.org/10.1002/bimj.202100015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assurance of a future clinical trial is a key quantitative tool for decision-making in drug development. It is derived from prior knowledge (Bayesian approach) about the clinical endpoint of interest, typically from previous clinical trials. In this paper, we examine assurance in the specific context of vaccine development, where early development (Phase 2) is often based on immunological endpoints (e.g., antibody levels), while the confirmatory trial (Phase 3) is based on the clinical endpoint (very large sample sizes and long follow-up). Our proposal is to use the Phase 2 vaccine efficacy predicted by the immunological endpoint (using a model estimated from epidemiological studies) as prior information for the calculation of the assurance.},
  archive      = {J_BIMJ},
  author       = {Andrea Callegaro and Toufik Zahaf and Fabian Tibaldi},
  doi          = {10.1002/bimj.202100015},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1434-1443},
  shortjournal = {Bio. J.},
  title        = {Assurance in vaccine efficacy clinical trial design based on immunological responses},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size calculation for two-arm trials with
time-to-event endpoint for nonproportional hazards using the concept of
relative time when inference is built on comparing weibull
distributions. <em>BIMJ</em>, <em>63</em>(7), 1406–1433. (<a
href="https://doi.org/10.1002/bimj.202000043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size calculations for two-arm clinical trials with a time-to-event endpoint have traditionally used the assumption of proportional hazards (PH) or the assumption of exponentially distributed survival times. Available software provides methods for sample size calculation using a nonparametric logrank test, Schoenfeld&#39;s formula for Cox PH model, or parametric calculations specific to the exponential distribution. In cases where the PH assumption is not valid, the first-choice method is to compute sample size assuming a piecewise linear survival curve (Lakatos approach) for both the control and treatment arms with judiciously chosen cut-points. Recent advances in literature have used the assumption of Weibull distributed times for single-arm trials, and, newer methods have emerged that allow sample size calculations for two-arm trials using the assumption of proportional time (PT) while considering non-PH. These methods, however, always assume an instantaneous effect of treatment relative to control requiring that the effect size be defined by a single number whose magnitude is preserved throughout the trial duration. Here, we consider the scenarios where the hypothesized benefit of treatment relative to control may not be constant giving rise to the notion of Relative Time (RT). By assuming that survival times for control and treatment arm come from two different Weibull distributions with different location and shape parameters, we develop the methodology for sample size calculation for specific cases of both non-PH and non-PT. Simulations are conducted to assess the operation characteristics of the proposed method and a practical example is discussed.},
  archive      = {J_BIMJ},
  author       = {Milind A. Phadnis and Matthew S. Mayo},
  doi          = {10.1002/bimj.202000043},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1406-1433},
  shortjournal = {Bio. J.},
  title        = {Sample size calculation for two-arm trials with time-to-event endpoint for nonproportional hazards using the concept of relative time when inference is built on comparing weibull distributions},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymptotic-based bootstrap approach for matched pairs with
missingness in a single arm. <em>BIMJ</em>, <em>63</em>(7), 1389–1405.
(<a href="https://doi.org/10.1002/bimj.202000051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of missing values is an arising difficulty when dealing with paired data. Several test procedures are developed in the literature to tackle this problem. Some of them are even robust under deviations and control type-I error quite accurately. However, most of these methods are not applicable when missing values are present only in a single arm. For this case, we provide asymptotic correct resampling tests that are robust under heteroskedasticity and skewed distributions. The tests are based on a meaningful restructuring of all observed information in quadratic form–type test statistics. An extensive simulation study is conducted exemplifying the tests for finite sample sizes under different missingness mechanisms. In addition, illustrative data examples based on real life studies are analyzed.},
  archive      = {J_BIMJ},
  author       = {Lubna Amro and Markus Pauly and Burim Ramosaj},
  doi          = {10.1002/bimj.202000051},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1389-1405},
  shortjournal = {Bio. J.},
  title        = {Asymptotic-based bootstrap approach for matched pairs with missingness in a single arm},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clinical risk prediction models and informative cluster
size: Assessing the performance of a suicide risk prediction algorithm.
<em>BIMJ</em>, <em>63</em>(7), 1375–1388. (<a
href="https://doi.org/10.1002/bimj.202000199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical visit data are clustered within people, which complicates prediction modeling. Cluster size is often informative because people receiving more care are less healthy and at higher risk of poor outcomes. We used data from seven health systems on 1,518,968 outpatient mental health visits from January 1, 2012 to June 30, 2015 to predict suicide attempt within 90 days. We evaluated true performance of prediction models using a prospective validation set of 4,286,495 visits from October 1, 2015 to September 30, 2017. We examined dividing clustered data on the person or visit level for model training and cross-validation and considered a within cluster resampling approach for model estimation. We evaluated optimism by comparing estimated performance from a left-out testing dataset to performance in the prospective dataset. We used two prediction methods, logistic regression with least absolute shrinkage and selection operator (LASSO) and random forest. The random forest model using a visit-level split for model training and testing was optimistic; it overestimated discrimination (area under the curve, AUC = 0.95 in testing versus 0.84 in prospective validation) and classification accuracy (sensitivity = 0.48 in testing versus 0.19 in prospective validation, 95th percentile cut-off). Logistic regression and random forest models using a person-level split performed well, accurately estimating prospective discrimination and classification: estimated AUCs ranged from 0.85 to 0.87 in testing versus 0.85 in prospective validation, and sensitivity ranged from 0.15 to 0.20 in testing versus 0.17 to 0.19 in prospective validation. Within cluster resampling did not improve performance. We recommend dividing clustered data on the person level, rather than visit level, to ensure strong performance in prospective use and accurate estimation of future performance at the time of model development.},
  archive      = {J_BIMJ},
  author       = {Rebecca Yates Coley and Rod L. Walker and Maricela Cruz and Gregory E. Simon and Susan M. Shortreed},
  doi          = {10.1002/bimj.202000199},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1375-1388},
  shortjournal = {Bio. J.},
  title        = {Clinical risk prediction models and informative cluster size: Assessing the performance of a suicide risk prediction algorithm},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing mediating effects of high-dimensional microbiome
measurements in dietary intervention studies. <em>BIMJ</em>,
<em>63</em>(7), 1366–1374. (<a
href="https://doi.org/10.1002/bimj.201900373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Habitual diet can influence health-related outcomes directly, but such effects may also be modulated indirectly by gut microbiota. We consider randomized trials and the question to what extent the effect of diet on an outcome of interest is mediated through the gut microbiome or whether there is a diet–microbiome interaction identifying subgroups of individuals who are more susceptible to specific dietary effects. The baseline microbiome by itself may be a modifier of the effects of diet on health. Yet, the high dimensionality of microbiome data requires innovative statistical approaches to identify potential mediating or moderating effects. To motivate our proposal for an appropriate analysis workflow, we consider a randomized trial that investigates the effect of a 4-week vegan diet on the diversity of gut microbiota and branched-chain amino acid metabolism in healthy omnivorous volunteers. To address the challenge of compositional microbiome data, we consider an adaptation of the lasso for penalized estimation of multivariable regression models with a large number of microbiotic taxa. This is plugged into a classical regression mediation effect analysis strategy. The interaction effects are obtained via an approach that can directly estimate them without having to deal with main effects. As a result we obtain signatures comprised of microbiotic taxa with potential mediating and moderating effects. Some taxa no longer show up as mediating, when taking moderating effects into account. Thus, the proposed analysis strategy allows to identify specific mediating effects, while avoiding potential erroneous conclusions, where moderating effects might have believed to be mediating effects.},
  archive      = {J_BIMJ},
  author       = {Nadine Binder and Ann-Kathrin Lederer and Karin B. Michels and Harald Binder},
  doi          = {10.1002/bimj.201900373},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1366-1374},
  shortjournal = {Bio. J.},
  title        = {Assessing mediating effects of high-dimensional microbiome measurements in dietary intervention studies},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Globaltest confidence regions and their application to ridge
regression. <em>BIMJ</em>, <em>63</em>(7), 1351–1365. (<a
href="https://doi.org/10.1002/bimj.202000063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct confidence regions in high dimensions by inverting the globaltest statistics, and use them to choose the tuning parameter for penalized regression. The selected model corresponds to the point in the confidence region of the parameters that minimizes the penalty, making it the least complex model that still has acceptable fit according to the test that defines the confidence region. As the globaltest is particularly powerful in the presence of many weak predictors, it connects well to ridge regression, and we thus focus on ridge penalties in this paper. The confidence region method is quick to calculate, intuitive, and gives decent predictive potential. As a tuning parameter selection method it may even outperform classical methods such as cross-validation in terms of mean squared error of prediction, especially when the signal is weak. We illustrate the method for linear models in simulation study and for Cox models in real gene expression data of breast cancer samples.},
  archive      = {J_BIMJ},
  author       = {Ningning Xu and Aldo Solari and Jelle Goeman},
  doi          = {10.1002/bimj.202000063},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1351-1365},
  shortjournal = {Bio. J.},
  title        = {Globaltest confidence regions and their application to ridge regression},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). Contents: Biometrical journal 7’21. <em>BIMJ</em>,
<em>63</em>(7), 1349–1350. (<a
href="https://doi.org/10.1002/bimj.202170075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170075},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {1349-1350},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 7&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). Cover picture: Biometrical journal 6’21. <em>BIMJ</em>,
<em>63</em>(6), NA. (<a
href="https://doi.org/10.1002/bimj.202170061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170061},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 6&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Growth dynamics and heritability for plant high-throughput
phenotyping studies using hierarchical functional data analysis.
<em>BIMJ</em>, <em>63</em>(6), 1325–1341. (<a
href="https://doi.org/10.1002/bimj.202000315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern high-throughput plant phenotyping, images of plants of different genotypes are repeatedly taken throughout the growing season, and phenotypic traits of plants (e.g., plant height) are extracted through image processing. It is of interest to recover whole trait trajectories and their derivatives at both genotype and plant levels based on observations made at irregular discrete time points. We propose to model trait trajectories using hierarchical functional principal component analysis (HFPCA) and show that the problem of recovering derivatives of the trajectories is reduced to estimating derivatives of eigenfunctions, which is solved by differentiating eigenequations. Based on HFPCA, we also propose a new measure for the broad-sense heritability by allowing it to vary over time during plant growth. Simulation studies show that the proposed procedure performs better than its competitors in terms of recovering both trait trajectories and their derivatives. Interesting characteristics of plant growth and heritability dynamics are revealed in the application to a modern plant phenotyping study.},
  archive      = {J_BIMJ},
  author       = {Yuhang Xu and Yehua Li and Yumou Qiu},
  doi          = {10.1002/bimj.202000315},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1325-1341},
  shortjournal = {Bio. J.},
  title        = {Growth dynamics and heritability for plant high-throughput phenotyping studies using hierarchical functional data analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new robust bayesian small area estimation via -stable
model for estimating the proportion of athletic students in california.
<em>BIMJ</em>, <em>63</em>(6), 1309–1324. (<a
href="https://doi.org/10.1002/bimj.202000235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, diabetes mellitus and obesity revealed to be one of the fastest-growing chronic diseases in youth in the United States. The number of new diabetes cases is dramatically increasing, and, for the moment, effective therapy does not exist. Experts believe that one of the causes of this increase is the decline in exercise behavior. The California Education Code requires local educational agencies (LEAs) to administer the FITNESSGRAM, the Physical Fitness Test (PFT), to Californian students of public schools. This test evaluates six fitness areas, and experts defined that a passing result on all six areas of the test represents a fitness level that offers some protection against the diseases associated with physical inactivity. We consider 2015–2016 data provided by the California Department of Education (CDE): for each Californian county ( ), we aim at estimating the county-level proportion of students with a score equal to six. To account for the heterogeneity of the phenomenon and the presence of outlying counties, we extend the standard area-level model by specifying the random effects as a symmetric -stable (S S) distribution that can accommodate different types of outlying observations. The model can accurately estimate the county-level proportion of students with a score equal to six. Results highlight some interesting relationships with social and economic situations in each county. The performance of the proposed model is also investigated through an extensive simulation study.},
  archive      = {J_BIMJ},
  author       = {Shaho Zarei and Serena Arima and Giovanna Jona Lasinio},
  doi          = {10.1002/bimj.202000235},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1309-1324},
  shortjournal = {Bio. J.},
  title        = {A new robust bayesian small area estimation via -stable model for estimating the proportion of athletic students in california},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantile modeling through multivariate
log-normal/independent linear regression models with application to
newborn data. <em>BIMJ</em>, <em>63</em>(6), 1290–1308. (<a
href="https://doi.org/10.1002/bimj.202000200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose and study the class of multivariate log-normal/independent distributions and linear regression models based on this class. The class of multivariate log-normal/independent distributions is very attractive for robust statistical modeling because it includes several heavy-tailed distributions suitable for modeling correlated multivariate positive data that are skewed and possibly heavy-tailed. Besides, expectation-maximization (EM)-type algorithms can be easily implemented for maximum likelihood estimation. We model the relationship between quantiles of the response variables and a set of explanatory variables, compute the maximum likelihood estimates of parameters through EM-type algorithms, and evaluate the model fitting based on Mahalanobis-type distances. The satisfactory performance of the quantile estimation is verified by simulation studies. An application to newborn data is presented and discussed.},
  archive      = {J_BIMJ},
  author       = {Raúl Alejandro Morán-Vásquez and Mauricio A. Mazo-Lopera and Silvia L. P. Ferrari},
  doi          = {10.1002/bimj.202000200},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1290-1308},
  shortjournal = {Bio. J.},
  title        = {Quantile modeling through multivariate log-normal/independent linear regression models with application to newborn data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and computation of multistep batch testing for
infectious diseases. <em>BIMJ</em>, <em>63</em>(6), 1272–1289. (<a
href="https://doi.org/10.1002/bimj.202000240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mathematical model based on probability theory to optimize COVID-19 testing by a multistep batch testing approach with variable batch sizes. This model and simulation tool dramatically increase the efficiency and efficacy of the tests in a large population at a low cost, particularly when the infection rate is low. The proposed method combines statistical modeling with numerical methods to solve nonlinear equations and obtain optimal batch sizes at each step of tests, with the flexibility to incorporate geographic and demographic information. In theory, this method substantially improves the false positive rate and positive predictive value as well. We also conducted a Monte Carlo simulation to verify this theory. Our simulation results show that our method significantly reduces the false negative rate. More accurate assessment can be made if the dilution effect or other practical factors are taken into consideration. The proposed method will be particularly useful for the early detection of infectious diseases and prevention of future pandemics. The proposed work will have broader impacts on medical testing for contagious diseases in general.},
  archive      = {J_BIMJ},
  author       = {Hongshik Ahn and Haoran Jiang and Xiaolin Li},
  doi          = {10.1002/bimj.202000240},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1272-1289},
  shortjournal = {Bio. J.},
  title        = {Modeling and computation of multistep batch testing for infectious diseases},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SvReg: Structural varying-coefficient regression to
differentiate how regional brain atrophy affects motor impairment for
huntington disease severity groups. <em>BIMJ</em>, <em>63</em>(6),
1254–1271. (<a href="https://doi.org/10.1002/bimj.202000312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Huntington disease, identification of brain regions related to motor impairment can be useful for developing interventions to alleviate the motor symptom, the major symptom of the disease. However, the effects from the brain regions to motor impairment may vary for different groups of patients. Hence, our interest is not only to identify the brain regions but also to understand how their effects on motor impairment differ by patient groups. This can be cast as a model selection problem for a varying-coefficient regression. However, this is challenging when there is a pre-specified group structure among variables. We propose a novel variable selection method for a varying-coefficient regression with such structured variables and provide a publicly available R package svreg for implementation of our method. Our method is empirically shown to select relevant variables consistently. Also, our method screens irrelevant variables better than existing methods. Hence, our method leads to a model with higher sensitivity, lower false discovery rate and higher prediction accuracy than the existing methods. Finally, we found that the effects from the brain regions to motor impairment differ by disease severity of the patients. To the best of our knowledge, our study is the first to identify such interaction effects between the disease severity and brain regions, which indicates the need for customized intervention by disease severity.},
  archive      = {J_BIMJ},
  author       = {Rakheon Kim and Samuel Müller and Tanya P. Garcia},
  doi          = {10.1002/bimj.202000312},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1254-1271},
  shortjournal = {Bio. J.},
  title        = {SvReg: Structural varying-coefficient regression to differentiate how regional brain atrophy affects motor impairment for huntington disease severity groups},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inference for the difference between two
maximized youden indices obtained from correlated biomarkers.
<em>BIMJ</em>, <em>63</em>(6), 1241–1253. (<a
href="https://doi.org/10.1002/bimj.202000128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is global interest in deriving new promising cancer biomarkers that could complement or substitute the conventional ones. Clinical decisions can often be based on the cutoff that corresponds to the maximized Youden index when maximum accuracy drives decisions. When more than one classification criteria are measured within the same individuals, correlated measurements arise. In this work, we propose hypothesis tests and confidence intervals for the comparison of two correlated receiver operating characteristic (ROC) curves in terms of their corresponding maximized Youden indices. We explore delta-based techniques under parametric assumptions, or power transformations. Nonparametric kernel-based methods are also examined. We evaluate our approaches through simulations and illustrate them using data from a metabolomic study referring to the detection of pancreatic cancer.},
  archive      = {J_BIMJ},
  author       = {Leonidas E. Bantis and Christos T. Nakas and Benjamin Reiser},
  doi          = {10.1002/bimj.202000128},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1241-1253},
  shortjournal = {Bio. J.},
  title        = {Statistical inference for the difference between two maximized youden indices obtained from correlated biomarkers},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining biomarkers by maximizing the true positive rate
for a fixed false positive rate. <em>BIMJ</em>, <em>63</em>(6),
1223–1240. (<a href="https://doi.org/10.1002/bimj.202000210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomarkers abound in many areas of clinical research, and often investigators are interested in combining them for diagnosis, prognosis, or screening. In many applications, the true positive rate (TPR) for a biomarker combination at a prespecified, clinically acceptable false positive rate (FPR) is the most relevant measure of predictive capacity. We propose a distribution-free method for constructing biomarker combinations by maximizing the TPR while constraining the FPR. Theoretical results demonstrate desirable properties of biomarker combinations produced by the new method. In simulations, the biomarker combination provided by our method demonstrated improved operating characteristics in a variety of scenarios when compared with alternative methods for constructing biomarker combinations. Thus, use of our method could lead to the development of better biomarker combinations, increasing the likelihood of clinical adoption.},
  archive      = {J_BIMJ},
  author       = {Allison Meisner and Marco Carone and Margaret S. Pepe and Kathleen F. Kerr},
  doi          = {10.1002/bimj.202000210},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1223-1240},
  shortjournal = {Bio. J.},
  title        = {Combining biomarkers by maximizing the true positive rate for a fixed false positive rate},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semiparametric method for evaluating causal effects in the
presence of error-prone covariates. <em>BIMJ</em>, <em>63</em>(6),
1202–1222. (<a href="https://doi.org/10.1002/bimj.202000069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of most empirical studies in social sciences and medical research is to determine whether an alteration in an intervention or a treatment will cause a change in the desired outcome response. Unlike randomized designs, establishing the causal relationship based on observational studies is a challenging problem because the ceteris paribus condition is violated. When the covariates of interest are measured with errors, evaluating the causal effects becomes a thorny issue. We propose a semiparametric method to establish the causal relationship, which yields a consistent estimator of the average causal effect. The method we proposed results in locally efficient estimators of the covariate effects. We study their theoretical properties and demonstrate their finite sample performance on simulated data. We further apply the proposed method to the Stroke Recovery in Underserved Populations (SRUP) study by the National Institute on Aging.},
  archive      = {J_BIMJ},
  author       = {Jianxuan Liu and Wei Li},
  doi          = {10.1002/bimj.202000069},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1202-1222},
  shortjournal = {Bio. J.},
  title        = {A semiparametric method for evaluating causal effects in the presence of error-prone covariates},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal dose-finding for efficacy–safety models.
<em>BIMJ</em>, <em>63</em>(6), 1185–1201. (<a
href="https://doi.org/10.1002/bimj.202000181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dose-finding is an important part of the clinical development of a new drug. The purpose of dose-finding studies is to determine a suitable dose for future development based on both efficacy and safety. Optimal experimental designs have already been used to determine the design of this kind of studies, however, often that design is focused on efficacy only. We consider an efficacy–safety model, which is a simplified version of the bivariate Emax model. We use here the clinical utility index concept, which provides the desirable balance between efficacy and safety. By maximizing the utility of the patients, we get the estimated dose. This desire leads us to locally -optimal designs. An algebraic solution for -optimal designs is determined for arbitrary vectors using a multivariate version of Elfving&#39;s method. The solution shows that the expected therapeutic index of the drug is a key quantity determining both the number of doses, the doses itself, and their weights in the optimal design. A sequential design is proposed to solve the complication of parameter dependency, and it is illustrated in a simulation study.},
  archive      = {J_BIMJ},
  author       = {Renata Eirini Tsirpitzi and Frank Miller},
  doi          = {10.1002/bimj.202000181},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1185-1201},
  shortjournal = {Bio. J.},
  title        = {Optimal dose-finding for efficacy–safety models},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Categories, components, and techniques in a modular
construction of basket trials for application and further research.
<em>BIMJ</em>, <em>63</em>(6), 1159–1184. (<a
href="https://doi.org/10.1002/bimj.202000314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basket trials have become a virulent topic in medical and statistical research during the last decade. The core idea of them is to treat patients, who express the same genetic predisposition—either personally or their disease—with the same treatment irrespective of the location of the disease. The location of the disease defines each basket and the pathway of the treatment uses the common genetic predisposition among the baskets. This opens the opportunity to share information among baskets, which can consequently increase the information of the basket-wise response with respect to the investigated treatment. This further allows dynamic decisions regarding futility and efficacy of individual baskets during the ongoing trial. Several statistical designs have been proposed on how a basket trial can be conducted and this has left an unclear situation with many options. The different designs propose different mathematical and statistical techniques, different decision rules, and also different trial purposes. This paper presents a broad overview of existing designs, categorizes them, and elaborates their similarities and differences. A uniform and consistent notation facilitates the first contact, introduction, and understanding of the statistical methodologies and techniques used in basket trials. Finally, this paper presents a modular approach for the construction of basket trials in applied medical science and forms a base for further research of basket trial designs and their techniques.},
  archive      = {J_BIMJ},
  author       = {Moritz Pohl and Johannes Krisam and Meinhard Kieser},
  doi          = {10.1002/bimj.202000314},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1159-1184},
  shortjournal = {Bio. J.},
  title        = {Categories, components, and techniques in a modular construction of basket trials for application and further research},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). Contents: Biometrical journal 6’21. <em>BIMJ</em>,
<em>63</em>(6), 1157–1158. (<a
href="https://doi.org/10.1002/bimj.202170064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170064},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {1157-1158},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 6&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). Cover picture: Biometrical journal 5’21. <em>BIMJ</em>,
<em>63</em>(5), NA. (<a
href="https://doi.org/10.1002/bimj.202170051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170051},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 5&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). List of reviewers for 2020. <em>BIMJ</em>, <em>63</em>(5),
1146–1149. (<a href="https://doi.org/10.1002/bimj.202170055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170055},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1146-1149},
  shortjournal = {Bio. J.},
  title        = {List of reviewers for 2020},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction. <em>BIMJ</em>, <em>63</em>(5), 1144–1145. (<a
href="https://doi.org/10.1002/bimj.202170054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170054},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1144-1145},
  shortjournal = {Bio. J.},
  title        = {Correction},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bounds for the weight of external data in shrinkage
estimation. <em>BIMJ</em>, <em>63</em>(5), 1131–1143. (<a
href="https://doi.org/10.1002/bimj.202000227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shrinkage estimation in a meta-analysis framework may be used to facilitate dynamical borrowing of information. This framework might be used to analyze a new study in the light of previous data, which might differ in their design (e.g., a randomized controlled trial and a clinical registry). We show how the common study weights arise in effect and shrinkage estimation, and how these may be generalized to the case of Bayesian meta-analysis. Next we develop simple ways to compute bounds on the weights, so that the contribution of the external evidence may be assessed a priori. These considerations are illustrated and discussed using numerical examples, including applications in the treatment of Creutzfeldt–Jakob disease and in fetal monitoring to prevent the occurrence of metabolic acidosis. The target study&#39;s contribution to the resulting estimate is shown to be bounded below. Therefore, concerns of evidence being easily overwhelmed by external data are largely unwarranted.},
  archive      = {J_BIMJ},
  author       = {Christian Röver and Tim Friede},
  doi          = {10.1002/bimj.202000227},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1131-1143},
  shortjournal = {Bio. J.},
  title        = {Bounds for the weight of external data in shrinkage estimation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate error control in high-dimensional association
testing using conditional false discovery rates. <em>BIMJ</em>,
<em>63</em>(5), 1096–1130. (<a
href="https://doi.org/10.1002/bimj.201900254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional hypothesis testing is ubiquitous in the biomedical sciences, and informative covariates may be employed to improve power. The conditional false discovery rate (cFDR) is a widely used approach suited to the setting where the covariate is a set of p-values for the equivalent hypotheses for a second trait. Although related to the Benjamini–Hochberg procedure, it does not permit any easy control of type-1 error rate and existing methods are over-conservative. We propose a new method for type-1 error rate control based on identifying mappings from the unit square to the unit interval defined by the estimated cFDR and splitting observations so that each map is independent of the observations it is used to test. We also propose an adjustment to the existing cFDR estimator which further improves power. We show by simulation that the new method more than doubles potential improvement in power over unconditional analyses compared to existing methods. We demonstrate our method on transcriptome-wide association studies and show that the method can be used in an iterative way, enabling the use of multiple covariates successively. Our methods substantially improve the power and applicability of cFDR analysis.},
  archive      = {J_BIMJ},
  author       = {James Liley and Chris Wallace},
  doi          = {10.1002/bimj.201900254},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1096-1130},
  shortjournal = {Bio. J.},
  title        = {Accurate error control in high-dimensional association testing using conditional false discovery rates},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple confidence interval and region formulas for comparing
diagnostic likelihood ratios under a paired design. <em>BIMJ</em>,
<em>63</em>(5), 1086–1095. (<a
href="https://doi.org/10.1002/bimj.202000146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A population-based paired design is often used for comparing the diagnostic likelihood ratios of two binary diagnostic tests. However, a case-control paired design, which involves the application of both diagnostic tests to two independent samples, is a good alternative study design especially when the disease is rare. Existing methods for comparing two diagnostic likelihood ratios have been mainly focused on the population-based paired design with little attention paid to the case-control paired design. In this paper, we derive a confidence interval formula for the relative diagnostic likelihood ratio (the ratio of two diagnostic likelihood ratios), which can be used for the comparison of two positive or negative diagnostic likelihood ratios separately. We also derive a confidence region formula for the two relative positive and negative diagnostic likelihood ratios, which allows simultaneous comparison of two positive and negative diagnostic likelihood ratios. The proposed confidence interval and region formulas are simple to compute and can be used for both population-based paired design and case-control paired designs. Simulation studies are used to assess the finite sample performance of the confidence interval and region formulas. The proposed methods are applied to a real data set on coronary artery disease and two diagnostic tests.},
  archive      = {J_BIMJ},
  author       = {Yougui Wu},
  doi          = {10.1002/bimj.202000146},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1086-1095},
  shortjournal = {Bio. J.},
  title        = {Simple confidence interval and region formulas for comparing diagnostic likelihood ratios under a paired design},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). D-optimal designs of mean-covariance models for longitudinal
data. <em>BIMJ</em>, <em>63</em>(5), 1072–1085. (<a
href="https://doi.org/10.1002/bimj.202000129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal data analysis has been very common in various fields. It is important in longitudinal studies to choose appropriate numbers of subjects and repeated measurements and allocation of time points as well. Therefore, existing studies proposed many criteria to select the optimal designs. However, most of them focused on the precision of the mean estimation based on some specific models and certain structures of the covariance matrix. In this paper, we focus on both the mean and the marginal covariance matrix. Based on the mean–covariance models, it is shown that the trick of symmetrization can generate better designs under a Bayesian D-optimality criterion over a given prior parameter space. Then, we propose a novel criterion to select the optimal designs. The goal of the proposed criterion is to make the estimates of both the mean vector and the covariance matrix more accurate, and the total cost is as low as possible. Further, we develop an algorithm to solve the corresponding optimization problem. Based on the algorithm, the criterion is illustrated by an application to a real dataset and some simulation studies. We show the superiority of the symmetric optimal design and the symmetrized optimal design in terms of the relative efficiency and parameter estimation. Moreover, we also demonstrate that the proposed criterion is more effective than the previous criteria, and it is suitable for both maximum likelihood estimation and restricted maximum likelihood estimation procedures.},
  archive      = {J_BIMJ},
  author       = {Siyu Yi and Yongdao Zhou and Jianxin Pan},
  doi          = {10.1002/bimj.202000129},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1072-1085},
  shortjournal = {Bio. J.},
  title        = {D-optimal designs of mean-covariance models for longitudinal data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size and power considerations for cluster randomized
trials with count outcomes subject to right truncation. <em>BIMJ</em>,
<em>63</em>(5), 1052–1071. (<a
href="https://doi.org/10.1002/bimj.202000230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) are widely used in epidemiological and public health studies assessing population-level effect of group-based interventions. One important application of CRTs is the control of vector-borne disease, such as malaria. However, a particular challenge for designing these trials is that the primary outcome involves counts of episodes that are subject to right truncation. While sample size formulas have been developed for CRTs with clustered counts, they are not directly applicable when the counts are right truncated. To address this limitation, we discuss two marginal modeling approaches for the analysis of CRTs with truncated counts and develop two corresponding closed-form sample size formulas to facilitate the design of such trials. The proposed sample size formulas allow investigators to explore the power under a large number of scenarios without computationally intensive simulations. The proposed formulas are validated in extensive simulations. We further explore the implication of right truncation on power and apply the proposed formulas to illustrate the power calculation for a malaria control CRT where the primary outcome is subject to right truncation.},
  archive      = {J_BIMJ},
  author       = {Fan Li and Guangyu Tong},
  doi          = {10.1002/bimj.202000230},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1052-1071},
  shortjournal = {Bio. J.},
  title        = {Sample size and power considerations for cluster randomized trials with count outcomes subject to right truncation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized expectile regression with flexible response
function. <em>BIMJ</em>, <em>63</em>(5), 1028–1051. (<a
href="https://doi.org/10.1002/bimj.202000203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectile regression, in contrast to classical linear regression, allows for heteroscedasticity and omits a parametric specification of the underlying distribution. This model class can be seen as a quantile-like generalization of least squares regression. Similarly as in quantile regression, the whole distribution can be modeled with expectiles, while still offering the same flexibility in the use of semiparametric predictors as modern mean regression. However, even with no parametric assumption for the distribution of the response in expectile regression, the model is still constructed with a linear relationship between the fitted value and the predictor. If the true underlying relationship is nonlinear then severe biases can be observed in the parameter estimates as well as in quantities derived from them such as model predictions. We observed this problem during the analysis of the distribution of a self-reported hearing score with limited range. Classical expectile regression should in theory adhere to these constraints, however, we observed predictions that exceeded the maximum score. We propose to include a response function between the fitted value and the predictor similarly as in generalized linear models. However, including a fixed response function would imply an assumption on the shape of the underlying distribution function. Such assumptions would be counterintuitive in expectile regression. Therefore, we propose to estimate the response function jointly with the covariate effects. We design the response function as a monotonically increasing P-spline, which may also contain constraints on the target set. This results in valid estimates for a self-reported listening effort score through nonlinear estimates of the response function. We observed strong associations with the speech reception threshold.},
  archive      = {J_BIMJ},
  author       = {Elmar Spiegel and Thomas Kneib and Petra von Gablenz and Fabian Otto-Sobotka},
  doi          = {10.1002/bimj.202000203},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1028-1051},
  shortjournal = {Bio. J.},
  title        = {Generalized expectile regression with flexible response function},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved generalized raking estimators to address dependent
covariate and failure-time outcome error. <em>BIMJ</em>, <em>63</em>(5),
1006–1027. (<a href="https://doi.org/10.1002/bimj.202000187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical studies that use electronic health records (EHR) data for inference are often subject to bias due to measurement error. The measurement error present in EHR data is typically complex, consisting of errors of unknown functional form in covariates and the outcome, which can be dependent. To address the bias resulting from such errors, generalized raking has recently been proposed as a robust method that yields consistent estimates without the need to model the error structure. We provide rationale for why these previously proposed raking estimators can be expected to be inefficient in failure-time outcome settings involving misclassification of the event indicator. We propose raking estimators that utilize multiple imputation, to impute either the target variables or auxiliary variables, to improve the efficiency. We also consider outcome-dependent sampling designs and investigate their impact on the efficiency of the raking estimators, either with or without multiple imputation. We present an extensive numerical study to examine the performance of the proposed estimators across various measurement error settings. We then apply the proposed methods to our motivating setting, in which we seek to analyze HIV outcomes in an observational cohort with EHR data from the Vanderbilt Comprehensive Care Clinic.},
  archive      = {J_BIMJ},
  author       = {Eric J. Oh and Bryan E. Shepherd and Thomas Lumley and Pamela A. Shaw},
  doi          = {10.1002/bimj.202000187},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {1006-1027},
  shortjournal = {Bio. J.},
  title        = {Improved generalized raking estimators to address dependent covariate and failure-time outcome error},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A product-limit estimator of the conditional survival
function when cure status is partially known. <em>BIMJ</em>,
<em>63</em>(5), 984–1005. (<a
href="https://doi.org/10.1002/bimj.202000173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a nonparametric estimator of the conditional survival function in the mixture cure model for right-censored data when cure status is partially known. The estimator is developed for the setting of a single continuous covariate but it can be extended to multiple covariates. It extends the estimator of Beran, which ignores cure status information. We obtain an almost sure representation, from which the strong consistency and asymptotic normality of the estimator are derived. Asymptotic expressions of the bias and variance demonstrate a reduction in the variance with respect to Beran&#39;s estimator. A simulation study shows that, if the bandwidth parameter is suitably chosen, our estimator performs better than others for an ample range of covariate values. A bootstrap bandwidth selector is proposed. Finally, the proposed estimator is applied to a real dataset studying survival of sarcoma patients.},
  archive      = {J_BIMJ},
  author       = {Wende Clarence Safari and Ignacio López-de-Ullibarri and María Amalia Jácome},
  doi          = {10.1002/bimj.202000173},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {984-1005},
  shortjournal = {Bio. J.},
  title        = {A product-limit estimator of the conditional survival function when cure status is partially known},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cox regression analysis for distorted covariates with an
unknown distortion function. <em>BIMJ</em>, <em>63</em>(5), 968–983. (<a
href="https://doi.org/10.1002/bimj.202000209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study inference for censored survival data where some covariates are distorted by some unknown functions of an observable confounding variable in a multiplicative form. An example of this kind of data in medical studies is normalizing some important observed exposure variables by patients&#39; body mass index , weight, or age. Such a phenomenon also appears frequently in environmental studies where an ambient measure is used for normalization and in genomic studies where the library size needs to be normalized for the next generation sequencing of data. We propose a new covariate-adjusted Cox proportional hazards regression model and utilize the kernel smoothing method to estimate the distorting function, then employ an estimated maximum likelihood method to derive the estimator for the regression parameters. We establish the large sample properties of the proposed estimator. Extensive simulation studies demonstrate that the proposed estimator performs well in correcting the bias arising from distortion. A real dataset from the National Wilms&#39; Tumor Study is used to illustrate the proposed approach.},
  archive      = {J_BIMJ},
  author       = {Yanyan Liu and Yuanshan Wu and Jing Zhang and Haibo Zhou},
  doi          = {10.1002/bimj.202000209},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {968-983},
  shortjournal = {Bio. J.},
  title        = {Cox regression analysis for distorted covariates with an unknown distortion function},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Functional modeling of recurrent events on time-to-event
processes. <em>BIMJ</em>, <em>63</em>(5), 948–967. (<a
href="https://doi.org/10.1002/bimj.202000374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical practice, it is often the case where the association between the occurrence of events and time-to-event outcomes is of interest; thus, it can be modeled within the framework of recurrent events. The purpose of our study is to enrich the information available for modeling survival with relevant dynamic features, properly taking into account their possibly time-varying nature, as well as to provide a new setting for quantifying the association between time-varying processes and time-to-event outcomes. We propose an innovative methodology to model information carried out by time-varying processes by means of functional data, modeling each time-varying variable as the compensator of marked point process the recurrent events are supposed to derive from. By means of Functional Principal Component Analysis, a suitable dimensional reduction of these objects is carried out in order to plug them into a Cox-type functional regression model for overall survival. We applied our methodology to data retrieved from the administrative databases of Lombardy Region (Italy), related to patients hospitalized for Heart Failure (HF) between 2000 and 2012. We focused on time-varying processes of HF hospitalizations and multiple drugs consumption and we studied how they influence patients&#39; overall survival. This novel way to account for time-varying variables allowed to model self-exciting behaviors, for which the occurrence of events in the past increases the probability of a new event, and to quantify the effect of personal behaviors and therapeutic patterns on survival, giving new insights into the direction of personalized treatment.},
  archive      = {J_BIMJ},
  author       = {Marta Spreafico and Francesca Ieva},
  doi          = {10.1002/bimj.202000374},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {948-967},
  shortjournal = {Bio. J.},
  title        = {Functional modeling of recurrent events on time-to-event processes},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Missing data: A statistical framework for practice.
<em>BIMJ</em>, <em>63</em>(5), 915–947. (<a
href="https://doi.org/10.1002/bimj.202000196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are ubiquitous in medical research, yet there is still uncertainty over when restricting to the complete records is likely to be acceptable, when more complex methods (e.g. maximum likelihood, multiple imputation and Bayesian methods) should be used, how they relate to each other and the role of sensitivity analysis. This article seeks to address both applied practitioners and researchers interested in a more formal explanation of some of the results. For practitioners, the framework, illustrative examples and code should equip them with a practical approach to address the issues raised by missing data (particularly using multiple imputation), alongside an overview of how the various approaches in the literature relate. In particular, we describe how multiple imputation can be readily used for sensitivity analyses, which are still infrequently performed. For those interested in more formal derivations, we give outline arguments for key results, use simple examples to show how methods relate, and references for full details. The ideas are illustrated with a cohort study, a multi-centre case control study and a randomised clinical trial.},
  archive      = {J_BIMJ},
  author       = {James R. Carpenter and Melanie Smuk},
  doi          = {10.1002/bimj.202000196},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {915-947},
  shortjournal = {Bio. J.},
  title        = {Missing data: A statistical framework for practice},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). Contents: Biometrical journal 5’21. <em>BIMJ</em>,
<em>63</em>(5), 913–914. (<a
href="https://doi.org/10.1002/bimj.202170056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170056},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {913-914},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 5&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Cover picture: Biometrical journal 4’21. <em>BIMJ</em>,
<em>63</em>(4), NA. (<a
href="https://doi.org/10.1002/bimj.202170041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170041},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 4&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correcting the bias of the net benefit estimator due to
right-censored observations. <em>BIMJ</em>, <em>63</em>(4), 893–906. (<a
href="https://doi.org/10.1002/bimj.202000001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized pairwise comparisons (GPCs) are a statistical method used in randomized clinical trials to simultaneously analyze several prioritized outcomes. This procedure estimates the net benefit (Δ). Δ may be interpreted as the probability for a random patient in the treatment group to have a better overall outcome than a random patient in the control group, minus the probability of the opposite situation. However, the presence of right censoring introduces uninformative pairs that will typically bias the estimate of Δ toward 0. We propose a correction to GPCs that estimates the contribution of each uninformative pair based on the average contribution of the informative pairs. The correction can be applied to the analysis of several prioritized outcomes. We perform a simulation study to evaluate the bias associated with this correction. When only one time-to-event outcome was generated, the corrected estimates were unbiased except in the presence of very heavy censoring. The correction had no effect on the power or type-1 error of the tests based on the Δ. Finally, we illustrate the impact of the correction using data from two randomized trials. The illustrative datasets showed that the correction had limited impact when the proportion of censored observations was around 20% and was most useful when this proportion was close to 70%. Overall, we propose an estimator for the net benefit that is minimally affected by censoring under the assumption that uninformative pairs are exchangeable with informative pairs.},
  archive      = {J_BIMJ},
  author       = {Julien Péron and Maryam Idlhaj and Delphine Maucort-Boulch and Joris Giai and Pascal Roy and Laurence Collette and Marc Buyse and Brice Ozenne},
  doi          = {10.1002/bimj.202000001},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {893-906},
  shortjournal = {Bio. J.},
  title        = {Correcting the bias of the net benefit estimator due to right-censored observations},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ecological hazard assessment via species sensitivity
distributions: The non-exchangeability issue. <em>BIMJ</em>,
<em>63</em>(4), 875–892. (<a
href="https://doi.org/10.1002/bimj.201900404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic approaches to hazard assessment use species sensitivity distributions (SSDs) to characterize hazard for toxicants exposure for different species within a community. Many of the assumptions at the core of SSDs are unrealistic, among them the assumption that the tolerance levels of all species in a specific ecological community are a priori exchangeable for each new toxic substance. Here we propose the use of a particular test to detect situations where such an assumption is violated. Then, a new method based on non-nested random effects model is required to identify novel SSDs capable of taking into account species non-exchangeability. Credible intervals, representing SSD uncertainty, could be determined based on our procedure. This leads to new and reliable estimates of the environmental hazard. We present a Bayesian modeling approach to address model inference issues, using Markov chain Monte Carlo sampling.},
  archive      = {J_BIMJ},
  author       = {Sonia Migliorati and Gianna Serafina Monti and Marco Vighi},
  doi          = {10.1002/bimj.201900404},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {875-892},
  shortjournal = {Bio. J.},
  title        = {Ecological hazard assessment via species sensitivity distributions: The non-exchangeability issue},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalised m-quantile random-effects model for discrete
response: An application to the number of visits to physicians.
<em>BIMJ</em>, <em>63</em>(4), 859–874. (<a
href="https://doi.org/10.1002/bimj.202000180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we extend the linear M-quantile random intercept model (MQRE) to discrete data and use the proposed model to evaluate the effect of selected covariates on two count responses: the number of generic medical examinations and the number of specialised examinations for health districts in three regions of central Italy. The new approach represents an outlier-robust alternative to the generalised linear mixed model with Gaussian random effects and it allows estimating the effect of the covariates at various quantiles of the conditional distribution of the target variable. Results from a simulation experiment, as well as from real data, confirm that the method proposed here presents good robustness properties and can be in certain cases more efficient than other approaches.},
  archive      = {J_BIMJ},
  author       = {Francesco Schirripa Spagnolo and Vincenzo Mauro and Nicola Salvati},
  doi          = {10.1002/bimj.202000180},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {859-874},
  shortjournal = {Bio. J.},
  title        = {Generalised M-quantile random-effects model for discrete response: An application to the number of visits to physicians},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parametric quantile regression approach for modelling
zero-or-one inflated double bounded data. <em>BIMJ</em>, <em>63</em>(4),
841–858. (<a href="https://doi.org/10.1002/bimj.202000126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decades, the challenges in applied regression have been changing considerably, and full probabilistic modeling rather than predicting just means is crucial in many applications. Motivated by two applications where the response variable is observed on the unit-interval and inflated at zero or one, we propose a parametric quantile regression considering the unit-Weibull distribution. In particular, we are interested in quantifying the influence of covariates on the quantiles of the response variable. The maximum likelihood method is used for parameters estimation. Monte Carlo simulations reveal that the maximum likelihood estimators are nearly unbiased and consistent. Also, we define a residual analysis to assess the goodness of fit.},
  archive      = {J_BIMJ},
  author       = {André F. B. Menezes and Josmar Mazucheli and Marcelo Bourguignon},
  doi          = {10.1002/bimj.202000126},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {841-858},
  shortjournal = {Bio. J.},
  title        = {A parametric quantile regression approach for modelling zero-or-one inflated double bounded data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical inferences on nonconstant relative potency with
quantal response data. <em>BIMJ</em>, <em>63</em>(4), 825–840. (<a
href="https://doi.org/10.1002/bimj.202000073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relative potency is widely used in toxicological and pharmacological studies to characterize potency of chemicals. The relative potency of a test chemical compared to a standard chemical is defined as the ratio of equally effective doses (standard divided by test). This classical concept relies on the assumption that the two chemicals are toxicologically similar—that is, they have parallel dose–response curves on log-dose scale—and thus have constant relative potency. Nevertheless, investigators are often faced with situations where the similarity assumption is deemed unreasonable, and hence the classical idea of constant relative potency fails to hold; in such cases, simply reporting a single constant value for relative potency can produce misleading conclusions. Relative potency functions, describing relative potency as a function of the mean response (or other quantities), is seen as a useful tool for handling nonconstant relative potency in the absence of similarity. Often, investigators are interested in assessing nonconstant relative potency at a finite set of some specific response levels for various regulatory concerns, rather than the entire relative potency function; this simultaneous assessment gives rise to multiplicity, which calls for efficient statistical inference procedures with multiplicity adjusted methods. In this paper, we discuss the estimation of relative potency at multiple response levels using the relative potency function, under the log-logistic dose–response model. We further propose and evaluate three approaches to calculating multiplicity-adjusted confidence limits as statistical inference procedures for assessing nonconstant relative potency. Monte Carlo simulations are conducted to evaluate the characteristics of the simultaneous limits.},
  archive      = {J_BIMJ},
  author       = {Lucy X. Kerns},
  doi          = {10.1002/bimj.202000073},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {825-840},
  shortjournal = {Bio. J.},
  title        = {Statistical inferences on nonconstant relative potency with quantal response data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Power and sample size for random coefficient regression
models in randomized experiments with monotone missing data.
<em>BIMJ</em>, <em>63</em>(4), 806–824. (<a
href="https://doi.org/10.1002/bimj.202000184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random coefficient regression (also known as random effects, mixed effects, growth curve, variance component, multilevel, or hierarchical linear modeling) can be a natural and useful approach for characterizing and testing hypotheses in data that are correlated within experimental units. Existing power and sample size software for such data are based on two variance component models or those using a two-stage formulation. These approaches may be markedly inaccurate in settings where more variance components (i.e., intercept, rate of change, and residual error) are warranted. We present variance, power, sample size formulae, and software (R Shiny app) for use with random coefficient regression models with possible missing data and variable follow-up. We illustrate sample size and study design planning using data from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database. We additionally examine the drivers of variability to better inform study design.},
  archive      = {J_BIMJ},
  author       = {Nan Hu and Howard Mackey and Ronald Thomas},
  doi          = {10.1002/bimj.202000184},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {806-824},
  shortjournal = {Bio. J.},
  title        = {Power and sample size for random coefficient regression models in randomized experiments with monotone missing data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric bayesian functional two-part random effects
model for longitudinal semicontinuous data analysis. <em>BIMJ</em>,
<em>63</em>(4), 787–805. (<a
href="https://doi.org/10.1002/bimj.201900280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal semicontinuous data, characterized by repeated measures of a large portion of zeros and continuous positive values, are frequently encountered in many applications including biomedical, epidemiological, and social science studies. Two-part random effects models (TPREM) have been used to investigate the association between such longitudinal semicontinuous data and covariates accounting for the within-subject correlation. The existing TPREM is, however, limited to incorporate a functional covariate, which is often available in a longitudinal study. Moreover, the existing TPREM typically assumes the normality of subject-specific random effects, which can be easily violated when there exists a subgroup structure. In this article, we propose a nonparametric Bayesian functional TPREM to assess the relationship between the longitudinal semicontinuous outcome and various types of covariates including a functional covariate. The proposed model also relaxes the normality assumption for the random effects through a Dirichlet process mixture of normals, which allows for identifying an underlying subgroup structure. The methodology is illustrated through an application to social insurance expenditure data collected by the Korean Welfare Panel Study and a simulation study.},
  archive      = {J_BIMJ},
  author       = {Jinsu Park and Taeryon Choi and Yeonseung Chung},
  doi          = {10.1002/bimj.201900280},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {787-805},
  shortjournal = {Bio. J.},
  title        = {Nonparametric bayesian functional two-part random effects model for longitudinal semicontinuous data analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing longitudinal clustered count data with zero
inflation: Marginal modeling using the conway–maxwell–poisson
distribution. <em>BIMJ</em>, <em>63</em>(4), 761–786. (<a
href="https://doi.org/10.1002/bimj.202000061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological and medical researchers often collect count data in clusters at multiple time points. The data can exhibit excessive zeros and a wide range of dispersion levels. In particular, our research was motivated by a dental dataset with such complex data features: the Iowa Fluoride Study (IFS). The study was designed to investigate the effects of various dietary and nondietary factors on the caries development of a cohort of Iowa school children at the ages of 5, 9, and 13. To analyze the multiyear IFS data, we propose a novel longitudinal method of a generalized estimating equations based marginal regression model. We use a zero-inflated model with a Conway–Maxwell–Poisson (CMP) distribution, which has the flexibility to account for all levels of dispersion. The parameters of interest are estimated through a modified expectation–solution algorithm to account for the clustered and temporal correlation structure. We fit the proposed zero-inflated CMP model and perform a comprehensive secondary analysis of the IFS dataset. It resulted in a number of notable conclusions that also make clinical sense. Additionally, we demonstrated the superiority of this modeling approach over two other popular competing models: the zero-inflated Poisson and negative binomial models. In the simulation studies, we further evaluate the performance of our point estimators, the variance estimators, and that of the large sample confidence intervals for the parameters of interest. It is also demonstrated that our longitudinal CMP model can correctly identify the time-varying dispersion patterns.},
  archive      = {J_BIMJ},
  author       = {Tong Kang and Steven M. Levy and Somnath Datta},
  doi          = {10.1002/bimj.202000061},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {761-786},
  shortjournal = {Bio. J.},
  title        = {Analyzing longitudinal clustered count data with zero inflation: Marginal modeling using the Conway–Maxwell–Poisson distribution},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical method for modeling sequencing data from
different technologies in longitudinal studies with application to
huntington disease. <em>BIMJ</em>, <em>63</em>(4), 745–760. (<a
href="https://doi.org/10.1002/bimj.201900235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancement of gene expression measurements in longitudinal studies enables the identification of genes associated with disease severity over time. However, problems arise when the technology used to measure gene expression differs between time points. Observed differences between the results obtained at different time points can be caused by technical differences. Modeling the two measurements jointly over time might provide insight into the causes of these different results. Our work is motivated by a study of gene expression data of blood samples from Huntington disease patients, which were obtained using two different sequencing technologies. At time point 1, DeepSAGE technology was used to measure the gene expression, with a subsample also measured using RNA-Seq technology. At time point 2, all samples were measured using RNA-Seq technology. Significant associations between gene expression measured by DeepSAGE and disease severity using data from the first time point could not be replicated by the RNA-Seq data from the second time point. We modeled the relationship between the two sequencing technologies using the data from the overlapping samples. We used linear mixed models with either DeepSAGE or RNA-Seq measurements as the dependent variable and disease severity as the independent variable. In conclusion, (1) for one out of 14 genes, the initial significant result could be replicated with both technologies using data from both time points; (2) statistical efficiency is lost due to disagreement between the two technologies, measurement error when predicting gene expressions, and the need to include additional parameters to account for possible differences.},
  archive      = {J_BIMJ},
  author       = {Angga M. Fuady and Willeke M. C. van Roon-Mom and Szymon M. Kiełbasa and Hae-Won Uh and Jeanine J. Houwing-Duistermaat},
  doi          = {10.1002/bimj.201900235},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {745-760},
  shortjournal = {Bio. J.},
  title        = {Statistical method for modeling sequencing data from different technologies in longitudinal studies with application to huntington disease},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint frailty model for recurrent events and death in
presence of cure fraction: Application to breast cancer data.
<em>BIMJ</em>, <em>63</em>(4), 725–744. (<a
href="https://doi.org/10.1002/bimj.201900113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many biomedical cohort studies, recurrent or repeated events for individuals can be terminated by a dependent terminal event like death. In this context, the time of death may be associated with the underlying recurrent process and there often exists the dependence between the occurrences of recurrent events. Moreover, there are some situations in which a portion of patients could be cured. In the present study, the term “cured” means that some patients may neither experience any recurrent events nor death induced by the disease under study. We proposed a joint frailty model in the presence of cure fraction for analysis of the recurrent and terminal events and estimated the effect of covariates on the cure rate and both aforementioned events concurrently. The use of two independent gamma distributed frailties in this model enabled us to consider both the dependence between the recurrences and the survival times and the interrecurrences dependence. The model parameters were estimated employing the maximum likelihood method for a piecewise constant and a parametric baseline hazard function. Our proposed model was evaluated by a simulation study and illustrated using a real data set on patients with breast cancer who had undergone surgery.},
  archive      = {J_BIMJ},
  author       = {Elaheh Talebi-Ghane and AhmadReza Baghestani and Farid Zayeri and Virgine Rondeau and Ali Akhavan},
  doi          = {10.1002/bimj.201900113},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {725-744},
  shortjournal = {Bio. J.},
  title        = {Joint frailty model for recurrent events and death in presence of cure fraction: Application to breast cancer data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted estimators of the complier average causal effect on
restricted mean survival time with observed instrument–outcome
confounders. <em>BIMJ</em>, <em>63</em>(4), 712–724. (<a
href="https://doi.org/10.1002/bimj.201900284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major concern in any observational study is unmeasured confounding of the relationship between a treatment and outcome of interest. Instrumental variable (IV) analysis methods are able to control for unmeasured confounding. However, IV analysis methods developed for censored time-to-event data tend to rely on assumptions that may not be reasonable in many practical applications, making them unsuitable for use in observational studies. In this report, we develop weighted estimators of the complier average causal effect (CACE) on the restricted mean survival time in the overall population as well as in an evenly matchable population (CACE-m). Our method is able to accommodate instrument–outcome confounding and adjust for covariate-dependent censoring, making it particularly suited for causal inference from observational studies. We establish the asymptotic properties and derive easily implementable asymptotic variance estimators for the proposed estimators. Through simulation studies, we show that the proposed estimators tend to be more efficient than instrument propensity score matching-based estimators or IPIW estimators. We apply our method to compare dialytic modality-specific survival for end stage renal disease patients using data from the U.S. Renal Data System.},
  archive      = {J_BIMJ},
  author       = {Sai H. Dharmarajan and Yun Li and Douglas Lehmann and Douglas E. Schaubel},
  doi          = {10.1002/bimj.201900284},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {712-724},
  shortjournal = {Bio. J.},
  title        = {Weighted estimators of the complier average causal effect on restricted mean survival time with observed instrument–outcome confounders},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-analysis of diagnostic accuracy studies with multiple
thresholds: Comparison of different approaches. <em>BIMJ</em>,
<em>63</em>(4), 699–711. (<a
href="https://doi.org/10.1002/bimj.202000091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for standard meta-analysis of diagnostic test accuracy studies are well established and understood. For the more complex case in which studies report test accuracy across multiple thresholds, several approaches have recently been proposed. These are based on similar ideas, but make different assumptions. In this article, we apply four different approaches to data from a recent systematic review in the area of nephrology and compare the results. The four approaches use: a linear mixed effects model, a Bayesian multinomial random effects model, a time-to-event model and a nonparametric model, respectively. In the case study data, the accuracy of neutrophil gelatinase-associated lipocalin for the diagnosis of acute kidney injury was assessed in different scenarios, with sensitivity and specificity estimates available for three thresholds in each primary study. All approaches led to plausible and mostly similar summary results. However, we found considerable differences in results for some scenarios, for example, differences in the area under the receiver operating characteristic curve (AUC) of up to 0.13. The Bayesian approach tended to lead to the highest values of the AUC, and the nonparametric approach tended to produce the lowest values across the different scenarios. Though we recommend using these approaches, our findings motivate the need for a simulation study to explore optimal choice of method in various scenarios.},
  archive      = {J_BIMJ},
  author       = {Antonia Zapf and Christian Albert and Cornelia Frömke and Michael Haase and Annika Hoyer and Hayley E. Jones and Gerta Rücker},
  doi          = {10.1002/bimj.202000091},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {699-711},
  shortjournal = {Bio. J.},
  title        = {Meta-analysis of diagnostic accuracy studies with multiple thresholds: Comparison of different approaches},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Contents: Biometrical journal 4’21. <em>BIMJ</em>,
<em>63</em>(4), 696–697. (<a
href="https://doi.org/10.1002/bimj.202170044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170044},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {696-697},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 4&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Cover picture: Biometrical journal 3’21. <em>BIMJ</em>,
<em>63</em>(3), NA. (<a
href="https://doi.org/10.1002/bimj.202170031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170031},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 3&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The modified fuzzy mortality model based on the algebra of
ordered fuzzy numbers. <em>BIMJ</em>, <em>63</em>(3), 671–689. (<a
href="https://doi.org/10.1002/bimj.202000025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new trends in fuzzy analysis are based on the algebraic approach to fuzzy numbers. The essential idea in such an approach is representing the membership function of a fuzzy number as an element of any square-integrable function space. As a starting point, we consider the Koissi–Shapiro model known as a fuzzy version of the Lee–Carter mortality model, in which triangular membership functions of fuzzy variables are assumed. In our approach, the algebra of ordered fuzzy numbers is used to reformulate the fuzzy Lee–Carter mortality model and to improve the prediction accuracy.},
  archive      = {J_BIMJ},
  author       = {Andrzej Szymański and Agnieszka Rossa},
  doi          = {10.1002/bimj.202000025},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {671-689},
  shortjournal = {Bio. J.},
  title        = {The modified fuzzy mortality model based on the algebra of ordered fuzzy numbers},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survival analysis for AdVerse events with VarYing follow-up
times (SAVVY): Rationale and statistical concept of a meta-analytic
study. <em>BIMJ</em>, <em>63</em>(3), 650–670. (<a
href="https://doi.org/10.1002/bimj.201900347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assessment of safety is an important aspect of the evaluation of new therapies in clinical trials, with analyses of adverse events being an essential part of this. Standard methods for the analysis of adverse events such as the incidence proportion, that is the number of patients with a specific adverse event out of all patients in the treatment groups, do not account for both varying follow-up times and competing risks. Alternative approaches such as the Aalen–Johansen estimator of the cumulative incidence function have been suggested. Theoretical arguments and numerical evaluations support the application of these more advanced methodology, but as yet there is to our knowledge only insufficient empirical evidence whether these methods would lead to different conclusions in safety evaluations. The Survival analysis for AdVerse events with VarYing follow-up times (SAVVY) project strives to close this gap in evidence by conducting a meta-analytical study to assess the impact of the methodology on the conclusion of the safety assessment empirically. Here we present the rationale and statistical concept of the empirical study conducted as part of the SAVVY project. The statistical methods are presented in unified notation, and examples of their implementation in R and SAS are provided.},
  archive      = {J_BIMJ},
  author       = {Regina Stegherr and Jan Beyersmann and Valentine Jehl and Kaspar Rufibach and Friedhelm Leverkus and Claudia Schmoor and Tim Friede},
  doi          = {10.1002/bimj.201900347},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {650-670},
  shortjournal = {Bio. J.},
  title        = {Survival analysis for AdVerse events with VarYing follow-up times (SAVVY): Rationale and statistical concept of a meta-analytic study},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate bayesian inference for multivariate point
pattern analysis in disease mapping. <em>BIMJ</em>, <em>63</em>(3),
632–649. (<a href="https://doi.org/10.1002/bimj.201900396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for analysing multivariate case-control georeferenced data in a Bayesian disease mapping context using stochastic partial differential equations (SPDEs) and the integrated nested Laplace approximation (INLA) for model fitting. In particular, we propose smooth terms based on SPDE models to estimate the underlying spatial variation as well as risk associated to pollution sources. Log-Gaussian Cox processes are used to estimate the intensity of the cases and controls, to account for risk factors and include a term to measure spatial residual variation. Each intensity is modelled on a baseline spatial effect (estimated from both controls and cases), a disease-specific spatial term and the effects of some covariates. By fitting these models, the residual spatial terms can be easily compared to detect high-risk areas not explained by the covariates. Three different types of effects to model exposure to pollution sources are considered on the distance to the source: a fixed effect, a smooth term to model non-linear effects by means of a discrete random walk of order one and a Gaussian process in one dimension with a Matérn covariance function. Spatial terms are modelled using a Gaussian process in two dimensions with a Matérn covariance function and are approximated using an approach based on solving an SPDE through INLA. Finally, this new framework is applied to a dataset of three different types of cancer and a set of controls from Alcalá de Henares (Madrid, Spain). Covariates available include the distance to several polluting industries and socioeconomic indicators. Our findings point to a possible risk increase due to the proximity to some of these industries.},
  archive      = {J_BIMJ},
  author       = {Francisco Palmí-Perales and Virgilio Gómez-Rubio and Gonzalo López-Abente and Rebeca Ramis and José Miguel Sanz-Anquela and Pablo Fernández-Navarro},
  doi          = {10.1002/bimj.201900396},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {632-649},
  shortjournal = {Bio. J.},
  title        = {Approximate bayesian inference for multivariate point pattern analysis in disease mapping},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subgroup-adaptive randomization for subgroup confirmation in
clinical trials. <em>BIMJ</em>, <em>63</em>(3), 616–631. (<a
href="https://doi.org/10.1002/bimj.201900333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well-known issue when testing for treatment-by-subgroup interaction is its low power, as clinical trials are generally powered for establishing efficacy claims for the overall population, and they are usually not adequately powered for detecting interaction (Alosh, Huque, &amp; Koch [2015] Journal of Biopharmaceutical Statistics , 25, 1161–1178). Hence, it is necessary to develop an adaptive design to improve the efficiency of detecting heterogeneous treatment effects within subgroups. Considering Neyman allocation can maximize the power of usual Z -test (see p. 194 of the book edited by Rosenberger and Lachin), we propose a subgroup-adaptive randomization procedure aiming to achieve Neyman allocation in both predefined subgroups and overall study population in this paper. To verify whether the proposed randomization procedure works as intended, relevant theoretical results are derived and displayed . Numerical studies show that the proposed randomization procedure has obvious advantages in power of tests compared with complete randomization and Pocock and Simon&#39;s minimization method.},
  archive      = {J_BIMJ},
  author       = {Zhongqiang Liu and Xuesi Ma and Zhaoliang Wang},
  doi          = {10.1002/bimj.201900333},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {616-631},
  shortjournal = {Bio. J.},
  title        = {Subgroup-adaptive randomization for subgroup confirmation in clinical trials},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of methods for analysing multiple outcome
measures in randomised controlled trials using a simulation study.
<em>BIMJ</em>, <em>63</em>(3), 599–615. (<a
href="https://doi.org/10.1002/bimj.201900040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple primary outcomes are sometimes collected and analysed in randomised controlled trials (RCTs), and are used in favour of a single outcome. By collecting multiple primary outcomes, it is possible to fully evaluate the effect that an intervention has for a given disease process. A simple approach to analysing multiple outcomes is to consider each outcome separately, however, this approach does not account for any pairwise correlations between the outcomes. Any cases with missing values must be ignored, unless an additional imputation step is performed. Alternatively, multivariate methods that explicitly model the pairwise correlations between the outcomes may be more efficient when some of the outcomes have missing values. In this paper, we present an overview of relevant methods that can be used to analyse multiple outcome measures in RCTs, including methods based on multivariate multilevel (MM) models. We perform simulation studies to evaluate the bias in the estimates of the intervention effects and the power of detecting true intervention effects observed when using selected methods. Different simulation scenarios were constructed by varying the number of outcomes, the type of outcomes, the degree of correlations between the outcomes and the proportions and mechanisms of missing data. We compare multivariate methods to univariate methods with and without multiple imputation. When there are strong correlations between the outcome measures ( ρ &gt; .4), our simulation studies suggest that there are small power gains when using the MM model when compared to analysing the outcome measures separately. In contrast, when there are weak correlations ( ρ &lt; .4), the power is reduced when using univariate methods with multiple imputation when compared to analysing the outcome measures separately.},
  archive      = {J_BIMJ},
  author       = {Victoria Vickerstaff and Gareth Ambler and Rumana Z. Omar},
  doi          = {10.1002/bimj.201900040},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {599-615},
  shortjournal = {Bio. J.},
  title        = {A comparison of methods for analysing multiple outcome measures in randomised controlled trials using a simulation study},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential change point detection for high-dimensional data
using nonconvex penalized quantile regression. <em>BIMJ</em>,
<em>63</em>(3), 575–598. (<a
href="https://doi.org/10.1002/bimj.202000078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a sequential change point detection method is developed to monitor structural change in smoothly clipped absolute deviation (SCAD) penalized quantile regression (SPQR) models. The asymptotic properties of the test statistic are derived from the null and alternative hypotheses. In order to improve the performance of the SPQR method, we propose a post-SCAD penalized quantile regression estimator (P-SPQR) for high-dimensional data. We examined the finite sample properties of the proposed methods via Monte Carlo studies under different scenarios. A real data application is provided to demonstrate the effectiveness of the method.},
  archive      = {J_BIMJ},
  author       = {Suthakaran Ratnasingam and Wei Ning},
  doi          = {10.1002/bimj.202000078},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {575-598},
  shortjournal = {Bio. J.},
  title        = {Sequential change point detection for high-dimensional data using nonconvex penalized quantile regression},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exact change point detection with improved power in
small-sample binomial sequences. <em>BIMJ</em>, <em>63</em>(3), 558–574.
(<a href="https://doi.org/10.1002/bimj.201900273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To detect a change in the probability of a sequence of independent binomial random variables, a variety of asymptotic and exact testing procedures have been proposed. Whenever the sample size or the event rate is small, asymptotic approximations of maximally selected test statistics have been shown to be inaccurate. Although exact methods control the type I error rate, they can be overly conservative due to the discreteness of the test statistics in these situations. We extend approaches by Worsley and Halpern to develop a test that is less discrete to increase the power. Building on ideas from binary segmentation, the proposed test utilizes unused information in the binomial sequences to add a new ordering to test statistics that are of equal value. The exact distributions are derived under side conditions that arise in hypothetical segmentation steps and do not depend on the type of test statistic used (e.g., log likelihood ratio, cumulative sum, or Fisher&#39;s exact test). Using the proposed exact segmentation procedure, we construct a change point test and prove that it controls the type-I-error rate at any given nominal level. Furthermore, we prove that the new test is uniformly at least as powerful as Worsley&#39;s exact test. In a Monte Carlo simulation study, the gain in power can be remarkable, especially in scenarios with small sample size. Giving a clinical database example about pin site infections and an example assessing publication bias in neuropsychiatric drug research, we demonstrate the wide-ranging applicability of the test.},
  archive      = {J_BIMJ},
  author       = {David Ellenberger and Berthold Lausen and Tim Friede},
  doi          = {10.1002/bimj.201900273},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {558-574},
  shortjournal = {Bio. J.},
  title        = {Exact change point detection with improved power in small-sample binomial sequences},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Making apples from oranges: Comparing noncollapsible effect
estimators and their standard errors after adjustment for different
covariate sets. <em>BIMJ</em>, <em>63</em>(3), 528–557. (<a
href="https://doi.org/10.1002/bimj.201900297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the well-known but often misunderstood issue of (non)collapsibility of effect measures in regression models for binary and time-to-event outcomes. We describe an existing simple but largely ignored procedure for marginalizing estimates of conditional odds ratios and propose a similar procedure for marginalizing estimates of conditional hazard ratios (allowing for right censoring), demonstrating its performance in simulation studies and in a reanalysis of data from a small randomized trial in primary biliary cirrhosis patients. In addition, we aim to provide an educational summary of issues surrounding (non)collapsibility from a causal inference perspective and to promote the idea that the words conditional and adjusted (likewise marginal and unadjusted ) should not be used interchangeably.},
  archive      = {J_BIMJ},
  author       = {Rhian Daniel and Jingjing Zhang and Daniel Farewell},
  doi          = {10.1002/bimj.201900297},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {528-557},
  shortjournal = {Bio. J.},
  title        = {Making apples from oranges: Comparing noncollapsible effect estimators and their standard errors after adjustment for different covariate sets},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Methods to estimate proportion and number of nonexposed
cases in a population. <em>BIMJ</em>, <em>63</em>(3), 514–527. (<a
href="https://doi.org/10.1002/bimj.201900190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {National mortality statistics commonly provide disease-specific absolute and relative frequencies of death by sex and age, but not by exposure status. However, it is often of interest to know how many of the diseased individuals, that is the cases, were exposed or not exposed to a specific risk factor. We present two methods to estimate the proportion and the number of exposed and nonexposed cases, both of which require an estimate of the exposure prevalence in the nondiseased population. Method I additionally requires an estimate of the relative effect of exposure, that is a relative risk function if the exposure has a continuous distribution, or a relative risk estimate for each category if the exposure is categorical. Method II additionally requires an estimate of the disease rate among the nonexposed. We provide theoretical justifications, discuss practical limitations, and provide an R script to calculate the probability for nonexposure among the diseased, and compare the approaches. Both methods are subsequently applied to the estimation of the number of never smokers among lung cancer deaths. The two suggested methods rely on the availability of specific data sources and might therefore be applicable in different research settings. Both methods yield unbiased estimates of the number of nonexposed cases, given that the respective underlying assumptions are fulfilled.},
  archive      = {J_BIMJ},
  author       = {Heiko Becher and Annette Aigner},
  doi          = {10.1002/bimj.201900190},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {514-527},
  shortjournal = {Bio. J.},
  title        = {Methods to estimate proportion and number of nonexposed cases in a population},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble approach to short-term forecast of COVID-19
intensive care occupancy in italian regions. <em>BIMJ</em>,
<em>63</em>(3), 503–513. (<a
href="https://doi.org/10.1002/bimj.202000189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of intensive care beds during the COVID-19 epidemic is crucial to guarantee the best possible treatment to severely affected patients. In this work we show a simple strategy for short-term prediction of COVID-19 intensive care unit (ICU) beds, that has proved very effective during the Italian outbreak in February to May 2020. Our approach is based on an optimal ensemble of two simple methods: a generalized linear mixed regression model, which pools information over different areas, and an area-specific nonstationary integer autoregressive methodology. Optimal weights are estimated using a leave-last-out rationale. The approach has been set up and validated during the first epidemic wave in Italy. A report of its performance for predicting ICU occupancy at regional level is included.},
  archive      = {J_BIMJ},
  author       = {Alessio Farcomeni and Antonello Maruotti and Fabio Divino and Giovanna Jona-Lasinio and Gianfranco Lovison},
  doi          = {10.1002/bimj.202000189},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {503-513},
  shortjournal = {Bio. J.},
  title        = {An ensemble approach to short-term forecast of COVID-19 intensive care occupancy in italian regions},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nowcasting the COVID-19 pandemic in bavaria. <em>BIMJ</em>,
<em>63</em>(3), 490–502. (<a
href="https://doi.org/10.1002/bimj.202000112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the current dynamics of an epidemic, it is central to collect information on the daily number of newly diseased cases. This is especially important in real-time surveillance, where the aim is to gain situational awareness, for example, if cases are currently increasing or decreasing. Reporting delays between disease onset and case reporting hamper our ability to understand the dynamics of an epidemic close to now when looking at the number of daily reported cases only. Nowcasting can be used to adjust daily case counts for occurred-but-not-yet-reported events. Here, we present a novel application of nowcasting to data on the current COVID-19 pandemic in Bavaria. It is based on a hierarchical Bayesian model that considers changes in the reporting delay distribution over time and associated with the weekday of reporting. Furthermore, we present a way to estimate the effective time-varying case reproduction number based on predictions of the nowcast. The approaches are based on previously published work, that we considerably extended and adapted to the current task of nowcasting COVID-19 cases. We provide methodological details of the developed approach, illustrate results based on data of the current pandemic, and evaluate the model based on synthetic and retrospective data on COVID-19 in Bavaria. Results of our nowcasting are reported to the Bavarian health authority and published on a webpage on a daily basis ( https://corona.stat.uni-muenchen.de/ ). Code and synthetic data for the analysis are available from https://github.com/FelixGuenther/nc_covid19_bavaria and can be used for adaption of our approach to different data.},
  archive      = {J_BIMJ},
  author       = {Felix Günther and Andreas Bender and Katharina Katz and Helmut Küchenhoff and Michael Höhle},
  doi          = {10.1002/bimj.202000112},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {490-502},
  shortjournal = {Bio. J.},
  title        = {Nowcasting the COVID-19 pandemic in bavaria},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Nowcasting fatal COVID-19 infections on a regional level in
germany. <em>BIMJ</em>, <em>63</em>(3), 471–489. (<a
href="https://doi.org/10.1002/bimj.202000143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the temporal and regional structure in mortality rates related to COVID-19 infections, making use of the openly available data on registered cases in Germany published by the Robert Koch Institute on a daily basis. Estimates for the number of present-day infections that will, at a later date, prove to be fatal are derived through a nowcasting model, which relates the day of death of each deceased patient to the corresponding day of registration of the infection. Our district-level modelling approach for fatal infections disentangles spatial variation into a global pattern for Germany, district-specific long-term effects and short-term dynamics, while also taking the age and gender structure of the regional population into account. This enables to highlight areas with unexpectedly high disease activity. The analysis of death counts contributes to a better understanding of the spread of the disease while being, to some extent, less dependent on testing strategy and capacity in comparison to infection counts. The proposed approach and the presented results thus provide reliable insight into the state and the dynamics of the pandemic during the early phases of the infection wave in spring 2020 in Germany, when little was known about the disease and limited data were available.},
  archive      = {J_BIMJ},
  author       = {Marc Schneble and Giacomo De Nicola and Göran Kauermann and Ursula Berger},
  doi          = {10.1002/bimj.202000143},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {471-489},
  shortjournal = {Bio. J.},
  title        = {Nowcasting fatal COVID-19 infections on a regional level in germany},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Contents: Biometrical journal 3’21. <em>BIMJ</em>,
<em>63</em>(3), 469–470. (<a
href="https://doi.org/10.1002/bimj.202170034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170034},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {469-470},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 3&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Cover picture: Biometrical journal 2’21. <em>BIMJ</em>,
<em>63</em>(2), NA. (<a
href="https://doi.org/10.1002/bimj.202170021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170021},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 2&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Component network meta-analysis compared to a matching
method in a disconnected network: A case study. <em>BIMJ</em>,
<em>63</em>(2), 447–461. (<a
href="https://doi.org/10.1002/bimj.201900339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis is a method to combine evidence from randomized controlled trials (RCTs) that compare a number of different interventions for a given clinical condition. Usually, this requires a connected network. A possible approach to link a disconnected network is to add evidence from nonrandomized comparisons, using propensity score or matching-adjusted indirect comparisons methods. However, nonrandomized comparisons may be associated with an unclear risk of bias. Schmitz et al. used single-arm observational studies for bridging the gap between two disconnected networks of treatments for multiple myeloma. We present a reanalysis of these data using component network meta-analysis (CNMA) models entirely based on RCTs, utilizing the fact that many of the treatments consisted of common treatment components occurring in both networks. We discuss forward and backward strategies for selecting appropriate CNMA models and compare the results to those obtained by Schmitz et al. using their matching method. CNMA models provided a good fit to the data and led to treatment rankings that were similar, though not fully equal to that obtained by Schmitz et al. We conclude that researchers encountering a disconnected network with treatments in different subnets having common components should consider a CNMA model. Such models, exclusively based on evidence from RCTs, are a promising alternative to matching approaches that require additional evidence from observational studies. CNMA models are implemented in the R package netmeta.},
  archive      = {J_BIMJ},
  author       = {Gerta Rücker and Susanne Schmitz and Guido Schwarzer},
  doi          = {10.1002/bimj.201900339},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {447-461},
  shortjournal = {Bio. J.},
  title        = {Component network meta-analysis compared to a matching method in a disconnected network: A case study},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A joint frailty-copula model for meta-analytic validation of
failure time surrogate endpoints in clinical trials. <em>BIMJ</em>,
<em>63</em>(2), 423–446. (<a
href="https://doi.org/10.1002/bimj.201900306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a meta-analysis framework, the classical approach for the validation of time-to-event surrogate endpoint is based on a two-step analysis. This approach often raises estimation issues. Recently, we proposed a one-step validation approach based on a joint frailty model. This approach was quite time consuming, despite parallel computing, due to individual-level frailties used to take into account heterogeneity in the data at the individual level. We now propose an alternative one-step approach for evaluating surrogacy, using a joint frailty-copula model. The model includes two correlated random effects treatment-by-trial interaction and a shared random effect associated with the baseline risks. At the individual level, the joint survivor functions of time-to-event endpoints are linked using copula functions. We used splines for the baseline hazard functions. We estimated parameters and hazard function using a semiparametric penalized marginal likelihood method, considering various numerical integration methods. Both individual-level and trial-level surrogacy were evaluated using Kendall&#39;s tau and coefficient of determination. The performance of the estimators was evaluated using simulation studies. The model was applied to individual patient data meta-analyses in advanced ovarian cancer to assess progression-free survival as a surrogate for overall survival, as part of the evaluation of new therapy. The model showed good performance and was quite robust regarding the integration methods and data variation, regardless of the surrogacy evaluation criteria. Kendall&#39;s Tau was better estimated using the Clayton copula model compared to the joint frailty model. The proposed model reduces the convergence and model estimation issues encountered in the two-step approach.},
  archive      = {J_BIMJ},
  author       = {Casimir L. Sofeu and Takeshi Emura and Virginie Rondeau},
  doi          = {10.1002/bimj.201900306},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {423-446},
  shortjournal = {Bio. J.},
  title        = {A joint frailty-copula model for meta-analytic validation of failure time surrogate endpoints in clinical trials},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bias-corrected meta-analysis model for combining, studies
of different types and quality. <em>BIMJ</em>, <em>63</em>(2), 406–422.
(<a href="https://doi.org/10.1002/bimj.201900376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public health researchers may have to decide whether to perform a meta-analysis including only high-quality randomized clinical trials (RCTs) or whether to include a mixture of all the available evidence, namely RCTs of varying quality and observational studies (OS). The main hurdle when combining disparate evidence in a meta-analysis is that we are not only combining results of interest but we are also combining multiple biases. Therefore, commonly applied meta-analysis methods may lead to misleading conclusions. In this paper, we present a new Bayesian hierarchical model, called the bias-corrected (BC) meta-analysis model, to combine different study types in meta-analysis. This model is based on a mixture of two random effects distributions, where the first component corresponds to the model of interest and the second component to the hidden bias structure. In this way, the resulting model of interest is adjusted by the internal validity bias of the studies included in a systematic review. We illustrate the BC model with two meta-analyses: The first one combines RCTs and OS to assess effectiveness of vaccination to prevent invasive pneumococcal disease. The second one investigates the effectiveness of stem cell treatment in heart disease patients. Our results show that ignoring internal validity bias in a meta-analysis may lead to misleading conclusions. However, if a meta-analysis model contemplates a bias adjustment, then RCTs results may increase their precision by including OS in the analysis. The BC model has been implemented in JAGS and R, which facilitate its application in practice.},
  archive      = {J_BIMJ},
  author       = {Pablo Emilio Verde},
  doi          = {10.1002/bimj.201900376},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {406-422},
  shortjournal = {Bio. J.},
  title        = {A bias-corrected meta-analysis model for combining, studies of different types and quality},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequentist performances of bayesian prediction intervals
for random-effects meta-analysis. <em>BIMJ</em>, <em>63</em>(2),
394–405. (<a href="https://doi.org/10.1002/bimj.201900351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction interval has been increasingly used in meta-analyses as a useful measure for assessing the magnitude of treatment effect and between-studies heterogeneity. In calculations of the prediction interval, although the Higgins–Thompson–Spiegelhalter method is used most often in practice, it might not have adequate coverage probability for the true treatment effect of a future study under realistic situations. An effective alternative candidate is the Bayesian prediction interval, which has also been widely used in general prediction problems. However, these prediction intervals are constructed based on the Bayesian philosophy, and their frequentist validities are only justified by large-sample approximations even if noninformative priors are adopted. There has been no certain evidence that evaluated their frequentist performances under realistic situations of meta-analyses. In this study, we conducted extensive simulation studies to assess the frequentist coverage performances of Bayesian prediction intervals with 11 noninformative prior distributions under general meta-analysis settings. Through these simulation studies, we found that frequentist coverage performances strongly depended on what prior distributions were adopted. In addition, when the number of studies was smaller than 10, there were no prior distributions that retained accurate frequentist coverage properties. We also illustrated these methods via applications to two real meta-analysis datasets. The resultant prediction intervals also differed according to the adopted prior distributions. Inaccurate prediction intervals may provide invalid evidence and misleading conclusions. Thus, if frequentist accuracy is required, Bayesian prediction intervals should be used cautiously in practice.},
  archive      = {J_BIMJ},
  author       = {Yuta Hamaguchi and Hisashi Noma and Kengo Nagashima and Tomohide Yamada and Toshi A. Furukawa},
  doi          = {10.1002/bimj.201900351},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {394-405},
  shortjournal = {Bio. J.},
  title        = {Frequentist performances of bayesian prediction intervals for random-effects meta-analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering with missing and left-censored data: A simulation
study comparing multiple-imputation-based procedures. <em>BIMJ</em>,
<em>63</em>(2), 372–393. (<a
href="https://doi.org/10.1002/bimj.201900366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis, commonly used to explore large biomedical datasets, can be challenging, notably due to missing data or left-censored data induced by the sensitivity limits of the biochemical measurement method. Usually, complete-case analysis, simple imputation, or stochastic simple imputation are applied before clustering. More recently, consensus methods following multiple imputation have been proposed. However, they ignore left-censoring and do not allow the number of clusters to vary across the partitions of each imputed dataset. Here, we developed a consensus-based clustering algorithm in which left-censored data are taken into account using a modified multiple imputation method and the number of clusters is estimated for each imputed dataset. A simulation study was conducted to assess the performance in terms of the number of clusters, the percentage of unclassified observations, and the adjusted Rand index. The simulation results showed that the investigated method works well compared to several alternative approaches. A real-world application in breast cancer patients showed that the proposed method may reveal novel clusters of patients.},
  archive      = {J_BIMJ},
  author       = {Lilith Faucheux and Matthieu Resche-Rigon and Emmanuel Curis and Vassili Soumelis and Sylvie Chevret},
  doi          = {10.1002/bimj.201900366},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {372-393},
  shortjournal = {Bio. J.},
  title        = {Clustering with missing and left-censored data: A simulation study comparing multiple-imputation-based procedures},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple imputation methods for handling missing values in
longitudinal studies with sampling weights: Comparison of methods
implemented in stata. <em>BIMJ</em>, <em>63</em>(2), 354–371. (<a
href="https://doi.org/10.1002/bimj.201900360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many analyses of longitudinal cohorts require incorporating sampling weights to account for unequal sampling probabilities of participants, as well as the use of multiple imputation (MI) for dealing with missing data. However, there is no guidance on how MI and sampling weights should be implemented together. We simulated a target population based on the Australian Bureau of Statistics Estimated Resident Population and drew 1000 random samples dependent on three design variables to mimic the Longitudinal Study of Australian Children. The target analysis was the weighted prevalence of overweight/obesity over childhood. We evaluated the performance of several MI approaches available in Stata, based on multivariate normal imputation (MVNI), fully conditional specification (FCS) and twofold FCS: a weighted imputation model, imputing missing data separately for each quintile sampling weight grouping, including the design stratum indicator in the imputation model, and using sampling weights as a covariate in the imputation model. Approaches based on available cases and inverse probability weighting (IPW), with time-varying weights, were also compared. We observed severe issues of convergence with FCS and twofold FCS. All MVNI-based approaches performed similarly, producing minimal bias and nominal coverage, except for when imputation was conducted separately for each quintile sampling weight group. IPW performed equally as well as MVNI-based approaches in terms of bias, however, was less precise. In similar longitudinal studies, we recommend using MVNI with the design stratum as a covariate in the imputation model. If this is unknown, including the sampling weight as a covariate is an appropriate alternative.},
  archive      = {J_BIMJ},
  author       = {Anurika P. De Silva and Alysha M. De Livera and Katherine J. Lee and Margarita Moreno-Betancur and Julie A. Simpson},
  doi          = {10.1002/bimj.201900360},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {354-371},
  shortjournal = {Bio. J.},
  title        = {Multiple imputation methods for handling missing values in longitudinal studies with sampling weights: Comparison of methods implemented in stata},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding disparities in cancer prognosis: An extension
of mediation analysis to the relative survival framework. <em>BIMJ</em>,
<em>63</em>(2), 341–353. (<a
href="https://doi.org/10.1002/bimj.201900355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis can be applied to investigate the effect of a third variable on the pathway between an exposure and the outcome. Such applications include investigating the determinants that drive differences in cancer survival across subgroups. However, cancer disparities may be the result of complex mechanisms that involve both cancer-related and other-cause mortality differences making it difficult to identify the causing factors. Relative survival, a commonly used measure in cancer epidemiology, can be used to focus on cancer-related differences. We extended mediation analysis to the relative survival framework for exploring cancer inequalities. The marginal effects were obtained using regression standardization, after fitting a relative survival model. Contrasts of interests included both marginal relative survival and marginal all-cause survival differences between exposure groups. Such contrasts include the indirect effect due to a mediator that is identifiable under certain assumptions. A separate model was fitted for the mediator and uncertainty was estimated using parametric bootstrapping. The avoidable deaths under interventions can also be estimated to quantify the impact of eliminating differences. The methods are illustrated using data for individuals diagnosed with colon cancer. Mediation analysis within relative survival allows focus on factors that account for cancer-related differences instead of all-cause differences and helps improve our understanding on cancer inequalities.},
  archive      = {J_BIMJ},
  author       = {Elisavet Syriopoulou and Mark J. Rutherford and Paul C. Lambert},
  doi          = {10.1002/bimj.201900355},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {341-353},
  shortjournal = {Bio. J.},
  title        = {Understanding disparities in cancer prognosis: An extension of mediation analysis to the relative survival framework},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint modeling of interval counts of recurrent events and
death. <em>BIMJ</em>, <em>63</em>(2), 323–340. (<a
href="https://doi.org/10.1002/bimj.201900367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a recurrent event process is ended by death, this may imply dependent censoring if the two processes are associated. Such dependent censoring would have to be modeled to obtain a valid inference. Moreover, the dependence between the recurrence process and the terminal event may be the primary topic of interest. Joint frailty models for recurrent events and death, which include a separate dependence parameter, have been proposed for exactly observed recurrence times. However, in many situations, only the number of events experienced during consecutive time intervals are available. We propose a method for estimating a joint frailty model based on such interval counts and observed or independently censored terminal events. The baseline rates of the two processes are modeled by piecewise constant functions, and Gaussian quadrature is used to approximate the marginal likelihood. Covariates can be included in a proportional rates setting. The observation intervals for the recurrent event counts can differ between individuals. Furthermore, we adapt a score test for the association between recurrent events and death to the setting in which only individual interval counts are observed. We study the performance of both approaches via simulation studies, and exemplify the methodology in a biodemographic study of the dependence between budding rates and mortality in the species Eleutheria dichotoma .},
  archive      = {J_BIMJ},
  author       = {Marie Böhnstedt and Hein Putter and Aleksandra Dańko and Maciej J. Dańko and Jutta Gampe},
  doi          = {10.1002/bimj.201900367},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {323-340},
  shortjournal = {Bio. J.},
  title        = {Joint modeling of interval counts of recurrent events and death},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dynamic monitoring of the effects of adherence to
medication on survival in heart failure patients: A joint modeling
approach exploiting time-varying covariates. <em>BIMJ</em>,
<em>63</em>(2), 305–322. (<a
href="https://doi.org/10.1002/bimj.201900365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adherence to medication is the process by which patients take their drugs as prescribed, and represents an issue in pharmacoepidemiological studies. Poor adherence is often associated with adverse health conditions and outcomes, especially in case of chronic diseases such as heart failure (HF). This turns out in an increased request for health care services, and in a greater burden for the health care system. In recent years, there has been a substantial growth in pharmacotherapy research, aimed at studying effects and consequences of proper/improper adherence to medication both for the increasing awareness of the problem and for the pervasiveness of poor adherence among patients. However, the way adherence is computed and accounted for into predictive models is far from being informative as it may be. In fact, it is usually analyzed as a fixed baseline covariate, without considering its time-varying behavior. The purpose and novelty of this study is to define a new personalized monitoring tool exploiting time-varying definition of adherence to medication, within a joint modeling approach. In doing so, we are able to capture and quantify the association between the longitudinal process of dynamic adherence to medication with the long-term survival outcome. Another novelty of this approach consists of exploiting the potential of health care administrative databases in order to reconstruct the dynamics of drugs consumption through pharmaceutical administrative registries. In particular, we analyzed administrative data provided by Regione Lombardia - Healthcare Division related to patients hospitalized for HF between 2000 and 2012.},
  archive      = {J_BIMJ},
  author       = {Marta Spreafico and Francesca Ieva},
  doi          = {10.1002/bimj.201900365},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {305-322},
  shortjournal = {Bio. J.},
  title        = {Dynamic monitoring of the effects of adherence to medication on survival in heart failure patients: A joint modeling approach exploiting time-varying covariates},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drug sensitivity prediction with normal inverse gaussian
shrinkage informed by external data. <em>BIMJ</em>, <em>63</em>(2),
289–304. (<a href="https://doi.org/10.1002/bimj.201900371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In precision medicine, a common problem is drug sensitivity prediction from cancer tissue cell lines. These types of problems entail modelling multivariate drug responses on high-dimensional molecular feature sets in typically &gt;1000 cell lines. The dimensions of the problem require specialised models and estimation methods. In addition, external information on both the drugs and the features is often available. We propose to model the drug responses through a linear regression with shrinkage enforced through a normal inverse Gaussian prior. We let the prior depend on the external information, and estimate the model and external information dependence in an empirical-variational Bayes framework. We demonstrate the usefulness of this model in both a simulated setting and in the publicly available Genomics of Drug Sensitivity in Cancer data.},
  archive      = {J_BIMJ},
  author       = {Magnus M. Münch and Mark A. van de Wiel and Sylvia Richardson and Gwenaël G. R. Leday},
  doi          = {10.1002/bimj.201900371},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {289-304},
  shortjournal = {Bio. J.},
  title        = {Drug sensitivity prediction with normal inverse gaussian shrinkage informed by external data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new measure of treatment effect in clinical trials
involving competing risks based on generalized pairwise comparisons.
<em>BIMJ</em>, <em>63</em>(2), 272–288. (<a
href="https://doi.org/10.1002/bimj.201900354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis with competing risks, the treatment effect is typically expressed using cause-specific or subdistribution hazard ratios, both relying on proportional hazards assumptions. This paper proposes a nonparametric approach to analyze competing risks data based on generalized pairwise comparisons (GPC). GPC estimate the net benefit, defined as the probability that a patient from the treatment group has a better outcome than a patient from the control group minus the probability of the opposite situation, by comparing all pairs of patients taking one patient from each group. GPC allow using clinically relevant thresholds and simultaneously analyzing multiple prioritized endpoints. We show that under proportional subdistribution hazards, the net benefit for competing risks settings can be expressed as a decreasing function of the subdistribution hazard ratio, taking a value 0 when the latter equals 1. We propose four net benefit estimators dealing differently with censoring. Among them, the Péron estimator uses the Aalen–Johansen estimator of the cumulative incidence functions to classify the pairs for which the patient with the best outcome could not be determined due to censoring. We use simulations to study the bias of these estimators and the size and power of the tests based on the net benefit. The Péron estimator was approximately unbiased when the sample size was large and the censoring distribution&#39;s support sufficiently wide. With one endpoint, our approach showed a comparable power to a proportional subdistribution hazards model even under proportional subdistribution hazards. An application of the methodology in oncology is provided.},
  archive      = {J_BIMJ},
  author       = {Eva Cantagallo and Mickaël De Backer and Michal Kicinski and Brice Ozenne and Laurence Collette and Catherine Legrand and Marc Buyse and Julien Péron},
  doi          = {10.1002/bimj.201900354},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {272-288},
  shortjournal = {Bio. J.},
  title        = {A new measure of treatment effect in clinical trials involving competing risks based on generalized pairwise comparisons},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size calculation based on precision for pilot
sequential multiple assignment randomized trial (SMART). <em>BIMJ</em>,
<em>63</em>(2), 247–271. (<a
href="https://doi.org/10.1002/bimj.201900364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequential multiple assignment randomized trial (SMART) is a design used to develop dynamic treatment regimes (DTRs). Given that DTRs are generally less well researched, pilot SMART studies are often necessary. One challenge in pilot SMART is to determine the sample size such that it is small yet meaningfully informative for future full-fledged SMART. Here, we develop a precision-based approach, where the calculated sample size confines the marginal mean outcome of a DTR within a prespecified margin of error. The sample size calculations will be presented for two-stage SMARTs, and for various common outcome types.},
  archive      = {J_BIMJ},
  author       = {Xiaoxi Yan and Palash Ghosh and Bibhas Chakraborty},
  doi          = {10.1002/bimj.201900364},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {247-271},
  shortjournal = {Bio. J.},
  title        = {Sample size calculation based on precision for pilot sequential multiple assignment randomized trial (SMART)},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Doug altman: Driving critical appraisal and improvements in
the quality of methodological and medical research. <em>BIMJ</em>,
<em>63</em>(2), 226–246. (<a
href="https://doi.org/10.1002/bimj.202000053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doug Altman was a visionary leader and one of the most influential medical statisticians of the last 40 years. Based on a presentation in the “Invited session in memory of Doug Altman” at the 40th Annual Conference of the International Society for Clinical Biostatistics (ISCB) in Leuven, Belgium and our long-standing collaborations with Doug, we discuss his contributions to regression modeling, reporting, prognosis research, as well as some more general issues while acknowledging that we cannot cover the whole spectrum of Doug&#39;s considerable methodological output. His statement “ To maximize the benefit to society, you need to not just do research but do it well ” should be a driver for all researchers. To improve current and future research, we aim to summarize Doug&#39;s messages for these three topics.},
  archive      = {J_BIMJ},
  author       = {Willi Sauerbrei and Martin Bland and Stephen J. W. Evans and Richard D. Riley and Patrick Royston and Martin Schumacher and Gary S. Collins},
  doi          = {10.1002/bimj.202000053},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {226-246},
  shortjournal = {Bio. J.},
  title        = {Doug altman: Driving critical appraisal and improvements in the quality of methodological and medical research},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Contents: Biometrical journal 2’21. <em>BIMJ</em>,
<em>63</em>(2), 221–222. (<a
href="https://doi.org/10.1002/bimj.202170024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170024},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {221-222},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 2&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Cover picture: Biometrical journal 1’21. <em>BIMJ</em>,
<em>63</em>(1), NA. (<a
href="https://doi.org/10.1002/bimj.202170011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170011},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 1&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian hierarchical models with applications using RPeter
d. Congdon (2019). Second edition, CRC press, pages 580, ISBN:
9781498785754. <em>BIMJ</em>, <em>63</em>(1), 213–214. (<a
href="https://doi.org/10.1002/bimj.202000297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Christian Stock},
  doi          = {10.1002/bimj.202000297},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {213-214},
  shortjournal = {Bio. J.},
  title        = {Bayesian hierarchical models with applications using RPeter d. congdon (2019). second edition, CRC press, pages 580, ISBN: 9781498785754},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A quantile-slicing approach for sufficient dimension
reduction with censored responses. <em>BIMJ</em>, <em>63</em>(1),
201–212. (<a href="https://doi.org/10.1002/bimj.201900250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) that effectively reduces the predictor dimension in regression has been popular in high-dimensional data analysis. Under the presence of censoring, however, most existing SDR methods suffer. In this article, we propose a new algorithm to perform SDR with censored responses based on the quantile-slicing scheme recently proposed by Kim et al. First, we estimate the conditional quantile function of the true survival time via the censored kernel quantile regression (Shin et al.) and then slice the data based on the estimated censored regression quantiles instead of the responses. Both simulated and real data analysis demonstrate promising performance of the proposed method.},
  archive      = {J_BIMJ},
  author       = {Hyungwoo Kim and Seung Jun Shin},
  doi          = {10.1002/bimj.201900250},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {201-212},
  shortjournal = {Bio. J.},
  title        = {A quantile-slicing approach for sufficient dimension reduction with censored responses},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of normal distribution–based and nonparametric
decision limits on the GH-2000 score for detecting growth hormone misuse
(doping) in sport. <em>BIMJ</em>, <em>63</em>(1), 187–200. (<a
href="https://doi.org/10.1002/bimj.202000019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by the GH-2000 biomarker test, though the discussion is applicable to other diagnostic tests. The GH-2000 biomarker test has been developed as a powerful technique to detect growth hormone misuse by athletes, based on the GH-2000 score. Decision limits on the GH-2000 score have been developed and incorporated into the guidelines of the World Anti-Doping Agency (WADA). These decision limits are constructed, however, under the assumption that the GH-2000 score follows a normal distribution. As it is difficult to affirm the normality of a distribution based on a finite sample, nonparametric decision limits, readily available in the statistical literature, are viable alternatives. In this paper, we compare the normal distribution–based and nonparametric decision limits. We show that the decision limit based on the normal distribution may deviate significantly from the nominal confidence level or nominal FPR when the distribution of the GH-2000 score departs only slightly from the normal distribution. While a nonparametric decision limit does not assume any specific distribution of the GH-2000 score and always guarantees the nominal confidence level and FPR, it requires a much larger sample size than the normal distribution–based decision limit. Due to the stringent FPR of the GH-2000 biomarker test used by WADA, the sample sizes currently available are much too small, and it will take many years of testing to have the minimum sample size required, in order to use the nonparametric decision limits. Large sample theory about the normal distribution–based and nonparametric decision limits is also developed in this paper to help understanding their behaviours when the sample size is large.},
  archive      = {J_BIMJ},
  author       = {Wei Liu and Frank Bretz and Dankmar Böhning and Richard Holt and W. Böhning and Nishan Guha and Peter Sönksen and David Cowan},
  doi          = {10.1002/bimj.202000019},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {187-200},
  shortjournal = {Bio. J.},
  title        = {Comparison of normal distribution–based and nonparametric decision limits on the GH-2000 score for detecting growth hormone misuse (doping) in sport},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying subgroups of age and cohort effects in obesity
prevalence. <em>BIMJ</em>, <em>63</em>(1), 168–186. (<a
href="https://doi.org/10.1002/bimj.201900287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The obesity epidemic represents an important public health issue in the United States. Studying obesity trends across age groups over time helps to identify crucial relationships between the disease and medical treatment allowing for the development of effective prevention policies. We aim to define subgroups of age and cohort effects in obesity prevalence over time by considering an optimization approach applied to the age-period-cohort (APC) model. We consider a heterogeneous regression problem where the regression coefficients are age dependent and belong to subgroups with unknown grouping information. Using the APC model, we apply the alternating direction method of multipliers (ADMM) algorithm to develop a two-step algorithm for (1) subgrouping of cohort effects based on similar characteristics and (2) subgrouping age effects over time. The proposed clustering approach is illustrated for the United States population, aged 18–79, during the period 1990–2017.},
  archive      = {J_BIMJ},
  author       = {Tatjana Miljkovic and Xin Wang},
  doi          = {10.1002/bimj.201900287},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {168-186},
  shortjournal = {Bio. J.},
  title        = {Identifying subgroups of age and cohort effects in obesity prevalence},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The nonparametric behrens–fisher problem in partially
complete clustered data. <em>BIMJ</em>, <em>63</em>(1), 148–167. (<a
href="https://doi.org/10.1002/bimj.201900310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized trials or observational studies involving clustered units, the assumption of independence within clusters is not practical. Existing parametric or semiparametric methods assume specific dependence structures within a cluster. Furthermore, parametric model assumptions may not even be realistic when data are measured in a nonmetric scale as commonly happens, for example, in quality-of-life outcomes. In this paper, nonparametric effect-size measures for clustered data that allow meaningful and interpretable probabilistic comparisons of treatments or intervention programs will be introduced. The dependence among observations within a cluster can be arbitrary. Point estimators along with their asymptotic properties for computing confidence intervals and performing hypothesis test will be discussed. Small sample approximations that retain some of the optimal asymptotic behaviors will be presented. In our setup, some clusters may involve observations coming from both intervention groups (referred to as complete clusters), while others may contain observations from one group only (referred to as incomplete clusters). In deriving the asymptotic theories, we do not impose any relation in the rate of divergence of the numbers of complete and incomplete clusters. Simulations show favorable performance of the methods for arbitrary combinations of complete and incomplete clusters. The developed nonparametric methods are illustrated using data from a randomized trial of indoor wood smoke reduction to improve asthma symptoms and a cluster-randomized trial for smoking cessation.},
  archive      = {J_BIMJ},
  author       = {Yue Cui and Frank Konietschke and Solomon W. Harrar},
  doi          = {10.1002/bimj.201900310},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {148-167},
  shortjournal = {Bio. J.},
  title        = {The nonparametric Behrens–Fisher problem in partially complete clustered data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Use of correlated scrambling variables in quantitative
randomized response technique. <em>BIMJ</em>, <em>63</em>(1), 134–147.
(<a href="https://doi.org/10.1002/bimj.201900137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a new methodology that indicates that the use of correlated scrambling variables in the randomized response technique may play an important role in increasing the efficiency of an estimator of the population mean of a sensitive variable. Although it is clear analytically that the proposed estimator is more efficient than its existing competitors, we have investigated the magnitude of the gain in efficiency through simulation studies that involve both real secondary data from the health sciences, as well as artificial data. We also derive an estimator of the variance of the proposed estimator of mean and we study the coverage of 95% confidence intervals based on this variance estimator. An application using real primary data on smoking by university students is also included.},
  archive      = {J_BIMJ},
  author       = {Maryam Murtaza and Sarjinder Singh and Zawar Hussain},
  doi          = {10.1002/bimj.201900137},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {134-147},
  shortjournal = {Bio. J.},
  title        = {Use of correlated scrambling variables in quantitative randomized response technique},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An iterative method to protect the type i error rate in
bioequivalence studies under two-stage adaptive 2×2 crossover designs.
<em>BIMJ</em>, <em>63</em>(1), 122–133. (<a
href="https://doi.org/10.1002/bimj.201900388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bioequivalence studies are the pivotal clinical trials submitted to regulatory agencies to support the marketing applications of generic drug products. Average bioequivalence (ABE) is used to determine whether the mean values for the pharmacokinetic measures determined after administration of the test and reference products are comparable. Two-stage 2×2 crossover adaptive designs (TSDs) are becoming increasingly popular because they allow making assumptions on the clinically meaningful treatment effect and a reliable guess for the unknown within-subject variability. At an interim look, if ABE is not declared with an initial sample size, they allow to increase it depending on the estimated variability and to enroll additional subjects at a second stage, or to stop for futility in case of poor likelihood of bioequivalence. This is crucial because both parameters must clearly be prespecified in protocols, and the strategy agreed with regulatory agencies in advance with emphasis on controlling the overall type I error. We present an iterative method to adjust the significance levels at each stage which preserves the overall type I error for a wide set of scenarios which should include the true unknown variability value. Simulations showed adjusted significance levels higher than 0.0300 in most cases with type I error always below 5%, and with a power of at least 80%. TSDs work particularly well for coefficients of variation below 0.3 which are especially useful due to the balance between the power and the percentage of studies proceeding to stage 2. Our approach might support discussions with regulatory agencies.},
  archive      = {J_BIMJ},
  author       = {Eduard Molins and Detlew Labes and Helmut Schütz and Erik Cobo and Jordi Ocaña},
  doi          = {10.1002/bimj.201900388},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {122-133},
  shortjournal = {Bio. J.},
  title        = {An iterative method to protect the type i error rate in bioequivalence studies under two-stage adaptive 2×2 crossover designs},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proportion of treatment effect mediated by surrogate
endpoints. <em>BIMJ</em>, <em>63</em>(1), 105–121. (<a
href="https://doi.org/10.1002/bimj.202000119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the central aims in randomized clinical trials is to find well-validated surrogate endpoints to reduce the sample size and/or duration of trials. Clinical researchers and practitioners have proposed various surrogacy measures for assessing candidate surrogate endpoints. However, most existing surrogacy measures have the following shortcomings: (i) they often fall outside the range [0,1], (ii) they are imprecisely estimated, and (iii) they ignore the interaction associations between a treatment and candidate surrogate endpoints in the evaluation of the surrogacy level. To overcome these difficulties, we propose a new surrogacy measure, the proportion of treatment effect mediated by candidate surrogate endpoints (PMS), based on the decomposition of the treatment effect into direct, indirect, and interaction associations mediated by candidate surrogate endpoints. In addition, we validate the advantages of PMS through Monte Carlo simulations and the application to empirical data from ORIENT (the Olmesartan Reducing Incidence of Endstage Renal Disease in Diabetic Nephropathy Trial).},
  archive      = {J_BIMJ},
  author       = {Manabu Kuroki and Ryusei Shingaki and Yongming Qu},
  doi          = {10.1002/bimj.202000119},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {105-121},
  shortjournal = {Bio. J.},
  title        = {Proportion of treatment effect mediated by surrogate endpoints},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new mixed-effects regression model for the analysis of
zero-modified hierarchical count data. <em>BIMJ</em>, <em>63</em>(1),
81–104. (<a href="https://doi.org/10.1002/bimj.202000046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data sets are traditionally analyzed using the ordinary Poisson distribution. However, such a model has its applicability limited as it can be somewhat restrictive to handle specific data structures. In this case, it arises the need for obtaining alternative models that accommodate, for example, (a) zero-modification (inflation or deflation at the frequency of zeros), (b) overdispersion, and (c) individual heterogeneity arising from clustering or repeated (correlated) measurements made on the same subject. Cases (a)–(b) and (b)–(c) are often treated together in the statistical literature with several practical applications, but models supporting all at once are less common. Hence, this paper&#39;s primary goal was to jointly address these issues by deriving a mixed-effects regression model based on the hurdle version of the Poisson–Lindley distribution. In this framework, the zero-modification is incorporated by assuming that a binary probability model determines which outcomes are zero-valued, and a zero-truncated process is responsible for generating positive observations. Approximate posterior inferences for the model parameters were obtained from a fully Bayesian approach based on the Adaptive Metropolis algorithm. Intensive Monte Carlo simulation studies were performed to assess the empirical properties of the Bayesian estimators. The proposed model was considered for the analysis of a real data set, and its competitiveness regarding some well-established mixed-effects models for count data was evaluated. A sensitivity analysis to detect observations that may impact parameter estimates was performed based on standard divergence measures. The Bayesian -value and the randomized quantile residuals were considered for model diagnostics.},
  archive      = {J_BIMJ},
  author       = {Wesley Bertoli and Katiane S. Conceição and Marinho G. Andrade and Francisco Louzada},
  doi          = {10.1002/bimj.202000046},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {81-104},
  shortjournal = {Bio. J.},
  title        = {A new mixed-effects regression model for the analysis of zero-modified hierarchical count data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing inflated zeros in binomial regression models.
<em>BIMJ</em>, <em>63</em>(1), 59–80. (<a
href="https://doi.org/10.1002/bimj.202000028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binomial regression models are commonly applied to proportion data such as those relating to the mortality and infection rates of diseases. However, it is often the case that the responses may exhibit excessive zeros; in such cases a zero-inflated binomial (ZIB) regression model can be applied instead. In practice, it is essential to test if there are excessive zeros in the outcome to help choose an appropriate model. The binomial models can yield biased inference if there are excessive zeros, while ZIB models may be unnecessarily complex and hard to interpret, and even face convergence issues, if there are no excessive zeros. In this paper, we develop a new test for testing zero inflation in binomial regression models by directly comparing the amount of observed zeros with what would be expected under the binomial regression model. A closed form of the test statistic, as well as the asymptotic properties of the test, is derived based on estimating equations. Our systematic simulation studies show that the new test performs very well in most cases, and outperforms the classical Wald, likelihood ratio, and score tests, especially in controlling type I errors. Two real data examples are also included for illustrative purpose.},
  archive      = {J_BIMJ},
  author       = {Peng Ye and Yi Tang and Liuquan Sun and Wan Tang and Hua He},
  doi          = {10.1002/bimj.202000028},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {59-80},
  shortjournal = {Bio. J.},
  title        = {Testing inflated zeros in binomial regression models},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint estimation of case fatality rate of COVID-19 and power
of quarantine strategy performed in wuhan, china. <em>BIMJ</em>,
<em>63</em>(1), 46–58. (<a
href="https://doi.org/10.1002/bimj.202000116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the first case of COVID-19 confirmed in Wuhan, the capital of Hubei Province, China, in early December 2019, it has been found in more than 160 countries and caused over 11,000 deaths as of March 20, 2020. Wuhan, as the city where the epidemic first broke out, has made great sacrifices to block the possible transmission. In this research, we estimate the case fatality rate (CFR) of COVID-19 and quantify the effect of quarantine strategy utilized in Wuhan by developing an extended Susceptible–Infected–Recovered (SIR) model. The outcomes suggest that the CFR is 4.4% (95% CI [3.6%, 5.2%]) and the effect of the quarantine strategy is 99.3% (95% CI [99.2%, 99.5%]), which implies that such a method can significantly reduce the number of infections.},
  archive      = {J_BIMJ},
  author       = {Rongxiang Rui and Maozai Tian},
  doi          = {10.1002/bimj.202000116},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {46-58},
  shortjournal = {Bio. J.},
  title        = {Joint estimation of case fatality rate of COVID-19 and power of quarantine strategy performed in wuhan, china},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate maximum likelihood estimation for logistic
regression with covariate measurement error. <em>BIMJ</em>,
<em>63</em>(1), 27–45. (<a
href="https://doi.org/10.1002/bimj.202000024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nutritional epidemiology, dietary intake assessed with a food frequency questionnaire is prone to measurement error. Ignoring the measurement error in covariates causes estimates to be biased and leads to a loss of power. In this paper, we consider an additive error model according to the characteristics of the European Prospective Investigation into Cancer and Nutrition (EPIC)-InterAct Study data, and derive an approximate maximum likelihood estimation (AMLE) for covariates with measurement error under logistic regression. This method can be regarded as an adjusted version of regression calibration and can provide an approximate consistent estimator. Asymptotic normality of this estimator is established under regularity conditions, and simulation studies are conducted to empirically examine the finite sample performance of the proposed method. We apply AMLE to deal with measurement errors in some interested nutrients of the EPIC-InterAct Study under a sensitivity analysis framework.},
  archive      = {J_BIMJ},
  author       = {Zhiqiang Cao and Man Yu Wong},
  doi          = {10.1002/bimj.202000024},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {27-45},
  shortjournal = {Bio. J.},
  title        = {Approximate maximum likelihood estimation for logistic regression with covariate measurement error},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian regularization for flexible baseline hazard
functions in cox survival models. <em>BIMJ</em>, <em>63</em>(1), 7–26.
(<a href="https://doi.org/10.1002/bimj.201900211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Bayesian methods for Cox models specify a model for the baseline hazard function. Parametric approaches generally provide monotone estimations. Semi-parametric choices allow for more flexible patterns but they can suffer from overfitting and instability. Regularization methods through prior distributions with correlated structures usually give reasonable answers to these types of situations. We discuss Bayesian regularization for Cox survival models defined via flexible baseline hazards specified by a mixture of piecewise constant functions and by a cubic B-spline function. For those “semi-parametric” proposals, different prior scenarios ranging from prior independence to particular correlated structures are discussed in a real study with microvirulence data and in an extensive simulation scenario that includes different data sample and time axis partition sizes in order to capture risk variations. The posterior distribution of the parameters was approximated using Markov chain Monte Carlo methods. Model selection was performed in accordance with the deviance information criteria and the log pseudo-marginal likelihood. The results obtained reveal that, in general, Cox models present great robustness in covariate effects and survival estimates independent of the baseline hazard specification. In relation to the “semi-parametric” baseline hazard specification, the B-splines hazard function is less dependent on the regularization process than the piecewise specification because it demands a smaller time axis partition to estimate a similar behavior of the risk.},
  archive      = {J_BIMJ},
  author       = {Elena Lázaro and Carmen Armero and Danilo Alvares},
  doi          = {10.1002/bimj.201900211},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {7-26},
  shortjournal = {Bio. J.},
  title        = {Bayesian regularization for flexible baseline hazard functions in cox survival models},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Contents: Biometrical journal 1’21. <em>BIMJ</em>,
<em>63</em>(1), 5–6. (<a
href="https://doi.org/10.1002/bimj.202170014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202170014},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {5-6},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 1&#39;21},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
