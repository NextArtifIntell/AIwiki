<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietcv---49">IETCV - 49</h2>
<ul>
<li><details>
<summary>
(2021). Complete/incomplete multi-view subspace clustering via soft
block-diagonal-induced regulariser. <em>IETCV</em>, <em>15</em>(8),
618–632. (<a href="https://doi.org/10.1049/cvi2.12077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel multi-view soft block diagonal representation framework for clustering complete and incomplete multi-view data. First, given that the multi-view self-representation model offers better performance in exploring the intrinsic structure of multi-view data, it can be nicely adopted to individually construct a graph for each view. Second, since an ideal block diagonal graph is beneficial for clustering, a ‘soft’ block diagonal affinity matrix is constructed by fusing multiple previous graphs. The soft diagonal block regulariser encourages a matrix to approximately have (not exactly) diagonal blocks, where is the number of clusters. This strategy adds robustness to noise and outliers. Third, to handle incomplete multi-view data, multiple indicator matrices are utilised, which can mark the position of missing elements of each view. Finally, the alternative direction of multipliers algorithm is employed to optimise the proposed model, and the corresponding algorithm complexity and convergence are also analysed. Extensive experimental results on several real-world datasets achieve the best performance among the state-of-the-art complete and incomplete clustering methods, which proves the effectiveness of the proposed methods.},
  archive      = {J_IETCV},
  author       = {Yongli Hu and Cuicui Luo and Boyue Wang and Junbin Gao and Yanfeng Sun and Baocai Yin},
  doi          = {10.1049/cvi2.12077},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {618-632},
  shortjournal = {IET Comput. Vis.},
  title        = {Complete/incomplete multi-view subspace clustering via soft block-diagonal-induced regulariser},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot learning with relation propagation and constraint.
<em>IETCV</em>, <em>15</em>(8), 608–617. (<a
href="https://doi.org/10.1049/cvi2.12074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous deep learning methods usually required large-scale annotated data, which is computationally exhaustive and unrealistic in certain scenarios. Therefore, few-shot learning, where only a few annotated training images are available for training, has attracted increasing attention these days, showing huge potential in practical applications, such as portable equipment or security inspection, and so on. However, current few-shot learning methods usually neglect the valuable semantic correlations between samples, thereby failing in extracting discriminating relations to achieve accurate predictive results. In this work, extending on a recent state-of-the-art few-shot learning method, transductive relation-propagation network (TRPN), which considers the correlations between training samples, a constrained relation-propagation network is proposed to further regularise the distilled correlations and thus achieve favourable few-shot classification performance. The proposed framework contains three main components, namely preprocess module, relational propagation module, and relation constraint module. First, sample features are extracted and a relation graph node is constructed by treating the relation of each support–query pair as a graph node in the preprocess module. After that, in the relation propagation module (RPM), the valuable information of support–query pairs is modelled and propagated to directly generate the relational representations for further prediction. Then, a relation constraint module is introduced to regularise the relational representations and make it consistent with the ground-truth relations as much as possible. With the guidance of the effective RPM and relation constraint module, the relational representations of the support–query pairs are distinguishable and thus can achieve accurate predictive results. Comprehensive experiments conducted on widely used benchmarks validate the effectiveness of our method compared to state-of-the-art few-shot classification approaches.},
  archive      = {J_IETCV},
  author       = {Huiyun Gong and Shuo Wang and Xiaowei Zhao and Yifan Yan and Yuqing Ma and Wei Liu and Xianglong Liu},
  doi          = {10.1049/cvi2.12074},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {608-617},
  shortjournal = {IET Comput. Vis.},
  title        = {Few-shot learning with relation propagation and constraint},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN-combined graph residual network with multilevel feature
fusion for hyperspectral image classification. <em>IETCV</em>,
<em>15</em>(8), 592–607. (<a
href="https://doi.org/10.1049/cvi2.12073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of graph convolutional networks (GCN) in hyperspectral image (HSI) classification has become a promising method, thanks to its flexible convolution operation in any irregular image region. For the classification of HSI, GCN can extract more superpixel-level features with a topological structure, in comparison to the traditional convolutional neural networks (CNNs) using fixed square kernels distilling pixel-level features. To fully leverage the different levels of features, this study proposes a novel deep network referred to as a CNN-combined graph residual network ( GRN), which integrates the multilevel graph residual module and spectral-spatial features continuous learning module. During the extraction of topology information using the former module, HSI pixels are divided into superpixels and served as input nodes of the module to reduce the computational complexity and obtain the multilevel spatial relevance between adjacent superpixels. Besides, for the latter module, the spectral-spatial features are learnt continuously, which could obtain the finer pixel-level features. Finally, the captured spectral-spatial features of different levels are concatenated. This strategy could not only adequately utilize the correlation and difference of adjacent spatial but also obtain the finer and more valuable spectral-spatial information, which makes a significant boost in the HSI classification. Additionally, the experiment results demonstrate the superiority and availability of the GRN on three benchmark datasets of HSI, compared with the state-of-the-art methods for the classification of HSI.},
  archive      = {J_IETCV},
  author       = {Wenhui Guo and Guixun Xu and Weifeng Liu and Baodi Liu and Yanjiang Wang},
  doi          = {10.1049/cvi2.12073},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {592-607},
  shortjournal = {IET Comput. Vis.},
  title        = {CNN-combined graph residual network with multilevel feature fusion for hyperspectral image classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental multi-view correlated feature learning based on
non-negative matrix factorisation. <em>IETCV</em>, <em>15</em>(8),
573–591. (<a href="https://doi.org/10.1049/cvi2.12067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, large amounts of data from multiple sources come in the form of streams. This makes multi-view feature learning cost much time when new instances rise incrementally. Dealing with these growing multi-view data becomes a challenging problem. Some single-view methods focus on processing the data dynamically, but they are not suitable for multi-view data. Some online multi-view methods are proposed to tackle it, but they ignore the influence of uncorrelated items in each view. Therefore, in this study, the authors propose a new algorithm, called Incremental Multi-view Correlated Feature Learning (IMCFL) based on non-negative matrix factorisation, to learn the common feature across views. By separating uncorrelated items of new instances and constructing incremental joint learning of correlated and uncorrelated features, the proposed IMCFL can eliminate the influence of uncorrelated information in the individual view and improve the effectiveness of incremental multi-view common feature learning. Extensive experiments on real-world datasets confirm its superiority by comparing it with other state-of-the-art incremental and non-incremental methods.},
  archive      = {J_IETCV},
  author       = {Liang Zhao and Tao Yang and Jie Zhang and Zhikui Chen},
  doi          = {10.1049/cvi2.12067},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {573-591},
  shortjournal = {IET Comput. Vis.},
  title        = {Incremental multi-view correlated feature learning based on non-negative matrix factorisation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid attention mechanism for few-shot relational learning
of knowledge graphs. <em>IETCV</em>, <em>15</em>(8), 561–572. (<a
href="https://doi.org/10.1049/cvi2.12066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot knowledge graph (KG) reasoning is the main focus in the field of knowledge graph reasoning. In order to expand the application fields of the knowledge graph, a large number of studies are based on a large number of training samples. However, we have learnt that there are actually many missing relationships or entities in the knowledge graph, and in most cases, there are not many training instances when implementing new relationships. To tackle it, in this study, the authors aim to predict a new entity given few reference instances, even only one training instance. A few-shot learning framework based on a hybrid attention mechanism is proposed. The framework employs traditional embedding models to extract knowledge, and uses an attenuated attention network and a self-attention mechanism to obtain the hidden attributes of entities. Thus, it can learn a matching metric by considering both the learnt embeddings and one-hop graph structures. The experimental results present that the model has achieved significant performance improvements on the NELL-One and Wiki-One datasets.},
  archive      = {J_IETCV},
  author       = {Ruixin Ma and Zeyang Li and Fangqing Guo and Liang Zhao},
  doi          = {10.1049/cvi2.12066},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {561-572},
  shortjournal = {IET Comput. Vis.},
  title        = {Hybrid attention mechanism for few-shot relational learning of knowledge graphs},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced three-dimensional u-net with graph-based refining
for segmentation of gastrointestinal stromal tumours. <em>IETCV</em>,
<em>15</em>(8), 549–560. (<a
href="https://doi.org/10.1049/cvi2.12051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gastrointestinal stromal tumour (GIST) is a common mesenchymal tumour that lacks specificity of clinical manifestations. Therefore, preoperative accurate localization and accurate prediction of tumour risk are of important clinical value. At present, the diagnosis of GIST relies mainly on manual annotation of CT by professional doctors, which is inefficient and affected by subjective factors. A GIST segmentation algorithm is proposed based on a convolutional neural network to fuse multi-scale features. The algorithm is applied to GIST segmentation with an improved 3-D U-Net method. Skip connections are introduced between encoders and decoders at different layers to account for the obvious differences in tumour size between different cases, which increases the path of information transmission in the network and solves the problem that U-Net is too weak to simultaneously extract the features of different scales. In addition, due to the difficulty of tumour labelling and the correlation between small intestine segmentation and GIST segmentation, the model of small intestine segmentation is transferred to the model of GIST segmentation. Experiments show that the proposed method achieves better performance than that of the traditional U-Net. Finally, the graph neural network is introduced to reduce the repetitive work of doctors in refining the segmentation results.},
  archive      = {J_IETCV},
  author       = {Qiong Wang and Zhipeng Li and Wanqing Zhao and Hao Wu and Fei Xie and Ziyu Guan and Wei Zhao},
  doi          = {10.1049/cvi2.12051},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {549-560},
  shortjournal = {IET Comput. Vis.},
  title        = {Enhanced three-dimensional U-net with graph-based refining for segmentation of gastrointestinal stromal tumours},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning view-invariant features using stacked autoencoder
for skeleton-based gait recognition. <em>IETCV</em>, <em>15</em>(7),
527–545. (<a href="https://doi.org/10.1049/cvi2.12050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human gait recognition in a multicamera environment is a challenging task in biometrics because of the presence of the large pose and variations in illumination among different views. In this work, to address the problem of variations in view, we present a novel stacked autoencoder for learning discriminant view-invariant gait representations. Our autoencoder can efficiently and progressively translate skeleton joint coordinates from any arbitrary view to a common canonical view without requiring the prior estimation of the view angle or covariate type and without losing temporal information. Then, we construct a discriminative gait feature vector by fusing the encoded features with two other spatiotemporal gait features to feed into the main recurrent neural network. Experimental evaluations of the challenging CASIA A and CASIA B gait datasets demonstrate that the proposed approach outperformed other state-of-the-art methods on single-view gait recognition. In particular, the proposed method achieved 46.31% and 33.86% average correct class recognition on probe set ProbeBG and ProbeCL, respectively, of the CASIA B dataset while considering the view variation; this is 0.3% and 30.68% higher than previous best-performing methods. Furthermore, in cross-view recognition, our method shows better results over other state-of-the-art methods when the view-angle variation is large than 36°.},
  archive      = {J_IETCV},
  author       = {Md Mahedi Hasan and Hossen Asiful Mustafa},
  doi          = {10.1049/cvi2.12050},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {527-545},
  shortjournal = {IET Comput. Vis.},
  title        = {Learning view-invariant features using stacked autoencoder for skeleton-based gait recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep analysis on high-resolution dermoscopic image
classification. <em>IETCV</em>, <em>15</em>(7), 514–526. (<a
href="https://doi.org/10.1049/cvi2.12048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been broadly employed in dermoscopic image analysis, mainly as a result of the large amount of data gathered by the International Skin Imaging Collaboration (ISIC). As in many other medical imaging domains, state-of-the-art methods take advantage of architectures developed for other tasks, frequently assuming full transferability between enormous sets of natural images (e.g. ImageNet) and dermoscopic images, which is not always the case. A comprehensive analysis on the effectiveness of state-of-the-art deep learning techniques when applied to dermoscopic image analysis is provided. To achieve this goal, the authors consider several CNNs architectures and analyse how their performance is affected by the size of the network, image resolution, data augmentation process, amount of available data, and model calibration. Moreover, taking advantage of the analysis performed, a novel ensemble method to further increase the classification accuracy is designed. The proposed solution achieved the third best result in the 2019 official ISIC challenge, with an accuracy of 0.593.},
  archive      = {J_IETCV},
  author       = {Federico Pollastri and Mario Parreño and Juan Maroñas and Federico Bolelli and Roberto Paredes and Daniel Ramos and Costantino Grana},
  doi          = {10.1049/cvi2.12048},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {514-526},
  shortjournal = {IET Comput. Vis.},
  title        = {A deep analysis on high-resolution dermoscopic image classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DualPathGAN: Facial reenacted emotion synthesis.
<em>IETCV</em>, <em>15</em>(7), 501–513. (<a
href="https://doi.org/10.1049/cvi2.12047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial reenactment has developed rapidly in recent years, but few methods have been built upon reenacted face in videos. Facial-reenacted emotion synthesis can make the process of facial reenactment more practical. A facial-reenacted emotion synthesis method is proposed that includes a dual-path generative adversarial network (GAN) for emotion synthesis and a residual-mask network to impose structural restrictions to preserve the mouth shape of the source person. To train the dual-path GAN more effectively, a learning strategy based on separated discriminators is proposed. The method is trained and tested on a very challenging imbalanced dataset to evaluate the ability to deal with complex practical scenarios. Compared with general emotion synthesis methods, the proposed method can generate more realistic facial emotion synthesised images or videos with higher quality while retaining the expression contents of the original videos. The DualPathGAN achieves a Fréchet inception distance (FID) score of 9.20, which is lower than the FID score of 11.37 achieved with state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Jiahui Kong and Haibin Shen and Kejie Huang},
  doi          = {10.1049/cvi2.12047},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {501-513},
  shortjournal = {IET Comput. Vis.},
  title        = {DualPathGAN: Facial reenacted emotion synthesis},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JCa2Co: A joint cascade convolution coding network based on
fuzzy regional characteristics for infrared and visible image fusion.
<em>IETCV</em>, <em>15</em>(7), 487–500. (<a
href="https://doi.org/10.1049/cvi2.12046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To facilitate the extraction of source image information, and preserve the consistency of heterogeneous regional features, a multiscale image fusion method based on a joint cascaded convolutional coding network (JCa2Co) is proposed. The JCa2Co network can retain vast quantities of information from source images in a multiscale perspective. The approach includes an encoder, a fusion layer and decoder. In the fusion layer, the Fuzzy Regional Characteristics (FRC) scheme is considered, and the multiscale feature maps are extracted from image subregions to ensure regional image consistency. Firstly, a joint cascaded encoder is used to extract multiscale features of the source image, in which the output of each layer is connected to every other layer. The fusion layer based on FRC is then performed to fuse each scale feature. Finally, the fused image is reconstructed by the decoder. In addition, to verify the regional consistency of the fused image, a regional consistency measure is proposed. Experiments are performed on the TNO Image Fusion Database. The experimental results exhibit that the proposed JCa2Co method has better comprehensive performance than the eight state-of-the-art fusion methods. Moreover, it can effectively integrate meaningful information in infrared and visible images and has excellent performance in objective evaluation and visual quality, which is beneficial to target recognition and tracking.},
  archive      = {J_IETCV},
  author       = {Zhao Xu and Gang Liu and Gang Xiao and Lili Tang and Yanhui Li},
  doi          = {10.1049/cvi2.12046},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {487-500},
  shortjournal = {IET Comput. Vis.},
  title        = {JCa2Co: A joint cascade convolution coding network based on fuzzy regional characteristics for infrared and visible image fusion},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale capsule generative adversarial network for snow
removal. <em>IETCV</em>, <em>15</em>(7), 474–486. (<a
href="https://doi.org/10.1049/cvi2.12038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snowflakes captured on photos may severely decrease the visual quality and cause difficulties for vision analysis systems. Most noise removal frameworks are designed for de-raining or de-hazing, regarding rain or haze as translucent masks on clean images. However, snowflakes are different from them in terms of sizes, shapes, transparencies and floating trajectories, which decreases the performance of de-raining or de-hazing models in processing snowy images. In this work, we propose an effective multi-scale generative adversarial network framework for single-image snow removal, which is built with a multi-scale structure to identify various scales of snowflakes and a capsule-based structure to fuse the features extracted from the multi-scale encoding branches, so that different scaled features could be summarised and learnt by a joint framework. The overall framework is supervised by a weighted joint loss with an iterative training procedure to keep the training stability for the multi-branch-based structure. The experimental results demonstrate that our model outperforms the state-of-the-art comparisons.},
  archive      = {J_IETCV},
  author       = {Fei Yang and Jialu Zhang and Qian Zhang},
  doi          = {10.1049/cvi2.12038},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {474-486},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-scale capsule generative adversarial network for snow removal},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fully automated glioma tumour segmentation using anatomical
symmetry plane detection in multimodal brain MRI. <em>IETCV</em>,
<em>15</em>(7), 463–473. (<a
href="https://doi.org/10.1049/cvi2.12035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic brain abnormality detection is a major challenge in medical image processing. Manual lesion delineation techniques are susceptible to subjective errors, and therefore, computer aided preliminary screening of a lesion is necessary. This study introduces an efficient and automated algorithm based on the symmetry of the brain structures in the two hemispheres for brain tumour segmentation using multimodal brain magnetic resonoce imaging. Symmetry is a vital clue for determining intensity-based lesion difference in the two hemispheres of brain. A reliable method is proposed for extracting the cancerous region in order to improve the speed and accuracy of brain tumour segmentation. First, a symmetry plane is detected and then through features extracted from both sides of the brain, a similarity measure for comparing the hemisphere is defined. The cancerous region is extracted using similarity measurement, and the accuracy is improved using postprocessing operation. This algorithm is evaluated against the BRATS datasets including high- and low-grade glioma brain tumours. The performance indices are calculated and comparative analysis is implemented as well. Experimental results demonstrate accuracy close to manual lesion demarcation with performance indices.},
  archive      = {J_IETCV},
  author       = {Zeynab Barzegar and Mansour Jamzad},
  doi          = {10.1049/cvi2.12035},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {463-473},
  shortjournal = {IET Comput. Vis.},
  title        = {Fully automated glioma tumour segmentation using anatomical symmetry plane detection in multimodal brain MRI},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale fully convolutional network-based approach for
multilingual character segmentation. <em>IETCV</em>, <em>15</em>(6),
449–461. (<a href="https://doi.org/10.1049/cvi2.12034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Character segmentation is a challenging task for optical character recognition systems. Traditional methods usually utilize rule-based algorithms but most of them are not applicable in modern intelligent recognition applications that require high accuracy. It is especially the case for text containing Eastern Asian language characters with complex pictograph structures, such as Chinese. To alleviate this problem, this study proposes an encoder–decoder structure-based multiscale fully convolutional network (MSFCN) model for optical character segmentation. Comparing with other methods, MSFCN can not only effectively extract semantic details from images but also exploit boundary information of intervals between characters, thereby distinguishing characters from a background in pixel level. Extensive experiments have been conducted on two benchmark data sets of ICDAR2013 and MLCS. Obtained results prove that MSFCN achieves state-of-the-art segmentation performance and indicated its practical application value.},
  archive      = {J_IETCV},
  author       = {Chao Yu and Jin Liu and Yunhui Li},
  doi          = {10.1049/cvi2.12034},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {449-461},
  shortjournal = {IET Comput. Vis.},
  title        = {Multiscale fully convolutional network-based approach for multilingual character segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Micro-expression recognition by two-stream difference
network. <em>IETCV</em>, <em>15</em>(6), 440–448. (<a
href="https://doi.org/10.1049/cvi2.12030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial micro-expression is a superposition of micro-expression features and identity information of a subject. For the problem of identity information interference in micro-expression recognition, this study proposes a new method for facial micro-expression recognition by de-identity information, called two-stream difference network (TSDN). First, a two-stream encoder-decoder network is trained by a convolutional neural network, where the input of the micro-expression stream is a micro-expression image, and the identity stream is a facial identity image. The micro-expression image is the apex image, and the identity image is the onset image in the micro-expression sequence. The identity information and micro-expression features are recorded in the intermediate layer of the micro-expression stream, while the intermediate layer of the identity stream contains only the identity information of a subject. Then, the identity information is removed by the difference network, but micro-expression features are stored in the intermediate layer of the micro-expression stream. Given the sequence of the micro-expressions, the TSDN model of de-identity information learns the difference that stores in the expression stream. Two public spontaneous facial micro-expression data sets (SMIC and CASME II) are employed in our experiments. The experiment results show that our model can achieve a superior performance in micro-expression recognition.},
  archive      = {J_IETCV},
  author       = {Hang Pan and Lun Xie and Juan Li and Zeping Lv and Zhiliang Wang},
  doi          = {10.1049/cvi2.12030},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {440-448},
  shortjournal = {IET Comput. Vis.},
  title        = {Micro-expression recognition by two-stream difference network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coconut trees detection and segmentation in aerial imagery
using mask region-based convolution neural network. <em>IETCV</em>,
<em>15</em>(6), 428–439. (<a
href="https://doi.org/10.1049/cvi2.12028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food resources face severe damages under extraordinary situations of catastrophes such as earthquakes, cyclones, and tsunamis. Under such scenarios, speedy assessment of food resources from agricultural land is critical as it supports aid activity in the disaster-hit areas. In this article, a deep learning approach was presented for the detection and segmentation of coconut trees in aerial imagery provided through the AI competition organised by the World Bank in collaboration with OpenAerialMap and WeRobotics . Masked Region-based Convolution Neural Network (Mask R-CNN) approach was used for identification and segmentation of coconut trees. For the segmentation task, Mask R-CNN model with ResNet50 and ResNet101 based architectures was used. Several experiments with different configuration parameters were performed and the best configuration for the detection of coconut trees with more than 90% confidence factor was reported. For the purpose of evaluation, Microsoft COCO dataset evaluation metric namely mean average precision (mAP) was used.An overall 91% mean average precision for coconut trees’ detection was achieved.},
  archive      = {J_IETCV},
  author       = {Muhammad Shakaib Iqbal and Hazrat Ali and Son N. Tran and Talha Iqbal},
  doi          = {10.1049/cvi2.12028},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {428-439},
  shortjournal = {IET Comput. Vis.},
  title        = {Coconut trees detection and segmentation in aerial imagery using mask region-based convolution neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing feature fusion with spatial aggregation and
channel fusion for semantic segmentation. <em>IETCV</em>,
<em>15</em>(6), 418–427. (<a
href="https://doi.org/10.1049/cvi2.12026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is crucial to the autonomous driving, as an accurate recognition and location of the surrounding scenes can be provided for the street scenes understanding task. Many existing segmentation networks usually fuse high-level and low-level features to boost segmentation performance. However, the simple fusion may impose a limited performance improvement because of the gap between high-level and low-level features. To alleviate this limitation, we respectively propose spatial aggregation and channel fusion to bridge the gap. Our implementation, inspired by the attention mechanism, consists of two steps: (1) Spatial aggregation relies on the proposed pyramid spatial context aggregation module to capture spatial similarities to enhance the spatial representation of high-level features, which is more effective for the latter fusion. (2) Channel fusion relies on the proposed attention-based channel fusion module to weight channel maps on different levels to enhance the fusion. In addition, the complete network with U-shape structure is constructed. A series of ablation experiments are conducted to demonstrate the effectiveness of our designs, and the network achieves mIoU score of 81.4% on Cityscapes test dataset and 84.6% on PASCALVOC 2012 test dataset.},
  archive      = {J_IETCV},
  author       = {Jie Hu and Huifang Kong and Lei Fan and Jun Zhou},
  doi          = {10.1049/cvi2.12026},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {418-427},
  shortjournal = {IET Comput. Vis.},
  title        = {Enhancing feature fusion with spatial aggregation and channel fusion for semantic segmentation},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-factor joint normalisation for face recognition in the
wild. <em>IETCV</em>, <em>15</em>(6), 405–417. (<a
href="https://doi.org/10.1049/cvi2.12025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has become very challenging in unconstrained conditions due to strong intra-personal variations, such as large pose changes. Face normalisation can help to resolve these problems and effectively improve the face recognition performance in unconstrained conditions by converting non-frontal faces to frontal ones. However, there are other complex facial variations in addition to pose, such as illumination and expression, which will also influence face recognition performance. The authors propose a well-designed generative adversarial network-based multi-factor joint normalisation network (MFJNN) to normalise multiple factors simultaneously. First, a multi-encoder generator and a feature fusion strategy are designed and implemented in the MFJNN to realise the joint normalisation of multiple factors in addition to pose. Second, a convolutional neural network-based (CNN-based) network is applied in the MFJNN, which allows the MFJNN to simultaneously realise image synthesis and facial representation learning. Moreover, an identity perceptive loss is introduced based on the CNN-based network to produce reliable identity-preserving features of the input face images. The experimental results demonstrate that the proposed method can synthesise multi-factor normalisation results with identity preservation and effectively improve the face recognition performance.},
  archive      = {J_IETCV},
  author       = {Yanfei Liu and Junhua Chen},
  doi          = {10.1049/cvi2.12025},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {405-417},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-factor joint normalisation for face recognition in the wild},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust 3D face reconstruction from single noisy depth image
through semantic consistency. <em>IETCV</em>, <em>15</em>(6), 393–404.
(<a href="https://doi.org/10.1049/cvi2.12024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the 3D face reconstruction and semantic annotation from a single-view noisy depth image. A deep neural network-based coarse-to-fine framework is presented to take advantage of 3D morphable model (3DMM) regression and per-vertex geometry refinement. The low-dimensional subspace coefficients of the 3DMM initialize the global facial geometry, being prone to be over-smooth because of the low-pass characteristics of the shape subspace. The proposed geometry refinement subnetwork predicts per-vertex displacements to enrich local details, which is learned from unlabelled noisy depth images based on the registration-like loss. In order to guarantee the semantic correspondence between the resultant 3D face and the depth image, a semantic consistency constraint is introduced to adapt an annotation model learned from the synthetic data to real noisy depth images. The resultant depth annotations are required to be consistent with the label propagation from the coarse and refined parametric 3D faces. The proposed coarse-to-fine reconstruction scheme and the semantic consistency constraint are evaluated on the depth-based 3D face reconstruction and semantic annotation. The series of experiments demonstrate that the proposed approach achieves the performance improvements over compared methods regarding 3D face reconstruction and depth image annotation.},
  archive      = {J_IETCV},
  author       = {Peixin Li and Yuru Pei and Yicheng Zhong and Yuke Guo and Hongbin Zha},
  doi          = {10.1049/cvi2.12024},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {393-404},
  shortjournal = {IET Comput. Vis.},
  title        = {Robust 3D face reconstruction from single noisy depth image through semantic consistency},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D object recognition with a linear time-varying system of
overlay layers. <em>IETCV</em>, <em>15</em>(5), 380–391. (<a
href="https://doi.org/10.1049/cvi2.12029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition is a challenging task in computer vision with numerous applications. The challenge is in selecting appropriate robust features with tolerable computing costs. Feature learning attempts to solve the feature extraction problem through a learning process using various samples of the objects. This research proposes a two-stage optimization framework to identify the structure of a first-order linear non-homogeneous difference equation which is a linear time-varying system of overlay layers (LtvoL) that construct an image. The first stage consists of the determination of a finite set of impulses, called overlay layers, by the application of a genetic algorithm. The second stage defines the coefficients of the corresponding difference equation derived from L 2 regularization. Classification of the test images is possible by a novel process exclusively designed for this model. Experiments on the Washington RGB-D dataset and ETH-80 show promising results which are comparable to those of state-of-the-art methods for RGB-D-based object recognition.},
  archive      = {J_IETCV},
  author       = {Mohammad Sohrabi Nasrabadi and Reza Safabakhsh},
  doi          = {10.1049/cvi2.12029},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {380-391},
  shortjournal = {IET Comput. Vis.},
  title        = {3D object recognition with a linear time-varying system of overlay layers},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SASO: Joint 3D semantic-instance segmentation via
multi-scale semantic association and salient point clustering
optimization. <em>IETCV</em>, <em>15</em>(5), 366–379. (<a
href="https://doi.org/10.1049/cvi2.12033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jointly performing semantic and instance segmentation of 3D point cloud remains a challenging task. In this work, a novel framework called joint 3D semantic-instance segmentation via multi-scale Semantic Association and Salient point clustering Optimization was proposed to tackle this problem. Inspired by the inherent correlation among objects in semantic space, a Multi-scale Semantic Association (MSA) module to explore the constructive effect of the context information for semantic segmentation is designed. For instance, segmentation, different from previous works utilising clustering only in inference procedure, a Salient Point Clustering Optimization (SPCO) module is put forward to introduce the clustering algorithm into the training phase, which impels the network to focus on points that are difficult to be distinguished. Furthermore, affected by the inherent structure of indoor scenes, the problem of uneven distribution of categories has rarely been considered in the previous work, but it significantly limits the performance of 3D scene perception. To address the issue, an adaptive Water Filling Sampling (WFS) algorithm to balance the category distribution of training data is presented. Extensive experiments on a variety of changing datasets show that the authors’ method outperforms the state-of-the-art methods in both tasks of semantic segmentation and instance segmentation.},
  archive      = {J_IETCV},
  author       = {Jingang Tan and Lili Chen and Kangru Wang and Jiamao Li and Xiaolin Zhang},
  doi          = {10.1049/cvi2.12033},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {366-379},
  shortjournal = {IET Comput. Vis.},
  title        = {SASO: Joint 3D semantic-instance segmentation via multi-scale semantic association and salient point clustering optimization},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control the number of skip-connects to improve robustness of
the NAS algorithm. <em>IETCV</em>, <em>15</em>(5), 356–365. (<a
href="https://doi.org/10.1049/cvi2.12036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the gradient-based neural architecture search has made remarkable progress with the characteristics of high efficiency and fast convergence. However, two common problems in the gradient-based NAS algorithms are found. First, with the increase in the raining time, the NAS algorithm tends to skip-connect operation, leading to performance degradation and instability results. Second, another problem is no reasonable allocation of computing resources on valuable candidate network models. The above two points lead to the difficulty in searching the optimal sub-network and poor stability. To address them, the trick of pre-training the super-net is applied, so that each operation has an equal opportunity to develop its strength, which provides a fair competition condition for the convergence of the architecture parameters. In addition, a skip-controller is proposed to ensure each sampled sub-network with an appropriate number of skip-connects. The experiments were performed on three mainstream datasets CIFAR-10, CIFAR-100 and ImageNet, in which the improved method achieves comparable results with higher accuracy and stronger robustness.},
  archive      = {J_IETCV},
  author       = {Bao Feng Zhang and Guo Qiang Zhou},
  doi          = {10.1049/cvi2.12036},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {356-365},
  shortjournal = {IET Comput. Vis.},
  title        = {Control the number of skip-connects to improve robustness of the NAS algorithm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive learning cost-sensitive convolutional neural
network. <em>IETCV</em>, <em>15</em>(5), 346–355. (<a
href="https://doi.org/10.1049/cvi2.12027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world classification often encounters a problem called class imbalance. When the data of some classes are redundant than that of other classes, traditional classifiers usually bias their decision boundaries to the redundant majority classes. Most proposed cost-sensitive strategies often ignore the hard-to-learn examples or have a large amount of hyper-parameters. This article proposes an adaptive learning cost-sensitive convolutional neural network to solve this problem. During the training process, the proposed method embeds a class-dependent cost to each class in the global error, making the decision boundary bias to the minority classes. Meanwhile, a distribution weight is assigned to each example to enhance the learning of the hard-to-learn examples. Both the class-dependent costs and distribution weights are learnt automatically in the net. This cost-sensitive approach makes the algorithm focus on the examples in the minority classes as well as the hard-to-learn examples in each class. Besides, this approach can be applied to both binary and multi-class image classification problems without any modification. Experiments are conducted on four image classification datasets to evaluate this algorithm. The experimental results show that the proposed method achieves better performance than the baseline algorithms and some other algorithms.},
  archive      = {J_IETCV},
  author       = {Yun Hou and Hong Fan and Li Li and Bailin Li},
  doi          = {10.1049/cvi2.12027},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {346-355},
  shortjournal = {IET Comput. Vis.},
  title        = {Adaptive learning cost-sensitive convolutional neural network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pop-net: A self-growth network for popping out the salient
object in videos. <em>IETCV</em>, <em>15</em>(5), 334–345. (<a
href="https://doi.org/10.1049/cvi2.12032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a big challenge for unsupervised video segmentation without any object annotation or prior knowledge. In this article, we formulate a completely unsupervised video object segmentation network which can pop out the most salient object in an input video by self-growth, called Pop-Net. Specifically, in this article, a novel self-growth strategy which helps a base segmentation network to gradually grow to stick out the salient object as the video goes on, is introduced. To solve the sample generation problem for the unsupervised method, the sample generation module which fuses the appearance and motion saliency is proposed. Furthermore, the proposed sample optimization module improves the samples by using contour constrains for each self-growth step. Experimental results on several datasets (DAVIS, DAVSOD, VideoSD, Segtrack-v2) show the effectiveness of the proposed method. In particular, the state-of-the-art methods on completely unfamiliar datasets (no fine-tuned datasets) are performed.},
  archive      = {J_IETCV},
  author       = {Hui Yin and Ning Chen and Lin Yang and Jin Wan},
  doi          = {10.1049/cvi2.12032},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {334-345},
  shortjournal = {IET Comput. Vis.},
  title        = {Pop-net: A self-growth network for popping out the salient object in videos},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online dense activity detection. <em>IETCV</em>,
<em>15</em>(5), 323–333. (<a
href="https://doi.org/10.1049/cvi2.12049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense activity detection is a subtask of activity detection that aims to localise and identify multiple human activities in video clips. Existing methods adopt offline frameworks that require video frames to be available when activity detection begins. These offline methods are unable to be applied to online scenarios. An online framework is proposed for dense activity detection. The framework has two stages: warm-up and detection. Warm-up is the initialisation of dense activity detection, which generates a contextual model called an online aggregated-event. After that, the method moves into the detection stage, which consists of two modules: coarse label prediction and refined label prediction. Coarse label prediction predicts activity labels by taking the online aggregated-event as a priori; then, prediction is refined by two techniques, human–object interaction detection and online relation reasoning. The proposed method is evaluated using two dense activity datasets: Charades and AVA. The experimental results show that the proposed method has better performance than existing offline methods after the whole video input is added to the algorithm.},
  archive      = {J_IETCV},
  author       = {Li Weiqi and Wang Jianming and Liang Jiayu and Jin Guanghao and Chung Tae-Sun},
  doi          = {10.1049/cvi2.12049},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {323-333},
  shortjournal = {IET Comput. Vis.},
  title        = {Online dense activity detection},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud super-resolution based on geometric constraints.
<em>IETCV</em>, <em>15</em>(4), 312–321. (<a
href="https://doi.org/10.1049/cvi2.12045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among all digital representations we have for real physical objects, three-dimensional ( 3D) is arguably the most expressive encoding. But due to the limitations of 3D scanning equipment, point cloud often becomes sparse or partially missing. A point cloud super-resolution (PCSR) method based on geometric constraints is proposed to solve the sparse problem of point clouds: it allows dense point clouds to be generated by sparse point clouds. The method is based on the conditional generative adversarial network including redesigned generator and discriminator for point cloud data specially. Moreover, the method can maintain the shape of the dense point cloud by adding geometric constraints. The contributions of our work are as follows: (1) a PCSR method based on geometric constraints is proposed; (2) add a module for obtaining point cloud neighbourhood information in the generator, called K-nn operation module; and (3) feature aggregation is performed using the weighted pooling to process the neighbourhood information obtained by the K-nn operation module. Extensive experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_IETCV},
  author       = {Xiaoqiang Li and Jitao Liu and Songmin Dai},
  doi          = {10.1049/cvi2.12045},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {312-321},
  shortjournal = {IET Comput. Vis.},
  title        = {Point cloud super-resolution based on geometric constraints},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An aggregated deep convolutional recurrent model for event
based surveillance video summarisation: A supervised approach.
<em>IETCV</em>, <em>15</em>(4), 297–311. (<a
href="https://doi.org/10.1049/cvi2.12044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surveillance video summarisation is characterised by extracting video segments containing abnormal events from surveillance video footages. Accurate identification of abnormal events from surveillance footages is of paramount importance in surveillance video summarisation. Accordingly, the proposed framework builds an aggregated convolutional recurrent model that can precisely detect the suspicious events in a surveillance footage, by employing a supervised learning which is found to yield better results compared with unsupervised counterparts. The preliminary stage in the model is a multilayer Convolutional Neural Network for frame-level feature extraction followed by stacked bidirectional Gated Recurrent Unit for sequence-level feature extraction and classification. Since the video clips used for training are not implicit to surveillance, a block-based approach for testing on surveillance videos is proposed. The results evaluated on two custom datasets, Streets and Campus, prove that the proposed model produces remarkable results leveraging the properties of bidirectional GRU with supervised learning. Extensive experimental analysis on selection of optimum architecture is conducted which substantiates the significance of stacked bidirectional GRUs over unidirectional ones. Additionally, qualitative results ensure that summaries produced are concise, representative, complete, diverse and informative. Moreover, comparison of the performance of the proposed model with state of the art certainly proves the superiority of the proposed model.},
  archive      = {J_IETCV},
  author       = {Sreeja M. U. and Binsu C. Kovoor},
  doi          = {10.1049/cvi2.12044},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {297-311},
  shortjournal = {IET Comput. Vis.},
  title        = {An aggregated deep convolutional recurrent model for event based surveillance video summarisation: A supervised approach},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video captioning via a symmetric bidirectional decoder.
<em>IETCV</em>, <em>15</em>(4), 283–296. (<a
href="https://doi.org/10.1049/cvi2.12043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dominant video captioning methods employ the attentional encoder–decoder architecture, where the decoder is an autoregressive structure that generates sentences from left-to-right. However, these methods generally suffer from the exposure bias issue and neglect the guidance of future output contexts obtained from the right-to-left decoding. Here, the authors propose a new symmetric bidirectional decoder for video captioning. The authors first integrate the self-attentive multi-head attention and bidirectional gated recurrent unit for capturing the long-term semantic dependencies in videos. The authors then apply one single decoder to generate accurate descriptions from left-to-right and right-to-left simultaneously. The decoder in each decoding direction performs two cross-attentive multi-head attention modules to consider both the past hidden states from the same decoding direction and the future hidden states from the reverse decoding direction at each time step. A symmetric semantic-guided gated attention module is specially devised to adaptively suppress the irrelevant or misleading contents in the past or future output contexts and retain the useful ones for avoiding under-description. Experimental evaluations on two widely applied benchmark datasets: Microsoft research video to text and Microsoft video description corpus, demonstrate that the authors&#39; proposed method obtains substantially state-of-the-art performance, which validates the superiority of the bidirectional decoder.},
  archive      = {J_IETCV},
  author       = {Shanshan Qi and Luxi Yang},
  doi          = {10.1049/cvi2.12043},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {283-296},
  shortjournal = {IET Comput. Vis.},
  title        = {Video captioning via a symmetric bidirectional decoder},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A coarse to fine network for fast and accurate object
detection in high-resolution images. <em>IETCV</em>, <em>15</em>(4),
274–282. (<a href="https://doi.org/10.1049/cvi2.12042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the popularisation of high-resolution images, detecting objects in these images quickly and accurately has attracted increasing attention in recent studies. Current convolutional neural networks (CNN)-based detection methods have limitations in detecting small objects owing to the interference of scale variation. In this work, we propose an improved generic framework based on YOLOv3. Equipped with multiresolution supervision for training and multiresolution aggregation for inference, this method can deal with the challenge of scale variation in high-resolution images. At first, we move up the multiscale prediction position and add a dilated convolution module on YOLOv3 to improve the accuracy of detection, especially for small objects. Then, we present a coarse to fine method to reduce the detection time. Experiments on a COCO dataset show that our approach achieves 2.8% better accuracy compared with the previous YOLOv3. On a Dataset for Object deTection in Aerial images dataset (a high-resolution remote sensing dataset), our approach outperformed the YOLOv3 by nearly three percentage points in mean average precision. Moreover, it is up to three times faster as well and two times smaller than the previous YOLOv3.},
  archive      = {J_IETCV},
  author       = {Yaguang Guo and Qi Zou and Lu Jin},
  doi          = {10.1049/cvi2.12042},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {274-282},
  shortjournal = {IET Comput. Vis.},
  title        = {A coarse to fine network for fast and accurate object detection in high-resolution images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking-DOSeqSLAM: A dynamic sequence-based visual place
recognition paradigm. <em>IETCV</em>, <em>15</em>(4), 258–273. (<a
href="https://doi.org/10.1049/cvi2.12041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) refers to a process that permits a mobile robot to build up a map of the environment and, at the same time, to use it to compute its location. One of its most important components is its ability to associate the most recently perceived visual measurement to the one derived from previsited locations, a technique widely known as loop closure detection. In this article, we evolve our previous approach, dubbed as ‘DOSeqSLAM’ by presenting a low complexity loop closure detection pipeline wherein the traversed trajectory (map) is represented by sequence-based locations (submaps). Each of these groups of images, referred to as place, is generated online through a point tracking repeatability check employed on the perceived visual sensory information. When querying the database, the proper candidate place is selected and, through an image-to-image search, the appropriate location is chosen. The method is subjected to an extensive evaluation on seven publicly available datasets, revealing a substantial improvement in computational complexity and performance over its predecessors, while performing favourably against other state-of-the art solutions. The system’s effectiveness is owed to the reduced number of places, which, compared to the original approach, is at least one order of magnitude less.},
  archive      = {J_IETCV},
  author       = {Konstantinos A. Tsintotas and Loukas Bampis and Antonios Gasteratos},
  doi          = {10.1049/cvi2.12041},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {258-273},
  shortjournal = {IET Comput. Vis.},
  title        = {Tracking-DOSeqSLAM: A dynamic sequence-based visual place recognition paradigm},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object tracking using temporally matching filters.
<em>IETCV</em>, <em>15</em>(4), 245–257. (<a
href="https://doi.org/10.1049/cvi2.12040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary challenges of visual tracking is the variable appearance of the target object. As tracking proceeds, the target object can change its appearance due to illumination changes, rotations, deformations etc. Modern trackers incorporate online updating to learn how the target changes over time. However, they do not use the history of target appearance. To address this shortcoming, we uniquely use domain adaptation with the target appearance history to efficiently learn a temporally matching filter (TMF) during online updating. This TMF emphasizes the persistent features found in different appearances of the target object. It also improves the classification accuracy of the convolutional neural network by assisting the training of the classification layers without incurring the runtime overhead of updating the convolutional layers. Extensive experimental results demonstrate that the proposed TMF-based tracker, which incorporates domain adaptation with the target appearance history, improves tracking performance on three benchmark video databases (OTB-50, OTB-100 and VOT2016) over other online learning trackers. Specifically, it improves the overlap success of VITAL and MDNet by 0.44 % and 1.03 % on the OTB-100 dataset and improves the accuracy of VITAL and MDNet by 0.55 % and 0.06 % on the VOT2016 dataset, respectively.},
  archive      = {J_IETCV},
  author       = {Brendan Robeson and Mohammadreza Javanmardi and Xiaojun Qi},
  doi          = {10.1049/cvi2.12040},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {245-257},
  shortjournal = {IET Comput. Vis.},
  title        = {Object tracking using temporally matching filters},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud classification by dynamic graph CNN with
adaptive feature fusion. <em>IETCV</em>, <em>15</em>(3), 235–244. (<a
href="https://doi.org/10.1049/cvi2.12039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep neural network has made the most advanced breakthrough in almost all 2D image tasks, so we consider the application of deep learning in 3D images. Point cloud data, as the most basic and important form of representation of 3D images, can accurately and intuitively show the real world. The authors propose a new network based on feature fusion to improve the point cloud classification and segmentation tasks. Our network mainly consists of three parts: global feature extractor, local feature extractor and adaptive feature fusion module. A multi-scale transformation network is devised to guarantee the invariance of the transformation of the global feature, and a residual block is introduced to alleviate the problem of gradient disappearance to enhance the global feature extractor. Based on the edge convolution and multi-layer perceptron, a local feature extractor is constructed. Finally, an adaptive feature-fusion module is proposed to complete the fusion of global features and local features. Extensive experiments on point cloud classification and segmentation tasks are carried out to verify the effectiveness of the proposed method. The classification accuracy of the ModelNet40 is 93.6%, which is 4.4% higher than that of the PointNet. Similarly, the segmentation accuracy on the ShapeNet is 85.6%, which is higher than other methods.},
  archive      = {J_IETCV},
  author       = {Rui Guo and Yong Zhou and Jiaqi Zhao and Yiyun Man and Minjie Liu and Rui Yao and Bing Liu},
  doi          = {10.1049/cvi2.12039},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {235-244},
  shortjournal = {IET Comput. Vis.},
  title        = {Point cloud classification by dynamic graph CNN with adaptive feature fusion},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How important is motion in sign language translation?
<em>IETCV</em>, <em>15</em>(3), 224–234. (<a
href="https://doi.org/10.1049/cvi2.12037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than 70 million people use at least one sign language (SL) as their main channel of communication. Nevertheless, the absence of effective mechanisms to translate massive information among sign, written and spoken languages is the main cause of a negligible inclusion of deaf people into society. Therefore, SL automatic recognition systems have widely proposed to support the characterisation of the sign structure. Today, natural and continuous SL recognition is an open research problem due to multiple spatio-temporal shape variations, challenging visual sign characterisation, as well as the non-linear correlation among signs to express a message. A compact sign is introduced to text architecture that explores motion as an alternative to support sign translation. Such characterisation results are robust to appearance variance with relative support to geometrical variations. The proposed representation focuses on the main spatio-temporal regions to each corresponding word. The proposed architecture was evaluated in a built SL data set (LSCDv1) dedicated to motion study and also in the state-of-the-art RWTH-Phoenix. From the LSCDv1 data set, the best configuration reports a BLEU-4 score of 63.04 in a testing set. Regarding RWTH-Phoenix, the proposed strategy achieved a BLEU-4 score in a test of 4.56, improving the results under similar reduced conditions.},
  archive      = {J_IETCV},
  author       = {Jefferson Rodriguez and Fabio Martínez},
  doi          = {10.1049/cvi2.12037},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {224-234},
  shortjournal = {IET Comput. Vis.},
  title        = {How important is motion in sign language translation?},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time multi-window stereo matching algorithm with fuzzy
logic. <em>IETCV</em>, <em>15</em>(3), 208–223. (<a
href="https://doi.org/10.1049/cvi2.12031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching obtains a depth map called a disparity map that indicates or shows the positions of the objects in a scene. To estimate a disparity map, the most popular trend consists of comparing two images (left-right) from two different points from the same scene. Unfortunately, small window sizes are suitable to preserve the edges, while large window sizes are required in homogeneous areas. To solve this problem, in this article, a novel real-time stereo matching algorithm embedded in an FPGA is proposed. The approach consists of estimating disparity maps with different window sizes by using the sum of absolute differences (SAD) as a local correlation metric. Once the disparity maps are obtained, the left-right consistency for each window size is computed. At the end of this stage, the centre pixel deviation is estimated through a 5 × 5 window and the Sobel gradient is extracted from the left image. Finally, both parameters are processed by a Fuzzy Inference System (FIS), which combines the calculated disparities and generates a final disparity map. An architecture embedded in FPGA is established and hardware acceleration strategies are discussed. Experimental results demonstrated that this algorithmic formulation provides promising results compared with the current state of the art.},
  archive      = {J_IETCV},
  author       = {Héctor-Daniel Vázquez-Delgado and Madaín Pérez-Patricio and Abiel Aguilar-González and Miguel-Octavio Arias-Estrada and Marco-Antonio Palacios-Ramos and Jorge Luis Camas-Anzueto and Antonio Pérez-Cruz and Sabino Velázquez-Trujillo},
  doi          = {10.1049/cvi2.12031},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {208-223},
  shortjournal = {IET Comput. Vis.},
  title        = {Real-time multi-window stereo matching algorithm with fuzzy logic},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical bilinear convolutional neural network for image
classification. <em>IETCV</em>, <em>15</em>(3), 197–207. (<a
href="https://doi.org/10.1049/cvi2.12023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is one of the mainstream tasks of computer vision. However, the most existing methods use labels of the same granularity level for training. This leads to ignoring the hierarchy that may help to differentiate different visual objects better. Embedding hierarchical information into the convolutional neural networks (CNNs) can effectively regulate the semantic space and thus reduce the ambiguity of prediction. To this end, a multi-task learning framework, named as Hierarchical Bilinear Convolutional Neural Network (HB-CNN), is developed by seamlessly integrating CNNs with multi-task learning over the hierarchical visual concept structures. Specifically, the labels with a tree structure are used as the supervision to hierarchically train multiple branch networks. In this way, the model can not only learn additional information (e.g. context information) as the coarse-level category features, but also focus the learned fine-level category features on the object properties. To smoothly pass hierarchical conceptual information and encourage feature reuse, a connectivity pattern is proposed to connect features at different levels. Furthermore, a bilinear module is embedded to generalise various orderless texture feature descriptors so that our model can capture more discriminative features. The proposed method is extensively evaluated on the CIFAR-10, CIFAR-100, and ‘Orchid’ Plant image sets. The experimental results show the effectiveness and superiority of our method.},
  archive      = {J_IETCV},
  author       = {Xiang Zhang and Lei Tang and Hangzai Luo and Sheng Zhong and Ziyu Guan and Long Chen and Chao Zhao and Jinye Peng and Jianping Fan},
  doi          = {10.1049/cvi2.12023},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {197-207},
  shortjournal = {IET Comput. Vis.},
  title        = {Hierarchical bilinear convolutional neural network for image classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural guided visual slam system with laplacian of gaussian
operator. <em>IETCV</em>, <em>15</em>(3), 181–196. (<a
href="https://doi.org/10.1049/cvi2.12022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) addresses the problem of constructing the map from noisy sensor data and tracking the robot&#39;s path within the built map. After decades of development, a lot of mature systems achieve competent results in feature-based implementations. However, there are still problems when migrating the technology to practical applications. One typical example is the accuracy and robustness of SLAM in environment with illuminance and texture variations. To this end, two modules in the existing systems are improved here namely tracking and camera relocalization. In tracking module, image pyramid is processed with Laplacian of Gaussian (LoG) operator in feature extraction for enhanced edges and details. A majority voting mechanism is proposed to dynamically evaluate and redetermine the zero-mean sum of square difference threshold according to the matching error estimation in patch search. In camera relocalization module, full convolutional neural network which focuses on certain parts of the input data is utilized in guiding for accurate output predictions. The authors implement the two modules into OpenvSLAM and propose a neural guided visual SLAM system named LoG-SLAM. Experiments on publicly available datasets show that the accuracy and efficiency increase with LoG-SLAM when compared with other feature-based methods, and relocalization accuracy also improves compared with the recently proposed deep learning pipelines.},
  archive      = {J_IETCV},
  author       = {Ge Zhang and Xiaoqiang Yan and Yulong Xu and Yangdong Ye},
  doi          = {10.1049/cvi2.12022},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {181-196},
  shortjournal = {IET Comput. Vis.},
  title        = {Neural guided visual slam system with laplacian of gaussian operator},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Entropy information-based heterogeneous deep selective fused
features using deep convolutional neural network for sketch recognition.
<em>IETCV</em>, <em>15</em>(3), 165–180. (<a
href="https://doi.org/10.1049/cvi2.12019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective feature representation can boost recognition tasks in the sketch domain. Due to an abstract and diverse structure of the sketch relatively with a natural image, it is complex to generate a discriminative features representation for sketch recognition. Accordingly, this article presents a novel scheme for sketch recognition. It generates a discriminative features representation as a result of integrating asymmetry essential information from deep features. This information is kept as an original feature-vector space for making a final decision. Specifically, five different well-known pre-trained deep convolutional neural networks (DCNNs), namely, AlexNet, VGGNet-19, Inception V3, Xception, and InceptionResNetV2 are fine-tuned and utilised for feature extraction. First, the high-level deep layers of the networks were used to get multi-features hierarchy from sketch images. Second, an entropy-based neighbourhood component analysis was employed to optimise the fusion of features in order of rank from multiple different layers of various deep networks. Finally, the ranked features vector space was fed into the support vector machine (SVM) classifier for sketch classification outcomes. The performance of the proposed scheme is evaluated on two different sketch datasets such as TU-Berlin and Sketchy for classification and retrieval tasks. Experimental outcomes demonstrate that the proposed scheme brings substantial improvement over human recognition accuracy and other state-of-the-art algorithms.},
  archive      = {J_IETCV},
  author       = {Shaukat Hayat and She Kun and Sara Shahzad and Parinya Suwansrikham and Muhammad Mateen and Yao Yu},
  doi          = {10.1049/cvi2.12019},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {165-180},
  shortjournal = {IET Comput. Vis.},
  title        = {Entropy information-based heterogeneous deep selective fused features using deep convolutional neural network for sketch recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Going deeper: Magnification-invariant approach for breast
cancer classification using histopathological images. <em>IETCV</em>,
<em>15</em>(2), 151–164. (<a
href="https://doi.org/10.1049/cvi2.12021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer has the highest fatality for women compared with other types of cancer. Generally, early diagnosis of cancer is crucial to increase the chances of successful treatment. Early diagnosis is possible through physical examination, screening, and obtaining a biopsy of the dubious area. In essence, utilizing histopathology slides of biopsies is more efficient than using typical screening methods. Nevertheless, the diagnosing process is still tiresome and is prone to human error during slide preparation, such as when dyeing and imaging. Therefore, a novel method is proposed for diagnosing breast cancer into benign or malignant in a magnification-specific binary (MSB) classification. Besides, the introduced method classifies each type into four subclasses in a magnification-specific multi-category (MSM) fashion. The proposed method involves normalizing the hematoxylin and eosin stains to enhance colour separation and contrast. Then, two types of novel features—deep and shallow features—are extracted using two deep structure networks based on DenseNet and Xception. Finally, a multi-classifier method based on the maximum value is utilized to achieve the best performance. The proposed method is evaluated using the BreakHis histopathology data set, and the results in terms of diagnostic accuracy are promising, achieving 99% and 92% in terms of MSB and MSM, respectively, compared with recent state-of-the-art methods reported in the survey conducted by Benhammou  on the BreakHis data set using deep learning and texture-based models.},
  archive      = {J_IETCV},
  author       = {S. Alkassar and Bilal A. Jebur and Mohammed A. M. Abdullah and Joanna H. Al-Khalidy and J. A. Chambers},
  doi          = {10.1049/cvi2.12021},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {151-164},
  shortjournal = {IET Comput. Vis.},
  title        = {Going deeper: Magnification-invariant approach for breast cancer classification using histopathological images},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TanhExp: A smooth activation function with high convergence
speed for lightweight neural networks. <em>IETCV</em>, <em>15</em>(2),
136–150. (<a href="https://doi.org/10.1049/cvi2.12020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight or mobile neural networks used for real-time computer vision tasks contain fewer parameters than normal networks, which lead to a constrained performance. Herein, a novel activation function named as Tanh Exponential Activation Function (TanhExp) is proposed which can improve the performance for these networks on image classification task significantly. The definition of TanhExp is f ( x ) = x tanh( e x ). The simplicity, efficiency, and robustness of TanhExp on various datasets and network models is demonstrated and TanhExp outperforms its counterparts in both convergence speed and accuracy. Its behaviour also remains stable even with noise added and dataset altered. It is shown that without increasing the size of the network, the capacity of lightweight neural networks can be enhanced by TanhExp with only a few training epochs and no extra parameters added.},
  archive      = {J_IETCV},
  author       = {Xinyu Liu and Xiaoguang Di},
  doi          = {10.1049/cvi2.12020},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {136-150},
  shortjournal = {IET Comput. Vis.},
  title        = {TanhExp: A smooth activation function with high convergence speed for lightweight neural networks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PL-VSCN: Patch-level vision similarity compares network for
image matching. <em>IETCV</em>, <em>15</em>(2), 122–135. (<a
href="https://doi.org/10.1049/cvi2.12018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matching plays an important role in various computer vision tasks, such as image retrieval and loop closure detection in Simultaneous Localization and Mapping. The authors propose a discriminative patch-based image matching method that converts the problem of whole image matching to that of local patch matching. To construct the patch representation, the Patch-Level Vision Similarity Compare Network (PL-VSCN) is proposed to produce the patch feature. In the image matching process, local patches that potentially contain objects within images are initially detected, and the discriminative feature of each patch is extracted based on the pre-trained PL-VSCN. Then, the similarities between the patch pairs are calculated to construct the similarity matrix, and the corresponding patch pairs are detected based on the mutual matching mechanism on the similarity matrix. Experimental results indicate that the proposed PL-VSCN can generate the discriminative patch feature, which can accurately match the patch pairs with the corresponding content and distinguish those with non-corresponding content. In addition, the comparison experiments demonstrate that the proposed image matching method outperforms existing approaches on most datasets and effectively completes the image matching task.},
  archive      = {J_IETCV},
  author       = {Xiong You and Qin Li and Ke Li and Anzhu Yu and Shuhui Bu},
  doi          = {10.1049/cvi2.12018},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {122-135},
  shortjournal = {IET Comput. Vis.},
  title        = {PL-VSCN: Patch-level vision similarity compares network for image matching},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tri-attention enhanced graph convolutional network for
skeleton-based action recognition. <em>IETCV</em>, <em>15</em>(2),
110–121. (<a href="https://doi.org/10.1049/cvi2.12017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has recently attracted a lot of research interests due to its advantage in computational efficiency. Some recent work building upon Graph Convolutional Networks (GCNs) has shown promising performance in this task by modelling intrinsic spatial correlations between skeleton joints. However, these methods only consider local properties of action sequences in the spatial-temporal domain, and consequently, are limited in distinguishing complex actions with similar local movements. To address this problem, a novel tri-attention module (TAM) is proposed to guide GCNs to perceive significant variations across local movements. Specifically, the devised TAM is implemented in three steps: i) A dimension permuting unit is proposed to characterise skeleton action sequences in three different domains: body poses, joint trajectories, and evolving projections. ii) A global statistical modelling unit is introduced to aggregate the first-order and second-order properties of global contexts to perceive the significant movement variations of each domain. iii) A fusion unit is presented to integrate the features of these three domains together and leverage as orientation for graph convolution at each layer. Through these three steps, significant-variation frames, joints, and channels can be enhanced. We conduct extensive experiments on two large-scale benchmark datasets, NTU RGB-D and Kinetics-Skeleton. Experimental results demonstrate that the proposed TAM can be easily plugged into existing GCNs and achieve comparable performance with the state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Xingming Li and Wei Zhai and Yang Cao},
  doi          = {10.1049/cvi2.12017},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {110-121},
  shortjournal = {IET Comput. Vis.},
  title        = {A tri-attention enhanced graph convolutional network for skeleton-based action recognition},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An interactive instance segmentation system with
multi-resolution convolutional neural networks. <em>IETCV</em>,
<em>15</em>(2), 99–109. (<a
href="https://doi.org/10.1049/cvi2.12016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a fast interactive instance segmentation (IIS) system is proposed and it is composed of an effective heatmap generator, a multi-resolution network (MRNet), and an adaptive threshold refiner to promptly and precisely predict the masks of the objects. The proposed heatmap generator after interaction clicks can help the MRNet to successfully learn the sensitive features for better prediction. Based on convolutional neural network models, the proposed MRNet backbone produces multiple features across multiple resolutions and can intrinsically predict the sharp contour of the object. After the probabilistic prediction achieved by the MRNet, the Otsu&#39;s threshold refiner is proposed to further remove some uncertain pixels in the predicted mask. Experimental results demonstrate that the proposed IIS system can promptly predict sharp masks of the targeted objects with mIoU of 89.1% in PASCAL VOC 2012 [1] validation set. Compared to other existing interactive methods, the proposed system can effectively predict the segmentation mask with higher accuracy and less interaction efforts.},
  archive      = {J_IETCV},
  author       = {Po-Wei Sung and Wei-Jong Yang and Jar-Ferr Yang and Din-Yuan Chan},
  doi          = {10.1049/cvi2.12016},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {99-109},
  shortjournal = {IET Comput. Vis.},
  title        = {An interactive instance segmentation system with multi-resolution convolutional neural networks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving vehicle re-identification using CNN latent spaces:
Metrics comparison and track-to-track extension. <em>IETCV</em>,
<em>15</em>(2), 85–98. (<a
href="https://doi.org/10.1049/cvi2.12010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herein, the problem of vehicle re-identification using distance comparison of images in CNN latent spaces is addressed. First, the impact of the distance metrics, comparing performances obtained with different metrics is studied: the minimal Euclidean distance ( MED ), the minimal cosine distance ( MCD ) and the residue of the sparse coding reconstruction ( RSCR ). These metrics are applied using features extracted from five different CNN architectures, namely ResNet18, AlexNet, VGG16, InceptionV3 and DenseNet201. We use the specific vehicle re-identification dataset VeRi to fine-tune these CNNs and evaluate results. Overall, independently of the CNN used, MCD outperforms MED , commonly used in the literature. These results are confirmed on other vehicle retrieval datasets. Second, the state-of-the-art image-to-track process (I2TP) is extended to a track-to-track process (T2TP). The three distance metrics are extended to measure distance between tracks, enabling T2TP. T2TP and I2TP are compared using the same CNN models. Results show that T2TP outperforms I2TP for MCD and RSCR. T2TP combining DenseNet201 and MCD -based metrics exhibits the best performances, outperforming the state-of-the-art I2TP-based models. Finally, experiments highlight two main results: i) the impact of metric choice in vehicle re-identification, and ii) T2TP improves the performances compared with I2TP, especially when coupled with MCD -based metrics.},
  archive      = {J_IETCV},
  author       = {Geoffrey Roman-Jimenez and Patrice Guyot and Thierry Malon and Sylvie Chambon and Vincent Charvillat and Alain Crouzil and André Péninou and Julien Pinquier and Florence Sedes and Christine Sénac},
  doi          = {10.1049/cvi2.12010},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {85-98},
  shortjournal = {IET Comput. Vis.},
  title        = {Improving vehicle re-identification using CNN latent spaces: Metrics comparison and track-to-track extension},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image translation with dual-directional generative
adversarial networks. <em>IETCV</em>, <em>15</em>(1), 73–83. (<a
href="https://doi.org/10.1049/cvi2.12011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between input images and output images. However, due to the unstable training and limited training samples, many existing GAN-based works have difficulty in producing photo-realistic images. Herein, dual-directional generative adversarial networks are proposed, which consist of four adversarial networks, to produce images of high perceptual quality. In this framework, self-reconstruction strategy is used to construct auxiliary sub-networks, which impose more effective constraints on encoder-generator pairs. Using this idea, this model can increase the use ratio of paired data conditioned on the same dataset and obtain well-trained encoder-generator pairs with the help of the proposed cross-network skip connections. Moreover, the proposed framework not only produces realistic images but also addresses the problem where condition GAN produces sharp images containing many small, hallucinated objects. Training on multiple supervised datasets, convincing evidences are shown to prove that this model can achieve compelling results by latently learning a common feature representation. Qualitative and quantitative comparisons against other methods, demonstrate the effectiveness and superiority of the method.},
  archive      = {J_IETCV},
  author       = {Congcong Ruan and Liuchun Yuan and Haifeng Hu and Dihu Chen},
  doi          = {10.1049/cvi2.12011},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {73-83},
  shortjournal = {IET Comput. Vis.},
  title        = {Image translation with dual-directional generative adversarial networks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level feature fusion network for crowd counting.
<em>IETCV</em>, <em>15</em>(1), 60–72. (<a
href="https://doi.org/10.1049/cvi2.12012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has become a noteworthy vision task due to the needs of numerous practical applications, but it remains challenging. State-of-the-art methods generally estimate the density map of the crowd image with the high-level semantic features of various deep convolutional networks. However, the absence of low-level spatial information may result in counting errors in the local details of the density map. To this end, a novel framework named Multi-level Feature Fusion Network (MFFN) for single image crowd counting is proposed. The proposed MFFN, which is constructed in an encoder–decoder fashion, incorporates semantic and spatial information for generating high-resolution density maps of input crowd images. Skip connections are developed between the encoder and the decoder so that low-level spatial information and high-level semantic features can be combined by element-wise addition. In addition, a dense dilated convolution block is placed behind the encoder, extracting multi-scale context features to guide feature fusion by a channel attention mechanism. The model is trained by multi-task learning; semantic segmentation supervision is introduced to enhance feature representation. Extensive experiments are conducted on three crowd counting datasets (ShanghaiTech, UCF_CC_50, UCF-QNRF), and the results show that MFFN outperforms state-of-the-art methods. In addition, sufficient ablation studies are performed to verify the effectiveness of each component in our proposed method.},
  archive      = {J_IETCV},
  author       = {Luyang Wang and Yun Li and Sifan Peng and Xiao Tang and Baoqun Yin},
  doi          = {10.1049/cvi2.12012},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {60-72},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-level feature fusion network for crowd counting},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-camera traffic scene mosaic based on camera
calibration. <em>IETCV</em>, <em>15</em>(1), 47–59. (<a
href="https://doi.org/10.1049/cvi2.12009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the traffic application based on vision in the single traffic-monitoring scene has been widely studied and developed. However, cross-regional research is still in its infancy. In order to help solve the application of cross-regional traffic surveillance scenarios, this paper proposes a more reliable and accurate road scene mosaic method under multi-camera surveillance. The mosaic road panorama contains physical information, which can be used to achieve a cross-regional measurement. It also lays the foundation for vehicle spatial location, vehicle speed and traffic incident detection across regions. First, the mapping relationship between the three-dimensional sub-world coordinates and their corresponding two-dimensional image coordinates is established by camera calibration. Second, the projection transformation relationship between two cameras is established by two sub-world coordinate systems and their common information. Finally, we use the proposed inverse projection idea and translation vector relationship to complete the mosaic of two traffic-monitoring road scenes. The experimental results show that the camera calibration accuracy can reach more than 97% in a single scene. The measurement accuracy of the mosaic block is over 95%. The experimental results show that the proposed method has a higher accuracy, which has great value in related theoretical research and practical applications.},
  archive      = {J_IETCV},
  author       = {Feifan Wu and Huansheng Song and Zhe Dai and Wei Wang and Junyan Li},
  doi          = {10.1049/cvi2.12009},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {47-59},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-camera traffic scene mosaic based on camera calibration},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). L4Net: An anchor-free generic object detector with attention
mechanism for autonomous driving. <em>IETCV</em>, <em>15</em>(1), 36–46.
(<a href="https://doi.org/10.1049/cvi2.12015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic object detection is a crucial task for autonomous driving. To devise a safe and efficient object detector, the following aspects are required to be considered: high accuracy, real-time inference speed and small model size. Herein, a simple yet effective anchor-free object detector named L4Net is proposed, which incorporates a keypoint detection backbone and a co-attention scheme into a unified framework, and achieves lower computation cost with higher detection accuracy than prior art across a wide spectrum of resource constrains. Specifically, the backbone utilizes M ulti-scale R eceptive-fields E nhancement module (MRE) to capture context-wise information, where the features of object scale and shape invariance are simultaneously considered. The co-attention scheme integrates the strength of both C lass-agnostic A ttention (CA) and S emantic A ttention (SA), and explores the valuable features from low-level to high-level to generate more accurate prediction boxes. Compared with previous feature fusion strategy, multi-scale features are selectively integrated by fully exploiting the different characteristics of low-level and high-level features, which leads to a small model size and faster inference speed. Extensive experiments on four well-known datasets demonstrate the effectiveness of our method. For instance, L4Net achieves 71.68% mAP on KITTI test set, with 13.7 M model size at the speed of 149 FPS on NVIDIA TX and 30.7 FPS on Qualcomm-based device, respectively, which is 4 x smaller and 2 x faster than baseline model.},
  archive      = {J_IETCV},
  author       = {Yanan Wu and Songhe Feng and Xiankai Huang and Zizhang Wu},
  doi          = {10.1049/cvi2.12015},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {36-46},
  shortjournal = {IET Comput. Vis.},
  title        = {L4Net: An anchor-free generic object detector with attention mechanism for autonomous driving},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Three-dimensional shape reconstruction of objects from a
single depth view using deep u-net convolutional neural network with
bottle-neck skip connections. <em>IETCV</em>, <em>15</em>(1), 24–35. (<a
href="https://doi.org/10.1049/cvi2.12014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) shape reconstruction of objects requires multiple scans and complex reconstruction algorithms. An alternative approach is to infer the 3D shape of an object from a single depth image (i.e. single depth view). This study presents such a 3D shape reconstructor based on U-Net 3D-convolutional neural network (3D-CNN) with bottle-neck skipped connection blocks (U-Net BNSC 3D-CNN) to infer the 3D shapes of objects from only a single depth view. The BNSC block is a fully convolutional block that uses skip connections to improve the performance of the sequential 3D-convolutional layers of U-Net. The primary advantage of U-Net BNSC 3D-CNN is improving the accuracy of shape reconstruction while reducing the computational load. The evaluation of the proposed U-Net BNSC 3D-CNN uses unseen views from trained and untrained objects with two public databases, ShapeNet and Grasp database. Our reconstructor achieves 72.17% and 69.97% accuracy in terms of the Jaccard similarity index for trained and untrained objects, respectively, with the ShapeNet database, whereas previous reconstructor based on 3D-CNN achieves 66.43% and 58.35%. With Grasp database, our reconstructor achieves 87.03% and 85.35%, whereas 3D-CNN 76.52% and 76.02%. Also, our U-Net BNSC 3D-CNN reduces the computational load of the standard 3D-CNN reconstructor by 6.67% in the computation time and by 98.69% in the number of trainable parameters.},
  archive      = {J_IETCV},
  author       = {Edwin Valarezo Añazco and Patricio Rivera Lopez and Tae-Seong Kim},
  doi          = {10.1049/cvi2.12014},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {24-35},
  shortjournal = {IET Comput. Vis.},
  title        = {Three-dimensional shape reconstruction of objects from a single depth view using deep U-net convolutional neural network with bottle-neck skip connections},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DVC-net: A deep neural network model for dense video
captioning. <em>IETCV</em>, <em>15</em>(1), 12–23. (<a
href="https://doi.org/10.1049/cvi2.12013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense video captioning (DVC) detects multiple events in an input video and generates natural language sentences to describe each event. Previous studies predominantly used convolutional neural networks to extract visual features from videos but failed to employ high-level semantics to effectively explain video content such as people, objects, actions, and places, and utilized only limited context information in generating natural language. To overcome these deficiencies, DVC-Net is proposed, a new deep neural network model that uses high-level semantics to efficiently represent important events as well as visual features. Additionally, DVC-Net uses a bidirectional long short-term memory network, a type of recurrent neural network, to detect events over time. Furthermore, DVC-Net applies an attention mechanism and context gating to effectively exploit context information in a caption generation step. In experiments conducted versus state-of-the-art models, DVC-Net presented relative gains of over 1.72% (BLEU@1 score increases from 12.22 to 13.94) and 3.19% (CIDEr score increases from 12.61 to 15.80) on the large-scale benchmark datasets, namely ActivityNet Captions and MSR-VTT, respectively.},
  archive      = {J_IETCV},
  author       = {Sujin Lee and Incheol Kim},
  doi          = {10.1049/cvi2.12013},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {12-23},
  shortjournal = {IET Comput. Vis.},
  title        = {DVC-net: A deep neural network model for dense video captioning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmentation of natural images based on super pixel and
graph merging. <em>IETCV</em>, <em>15</em>(1), 1–11. (<a
href="https://doi.org/10.1049/cvi2.12008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of natural image segmentation is one of the most researched topics of computer vision. There are mainly two principal approaches for the task, the statistical approach and the supervised approach. The proposed methodology segments natural images combining a set of statistical algorithms. First, the image is preprocessed to enhance the edges. Weighted average of the denoised image and its derivatives is the preprocessed output. Thereafter, an energy based super pixelation is applied to over segment the image. Finally, a connectivity graph is built where nodes correspond to super pixels and edges connect the adjacent super pixels. The adjacent super pixels are merged based on the confidence value defined in terms of their textural and colour similarity. Proposed methodology has been applied on the images of BSDS500 dataset. Performance of the proposed work has been compared with that of other works based on detected edge maps. Few works generate ultrametric contour maps (UCM). To compare the performance with those works, UCM is also generated by the proposed methodology. To do so images at multiple scales are considered. It is observed that the output of segmentation is better in case of the proposed methodology. Proposed methodology is much faster than others. Thus, makes it suitable for real time application in robot vision.},
  archive      = {J_IETCV},
  author       = {Aritra Mukherjee and Soumik Sarkar and Sanjoy K. Saha},
  doi          = {10.1049/cvi2.12008},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IET Comput. Vis.},
  title        = {Segmentation of natural images based on super pixel and graph merging},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
