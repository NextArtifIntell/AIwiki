<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EXSY_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="exsy---147">EXSY - 147</h2>
<ul>
<li><details>
<summary>
(2021). Integrating structural control, health monitoring, and
energy harvesting for smart cities. <em>EXSY</em>, <em>38</em>(8),
e12845. (<a href="https://doi.org/10.1111/exsy.12845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cities that are adopting innovative and technology-driven solutions to improve the city&#39;s efficiency are considered smart cities . With the increased attention on smart cities with self-driving vehicles, drones, and robots, designing smart infrastructure is only a natural extension. Smart infrastructures aim to self-diagnose, self-power, self-adapt, and self-heal during normal and extreme operating conditions. Structural vibration control (SVC) and structural health monitoring (SHM) technologies, in particular, are expected to play pivotal roles in the development of modern smart and resilient structures. SVC methodologies intend to provide supplemental damping and reduce the structural dynamic responses during normal and extreme events. SHM methodologies offer valuable information about the structure&#39;s condition that is useful for maintenance purposes and rapid damage detection in post-hazard events. The collapse of the 12-story Champlain Towers South, a beachfront condominium in the Miami suburb of Surfside, Florida, could have been known in advance with an embedded SHM technology. More recently, the integrated structural control and health monitoring (ISCHM) systems have shown promise in the development of smart cities of the future. The integrated architecture incorporates the control and health monitoring components as complementary technologies and simultaneously takes advantage of both technologies. This article provides a state-of-the-art review of ISCHM ideas and systems. It presents recent significant developments in structural control, SHM, and energy harvesting that are paving the way towards the advent of integrated ISCHM systems, including damage-tolerant control systems. This article also identifies future promising research areas for designing the next generation of autonomous ISCHM systems for smart cities.},
  archive      = {J_EXSY},
  author       = {Sajad Javadinasab Hormozabad and Mariantonieta Gutierrez Soto and Hojjat Adeli},
  doi          = {10.1111/exsy.12845},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12845},
  shortjournal = {Expert Syst.},
  title        = {Integrating structural control, health monitoring, and energy harvesting for smart cities},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Response surface methodology to tune artificial neural
network hyper-parameters. <em>EXSY</em>, <em>38</em>(8), e12792. (<a
href="https://doi.org/10.1111/exsy.12792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural network is a machine learning algorithm that has been widely used in many application areas. The performance of the algorithm depends on the type of the problem, the size of the problem, and the architecture of the algorithm. One of the most important things that affect ANN&#39;s performance is the selection of hyper-parameters, but there is not a specific rule to determine the hyper-parameters of the algorithm. Although there is no single well-known method in hyper-parameter tuning, this issue has been discussed in many studies. In this study, a central composite design which is a successful response surface methodology technique that considers factor interactions is used for hyper-parameter optimization. A categorical central composite design that has 39 experimental runs was used to predict accuracy and F-score. The effect of ANN hyper-parameters on selected performance indicators is investigated using two different size customer churn prediction data set, which is widely used in the literature. Using the desirability functions, multiple objectives are combined and the best hyper-parameter levels are selected. As a result of verification tests, the accuracy values are 85.79% and 83.29% for the first and second data set, respectively. And, the F-score values are 86.15% and 82.33% for the first and second data set, respectively. The results show the effectiveness of the adopted RSM technique.},
  archive      = {J_EXSY},
  author       = {Sinem Bozkurt Keser and Yeliz Buruk Sahin},
  doi          = {10.1111/exsy.12792},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12792},
  shortjournal = {Expert Syst.},
  title        = {Response surface methodology to tune artificial neural network hyper-parameters},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diffusion strategies for face swiping medical service using
evolutionary game theory. <em>EXSY</em>, <em>38</em>(8), e12790. (<a
href="https://doi.org/10.1111/exsy.12790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face swiping is the action of registering facial recognition data with a biometric authentication application. Face swiping-based medical services (FSMSs) is an emerging measurement technology underpinning modern intelligent medical services in China. In view of convenience in the delivery of medical services and the potential to reduce medical costs, FSMSs have become an important part of China&#39;s medical reform. However, modelling its eventual uses has not been fully explored in theory and practice, and this is hindering the take up of FSMSs in China. In this paper, we build an evolutionary game model to explore three stakeholders—policy makers, hospitals, and patients—and their dynamic interdependence of their decision-making when adopting FSMS. Based on our game theoretic analysis, we develop a novel model to track the behaviour of these three stakeholders and analyse the corresponding payoff matrix to establish the replication dynamic equation (RDE) for the game. Further, we then use RDE to calculate the different stability points of players and determine the game&#39;s stable strategy. Finally, we validate the proposed model with a detailed simulation. Our observations may benefit not only FSMS participants but also several other forms of medical services and industries.},
  archive      = {J_EXSY},
  author       = {Xiaojia Wang and Peiling Cheng and Keyu Zhu and Sheng Xu and Shanshan Zhang and Weiqun Xu and Yuxiang Guan},
  doi          = {10.1111/exsy.12790},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12790},
  shortjournal = {Expert Syst.},
  title        = {Diffusion strategies for face swiping medical service using evolutionary game theory},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MCGDM with complex pythagorean fuzzy -soft model.
<em>EXSY</em>, <em>38</em>(8), e12783. (<a
href="https://doi.org/10.1111/exsy.12783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research article, we introduce the fundamental facts of a new multi-skilled and powerful decision-making methodology, namely, the complex Pythagorean fuzzy -soft TOPSIS (CPFNS-TOPSIS) method. This newly developed strategy serves as a competent mathematical tool for the rating-based modelling of two-dimensional inconsistent parameterized data. The CPFNS-TOPSIS method enables us to take advantage of both the decision-making abilities of the TOPSIS method and the very general parametric theory of the complex Pythagorean fuzzy -soft (CPFNSS) model. The proposed technique is designed for the identification of an optimum solution which is nearest to the complex Pythagorean fuzzy -soft positive ideal solution (CPFNS-PIS) and farthest from the complex Pythagorean fuzzy N -soft negative ideal solution (CPFNS-NIS). We put forward a complex Pythagorean fuzzy -soft weighted averaging (CPFNSWA) operator and a normalized Euclidean distance measure for the theoretical background of our strategy. The proposed technique permits the experts to dynamically express their independent assessments regarding the proficiency of the alternatives and the weightage of the decision criteria. We aggregate the evaluated interpretations of the experts in the light of the novel CPFNSWA operator. Further, we utilize a calibrated score matrix for the determination of CPFNS-PIS and CPFNS-NIS, and we employ the proposed distance measures to analyse the proximity of the alternatives from the ideal solutions. We hierarchically categorize the feasible set of alternatives in descending order given by their revised closeness index. Moreover, we elaborate the methodology of our CPFNS-TOPSIS method with the help of an illustrative flow chart. In relation to potential applications, we investigate a case study that concerns the selection of the best earthwork construction equipment required for a newly endorsed heavy construction project. This exercise demonstrates the pertinence and feasibility of our approach. In addition, we conduct a comparative analysis of the proposed strategy with an existing multiple criteria group decision making strategy, namely, the Pythagorean fuzzy TOPSIS (PF-TOPSIS) method. This speaks for the feasibility and authenticity of the technique developed in this paper. We also present a unified discussion of our comparative study along with an interpretative bar chart which illustrates the remarkable features of the proposed strategy by validating the compatibility and accuracy of quantified similar end-results. To conclude, we explore the rationality of the developed methodology to justify its superiority over the available approaches.},
  archive      = {J_EXSY},
  author       = {Muhammad Akram and Faiza Wasim and Faruk Karaaslan},
  doi          = {10.1111/exsy.12783},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12783},
  shortjournal = {Expert Syst.},
  title        = {MCGDM with complex pythagorean fuzzy -soft model},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent knowledge tracing machine based on the knowledge
state of students. <em>EXSY</em>, <em>38</em>(8), e12782. (<a
href="https://doi.org/10.1111/exsy.12782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) aims to closely trace the knowledge level of students during their learning. KT is often implemented by intelligent tutoring systems (ITS) to predict the student performance, and schedule the individual learning plan for each student. However, most existing KT models cannot predict the knowledge forgetting well due to the lack of temporal information and some KT models for knowledge forgetting prediction are not accurate enough. To resolve these issues, this paper proposes a recurrent knowledge tracing machine (RKTM), which temporally enriches the encoding of knowledge tracing machine (KTM) and difficulty, student ability, skill, and student skill practice history (DAS3H) with the knowledge state of students. RKTM consists of two major components, including the tracing component and the predicting component. The tracing component temporally traces the knowledge state of a student, while the predicting component captures the interaction between the current learning scenario and the current knowledge state of that student and provides accurate prediction of student performance. Experiments show that the proposed RKTM well outperforms KTM, DAS3H, and some other models.},
  archive      = {J_EXSY},
  author       = {Zefeng Lai and Lei Wang and Qiang Ling},
  doi          = {10.1111/exsy.12782},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12782},
  shortjournal = {Expert Syst.},
  title        = {Recurrent knowledge tracing machine based on the knowledge state of students},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Multi-swarm and chaotic whale-particle swarm optimization
algorithm with a selection method based on roulette wheel.
<em>EXSY</em>, <em>38</em>(8), e12779. (<a
href="https://doi.org/10.1111/exsy.12779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The particle swarm optimization (PSO) and the whale optimization algorithm (WOA) are two admired optimization methods that have drawn various researchers&#39; attention. The PSO implements some particles&#39; intelligent movements in a search space, and the WOA is originated based on the hunting mechanism of humpback whales. The PSO and WOA have different strategies for moving towards the optimum solution. Nevertheless, both algorithms&#39; performances encounter several problems, such as premature convergence and falling in local optimums. Several approaches have been proposed to enhance meta-heuristic algorithms&#39; performance, such as applying the chaotic maps, adding mathematical or stochastic operators or local searches, and hybridizing the algorithms. In this article, a new hybrid algorithm denoted as chaotic-based hybrid whale and PSO has been presented by improving the WOA, combining it with PSO, and using the chaotic maps. The hybrid algorithm has significantly more diverse movements than both of the mentioned algorithms. Therefore, it explores different regions of a problem&#39;s search space more precisely and avoids local optima. The roulette wheel selection operator has also been applied based on their fitness value to select the proposed algorithm&#39;s search agents and exploit promising regions of the search space. In the hybrid algorithm, the chaotic maps have been applied to initialize the whales&#39; population, particles of the particle swarm, and adjust motion parameters to increase population diversity. The multi-swarm version of the proposed algorithm with higher performance than the single-swarm version and other methods has been introduced in this article too. The proposed algorithms have been evaluated using 23 mathematical benchmark functions, including unimodal, multimodal, and composite functions and four engineering optimization problems. The obtained results and statistical tests prove that the proposed algorithms provide competitive solutions for most of the experiments, compared to the state-of-the-art and well-known optimization meta-heuristic methods in terms of convergence towards the global optimum, local optima avoidance, exploration, and exploitation.},
  archive      = {J_EXSY},
  author       = {Kayvan Asghari and Mohammad Masdari and Farhad Soleimanian Gharehchopogh and Rahim Saneifard},
  doi          = {10.1111/exsy.12779},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12779},
  shortjournal = {Expert Syst.},
  title        = {Multi-swarm and chaotic whale-particle swarm optimization algorithm with a selection method based on roulette wheel},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On correlated information for learning predictive models
under the choquet integral. <em>EXSY</em>, <em>38</em>(8), e12777. (<a
href="https://doi.org/10.1111/exsy.12777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive models based on the Choquet integral have been applied successfully in machine learning and multi criteria decision making context. The ability of the Choquet integral to recognize and capture non-linear dependencies and its comprehensibility make it a very appealing tool. Yet, its complexity is often an obstacle to estimate model parameters in the presence of a large number of attributes. The other concern is the so-called numerical instability, that is, learning the Choquet integral for a high degree of complexity exhibits a numerical instability. This study addresses complexity reduction and presents a novel algorithm in order to enhance the quality of optimization results and computational efficiency of learning predictive models underlying the Choquet integral.},
  archive      = {J_EXSY},
  author       = {Ali Fallah Tehrani},
  doi          = {10.1111/exsy.12777},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12777},
  shortjournal = {Expert Syst.},
  title        = {On correlated information for learning predictive models under the choquet integral},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Requirement prioritization framework using case-based
reasoning: A mining-based approach. <em>EXSY</em>, <em>38</em>(8),
e12770. (<a href="https://doi.org/10.1111/exsy.12770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current era of technology and development, component-based software development (CBSD) has been progressively implemented. Components are used in large products for multiple users with diverse viewpoints and are highly configurable to provide higher satisfaction. Components are reused because of their similar functionality to reduce complexity and ensure the correct interaction between interfaces during product development. However, implementation of component functionalities creates complications due to improper specification, prioritization of components requirements, encapsulated functionalities, and more human interaction. Furthermore, due to configurability and the involvement of multiple stakeholders, ambiguity and semantic issues arise in the behaviour of reusable components. To overcome semantic-based specification and prioritization components&#39; related issues, we propose a framework that uses text mining and case-based reasoning (CBR) techniques. Results of our empirical evaluation demonstrate that the proposed framework significantly outperforms the conventional technique.},
  archive      = {J_EXSY},
  author       = {Sadia Ali and Yaser Hafeez and Shariq Hussain and Shunkun Yang and Muhammad Jamal},
  doi          = {10.1111/exsy.12770},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12770},
  shortjournal = {Expert Syst.},
  title        = {Requirement prioritization framework using case-based reasoning: A mining-based approach},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fermatean fuzzy set extensions of SAW, ARAS, and VIKOR with
applications in COVID-19 testing laboratory selection problem.
<em>EXSY</em>, <em>38</em>(8), e12769. (<a
href="https://doi.org/10.1111/exsy.12769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple attribute decision-making models are empowered with the support of fuzzy sets such as intuitionistic, q-rung orthopair, Pythagorean, and picture fuzzy sets, and also neutrosophic sets, etc. These concepts generate varying representation opportunities for the decision-maker&#39;s preferences and expertise. Pythagorean and Fermatean fuzzy sets are special cases of q-rung orthopair fuzzy set when q = 2 and q = 3, respectively. From a geometric perspective, the latter provides a broader representation domain than the former does. In this study, the emerging concept of Fermatean fuzzy set is studied in detail and three well-known multi-attribute evaluation methods, namely SAW, ARAS, and VIKOR are extended under Fermatean fuzzy environment. In this manner, the decision-makers will have more freedom in specifying their preferences, thoughts, and expertise, and the abovementioned decision approaches will be able to handle this new type of data. The applicability of the propositions is shown in determining the best Covid-19 testing laboratory which is an important topic of the ongoing global health crisis. To validate the proposed methods, a benchmark analysis covering the results of the existing Fermatean fuzzy set-based decision methods, namely TOPSIS, WPM, and Yager aggregation operators is presented.},
  archive      = {J_EXSY},
  author       = {Sait Gül},
  doi          = {10.1111/exsy.12769},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12769},
  shortjournal = {Expert Syst.},
  title        = {Fermatean fuzzy set extensions of SAW, ARAS, and VIKOR with applications in COVID-19 testing laboratory selection problem},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-heuristic optimization algorithm for predicting
software defects. <em>EXSY</em>, <em>38</em>(8), e12768. (<a
href="https://doi.org/10.1111/exsy.12768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software engineering companies strive to improve software quality by predicting software defects-prone modules. Although various data mining methods have been developed, unstable accuracy rates are still critical issues owing to the imbalanced nature and high dimensionality of software defect datasets. To deal with this issue, we propose a spotted hyena, a novel meta-heuristic optimization algorithm for predicting software defects. Support and confidence in classification rules are the basis of a multi-objective fitness function that assists the spotted hyena algorithm in serving as a classifier by finding the fittest classification or standard rules among individuals. Experiments were conducted on four NASA software datasets, JM1, KC2, KC1, and PC3. The spotted hyena classifier provides an accuracy of 85.2, 84, 89.6, and 81.8%, respectively, for these datasets. These accuracy rates are better than those achieved using other popular data mining techniques. We also discuss other classification measures in connection with the experimental results, such as precision, recall, and confusion matrices, in connection with the experimental results. Moreover, the Gaussian mixture model is used to study the uncertainty quantification of the proposed classifier. The study proved the feasible performance of the spotted hyena classifier in four different case studies.},
  archive      = {J_EXSY},
  author       = {Mahmoud A. Elsabagh and Marwa S. Farhan and Mona G. Gafar},
  doi          = {10.1111/exsy.12768},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12768},
  shortjournal = {Expert Syst.},
  title        = {Meta-heuristic optimization algorithm for predicting software defects},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Applying the peak-end rule to decision-making regarding
similar products: A case-based decision approach. <em>EXSY</em>,
<em>38</em>(8), e12763. (<a
href="https://doi.org/10.1111/exsy.12763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a marketing decision support system (DSS) for firms using case-based and peak-end approaches to model consumers. The proposed DSS is composed of model and estimation methods. We employ a similarity function used in case-based decision theory to examine the degree of similarity between the past and current products offered to a consumer. The contributions of this study are as follows: First, by extending the peak-end approach, the proposed model could be utilized to analyse not only the same product but also multiple similar products. The DSS could be applied to a broad range of decision problems. Second, by extending the case-based decision model, our DSS considerably reduces the number of computational operations needed. Third, the model demonstrated the best fit among the compared models and possessed high prediction accuracy when analysing the viewing data for Japanese television dramas. The DSS could increase the future purchase probability of a product. Our research bridges two significant concepts: case-based decision models in DSSs and peak-end rules in behavioural economics.},
  archive      = {J_EXSY},
  author       = {Keita Kinjo and Takeshi Ebina},
  doi          = {10.1111/exsy.12763},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12763},
  shortjournal = {Expert Syst.},
  title        = {Applying the peak-end rule to decision-making regarding similar products: A case-based decision approach},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rule-based preprocessing for data stream mining using
complex event processing. <em>EXSY</em>, <em>38</em>(8), e12762. (<a
href="https://doi.org/10.1111/exsy.12762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data preprocessing is known to be essential to produce accurate data from which mining methods are able to extract valuable knowledge. When data constantly arrives from one or more sources, preprocessing techniques need to be adapted to efficiently handle these data streams. To help domain experts to define and execute preprocessing tasks for data streams, this paper proposes the use of active rule-based systems and, more specifically, complex event processing (CEP) languages and engines. The main contribution of our approach is the formulation of preprocessing procedures as event detection rules, expressed in an SQL-like language, that provide domain experts a simple way to manipulate temporal data. This idea is materialized into a publicly available solution that integrates a CEP engine with a library for online data mining. To evaluate our approach, we present three practical scenarios in which CEP rules preprocess data streams with the aim of adding temporal information, transforming features and handling missing values. Experiments show how CEP rules provide an effective language to express preprocessing tasks in a modular and high-level manner, without significant time and memory overheads. The resulting data streams do not only help improving the predictive accuracy of classification algorithms, but also allow reducing the complexity of the decision models and the time needed for learning in some cases.},
  archive      = {J_EXSY},
  author       = {Aurora Ramírez and Nathalie Moreno and Antonio Vallecillo},
  doi          = {10.1111/exsy.12762},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12762},
  shortjournal = {Expert Syst.},
  title        = {Rule-based preprocessing for data stream mining using complex event processing},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RQCSNet: A deep learning approach to quantized compressed
sensing of remote sensing images. <em>EXSY</em>, <em>38</em>(8), e12755.
(<a href="https://doi.org/10.1111/exsy.12755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a deep neural network is proposed that can be used to perform a quantized compressed sensing of remote sensing images. This network, dubbed RQCSNet consists of three parts: sensing, nonlinear quantization, and reconstruction. All network&#39;s parts are trained simultaneously according to the input data. RQCSNet has fully convolutional architecture, and it can work with different resolutions of the input images without the need for large initial fully connected layers. Furthermore, its structure reduces the blocky artefacts in the final reconstructed images by considering the effects of different image blocks in the reconstruction process. The proposed model has the ability to learn the nonlinear quantization function according to the input data and sending the measured values from the transmitter to the receiver with a fewer bit budget. Also, RQCSNet can learn the compressed sensing reconstruction algorithm without initial image reconstruction (that is usually the first step of reconstruction in other models), which reduces network flops. The results are presented on standard datasets of UCMerced as a remote sensing data that is the main concentration of this article, and as well as the set11 which is a dataset widely used for the benchmark of the state-of-the-art models. The experiments show that RQCSNet has a better performance than existing models in terms of the quality of image reconstruction and time complexity. As a benchmark, our network has a mean PSNR improvement of about 1.2 dB in reconstructed images compared to available techniques, at a measurement rate of 0.25. The effect of adding noise to the quantized measured data from the transmitter to the receiver at the time of transmission is also investigated in this article, and it is shown that the quality of image reconstruction will be robust to the noise once adding a noise layer to the network during training.},
  archive      = {J_EXSY},
  author       = {Alireza Mirrashid and Ali-Asghar Beheshti Shirazi},
  doi          = {10.1111/exsy.12755},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12755},
  shortjournal = {Expert Syst.},
  title        = {RQCSNet: A deep learning approach to quantized compressed sensing of remote sensing images},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flight scheduling incorporating bad weather conditions
through big data analytics: A comparison of metaheuristics.
<em>EXSY</em>, <em>38</em>(8), e12752. (<a
href="https://doi.org/10.1111/exsy.12752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important mode of long-distance transportation for people to use time efficiently is air transportation. One of the most significant factors determining air traffic performance is the scheduling created by companies. When creating schedules, various parameters are used, mainly the demand for the determined route, and the availability of airplanes and flight crews. Known climatic conditions on the route are generally neglected. Within the scope of this study, the creation of flight schedules, which are normally the preparation of flight schedules by considering the monthly, daily, and hourly constraints that could cause flight delays, were determined by also considering weather conditions. In this determination process, 32-year flight data between 1987 and 2018 were used as a basis. Apache Spark, one of the big data technologies, was used to determine these constraints. The scheduling was optimized by using the constraints used to prevent flight delays. It was observed in the results obtained that simulated annealing (SA) achieved the most optimal results compared to the genetic algorithm (GA) and the artificial bee colony (ABC) algorithm. As the dimensions of the searched space increased, solving the problem with metaheuristic approaches was more advantageous in terms of time compared to the classical method.},
  archive      = {J_EXSY},
  author       = {Ebru Erdem and Tolga Aydın and Burak Erkayman},
  doi          = {10.1111/exsy.12752},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12752},
  shortjournal = {Expert Syst.},
  title        = {Flight scheduling incorporating bad weather conditions through big data analytics: A comparison of metaheuristics},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment analysis for urdu online reviews using deep
learning models. <em>EXSY</em>, <em>38</em>(8), e12751. (<a
href="https://doi.org/10.1111/exsy.12751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing studies are focused on popular languages like English, Spanish, Chinese, Japanese, and others, however, limited attention has been paid to Urdu despite having more than 60 million native speakers. In this paper, we develop a deep learning model for the sentiments expressed in this under-resourced language. We develop an open-source corpus of 10,008 reviews from 566 online threads on the topics of sports, food, software, politics, and entertainment. The objectives of this work are bi-fold (a) the creation of a human-annotated corpus for the research of sentiment analysis in Urdu; and (b) measurement of up-to-date model performance using a corpus. For their assessment, we performed binary and ternary classification studies utilizing another model, namely long short-term memory (LSTM), recurrent convolutional neural network (RCNN) Rule-Based, N -gram, support vector machine , convolutional neural network, and LSTM. The RCNN model surpasses standard models with 84.98% accuracy for binary classification and 68.56% accuracy for ternary classification. To facilitate other researchers working in the same domain, we have open-sourced the corpus and code developed for this research.},
  archive      = {J_EXSY},
  author       = {Iqra Safder and Zainab Mahmood and Raheem Sarwar and Saeed-Ul Hassan and Farooq Zaman and Rao Muhammad Adeel Nawab and Faisal Bukhari and Rabeeh Ayaz Abbasi and Salem Alelyani and Naif Radi Aljohani and Raheel Nawaz},
  doi          = {10.1111/exsy.12751},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12751},
  shortjournal = {Expert Syst.},
  title        = {Sentiment analysis for urdu online reviews using deep learning models},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A double moral hazard model for inter-organizational
knowledge sharing. <em>EXSY</em>, <em>38</em>(8), e12750. (<a
href="https://doi.org/10.1111/exsy.12750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In inter-organizational knowledge sharing, the firms share sensitive organizational knowledge to jointly solve a problem. However, the knowledge sender may conceal knowledge and the knowledge receiver may misappropriate the knowledge received from the sender. We formalize this situation using the double moral hazard framework and develop Stackelberg game models for symmetric and asymmetric information scenarios. First, we analyse the motivation for triggering double moral hazard. We find that by concealing knowledge, the sender cannot reduce the loss due to misappropriating knowledge. Second, we analyse the impact of improved punishment mechanism on the behaviours that trigger double moral hazard, and find that there exists a threshold policy with respect to monitoring effort to prevent double moral hazard. Third, we get the conditions under which the partner has the motivation to prevent the monitor from adopting a deception strategy.},
  archive      = {J_EXSY},
  author       = {Guo Chen and Baixun Li},
  doi          = {10.1111/exsy.12750},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12750},
  shortjournal = {Expert Syst.},
  title        = {A double moral hazard model for inter-organizational knowledge sharing},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using semantic annotations on political debate videos for
building open government based lawmaking. <em>EXSY</em>, <em>38</em>(8),
e12748. (<a href="https://doi.org/10.1111/exsy.12748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional concept and practice of lawmaking is being transformed by the open government paradigm. It involves a new digital model that links three principles: transparency, participation and collaboration. However, the information system-enabled solutions in this context are still in their infancy, which none available that address the three crucial challenges: (1) providing the functionalities for semantic search and retrieval of video clips on political debates about legislative proposals, (2) support of open participation and collaboration with contextual information in lawmaking processes and, (3) automatic generation of outcomes achieved in the lawmaking lifecycle. In this article, we describe the underlying methods and techniques leading to address these challenges when a semantic-driven approach is applied for the development and validation of the real-world scenario of open government-based lawmaking systems. To evaluate the value created by using a semantic-driven approach as computational backbone, the development and testing of a software platform as case study was carried out together with experts from a parliamentary institution and analysed the user experience in parliamentary settings with end-users. The results show how the use of the appropriate techniques for the adoption of a semantic assumption is a key enabler for the management and retrieval of information tasks relating to the lawmaking lifecycle providing a transparent, participatory and collaborative approach in a holistic way.},
  archive      = {J_EXSY},
  author       = {Elena Sánchez-Nielsen and Francisco Chávez-Gutiérrez},
  doi          = {10.1111/exsy.12748},
  journal      = {Expert Systems},
  month        = {12},
  number       = {8},
  pages        = {e12748},
  shortjournal = {Expert Syst.},
  title        = {Using semantic annotations on political debate videos for building open government based lawmaking},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated detection of orofacial pain from thermograms using
machine learning and deep learning approaches. <em>EXSY</em>,
<em>38</em>(7), e12747. (<a
href="https://doi.org/10.1111/exsy.12747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objectives of this study are (i) to perform automated segmentation of facial regions from thermograms using k-means clustering algorithm and to classify the data into normal and orofacial pain (OFP) categories using various machine learning classifiers (ii) to implement the convolutional neural network (CNN) for classification of normal and OFP subjects which involves automated feature extraction and feature selection process. Fifty normal and 50 diseased cases suffering from orofacial pain were included in the study. Facial thermograms were segmented using k-means algorithm, then statistical features were extracted and classified into normal and OFP using various machine learning classifier. Further, the deep learning networks such as VGG-16 and DenseNet-121 were used for automated feature extraction and classification of facial thermograms. The facial temperature variations of 3.46%, 3.4%, and 3.27% were observed in the front, right and left side facial regions respectively between the normal and the OFP subjects. Machine learning classifiers such as support vector machine (SVM) and random forest (RF) classifier provided the highest accuracy of 99%. On the other hand, deep learning models such as modified VGG-16 achieved an average accuracy of 97% compared to modified DenseNet-121 which produced an average accuracy of 68% in classification of normal and OFP thermograms. Thus, computer aided diagnosis of facial thermography could be used as a viable screening device for a reliable identification of tooth pathology before the occurrence of structural changes and complications.},
  archive      = {J_EXSY},
  author       = {Snekhalatha Umapathy and Palani Thanaraj Krishnan},
  doi          = {10.1111/exsy.12747},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12747},
  shortjournal = {Expert Syst.},
  title        = {Automated detection of orofacial pain from thermograms using machine learning and deep learning approaches},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cassava disease recognition from low-quality images using
enhanced data augmentation model and deep learning. <em>EXSY</em>,
<em>38</em>(7), e12746. (<a
href="https://doi.org/10.1111/exsy.12746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improvement of deep learning algorithms in smart agriculture is important to support the early detection of plant diseases, thereby improving crop yields. Data acquisition for machine learning applications is an expensive task due to the requirements of expert knowledge and professional equipment. The usability of any application in a real-world setting is often limited by unskilled users and the limitations of devices used for acquiring images for classification. We aim to improve the accuracy of deep learning models on low-quality test images using data augmentation techniques for neural network training. We generate synthetic images with a modified colour value distribution to expand the trainable image colour space and to train the neural network to recognize important colour-based features, which are less sensitive to the deficiencies of low-quality images such as those affected by blurring or motion. This paper introduces a novel image colour histogram transformation technique for generating synthetic images for data augmentation in image classification tasks. The approach is based on the convolution of the Chebyshev orthogonal functions with the probability distribution functions of image colour histograms. To validate our proposed model, we used four methods (resolution down-sampling, Gaussian blurring, motion blur, and overexposure) for reducing image quality from the Cassava leaf disease dataset. The results based on the modified MobileNetV2 neural network showed a statistically significant improvement of cassava leaf disease recognition accuracy on lower-quality testing images when compared with the baseline network. The model can be easily deployed for recognizing and detecting cassava leaf diseases in lower quality images, which is a major factor in practical data acquisition.},
  archive      = {J_EXSY},
  author       = {Olusola Oluwakemi Abayomi-Alli and Robertas Damaševičius and Sanjay Misra and Rytis Maskeliūnas},
  doi          = {10.1111/exsy.12746},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12746},
  shortjournal = {Expert Syst.},
  title        = {Cassava disease recognition from low-quality images using enhanced data augmentation model and deep learning},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Business analytics in industry 4.0: A systematic review.
<em>EXSY</em>, <em>38</em>(7), e12741. (<a
href="https://doi.org/10.1111/exsy.12741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the term “Industry 4.0” has emerged to characterize several Information Technology and Communication (ICT) adoptions in production processes (e.g., Internet-of-Things, implementation of digital production support information technologies). Business Analytics is often used within the Industry 4.0, thus incorporating its data intelligence (e.g., statistical analysis, predictive modelling, optimization) expert system component. In this paper, we perform a Systematic Literature Review (SLR) on the usage of Business Analytics within the Industry 4.0 concept, covering a selection of 169 papers obtained from six major scientific publication sources from 2010 to March 2020. The selected papers were first classified in three major types, namely, Practical Application, Reviews and Framework Proposal. Then, we analysed with more detail the practical application studies which were further divided into three main categories of the Gartner analytical maturity model, Descriptive Analytics, Predictive Analytics and Prescriptive Analytics. In particular, we characterized the distinct analytics studies in terms of the industry application and data context used, impact (in terms of their Technology Readiness Level) and selected data modelling method. Our SLR analysis provides a mapping of how data-based Industry 4.0 expert systems are currently used, disclosing also research gaps and future research opportunities.},
  archive      = {J_EXSY},
  author       = {António João Silva and Paulo Cortez and Carlos Pereira and André Pilastri},
  doi          = {10.1111/exsy.12741},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12741},
  shortjournal = {Expert Syst.},
  title        = {Business analytics in industry 4.0: A systematic review},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of 3D design lower limb exoskeleton on human
musculoskeletal with various loads. <em>EXSY</em>, <em>38</em>(7),
e12738. (<a href="https://doi.org/10.1111/exsy.12738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface electromyography (SEMG) based exoskeleton presents a new opportunity for human augmentation and rehabilitation. Developing an efficient exoskeleton in real-time is challenging as each individual&#39;s muscles and joint forces are unique. The aim of this research article is to analyze and evaluate the design of the lower limb exoskeleton during the squatting movement in a simulated environment to address problems concerning the development of a functional exoskeleton for an individual. An exoskeleton was designed in SolidWorks CAD software and imported into AnyBody Modelling Software (AMS). Thereafter, the performance of 3D designed exoskeleton was evaluated by placing various loads (0:5:25 kg) on both the shoulders of the human musculoskeletal. The results show the force in the knee muscles with the assistance of the exoskeleton were reduced significantly by 65.18–97.20% in the biceps femoris, 50.01–33.16% in the rectus femoris, 41.87–28.31% in the vastus lateralis, 42.25–28.78% in the vastus medialis, 7.28–22.91% in gluteus medius, and 22.54–13.13% in semitendinosus. The force in the knee joint was reduced by 44.04–31.43% as the load increases. Individual muscle force estimated from the SEMG signal and AMS during squatting was also compared for validation. The developed model helps in understanding the load effects on different muscles and provides useful information for the construction of an individual&#39;s optimized exoskeleton.},
  archive      = {J_EXSY},
  author       = {Anurag Sohane and Ravinder Agarwal},
  doi          = {10.1111/exsy.12738},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12738},
  shortjournal = {Expert Syst.},
  title        = {Evaluation of 3D design lower limb exoskeleton on human musculoskeletal with various loads},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online one pass clustering of data streams based on growing
neural gas and fuzzy inference systems. <em>EXSY</em>, <em>38</em>(7),
e12736. (<a href="https://doi.org/10.1111/exsy.12736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering of big data streams has become a challenging task due to time and space constraints of the hardware and decreasing accuracy when the dimensionality of input data grows in time. In this paper, fuzzy growing neural gas is introduced, an online fuzzy approach for clustering data streams based on the growing neural gas algorithm, by adopting more restrictive criteria for selecting the winner nodes in the topological graph constructed at each iteration of the algorithm. The algorithm is tested on public datasets, and the results show improvements over existing clustering methods.},
  archive      = {J_EXSY},
  author       = {Ali Mahmoudabadi and Marjan Kuchaki Rafsanjani and Mohammad Masoud Javidi},
  doi          = {10.1111/exsy.12736},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12736},
  shortjournal = {Expert Syst.},
  title        = {Online one pass clustering of data streams based on growing neural gas and fuzzy inference systems},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A fixed structure learning automata-based optimization
algorithm for structure learning of bayesian networks. <em>EXSY</em>,
<em>38</em>(7), e12734. (<a
href="https://doi.org/10.1111/exsy.12734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the useful knowledge representation tools, which can describe the joint probability distribution between some random variables with a graphical model and can be trained by a dataset, is the Bayesian network (BN). A BN is composed of a network structure and a conditional probability distribution table for each node. Discovering an optimal BN structure is an NP-hard optimization problem that various meta-heuristic algorithms are applied to solve this problem by researchers. The genetic algorithms, ant colony optimization, evolutionary programming, artificial bee colony, and bacterial foraging optimization are some of the meta-heuristic methods to solve this problem using a dataset. Most of these methods are applying a scoring metric to generate the best network structure from a set of candidates. A Fixed Structure Learning Automata-Based (FSLA-B) algorithm is presented in this paper to solve the structure learning problem of BNs. There is a fixed structure learning automaton for each pair of vertices in the BN&#39;s graph structure in the proposed algorithm. The action of this automaton determines the presence and direction of an edge between the vertices. The proposed algorithm performs a guided search procedure using the FSLA and escapes from local optimums. Several datasets are utilised in this paper to evaluate the performance of the proposed algorithm. By performing various experiments, multiple meta-heuristic algorithms are compared with the introduced new one. The obtained results represented that the proposed algorithm could produce competitive results and find the near-optimal solution for the BN structure learning problem.},
  archive      = {J_EXSY},
  author       = {Kayvan Asghari and Mohammad Masdari and Farhad Soleimanian Gharehchopogh and Rahim Saneifard},
  doi          = {10.1111/exsy.12734},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12734},
  shortjournal = {Expert Syst.},
  title        = {A fixed structure learning automata-based optimization algorithm for structure learning of bayesian networks},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tuning structural parameters of neural networks using
genetic algorithm: A credit scoring application. <em>EXSY</em>,
<em>38</em>(7), e12733. (<a
href="https://doi.org/10.1111/exsy.12733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) have successfully been applied to classification problems including credit scoring. The tuning of the structural parameters of the NNs has a direct impact on their accuracy. In this paper, a hybrid approach based on the genetic algorithm (GA) is proposed to adjust the structural parameters of a classifier NN to achieve high accuracy. Two well-known credit scoring datasets—Australian and German datasets—are used to test the proposed approach. The results indicate that the proposed hybrid approach is able to successfully tune the structural parameters, while the accuracy of classification is enhanced and its complexity dramatically diminished in comparison with other existing approaches. The performance of the proposed algorithm has been investigated through statistical analysis The best-known solutions achieved by the proposed approach have an accuracy equal to 97.78% and 87.1% for Australian and German datasets, respectively. The results indicate 2.68% and 0.1% improvement in comparison with the best results reported in the literature, respectively. This improvement is important for real cases in which millions of loans are allocated using credit scoring approaches.},
  archive      = {J_EXSY},
  author       = {Hamid Reza Kazemi and Kaveh Khalili-Damghani and Soheil Sadi-Nezhad},
  doi          = {10.1111/exsy.12733},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12733},
  shortjournal = {Expert Syst.},
  title        = {Tuning structural parameters of neural networks using genetic algorithm: A credit scoring application},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Window data envelopment analysis approach: A review and
bibliometric analysis. <em>EXSY</em>, <em>38</em>(7), e12721. (<a
href="https://doi.org/10.1111/exsy.12721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Window Data Envelopment Analysis (WDEA) is a popular, effective, and applicable methods for dynamic performance assessment of peer decision making units (DMUs). WDEA is a non-parametric panel method that operates based on the principle of moving averages and establishes efficiency measures by treating each DMU in different periods as a separate DMU. By applying the WDEA approach, a decision-maker (DM) can measure the efficiency of different DMUs in different periods through a sequence of overlapping windows. Also, WDEA can increase the discrimination power by increasing the number of DMUs when a limited number of DMUs is available. Given the advantages of the WDEA approach and its applications in real-world problems, this paper surveys and analyses 387 WDEA papers published from 1985 to 2020. The paper also recommends some suggestions, guidelines, and opportunities for future research. Notably, the findings show the applicability and efficacy of WDEA in the literature.},
  archive      = {J_EXSY},
  author       = {Pejman Peykani and Reza Farzipoor Saen and Fatemeh Sadat Seyed Esmaeili and Jafar Gheidar-Kheljani},
  doi          = {10.1111/exsy.12721},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12721},
  shortjournal = {Expert Syst.},
  title        = {Window data envelopment analysis approach: A review and bibliometric analysis},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constriction coefficient based particle swarm optimization
and gravitational search algorithm for multilevel image thresholding.
<em>EXSY</em>, <em>38</em>(7), e12717. (<a
href="https://doi.org/10.1111/exsy.12717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is one of the pivotal steps in image processing. Actually, it deals with the partitioning of the image into different classes based on pixel intensities. This work introduces a new image segmentation method based on the constriction coefficient-based particle swarm optimization and gravitational search algorithm (CPSOGSA). The random samples of the image act as searcher agents of the CPSOGSA algorithm. The optimal number of thresholds is determined using Kapur&#39;s entropy method. The effectiveness and applicability of CPSOGSA in image segmentation is accomplished by applying it to five standard images from the USC-SIPI image database, namely Aeroplane, Cameraman, Clock, Lena, and Pirate. Various performance metrics are employed to investigate the simulation outcomes, including optimal thresholds, standard deviation, MSE (mean square error), run time analysis, PSNR (peak signal to noise ratio), best fitness value calculation, convergence maps, segmented image graphs, and box plot analysis. Moreover, image accuracy is benchmarked by utilizing SSIM (structural similarity index measure) and FSIM (feature similarity index measure) metrics. Also, a pairwise non-parametric signed Wilcoxon rank-sum test is utilized for statistical verification of simulation results. In addition, the experimental outcomes of CPSOGSA are compared with eight different algorithms including standard PSO, classical GSA, PSOGSA, SCA (sine cosine algorithm), SSA (salp swarm algorithm), GWO (grey wolf optimizer), MFO (moth flame optimizer), and ABC (artificial bee colony). The simulation results clearly indicate that the hybrid CPSOGSA has successfully provided the best SSIM, FSIM, and threshold values to the benchmark images.},
  archive      = {J_EXSY},
  author       = {Sajad Ahmad Rather and P. Shanthi Bala},
  doi          = {10.1111/exsy.12717},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12717},
  shortjournal = {Expert Syst.},
  title        = {Constriction coefficient based particle swarm optimization and gravitational search algorithm for multilevel image thresholding},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Breast ultrasound tumour classification: A machine
learning—radiomics based approach. <em>EXSY</em>, <em>38</em>(7),
e12713. (<a href="https://doi.org/10.1111/exsy.12713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of breast tumour malignancy using ultrasound imaging, is an important step for early detection of breast cancer. An efficient prediction system can be a great help to improve the survival chances of the involved patients. In this work, a machine learning (ML)—radiomics based classification pipeline is proposed, to perform this predictive modelling task, in a much more efficient manner. Multiple different types of image features of the region of interests are considered in this work, followed by a recursive feature elimination based feature selection step. Furthermore, a synthetic minority oversampling technique based step is also included in the pipeline, to deal with the class imbalance problem, that is often present in medical imaging datasets. Various ML models are considered in the subsequent model training phase, on a publicly available breast ultrasound image dataset (BUSI). From experimental analysis it has been observed that, shape, texture and histogram oriented gradients related features are the most informative, with respect to the predictive modelling task. Furthermore, it was observed that ensemble learners such as random forest, gradient boosting and AdaBoost classifiers are able to achieve significant results with respect to multiple evaluation metrics. The proposed approach achieved the state-of-the-art accuracy, area under the curve, F 1-score and Mathews correlation coefficient values of 0.974, 0.97, 0.94 and 0.959, respectively, on the BUSI dataset. Such kind of impressive results suggest that the proposed approach can have a very high practical utility, in real medical diagnostic settings.},
  archive      = {J_EXSY},
  author       = {Arnab K. Mishra and Pinki Roy and Sivaji Bandyopadhyay and Sujit K. Das},
  doi          = {10.1111/exsy.12713},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12713},
  shortjournal = {Expert Syst.},
  title        = {Breast ultrasound tumour classification: A machine Learning—Radiomics based approach},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-label cascaded neural network classification
algorithm for automatic training and evolution of deep cascaded
architecture. <em>EXSY</em>, <em>38</em>(7), e12671. (<a
href="https://doi.org/10.1111/exsy.12671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification algorithms deal with classification problems where a single datapoint can be classified (or labelled) with more than one class (or label) at the same time. Early multi-label approaches like binary relevance consider each label individually and train individual binary classifier models for each label. State-of-the-art algorithms like RAkEL, classifier chains, calibrated label ranking, IBLR-ML+, and BPMLL also consider the associations between labels for improved performance. Like most machine learning algorithms, however, these approaches require careful hyper-parameter tuning, a computationally expensive optimisation problem. There is a scarcity of multi-label classification algorithms that require minimal hyper-parameter tuning. This paper addresses this gap in the literature by proposing CascadeML, a multi-label classification method based on the existing cascaded neural network architecture, which also takes label associations into consideration. CascadeML grows a neural network architecture incrementally (deep as well as wide) in a two-phase process as it learns network weights using an adaptive first-order gradient descent algorithm. This omits the requirement of preselecting the number of hidden layers, nodes, activation functions, and learning rate. The performance of the CascadeML algorithm was evaluated using 13 multi-label datasets and compared with nine existing multi-label algorithms. The results show that CascadeML achieved the best average rank over the datasets, performed better than BPMLL (one of the earliest well known multi-label specific neural network algorithms), and was similar to the state-of-the-art classifier chains and RAkEL algorithms.},
  archive      = {J_EXSY},
  author       = {Arjun Pakrashi and Brian Mac Namee},
  doi          = {10.1111/exsy.12671},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12671},
  shortjournal = {Expert Syst.},
  title        = {A multi-label cascaded neural network classification algorithm for automatic training and evolution of deep cascaded architecture},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Secure third-party data clustering using SecureCL, φ-data
and multi-user order preserving encryption. <em>EXSY</em>,
<em>38</em>(7), e12581. (<a
href="https://doi.org/10.1111/exsy.12581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure collaborative data clustering using SecureCL is presented. SecureCL is founded on the concept of Φ-data implemented using Super Secure Chain Distance Matrices and encrypted using Multi-User Order Preserving Encryption. The advantage offered, unlike comparable systems, is that SecureCL does not require any user participation once the Φ-data proxy has been encrypted; it does not require recourse to Secure Multi-Party Computation protocols or ‘secret sharing’ mechanisms. The utility of SecureCL is illustrated using Nearest Neighbour Clustering and Density-Based Spatial Clustering of Applications with Noise, although it can be applied to any data clustering algorithm that involves distance comparison. The reported experiments demonstrate that SecureCL can produce securely cluster configurations comparable to those produced using standard, non-encrypted, approaches without entailing any significant computational overhead, thus indicating its suitability in the context of Data Mining as a Service.},
  archive      = {J_EXSY},
  author       = {Nawal Almutairi and Frans Coenen and Keith Dures},
  doi          = {10.1111/exsy.12581},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12581},
  shortjournal = {Expert Syst.},
  title        = {Secure third-party data clustering using SecureCL, Φ-data and multi-user order preserving encryption},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GEP-based classifiers with drift-detection. <em>EXSY</em>,
<em>38</em>(7), e12571. (<a
href="https://doi.org/10.1111/exsy.12571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we propose two gene expression programming (GEP)-based ensemble classifiers with different drift detection mechanisms. In the related work section, we briefly review GEP as a classification tool, incremental classifiers, and concept drift detectors. Next, the structure of our two-level GEP ensemble with metagenes is described. Further on, two integrated classifiers with drift detection algorithm and Wilcoxon rank sum test drift detector are proposed. The approach is validated in the computational experiment in which several real-life and artificial datasets with concept drift have been used. Experiment confirmed that the proposed approach can be competitive to existing solutions. In the conclusion section, we briefly outline directions for future research.},
  archive      = {J_EXSY},
  author       = {Joanna Jedrzejowicz and Piotr Jedrzejowicz},
  doi          = {10.1111/exsy.12571},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12571},
  shortjournal = {Expert Syst.},
  title        = {GEP-based classifiers with drift-detection},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Increasing sample efficiency in deep reinforcement learning
using generative environment modelling. <em>EXSY</em>, <em>38</em>(7),
e12537. (<a href="https://doi.org/10.1111/exsy.12537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning is a broad scheme of learning algorithms that, in recent times, has shown astonishing performance in controlling agents in environments presented as Markov decision processes. There are several unsolved problems in current state-of-the-art that causes algorithms to learn suboptimal policies, or even diverge and collapse completely. Parts of the solution to address these issues may be related to short- and long-term planning, memory management and exploration for reinforcement learning algorithms. Games are frequently used to benchmark reinforcement learning algorithms as they provide a flexible, reproducible and easy to control environments. Regardless, few games feature the ability to perceive how the algorithm performs exploration, memorization and planning. This article presents The Dreaming Variational Autoencoder with Stochastic Weight Averaging and Generative Adversarial Networks (DVAE-SWAGAN), a neural network-based generative modelling architecture for exploration in environments with sparse feedback. We present deep maze, a novel and flexible maze game-engine that challenges DVAE-SWAGAN in partial and fully observable state-spaces, long-horizon tasks and deterministic and stochastic problems. We show results between different variants of the algorithm and encourage future study in reinforcement learning driven by generative exploration.},
  archive      = {J_EXSY},
  author       = {Per-Arne Andersen and Morten Goodwin and Ole-Christoffer Granmo},
  doi          = {10.1111/exsy.12537},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12537},
  shortjournal = {Expert Syst.},
  title        = {Increasing sample efficiency in deep reinforcement learning using generative environment modelling},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis and prediction of high-speed train wheel wear based
on SIMPACK and backpropagation neural networks. <em>EXSY</em>,
<em>38</em>(7), e12417. (<a
href="https://doi.org/10.1111/exsy.12417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As train running speeds increase, the wheel–rail interactions of high-speed trains are becoming more complicated, and predicting and monitoring wheel wear are becoming increasingly important for the safe operation of high-speed trains. Therefore, identifying the critical factors that affect the wear of wheel–rail interactions and developing novel methods to predict wheel wear are of great importance. In this work, SIMPACK is used to establish a dynamic model of a high-speed train and to investigate the normal and lateral contact forces of the wheel–rail interfaces and the wear of the wheels for a train passing through a specially designed route that consists of straight-line, smooth-curved, and circular tracks. The wheel wear is predicted by means of the Archard wear model based on the SIMPACK analysis, and the wear is validated by a backpropagation neural network (BPNN) classification based on daily measured data provided by the Beijing Railway Administration. The results from the SIMPACK dynamic simulation and the BPNN classification show that the position of a wheel in a bogie has a significant effect on the wheel wear, but the position of a carriage in a train does not have a significant effect on the wheel wear. The findings from this study are very useful for the maintenance and safe operation of high-speed trains.},
  archive      = {J_EXSY},
  author       = {Shuwen Wang and Hao Yan and Caixia Liu and Ning Fan and Xiaoming Liu and Chengguo Wang},
  doi          = {10.1111/exsy.12417},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12417},
  shortjournal = {Expert Syst.},
  title        = {Analysis and prediction of high-speed train wheel wear based on SIMPACK and backpropagation neural networks},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEMAFLEX: A novel approach for implementing workflow
flexibility by deviation based on constraint satisfaction problem
solving. <em>EXSY</em>, <em>38</em>(7), e12385. (<a
href="https://doi.org/10.1111/exsy.12385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept developed during the SEMAFLEX project combines knowledge-based document management with flexible workflow management to achieve ideal support for small- and medium-sized enterprises. This paper focusses on the workflow approach that implements flexibility by deviation on the basis of constraint satisfaction problem solving. Workflow deviations due to unpredictable circumstances or upcoming events are tolerated at runtime, but still, support is maintained and possible succeeding work items are proposed. Procedurally modelled workflows are transformed to declarative constraints, which serve as database for the workflow engine. The described approach is fully implemented, and our experiments demonstrate sufficient runtime performance as well as improvements for practical use.},
  archive      = {J_EXSY},
  author       = {Lisa Grumbach and Ralph Bergmann},
  doi          = {10.1111/exsy.12385},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12385},
  shortjournal = {Expert Syst.},
  title        = {SEMAFLEX: A novel approach for implementing workflow flexibility by deviation based on constraint satisfaction problem solving},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ordered sparse subspace clustering algorithm based on
p-norm. <em>EXSY</em>, <em>38</em>(7), e12368. (<a
href="https://doi.org/10.1111/exsy.12368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images in video may include both Gaussian noise and geometric rotation. Thus, it is challenging to represent an image sequence in its intrinsically low-dimensional space in a noise-robust and rotation-robust manner. In this paper, we propose a novel-ordered sparse subspace clustering algorithm based on a p-norm to achieve an effective clustering of sequential data under heavy noise conditions. We also use the wavelet-histogram of oriented gradient (HOG) transform in the kernel view to extract both the global features (with the wavelet process) and the local features (with the HOG process) from the image. In addition, we assign different weights to different features to obtain a sparse coefficient matrix that helps to emphasize the global and local correlations in each sample. Similarly, the clustering algorithm based on the p-norm for sequential images emphasizes the within-class correlations amongst samples. Therefore, in this paper, we select additional denoising main components under a Laplacian constraint to achieve a better block-diagonal structure and highlight the independence of different clusters. Extensive experiments performed on various public datasets (including the ordered face dataset, handwritten recognition dataset, video scene segmentation dataset, and object recognition dataset) demonstrate that the proposed method is more resilient to noise and rotation than other representative sparse subspace clustering methods.},
  archive      = {J_EXSY},
  author       = {Liping Chen and Gongde Guo and Hui Wang},
  doi          = {10.1111/exsy.12368},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12368},
  shortjournal = {Expert Syst.},
  title        = {An ordered sparse subspace clustering algorithm based on p-norm},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Formal ways for measuring relations between concepts in
conceptual spaces†. <em>EXSY</em>, <em>38</em>(7), e12348. (<a
href="https://doi.org/10.1111/exsy.12348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The highly influential framework of conceptual spaces provides a geometric way of representing knowledge. Instances are represented by points in a high-dimensional space and concepts are represented by regions in this space. In this article, we extend our recent mathematical formalization of this framework by providing quantitative mathematical definitions for measuring relations between concepts. We develop formal ways for computing concept size, subsethood, implication, similarity, and betweenness. This considerably increases the representational capabilities of our formalization.},
  archive      = {J_EXSY},
  author       = {Lucas Bechberger and Kai-Uwe Kühnberger},
  doi          = {10.1111/exsy.12348},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12348},
  shortjournal = {Expert Syst.},
  title        = {Formal ways for measuring relations between concepts in conceptual spaces†},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emotion-aware polarity lexicons for twitter sentiment
analysis. <em>EXSY</em>, <em>38</em>(7), e12332. (<a
href="https://doi.org/10.1111/exsy.12332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretical frameworks in psychology map the relationships between emotions and sentiments. In this paper, we study the role of such mapping for computational emotion detection from text (e.g., social media) with an aim to understand the usefulness of an emotion-rich corpus of documents (e.g., tweets) to learn polarity lexicons for sentiment analysis. We propose two different methods that leverage a corpus of emotion-labelled tweets to learn word-polarity lexicons. The proposed methods model the emotion corpus using a generative unigram mixture model, combined with the emotion-sentiment mapping proposed in psychology for automated generation of word-polarity lexicons that capture emotion-rich vocabulary. We comparatively evaluate the quality of the proposed mixture model in learning emotion-aware sentiment lexicons with those generated using supervised latent dirichlet allocation (sLDA) and word-document-frequency (WDF) statistics. Sentiment analysis experiments on benchmark Twitter data sets confirm the quality of our proposed lexicons. Further, a comparative analysis with sLDA, WDF-based emotion-aware lexicons, and standard sentiment lexicons that are agnostic to emotion knowledge suggests that the proposed lexicons lead to a significantly better performance in both sentiment classification and sentiment intensity prediction tasks.},
  archive      = {J_EXSY},
  author       = {Anil Bandhakavi and Nirmalie Wiratunga and Stewart Massie and Deepak P.},
  doi          = {10.1111/exsy.12332},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12332},
  shortjournal = {Expert Syst.},
  title        = {Emotion-aware polarity lexicons for twitter sentiment analysis},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Job shop planning and scheduling for manufacturers with
manual operations. <em>EXSY</em>, <em>38</em>(7), e12315. (<a
href="https://doi.org/10.1111/exsy.12315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job shop scheduling systems are widely employed to optimise the efficiency of machine utilisation in the manufacturing industry, by searching the most cost-effective permutation of job operations based on the cost of each operation on each compatible machine and the relations between job operations. Such systems are paralysed when the cost of operations are not predictable led by the involvement of complex manual operations. This paper proposes a new genetic algorithm-based job shop scheduling system by integrating a fuzzy learning and inference subsystem in an effort to address this limitation. In particular, the fuzzy subsystem adaptively estimates the completion time and thus cost of each manual task under different conditions based on a knowledge base that is initialised by domain experts and then constantly updated based on its built-in learning ability and adaptability. The manufacturer of Point of Sale and Point of Purchase products has been utilised in this paper as an example case for both theoretical discussion and experimental study. The experimental results demonstrate the promising of the proposed system in improving the efficiency of manual manufacturing operations.},
  archive      = {J_EXSY},
  author       = {Longzhi Yang and Jie Li and Fei Chao and Phil Hackney and Mark Flanagan},
  doi          = {10.1111/exsy.12315},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12315},
  shortjournal = {Expert Syst.},
  title        = {Job shop planning and scheduling for manufacturers with manual operations},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the square root of probability density function
on riemannian manifold. <em>EXSY</em>, <em>38</em>(7), e12266. (<a
href="https://doi.org/10.1111/exsy.12266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose that the square root of a probability density function can be represented as a linear combination of Gaussian kernels. It is shown that if the Gaussian kernel centres and kernel width are known, then the maximum likelihood parameter estimator can be formulated as a Riemannian optimisation problem on sphere manifold. The first order Riemannian geometry of the sphere manifold and vector transport are initially explored, then the well-known Riemannian conjugate gradient algorithm is used to estimate the model parameters. For completeness, the k-means clustering algorithm and a grid search are employed to determine the centres and kernel width, respectively. Simulated examples are employed to demonstrate that the proposed approach is effective in constructing the estimate of the square root of probability density function.},
  archive      = {J_EXSY},
  author       = {Xia Hong and Junbin Gao},
  doi          = {10.1111/exsy.12266},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12266},
  shortjournal = {Expert Syst.},
  title        = {Estimating the square root of probability density function on riemannian manifold},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving e-learning recommendation by using background
knowledge. <em>EXSY</em>, <em>38</em>(7), e12265. (<a
href="https://doi.org/10.1111/exsy.12265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is currently a large amount of e-Learning resources available to learners on the Web. However, learners often have difficulty finding and retrieving relevant materials to support their learning goals because they lack the domain knowledge to craft effective queries that convey what they wish to learn. In addition, the unfamiliar vocabulary often used by domain experts makes it difficult to map a learner&#39;s query to a relevant learning material. We address these challenges by introducing an innovative method that automatically builds background knowledge for a learning domain. In creating our method, we exploit a structured collection of teaching materials as a guide for identifying the important domain concepts. We enrich the identified concepts with discovered text from an encyclopedia, thereby increasing the richness of our acquired knowledge. We employ the developed background knowledge for influencing the representation and retrieval of learning resources to improve e-Learning recommendation. The effectiveness of our method is evaluated using a collection of Machine Learning and Data Mining papers. Our method outperforms the benchmark, demonstrating the advantage of using background knowledge for improving the representation and recommendation of e-Learning materials.},
  archive      = {J_EXSY},
  author       = {Blessing Mbipom and Susan Craw and Stewart Massie},
  doi          = {10.1111/exsy.12265},
  journal      = {Expert Systems},
  month        = {11},
  number       = {7},
  pages        = {e12265},
  shortjournal = {Expert Syst.},
  title        = {Improving e-learning recommendation by using background knowledge},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on future hybrid artificial intelligence and
machine learning for smart expert systems. <em>EXSY</em>,
<em>38</em>(6), e12757. (<a
href="https://doi.org/10.1111/exsy.12757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Gustavo Ramirez-Gonzalez},
  doi          = {10.1111/exsy.12757},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12757},
  shortjournal = {Expert Syst.},
  title        = {Special issue on future hybrid artificial intelligence and machine learning for smart expert systems},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid decision-making framework under complex spherical
fuzzy prioritized weighted aggregation operators. <em>EXSY</em>,
<em>38</em>(6), e12712. (<a
href="https://doi.org/10.1111/exsy.12712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex spherical fuzzy set, an extended version of spherical fuzzy set, is a very powerful tool to capture fourfold information (typically yes, no, abstain and refusal), in which the range of degrees occurs in the complex plane with unit disk. Through this prominent feature, complex spherical fuzzy sets outperform earlier concepts of fuzzy sets and their extensions. This research article utilizes complex spherical fuzzy sets and prioritized weighted aggregation operators to construct the complex spherical fuzzy prioritized weighted averaging/geometric operators. We present their most noticeable properties. Further, we establish a decision-making approach that takes full advantage of the aforesaid operators. To explore their superiority and applicability in decision making, we apply our algorithm to a numerical example. Finally, we compare this decision-making approach with prevailing methods in this context.},
  archive      = {J_EXSY},
  author       = {Muhammad Akram and Ayesha Khan and José Carlos R. Alcantud and Gustavo Santos-García},
  doi          = {10.1111/exsy.12712},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12712},
  shortjournal = {Expert Syst.},
  title        = {A hybrid decision-making framework under complex spherical fuzzy prioritized weighted aggregation operators},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inmplode: A framework to interpret multiple related
rule-based models. <em>EXSY</em>, <em>38</em>(6), e12702. (<a
href="https://doi.org/10.1111/exsy.12702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing trend to split problems into separate subproblems and develop separate models for each (e.g., different churn models for separate customer segments; different failure prediction models for separate university courses, etc.). While it may lead to better predictive models, the use of multiple models makes interpretability more challenging. In this paper, we address the problem of synthesizing the knowledge contained in a set of models without a significant loss of prediction performance. We focus on decision tree models because their interpretability makes them suitable for problems involving knowledge extraction. We detail the process, identifying alternative methods to address the different phases involved. An extensive set of experiments is carried out on the problem of predicting the failure of students in courses at the University of Porto. We assess the effect of using different methods for the operations of the methodology, both in terms of the knowledge extracted as well as the accuracy of the combined models.},
  archive      = {J_EXSY},
  author       = {Pedro Strecht and João Mendes-Moreira and Carlos Soares},
  doi          = {10.1111/exsy.12702},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12702},
  shortjournal = {Expert Syst.},
  title        = {Inmplode: A framework to interpret multiple related rule-based models},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new feature extraction approach for script invariant
handwritten numeral recognition. <em>EXSY</em>, <em>38</em>(6), e12699.
(<a href="https://doi.org/10.1111/exsy.12699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten numeral recognition is a challenging research problem because of the enormous varieties of styles in which human beings write the numerals. Several researchers have tried to find solutions to this problem with exceptional recognition accuracies. However, most of these solutions have been dedicated to single script numerals. Such methods are inappropriate for multi-lingual nations such as India where a large number of scripts are used. Keeping this issue in mind, a new feature descriptor named symbolization of binary images ( SBI ) is introduced here for the recognition of handwritten numerals of different scripts. Effectiveness of SBI is supported with experiments showing its script-invariant nature. Classification of numerals using a multiclass support vector machine (SVM) classifier yields the recognition accuracies of 98.18, 96.22, 96.52, and 95.53% on datasets of numerals written in four popular scripts of the world: Arabic , Bangla , Devanagari , and Latin , respectively. This scheme has also been extended to the situation when the script used is not known a priori or the numerals written in a document belong to pairs of mixed scripts of { Arabic , Devanagari , Bangla } with Latin producing recognition rates of 92.97, 91.25, and 91.67%, respectively. When all four scripts are mixed, the recognition rate is still 90.98% overall. Encouraging outcomes suggest that the proposed SBI feature descriptor can recognize numerals invariant of the script class.},
  archive      = {J_EXSY},
  author       = {Pawan Kumar Singh and Iman Chatterjee and Ram Sarkar and Elisa Barney Smith and Mita Nasipuri},
  doi          = {10.1111/exsy.12699},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12699},
  shortjournal = {Expert Syst.},
  title        = {A new feature extraction approach for script invariant handwritten numeral recognition},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The boundary node method for multi-robot multi-goal path
planning problems. <em>EXSY</em>, <em>38</em>(6), e12691. (<a
href="https://doi.org/10.1111/exsy.12691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the multi-goal path planning problem to find the shortest collision-free path connecting a given set of goal points located in the robot working environment. This problem combines two sub-problems: first, optimize the sequence of the goal points located in the free workspace; second, compute the shortest collision-free path between goal points. In this study, a genetic algorithm is used to optimize the sequence of the goal points that the robot should visit. Once the sequence of the goal points is available, our developed method called Boundary Node Method ( BNM ) is applied to generate an initial collision-free path between every pair of the sequenced goal points. Subsequently, an additional developed method called Path Enhancement Method ( PEM ) is used to find an optimal or near-optimal path by reducing the overall initial path length. In the BNM , the robot and its immediate surroundings are simulated by a nine-node quadrilateral element, where the centroid node represents the robot&#39;s position. The robot moves between goals with boundary nodes in the working environment depending on the boundary node&#39;s potential value. The potential value of each point in the working environment is calculated based on the proposed potential function. Additionally, this article investigates the multi-goal path planning problem for multi-robot systems, when each goal reached by several robots. Finally, simulations and experiments are performed on a real mobile robot to demonstrate the effectiveness of the developed methods.},
  archive      = {J_EXSY},
  author       = {Raza Abdulla Saeed and Diego Reforgiato Recupero and Paolo Remagnino},
  doi          = {10.1111/exsy.12691},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12691},
  shortjournal = {Expert Syst.},
  title        = {The boundary node method for multi-robot multi-goal path planning problems},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decision support system on credit operation using linear and
logistic regression. <em>EXSY</em>, <em>38</em>(6), e12578. (<a
href="https://doi.org/10.1111/exsy.12578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The act of lending is based on trust in the borrower to honour the obligation of paying back the lender. Greater spreads on credit operations may help predict the expected recovery of the credit, based on the sufficiency and liquidity of the guarantee. This study aims to understand how predictive models can provide different estimations of expected recovery based on the same data sets. It classifies credit by the formulation of a rule that describes the values of a categorical variable according to some specified definition. It finds that a simple logistic regression model can easily be extended to a multiple logistic regression model by integrating more than one prediction variable, which indicates increasing difficulty in obtaining multiple observations with an increasing number of independent variables. It compares the efficiency of the logistic regression with that of a linear regression in predicting whether recovery is due in a credit operation, and, thus, identifies the best model for this purpose.},
  archive      = {J_EXSY},
  author       = {Germanno Teles and Joel J. P. C. Rodrigues and Sergei A. Kozlov and Ricardo A. L. Rabêlo and Victor Hugo C. Albuquerque},
  doi          = {10.1111/exsy.12578},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12578},
  shortjournal = {Expert Syst.},
  title        = {Decision support system on credit operation using linear and logistic regression},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ResNet-attention model for human authentication using ECG
signals. <em>EXSY</em>, <em>38</em>(6), e12547. (<a
href="https://doi.org/10.1111/exsy.12547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication is the process of verifying the claimed identity of the user. Recently, traditional authentication methods such as passwords, tokens, and so on are no longer used for authentication as they are more prone to theft and different types of violations. Therefore, new authentication approaches based on biometric modalities such as heartbeat pattern obtained from electrocardiogram (ECG) signals are considered. Unlike other biometrics, ECG provides the assurance that the person is alive, and is considered as one of the most accurate recent methods for authentication. In this article, two end-to-end deep neural network models for ECG-based authentication are proposed. In the first model, a convolutional neural network (CNN) is developed and in the second model, a residual convolutional neural network (ResNet) with attention mechanism called ResNet-Attention is designed for human authentication. We have used 2-s duration ECG signals obtained from two ECG databases (Physikalisch-Technische Bundesanstalt [PTB] and Check Your Bio-signals Here initiative [CYBHi]) for authentication. Our proposed ResNet-Attention algorithm achieved an accuracy of 98.85 and 99.27% using PTB and CYBHi, respectively. The results obtained by our developed model show that the performance is better than existing algorithms and can be used in real-time authentication systems after the validation with more diverse ECG data.},
  archive      = {J_EXSY},
  author       = {Mohamed Hammad and Paweł Pławiak and Kuanquan Wang and Udyavara Rajendra Acharya},
  doi          = {10.1111/exsy.12547},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12547},
  shortjournal = {Expert Syst.},
  title        = {ResNet-attention model for human authentication using ECG signals},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial plant optimization algorithm to detect infected
leaves using machine learning. <em>EXSY</em>, <em>38</em>(6), e12501.
(<a href="https://doi.org/10.1111/exsy.12501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant leaves play an important role in the diagnosis of plant diseases. Losses from such diseases can have a significant economic as well as environmental impact. Thus, examination of leaves into a healthy or infected carries substantial importance. An improved artificial plant optimization (IAPO) algorithm using machine learning has been introduced that identifies the plant diseases and categorize the leaves into healthy and infected on a private dataset of 236 images. Features are extracted from the images using histogram of oriented gradients (descriptor). The concepts of artificial plant optimization are then applied to study the features of healthy leaves using IAPO. A machine learning algorithm has been created to make the model adaptive with varied datasets. The degree of infection is eventually computed, and the leaves with infection greater than a certain calculated threshold are classified as infected leaves. The results show that IAPO can be used for classification of infected and healthy leaves and this algorithm can be generalized to solve problems in other domains as well. The proposed IAPO is also compared with other classification algorithms including k-nearest neighbours, support vector machine, random forest and convolution neural network that show accuracies of 78.24%, 83.48%, 87.83%, and 91.26%, respectively, whereas IAPO shows quite accurate results in classification of leaves with an accuracy of 97.45% on training set and 95.0% accuracy on test set.},
  archive      = {J_EXSY},
  author       = {Deepak Gupta and Prerna Sharma and Krishna Choudhary and Kshitij Gupta and Rahul Chawla and Ashish Khanna and Victor Hugo C. de Albuquerque},
  doi          = {10.1111/exsy.12501},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12501},
  shortjournal = {Expert Syst.},
  title        = {Artificial plant optimization algorithm to detect infected leaves using machine learning},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residents’ perceptions of smart energy meters.
<em>EXSY</em>, <em>38</em>(6), e12500. (<a
href="https://doi.org/10.1111/exsy.12500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart meters are a form of expert system with performance features beyond energy-consumption record keeping, to include monitoring, analysing, and estimating meter readings. Although smart meters have great capabilities, this technology is still in its infancy in many developing countries, and little is known about the kinds of risks associated with smart meters from residents&#39; perspectives. This research therefore aims to fill this gap by examining the influence of four different types of perceived risk on residents&#39; intentions to use smart meters in Jordan. By following a quantitative approach, 242 survey responses were tested by using structural equation modelling–partial least squares. The statistical results indicated that perceived security and technical risks have a significant and negative impact on residents&#39; intentions to use smart meters. However, perceived privacy and health risks, surprisingly, were found to have no significant negative influence on intention to use. Theoretical and practical implications are indicated, and directions of future research are subsequently specified.},
  archive      = {J_EXSY},
  author       = {Ahmed Haitham Shuhaiber},
  doi          = {10.1111/exsy.12500},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12500},
  shortjournal = {Expert Syst.},
  title        = {Residents&#39; perceptions of smart energy meters},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 2DSM vs FFDM: A computeraided diagnosis based comparative
study for the early detection of breast cancer. <em>EXSY</em>,
<em>38</em>(6), e12474. (<a
href="https://doi.org/10.1111/exsy.12474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {U. Raghavendra and Anjan Gudigar and Edward J. Ciaccio and Kwan Hoong Ng and Wai Yee Chan and Kartini Rahmat and U. Rajendra Acharya},
  doi          = {10.1111/exsy.12474},
  journal      = {Expert Systems},
  month        = {9},
  number       = {6},
  pages        = {e12474},
  shortjournal = {Expert Syst.},
  title        = {2DSM vs FFDM: A computeraided diagnosis based comparative study for the early detection of breast cancer},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on recent advances in data science and
systems. <em>EXSY</em>, <em>38</em>(5), e12735. (<a
href="https://doi.org/10.1111/exsy.12735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Longzhi Yang and Jia Hu and Che-Lun Hung},
  doi          = {10.1111/exsy.12735},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12735},
  shortjournal = {Expert Syst.},
  title        = {Special issue on recent advances in data science and systems},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue: WorldCist18. <em>EXSY</em>, <em>38</em>(5),
e12722. (<a href="https://doi.org/10.1111/exsy.12722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Alberto Freitas},
  doi          = {10.1111/exsy.12722},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12722},
  shortjournal = {Expert Syst.},
  title        = {Special issue: WorldCist18},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neutrosophic game pricing methods with risk aversion for
pricing of data products. <em>EXSY</em>, <em>38</em>(5), e12697. (<a
href="https://doi.org/10.1111/exsy.12697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the progressive development of satellite image data products, their pricing strategies become more and more important for enterprises to earn profits. The objective of this study is to explore several game pricing methods with risk aversion for pricing of data products in neutrosophic environments. First, to reflect the uncertainty of problem parameters, the idea of neutrosophic variables is adopted. With the aid of neutrosophic variables, the truth, indeterminacy and falsity degrees of players can be intuitively and conveniently obtained. Subsequently, considering the risk aversion of decision makers, the optimistic value theory is introduced into neutrosophic variables for calculating the optimistic value of player&#39;s profits. Then, different pricing models are constructed under the Bertrand and Stackelberg game scenarios, respectively. After deriving the corresponding equilibrium equations, some numerical instances are provided to testify the feasibility of our methods. Furthermore, the influences of dissimilar market power structures are examined. Finally, the effects of seven problem parameters and players&#39; confidence levels on pricing results are investigated through sensitivity analyses. The results show that the proposed methods are practicable and can offer guidance for the pricing decision of data products.},
  archive      = {J_EXSY},
  author       = {Suizhi Luo and Lining Xing},
  doi          = {10.1111/exsy.12697},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12697},
  shortjournal = {Expert Syst.},
  title        = {Neutrosophic game pricing methods with risk aversion for pricing of data products},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ranking range models under incomplete attribute weight
information in the selected six MADM methods. <em>EXSY</em>,
<em>38</em>(5), e12696. (<a
href="https://doi.org/10.1111/exsy.12696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple attribute decision making (MADM) is used to rank the alternatives according to evaluation information based on multiple attributes, and many MADM methods have been studied to deal with the MADM problems. In existing MADM methods, when setting different attribute weights, the ranking of alternatives are different. And ranking range can be used to measure a lower bound and an upper bound of rankings of alternatives with the change of the attribute weights. Also, in some real MADM problems, the information on attribute weights may be unknown or partially known, which is called incomplete attribute weight information. Then, this study investigates the ranking range models (RRMs) under incomplete attribute weight information in the selected six MADM methods: Weighted geometric averaging (WGA), Ordered weighted geometric averaging (OWGA), TOPSIS, VIKOR, PROMETHEE and ELECTRE. Particularly, we can construct several 0-1 mathematical programming models to compute the ranking range of alternatives under incomplete attribute weight information for the selected six MADM methods. Then, two case studies on project investment and Academic Ranking of World Universities (ARWU) are used to justify the validity of the RRMs under incomplete attribute weight information in the selected six MADM methods.},
  archive      = {J_EXSY},
  author       = {Yating Liu and Huali Tang and Haiming Liang and Hengjie Zhang and Cong-Cong Li and Yucheng Dong},
  doi          = {10.1111/exsy.12696},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12696},
  shortjournal = {Expert Syst.},
  title        = {Ranking range models under incomplete attribute weight information in the selected six MADM methods},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On-line modelling and planning for urban traffic control.
<em>EXSY</em>, <em>38</em>(5), e12693. (<a
href="https://doi.org/10.1111/exsy.12693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban Traffic Control is a key problem for most big cities. Current approaches to handle the city traffic rely on controlling traffic lights. The systems in operation range from static control of traffic light phases to adaptive systems based on numeric models and traffic sensors. Recently, some planning-based approaches have also been proposed. These approaches work at a higher level of abstraction, but have been found to work well if complemented by low-level systems. We have identified two main difficulties for the wide use of planning techniques in this domain: generating the control models is a difficult task; and some algorithms scale poorly. In this paper we present Automated Planning for Traffic Control (APTC), a control system based on Automated Planning, that successfully overcomes these two problems. It combines techniques that continuously: learn an accurate planning model; and also divide the city for distributed reasoning in order to scale to large city networks. Experimental results show that APTC outperforms static approaches as well as other planning-based systems. We also show that the combination of both approaches improves compared with using only one of them.},
  archive      = {J_EXSY},
  author       = {Alberto Pozanco and Susana Fernández and Daniel Borrajo},
  doi          = {10.1111/exsy.12693},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12693},
  shortjournal = {Expert Syst.},
  title        = {On-line modelling and planning for urban traffic control},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from mistakes: Improving spelling correction
performance with automatic generation of realistic misspellings.
<em>EXSY</em>, <em>38</em>(5), e12692. (<a
href="https://doi.org/10.1111/exsy.12692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence to sequence models (seq2seq) require a large amount of labelled training data to learn the mapping between the input and output. A large set of misspelled words together with their corrections is needed to train a seq2seq spelling correction system. Low-resource languages such as Turkish usually lack such large annotated datasets. Although misspelling-reference pairs can be synthesized with a random procedure, the generated dataset may not well match to genuine human-made misspellings. This might degrade the performance in realistic test scenarios. In this paper, we propose a novel procedure to automatically introduce human-like misspellings to legitimate words in Turkish language. Generated human-like misspellings are used to improve the performance of a seq2seq spelling correction system. The proposed system consists of two separate models; a misspelling generator and a spelling corrector. The generator is trained using a relatively small number of human-made misspellings and their manual corrections. Reference words and their misspellings are used as inputs and outputs of the generator, respectively. As a result, it is trained to add realistic spelling errors to the valid words. Training data of the spelling corrector is augmented by the generator&#39;s human-like misspellings. In the experiments, we observe that the data augmentation significantly improves the spelling correction performance. Our proposed method yields 5% absolute improvement over the state-of-the-art Turkish spelling correction systems in a test set which contains human-made misspellings from Twitter messages.},
  archive      = {J_EXSY},
  author       = {Osman Büyük and Levent M. Arslan},
  doi          = {10.1111/exsy.12692},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12692},
  shortjournal = {Expert Syst.},
  title        = {Learning from mistakes: Improving spelling correction performance with automatic generation of realistic misspellings},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate-bounded gaussian mixture model with minimum
message length criterion for model selection. <em>EXSY</em>,
<em>38</em>(5), e12688. (<a
href="https://doi.org/10.1111/exsy.12688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bounded support Gaussian mixture model (BGMM) has been proposed for data modelling as an alternative to unbounded support mixture models for the cases when the data lies in bounded support. In this paper, we propose applications of multivariate BGMM in data clustering for more insightful analysis of the model. We also propose minimum message length (MML) criterion for model selection in data clustering using multivariate BGMM. The presented model is applied to data clustering in several speech (TSP and Spoken Digits) and image databases (MNIST and Fashion MNIST). We also propose the application of BGMM in code-book generation at feature extraction phase. Inspired by the success of bag of visual words approach in computer vision, it is also introduced in speech data representation and validated through experiments presented in this paper. For validation of model selection criterion, MML is applied to different medical, speech and image datasets. Experimental results obtained during the model selection through MML are further compared with seven different model selection criteria. The results presented in the paper demonstrate the effectiveness of BGMM for clustering speech and image databases, code-book generation through clustering for feature representation and model selection.},
  archive      = {J_EXSY},
  author       = {Muhammad Azam and Nizar Bouguila},
  doi          = {10.1111/exsy.12688},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12688},
  shortjournal = {Expert Syst.},
  title        = {Multivariate-bounded gaussian mixture model with minimum message length criterion for model selection},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label learning on principles of reverse k-nearest
neighbourhood. <em>EXSY</em>, <em>38</em>(5), e12615. (<a
href="https://doi.org/10.1111/exsy.12615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel neighbourhood based multi-label classifier, Multi-label Learning on principles of Reverse k-Nearest Neighbourhood (ML-RkNN) where we estimate the neighbourhood of the points on the basis of their reverse k -nearest neighbourhood (RkNN). Through RkNN, for the same value of k , we get different number of neighbours for different instances and this happens adaptively according to the neighbourhood configuration of the points. The automatically adaptive neighbourhood helps us in better learning of the local configurations around the points. Our scheme also facilitates implicit handling of the local imbalances prevailing in the datasets by comparing the class distributions of the test points and their reverse nearest neighbours. This implicit and adaptive handling is particularly useful for multi-label label datasets, whose labels are differentially imbalanced. Empirical study is performed on 10 real-world multi-label datasets considering five neighbourhood based multi-label learners. Macro F 1 is used as the evaluating metric. The proposed method has given statistically superior and statistically comparable performances with respect to three and two comparing methods respectively. Additionally, we have explored the use of two different distance metrics, Euclidean and Jaccard in our scheme for nominal datasets.},
  archive      = {J_EXSY},
  author       = {Payel Sadhukhan and Sarbani Palit},
  doi          = {10.1111/exsy.12615},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12615},
  shortjournal = {Expert Syst.},
  title        = {Multi-label learning on principles of reverse k-nearest neighbourhood},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What makes trading strategies based on chart pattern
recognition profitable? <em>EXSY</em>, <em>38</em>(5), e12596. (<a
href="https://doi.org/10.1111/exsy.12596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating chart pattern recognition is a relevant issue addressed by researchers and practitioners when designing a system that considers technical analysis for trading purposes. This article proposes the design of a trading system that takes into account any generic pattern that has been proven to be profitable in the past, without restricting the search to the specific technical patterns reported in the literature, hence the term generic pattern recognition . A fast version of dynamic time warping, the University College Riverside subsequence search suite (called the UCR suite), is employed for the pattern recognition task in an effort to produce trading signals in realistic timescales. This article evaluates the significance of the relation between the system&#39;s profitability and (a) the pattern length, (b) the take-profit and stop-loss levels and (c) the performance consensus of past patterns. The trading system is assessed under the mean–variance perspective by using 560 NYSE stocks. The results obtained by the different parameter configurations are reported, controlling for both data-snooping and transaction costs. On average, the proposed system dominates the market index in the mean–variance sense. Although transaction costs reduce the profitability of the proposed trading system, 92.5% of the experiments are profitable if the analysis is reduced to the parameter values aligned with the technical analysis.},
  archive      = {J_EXSY},
  author       = {Prodromos Tsinaslanidis and Francisco Guijarro},
  doi          = {10.1111/exsy.12596},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12596},
  shortjournal = {Expert Syst.},
  title        = {What makes trading strategies based on chart pattern recognition profitable?},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building a cognizant honeypot for detecting active
fingerprinting attacks using dynamic fuzzy rule interpolation.
<em>EXSY</em>, <em>38</em>(5), e12557. (<a
href="https://doi.org/10.1111/exsy.12557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic fuzzy rule interpolation (D-FRI) technique delivers a dynamic rule base through the utilisation of fuzzy rule interpolation to infer more accurate results for a given application problem. D-FRI offered dynamic rule base is very useful in security areas where network conditions are always volatile and require the most updated rule base. A honeypot is a vital part of any security infrastructure for directly investigating attacks and attackers in real-time to strengthen the overall security of the network. However, a honeypot as a concealed system can only function successfully while its identity is not revealed to any attackers. Attackers always attempt to uncover such honeypots for avoiding any trap and strengthening their attacks. Active fingerprinting attack is used to detect these honeypots by injecting purposefully designed traffic to a network. Such an attack can be prevented by controlling the traffic but this will make honeypot unusable system if its interaction with the outside world is limited. Alternatively, it is practically more useful if this fingerprinting attack is detected in real-time to manage its immediate consequences and preventing the honeypot. This article offers an approach to building a cognizant honeypot for detecting active fingerprinting attacks through the utilisation of the established D-FRI technique. It is based on the use of just a sparse rule base while remaining capable of detecting active fingerprinting attacks when the system does not find any matching rules. Also, it learns from current network conditions and offers a dynamic rule base to facilitate more accurate and efficient detection.},
  archive      = {J_EXSY},
  author       = {Nitin Naik and Changjing Shang and Paul Jenkins and Qiang Shen},
  doi          = {10.1111/exsy.12557},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12557},
  shortjournal = {Expert Syst.},
  title        = {Building a cognizant honeypot for detecting active fingerprinting attacks using dynamic fuzzy rule interpolation},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive stacked hourglass network with kalman filter for
estimating 2D human pose in video. <em>EXSY</em>, <em>38</em>(5),
e12552. (<a href="https://doi.org/10.1111/exsy.12552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges in computer science and image processing is 2D human pose estimation. Specifically, occlusion and in particular occlusion of human joints caused by camera angle are of paramount importance. In this paper, a new highly accurate network was proposed that can estimate 2D human poses in video images using deep learning. We employ the Single Shot MultiBox Detector network to detect the centre position of each human within a video frame and then use the stacked hourglass network to estimate the 2D human pose. We approximate the human motion as a linear motion between different frames in a certain period; and optimize the human centres based on the local outlier factor and Kalman filters. The same method is applied to optimize the human pose estimations in video, which can address the inaccurate prediction caused by human joints occlusion. The proposed adaptive network is tested using the two well-known benchmarks for human pose estimation (MPII and Joint-annotated Human Motion Data Base datasets), and we also generate some 2D human pose estimating qualitative results of single and multiple people in Internet videos. The experimental results show that the proposed network has strong practicability and can achieve high accuracy on adaptive estimating the 2D human pose in video.},
  archive      = {J_EXSY},
  author       = {Tao Hu and Chunxia Xiao and Geyong Min and Noushin Najjari},
  doi          = {10.1111/exsy.12552},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12552},
  shortjournal = {Expert Syst.},
  title        = {An adaptive stacked hourglass network with kalman filter for estimating 2D human pose in video},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient weighted probabilistic frequent itemset mining in
uncertain databases. <em>EXSY</em>, <em>38</em>(5), e12551. (<a
href="https://doi.org/10.1111/exsy.12551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertain data mining has attracted so much interest in many emerging applications over the past decade. An issue of particular interest is to discover the frequent itemsets in uncertain databases. As an item would not appear in a transaction of such database for certain, several probability models are presented to measure the frequency of an itemset, and the frequent itemset over probabilistic data generally has two different definitions: the expected support-based frequent itemset and probabilistic frequent itemset. Meanwhile, it is noted that the frequency itself cannot identify useful or meaningful patterns in some scenarios. Other measures such as the importance of items should be also taken into account. To this end, some studies recently have been done on weighted (importance) frequent itemset mining in uncertain databases. However, they are only designed for the expected support-based frequent itemset, and suffer from low efficiency due to generating too many frequent itemset candidates. To address this issue, we propose a novel weighted probabilistic frequent itemsets (w-PFIs) algorithm. Moreover, we derive a probability model for the support of a w-PFI candidate in our method and present three pruning techniques to narrow the search space and remove the unpromising candidates immediately. Extensive experiments have been conducted on both real and synthetic datasets, to evaluate the performance of our w-PFI algorithm in terms of runtime, accuracy and scalability. Results show that our algorithm yields the best performance among the existing algorithms.},
  archive      = {J_EXSY},
  author       = {Zhiyang Li and Fengjuan Chen and Junfeng Wu and Zhaobin Liu and Weijiang Liu},
  doi          = {10.1111/exsy.12551},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12551},
  shortjournal = {Expert Syst.},
  title        = {Efficient weighted probabilistic frequent itemset mining in uncertain databases},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hand gesture recognition using multimodal data fusion and
multiscale parallel convolutional neural network for human–robot
interaction. <em>EXSY</em>, <em>38</em>(5), e12490. (<a
href="https://doi.org/10.1111/exsy.12490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture recognition plays an important role in human–robot interaction. The accuracy and reliability of hand gesture recognition are the keys to gesture-based human–robot interaction tasks. To solve this problem, a method based on multimodal data fusion and multiscale parallel convolutional neural network (CNN) is proposed in this paper to improve the accuracy and reliability of hand gesture recognition. First of all, data fusion is conducted on the sEMG signal, the RGB image, and the depth image of hand gestures. Then, the fused images are generated to two different scale images by downsampling, which are respectively input into two subnetworks of the parallel CNN to obtain two hand gesture recognition results. After that, hand gesture recognition results of the parallel CNN are combined to obtain the final hand gesture recognition result. Finally, experiments are carried out on a self-made database containing 10 common hand gestures, which verify the effectiveness and superiority of the proposed method for hand gesture recognition. In addition, the proposed method is applied to a seven-degree-of-freedom bionic manipulator to achieve robotic manipulation with hand gestures.},
  archive      = {J_EXSY},
  author       = {Qing Gao and Jinguo Liu and Zhaojie Ju},
  doi          = {10.1111/exsy.12490},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12490},
  shortjournal = {Expert Syst.},
  title        = {Hand gesture recognition using multimodal data fusion and multiscale parallel convolutional neural network for human–robot interaction},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The use of a weighted affective lexicon in a tutoring system
to improve student motivation to learn. <em>EXSY</em>, <em>38</em>(5),
e12483. (<a href="https://doi.org/10.1111/exsy.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, several educational systems have integrated text-based affective feedback. However, analyses of the educational lexicon from the affectivity perspective are scarce. The provision of affective support by these systems is significant because it improves student motivation to learn. This study analyzes the use of a weighted affective lexicon in tutoring systems to improve student motivation. First, based on the suggestions of 166 undergraduate students, this study constructed and evaluated an educational lexicon in Spanish. Additionally, after considering the assessment of another group of 185 undergraduate students, this study provides an interpretation of words/phrases on a valence and arousal scale. The authors carried out a comprehensive analysis to categorize the words/phrases. Furthermore, this study integrates an affective lexicon in a tutoring system. Finally, this paper presents a review of the impacts of this system on the motivation of its potential users. The authors believe that the results of the present study will support the development of a highly adopted Intelligent Tutoring Systems (ITSs), which will benefit prospective users.},
  archive      = {J_EXSY},
  author       = {Samantha Jiménez and Reyes Juárez-Ramírez and Víctor H. Castillo and Alan Ramírez-Noriega and Ángeles Quezada},
  doi          = {10.1111/exsy.12483},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12483},
  shortjournal = {Expert Syst.},
  title        = {The use of a weighted affective lexicon in a tutoring system to improve student motivation to learn},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A sentiment analysis approach to improve authorship
identification. <em>EXSY</em>, <em>38</em>(5), e12469. (<a
href="https://doi.org/10.1111/exsy.12469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writing style is considered the manner in which an author expresses his thoughts, influenced by language characteristics, period, school, or nation. Often, this writing style can identify the author. One of the most famous examples comes from 1914 in Portuguese literature. With Fernando Pessoa and his heteronyms Alberto Caeiro, Álvaro de Campos, and Ricardo Reis, who had completely different writing styles, led people to believe that they were different individuals. Currently, the discussion of authorship identification is more relevant because of the considerable amount of widespread fake news in social media, in which it is hard to identify who authored a text and even a simple quote can impact the public image of an author, especially if these texts or quotes are from politicians. This paper presents a process to analyse the emotion contained in social media messages such as Facebook to identify the author&#39;s emotional profile and use it to improve the ability to predict the author of the message. Using preprocessing techniques, lexicon-based approaches, and machine learning, we achieved an authorship identification improvement of approximately 5% in the whole dataset and more than 50% in specific authors when considering the emotional profile on the writing style, thus increasing the ability to identify the author of a text by considering only the author&#39;s emotional profile, previously detected from prior texts.},
  archive      = {J_EXSY},
  author       = {Ricardo Martins and José João Almeida and Pedro Henriques and Paulo Novais},
  doi          = {10.1111/exsy.12469},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12469},
  shortjournal = {Expert Syst.},
  title        = {A sentiment analysis approach to improve authorship identification},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of user interface design metrics by generating
realistic-looking dashboard samples. <em>EXSY</em>, <em>38</em>(5),
e12434. (<a href="https://doi.org/10.1111/exsy.12434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of user interfaces using quantitative metrics is a straightforward way to quickly measure interface usability and other various design aspects (such as the suitability of page layout or selected colors). Development and evaluation of objective metrics corresponding with user perception, however, usually requires a sufficiently large training set of user interface samples. Finding real user interface samples might not be easy. Therefore, we rather use generated samples. In such case, we need to provide a realistic-looking appearance of samples. This paper describes a workflow of the preparation of such samples. It presents a configurable generator based on the composition of simple widgets according to a predefined model. It also describes a reusable library for simple creation of widgets using capabilities of the JavaScript framework Vue.js. Finally, we demonstrate the applicability of the generator on a generation of dashboard samples which are used to evaluate existing metrics of interface aesthetics and show the possibility of their improvement.},
  archive      = {J_EXSY},
  author       = {Olena Pastushenko and Jiří Hynek and Tomáš Hruška},
  doi          = {10.1111/exsy.12434},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12434},
  shortjournal = {Expert Syst.},
  title        = {Evaluation of user interface design metrics by generating realistic-looking dashboard samples},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Authoring tools for creating 360 multisensory
videos—evaluation of different interfaces. <em>EXSY</em>,
<em>38</em>(5), e12418. (<a
href="https://doi.org/10.1111/exsy.12418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authoring 360 multisensory videos is a true challenge as the authoring tools available are scarce and restrictive. In this paper, we propose an authoring tool with three different authoring interfaces (desktop, immersive, and tangible interface) for creating multisensory 360 videos with the advantage of having a live preview of the multisensory content that is being produced. An evaluation of the three authoring tools having into account gender, system usability, presence, satisfaction, and effectiveness (time to accomplish tasks, number of errors, and number of help requests) is presented. The sample consisted of 48 participants (24 males and 24 females) evenly distributed between the different interfaces (8 males and 8 females for each interface). The results revealed that gender does not have any impact in the studied interfaces regarding all the dependent variables; immersive and tangible interfaces have higher levels of satisfaction than desktop interface as it allows more interaction freedom, and desktop interface have the lowest time to accomplish the tasks because people are more familiar with keyboard and mouse.},
  archive      = {J_EXSY},
  author       = {Hugo Coelho and Miguel Melo and Luís Barbosa and José Martins and Mário Sérgio Teixeira and Maximino Bessa},
  doi          = {10.1111/exsy.12418},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12418},
  shortjournal = {Expert Syst.},
  title        = {Authoring tools for creating 360 multisensory videos—Evaluation of different interfaces},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing decision making by providing a unified system for
computer-interpretable guideline management. <em>EXSY</em>,
<em>38</em>(5), e12412. (<a
href="https://doi.org/10.1111/exsy.12412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for integration of clinical practice guidelines (CPGs) in daily clinical practice calls for computational systems able to operationalise their knowledge and provide an enhanced experience in their enactment. Current approaches lack in functionalities such as scheduling and temporal management of CPGs, the combination of CPGs, and user-friendly systems for computer-interpretable guideline (CIG) creation and editing. This paper presents a comprehensive architecture for the deployment of CIGs, featuring components that allow the following: the creation and manipulation of clinical practice guideline knowledge elements, execution of CIGs with the temporal verification of clinical tasks, and drug conflict identification and resolution. This comprehensive approach provides a step-by-step assistant for health care professionals in the form of an agenda of activities that detects drug interactions when they are prescribed simultaneously and applies a mitigation algorithm to select possible and conflict-free alternatives. This work addresses the lack of a unified pipeline and mitigation features shown in approaches to CIG conflict mitigation in use.},
  archive      = {J_EXSY},
  author       = {António Silva and Tiago Oliveira and Filipe Gonçalves and Paulo Novais},
  doi          = {10.1111/exsy.12412},
  journal      = {Expert Systems},
  month        = {8},
  number       = {5},
  pages        = {e12412},
  shortjournal = {Expert Syst.},
  title        = {Enhancing decision making by providing a unified system for computer-interpretable guideline management},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The journal of knowledge engineering special issue on
WorldCist’19—seventh world conference on information systems and
technologies. <em>EXSY</em>, <em>38</em>(4), e12711. (<a
href="https://doi.org/10.1111/exsy.12711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Fernando Moreira},
  doi          = {10.1111/exsy.12711},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12711},
  shortjournal = {Expert Syst.},
  title        = {The journal of knowledge engineering special issue on WorldCist&#39;19—Seventh world conference on information systems and technologies},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special section on mining knowledge from scientific data.
<em>EXSY</em>, <em>38</em>(4), e12710. (<a
href="https://doi.org/10.1111/exsy.12710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Tanmoy Chakraborty and Sumit Bhatia and Cornelia Caragea},
  doi          = {10.1111/exsy.12710},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12710},
  shortjournal = {Expert Syst.},
  title        = {Special section on mining knowledge from scientific data},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temperature dependent optimal power flow using chaotic whale
optimization algorithm. <em>EXSY</em>, <em>38</em>(4), e12685. (<a
href="https://doi.org/10.1111/exsy.12685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a novel, nature inspired evolutionary based approach, the chaotic whale optimization algorithm, to solve a temperature dependent optimal power flow problem of a power system. Whale optimization is inspired by the bubble-net hunting strategy of the humpback whales; logistic chaotic maps are used to improve its performance. Whale optimization and our proposal are evaluated on three test systems namely, the IEEE 30-bus test power system, the 2383-bus Winter Peak Polish system and the 2736-bus Summer Peak Polish system to give a solution to the temperature dependant optimal power flow of the power systems where control of generator bus voltages, transformer tap ratios and reactive power sources are involved. Minimization of total fuel cost is considered here as the objective function for this problem. The superiority and the effectiveness of the proposed algorithm technique have been exhibited in comparison to the other evolutionary optimization techniques identified in the recent literature.},
  archive      = {J_EXSY},
  author       = {Dharmbir Prasad and Aparajita Mukherjee and Vivekananda Mukherjee},
  doi          = {10.1111/exsy.12685},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12685},
  shortjournal = {Expert Syst.},
  title        = {Temperature dependent optimal power flow using chaotic whale optimization algorithm},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approach for constructing expert yellow pages for
community question answering sites. <em>EXSY</em>, <em>38</em>(4),
e12684. (<a href="https://doi.org/10.1111/exsy.12684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid increase in the number of community-based question-and-answer services is attracting many users. Questions are posted and answered by community members. These users, who can help other users answer questions, can be considered experts. To facilitate finding a suitable expert and alleviate information overload, in this paper, expert yellow pages (EYP) for community question answering (CQA) are constructed. Considering the various lengths of texts, the biterm topic model (BTM) is used to model questions and fields of expertise. Then, two-dimensional EYP (2DEYP), which are composed of expertise field dimensions and question dimensions, are constructed. The intersections represent the cluster of experts. The proposed 2DEYP can be expanded both laterally and vertically for a more in-depth understanding and a more precise location. As the closer neurons represent similar topics, a novel labelling method is proposed to identify topic words for navigation. The method uses the distance between neurons as the differentiation capability. To further distinguish experts, a ranking mechanism is proposed. The experts can be ranked by integrating their expertise and activity levels. The expertise level is novel and characterized by both breadth and depth aspects. The proposed approach is evaluated via a real dataset, and the experimental results show that the proposed algorithm is feasible and performs well.},
  archive      = {J_EXSY},
  author       = {Ming Li and Xiaoyu Qi and Ying Li and Xiuzhi Lu and Li Wang},
  doi          = {10.1111/exsy.12684},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12684},
  shortjournal = {Expert Syst.},
  title        = {An approach for constructing expert yellow pages for community question answering sites},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible integrated scheduling algorithm based on remaining
work probability selection coding. <em>EXSY</em>, <em>38</em>(4),
e12683. (<a href="https://doi.org/10.1111/exsy.12683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the integrated scheduling problem of tree-structured products with flexible machine selection, this article proposes a flexible integrated scheduling algorithm based on remaining work probability selection coding. The algorithm is based on the framework of a genetic algorithm. First, in order to ensure the diversity and goodness of the initial population, an encoding method based on remaining work probability selection is proposed. Second, two new different crossover and mutation methods are designed based on operation and position respectively, which ensure the legitimacy of the generated offspring individuals. Then, in order to enhance the searchability of the algorithm for an optimal solution of the problem, a local search strategy based on the machine is proposed. Finally, a simple and effective decoding method based on the idle period is given. The algorithm is tested by the existing instance and randomly generated instances. The experimental results show that the proposed algorithm&#39;s solving speed and solution quality outperform other comparison algorithms.},
  archive      = {J_EXSY},
  author       = {Yilong Gao and Zhiqiang Xie and Dan Yang and Xu Yu},
  doi          = {10.1111/exsy.12683},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12683},
  shortjournal = {Expert Syst.},
  title        = {Flexible integrated scheduling algorithm based on remaining work probability selection coding},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An attention-driven videogame based on steady-state motion
visual evoked potentials. <em>EXSY</em>, <em>38</em>(4), e12682. (<a
href="https://doi.org/10.1111/exsy.12682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Neuroscience, steady-state visually evoked potentials (SSVEPs) have been generally used for the characterization of dynamic visual processes along the afferent pathway. In Neuroengineering, SSVEP-based brain-computer interfaces (SSVEP-BCI) have been used in applications such as communications or entertainment for the detection of the overt attention to static-flickering stimuli or other structured stimulation that involves motion (SSMVEP-BCI). In this work, we propose an attention-driven videogame controlled by an SSVEP-BCI with mobile-flickering stimuli. In this game, enemy avatars launch attacks that are presented as mobile ring-shaped checkerboards that flicker at 15 Hz to the player, who must deflect them by exerting attention. We detected the attention of the participants based on the power spectral density of the elicited SSVEP and the adjacent electroencephalographic background noise. Twenty volunteers participated in this study. After completion, according to a game experience survey, the participants described the game as amusing and challenging. We also conducted a level survey that revealed a significant difference between the levels of attention exerted by the participants when they passed the levels with respect to when they missed ( p -value = 17 · 10 −5 ). Additionally, the detection accuracy of the SSVEP-BCI had a baseline level of 84%. The results suggest that mobile flickers can be robustly detected within few seconds by means of an SSVEP-BCI. This principle could be used as a serious game to play or train the attention and the visual tracking capabilities to mobile target stimuli in special needs education schools or in centres dealing with attention disorders.},
  archive      = {J_EXSY},
  author       = {Eduardo Perez-Valero and Miguel Angel Lopez-Gordo and Miguel A. Vaquero-Blasco},
  doi          = {10.1111/exsy.12682},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12682},
  shortjournal = {Expert Syst.},
  title        = {An attention-driven videogame based on steady-state motion visual evoked potentials},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended TODIM-PROMETHEE II method with hesitant
probabilistic information for solving potential risk evaluation problems
of water resource carrying capacity. <em>EXSY</em>, <em>38</em>(4),
e12681. (<a href="https://doi.org/10.1111/exsy.12681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the excessive consumption and pollution of water resources, the sustainable development of water resources poses a serious threat currently. How to perceive and prevent the degradation of water resource in advance is an urgent problem. The water resource carrying capacity (WRCC) is a significant indicator to reflect the condition of water resources in a region. Resounding to these circumstances, our research establishes a decision support framework to solve WRCC risk evaluation issues. First, hesitant probabilistic fuzzy sets (HPFSs) are selected as a representation for the evaluation information in the expert group. Aimed at existing studies of HPFSs, some limitations are overcome involving the distance and comparison rule. Secondly, a TODIM-PROMETHEE II based multi-criteria group decision making (MCGDM) method is developed to overcome the inherent restrictions of PROMETHEE II method and make it suitable for a practical decision-making condition with bounded rationality. Subsequently, a case study is utilized to demonstrate the feasibility of our newly proposed decision support framework, followed by a sensitivity analysis and a comparison analysis. The outcome indicates that the framework has an excellent performance to solve this kind of MCGDM issues.},
  archive      = {J_EXSY},
  author       = {Xiao-kang Wang and Hong-yu Zhang and Jian-qiang Wang and Jun-bo Li and Lin Li},
  doi          = {10.1111/exsy.12681},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12681},
  shortjournal = {Expert Syst.},
  title        = {Extended TODIM-PROMETHEE II method with hesitant probabilistic information for solving potential risk evaluation problems of water resource carrying capacity},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel cost-sensitive algorithm and new evaluation
strategies for regression in imbalanced domains. <em>EXSY</em>,
<em>38</em>(4), e12680. (<a
href="https://doi.org/10.1111/exsy.12680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world data mining applications involve obtaining predictive models using imbalanced datasets. Frequently, the least common target variables present within datasets are associated with events that are highly relevant for end users. When these variables are nominal, we have a class-imbalance problem which has been thoroughly studied within machine learning. As for regression tasks where target variables are continuous, few predictive models and evaluation techniques exist. This paper proposes a solution to these challenges. First, we introduce a cost-sensitive learning algorithm based on a neural network trained on the minimization of a biased loss function. Results show a higher or comparable performance and convergence speed to existent techniques. Second, we develop new approaches for performance assessment of regression tasks within imbalanced domains by proposing new scalar measures, namely Geometric Mean Error ( GME ) and Class-Weighted Error ( CWE ), as well as new graphical-based measures, namely REC TPR , REC TNR , REC G − Mean and REC CWA curves. Unlike standard measures, our evaluation strategies are shown to be more robust to data imbalance as they reflect the performance of both rare and frequent events.},
  archive      = {J_EXSY},
  author       = {Lamyaa Sadouk and Taoufiq Gadi and El Hassan Essoufi},
  doi          = {10.1111/exsy.12680},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12680},
  shortjournal = {Expert Syst.},
  title        = {A novel cost-sensitive algorithm and new evaluation strategies for regression in imbalanced domains},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Project performance prediction model linking agility and
flexibility demands to project type. <em>EXSY</em>, <em>38</em>(4),
e12675. (<a href="https://doi.org/10.1111/exsy.12675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of this work is the development of a performance prediction model of projects, considering the influence of the leadership style and organizational factors on the agility and flexibility of the organization. The motivation of this work is the absence in the literature of a model to establish the relationship among leadership and agility factors, by means of the integration of prediction and sensitivity analysis capacities. The originality of this work lies on the presentation of a general model integrating both capacities, dedicated to the analysis of different kind of projects. The first part of the study deals with a literature review regarding the components of leadership, organizational structure, agility, flexibility and organizational factors and the intrinsic relationship among them. This information is the basis of the framework of this study that serves for the construction of the model. Artificial intelligence (AI) techniques and statistical tools are used for modeling purposes. From the model application, it is possible to identify under which conditions of leadership style and organizational factors there are the highest chances of high project performances. The main results are the characterization of agility and flexibility demands for each type of project, the understanding of how leadership affects agility and flexibility, and the impacts on the project performance.},
  archive      = {J_EXSY},
  author       = {Marco Aurélio de Oliveira and Luiz V. O. Dalla Valentina and André Hideto Futami and Osmar Possamai and Carlos Alberto Flesch},
  doi          = {10.1111/exsy.12675},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12675},
  shortjournal = {Expert Syst.},
  title        = {Project performance prediction model linking agility and flexibility demands to project type},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of parkinson disease using binary rao
optimization algorithms. <em>EXSY</em>, <em>38</em>(4), e12674. (<a
href="https://doi.org/10.1111/exsy.12674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rao algorithms are recently proposed optimization algorithms used to solve optimization problems. These algorithms are based on the best and the worst solutions, which are computed during the optimization process. However, these algorithms apply to continuous problems only. In this article, the binary versions of Rao algorithms are proposed, which can be used for solving feature selection problems. These are applied to four publicly available Parkinson&#39;s disease datasets. Besides providing an optimal set of features, the k parameter of the k-nearest neighbour classifier is also optimized by the proposed approach. The performance of these algorithms has been measured taking an average of 30 independent runs using a 10-fold cross-validation procedure. Also, a comparison of the performance has been made with the other state of the art methods. Significance analysis of these algorithms has been made with the Friedman rank test.},
  archive      = {J_EXSY},
  author       = {Suvita Rani Sharma and Birmohan Singh and Manpreet Kaur},
  doi          = {10.1111/exsy.12674},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12674},
  shortjournal = {Expert Syst.},
  title        = {Classification of parkinson disease using binary rao optimization algorithms},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An expert system for low-power and lossy indoor sensor
networks. <em>EXSY</em>, <em>38</em>(4), e12650. (<a
href="https://doi.org/10.1111/exsy.12650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have developed an expert system comprising a self-aware framework for resource-efficient and accurate data transmission within a low-power lossy sensor network (LLN) deployed for indoor monitoring. We derived both individual and group awareness, which could ensure the awareness of each sensor regarding its resources, neighbours and network environment. The proposed expert system facilitates decision-making under dynamic environmental conditions and employs a multi-criteria decision-making (MCDM) model to determine the selection of the best path towards the sink node with awareness of the existing network environment. The proposed system is validated by constructing a 6LoWPAN network in the Contiki Cooja simulator. MCDM is applied to generate an adaptive objective function for the IPv6 routing protocol for the LLN (RPL) and to aid in ranking the nodes to select the best available neighbouring node, while the data accuracy is ensured by the cluster head through data correlation among its associated members. The network performance is assessed by analyzing the packet delivery rate, throughput and energy consumption against varying sensors and by comparing our proposed MCDM-RPL with a standard RPL and a fuzzy-based RPL, where the results show that our framework is found to be better with gains of 13, 25 and 13%, respectively.},
  archive      = {J_EXSY},
  author       = {Sami J. Habib and Paulvanna N. Marimuthu and Pravin Renold and Balaji Ganesh Athi},
  doi          = {10.1111/exsy.12650},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12650},
  shortjournal = {Expert Syst.},
  title        = {An expert system for low-power and lossy indoor sensor networks},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biometrics and quality of life of lymphoma patients: A
longitudinal mixed-model approach. <em>EXSY</em>, <em>38</em>(4),
e12640. (<a href="https://doi.org/10.1111/exsy.12640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Engineering has become essential in the fields of Medical and Health Care with emphasis for helping citizens to improve their health and quality of life. This includes individual methods and techniques in health-related knowledge acquisition and representation and their application in the construction of intelligent systems capable of using the acquired information to improve the patients&#39; health and/or quality of life. Haemato-oncological diseases can provide significant disability and suffering, with severe symptoms and psychological distress. They can create difficulties in fulfilling professional, family and social roles, affecting an individual&#39;s quality of life. Health related quality of life (HRQoL) is a subjective concept but there is also an objective component related to physiological indicators. Some of these physiological indicators can be easily assessed by wearable technology such heart rate variability (HRV). This paper introduces an intelligent system to assess, in real-time, potential HRV indices, that can predict HRQoL in lymphoma patients throughout chemotherapy treatment and to account the individuals&#39; variability. The system is based on wearable technology and intelligent processing of the patients&#39; biometric information to assess some quality of life related parameters. A longitudinal study was conducted among 16 lymphoma patients using this intelligent system. Mixed-effect regression models were performed to investigate predictors for and time effects on HRQoL. There were no significant changes in all HRQoL domains over time. Some quality of life domains revealed similar time trends as HRV indices. These HRV indices also have a significant effect on the domains of quality of life.},
  archive      = {J_EXSY},
  author       = {Alexandra Oliveira and Eliana Silva and Joyce Aguiar and Brígida Mónica Faria and Luís Paulo Reis and Henrique Cardoso and Joaquim Gonçalves and Jorge Oliveira e Sá and Victor Carvalho and Herlander Marques},
  doi          = {10.1111/exsy.12640},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12640},
  shortjournal = {Expert Syst.},
  title        = {Biometrics and quality of life of lymphoma patients: A longitudinal mixed-model approach},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A co-training-based approach for the hierarchical
multi-label classification of research papers. <em>EXSY</em>,
<em>38</em>(4), e12613. (<a
href="https://doi.org/10.1111/exsy.12613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of the hierarchical multi-label classification of research papers, which is the task of assigning the set of relevant labels for a paper from a hierarchy, using reduced amounts of labelled training data. Specifically, we study leveraging unlabelled data, which are usually plentiful and easy to collect, in addition to the few available labelled ones in a semi-supervised learning framework for achieving better performance results. Thus, in this paper, we propose a semi-supervised approach for the hierarchical multi-label classification task of research papers based on the well-known Co-training algorithm, which exploit content and bibliographic coupling information as two distinct papers&#39; views. In our approach, two hierarchical multi-label classifiers, are learnt on different views of the labelled data, and iteratively select their most confident unlabelled samples, which are further added to the labelled set. The success of our suggested Co-training-based approach lies in two main components. The first is the use of two suggested selection criteria (i.e., Maximum Agreement and Labels Cardinality Consistency) that enforce selecting confident unlabelled samples. The second is the appliance of an oversampling method that rebalances the labels distribution of the initial labelled set, which reduces the reinforcement of the label imbalance issue during the Co-training learning. The proposed approach is evaluated using a collection of scientific papers extracted from the ACM digital library. Performed experiments show the effectiveness of our approach with regards to several baseline methods.},
  archive      = {J_EXSY},
  author       = {Abir Masmoudi and Hatem Bellaaj and Khalil Drira and Mohamed Jmaiel},
  doi          = {10.1111/exsy.12613},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12613},
  shortjournal = {Expert Syst.},
  title        = {A co-training-based approach for the hierarchical multi-label classification of research papers},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrity verification and behavioral classification of a
large dataset applications pertaining smart OS via blockchain and
generative models. <em>EXSY</em>, <em>38</em>(4), e12611. (<a
href="https://doi.org/10.1111/exsy.12611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware analysis and detection over the Android have been the focus of considerable research, during recent years, as customer adoption of Android attracted a corresponding number of malware writers. Antivirus companies commonly rely on signatures and are error-prone. Traditional machine learning techniques are based on static, dynamic, and hybrid analysis; however, for large scale Android malware analysis, these approaches are not feasible. Deep neural architectures are able to analyze large scale static details of the applications, but static analysis techniques can ignore many malicious behaviors of applications. The study contributes to the documentation of various approaches for detection of malware, traditional and state-of-the-art models, developed for analysis that facilitates the provision of basic insights for researchers working in malware analysis, and the study also provides a dynamic approach that employs deep neural network models for detection of malware. Moreover, the study uses Android permissions as a parameter to measure the dynamic behavior of around 16,900 benign and intruded applications. A dataset is created which encompasses a large set of permissions-based dynamic behavior pertaining applications, with an aim to train deep learning models for prediction of behavior. The proposed architecture extracts representations from input sequence data with no human intervention. The state-of-the-art Deep Convolutional Generative Adversarial Network extracted deep features and accomplished a general validation accuracy of 97.08% with an F1-score of 0.973 in correctly classifying input. Furthermore, the concept of blockchain is utilized to preserve the integrity of the dataset and the results of the analysis.},
  archive      = {J_EXSY},
  author       = {Salman Jan and Shahrulniza Musa and Toqeer Ali and Mohammad Nauman and Sajid Anwar and Tamleek Ali Tanveer and Babar Shah},
  doi          = {10.1111/exsy.12611},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12611},
  shortjournal = {Expert Syst.},
  title        = {Integrity verification and behavioral classification of a large dataset applications pertaining smart OS via blockchain and generative models},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated information systems architecture for the
agri-food industry. <em>EXSY</em>, <em>38</em>(4), e12599. (<a
href="https://doi.org/10.1111/exsy.12599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As information systems and technologies grow in usage in the agri-food industry, the same has happened to the relevance of Information Systems (IS) that allow for a parallel control, monitoring and management of the organizations&#39; activities and business processes. As the literature proves, the benefits of implementing adequate and interoperable IS are very numerous and tend to represent a significant determinant regarding the organizations&#39; overall success. Despite this, to the best of our knowledge there currently is no IS architecture designed to serve the specificities of the agri-food industry. With this study a novel information systems architecture for the agri-food sector is proposed. The artefact is composed by 12 integrated main components and a set of subcomponents aimed at supporting all the monitoring, control and management activities. In order to validate the proposed architecture a case study was implemented at a mushroom production organization. This allowed us to perceive the ability of our artefact to serve as the basis for the development of IS that address all of the organization&#39;s business and environmental needs.},
  archive      = {J_EXSY},
  author       = {Frederico Branco and Ramiro Gonçalves and Fernando Moreira and Manuel Au-Yong-Oliveira and José Martins},
  doi          = {10.1111/exsy.12599},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12599},
  shortjournal = {Expert Syst.},
  title        = {An integrated information systems architecture for the agri-food industry},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rational, emotional, and attentional models for recommender
systems. <em>EXSY</em>, <em>38</em>(4), e12594. (<a
href="https://doi.org/10.1111/exsy.12594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work analyses the decision-making process underlying choice behaviour. First, neural and gaze activity were recorded experimentally from different subjects performing a choice task in a Web Interface. Second, choice models and ensembles were fitted using rational, emotional, and attentional features. The model&#39;s predictions were evaluated in terms of their accuracy and rankings were made for each user. The results show that (a) the attentional models are the best in terms of its average performance across all users, (b) each subject shows a different best model, and (c) ensembles may perform better than single choice models but an optimal building method has to be found.},
  archive      = {J_EXSY},
  author       = {Ameed Almomani and Cristina Monreal and Jorge Sieira and Juan Graña and Eduardo Sánchez},
  doi          = {10.1111/exsy.12594},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12594},
  shortjournal = {Expert Syst.},
  title        = {Rational, emotional, and attentional models for recommender systems},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A neural approach for detecting inline mathematical
expressions from scientific documents. <em>EXSY</em>, <em>38</em>(4),
e12576. (<a href="https://doi.org/10.1111/exsy.12576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific documents generally contain multiple mathematical expressions in them. Detecting inline mathematical expressions are one of the most important and challenging tasks in scientific text mining. Recent works that detect inline mathematical expressions in scientific documents have looked at the problem from an image processing perspective. There is little work that has targeted the problem from NLP perspective. Towards this, we define a few features and applied Conditional Random Fields (CRF) to detect inline mathematical expressions in scientific documents. Apart from this feature based approach, we also propose a hybrid algorithm that combines Bidirectional Long Short Term Memory networks (Bi-LSTM) and feature-based approach for this task. Experimental results suggest that this proposed hybrid method outperforms several baselines in the literature and also individual methods in the hybrid approach.},
  archive      = {J_EXSY},
  author       = {Sreekanth Madisetty and Kaushal Kumar Maurya and Akiko Aizawa and Maunendra Sankar Desarkar},
  doi          = {10.1111/exsy.12576},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12576},
  shortjournal = {Expert Syst.},
  title        = {A neural approach for detecting inline mathematical expressions from scientific documents},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the identification and analysis of citation pattern
irregularities among journals. <em>EXSY</em>, <em>38</em>(4), e12561.
(<a href="https://doi.org/10.1111/exsy.12561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies report that few journals are adopting unethical citation practices to inflate Impact Factor (IF) artificially. “Clarivate Analytics” has started to blacklist such journals since 2006. As reported in the literature, evaluation of journals individually, to detect anomalies from vast and dynamically changing citation network is not efficient. The primary purpose of this work is to define a diverse feature set that can identify such cases of extreme outliers and reason them. The sample size is narrowed down using an unsupervised clustering algorithm in the absence of a labeled training dataset. Next, time-series IF data is analyzed to detect point outliers. Furthermore, microscopic features are identified to reason them. Results reflected from the F -value after ANOVA analysis reveals that geometrical patterns ( self-loop , pairwise and group mutual-citation ) among journals, an abrupt increase in the paper count of donor and corresponding IF inflation of recipient are some of the essential features. Microscopic features include social factor (calculation of revised IF after removing directed self or mutual-citation), impact of the field of study , impact of publication house and author factor that includes author self and mutual-citation. The significance of this work is to ensure that the quality of a journal is withheld without compromising research integrity by controlling or auditing individual features periodically.},
  archive      = {J_EXSY},
  author       = {Joyita Chakraborty and Dinesh K. Pradhan and Subrata Nandi},
  doi          = {10.1111/exsy.12561},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12561},
  shortjournal = {Expert Syst.},
  title        = {On the identification and analysis of citation pattern irregularities among journals},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using virtual programming lab to improve learning
programming: The case of algorithms and programming. <em>EXSY</em>,
<em>38</em>(4), e12531. (<a
href="https://doi.org/10.1111/exsy.12531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming is one of the basic skills that students must acquire. However, learning to program is not an easy task. Also teaching programming is an arduous but challenging task, requiring close follow-up and constant and meaningful feedback. So the main question is: how can we help teachers and students to achieve these goals? We identified a tool that can be useful to this purpose. That is Virtual Programming Lab (VPL), a Moodle plugin that allows students to submit their code and get prompt feedback without the teacher&#39;s intervention. In order to test this concept, an experiment was performed with several classes of beginner programming students, in two editions of Algorithms and Programming course unit of the degree in Informatics Engineering lectured at the Informatics Engineering Department at the School of Engineering, Polytechnic Institute of Porto. The students were challenged to test their assignments in VPL with a set of test values previously defined by the teachers. After the experiments, we used surveys to gather the involved students&#39; and teachers&#39; opinion, and more than 70% of the students answered that they considered the VPL an added value for the teaching–learning process. The dynamics verified in the classes, the general opinion of the teachers, and the acceptance and participation of the students allow to classify the experience as positive.},
  archive      = {J_EXSY},
  author       = {Marílio Cardoso and Rui Marques and António Vieira de Castro and Álvaro Rocha},
  doi          = {10.1111/exsy.12531},
  journal      = {Expert Systems},
  month        = {6},
  number       = {4},
  pages        = {e12531},
  shortjournal = {Expert Syst.},
  title        = {Using virtual programming lab to improve learning programming: The case of algorithms and programming},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary fusion of classifiers trained on linear
prediction based features for replay attack detection. <em>EXSY</em>,
<em>38</em>(3), e12670. (<a
href="https://doi.org/10.1111/exsy.12670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, linear prediction analysis (LP) related features have been successfully used for replay attack detection due to the imperfection in the LP-based signal produced by recording and playback devices. In this paper, we propose a weighted linear combination of classifier scores for replay attack detection where our classifiers, including Gaussian mixture models (GMMs) and support vector machines (SVMs), are trained on a variety of LP and LP residual-based features. In this way, we can benefits from all of the LP-related features when we combine classifiers trained on these features. We determine classifier weights using two evolutionary algorithms: genetic algorithm and particle swarm optimization. Furthermore, we propose a new feature based on performing LP residuals analysis of Mel sub-band energies. We also propose a deep structure for extracting deep features from LP-based coefficients to consider the class labels (genuine or spoofed speaker) in the feature extraction process. Results of our classifier system on the ASVspoof 2017 version 2 dataset show equal error rates of 0.3% and 4.8% for its development and evaluation subset, respectively. We also applied our proposed replay attack detection method to an ASV system that has acceptable results.},
  archive      = {J_EXSY},
  author       = {Babak Nasersharif and Morteza Yazdani},
  doi          = {10.1111/exsy.12670},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12670},
  shortjournal = {Expert Syst.},
  title        = {Evolutionary fusion of classifiers trained on linear prediction based features for replay attack detection},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of some techniques for solving system of linear
and nonlinear equations via hybrid algorithm. <em>EXSY</em>,
<em>38</em>(3), e12669. (<a
href="https://doi.org/10.1111/exsy.12669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this article is to introduce several new methods or techniques for solving simultaneous linear and nonlinear system of equations with the help of a new hybrid algorithm based on advanced quantum behaved particle swarm optimization and the concept of binary tournamenting process. Depending on different options of binary tournamenting, six different variants of hybrid algorithms are proposed. To examine the effectiveness of the proposed hybrid algorithms five well known benchmark bound-constrained optimization problems are solved. Among the six different variants of hybrid algorithms, the best algorithm is selected on the basis of their performances in these problems. This best algorithm is then applied in solving simultaneous linear and nonlinear system of equations transforming these equations into optimization problems. In case of linear system, just two systems are solved while in case of nonlinear system seven complicated problems are solved and finally a comparison of best found solutions are estimated with the same of existing algorithms provided in the literature.},
  archive      = {J_EXSY},
  author       = {Nirmal Kumar and Ali Akbar Shaikh and Sanat Kumar Mahato and Asoke Kumar Bhunia},
  doi          = {10.1111/exsy.12669},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12669},
  shortjournal = {Expert Syst.},
  title        = {Development of some techniques for solving system of linear and nonlinear equations via hybrid algorithm},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of artificial intelligence on english speaking
anxiety and speaking performance: A case study. <em>EXSY</em>,
<em>38</em>(3), e12667. (<a
href="https://doi.org/10.1111/exsy.12667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreign language anxiety (FLA) has been a perennial concern in language learning, as foreign language (FL) learners often communicate feelings of anxiety, stress, or nervousness. This study explored the role of artificial intelligence (AI) applications in speaking practice for FLA management of 48 undergraduate participants in an EFL class in Egypt. An eight-week, quasi-experimental pretest–posttest design examined learner anxiety levels using a 33-item FLA questionnaire. Their oral proficiency was assessed via roleplaying using an interaction-enhanced public version of the IELTS speaking evaluation rubric. The results confirmed that FL learners experienced FLA pre- and post-intervention. The identified anxiety levels played a facilitative role in FL learning with several ensuing gains. The use of conversationally enhanced AI chatbots in interactive activities slightly intensified learners&#39; FLA, thus meriting further investigation of these objectives. Overall, subject to further development, AI chatbots are promising for significantly improving linguistic output gains; however, this study found that learners&#39; speech-related anxieties were not reduced following the interactions with the chatbots. It is concluded that FLA plays an underexplored facilitative role in sharpening learner cognitive faculties and linguistic capacities. Moreover, AI chatbots may be beneficial in advancing FL learning with significant potential in EFL contexts, facilitating improved interaction and oral communication. These findings support the integration of AI technologies as effective tools in FL education, providing flexible, interactive, and learner-centred learning. This study is expected to be of considerable interest to FL educators and learners.},
  archive      = {J_EXSY},
  author       = {Reham El Shazly},
  doi          = {10.1111/exsy.12667},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12667},
  shortjournal = {Expert Syst.},
  title        = {Effects of artificial intelligence on english speaking anxiety and speaking performance: A case study},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust design of a robot gripper mechanism using new hybrid
grasshopper optimization algorithm. <em>EXSY</em>, <em>38</em>(3),
e12666. (<a href="https://doi.org/10.1111/exsy.12666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural design and optimization are important topics for the control and design of industrial robots. The motivation behind this research is to design a robot gripper mechanism. To explore robust design of the robot gripper mechanism, a new optimization approach based on a grasshopper optimization algorithm and Nelder–Mead algorithm is developed for requiring a fast and accurate solution. Additionally, a vehicle side crash design problem, a multi-clutch disc problem, and a manufacturing optimization problem are solved with the developed method to show the advantages of the new technique (HGOANM). Both engineering comparisons and production problem results in which HGOANM is applied are compared with the latest optimization techniques in the literature. The results of the problems resolved in this article reveal that the developed HGOANM is an essential optimization approach by solving real-world engineering problems quickly and accurately.},
  archive      = {J_EXSY},
  author       = {Betül Sultan Yildiz and Nantiwat Pholdee and Sujin Bureerat and Ali Riza Yildiz and Sadiq M. Sait},
  doi          = {10.1111/exsy.12666},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12666},
  shortjournal = {Expert Syst.},
  title        = {Robust design of a robot gripper mechanism using new hybrid grasshopper optimization algorithm},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving the rubik’s cube with stepwise deep learning.
<em>EXSY</em>, <em>38</em>(3), e12665. (<a
href="https://doi.org/10.1111/exsy.12665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a novel technique for learning the fitness function for search algorithms such as evolutionary strategies and hillclimbing. The aim of the new technique is to learn a fitness function (called a Learned Guidance Function ) from a set of sample solutions to the problem. These functions are learned using a supervised learning approach based on deep neural network learning, that is, neural networks with a number of hidden layers. This is applied to a test problem: unscrambling the Rubik&#39;s Cube using evolutionary and hillclimbing algorithms. Comparisons are made with a previous LGF approach based on random forests, with a baseline approach based on traditional error-based fitness, and with other approaches in the literature. This demonstrates how a fitness function can be learned from existing solutions, rather than being provided by the user, increasing the autonomy of AI search processes.},
  archive      = {J_EXSY},
  author       = {Colin G. Johnson},
  doi          = {10.1111/exsy.12665},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12665},
  shortjournal = {Expert Syst.},
  title        = {Solving the rubik&#39;s cube with stepwise deep learning},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of cardiac disorders using 1D local ternary
patterns based on pulse plethysmograph signals. <em>EXSY</em>,
<em>38</em>(3), e12664. (<a
href="https://doi.org/10.1111/exsy.12664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart diseases are a major cause of human casualties each year. An accurate and efficient diagnosis is essential to minimize their risk. This paper presents a system for the classification of multiple cardiac disorders based on pulse plethysmographic (PuPG) signal analysis. In particular, the work focuses on the detection and classification of ischemic and rheumatic heart diseases using proposed 1D local ternary patterns of PuPG signals. The proposed methodology is applied on a self-collected dataset consisting of 250 PuPG signals from 50 normal/healthy subjects, 140 signals from 28 ischemic patients, and 180 signals from 36 rheumatic patients. The effectiveness of proposed feature descriptors to precisely represent different classes of data is verified by several classifiers namely K-nearest neighbours (KNN), support vector machines (SVM), and decision tree (DT) with 10-fold cross-validation. The proposed methodology achieves the best detection performance with 99% accuracy, 100% sensitivity, and 98% specificity using an SVM classifier with cubic kernel. The comparative analysis demonstrates that the proposed method is more accurate and reliable as compared to several existing works on cardiac disorders classification.},
  archive      = {J_EXSY},
  author       = {Sumair Aziz and Muhammad Awais and Muhammad Umar Khan and Khushbakht Iqtidar and Usman Qamar},
  doi          = {10.1111/exsy.12664},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12664},
  shortjournal = {Expert Syst.},
  title        = {Classification of cardiac disorders using 1D local ternary patterns based on pulse plethysmograph signals},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy preservation of data using modified rider
optimization algorithm: Optimal data sanitization and restoration model.
<em>EXSY</em>, <em>38</em>(3), e12663. (<a
href="https://doi.org/10.1111/exsy.12663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data preservation is the mechanism of protecting and safeguarding the confidentiality and integrity of data. Data stored in huge databases may contain metadata, elements that may be imprecise and unstable, It may include sensitive data, personal profiles and so on, which is vulnerable to third parties such as hackers or attackers. They may misuse the data and as a consequence of this the confidentiality and privacy of the data gets lost. There is a need to conserve the data and make it available for reuse when needed. Hence, it needs a proficient method to maintain and protect individuals&#39; data privacy regarding confidentiality and reliability. This paper intends to develop an advanced model for privacy preservation of huge data with the accomplishment of two stages, namely data sanitization and data restoration. Data sanitization process preserves the safety of sensitive data stored in huge databases, by means of hiding those sensitive data from unauthorized users. Data restoration is the process of recovering or restoring of data that is sanitized at the sender side. Concerning the secrecy, there is a need for an optimal key to hide the sensitive data at sender as well as receiver side. Subsequent to the data sanitization, it requires the same key to restore the sanitized data. Thus, the optimal key generation plays a vital role to maintain privacy preservation. In order to choose an optimal key, a modified Rider optimization Algorithm (ROA) named as Randomized ROA (RROA) model is implemented in this work. Furthermore, the efficiency of the proposed work is compared over the state-of-the-arts models by concerning the sanitization as well as restoration efficiency.},
  archive      = {J_EXSY},
  author       = {Mohana Shivashankar and Sahaaya Arul Mary},
  doi          = {10.1111/exsy.12663},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12663},
  shortjournal = {Expert Syst.},
  title        = {Privacy preservation of data using modified rider optimization algorithm: Optimal data sanitization and restoration model},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Influence of codebook patterns on writer recognition: An
experimental study. <em>EXSY</em>, <em>38</em>(3), e12662. (<a
href="https://doi.org/10.1111/exsy.12662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Codebook-based writer characterization is an effective technique that has been investigated in a number of recent studies on identification and verification of writers. These methods divide a set of writing samples into small units (fragments or graphemes) and cluster these patterns to produce a codebook. Writer of a handwritten sample is then characterized by the probability (distribution) of producing the codebook patterns. In most cases, a small subset of the database under study is employed to produce the codebook while the rest of the database is used in evaluations. This work aims to validate the hypothesis that the codebook simply serves as a representation space to compare different writings and, in most cases, the patterns in the codebook do not significantly influence the identification and verification performance. The hypothesis is validated by generating a number of codebooks using Greek, Arabic and Chinese handwritten samples. Moreover, codebooks using fragments of handwritten music scores, printed text and synthetic data are also investigated. Evaluations on three well-known handwriting databases (CVL, BFL and IAM) validate the idea that, in general, the codebook patterns do not have a significant impact on characterizing writer from handwriting.},
  archive      = {J_EXSY},
  author       = {Chawki Djeddi and Imran Siddiqi and Abdeljalil Gattal and Somaya Al-Maadeed and Abdellatif Ennaji},
  doi          = {10.1111/exsy.12662},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12662},
  shortjournal = {Expert Syst.},
  title        = {Influence of codebook patterns on writer recognition: An experimental study},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-level pruning based ensemble with abstained learners for
concept drift in data streams. <em>EXSY</em>, <em>38</em>(3), e12661.
(<a href="https://doi.org/10.1111/exsy.12661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining data streams for predictive analysis is one of the most interesting topics in machine learning. With the drifting data distributions, it becomes important to build adaptive systems which are dynamic and accurate. Although ensembles are powerful in improving accuracy of incremental learning, it is crucial to maintain a set of best suitable learners in the ensemble while considering the diversity between them. By adding diversity-based pruning to the traditional accuracy-based pruning, this paper proposes a novel concept drift handling approach named Two-Level Pruning based Ensemble with Abstained Learners (TLP-EnAbLe). In this approach, deferred similarity based pruning delays the removal of under performing similar learners until it is assured that they are no longer fit for prediction. The proposed scheme retains diverse learners that are well suited for current concept. Two-level abstaining monitors performance of learners and chooses the best set of competent learners for participating in decision making. This is an enhancement to traditional majority voting system which dynamically chooses high performing learners and abstains the ones which are not suitable for prediction. In our experiments, it has been demonstrated that TLP-EnAbLe handles concept drift more effectively than other state-of-the-art algorithms on nineteen artificially drifting and ten real-world datasets. Further, statistical tests conducted on various drift patterns which include gradual, abrupt, recurring and their combinations prove efficiency of the proposed approach.},
  archive      = {J_EXSY},
  author       = {Kanu Goel and Shalini Batra},
  doi          = {10.1111/exsy.12661},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12661},
  shortjournal = {Expert Syst.},
  title        = {Two-level pruning based ensemble with abstained learners for concept drift in data streams},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge representation and acquisition using r-numbers
petri nets considering conflict opinions. <em>EXSY</em>, <em>38</em>(3),
e12660. (<a href="https://doi.org/10.1111/exsy.12660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a vital modelling technique, fuzzy Petri nets (FPNs) have been widely used in various areas for knowledge representation and reasoning. However, the conventional FPNs have many deficiencies in representing inaccurate knowledge, acquiring knowledge parameters and conducting approximate reasoning when used in the real world. In this article, a new version of FPNs, called R-numbers Petri nets (RPNs), is proposed to overcome the shortcomings and enhance the effectiveness of FPNs. Based on R-numbers, expert knowledge is depicted in the form of weighted R-numbers production rules. The interrelationships among input places (or transitions) are modelled by the R-numbers Maclaurin symmetric mean operator in the knowledge reasoning process. In addition, the conflict opinions of experts are handled with the proposed RPN model in order to obtain more precise knowledge parameters. Finally, the effectiveness and practicality of the proposed RPNs are illustrated by a realistic example concerning reliability analysis of an electric vehicle motor.},
  archive      = {J_EXSY},
  author       = {Xun Mou and Qi-Zhen Zhang and Hu-Chen Liu and Jianshen Zhao},
  doi          = {10.1111/exsy.12660},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12660},
  shortjournal = {Expert Syst.},
  title        = {Knowledge representation and acquisition using R-numbers petri nets considering conflict opinions},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An overlap graph model for large-scale group decision making
with social trust information considering the multiple roles of experts.
<em>EXSY</em>, <em>38</em>(3), e12659. (<a
href="https://doi.org/10.1111/exsy.12659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social network analysis is an efficient tool to investigate the relationships of decision-makers in large-scale group decision making (LSGDM). Existing social network-based LSGDM studies generally assumed that each decision-maker has a single role or belongs to only one subgroup. The assumption that a decision-maker has multiple roles or belongs to multiple subgroups is rarely taken into consideration. In this regard, this study proposes an overlap graph model (OGM) in which decision-makers can participate in the decision-making process in multiple roles to solve LSGDM problems with social trust information. In the OGM, decision-makers are firstly divided into two types: multiple-role decision-makers and single-role decision-makers. Since it is unpractical for a decision-maker to evaluate all others in a LSGDM problem, we then investigate how to construct a complete social trust network based on an Agent mechanism. A two-stage consensus reaching process is proposed to reduce the discrepancies among decision-makers: The first stage is for single-role decision-makers within a subgroup while the second stage is for Agents and multiple-role decision-makers. Finally, an illustrative example regarding selecting treatment plans for critical patients in COVID-19 is provided to test the applicability and rationality of the proposed model.},
  archive      = {J_EXSY},
  author       = {Huchang Liao and Runzhi Tan and Ming Tang},
  doi          = {10.1111/exsy.12659},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12659},
  shortjournal = {Expert Syst.},
  title        = {An overlap graph model for large-scale group decision making with social trust information considering the multiple roles of experts},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HFS-LightGBM: A machine learning model based on hybrid
feature selection for classifying ICU patient readmissions.
<em>EXSY</em>, <em>38</em>(3), e12658. (<a
href="https://doi.org/10.1111/exsy.12658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to patients readmitted to general wards, readmitted patients in the intensive care unit (ICU) are exposed to higher mortality rates and prolonged hospital stays. Moreover, the readmission of ICU patients brings pressing challenges for ICU management. Most models are devoted to identifying the risk factors and developing classification models that can predict whether ICU patients will be readmitted. Though these models are prominent, they do not provide estimates for the frequency of readmissions. This paper establishes a prediction model, hybrid feature selection-LightGBM (HFS-LightGBM), to evaluate the probability and frequency of ICU patient readmissions empirically. In terms of feature selection, a hybrid feature selection (HFS) algorithm for LightGBM combines the filter and wrapper methods. Pearson&#39;s correlation coefficient is employed in the filter procedure. Then we adopt the targeted LightGBM classifier along with the recursive feature elimination and cross-validated (RFECV) to produce the optimal feature subset. Additionally, the hyperparameters of the HFS-LightGBM are optimized. The HFS-LightGBM is employed on the real-world ICU dataset containing 1722 patients&#39; electronic health records. This model outperforms the current prevailing readmission models. The identified frequency can assist doctors in making specific interventions for patients to reduce the ICU readmission rate.},
  archive      = {J_EXSY},
  author       = {Yan Qiu and Shuai Ding and Ningguang Yao and Dongxiao Gu and Xiaojian Li},
  doi          = {10.1111/exsy.12658},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12658},
  shortjournal = {Expert Syst.},
  title        = {HFS-LightGBM: A machine learning model based on hybrid feature selection for classifying ICU patient readmissions},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel data clustering approach based on whale optimization
algorithm. <em>EXSY</em>, <em>38</em>(3), e12657. (<a
href="https://doi.org/10.1111/exsy.12657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clustering is an important technique of data mining in which the objective is to partition N data objects into K clusters that minimize the sum of intra-cluster distances between each data object to its nearest centroid. This is an optimization problem, and various optimization algorithms have been suggested for capturing the position vectors of optimal centroids. However, in these approaches, the problem of local entrapment is very common due to weak exploration mechanism. In this paper, a novel approach based on a whale optimization algorithm (WOA) is suggested for data clustering. The performance of the suggested approach is validated using 14 benchmark datasets of the UCI machine learning repository. The experimental results and various statistical tests have justified the efficacy of the suggested approach.},
  archive      = {J_EXSY},
  author       = {Tribhuvan Singh},
  doi          = {10.1111/exsy.12657},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12657},
  shortjournal = {Expert Syst.},
  title        = {A novel data clustering approach based on whale optimization algorithm},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised, knowledge-integrated pattern learning
approach for fact extraction from judicial text. <em>EXSY</em>,
<em>38</em>(3), e12656. (<a
href="https://doi.org/10.1111/exsy.12656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremendous growth in the availability of judicial documents has demanded the rise of information extraction (IE) techniques that support the automatic extraction of relevant concepts or data from judicial texts. Among various approaches available for IE, ontology-based IE has proven to be the most appropriate for extracting domain-specific information from natural language text. Through this article, we propose a knowledge-driven, semi-supervised pattern-based learning (bootstrapping) approach to extract domain-specific facts from judicial text, starting with a small set of seed facts. In the semantic analysis of legal text, fact extraction is the next step to entity identification, which involves the identification of roles played by each entity in the judicial text. The proposed methodology learns extraction patterns for 12 classes of facts from the judicial text through the integration of the domain ontology called judicial case ontology (JCO). The experimental results were evaluated by human judges and found to be quite promising. One main feature of the proposed methodology is its portability across various domains (such as medical, banking, insurance, etc.) which in turn helps build expert systems in various sectors.},
  archive      = {J_EXSY},
  author       = {Anu Thomas and Sivanesan Sangeetha},
  doi          = {10.1111/exsy.12656},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12656},
  shortjournal = {Expert Syst.},
  title        = {Semi-supervised, knowledge-integrated pattern learning approach for fact extraction from judicial text},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent control strategy for cancer cells reduction
in patients with chronic myelogenous leukaemia using the reinforcement
learning and considering side effects of the drug. <em>EXSY</em>,
<em>38</em>(3), e12655. (<a
href="https://doi.org/10.1111/exsy.12655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic Myelogenous Leukaemia (CML) is a haematopoietic stem cells disease with complex dynamical behaviour. One of the effective factors in treating patients is to determine the appropriate drug dosage. A physician should test the different drug dosages through trial and error in order to find its optimal value. This procedure is normally a time-consuming and error-prone task that can even be harmful. The contribution of this paper is to design an intelligent control strategy, which can be used to help physicians, by finding a drug treatment regimen to minimize the number of cancer cells for a CML patient. In this paper, the eligibility traces algorithm and Q-learning approach are adopted as sub-optimal methods for progressively reducing the population of cancer cells. In addition, the injected dosage of the drug has improved, compared with previous methods. More importantly, the proposed method is followed by the reduction in side effects of the drug. The advantage of the backward view and the previous states investigation are applied in the Eligibility Traces algorithm. These effects increase the learning procedure and decrease the growth rate of cancer cells and total dosage of the injected drug during the treatment period of time. The proposed strategy mitigates the side effects of the drug on the normal cells.},
  archive      = {J_EXSY},
  author       = {Amin Noori and Alireza Alfi and Ghazaleh Noori},
  doi          = {10.1111/exsy.12655},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12655},
  shortjournal = {Expert Syst.},
  title        = {An intelligent control strategy for cancer cells reduction in patients with chronic myelogenous leukaemia using the reinforcement learning and considering side effects of the drug},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel disturbance rejection factor based stable direct
adaptive fuzzy control strategy for a class of nonlinear systems.
<em>EXSY</em>, <em>38</em>(3), e12651. (<a
href="https://doi.org/10.1111/exsy.12651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a unique disturbance rejection factor (DRF) based design of direct stable adaptive fuzzy logic controllers (AFLCs) for a class of non-linear systems with large and fast disturbances. The proposed AFLCs are realized by employing hybrid combinations of Lyapunov theory based local adaptation and harmony search algorithm based global optimization technique. These hybrid AFLCs are designed with the objective of optimizing both the structure and free parameters of it with guaranteed stability and, at the same time, simultaneously achieving satisfactory tracking performance and disturbance rejection. The novelty of the proposed work lies in the fact that, in a bid to perform the disturbance rejection, the nature of the disturbance itself is used in designing the tracking control law. The proposed DRF based hybrid stable AFLCs are implemented for several benchmark case studies and extensive performance evaluations demonstrate their usefulness.},
  archive      = {J_EXSY},
  author       = {Kaushik Das Sharma and Amitava Chatterjee and Patrick Siarry and Anjan Rakshit},
  doi          = {10.1111/exsy.12651},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12651},
  shortjournal = {Expert Syst.},
  title        = {A novel disturbance rejection factor based stable direct adaptive fuzzy control strategy for a class of nonlinear systems},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pairs trading on different portfolios based on machine
learning. <em>EXSY</em>, <em>38</em>(3), e12649. (<a
href="https://doi.org/10.1111/exsy.12649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an advanced visualization and analytics approach for financial research. Statistical arbitrage, particularly pairs trading strategy, has gained ground in the financial market and machine learning techniques are applied to the finance field. The cointegration approach and long short-term memory (LSTM) were utilized to achieve stock pairs identification and price prediction purposes, respectively, in this project. This article focused on the US stock market, investigating the performance of pairs trading on different types of portfolios (aggressive and defensive portfolio) and compare the accuracy of price prediction based on LSTM. It can be briefly concluded that LSTM offers higher prediction precision on aggressive stocks and implementing pairs trading on the defensive portfolio would gain higher profitability during a specific period between 2016 and 2017. However, predicting tools like LSTM only offer limited advice on stock movement and should be cautiously utilized. We conclude that analytics and visualization can be effective for financial analysis, forecasting and investment strategy.},
  archive      = {J_EXSY},
  author       = {Victor Chang and Xiaowen Man and Qianwen Xu and Ching-Hsien Hsu},
  doi          = {10.1111/exsy.12649},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12649},
  shortjournal = {Expert Syst.},
  title        = {Pairs trading on different portfolios based on machine learning},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Addressing the new item problem in video recommender systems
by incorporation of visual features with restricted boltzmann machines.
<em>EXSY</em>, <em>38</em>(3), e12645. (<a
href="https://doi.org/10.1111/exsy.12645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past years, the research of video recommender systems (RSs) has been mainly focussed on the development of novel algorithms. Although beneficial, still any algorithm may fail to recommend video items that the system has no form of data associated to them (New Item Cold Start). This problem occurs when a new item is added to the catalogue of the system and no data are available for that item. In content-based RSs, the video items are typically represented by semantic attributes, when generating recommendations. These attributes require a group of experts or users for annotation, and still, the generated recommendations might not capture a complete picture of the users&#39; preferences, for example, the visual tastes of users on video style. This article addresses this problem by proposing recommendation based on novel visual features that do not require human annotation and can represent visual aspects of video items. We have designed a novel evaluation methodology considering three realistic scenarios, that is, (a) extreme cold start, (b) moderate cold start and (c) warm-start scenario. We have conducted a set of comprehensive experiments, and our results have shown the superior performance of recommendations based on visual features, in all of the evaluation scenarios.},
  archive      = {J_EXSY},
  author       = {Naieme Hazrati and Mehdi Elahi},
  doi          = {10.1111/exsy.12645},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12645},
  shortjournal = {Expert Syst.},
  title        = {Addressing the new item problem in video recommender systems by incorporation of visual features with restricted boltzmann machines},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining specific and representative information by the
attribute-oriented induction method. <em>EXSY</em>, <em>38</em>(3),
e12643. (<a href="https://doi.org/10.1111/exsy.12643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-oriented induction (AOI) is a data analysis technique based on induction. The traditional AOI algorithm requires a threshold given by users to determine the number of output tuples. However, it is not easy to set an appropriate tuple threshold, and there is usually noise contained in a dataset. The traditional AOI algorithm can only generate a summary output of a fixed size, but it cannot guarantee that all generalized tuples have sufficient specificity and representativeness. In this article, a new AOI method is proposed to make up for the shortcomings. We introduce the concept of cost to measure the loss of accuracy due to attribute ascension. We also propose two algorithms based on the hierarchical clustering method. By setting cost constraints on each generalized tuple, our method can generate accurate output while eliminating noise, and help users get more informative and clearer results.},
  archive      = {J_EXSY},
  author       = {Chia-Chi Wu and Yen-Liang Chen and Mei-Ru Yu},
  doi          = {10.1111/exsy.12643},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12643},
  shortjournal = {Expert Syst.},
  title        = {Mining specific and representative information by the attribute-oriented induction method},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A modified butterfly optimization algorithm: An adaptive
algorithm for global optimization and the support vector machine.
<em>EXSY</em>, <em>38</em>(3), e12642. (<a
href="https://doi.org/10.1111/exsy.12642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A modified adaptive butterfly optimization algorithm is established with the aim of addressing the “early search blindness” and the relatively poor adaptability of the sensory modality. A normal-distribution-based model and a Weibull-distribution-based adaptive model of sensory modalities are respectively proposed for the global search process and iteration process. Among them, the Weibull-distribution-based adaptive model of sensory modalities is mainly manifested as the c value, that is, the adaptive change trend based on the Weibull model. The performance of the modified butterfly optimization algorithm is validated using a 14-benchmark test function and compared with performances of some latest algorithms. The experimental results indicate that the modified algorithm performs competitively in terms of accuracy and stability. Following the experiment, the modified algorithm is further tested by running a support-vector-machine prediction model based on engineering data of a pipe belt conveyor&#39;s flat-pipe/pipe-flat transition segment. The results of the modified algorithm are then compared with test-run outcomes of the back-propagation prediction model and KCV-SVM model. The results show that the prediction error is well within 10%, demonstrating the method&#39;s competence as a reliable reference for future designs of pipe belt conveyors.},
  archive      = {J_EXSY},
  author       = {Kun Hu and Hao Jiang and Chen-Guang Ji and Ze Pan},
  doi          = {10.1111/exsy.12642},
  journal      = {Expert Systems},
  month        = {5},
  number       = {3},
  pages        = {e12642},
  shortjournal = {Expert Syst.},
  title        = {A modified butterfly optimization algorithm: An adaptive algorithm for global optimization and the support vector machine},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nested variational autoencoder for topic modelling on
microtexts with word vectors. <em>EXSY</em>, <em>38</em>(2), e12639. (<a
href="https://doi.org/10.1111/exsy.12639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the information on the Internet is represented in the form of microtexts , which are short text snippets such as news headlines or tweets. These sources of information are abundant, and mining these data could uncover meaningful insights. Topic modelling is one of the popular methods to extract knowledge from a collection of documents; however, conventional topic models such as latent Dirichlet allocation (LDA) are unable to perform well on short documents, mostly due to the scarcity of word co-occurrence statistics embedded in the data. The objective of our research is to create a topic model that can achieve great performances on microtexts while requiring a small runtime for scalability to large datasets. To solve the lack of information of microtexts, we allow our method to take advantage of word embeddings for additional knowledge of relationships between words. For speed and scalability, we apply autoencoding variational Bayes, an algorithm that can perform efficient black-box inference in probabilistic models. The result of our work is a novel topic model called the nested variational autoencoder , which is a distribution that takes into account word vectors and is parameterized by a neural network architecture. For optimization, the model is trained to approximate the posterior distribution of the original LDA model. Experiments show the improvements of our model on microtexts as well as its runtime advantage.},
  archive      = {J_EXSY},
  author       = {Trung Trinh and Tho Quan and Trung Mai},
  doi          = {10.1111/exsy.12639},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12639},
  shortjournal = {Expert Syst.},
  title        = {Nested variational autoencoder for topic modelling on microtexts with word vectors},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NetHALOC: A learned global image descriptor for loop closing
in underwater visual SLAM. <em>EXSY</em>, <em>38</em>(2), e12635. (<a
href="https://doi.org/10.1111/exsy.12635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the experimental assessment of a hash-based loop closure detection methodology for visual simultaneous localization and mapping (SLAM), addressed to underwater autonomous vehicles. This methodology uses a new global image descriptor called net hash-based loop closure (NetHALOC), which is learned with a simple and fast convolutional neural network . The results using NetHALOC have been compared with the results using three different top-quality state-of-the-art global image descriptors: Net vector of locally aggregated descriptors , hash-based loop closure and deep loop closure . The transformation from images to hashes implies a considerable reduction of data to be processed, shared, compared and transmitted in global navigation, localization or mapping tasks. Complete tests with an extensive set of real underwater imagery were done to compare the performance of the defined loop closure detection approach using the aforementioned four hashing techniques, in order to conclude which of them is the more adequate for visual SLAM applications under the sea.},
  archive      = {J_EXSY},
  author       = {Francisco Bonin-Font and Antoni Burguera Burguera},
  doi          = {10.1111/exsy.12635},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12635},
  shortjournal = {Expert Syst.},
  title        = {NetHALOC: A learned global image descriptor for loop closing in underwater visual SLAM},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recommendation of users in social networks: A semantic and
social based classification approach. <em>EXSY</em>, <em>38</em>(2),
e12634. (<a href="https://doi.org/10.1111/exsy.12634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the study of social network-based recommender systems has become an active research topic. The integration of the social relationships that exist between users can improve the accuracy of recommendation results since the users&#39; preferences are similar or influenced by their connected friends. We focus in this article on the recommendation of users in social networks. Our approach is based on semantic and social representations of the users&#39; profiles. We have formalized and illustrated these two dimensions using the Yelp social network. The novelty of our approach concerns the modelling of the credibility of the user, through his/her trust and commitment in the social network. Moreover, in order to optimize the performance of the recommendation process, we have used two classification techniques: an unsupervised technique that uses the K-means algorithm (applied initially to all users); and a supervised technique that uses the K-Nearest Neighbours algorithm (applied to newly added users). A recommendation algorithm has been proposed taking into account the cold-start and sparsity problems. A prototype of a recommender system has been developed and tested using two publicly available datasets: the Yelp database and the Rich Epinions database. The comparative evaluation results show the effectiveness of combining the semantic, the social and the credibility information in an approach that appropriately uses the K-means and K-Nearest Neighbours algorithms.},
  archive      = {J_EXSY},
  author       = {Lamia Berkani and Sami Belkacem and Mounira Ouafi and Ahmed Guessoum},
  doi          = {10.1111/exsy.12634},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12634},
  shortjournal = {Expert Syst.},
  title        = {Recommendation of users in social networks: A semantic and social based classification approach},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid model for financial time-series forecasting based
on mixed methodologies. <em>EXSY</em>, <em>38</em>(2), e12633. (<a
href="https://doi.org/10.1111/exsy.12633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a hybrid model that combines ensemble empirical mode decomposition (EEMD), autoregressive integrated moving average (ARIMA), and Taylor expansion using a tracking differentiator to forecast financial time series. Specifically, the financial time series is decomposed by EEMD into some subseries. Then, the linear portion of each subseries is forecasted by the linear ARIMA model, while the nonlinear portion is predicted by the nonlinear Taylor expansion model. The forecasting results of the linear and nonlinear models are combined into the predicted result of each subseries. The final prediction result is obtained by combining the prediction values of all the subseries. The empirical results with real financial time series data demonstrate that this new hybrid approach outperforms the benchmark hybrid models considered in this paper.},
  archive      = {J_EXSY},
  author       = {Zhidan Luo and Wei Guo and Qingfu Liu and Zhengjun Zhang},
  doi          = {10.1111/exsy.12633},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12633},
  shortjournal = {Expert Syst.},
  title        = {A hybrid model for financial time-series forecasting based on mixed methodologies},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge based approach to ground refuelling optimization
of commercial airplanes. <em>EXSY</em>, <em>38</em>(2), e12631. (<a
href="https://doi.org/10.1111/exsy.12631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to establish a general and optimized procedure for the initial refuelling of commercial airplanes, as this loading process is strongly related to safety and energy saving issues. The on-ground refuelling is addressed as an optimization problem whose cost function involves expert knowledge about constraints and factors that influence the aircraft stability and performance. Several heterogeneous criteria (fuelling time, structural load, flow transfers, etc.) have been considered and weighted accordance to its importance in terms of stability. This allows us to adapt the strategy to any type and planned trip of the airplane. The priority is the positioning of the centre of mass of the civil aircraft within safety and manoeuvrability margins, and near the optimal position. Evolutive algorithms are applied, keeping feasible solutions by modifying genetic operators. As a case of study, the initial refuelling of a long range type commercial aircraft, the Airbus A330-200, is analysed. Simulation results have proved this methodology to be efficient and optimal. Even more, this heuristic and general approach improves the traditional solution that follows a set of pre-defined rules that are specific for each type of aircraft.},
  archive      = {J_EXSY},
  author       = {Elías Plaza and Matilde Santos},
  doi          = {10.1111/exsy.12631},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12631},
  shortjournal = {Expert Syst.},
  title        = {Knowledge based approach to ground refuelling optimization of commercial airplanes},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A semantic-enabled and context-aware monitoring system for
the internet of medical things. <em>EXSY</em>, <em>38</em>(2), e12629.
(<a href="https://doi.org/10.1111/exsy.12629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the Internet of Things (IoT) in the medical field has led to the massive deployment of a myriad of medical connected objects (MCOs). These MCOs are being developed and implemented for remote healthcare monitoring purposes including elderly patients with chronic diseases, pregnant women, and patients with disabilities. Accordingly, different associated challenges are emerging and include the heterogeneity of the gathered health data from these MCOs with ever-changing contexts. These contexts are relative to the continuous change of constraints and requirements of the MCOs deployment (time, location, state). Other contexts are related to the patient (medical record, state, age, sex, etc.) that should be taken into account to ensure a more precise and appropriate treatment of the patient. These challenges are difficult to address due to the absence of a reference model for describing the health data and their sources and linking these data with their contexts. This article addresses this problem and introduces a semantic-based context-aware system (IoT Medicare system) for patient monitoring with MCOs. This system is based on a core domain ontology (HealthIoT-O), that is, designed to describe the semantic of heterogeneous MCOs and their data. Moreover, an efficient interpretation and management of this knowledge in diverse contexts are ensured through SWRL rules such as the verification of the proper functioning of the MCOs and the analysis of the health data for diagnosis and treatment purposes. A case study of gestational diabetes disease management is proposed to evaluate the effectiveness of the implemented IoT Medicare system. An evaluation phase is provided and focuses on the quality of the elaborated semantic model and the performance of the system.},
  archive      = {J_EXSY},
  author       = {Ahlem Rhayem and Mohamed Ben Ahmed Mhiri and Khalil Drira and Said Tazi and Faiez Gargouri},
  doi          = {10.1111/exsy.12629},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12629},
  shortjournal = {Expert Syst.},
  title        = {A semantic-enabled and context-aware monitoring system for the internet of medical things},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complex pythagorean dombi fuzzy operators using aggregation
operators and their decision-making. <em>EXSY</em>, <em>38</em>(2),
e12626. (<a href="https://doi.org/10.1111/exsy.12626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex Pythagorean fuzzy set, an extension of Pythagorean fuzzy set, is a powerful tool to handle two dimension phenomenon. Dombi operators with operational parameters have outstanding flexibility. This article presents certain aggregation operators under complex Pythagorean fuzzy environment, including complex Pythagorean Dombi fuzzy weighted arithmetic averaging (CPDFWAA) operator, complex Pythagorean Dombi fuzzy weighted geometric averaging (CPDFWGA) operator, complex Pythagorean Dombi fuzzy ordered weighted arithmetic averaging (CPDFOWAA) operator and complex Pythagorean Dombi fuzzy ordered weighted geometric averaging (CPDFOWGA) operator. Moreover, this paper explores some fundamental properties of these operators with appropriate elaboration. A decision-making numerical example related to the selection of bank to purchase loan is given to demonstrate the significance of our proposed approach. Finally, a comparative analysis with existing operators is given to demonstrate the peculiarity of our proposed operators.},
  archive      = {J_EXSY},
  author       = {Muhammad Akram and Ayesha Khan and Arsham Borumand Saeid},
  doi          = {10.1111/exsy.12626},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12626},
  shortjournal = {Expert Syst.},
  title        = {Complex pythagorean dombi fuzzy operators using aggregation operators and their decision-making},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A smartly designed automated map based clustering algorithm
for the enhanced diagnosis of pathologies in brain MR images.
<em>EXSY</em>, <em>38</em>(2), e12625. (<a
href="https://doi.org/10.1111/exsy.12625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The competitive segmentation of fuzzy clustering is utilized in a greater manner to deal with the local spatial information of input medical images. Fuzzy clustering favours lesions and tumour identification through the segmentation process where less accuracy attainment and time complexity might be instigated for the identification of oddities. To rectify the above-said problems, a novel methodology that encapsulates the combination of unsupervised neural network and fuzzy clustering processes, which effortlessly distinguishes the lesion and tumour region in MR brain images is developed through this study. The initial process of the proposed algorithm employs the histogram-based feature extraction of the input images; whereof, a feature vector selection is made for the operation of self-organizing map (SOM), which is a neural network functionary that progresses through the mapping process. Modification regarding the membership function of fuzzy entropy clustering (MFEC) is done based on the entropy value of the input image that results in quicker convergence. Finally, the updated objective function of MFEC algorithm augments the SOM result. It is found that the proposed SOM based MFEC algorithm is superior to other traditional segmentation algorithms, which have rendered the better visible understanding of the image. Further, the end-results of the algorithm are verified through the evaluation of quantity metrics using ground truth of the brain MR images. The proposed SOM based MFEC algorithm precisely provides 82.26% of Jaccard value and 90.05% of Dice Overlap Index value, and these values prove better brain slices segmentation and provide enormous help to radiologists during patient diagnosis.},
  archive      = {J_EXSY},
  author       = {Vigneshwaran Senthilvel and Vishnuvarthanan Govindaraj and Yu-Dong Zhang and Pallikonda Rajasekaran Murugan and Arun Prasath Thiyagarajan},
  doi          = {10.1111/exsy.12625},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12625},
  shortjournal = {Expert Syst.},
  title        = {A smartly designed automated map based clustering algorithm for the enhanced diagnosis of pathologies in brain MR images},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building an expert system for printer forensics: A new
printer identification model based on niching genetic algorithm.
<em>EXSY</em>, <em>38</em>(2), e12624. (<a
href="https://doi.org/10.1111/exsy.12624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inside digital forensic science, expert systems are utilized to clarify suspicions where normally one or more human experts would need to be consulted. Expert systems-based printer identification is provided with the objective of distinguishing the printer that produced a suspicious or questioned document. The arising problem is that the extraction of many features of the printed document for printer forensics sometimes increases the time and decreases the classification accuracy as many of the printed document descriptors may emanate to be recurring and non-valuable. Therefore, the distinct combinatorial collection of features (knowledge base) will demand to be acquired in order to preserve the essence of operative features&#39; fusion to accomplish the maximum accuracy. This paper presents a bio-inspired expert system for printer forensics that integrates both texture features conveyed from the grey level co-occurrence matrix of the printed letter ‘WOO’ and niching genetic search to select the good enough reduced feature set. This combination intends to realize high classification precision relies on a trivial collection of discriminative descriptors. Niching methods extend genetic algorithms to domains that require the location and maintenance of multiple solutions based on adjusting the crossover ratio and occurrence of mutation of each individual and employs the slope of the individuals to choose their mutation value. For categorization, the scheme exploits k-nearest neighbours ( KNN ) to distinguish the brand of the printer for its simplicity. Results confirm that the suggested approach has high classification accuracy and needs less computation time.},
  archive      = {J_EXSY},
  author       = {Saad M. Darwish and Hany M. ELgohary},
  doi          = {10.1111/exsy.12624},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12624},
  shortjournal = {Expert Syst.},
  title        = {Building an expert system for printer forensics: A new printer identification model based on niching genetic algorithm},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dog food recommendation system based on nutrient
suitability. <em>EXSY</em>, <em>38</em>(2), e12623. (<a
href="https://doi.org/10.1111/exsy.12623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for a food recommendation service for dogs has rapidly increased with the increasing number of pet owners, because it is generally difficult for dog owners to find food that is perfectly suitable for their dogs&#39; health condition. The purpose of this study is to develop an algorithm for recommending dog food that contains appropriate nutrients based on the physical and health conditions of the dogs. This study proposes a nutrient profiling-based recommendation algorithm (NRA) for dog food. The proposed algorithm tries to recommend appropriate or inappropriate dog food by using collective intelligence based on user experience and the prior knowledge of experts. Based on the physical and health status of dogs, this study extracts which nutrients are necessary for the dogs and recommends the most suitable dog food containing these nutrients. A performance evaluation was implemented in terms of recall, precision, F1 and AUC. As a result of the performance evaluation, the AUC performance of this NRA is 20% higher than k-NN and 9.7% higher than the SVD model. In addition, the NRA proved to be an evolving system in which the performance of recommendations improves as users&#39; feedback accumulates.},
  archive      = {J_EXSY},
  author       = {Hee Seok Song and Young Ae Kim},
  doi          = {10.1111/exsy.12623},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12623},
  shortjournal = {Expert Syst.},
  title        = {A dog food recommendation system based on nutrient suitability},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid MCDM model combining DANP with TODIM to evaluate
the information quality of community question answering in a
two-dimensional linguistic environment. <em>EXSY</em>, <em>38</em>(2),
e12619. (<a href="https://doi.org/10.1111/exsy.12619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the information quality of cQA (community question-answering) websites helps users select a cQA website with high-quality information and improves the information quality. In this paper, an approach to evaluating the information quality of cQA based on a novel hybrid multicriteria decision-making (MCDM) model is proposed. First, the source, content, expression and usefulness criteria for evaluating the information quality of the cQA are established. Then, considering the ambiguity and inner correlations of the criteria, DANP ([DEMATEL]-based Analytic Network Process) and TODIM (interactive and multiple attribute decision making, in Portuguese) methods are combined in a two-dimensional linguistic environment to process linguistic evaluation information. Lastly, the proposed approach is applied to evaluate the quality of five popular cQA websites. The key criteria are identified, and the evaluation results are derived comprehensively. The key factors consist of reputation, coverage, politeness, usability, helpfulness, clarity, readability and conciseness. The websites, Know almost and Baidu knows, perform better in terms of information quality. The application, along with the sensitivity analysis and comparative analysis, shows the effectiveness of the proposed model. The proposed approach has both practical and research implications. It provides an approach for users to choose websites, and, for operators, to improve the information quality of their website. The evaluation criteria and their relations provide a reference for research on the information quality. The evaluation method considers the user&#39;s psychological information to provide a more accurate MCDM approach. The comprehensive aspects of the experiment can be used to verify the other MCDM methods.},
  archive      = {J_EXSY},
  author       = {Ming Li and Ying Li and Qijin Peng and Jun Wang},
  doi          = {10.1111/exsy.12619},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12619},
  shortjournal = {Expert Syst.},
  title        = {A hybrid MCDM model combining DANP with TODIM to evaluate the information quality of community question answering in a two-dimensional linguistic environment},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Positioning push–pull boundary in a hesitant fuzzy
environment. <em>EXSY</em>, <em>38</em>(2), e12616. (<a
href="https://doi.org/10.1111/exsy.12616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, fierce competition enforces supply chain planners to develop market-oriented production strategies. Customer order decoupling point (CODP) could increase the supply chain efficiency and responsiveness simultaneously. The right position of CODP in production industries will result in a pattern for trade-off between responsiveness and operational efficiency. The purpose of this paper is to address the positioning problem of a push-pull boundary in a fuzzy hesitant environment. To this end, a hybrid multi-criteria decision-making methodology of analytic network process (ANP) and VIKOR is proposed in a hesitant judgement environment to determine the position of CODP in a supply chain. Finally, CODP positioning in the apparel supply chain, as an industry-based example, is analysed to show the applicability of the proposed method.},
  archive      = {J_EXSY},
  author       = {Seyedeh Roya Pournamazi and R. Ghasemy Yaghin and Fariborz Jolai},
  doi          = {10.1111/exsy.12616},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12616},
  shortjournal = {Expert Syst.},
  title        = {Positioning push–pull boundary in a hesitant fuzzy environment},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error prediction and structure determination for CMAC neural
network based on the uniform design method. <em>EXSY</em>,
<em>38</em>(2), e12614. (<a
href="https://doi.org/10.1111/exsy.12614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insufficient study on error bound of cerebellar model articulation controller (CMAC) severely limits its application. To investigate the error prediction and structure determination of CMAC for multi-dimensional and data-generation objects, this paper builds a 10-input 2-output model for a desulfurization system to test 44,640 sets of operation data. Four test groups and one prediction group are designed and performed based on uniform design method. Regression analysis and curve fitting methods are applied for error analyses. The focus of regression analysis method is the influence of uniform table&#39;s level on its prediction formulas&#39; accuracies, whereas curve fitting&#39;s is the impact of theoretical memory space (location number) and actual storage space (address number) of CMAC on output error. Based on the results, the following conclusions are obtained. (a) The prediction accuracy of the linear regression equation is not monotonous with the level of the uniform design table, but there is a distinct region with local high precision. (b) Compared with regression analysis and address number fitting methods, location number analysis method has distinct advantages of prediction range, accuracy and flexibility. (c) In terms of location number analysis, different intervals may correspond to different optimal fitting functions, but only power function maintains high prediction accuracy in the whole range where the method works. Besides, the scope of location number analysis is also studied, of which the lower borders are 10 9 and 10 10 approximately for model&#39;s two outputs, respectively.},
  archive      = {J_EXSY},
  author       = {Zhiwei Kong and Yong Zhang and Xudong Wang and Shuanzhu Sun and Chunlei Zhou and Dou Li and Baosheng Jin},
  doi          = {10.1111/exsy.12614},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12614},
  shortjournal = {Expert Syst.},
  title        = {Error prediction and structure determination for CMAC neural network based on the uniform design method},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Serial and parallel memetic algorithms for the bounded
diameter minimum spanning tree problem. <em>EXSY</em>, <em>38</em>(2),
e12610. (<a href="https://doi.org/10.1111/exsy.12610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a connected, weighted, undirected graph G = ( V , E ) and an integer D ≥ 2, the bounded diameter minimum spanning tree (BDMST) problem seeks a spanning tree of minimum cost, whose diameter is no greater than D . The problem is known to be NP-hard, and finds application in various domains such as information retrieval, wireless sensor networks and distributed mutual exclusion. This article presents a two-phase memetic algorithm for the BDMST problem that combines a specialized recombination operator (proposed in this work) with good heuristics in order to more effectively direct the exploration of the search space into regions containing better solutions. A parallel meta-heuristic is also proposed and shown to obtain very good speedups – super-linear, in several cases – vis-à-vis the serial memetic algorithm. To the best of the authors&#39; knowledge, this is the first parallel meta-heuristic proposed for the BDMST problem, and potentially paves the way for handling much larger problems than reported in the literature. Some observations and theorems are presented in order to provide the underlying framework for the proposed algorithms. Further, the proposed memetic algorithm and parallel meta-heuristic are shown, in the course of several computational experiments, to in general obtain superior solution quality with much lesser computational effort in comparison to the best known meta-heuristics in the literature over a wide range of benchmark instances.},
  archive      = {J_EXSY},
  author       = {Prem Prakash Vuppuluri and Patvardhan Chellapilla},
  doi          = {10.1111/exsy.12610},
  journal      = {Expert Systems},
  month        = {3},
  number       = {2},
  pages        = {e12610},
  shortjournal = {Expert Syst.},
  title        = {Serial and parallel memetic algorithms for the bounded diameter minimum spanning tree problem},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel applications of soft computing techniques for
industrial and environmental enterprises. <em>EXSY</em>, <em>38</em>(1),
e12654. (<a href="https://doi.org/10.1111/exsy.12654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Álvaro Herrero and Alfredo Jiménez and Secil Bayraktar and Ángel Arroyo},
  doi          = {10.1111/exsy.12654},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12654},
  shortjournal = {Expert Syst.},
  title        = {Novel applications of soft computing techniques for industrial and environmental enterprises},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on intelligent biomedical data analysis and
processing. <em>EXSY</em>, <em>38</em>(1), e12653. (<a
href="https://doi.org/10.1111/exsy.12653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Deepak Gupta and Joel J. P. C. Rodrigues and Oscar Castillo},
  doi          = {10.1111/exsy.12653},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12653},
  shortjournal = {Expert Syst.},
  title        = {Special issue on intelligent biomedical data analysis and processing},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing a bi-objective vehicle routing problem that
appears in industrial enterprises. <em>EXSY</em>, <em>38</em>(1),
e12638. (<a href="https://doi.org/10.1111/exsy.12638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new solution method is implemented to solve a bi-objective variant of the vehicle routing problem that appears in industry and environmental enterprises. The solution involves designing a set of routes for each day in a period, in which the service frequency is a decision variable. The proposed algorithm, a muti-start multi-objective local search algorithm (MSMLS), minimizes total emissions produced by all vehicles and maximizes the service quality measured as the number of times that a customer is visited by a vehicle in order to be served. The MSMLS is a neighbourhood-based metaheuristic that obtains high-quality solutions and that is capable of achieving better performance than other competitive algorithms. Furthermore, the proposed algorithm is able to perform rapid movements thanks to the easy representation of the solutions.},
  archive      = {J_EXSY},
  author       = {Ana D. López-Sánchez and Julián Molina and Manuel Laguna and Alfredo G. Hernández-Díaz},
  doi          = {10.1111/exsy.12638},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12638},
  shortjournal = {Expert Syst.},
  title        = {Optimizing a bi-objective vehicle routing problem that appears in industrial enterprises},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of anomalous episodes in urban ozone maps.
<em>EXSY</em>, <em>38</em>(1), e12636. (<a
href="https://doi.org/10.1111/exsy.12636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addition to classification and regression, outlier detection has emerged as a relevant activity in deep learning. In comparison with previous approaches where the original features of the examples were used for separating the examples with high dissimilarity from the rest of the examples, deep learning can automatically extract useful features from raw data, thus removing the need for most of the feature engineering efforts usually required with classical machine learning approaches. This requires training the deep learning algorithm with labels identifying the examples or with numerical values. Although outlier detection in deep learning has been usually undertaken by training the algorithm with categorical labels—classifier—, it can also be performed by using the algorithm as regressor. Nowadays numerous urban areas have deployed a network of sensors for monitoring multiple variables about air quality. The measurements of these sensors can be treated individually—as time series—or collectively. Collectively, a variable monitored by a network of sensors can be transformed into a map. Maps can be used as images in machine learning algorithms—including computer vision algorithms—for outlier detection. The identification of anomalous episodes in air quality monitoring networks allows later processing this time period with finer-grained scientific packages involving fluid dynamic and chemical evolution software, or the identification of malfunction stations. In this work, a Convolutional Neural Network is trained—as a regressor—using as input Ozone-urban images generated from the Air Quality Monitoring Network of Madrid (Spain). The learned features are processed by Density-based Spatial Clustering of Applications with Noise (DBSCAN) algorithm for identifying anomalous maps. Comparisons with other deep learning architectures are undertaken, for instance, autoencoders—undercomplete and denoizing—for learning salient features of the maps and later to use as input of DBSCAN. The proposed approach is able efficiently find maps with local anomalies compared to other approaches based on raw images or latent features extracted with autoencoders architectures with DBSCAN.},
  archive      = {J_EXSY},
  author       = {Miguel Cárdenas-Montes},
  doi          = {10.1111/exsy.12636},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12636},
  shortjournal = {Expert Syst.},
  title        = {Detection of anomalous episodes in urban ozone maps},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive dialogue management using intent clustering and
fuzzy rules. <em>EXSY</em>, <em>38</em>(1), e12630. (<a
href="https://doi.org/10.1111/exsy.12630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational systems have become an element of everyday life for billions of users who use speech-based interfaces to services, engage with personal digital assistants on smartphones, social media chatbots, or smart speakers. One of the most complex tasks in the development of these systems is to design the dialogue model, the logic that provided a user input selects the next answer. The dialogue model must also consider mechanisms to adapt the response of the system and the interaction style according to different groups and user profiles. Rule-based systems are difficult to adapt to phenomena that were not taken into consideration at design-time. However, many of the systems that are commercially available are based on rules, and so are the most widespread tools for the development of chatbots and speech interfaces. In this article, we present a proposal to: (a) automatically generate the dialogue rules from a dialogue corpus through the use of evolving algorithms, (b) adapt the rules according to the detected user intention. We have evaluated our proposal with several conversational systems of different application domains, from which our approach provided an efficient way for adapting a set of dialogue rules considering user utterance clusters.},
  archive      = {J_EXSY},
  author       = {David Griol and Zoraida Callejas and Jose Manuel Molina and Araceli Sanchis},
  doi          = {10.1111/exsy.12630},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12630},
  shortjournal = {Expert Syst.},
  title        = {Adaptive dialogue management using intent clustering and fuzzy rules},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithms for complex interval-valued q-rung orthopair
fuzzy sets in decision making based on aggregation operators, AHP, and
TOPSIS. <em>EXSY</em>, <em>38</em>(1), e12609. (<a
href="https://doi.org/10.1111/exsy.12609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interval-valued q-rung orthopair fuzzy set (IVq-ROFS) and complex fuzzy set (CFS) are two generalizations of the fuzzy set (FS) to cope with uncertain information in real decision making problems. The aim of the present work is to develop the concept of complex interval-valued q-rung orthopair fuzzy set (CIVq-ROFS) as a generalization of interval-valued complex fuzzy set (IVCFS) and q-rung orthopair fuzzy set (q-ROFS), which can better express the time-periodic problems and two-dimensional information in a single set. In this article not only basic properties of CIVq-ROFSs are discussed but also averaging aggregation operator (AAO) and geometric aggregation operator (GAO) with some desirable properties and operations on CIVq-ROFSs are discussed. The proposed operations are the extension of the operations of IVq-ROFS, q-ROFS, interval-valued Pythagorean fuzzy, Pythagorean fuzzy (PF), interval-valued intuitionistic fuzzy, intuitionistic fuzzy, complex q-ROFS, complex PF, and complex intuitionistic fuzzy theories. Further, the Analytic hierarchy process (AHP) and technique for order preference by similarity to ideal solution (TOPSIS) method are also examine based on CIVq-ROFS to explore the reliability and proficiency of the work. Moreover, we discussed the advantages of CIVq-ROFS and showed that the concepts of IVCFS and q-ROFS are the special cases of CIVq-ROFS. Moreover, the flexibility of proposed averaging aggregation operator and geometric aggregation operator in a multi-attribute decision making (MADM) problem are also discussed. Finally, a comparative study of CIVq-ROFSs with pre-existing work is discussed in detail.},
  archive      = {J_EXSY},
  author       = {Harish Garg and Zeeshan Ali and Tahir Mahmood},
  doi          = {10.1111/exsy.12609},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12609},
  shortjournal = {Expert Syst.},
  title        = {Algorithms for complex interval-valued q-rung orthopair fuzzy sets in decision making based on aggregation operators, AHP, and TOPSIS},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automating test oracles from restricted natural language
agile requirements. <em>EXSY</em>, <em>38</em>(1), e12608. (<a
href="https://doi.org/10.1111/exsy.12608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual testing of software requirements written in natural language for agile or any other methodology requires more time and human resources. This leaves the testing process error prone and time consuming. For satisfied end users with bug-free software delivered on time, there is a need to automate the test oracle process for natural language or informal requirements. The automation of the test oracle is relatively easier with formal requirements, but this task is difficult to achieve with natural language requirements. This study proposes an approach called Restricted Natural Language Agile Requirements Testing (ReNaLART) to automate the test oracle from restricted natural language agile requirements. For this purpose, it uses an existing user story template with some modifications for writing user stories. This helps in identifying test input and expected output for a user story. For comparison of expected and observed outputs it makes use of a regex pattern and string distance functions. It is capable of assigning different types of verdicts automatically depending upon the similarity/dissimilarity between observed and expected outputs of user stories. ReNaLART is validated using several case studies of different domains, namely, OLX Pakistan, Mental Health Tests, McDelivery Pakistan, BlueStacks, Power Searching with Google, TensorFlow Playground, w3Schools 2018 offline and Touch&#39;D. It revealed several faults in five of the above listed eight applications. Plus, the proposed test oracle on an average took 0.02 s for test data generation, expected output generation and verdict assignment. Both these facts show the fault revealing effectiveness and efficiency of ReNaLART.},
  archive      = {J_EXSY},
  author       = {Maryam Imtiaz Malik and Muddassar Azam Sindhu and Akmal Saeed Khattak and Rabeeh Ayaz Abbasi and Khalid Saleem},
  doi          = {10.1111/exsy.12608},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12608},
  shortjournal = {Expert Syst.},
  title        = {Automating test oracles from restricted natural language agile requirements},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CoGCN: Combining co-attention with graph convolutional
network for entity linking with knowledge graphs. <em>EXSY</em>,
<em>38</em>(1), e12606. (<a
href="https://doi.org/10.1111/exsy.12606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity linking is a fundamental task in natural language processing. The task of entity linking with knowledge graphs aims at linking mentions in text to their correct entities in a knowledge graph like DBpedia or YAGO2. Most of existing methods rely on hand-designed features to model the contexts of mentions and entities, which are sparse and hard to calibrate. In this paper, we present a neural model that first combines co-attention mechanism with graph convolutional network for entity linking with knowledge graphs, which extracts features of mentions and entities from their contexts automatically. Specifically, given the context of a mention and one of its candidate entities&#39; context, we introduce the co-attention mechanism to learn the relatedness between the mention context and the candidate entity context, and build the mention representation in consideration of such relatedness. Moreover, we propose a context-aware graph convolutional network for entity representation, which takes both the graph structure of the candidate entity and its relatedness with the mention context into consideration. Experimental results show that our model consistently outperforms the baseline methods on five widely used datasets.},
  archive      = {J_EXSY},
  author       = {Ningning Jia and Xiang Cheng and Sen Su and Liyuan Ding},
  doi          = {10.1111/exsy.12606},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12606},
  shortjournal = {Expert Syst.},
  title        = {CoGCN: Combining co-attention with graph convolutional network for entity linking with knowledge graphs},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Authority updating: An expert authority evaluation algorithm
considering post-evaluation and power indices in social networks.
<em>EXSY</em>, <em>38</em>(1), e12605. (<a
href="https://doi.org/10.1111/exsy.12605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In group assessment, the focus is on finding high-authority experts to improve the reliability of assessment results. In this study, we propose an authority updating algorithm while considering the power and judgement reliability of an expert on the basis of social networks and post-evaluations. A network power index is established and used to reflect the power of an expert while considering social networks. The measurement of the judgement reliability of an expert considers the post-evaluation of the objects selected by experts, thereby more scientifically reflecting the reliability of experts. The analysis shows the following: although the social-network structure influences the authority of experts, the influence weakens when the assessment group is a highly or even fully connected group; the network effect may increase the authority of some experts and reduce that of others, and it will weaken as the network connectivity increases; moreover, the judgement reliability and authority of an expert while considering post-evaluation can encourage him/her to make fair assessments and strive to reduce his/her motivation and cognitive biases.},
  archive      = {J_EXSY},
  author       = {Ruili Shi and Chunxiang Guo and Xin Gu},
  doi          = {10.1111/exsy.12605},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12605},
  shortjournal = {Expert Syst.},
  title        = {Authority updating: An expert authority evaluation algorithm considering post-evaluation and power indices in social networks},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving answer selection with global features.
<em>EXSY</em>, <em>38</em>(1), e12603. (<a
href="https://doi.org/10.1111/exsy.12603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a question and its answer candidates (named QA corpus), answer selection is the task of identifying the most relevant answers to the question. Answer selection is widely used in question answering, web search, and so on. Current deep neural network models primarily utilize local features extracted from input question-answer pairs (QA pairs). However, the global features contained in QA corpora are under-utilized, and we argue that these global features substantially contribute to the answer selection task. To verify this point of view, we propose a novel model that combines local and global features for answer selection. In our model, two different global feature extractors are employed to extract statistical global features and deep global features from a QA corpus, respectively. Furthermore, we investigate the integration of these global features with local features in various experimental settings: statistical global features, deep global features, and a combination of statistical and deep global features. Our experimental results show that the global features are effective for answer selection. Our model obtains new state-of-the-art results on two public answer selection datasets and performs especially well on YahooCQA, where it achieves 9.2 and 6% higher precision@1 (P@1) and mean reciprocal rank (MRR) scores than previously published models.},
  archive      = {J_EXSY},
  author       = {Shengwei Gu and Xiangfeng Luo and Hao Wang and Jing Huang and Qin Wei and Subin Huang},
  doi          = {10.1111/exsy.12603},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12603},
  shortjournal = {Expert Syst.},
  title        = {Improving answer selection with global features},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Common set of weights in data envelopment analysis under
prospect theory. <em>EXSY</em>, <em>38</em>(1), e12602. (<a
href="https://doi.org/10.1111/exsy.12602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data envelopment analysis (DEA) is a data-driven tool for performance evaluation, measuring decision-making units (DMUs) and designating them with specific weightings. The standard DEA model typically sets up that decision-makers (DMs) are wholly rational to select the most favourable weights to obtain the maximum performance score, but does not take into account their attitude toward risk during the assessment. The prospect theory generally matches humans&#39; psychological behaviours. Thus, our study captures the non-rational behaviours of DMs, performing under risk scenarios, in order to construct a novel common-weights DEA model that maximizes the total prospect value, which can vary more steeply for losses than for gains, hence obtaining a more realistic common weight scheme. Our proposed model not only generates DMUs, with higher total prospect values, but also greater degrees of satisfaction. The current study shows that the prospect theory can be aptly extended to the DEA research area, supplying a proper guideline for future DEA research.},
  archive      = {J_EXSY},
  author       = {Yu Yu and Weiwei Zhu and Qinfen Shi and Shangwen Zhuang},
  doi          = {10.1111/exsy.12602},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12602},
  shortjournal = {Expert Syst.},
  title        = {Common set of weights in data envelopment analysis under prospect theory},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter reductions in n-soft sets and their applications
in decision-making. <em>EXSY</em>, <em>38</em>(1), e12601. (<a
href="https://doi.org/10.1111/exsy.12601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter reduction is an important operation for improving the performance of decision-making processes in various uncertainty theories. The theory of N -soft sets is emerging as a powerful mathematical tool for dealing with uncertainties beyond the standard formulation of the soft set theory. In this research article, we extend the notion of parameter reduction to N -soft set theory, and we also justify its practical calculation. To this purpose, we define related theoretical concepts (e.g. N -soft subset, reduct N -soft set and redundant parameter) and examine some of their fundamental properties. Then, we argue that the idea of attributes reduction from the rough set theory cannot be employed in the N -soft set theory in order to reduce the number of parameters. Consequently, we take an original position in order to adequately define and compute parameter reductions in N -soft sets. Finally, we develop an application of parameter reduction of N -soft sets.},
  archive      = {J_EXSY},
  author       = {Muhammad Akram and Ghous Ali and José C. R. Alcantud and Fatia Fatimah},
  doi          = {10.1111/exsy.12601},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12601},
  shortjournal = {Expert Syst.},
  title        = {Parameter reductions in N-soft sets and their applications in decision-making},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Are you sure? Prediction revision in automated
decision-making. <em>EXSY</em>, <em>38</em>(1), e12577. (<a
href="https://doi.org/10.1111/exsy.12577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid improvements in machine learning and deep learning, decisions made by automated decision support systems (DSS) will increase. Besides the accuracy of predictions, their explainability becomes more important. The algorithms can construct complex mathematical prediction models. This causes insecurity to the predictions. The insecurity rises the need for equipping the algorithms with explanations. To examine how users trust automated DSS, an experiment was conducted. Our research aim is to examine how participants supported by an DSS revise their initial prediction by four varying approaches (treatments) in a between-subject design study. The four treatments differ in the degree of explainability to understand the predictions of the system. First we used an interpretable regression model, second a Random Forest (considered to be a black box [BB]), third the BB with a local explanation and last the BB with a global explanation. We noticed that all participants improved their predictions after receiving an advice whether it was a complete BB or an BB with an explanation. The major finding was that interpretable models were not incorporated more in the decision process than BB models or BB models with explanations.},
  archive      = {J_EXSY},
  author       = {Nadia Burkart and Sebastian Robert and Marco F. Huber},
  doi          = {10.1111/exsy.12577},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12577},
  shortjournal = {Expert Syst.},
  title        = {Are you sure? prediction revision in automated decision-making},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel method for the classification of alzheimer’s disease
from normal controls using magnetic resonance imaging. <em>EXSY</em>,
<em>38</em>(1), e12566. (<a
href="https://doi.org/10.1111/exsy.12566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is the most prevalent form of dementia. Although fewer people, who suffer from AD are correctly and promptly diagnosed, due to a lack of knowledge of its cause and unavailability of treatment, AD is more manageable if the symptoms of mild cognitive impairment (MCI) are in an early stage. In recent years, computer-aided diagnosis has been widely used for the diagnosis of AD. The main motive of this paper is to improve the classification and prediction accuracy of AD. In this paper, a novel approach is developed to classify MCI, normal control (NC), and AD using structural magnetic resonance imaging (sMRI) from the Alzheimer&#39;s disease Neuroimaging Initiative (ADNI) dataset (50 AD, 50 NC, 50 MCI subjects). FreeSurfer is used to process these MRI data and obtain cortical features such as volume, surface area, thickness, white matter (WM), and intrinsic curvature of the brain regions. These features are modified by normalizing each cortical region&#39;s features using the absolute maximum value of that region&#39;s features from all subjects in each group of MCI, NC, and AD independently. A total of 420 features are obtained. To address the curse of dimensionality, the obtained features are reduced to 30 features using a sequential feature selection technique. Three classifiers, namely the twin support vector machine (TSVM), least squares TSVM (LSTSVM), and robust energy-based least squares TSVM (RELS-TSVM), are used to evaluate the classification accuracy from the obtained features. Five-fold and 10-fold cross-validation are used to validate the proposed method. Experimental results show an accuracy of 100% for the studied database. The proposed approach is innovative due to its higher classification accuracy compared to methods in the existing literature.},
  archive      = {J_EXSY},
  author       = {Riyaj Uddin Khan and Mohammad Tanveer and Ram Bilas Pachori},
  doi          = {10.1111/exsy.12566},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12566},
  shortjournal = {Expert Syst.},
  title        = {A novel method for the classification of alzheimer&#39;s disease from normal controls using magnetic resonance imaging},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real time detection of acoustic anomalies in industrial
processes using sequential autoencoders. <em>EXSY</em>, <em>38</em>(1),
e12564. (<a href="https://doi.org/10.1111/exsy.12564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of intelligent systems with the pursuit of detecting abnormal events in real world and in real time is challenging due to difficult environmental conditions, hardware limitations, and computational algorithmic restrictions. As a result, degradation of detection performance in dynamically changing environments is often encountered. However, in the next-generation factories, an anomaly detection system based on acoustic signals is especially required to quickly detect and interfere with the abnormal events during the industrial processes due to the increased cost of complex equipment and facilities. In this study we propose a real time Acoustic Anomaly Detection (AAD) system with the use of sequence-to-sequence Autoencoder (AE) models in the industrial environments. The proposed processing pipeline makes use of the audio features extracted from the streaming audio signal captured by a single-channel microphone. The reconstruction error generated by the AE model is calculated to measure the degree of abnormality of the sound event. The performance of Convolutional Long Short-Term Memory AE (Conv-LSTMAE) is evaluated and compared with sequential Convolutional AE (CAE) using sounds captured from various industrial manufacturing processes. In the experiments conducted with the real time AAD system, it is shown that the Conv-LSTMAE-based AAD demonstrates better detection performance than CAE model-based AAD under different signal-to-noise ratio conditions of sound events such as explosion, fire and glass breaking.},
  archive      = {J_EXSY},
  author       = {Barış Bayram and Taha Berkay Duman and Gökhan Ince},
  doi          = {10.1111/exsy.12564},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12564},
  shortjournal = {Expert Syst.},
  title        = {Real time detection of acoustic anomalies in industrial processes using sequential autoencoders},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling material flow using the milk run and kanban
systems in the automotive industry. <em>EXSY</em>, <em>38</em>(1),
e12546. (<a href="https://doi.org/10.1111/exsy.12546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material flow management refers to the analysis and specific optimization of the inventory-production system. Material flow can be characterized as the organized flow of material in a production process with the required sequence determined by a technological procedure. The Milk run system assures the transportation of materials at the right time and in an optimal manner. It should be combined with the Kanban system to highlight when something is required in the production process. This paper presents biological swarm intelligence, in general, and a particular model, particle swarm optimization (PSO), for modelling material flow using a Milk run system supported by a Kanban system in the automotive industry. The aim of this study is to create a new model for the optimal number of trailers for one train and optimal number of containers in a tugger train system when the route time period has been defined. A new modified PSO approach for integrating inventory-production in a unique optimization model is used. The major modification to the original PSO is using the capacity of a container instead of a velocity component. Each new Kanban trigger is checked, and the total timing for the Milk run delivery solution is calculated for the necessary raw material capacity for each shop floor.},
  archive      = {J_EXSY},
  author       = {Dragan Simić and Vasa Svirčević and Emilio Corchado and José L. Calvo-Rolle and Svetislav D. Simić and Svetlana Simić},
  doi          = {10.1111/exsy.12546},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12546},
  shortjournal = {Expert Syst.},
  title        = {Modelling material flow using the milk run and kanban systems in the automotive industry},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Creation of original tamil character dataset through
segregation of ancient palm leaf manuscripts in medicine. <em>EXSY</em>,
<em>38</em>(1), e12538. (<a
href="https://doi.org/10.1111/exsy.12538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palm leaves were one of the essential and primary sources for writing before the advent of paper. Tamil is one of the oldest southern Indian languages and among the ten oldest languages of the world. Agathiyar, a renowned Siddhar of ancient India, considered as the Father of Siddha Medicine, wrote all his therapeutic procedures only on palm leaf manuscripts, in the Tamil language. For modern day, people who try to only write and read new aspects of Tamil, identifying the ancient characters of the language is difficult. To expand readability and secure the written medicinal practices and traditions, a better recognition system is needed that can transform the ancient text images to modern ones, interpreting the ancient Tamil characters from palm leaves and understanding their contexts a time-consuming and complicated process. Especially when it comes to medicine, the practitioners need to understand the contents of a manuscript, to apply them on a daily basis. Therefore, a recognition system is of much use to understand, interpret, and apply the techniques explained in the manuscript on a daily basis. This study is an attempt to create a considerable volume of Tamil character datasets through the segregation of ancient Tamil palm leaf manuscripts related to the field of medicine. In this study, the characters created are fed as inputs to expert systems for intelligent recognition of the context and content perceived to be present in the selected medical manuscripts. The characters have been identified in large numbers manually, and datasets are created using Gaussian distortion.},
  archive      = {J_EXSY},
  author       = {Kavitha Subramani and Murugavalli Subramaniam},
  doi          = {10.1111/exsy.12538},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12538},
  shortjournal = {Expert Syst.},
  title        = {Creation of original tamil character dataset through segregation of ancient palm leaf manuscripts in medicine},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent hybrid approach for task scheduling in
cluster computing environments as an infrastructure for biomedical
applications. <em>EXSY</em>, <em>38</em>(1), e12536. (<a
href="https://doi.org/10.1111/exsy.12536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, increase in time complexity of applications and decrease in hardware costs are two major contributing drivers for the utilization of high-performance architectures such as cluster computing systems. Actually, cluster computing environments, in the contemporary sophisticated data centres, provide the main infrastructure to process various data, where the biomedical one is not an exception. Indeed, optimized task scheduling is key to achieve high performance in such computing environments. The most distractive assumption about the problem of task scheduling, made by the state-of-the-art approaches, is to assume the problem as a whole and try to enhance the overall performance, while the problem is actually consisted of two disparate-in-nature subproblems, that is, sequencing subproblem and assigning one, each of which needs some special considerations. In this paper, an efficient hybrid approach named ACO-CLA is proposed to solve task scheduling problem in the mesh-topology cluster computing environments. In the proposed approach, an enhanced ant colony optimization (ACO) is developed to solve the sequence subproblem, whereas a cellular learning automata (CLA) machine tackles the assigning subproblem. The utilization of background knowledge about the problem (i.e., tasks&#39; priorities) has made the proposed approach very robust and efficient. A randomly generated data set consisting of 125 different random task graphs with various shape parameters, like the ones frequently encountered in the biomedicine, has been utilized for the evaluation of the proposed approach. The conducted comparison study clearly shows the efficiency and superiority of the proposed approach versus traditional counterparts in terms of the performance. From our first metric, that is, the NSL (normalized schedule length) point of view, the proposed ACO-CLA is 2.48% and 5.55% better than the ETF (earliest time first), which is the second-best approach, and the average performance of all other competing methods. On the other hand, from our second metric, that is, the speedup perspective, the proposed ACO-CLA is 2.66% and 5.15% better than the ETF (the second-best approach) and the average performance of all the other competitors.},
  archive      = {J_EXSY},
  author       = {Hamid Reza Boveiri and Reza Javidan and Raouf Khayami},
  doi          = {10.1111/exsy.12536},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12536},
  shortjournal = {Expert Syst.},
  title        = {An intelligent hybrid approach for task scheduling in cluster computing environments as an infrastructure for biomedical applications},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic decision support framework for production scheduling
using a combined genetic algorithm and multiagent model. <em>EXSY</em>,
<em>38</em>(1), e12533. (<a
href="https://doi.org/10.1111/exsy.12533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the dynamic nature, complexity, and interactivity of production scheduling in an actual business environment, suitable combined and hybrid methods are necessary. This paper takes prefabricated concrete components as an example and develops the dynamic decision support framework based on a genetic algorithm and multiagent system (MAS) to optimize and simulate the production scheduling. First, a multiobjective genetic algorithm is integrated into the MAS for preliminary optimization and a series of near-optimal solutions are obtained. Subsequently, considering the resource constraints and uncertainties, the MAS is used to simulate complex real-world production environments. Considering the different types of uncertainty factors, the paper proposes the corresponding dynamic scheduling method and uses MAS to generate the optimal production schedule. Finally, a practical prefabricated construction case is used to validate the proposed model. The results show that the model can effectively address the occurrence of uncertain events and can provide dynamic decision support for production scheduling.},
  archive      = {J_EXSY},
  author       = {Juan Du and Peng Dong and Vijayan Sugumaran and Daniel Castro-Lacouture},
  doi          = {10.1111/exsy.12533},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12533},
  shortjournal = {Expert Syst.},
  title        = {Dynamic decision support framework for production scheduling using a combined genetic algorithm and multiagent model},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent hybrid approach for hepatitis disease
diagnosis: Combining enhanced k-means clustering and improved ensemble
learning. <em>EXSY</em>, <em>38</em>(1), e12526. (<a
href="https://doi.org/10.1111/exsy.12526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real world, the automatic detection of liver disease is a challenging problem among medical practitioners. The intent of this work is to propose an intelligent hybrid approach for the diagnosis of hepatitis disease. The diagnosis is performed with the combination of k- means clustering and improved ensemble-driven learning. To avoid clinical experience and to reduce the evaluation time, ensemble learning is deployed, which constructs a set of hypotheses by using multiple learners to solve a liver disease problem. The performance analysis of the proposed integrated hybrid system is compared in terms of accuracy, true positive rate, precision, f-measure, kappa statistic, mean absolute error, and root mean squared error. Simulation results showed that the enhanced k- means clustering and improved ensemble learning with enhanced adaptive boosting, bagged decision tree, and J48 decision tree-based intelligent hybrid approach achieved better prediction outcomes than other existing individual and integrated methods.},
  archive      = {J_EXSY},
  author       = {Aman Singh and Jaydip Chandrakant Mehta and Divya Anand and Pinku Nath and Babita Pandey and Aditya Khamparia},
  doi          = {10.1111/exsy.12526},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12526},
  shortjournal = {Expert Syst.},
  title        = {An intelligent hybrid approach for hepatitis disease diagnosis: Combining enhanced k-means clustering and improved ensemble learning},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection of optimized features for fusion of palm print and
finger knuckle-based person authentication. <em>EXSY</em>,
<em>38</em>(1), e12523. (<a
href="https://doi.org/10.1111/exsy.12523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of digital technology in biometrics is much more efficient at interpreting data than humans, which results in completely replacement of manual identification procedures in forensic science. Because the single modality-based biometric frameworks limit performance in terms of accuracy and anti-spoofing capabilities due to the presence of low quality data, therefore, information fusion of more than one biometric characteristic in pursuit of high recognition results can be beneficial. In this article, we present a multimodal biometric system based on information fusion of palm print and finger knuckle traits, which are least associated to any criminal investigation as evidence yet. The proposed multimodal biometric system might be useful to identify the suspects in case of physical beating or kidnapping and establish supportive scientific evidences, when no fingerprint or face information is present in photographs. The first step in our work is data preprocessing, in which region of interest of palm and finger knuckle images have been extracted. To minimize nonuniform illumination effects, we first normalize the detected circular palm or finger knuckle and then apply line ordinal pattern (LOP)-based encoding scheme for texture enrichment. The nondecimated quaternion wavelet provides denser feature representation at multiple scales and orientations when extracted over proposed LOP encoding and increases the discrimination power of line and ridge features. To best of our knowledge, this first attempt is a combination of backtracking search algorithm and 2D 2 LDA has been employed to select the dominant palm and knuckle features for classification. The classifiers output for two modalities are combined at unsupervised rank level fusion rule through Borda count method, which shows an increase in performance in terms of recognition and verification, that is, 100% (correct recognition rate), 0.26% (equal error rate), 3.52 (discriminative index), and 1,262 m (speed).},
  archive      = {J_EXSY},
  author       = {Gaurav Jaswal and Ramesh Chandra Poonia},
  doi          = {10.1111/exsy.12523},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12523},
  shortjournal = {Expert Syst.},
  title        = {Selection of optimized features for fusion of palm print and finger knuckle-based person authentication},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evolutionary lion optimization algorithm-based image
compression technique for biomedical applications. <em>EXSY</em>,
<em>38</em>(1), e12508. (<a
href="https://doi.org/10.1111/exsy.12508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, medical image compression becomes essential to effectively handle large amounts of medical data for storage and communication purposes. Vector quantization (VQ) is a popular image compression technique, and the commonly used VQ model is Linde–Buzo–Gray (LBG) that constructs a local optimal codebook to compress images. The codebook construction was considered as an optimization problem, and a bioinspired algorithm was employed to solve it. This article proposed a VQ codebook construction approach called the L2-LBG method utilizing the Lion optimization algorithm (LOA) and Lempel Ziv Markov chain Algorithm (LZMA). Once LOA constructed the codebook, LZMA was applied to compress the index table and further increase the compression performance of the LOA. A set of experimentation has been carried out using the benchmark medical images, and a comparative analysis was conducted with Cuckoo Search-based LBG (CS-LBG), Firefly-based LBG (FF-LBG) and JPEG2000. The compression efficiency of the presented model was validated in terms of compression ratio (CR), compression factor (CF), bit rate, and peak signal to noise ratio (PSNR). The proposed L2-LBG method obtained a higher CR of 0.3425375 and PSNR value of 52.62459 compared to CS-LBG, FA-LBG, and JPEG2000 methods. The experimental values revealed that the L2-LBG process yielded effective compression performance with a better-quality reconstructed image.},
  archive      = {J_EXSY},
  author       = {Karuppaiah Geetha and Veerasamy Anitha and Mohamed Elhoseny and Shankar Kathiresan and Pourya Shamsolmoali and Mahmoud M. Selim},
  doi          = {10.1111/exsy.12508},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12508},
  shortjournal = {Expert Syst.},
  title        = {An evolutionary lion optimization algorithm-based image compression technique for biomedical applications},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing proportional integral controller for a solar
powered DC-DC converter using PIPSO algorithm and deep learning approach
in intelligent bio-medical applications. <em>EXSY</em>, <em>38</em>(1),
e12488. (<a href="https://doi.org/10.1111/exsy.12488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous engineering complexities are simplified using optimization algorithms. In a solar power system, the necessity of the voltage regulator is obvious. To control the regulator existent research works used PI, PID controllers that might have an unwanted transient response. To overcome such drawbacks here, a fresh scheme is proposed for the designing of the adaptive sliding mode (SM) controller of a solar powered LUO converter using optimization algorithms. The PSO (‘Particle Swarm Optimization&#39;) is proved to expedite the convergence characteristic for many applications. Here, an ameliorated PSO version is developed. This algorithm is termed the Parameter Improved-PSO (PIPSO) algorithm. In this algorithm, the parameters, say, inertia weight, social along with cognitive agents is updated in every generation. The Proportional Integrator (PI) controller is used. The gain of this controller is tuned using the PIPSO. This algorithm&#39;s objective function is to lessen ISE (‘Integral Squared Error’) of the converter&#39;s output voltage. This parameter is picked as the objective function of the optimization algorithm. The proposed PIPSO is established to show better outcomes when contrasted to the traditional PSO concerning tuning a collection of parameters. An analysis is also made to evaluate the effect of usage of the solar panel () in the proposed work.},
  archive      = {J_EXSY},
  author       = {S. Rajarajacholan and M. Balasingh Moses and J. Barsanabanu},
  doi          = {10.1111/exsy.12488},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12488},
  shortjournal = {Expert Syst.},
  title        = {Designing proportional integral controller for a solar powered DC-DC converter using PIPSO algorithm and deep learning approach in intelligent bio-medical applications},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid particle swarm optimization for rule discovery in the
diagnosis of coronary artery disease. <em>EXSY</em>, <em>38</em>(1),
e12485. (<a href="https://doi.org/10.1111/exsy.12485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) is one of the major causes of mortality worldwide. Knowledge about risk factors that increase the probability of developing CAD can help to understand the disease better and assist in its treatment. Recently, modern computer-aided approaches have been used for the prediction and diagnosis of diseases. Swarm intelligence algorithms like particle swarm optimization (PSO) have demonstrated great performance in solving different optimization problems. As rule discovery can be modelled as an optimization problem, it can be mapped to an optimization problem and solved by means of an evolutionary algorithm like PSO. An approach for discovering classification rules of CAD is proposed. The work is based on the real-world CAD data set and aims at the detection of this disease by producing the accurate and effective rules. The proposed algorithm is a hybrid binary-real PSO, which includes the combination of categorical and numerical encoding of a particle and a different approach for calculating the velocity of particles. The rules were developed from randomly generated particles, which take random values in the range of each attribute in the rule. Two different feature selection methods based on multi-objective evolutionary search and PSO were applied on the data set, and the most relevant features were selected by the algorithms. The accuracy of two different rule sets were evaluated. The rule set with 11 features obtained more accurate results than the rule set with 13 features. Our results show that the proposed approach has the ability to produce effective rules with highest accuracy for the detection of CAD.},
  archive      = {J_EXSY},
  author       = {Mariam Zomorodi-moghadam and Moloud Abdar and Zohreh Davarzani and Xujuan Zhou and Pawel Pławiak and U.Rajendra Acharya},
  doi          = {10.1111/exsy.12485},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12485},
  shortjournal = {Expert Syst.},
  title        = {Hybrid particle swarm optimization for rule discovery in the diagnosis of coronary artery disease},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toc-tum mini-games: An educational game accessible for deaf
culture based on virtual reality. <em>EXSY</em>, <em>38</em>(1), e12470.
(<a href="https://doi.org/10.1111/exsy.12470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human hearing is an important sense, which complements the other senses, and is essential for children to begin to acquire basic concepts of the world; however, in severe cases of disability, children become marginalized from these benefits. An example of this is with music, a deaf individual is to be able to perceive it. With this in mind, we developed a novel medical expert system based on virtual reality, called Toc-Tum mini-games, applying basic concepts of music in an accessible way to the deaf culture. The intelligent proposed system is validated in two different tests: (a) with the target audience to evaluate the interest in the game and (b) with professionals knowledgeable in the field of music or who have had contact with the deaf, evaluate the impact of the game teaching music to the deaf. The tests with the target audience showed that the methods of interaction allowed the children to understand most of the stages and that they were animated during the test days. Regarding the professionals, they were interested in the possibility of using the proposed system in question, even if some modifications in the design had to be made. The results showed that the Toc-Tum mini-games is an intelligent tool capable of introducing music in a playful and manageable way using a virtual environment.},
  archive      = {J_EXSY},
  author       = {Edilson M. Chaves and Paulo Bruno de A. Braga and Yuri Fontenelle L. Montenegro and Vitória B. Rodrigues and Marilene C. Munguba and Victor Hugo C. de Albuquerque},
  doi          = {10.1111/exsy.12470},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12470},
  shortjournal = {Expert Syst.},
  title        = {Toc-tum mini-games: An educational game accessible for deaf culture based on virtual reality},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Android application behavioural analysis for data leakage.
<em>EXSY</em>, <em>38</em>(1), e12468. (<a
href="https://doi.org/10.1111/exsy.12468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An android application requires specific permissions from the user to access the system resources and perform required functionalities. Recently, the android market has experienced exponential growth, which leads to malware applications. These applications are purposefully developed by hackers to access private data of the users and adversely affect the application usability. A suitable tool to detect malware is urgently needed, as malware may harm the user. As both malware and clean applications require similar types of permissions, so it becomes a very challenging task to differentiate between them. A novel algorithm is proposed to identify the malware-based applications by probing the permission patterns. The proposed method uses the k-means algorithm to quarantine the malware application by obtaining permission clusters. An efficiency of 90% (approx.) is attained for malicious behaviour, which validates this work. This work substantiates the use of application permissions for potential applications in android malware detection.},
  archive      = {J_EXSY},
  author       = {Gulshan Shrivastava and Prabhat Kumar},
  doi          = {10.1111/exsy.12468},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12468},
  shortjournal = {Expert Syst.},
  title        = {Android application behavioural analysis for data leakage},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance-enhanced modified self-organising map for iris
data classification. <em>EXSY</em>, <em>38</em>(1), e12467. (<a
href="https://doi.org/10.1111/exsy.12467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric systems are widely used in applications such as forensics and military. Biometric authentication is a challenging and complex task. These biometric systems must be accurate for practical applications. In this era of artificial intelligence, artificial neural network-based classifiers are widely used in biometric-based systems. However, most of the artificial neural network-based classifiers are less accurate and computationally complex. In this work, two modified self-organising map (SOM) networks are proposed for iris image classification to improve the performance measures. Particle swarm optimization technique is used in the training process of conventional SOM. The experiments are carried out with conventional and modified classifiers. The proposed modified classifiers provide better performance than the conventional SOM classifier.},
  archive      = {J_EXSY},
  author       = {J. Jenkin Winston and D. Jude Hemanth},
  doi          = {10.1111/exsy.12467},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12467},
  shortjournal = {Expert Syst.},
  title        = {Performance-enhanced modified self-organising map for iris data classification},
  volume       = {38},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
