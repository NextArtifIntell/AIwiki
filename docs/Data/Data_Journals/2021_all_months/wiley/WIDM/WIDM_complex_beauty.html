<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm---38">WIDM - 38</h2>
<ul>
<li><details>
<summary>
(2021). Overview of accurate coresets. <em>WIDM</em>,
<em>11</em>(6), e1429. (<a
href="https://doi.org/10.1002/widm.1429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A coreset of an input set is its small summarization, such that solving a problem on the coreset as its input, provably yields the same result as solving the same problem on the original (full) set, for a given family of problems (models/classifiers/loss functions). Coresets have been suggested for many fundamental problems, for example, in machine/deep learning, computer vision, databases, and theoretical computer science. This introductory paper was written following requests regarding the many inconsistent coreset definitions, lack of source code, the required deep theoretical background from different fields, and the dense papers that make it hard for beginners to apply and develop coresets. The article provides folklore, classic, and simple results including step-by-step proofs and figures, for the simplest (accurate) coresets. Nevertheless, we did not find most of their constructions in the literature. Moreover, we expect that putting them together in a retrospective context would help the reader to grasp current results that usually generalize these fundamental observations. Experts might appreciate the unified notation and comparison table for existing results. Open source code is provided for all presented algorithms, to demonstrate their usage, and to support the readers who are more familiar with programming than mathematics. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ibrahim Jubran and Alaa Maalouf and Dan Feldman},
  doi          = {10.1002/widm.1429},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1429},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Overview of accurate coresets},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining text from natural scene and video images: A survey.
<em>WIDM</em>, <em>11</em>(6), e1428. (<a
href="https://doi.org/10.1002/widm.1428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer terminology, mining is considered as extracting meaningful information or knowledge from a large amount of data/information using computers. The meaningful information can be extracted from normal text, and images obtained from different resources, such as natural scene images, video, and documents by deriving semantics from text and content of the images. Although there are many pieces of work on text/data mining and several survey/review papers are published in the literature, to the best of our knowledge there is no survey paper on mining textual information from the natural scene, video, and document images considering word spotting techniques. In this article, we, therefore, provide a comprehensive review of both the non-spotting and spotting based mining techniques. The mining approaches are categorized as feature, learning and hybrid-based methods to analyze the strengths and limitations of the models of each category. In addition, it also discusses the usefulness of the methods according to different situations and applications. Furthermore, based on the review of different mining approaches, this article identifies the limitations of the existing methods and suggests new applications and future directions to continue the research in multiple directions. We believe such a review article will be useful to the researchers to quickly become familiar with the state-of-the-art information and progresses made toward mining textual information from natural scene and video images. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Palaiahnakote Shivakumara and Alireza Alaei and Umapada Pal},
  doi          = {10.1002/widm.1428},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1428},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Mining text from natural scene and video images: A survey},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Critical insights into modern hyperspectral image
applications through deep learning. <em>WIDM</em>, <em>11</em>(6),
e1426. (<a href="https://doi.org/10.1002/widm.1426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging has shown tremendous growth over the past three decades. Hyperspectral imaging was evolved through remote sensing. Along, with the technological enhancements hyperspectral imaging has outgrown, conquering over other various application areas. In addition to it, data enriched data cubes with abundant spectral and spatial information works as perk for capturing, analyzing, reviewing, and interpreting results from data. This review concentrates on emerging application areas of hyperspectral imaging. Emerging application areas are selected in ways where there is a vast scope for future enhancements by exploiting cutting edge technology, that is, deep learning. Applications of hyperspectral imaging techniques in some selected areas (remote sensing, document forgery, history and archaeology conservation, surveillance and security, machine vision for fruit quality inspection, medical imaging) are focused. The review pivots around the publicly available datasets and features used domain wise. This review can act as a baseline for deep learning and machine vision experts, historical geographers, and scholars by providing them a view of how hyperspectral imaging is implemented in multiple domains along with future research prospects. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Garima Jaiswal and Arun Sharma and Sumit Kumar Yadav},
  doi          = {10.1002/widm.1426},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1426},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Critical insights into modern hyperspectral image applications through deep learning},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Text-based question answering from information retrieval and
deep neural network perspectives: A survey. <em>WIDM</em>,
<em>11</em>(6), e1412. (<a
href="https://doi.org/10.1002/widm.1412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based question answering (QA) is a challenging task which aims at finding short concrete answers for users&#39; questions. This line of research has been widely studied with information retrieval (IR) techniques and has received increasing attention in recent years by considering deep neural network approaches. Deep learning (DL) approaches, which are the main focus of this paper, provide a powerful technique to learn multiple layers of representations and interaction between the questions and the answer sentences. In this paper, we provide a comprehensive overview of different models proposed for the QA task, including both a traditional IR perspective and a more recent deep neural network environment. We also introduce well-known datasets for the task and present available results from the literature to have a comparison between different techniques. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Zahra Abbasiantaeb and Saeedeh Momtazi},
  doi          = {10.1002/widm.1412},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1412},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Text-based question answering from information retrieval and deep neural network perspectives: A survey},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The development of regional smart energy systems in the
world and china: The concepts, practices, and a new perspective.
<em>WIDM</em>, <em>11</em>(6), e1409. (<a
href="https://doi.org/10.1002/widm.1409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To realize a low-carbon and sustainable energy transition, smart energy systems (SES) assisted by data and information technology are regarded as promising solutions for energy system integration (ESI) and have been put into regional practices. However, there is still lacking attention on the development of multiregional smart energy systems (MRSES), which include three or more areas. This article aims to analyze concepts and practices of SES and enlighten a new perspective of MRSES. The conceptual evolution and regional practices of SES in the world were first reviewed, and it was found out that SES does not means the end of the conceptual evolution of ESI. Current regional practices are still limited in small areas, being typically remote areas, urban areas, and industrial areas. Secondly, the review of concepts and practices of SES in China indicate that the understanding of SES concepts are still confusing in national scale, and the apparent regional disparity in China is calling attention on the development of MRSES. Finally, a preliminary concept of MRSES was proposed and its perspective in China and the world, which is composed by four connected sub-SES and named as a coordinated development of “smart energy farms + smart energy towns + smart energy industrial parks + smart energy transportation networks” was discussed. The former three sub-SES are identified according to various economic characteristics and resources endowment in different regions, and they are all connected by the forth sub-SES. Although this concept is still preliminary, it provides an imagination of future large-scale SES, and the realization of this concept needs further breakthrough of data technology. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Yunlong Zhao and Linwei Ma and Zheng Li and Weidou Ni},
  doi          = {10.1002/widm.1409},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1409},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The development of regional smart energy systems in the world and china: The concepts, practices, and a new perspective},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on machine learning based light curve analysis for
variable astronomical sources. <em>WIDM</em>, <em>11</em>(5), e1425. (<a
href="https://doi.org/10.1002/widm.1425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement of observation capabilities has expanded the scale of new data available for time domain astronomy research, and the accumulation of observational data continues to accelerate. However, traditional data analysis methods are difficult to fully tap the potential scientific value of all data. Therefore, in the current and future research on light curve analysis, it is inevitable to use artificial intelligence (AI) technology to assist in data analysis in order to obtain as many candidates as possible with scientific research goals. This survey reviews important developments in light curve analysis over the past years, summarizes the basic concepts in machine learning and their applications in light curve analysis and concludes perspectives and challenges for light curve analysis in the near future. The full exploration of light curves of variable celestial objects relies heavily on new techniques derived from promotion of machine learning and deep learning in the astronomical big data era. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ce Yu and Kun Li and Yanxia Zhang and Jian Xiao and Chenzhou Cui and Yihan Tao and Shanjiang Tang and Chao Sun and Chongke Bi},
  doi          = {10.1002/widm.1425},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1425},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on machine learning based light curve analysis for variable astronomical sources},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable artificial intelligence: An analytical review.
<em>WIDM</em>, <em>11</em>(5), e1424. (<a
href="https://doi.org/10.1002/widm.1424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a brief analytical review of the current state-of-the-art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Plamen P. Angelov and Eduardo A. Soares and Richard Jiang and Nicholas I. Arnold and Peter M. Atkinson},
  doi          = {10.1002/widm.1424},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1424},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Explainable artificial intelligence: An analytical review},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigation of PM10 prediction utilizing data mining
techniques: Analyze by topic. <em>WIDM</em>, <em>11</em>(5), e1423. (<a
href="https://doi.org/10.1002/widm.1423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse particulate matter (PM 10 ), the inhalable particles with an aerodynamic diameter smaller than 10 micrometers are one of the major air pollutions that affect human health. Over the previous decade, a number of researchers applied various data mining techniques to create a temporal prediction model. This study reviews and discusses 100 research articles in computer science and environmental science coming from the Scopus database. The three processes of data mining techniques, including data preparation, model creation, and model evaluation for prediction PM 10 are highlighted. A summary of the overall process directions of data mining as well as their output are revealed. Additionally, recommendations for future research are identified. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Krittakom Srijiranon and Narissara Eiamkanitchat and Sakgasit Ramingwong and Kenneth Cosh and Lachana Ramingwong},
  doi          = {10.1002/widm.1423},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1423},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Investigation of PM10 prediction utilizing data mining techniques: Analyze by topic},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trending machine learning models in cyber-physical building
environment: A survey. <em>WIDM</em>, <em>11</em>(5), e1422. (<a
href="https://doi.org/10.1002/widm.1422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity usage of buildings (including offices, malls, and residential apartments) represents a significant portion of a nation&#39;s energy expenditure and carbon footprint. In the United States, the buildings&#39; appliances consume 72% of the total produced electricity approximately. In this regard, cyber-physical system (CPS) researchers have put forth associated research questions to reduce cyber-physical building environment energy consumption by minimizing the energy dissipation while securing occupants&#39; comfort. Some of the questions in CPS building include finding the optimal HVAC control, monitoring appliances&#39; energy usage, detecting insulation problems, estimating the occupants&#39; number and activities, managing thermal comfort, intelligently interacting with the smart grid. Various machine learning (ML) applications have been studied in recent CPS researches to improve building energy efficiency by addressing these questions. In this paper, we comprehensively review and report on the contemporary applications of ML algorithms such as deep learning, transfer learning, active learning, reinforcement learning, and other emerging techniques that propose and envision to address the above challenges in the CPS building environment. Finally, we conclude this article by discussing diverse existing open questions and prospective future directions in the CPS building environment research. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Zahid Hasan and Nirmalya Roy},
  doi          = {10.1002/widm.1422},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1422},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Trending machine learning models in cyber-physical building environment: A survey},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal sentimental analysis for social media
applications: A comprehensive review. <em>WIDM</em>, <em>11</em>(5),
e1415. (<a href="https://doi.org/10.1002/widm.1415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of sentiments is essential in identifying and classifying opinions regarding a source material that is, a product or service. The analysis of these sentiments finds a variety of applications like product reviews, opinion polls, movie reviews on YouTube, news video analysis, and health care applications including stress and depression analysis. The traditional approach of sentiment analysis which is based on text involves the collection of large textual data and different algorithms to extract the sentiment information from it. But multimodal sentimental analysis provides methods to carry out opinion analysis based on the combination of video, audio, and text which goes a way beyond the conventional text-based sentimental analysis in understanding human behaviors. The remarkable increase in the use of social media provides a large collection of multimodal data that reflects the user&#39;s sentiment on certain aspects. This multimodal sentimental analysis approach helps in classifying the polarity (positive, negative, and neutral) of the individual sentiments. Our work aims to present a survey of recent developments in analyzing the multimodal sentiments (involving text, audio, and video/image) which involve human–machine interaction and challenges involved in analyzing them. A detailed survey on sentimental dataset, feature extraction algorithms, data fusion methods, and efficiency of different classification techniques are presented in this work. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ganesh Chandrasekaran and Tu N. Nguyen and Jude Hemanth D.},
  doi          = {10.1002/widm.1415},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1415},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Multimodal sentimental analysis for social media applications: A comprehensive review},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Big data analytics in single-cell transcriptomics: Five
grand opportunities. <em>WIDM</em>, <em>11</em>(4), e1414. (<a
href="https://doi.org/10.1002/widm.1414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-cell omics technologies provide biologists with a new dimension for systematically dissecting the underlying complexities within biological systems. These powerful technologies have triggered a wave of rapid development and deployment of new computational tools capable of teasing out critical insights by analysis of large volumes of omics data at single-cell resolution. Some of the key advancements include identifying molecular signatures imparting cellular identities, their evolutionary relationships, identifying novel and rare cell-types, and establishing a direct link between cellular genotypes and phenotypes. With the sharp increase in the throughput of single-cell platforms, the demand for efficient computational algorithms has become prominent. As such, devising novel computational strategies is critical to ensure optimal use of this wealth of molecular data for gaining newer insights into cellular biology. Here we discuss some of the grand opportunities of computational breakthroughs which would accelerate single-cell research. These are: predicting cellular identity, single-cell guided in silico drug screening for precision medicine, transfer learning methods to handle sparsity and heterogeneity of expression data, establishing genotype–phenotype relationships at single-cell resolution, and developing computational platforms for handling big data. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Namrata Bhattacharya and Colleen C. Nelson and Gaurav Ahuja and Debarka Sengupta},
  doi          = {10.1002/widm.1414},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1414},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Big data analytics in single-cell transcriptomics: Five grand opportunities},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating domain ontology information into clustering in
heterogeneous networks. <em>WIDM</em>, <em>11</em>(4), e1413. (<a
href="https://doi.org/10.1002/widm.1413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering of structure-rich heterogeneous information networks composed of multiple types of objects and relationships, which has become a challenge in data mining. Most of the existing clustering heterogeneous network methods focus on the internal information of the dataset while ignoring the domain knowledge outside the dataset. However, in real-world scenarios, domain knowledge can often offer valuable information for clustering. In this study, we propose a three-layer model OntoHeteClus, which is able to cluster multitype objects in star-structured heterogeneous networks by considering both the dataset itself and the background information quantified via the ontology. OntoHeteClus first evaluates the similarity between central objects according to formalized domain ontology information, based on which central objects are subsequently clustered. Finally, attribute objects are clustered according to the central object clustering result. A numerical example is presented to illustrate the modeling concept and working principle of the proposed method, and experiments on a real-world dataset demonstrate the effectiveness of the proposed algorithms. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Yue Huang},
  doi          = {10.1002/widm.1413},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1413},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Incorporating domain ontology information into clustering in heterogeneous networks},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic segmentation to characterize anthropometric
parameters and cardiovascular indicators in children. <em>WIDM</em>,
<em>11</em>(4), e1411. (<a
href="https://doi.org/10.1002/widm.1411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new predictive model to classify childhood obesity was implemented using machine learning techniques. The first step was to calculate the most relevant anthropomorphic and cardiovascular parameters of 187 children through principal component analysis (PCA) and cluster classification. Then Naïve-Bayes method classified these children into six groups using anthropometric Z Score, measurements of abdominal obesity, and arterial pressure: Group I (20.32% of total): composed mainly by accentuated malnutrition and malnutrition children; Group II (36.36%): composed primarily by eutrophic children; Group III (21.4%): constituted by eutrophic plus overweight children; Group IV (14.97%): comprised mainly by overweight and obese children; Group V (5.34%): Obese and overweight children; and Group VI (1.6%): obese at risk children. From Group II to VI, the proportion of pre-hypertensive and hypertensive children increased monotonically from 5 to 33%. This classification modes was tested on 66 children that were not originally included with a success rate of 97%. This predictive model will facilitate future longitudinal studies of obesity in children and will help plan interventions and evaluations of their results. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Daniel Gustavo Goroso and Alvaro Fraga and Michel Macedo and Carla Fernanda de Miranda Rodrigues and Bruno Mendes de Oliveira Silva and William Tsutomu Watanabe and Diego Pereira da Silva and Robson Rodrigues da Silva and José Luis Puglisi and James Marcin and Madan Dharmar},
  doi          = {10.1002/widm.1411},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1411},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Automatic segmentation to characterize anthropometric parameters and cardiovascular indicators in children},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A 2021 update on cancer image analytics with deep learning.
<em>WIDM</em>, <em>11</em>(4), e1410. (<a
href="https://doi.org/10.1002/widm.1410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Nikhil Cherian Kurian and Amit Sethi and Anil Reddy Konduru and Abhishek Mahajan and Swapnil Ulhas Rane},
  doi          = {10.1002/widm.1410},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1410},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A 2021 update on cancer image analytics with deep learning},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foundational ontologies, ontology-driven conceptual
modeling, and their multiple benefits to data mining. <em>WIDM</em>,
<em>11</em>(4), e1408. (<a
href="https://doi.org/10.1002/widm.1408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many years, the role played by domain knowledge in all stages of knowledge discovery has been recognized. However, the real-world semantics embedded in data is often still not fully considered in traditional data mining methods. In this article, we argue that the quality of data mining results is directly related to the extent that they reflect important properties of real-world entities represented therein. Analyzing and characterizing the nature of these entities is the very business of the area of formal ontology. We briefly elaborate on two particular types of artifacts produced by this area: foundational ontologies and ontology-driven conceptual modeling languages grounded on them. We then elaborate on the benefits they can bring to several activities in a data mining process. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Glenda Amaral and Fernanda Baião and Giancarlo Guizzardi},
  doi          = {10.1002/widm.1408},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1408},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Foundational ontologies, ontology-driven conceptual modeling, and their multiple benefits to data mining},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Table understanding approaches for extracting knowledge from
heterogeneous tables. <em>WIDM</em>, <em>11</em>(4), e1407. (<a
href="https://doi.org/10.1002/widm.1407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table understanding methods extract, transform, and interpret the information contained in tabular data embedded in documents/files of different formats. Such automatic understanding would allow to exploit tabular information with the aim of accurately answering queries, or integrating heterogeneous repositories of information in a common knowledge base, or exchanging information among different sources. The purpose of this survey is to provide a comprehensive analysis of the research efforts so far devoted to the problem of table understanding and to describe systems that support the transformation of heterogeneous tables into meaningful information. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sara Bonfitto and Elena Casiraghi and Marco Mesiti},
  doi          = {10.1002/widm.1407},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1407},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Table understanding approaches for extracting knowledge from heterogeneous tables},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data mining for energy systems: Review and prospect.
<em>WIDM</em>, <em>11</em>(4), e1406. (<a
href="https://doi.org/10.1002/widm.1406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An in-depth study on big data mining is urgently needed for the next-generation energy systems, which are characterized by a deep integration of cyber, physical, and social components. This paper presents an initial discussion on big data mining and its applications in intelligent energy systems. New progress in big data mining, such as deep learning, transfer learning, randomized learning, granular computing, and multisource data fusion, is introduced first. Some applications of data mining in energy systems, such as load forecasting and modeling, integrated power and transportation system, and electricity market forecasting and simulation, are discussed then. Moreover, some research problems in energy system data mining, such as cyber–physical–social system modeling and super-resolution perception for smart meter data, which require further attention in the future, are also discussed. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Wenxuan Liu and Junhua Zhao and Dianhui Wang},
  doi          = {10.1002/widm.1406},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1406},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data mining for energy systems: Review and prospect},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data stream analysis: Foundations, major tasks and tools.
<em>WIDM</em>, <em>11</em>(3), e1405. (<a
href="https://doi.org/10.1002/widm.1405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant growth of interconnected Internet-of-Things (IoT) devices, the use of social networks, along with the evolution of technology in different domains, lead to a rise in the volume of data generated continuously from multiple systems. Valuable information can be derived from these evolving data streams by applying machine learning. In practice, several critical issues emerge when extracting useful knowledge from these potentially infinite data, mainly because of their evolving nature and high arrival rate which implies an inability to store them entirely. In this work, we provide a comprehensive survey that discusses the research constraints and the current state-of-the-art in this vibrant framework. Moreover, we present an updated overview of the latest contributions proposed in different stream mining tasks, particularly classification , regression , clustering , and frequent patterns . This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Maroua Bahri and Albert Bifet and João Gama and Heitor Murilo Gomes and Silviu Maniu},
  doi          = {10.1002/widm.1405},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1405},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data stream analysis: Foundations, major tasks and tools},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series analysis via network science: Concepts and
algorithms. <em>WIDM</em>, <em>11</em>(3), e1404. (<a
href="https://doi.org/10.1002/widm.1404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is nowadays a constant flux of data being generated and collected in all types of real world systems. These data sets are often indexed by time, space, or both requiring appropriate approaches to analyze the data. In univariate settings, time series analysis is a mature field. However, in multivariate contexts, time series analysis still presents many limitations. In order to address these issues, the last decade has brought approaches based on network science. These methods involve transforming an initial time series data set into one or more networks, which can be analyzed in depth to provide insight into the original time series. This review provides a comprehensive overview of existing mapping methods for transforming time series into networks for a wide audience of researchers and practitioners in machine learning, data mining, and time series. Our main contribution is a structured review of existing methodologies, identifying their main characteristics, and their differences. We describe the main conceptual approaches, provide authoritative references and give insight into their advantages and limitations in a unified way and language. We first describe the case of univariate time series, which can be mapped to single layer networks, and we divide the current mappings based on the underlying concept: visibility, transition, and proximity. We then proceed with multivariate time series discussing both single layer and multiple layer approaches. Although still very recent, this research area has much potential and with this survey we intend to pave the way for future research on the topic. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Vanessa Freitas Silva and Maria Eduarda Silva and Pedro Ribeiro and Fernando Silva},
  doi          = {10.1002/widm.1404},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1404},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Time series analysis via network science: Concepts and algorithms},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Review on publicly available datasets for educational data
mining. <em>WIDM</em>, <em>11</em>(3), e1403. (<a
href="https://doi.org/10.1002/widm.1403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of a dataset represents a critical component in educational data mining (EDM) pipelines. Once the dataset is at hand, the next steps within the research methodology regard proper research issue formulation, data analysis pipeline design and implementation and, finally, presentation of validation results. As the EDM research area is continuously growing due to the increasing number of available tools and technologies, one of the critical issues that constitute a bottleneck regards a properly documented review on publicly available datasets. This paper aims to present a succinct, yet informative, description of the most used publicly available data sources along with their associated EDM tasks, used algorithms, experimental results and main findings. We have found that there are three types of data sources: well-known data sources, datasets used in EDM competitions and standalone EDM datasets. We conclude that the success of the future of EDM data sources will rely on their ability to manage proposed approaches and their experimental results as a dashboard of benchmarked runs. Under these circumstances, the reproducibility of data analysis pipelines and benchmarking of proposed algorithms becomes at hand for the research community such that progress in the EDM domain may be much more easily acquired. The most crucial outcome regards the possibility of continuously improving existing data analysis pipelines by tackling EDM tasks that rely on publicly available datasets and benchmarking data analysis pipelines that use open-source implementations. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Marian Cristian Mihaescu and Paul Stefan Popescu},
  doi          = {10.1002/widm.1403},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1403},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Review on publicly available datasets for educational data mining},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy rough sets and fuzzy rough neural networks for feature
selection: A review. <em>WIDM</em>, <em>11</em>(3), e1402. (<a
href="https://doi.org/10.1002/widm.1402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to select a feature subset from an original feature set based on a certain evaluation criterion. Since feature selection can achieve efficient feature reduction, it has become a key method for data preprocessing in many data mining tasks. Recently, many feature selection strategies have been developed since in most cases it is infeasible to obtain an optimal/reduced feature subset by using exhaustive search. Among these strategies, fuzzy rough set theory has proved to be an ideal candidate for dealing with uncertain information. This article provides a comprehensive review on the fuzzy rough set theory and two fuzzy rough set theory based feature selection methods, that is, fuzzy rough set based feature selection methods and fuzzy rough neural network based feature selection methods. We review the publications related to the fuzzy rough theory and its applications in feature selection. In addition, the challenges in the two types of feature selection methods are also discussed. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Wanting Ji and Yan Pang and Xiaoyun Jia and Zhongwei Wang and Feng Hou and Baoyan Song and Mingzhe Liu and Ruili Wang},
  doi          = {10.1002/widm.1402},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1402},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Fuzzy rough sets and fuzzy rough neural networks for feature selection: A review},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the ratings of amazon products using big data.
<em>WIDM</em>, <em>11</em>(3), e1400. (<a
href="https://doi.org/10.1002/widm.1400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to apply several machine learning (ML) models to the massive dataset present in the area of e-commerce from Amazon to analyze and predict ratings and to recommend products. For this purpose, we have used both traditional and Big Data algorithms. As the Amazon product review dataset is large, we present Big Data architecture suitable massive dataset for storing and computation, which is not possible with the traditional architecture. Furthermore, the dataset contains 15 attributes and has about 7 million records. With the dataset, we develop several models in Oracle Big Data and Azure Cloud Computing services to predict the review rating and recommendation for the items at Amazon. We present a comparative conclusion in terms of the accuracy as well as the efficiency with Spark ML—the Big Data architecture, and Azure ML—the traditional architecture. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jongwook Woo and Monika Mishra},
  doi          = {10.1002/widm.1400},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1400},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Predicting the ratings of amazon products using big data},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy preserving classification over differentially
private data. <em>WIDM</em>, <em>11</em>(3), e1399. (<a
href="https://doi.org/10.1002/widm.1399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy preserving data classification is an important research area in data mining field. The goal of a privacy preserving classification algorithm is to protect the sensitive information as much as possible, while providing satisfactory classification accuracy. Differential privacy is a strong privacy guarantee that enables privacy of sensitive data stored in a database by determining the ratio of sensitive information leakage with respect to an ɛ parameter. In this study, our aim is to investigate the classification performance of the state-of-the-art classification algorithms such as C4.5, Naïve Bayes, One Rule, Bayesian Networks, PART, Ripper, K*, IBk, and Random tree for performing privacy preserving classification. To preserve privacy of the data to be classified, we applied input perturbation technique coming from differential privacy, and observed the relationship between the ɛ parameter values and accuracy of the classifiers. To our best knowledge, this article is the first study that analyzes the performances of the well-known classification algorithms over differentially private data, and discovers which datasets are more suitable for privacy preserving classification when input perturbation is applied to provide data privacy. The classification algorithms are compared by using the differentially private versions of the well-known datasets from the UCI repository. According to the experimental results, we observed that, as ɛ parameter value increases, better classification accuracies are achieved with lower privacy levels. When the classifiers are compared, Naïve Bayes classifier is the most successful method. The ɛ parameter should be greater than or equal to 2 (i.e., ɛ ≥2) to achieve cloud server is malicious and untrusted, sensitive data will satisfactory classification accuracies. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ezgi Zorarpacı and Selma Ayşe Özel},
  doi          = {10.1002/widm.1399},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1399},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Privacy preserving classification over differentially private data},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to neural network-based question answering over
knowledge graphs. <em>WIDM</em>, <em>11</em>(3), e1389. (<a
href="https://doi.org/10.1002/widm.1389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering has emerged as an intuitive way of querying structured data sources and has attracted significant advancements over the years. A large body of recent work on question answering over knowledge graphs (KGQA) employs neural network-based systems. In this article, we provide an overview of these neural network-based methods for KGQA. We introduce readers to the formalism and the challenges of the task, different paradigms and approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point to semantic parsing for KGQA, and ease their process of making informed decisions while creating their own QA systems. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Nilesh Chakraborty and Denis Lukovnikov and Gaurav Maheshwari and Priyansh Trivedi and Jens Lehmann and Asja Fischer},
  doi          = {10.1002/widm.1389},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1389},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Introduction to neural network-based question answering over knowledge graphs},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive review on updating concept lattices and its
application in updating association rules. <em>WIDM</em>,
<em>11</em>(2), e1401. (<a
href="https://doi.org/10.1002/widm.1401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal concept analysis (FCA) visualizes formal concepts in terms of a concept lattice. Usually, it is an NP-problem and consumes plenty of time and storage space to update the changes of the lattice. Thus, introducing an efficient way to update and maintain such lattices is a significant area of interest within the field of FCA and its applications. One of those vital FCA applications is the association rule mining (ARM), which aims at generating a loss-less nonredundant compact Association Rule-basis (AR-basis). Currently, the real-world data rapidly overgrow that asks the need for updating the existing concept lattice and AR-basis upon data change continually. Intuitively, updating and maintaining an existing concept-lattice or AR-basis is much more efficient and consistent than reconstructing them from scratch, particularly in the case of massive data. So far, the area of updating both concept lattice and AR-basis has not received much attention. Besides, few noncomprehensive studies have focused only on updating the concept lattice. From this point, this article comprehensively introduces basic knowledge regarding updating both concept lattices and AR-basis with new illustrations, formalization, and examples. Also, the article reviews and compares recent remarkable works and explores the emerging future research trends. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ebtesam Shemis and Ammar Mohammed},
  doi          = {10.1002/widm.1401},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1401},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A comprehensive review on updating concept lattices and its application in updating association rules},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive analysis of real-time strategy games: A graph
mining approach. <em>WIDM</em>, <em>11</em>(2), e1398. (<a
href="https://doi.org/10.1002/widm.1398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and computational intelligence have facilitated the development of recommendation systems for a broad range of domains. Such recommendations are based on contextual information that is explicitly provided or pervasively collected. Recommendation systems often improve decision-making or increase the efficacy of a task. Real-time strategy (RTS) video games are not only a popular entertainment medium, they also are an abstraction of many real-world applications where the aim is to increase your resources and decrease those of your opponent. Using predictive analytics, which examines past examples of success and failure, we can learn how to predict positive outcomes for such scenarios. The goal of our research is to develop an accurate predictive recommendation system for multiplayer strategic games to determine recommendations for moves that a player should, and should not, make and thereby provide a competitive advantage. Herein we compare two techniques, frequent and discriminative subgraph mining, in terms of the error rates associated with their predictions in this context. As proof of concept, we present the results of an experiment that utilizes our strategies for two particular RTS games. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Isam A. Alobaidi and Jennifer L. Leopold and Ali A. Allami and Nathan W. Eloe and Dustin Tanksley},
  doi          = {10.1002/widm.1398},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1398},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Predictive analysis of real-time strategy games: A graph mining approach},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smartphones for public transport planning and recommendation
in developing countries—a review. <em>WIDM</em>, <em>11</em>(2), e1397.
(<a href="https://doi.org/10.1002/widm.1397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this era of connected systems that have penetrated everywhere, transport units have become a significant source of data, collected from commuters, vehicles, drivers, or any section being touched by the transport system. These data, which have both spatial as well as temporal aspects, is utilized for a plethora of services like travel assistant systems, multi-modal transport solutions, real-time travel information, smart parking, autonomous vehicles, to name a few. With the current buzz of sustainable transport, the use of public transport systems have been popularized owing to the economic and environmental savings. In this review article, we provide a highlight of works, which have tried to utilize techniques to improve multiple sections of the public transport system, primarily focusing on developing economies, thus improving the overall commute experience at various countries. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Rohit Verma and Sandip Chakraborty},
  doi          = {10.1002/widm.1397},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1397},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Smartphones for public transport planning and recommendation in developing countries—A review},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scholarly data mining: A systematic review of its
applications. <em>WIDM</em>, <em>11</em>(2), e1395. (<a
href="https://doi.org/10.1002/widm.1395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the last few decades, the widespread growth of scholarly networks and digital libraries has resulted in an explosion of publicly available scholarly data in various forms such as authors, papers, citations, conferences, and journals. This has created interest in the domain of big scholarly data analysis that analyses worldwide dissemination of scientific findings from different perspectives. Although the study of big scholarly data is relatively new, some studies have emerged on how to investigate scholarly data usage in different disciplines. These studies motivate investigating the scholarly data generated via academic technologies such as scholarly networks and digital libraries for building scalable approaches for retrieving, recommending, and analyzing the scholarly content. We have analyzed these studies following a systematic methodology, classifying them into different applications based on literature features and highlighting the machine learning techniques used for this purpose. We also discuss open challenges that remain unsolved to foster future research in the field of scholarly data mining. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Amna Dridi and Mohamed Medhat Gaber and R. Muhammad Atif Azad and Jagdev Bhogal},
  doi          = {10.1002/widm.1395},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1395},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Scholarly data mining: A systematic review of its applications},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamical algorithms for data mining and machine learning
over dynamic graphs. <em>WIDM</em>, <em>11</em>(2), e1393. (<a
href="https://doi.org/10.1002/widm.1393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many modern applications, the generated data is a dynamic network . These networks are graphs that change over time by a sequence of update operations (node addition, node deletion, edge addition, edge deletion, and edge weight change). In such networks, it is inefficient to compute from scratch the solution of a data mining/machine learning task, after any update operation. Therefore in recent years, several so-called dynamical algorithms have been proposed that update the solution, instead of computing it from scratch. In this paper, first we formulate this emerging setting and discuss its high-level algorithmic aspects. Then, we review state of the art dynamical algorithms proposed for several data mining and machine learning tasks, including frequent pattern discovery, betweenness/closeness/PageRank centralities, clustering, classification, and regression. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mostafa Haghir Chehreghani},
  doi          = {10.1002/widm.1393},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1393},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Dynamical algorithms for data mining and machine learning over dynamic graphs},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction model for recurrence of hepatocellular carcinoma
after resection by using neighbor2vec based algorithms. <em>WIDM</em>,
<em>11</em>(2), e1390. (<a
href="https://doi.org/10.1002/widm.1390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver cancer has become the third cause that leads to the cancer death. For hepatocellular carcinoma (HCC), as the highly malignant type of liver cancer, its recurrence rate after operation is still very high because there is no reliable clinical data to provide better advice for patients after operation. To solve the challenging issue, in this work, we design a novel prediction model for recurrence of HCC using neighbor2vec based algorithm. It consists of three stages: (a) In the preparation stage, the Pearson correlation coefficient was used to explore the independent predictors of HCC recurrence, (b) due to the low correlation between individual dimension and prediction target, K-nearest neighbors (KNN) were found as a K -vectors list for each patient (neighbor2vec), (c) all vectors lists were applied as the input of machine learning methods such as logistic regression, KNN, decision tree, naive Bayes (NB), and deep neural network to establish the neighbor2vec based prediction model. From the experimental results on the real data from Shandong Provincial Hospital in China, the proposed neighbor2vec based prediction model outperforms all the other models. Especially, the NB model with neighbor2vec achieves up to 83.02, 82.86, 77.6%, in terms of accuracy, recall rates, and precision. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Yuankui Cao and Junqing Fan and Hong Cao and Yunliang Chen and Jie Li and Jianxin Li and Simin Zhang},
  doi          = {10.1002/widm.1390},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1390},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Prediction model for recurrence of hepatocellular carcinoma after resection by using neighbor2vec based algorithms},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting disease-associated genes: Computational methods,
databases, and evaluations. <em>WIDM</em>, <em>11</em>(2), e1383. (<a
href="https://doi.org/10.1002/widm.1383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex diseases are associated with a set of genes (called disease genes), the identification of which can help scientists uncover the mechanisms of diseases and develop new drugs and treatment strategies. Due to the huge cost and time of experimental identification techniques, many computational algorithms have been proposed to predict disease genes. Although several review publications in recent years have discussed many computational methods, some of them focus on cancer driver genes while others focus on biomolecular networks, which only cover a specific aspect of existing methods. In this review, we summarize existing methods and classify them into three categories based on their rationales. Then, the algorithms, biological data, and evaluation methods used in the computational prediction are discussed. Finally, we highlight the limitations of existing methods and point out some future directions for improving these algorithms. This review could help investigators understand the principles of existing methods, and thus develop new methods to advance the computational prediction of disease genes. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ping Luo and Bolin Chen and Bo Liao and Fang-Xiang Wu},
  doi          = {10.1002/widm.1383},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1383},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Predicting disease-associated genes: Computational methods, databases, and evaluations},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting land surface temperature with geographically
weighed regression and deep learning. <em>WIDM</em>, <em>11</em>(1),
e1396. (<a href="https://doi.org/10.1002/widm.1396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For prediction of urban remote sensing surface temperature, cloud, cloud shadow and snow contamination lead to the failure of surface temperature inversion and vegetation-related index calculation. A time series prediction framework of urban surface temperature under cloud interference is proposed in this paper. This is helpful to solve the problem of the impact of data loss on surface temperature prediction. Spatial and temporal variation trends of surface temperature and vegetation index are analyzed using Landsat 7/8 remote sensing data of 2010 to 2019 from Beijing. The geographically weighed regression (GWR) method is used to realize the simulation of surface temperature based on the current date. The deep learning prediction network based on convolution and long short-term memory (LSTM) networks was constructed to predict the spatial distribution of surface temperature on the next observation date. The time series analysis shows that the NDBI is less than −0.2, which indicates that there may be cloud contamination. The land surface temperature (LST) modeling results show that the precision of estimation using GWR method on impervious surface and water bodies is superior compared to the vegetation area. For LST prediction using deep learning methods, the result of the prediction on surface temperature space distribution was relatively good. The purpose of this study is to make up for the missing data affected by cloud, snow, and other interference factors, and to be applied to the prediction of the spatial and temporal distributions of LST. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Hongfei Jia and Dehe Yang and Weiping Deng and Qing Wei and Wenliang Jiang},
  doi          = {10.1002/widm.1396},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1396},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Predicting land surface temperature with geographically weighed regression and deep learning},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of biodiversity informatics: Concepts, practices,
and challenges. <em>WIDM</em>, <em>11</em>(1), e1394. (<a
href="https://doi.org/10.1002/widm.1394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented size of the human population, along with its associated economic activities, has an ever-increasing impact on global environments. Across the world, countries are concerned about the growing resource consumption and the capacity of ecosystems to provide resources. To effectively conserve biodiversity, it is essential to make indicators and knowledge openly available to decision-makers in ways that they can effectively use them. The development and deployment of tools and techniques to generate these indicators require having access to trustworthy data from biological collections, field surveys and automated sensors, molecular data, and historic academic literature. The transformation of these raw data into synthesized information that is fit for use requires going through many refinement steps. The methodologies and techniques applied to manage and analyze these data constitute an area usually called biodiversity informatics . Biodiversity data follow a life cycle consisting of planning, collection, certification, description, preservation, discovery, integration, and analysis. Researchers, whether producers or consumers of biodiversity data, will likely perform activities related to at least one of these steps. This article explores each stage of the life cycle of biodiversity data, discussing its methodologies, tools, and challenges. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Luiz M. R. Gadelha and Pedro C. de Siracusa and Eduardo Couto Dalcin and Luís Alexandre Estevão da Silva and Douglas A. Augusto and Eduardo Krempser and Helen Michelle Affe and Raquel Lopes Costa and Maria Luiza Mondelli and Pedro Milet Meirelles and Fabiano Thompson and Marcia Chame and Artur Ziviani and Marinez Ferreira de Siqueira},
  doi          = {10.1002/widm.1394},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1394},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of biodiversity informatics: Concepts, practices, and challenges},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data mining privacy preserving: Research agenda.
<em>WIDM</em>, <em>11</em>(1), e1392. (<a
href="https://doi.org/10.1002/widm.1392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern days, the amount of the data and information is increasing along with their accessibility and availability, due to the Internet and social media. To be able to search this vast data set and to discover unknown useful data patterns and predictions, the data mining method is used. Data mining allows for unrelated data to be connected in a meaningful way, to analyze the data, and to represent the results in the form of useful data patterns and predictions that help and predict future behavior. The process of data mining can potentially violate sensitive and personal data. Individual privacy is under attack if some of the information leaks and reveals the identity of a person whose personal data were used in the data mining process. There are many privacy-preserving data mining (PPDM) techniques and methods that have a task to preserve the privacy and sensitive data while providing accurate data mining results at the same time. PPDM techniques and methods incorporate different approaches that protect data in the process of data mining. The methodology that was used in this article is the systematic literature review and bibliometric analysis. This article identifieds the current trends, techniques, and methods that are being used in the privacy-preserving data mining field to make a clear and concise classification of the PPDM methods and techniques with possibly identifying new methods and techniques that were not included in the previous classification, and to emphasize the future research directions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Inda Kreso and Amra Kapo and Lejla Turulja},
  doi          = {10.1002/widm.1392},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1392},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data mining privacy preserving: Research agenda},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A historical perspective of explainable artificial
intelligence. <em>WIDM</em>, <em>11</em>(1), e1391. (<a
href="https://doi.org/10.1002/widm.1391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision-making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge-based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural-symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human-understandable explainable systems. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Roberto Confalonieri and Ludovik Coba and Benedikt Wagner and Tarek R. Besold},
  doi          = {10.1002/widm.1391},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1391},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A historical perspective of explainable artificial intelligence},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy preserving big data analytics: A critical analysis
of state-of-the-art. <em>WIDM</em>, <em>11</em>(1), e1387. (<a
href="https://doi.org/10.1002/widm.1387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of “big data,” a huge number of people, devices, and sensors are connected via digital networks and the cross-plays among these entities generate enormous valuable data that facilitate organizations to innovate and grow. However, the data deluge also raises serious privacy concerns which may cause a regulatory backlash and hinder further organizational innovation. To address the challenge of information privacy, researchers have explored privacy-preserving methodologies in the past two decades. However, a thorough study of privacy preserving big data analytics is missing in existing literature. The main contributions of this article include a systematic evaluation of various privacy preservation approaches and a critical analysis of the state-of-the-art privacy preserving big data analytics methodologies. More specifically, we propose a four-dimensional framework for analyzing and designing the next generation of privacy preserving big data analytics approaches. Besides, we contribute to pinpoint the potential opportunities and challenges of applying privacy preserving big data analytics to business settings. We provide five recommendations of effectively applying privacy-preserving big data analytics to businesses. To the best of our knowledge, this is the first systematic study about state-of-the-art in privacy-preserving big data analytics. The managerial implication of our study is that organizations can apply the results of our critical analysis to strengthen their strategic deployment of big data analytics in business settings, and hence to better leverage big data for sustainable organizational innovation and growth. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {M. Ileas Pramanik and Raymond Y. K. Lau and Md Sakir Hossain and Md Mizanur Rahoman and Sumon Kumar Debnath and Md Golam Rashed and Md Zasim Uddin},
  doi          = {10.1002/widm.1387},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1387},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Privacy preserving big data analytics: A critical analysis of state-of-the-art},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of digital circuit testing in the light of machine
learning. <em>WIDM</em>, <em>11</em>(1), e1360. (<a
href="https://doi.org/10.1002/widm.1360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The insistent trend in today&#39;s nanoscale technology, to keep abreast of the Moore&#39;s law, has been continually opening up newer challenges to circuit designers. With rapid downscaling of integration, the intricacies involved in the manufacturing process have escalated significantly. Concomitantly, the nature of defects in silicon chips has become more complex and unpredictable, adding further difficulty in circuit testing and diagnosis. The volume of test data has surged and the parameters that govern testing of integrated circuits have increased not only in dimension but also in the complexity of their correlation. Evidently, the current scenario serves as a pertinent platform to explore new test solutions based on machine learning. In this survey, we look at various recent advances in this evolving domain in the context of digital logic testing and diagnosis. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Manjari Pradhan and Bhargab B. Bhattacharya},
  doi          = {10.1002/widm.1360},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1360},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of digital circuit testing in the light of machine learning},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Behavioral fingerprinting of internet-of-things devices.
<em>WIDM</em>, <em>11</em>(1), e1337. (<a
href="https://doi.org/10.1002/widm.1337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advances in the Internet-of-Things (IoT) domain have led to the development of several useful and interesting devices that have enhanced the quality of home living and industrial automation. The vulnerabilities in the IoT devices have rendered them susceptible to compromise and forgery. The problem of device authentication, that is, the question of whether a device&#39;s identity is what it claims to be, is still an open problem. Device fingerprinting seems to be a promising authentication mechanism. Device fingerprinting profiles a device based on information available about the device and generate a robust, verifiable and unique identity for the device. Existing approaches for device fingerprinting may not be feasible or cost-effective for the IoT domain due to the resource constraints and heterogeneity of the IoT devices. Due to resource and cost constraints, behavioral fingerprinting provides promising directions for fingerprinting IoT devices. Behavioral fingerprinting allows security researchers to understand the behavioral profile of a device and to establish some guidelines regarding the device operations. In this article, we discuss existing approaches for behavioral fingerprinting of devices in general and evaluate their applicability for IoT devices. Furthermore, we discuss potential approaches for fingerprinting IoT devices and give an overview of some of the preliminary attempts to fingerprint IoT devices. We conclude by highlighting the future research directions for fingerprinting in the IoT domain. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Bruhadeshwar Bezawada and Indrakshi Ray and Indrajit Ray},
  doi          = {10.1002/widm.1337},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1337},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Behavioral fingerprinting of internet-of-things devices},
  volume       = {11},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
