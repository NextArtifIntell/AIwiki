<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---426">SIM - 426</h2>
<ul>
<li><details>
<summary>
(2021). Adaptively stacking ensembles for influenza forecasting.
<em>SIM</em>, <em>40</em>(30), 6931–6952. (<a
href="https://doi.org/10.1002/sim.9219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seasonal influenza infects between 10 and 50 million people in the United States every year. Accurate forecasts of influenza and influenza-like illness (ILI) have been named by the CDC as an important tool to fight the damaging effects of these epidemics. Multi-model ensembles make accurate forecasts of seasonal influenza, but current operational ensemble forecasts are static: they require an abundance of past ILI data and assign fixed weights to component models at the beginning of a season, but do not update weights as new data on component model performance is collected. We propose an adaptive ensemble that (i) does not initially need data to combine forecasts and (ii) finds optimal weights which are updated week-by-week throughout the influenza season. We take a regularized likelihood approach and investigate this regularizer&#39;s ability to impact adaptive ensemble performance. After finding an optimal regularization value, we compare our adaptive ensemble to an equal-weighted and static ensemble. Applied to forecasts of short-term ILI incidence at the regional and national level, our adaptive model outperforms an equal-weighted ensemble and has similar performance to the static ensemble using only a fraction of the data available to the static ensemble. Needing no data at the beginning of an epidemic, an adaptive ensemble can quickly train and forecast an outbreak, providing a practical tool to public health officials looking for a forecast to conform to unique features of a specific season.},
  archive      = {J_SIM},
  author       = {Thomas McAndrew and Nicholas G. Reich},
  doi          = {10.1002/sim.9219},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6931-6952},
  shortjournal = {Stat. Med.},
  title        = {Adaptively stacking ensembles for influenza forecasting},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric regression analysis of clustered
interval-censored failure time data with a cured subgroup. <em>SIM</em>,
<em>40</em>(30), 6918–6930. (<a
href="https://doi.org/10.1002/sim.9218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses regression analysis of clustered interval-censored failure time data in the presence of a cured fraction or subgroup. Such data often occur in many areas, including epidemiological studies, medical studies, and social sciences. For the problem, a class of semiparametric transformation nonmixture cure models is presented and for estimation, the maximum likelihood estimation procedure is derived. For the implementation of the proposed method, we develop a novel EM algorithm based on a Poisson variable-based augmentation. An extensive simulation study is conducted and suggests that the proposed approach works well in practical situations. Finally the method is applied to an example that motivated this study.},
  archive      = {J_SIM},
  author       = {Dian Yang and Mingyue Du and Jianguo Sun},
  doi          = {10.1002/sim.9218},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6918-6930},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric regression analysis of clustered interval-censored failure time data with a cured subgroup},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the optimal timing of surgery by imputing
potential outcomes. <em>SIM</em>, <em>40</em>(30), 6900–6917. (<a
href="https://doi.org/10.1002/sim.9217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypoplastic left heart syndrome is a congenital anomaly that is uniformly fatal in infancy without immediate treatment. The standard treatment consists of an initial Norwood procedure (stage 1) followed some months later by stage 2 palliation (S2P). The ideal timing of the S2P is uncertain. The Single Ventricle Reconstruction Trial (SVRT) randomized the procedure used in the initial Norwood operation, leaving the timing of the S2P to the discretion of the surgical team. To estimate the causal effect of the timing of S2P, we propose to impute the potential post-S2P survival outcomes using statistical models under the Rubin Causal Model framework. With this approach, it is straightforward to estimate the causal effect of S2P timing on post-S2P survival by directly comparing the imputed potential outcomes. Specifically, we consider a lognormal model and a restricted cubic spline model, evaluating their performance in Monte Carlo studies. When applied to the SVRT data, the models give somewhat different imputed values, but both support the conclusion that the optimal time for the S2P is at 6 months after the Norwood procedure.},
  archive      = {J_SIM},
  author       = {Xiaofei Chen and Daniel F. Heitjan and Gerald Greil and Haekyung Jeon-Slaughter},
  doi          = {10.1002/sim.9217},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6900-6917},
  shortjournal = {Stat. Med.},
  title        = {Estimating the optimal timing of surgery by imputing potential outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Censoring-robust time-dependent receiver operating
characteristic curve estimators. <em>SIM</em>, <em>40</em>(30),
6885–6899. (<a href="https://doi.org/10.1002/sim.9216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-dependent receiver operating characteristic curves are often used to evaluate the classification performance of continuous measures when considering time-to-event data. When one is interested in evaluating the predictive performance of multiple covariates, it is common to use the Cox proportional hazards model to obtain risk scores; however, previous work has shown that when the model is mis-specified, the estimand corresponding to the partial likelihood estimator depends on the censoring distribution. In this manuscript, we show that when the risk score model is mis-specified, the AUC will also depend on the censoring distribution, leading to either over- or under-estimation of the risk score&#39;s predictive performance. We propose the use of censoring-robust estimators to remove the dependence on the censoring distribution and provide empirical results supporting the use of censoring-robust risk scores.},
  archive      = {J_SIM},
  author       = {Michelle M. Nuño and Daniel L. Gillen},
  doi          = {10.1002/sim.9216},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6885-6899},
  shortjournal = {Stat. Med.},
  title        = {Censoring-robust time-dependent receiver operating characteristic curve estimators},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing the sensitivities of two screening tests in
nonblinded randomized paired screen-positive trials with differential
screening uptake. <em>SIM</em>, <em>40</em>(30), 6873–6884. (<a
href="https://doi.org/10.1002/sim.9215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before a new screening test can be used in routine screening, its performance needs to be compared to the standard screening test. This comparison is generally done in population screening trials with a screen-positive design where participants undergo one or both screening tests after which disease verification takes place for those positive on at least one screening test. We consider the randomized paired screen-positive design of Alonzo and Kittelson where participants are randomized to receive one of the two screening tests and only participants with a positive screening test subsequently receive the other screening test followed by disease verification. The tests are usually offered in an unblinded fashion in which case the screening uptake may differ between arms, in particular when one test is more burdensome than the other. When uptake is associated with disease, the estimator for the relative sensitivity derived by Alonzo and Kittelson may be biased and the type I error of the associated statistical test is no longer guaranteed to be controlled. We present methods for comparing sensitivities of screening tests in randomized paired screen-positive trials that are robust to differential screening uptake. In a simulation study, we show that our methods adequately control the type I error when screening uptake is associated with disease. We apply the developed methods to data from the IMPROVE trial, a nonblinded cervical cancer screening trial comparing the accuracy of HPV testing on self-collected versus provider-collected samples. In this trial, screening uptake was higher among participants randomized to self-collection.},
  archive      = {J_SIM},
  author       = {Peter M. van de Ven and Andrea Bassi and Johannes Berkhof},
  doi          = {10.1002/sim.9215},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6873-6884},
  shortjournal = {Stat. Med.},
  title        = {Comparing the sensitivities of two screening tests in nonblinded randomized paired screen-positive trials with differential screening uptake},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian inference and dynamic prediction of multivariate
joint model with functional data: An application to alzheimer’s disease.
<em>SIM</em>, <em>40</em>(30), 6855–6872. (<a
href="https://doi.org/10.1002/sim.9214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is a severe neurodegenerative disorder impairing multiple domains, for example, cognition and behavior. Assessing the risk of AD progression and initiating timely interventions at early stages are critical to improve the quality of life for AD patients. Due to the heterogeneous nature and complex mechanisms of AD, one single longitudinal outcome is insufficient to assess AD severity and disease progression. Therefore, AD studies collect multiple longitudinal outcomes, including cognitive and behavioral measurements, as well as structural brain images such as magnetic resonance imaging (MRI). How to utilize the multivariate longitudinal outcomes and MRI data to make efficient statistical inference and prediction is an open question. In this article, we propose a multivariate joint model with functional data (MJM-FD) framework that relates multiple correlated longitudinal outcomes to a survival outcome, and use the scalar-on-function regression method to include voxel-based whole-brain MRI data as functional predictors in both longitudinal and survival models. We adopt a Bayesian paradigm to make statistical inference and develop a dynamic prediction framework to predict an individual&#39;s future longitudinal outcomes and risk of a survival event. We validate the MJM-FD framework through extensive simulation studies and apply it to the motivating Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) study.},
  archive      = {J_SIM},
  author       = {Haotian Zou and Kan Li and Donglin Zeng and Sheng Luo},
  doi          = {10.1002/sim.9214},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6855-6872},
  shortjournal = {Stat. Med.},
  title        = {Bayesian inference and dynamic prediction of multivariate joint model with functional data: An application to alzheimer&#39;s disease},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust approach for variable selection with high dimensional
longitudinal data analysis. <em>SIM</em>, <em>40</em>(30), 6835–6854.
(<a href="https://doi.org/10.1002/sim.9213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new robust smooth-threshold estimating equation to select important variables and automatically estimate parameters for high dimensional longitudinal data. A novel working correlation matrix is proposed to capture correlations within the same subject. The proposed procedure works well when the number of covariates increases as the number of subjects n increases. The proposed estimates are competitive with the estimates obtained with the true correlation structure, especially when the data are contaminated. Moreover, the proposed method is robust against outliers in the response variables and/or covariates. Furthermore, the oracle properties for robust smooth-threshold estimating equations under “large n , diverging ” are established under some regularity conditions. Extensive simulation studies and a yeast cell cycle data are used to evaluate the performance of the proposed method, and results show that the proposed method is competitive with existing robust variable selection procedures.},
  archive      = {J_SIM},
  author       = {Liya Fu and Jiaqi Li and You-Gan Wang},
  doi          = {10.1002/sim.9213},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6835-6854},
  shortjournal = {Stat. Med.},
  title        = {Robust approach for variable selection with high dimensional longitudinal data analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust group variable screening based on maximum
lq-likelihood estimation. <em>SIM</em>, <em>40</em>(30), 6818–6834. (<a
href="https://doi.org/10.1002/sim.9212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable screening plays an important role in ultra-high-dimensional data analysis. Most of the previous analyses have focused on individual predictor screening using marginal correlation or other rank-based techniques. When predictors can be naturally grouped, the structure information should be incorporated while applying variable screening. This study presents a group screening procedure that is based on maximum Lq-likelihood estimation, which is being increasingly used for robust estimation. The proposed method is robust against data contamination, including a heavy-tailed distribution of the response and a mixture of observations from different distributions. The sure screening property is rigorously established. Simulations demonstrate the competitive performance of the proposed method, especially in terms of its robustness against data contamination. Two real data analyses are presented to further illustrate its performance.},
  archive      = {J_SIM},
  author       = {Yang Li and Rong Li and Yichen Qin and Cunjie Lin and Yuhong Yang},
  doi          = {10.1002/sim.9212},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6818-6834},
  shortjournal = {Stat. Med.},
  title        = {Robust group variable screening based on maximum lq-likelihood estimation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-phase sample selection strategies for design and
analysis in post-genome-wide association fine-mapping studies.
<em>SIM</em>, <em>40</em>(30), 6792–6817. (<a
href="https://doi.org/10.1002/sim.9211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-GWAS analysis, in many cases, focuses on fine-mapping targeted genetic regions discovered at GWAS-stage; that is, the aim is to pinpoint potential causal variants and susceptibility genes for complex traits and disease outcomes using next-generation sequencing (NGS) technologies. Large-scale GWAS cohorts are necessary to identify target regions given the typically modest genetic effect sizes. In this context, two-phase sampling design and analysis is a cost-reduction technique that utilizes data collected during phase 1 GWAS to select an informative subsample for phase 2 sequencing. The main goal is to make inference for genetic variants measured via NGS by efficiently combining data from phases 1 and 2. We propose two approaches for selecting a phase 2 design under a budget constraint. The first method identifies sampling fractions that select a phase 2 design yielding an asymptotic variance covariance matrix with certain optimal characteristics, for example, smallest trace, via Lagrange multipliers (LM). The second relies on a genetic algorithm (GA) with a defined fitness function to identify exactly a phase 2 subsample. We perform comprehensive simulation studies to evaluate the empirical properties of the proposed designs for a genetic association study of a quantitative trait. We compare our methods against two ranked designs: residual-dependent sampling and a recently identified optimal design. Our findings demonstrate that the proposed designs, GA in particular, can render competitive power in combined phase 1 and 2 analysis compared with alternative designs while preserving type 1 error control. These results are especially evident under the more practical scenario where design values need to be defined a priori and are subject to misspecification. We illustrate the proposed methods in a study of triglyceride levels in the North Finland Birth Cohort of 1966. R code to reproduce our results is available at github.com/egosv/TwoPhase_postGWAS .},
  archive      = {J_SIM},
  author       = {Osvaldo Espin-Garcia and Radu V. Craiu and Shelley B. Bull},
  doi          = {10.1002/sim.9211},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6792-6817},
  shortjournal = {Stat. Med.},
  title        = {Two-phase sample selection strategies for design and analysis in post-genome-wide association fine-mapping studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining multiple imputation with raking of weights: An
efficient and robust approach in the setting of nearly true models.
<em>SIM</em>, <em>40</em>(30), 6777–6791. (<a
href="https://doi.org/10.1002/sim.9210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation (MI) provides us with efficient estimators in model-based methods for handling missing data under the true model. It is also well-understood that design-based estimators are robust methods that do not require accurately modeling the missing data; however, they can be inefficient. In any applied setting, it is difficult to know whether a missing data model may be good enough to win the bias-efficiency trade-off. Raking of weights is one approach that relies on constructing an auxiliary variable from data observed on the full cohort, which is then used to adjust the weights for the usual Horvitz-Thompson estimator. Computing the optimally efficient raking estimator requires evaluating the expectation of the efficient score given the full cohort data, which is generally infeasible. We demonstrate MI as a practical method to compute a raking estimator that will be optimal. We compare this estimator to common parametric and semi-parametric estimators, including standard MI. We show that while estimators, such as the semi-parametric maximum likelihood and MI estimator, obtain optimal performance under the true model, the proposed raking estimator utilizing MI maintains a better robustness-efficiency trade-off even under mild model misspecification. We also show that the standard raking estimator, without MI, is often competitive with the optimal raking estimator. We demonstrate these properties through several numerical examples and provide a theoretical discussion of conditions for asymptotically superior relative efficiency of the proposed raking estimator.},
  archive      = {J_SIM},
  author       = {Kyunghee Han and Pamela A. Shaw and Thomas Lumley},
  doi          = {10.1002/sim.9210},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6777-6791},
  shortjournal = {Stat. Med.},
  title        = {Combining multiple imputation with raking of weights: An efficient and robust approach in the setting of nearly true models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double-wavelet transform for multi-subject resting state
functional magnetic resonance imaging data. <em>SIM</em>,
<em>40</em>(30), 6762–6776. (<a
href="https://doi.org/10.1002/sim.9209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional regions of interest (ROIs)—level resting state fMRI (functional magnetic resonance imaging) response analyses do not rigorously model the underlying spatial correlation within each ROI. This can result in misleading inference. Moreover, they tend to estimate the temporal covariance matrix with the assumption of stationary time series, which may not always be valid. To overcome these limitations, we propose a double-wavelet approach that simplifies temporal and spatial covariance structure because wavelet coefficients are approximately uncorrelated under mild regularity conditions. This property allows us to analyze much larger dimensions of spatial and temporal resting-state fMRI data with reasonable computational burden. Another advantage of our double-wavelet approach is that it does not require the stationarity assumption. Simulation studies show that our method reduced false positive and false negative rates by properly taking into account spatial and temporal correlations in data. We also demonstrate advantages of our method by using resting-state fMRI data to study the difference in resting-state functional connectivity between healthy subjects and patients with major depressive disorder.},
  archive      = {J_SIM},
  author       = {Minchun Zhou and Brian D. Boyd and Warren D. Taylor and Hakmook Kang},
  doi          = {10.1002/sim.9209},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6762-6776},
  shortjournal = {Stat. Med.},
  title        = {Double-wavelet transform for multi-subject resting state functional magnetic resonance imaging data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian model-averaged meta-analysis in medicine.
<em>SIM</em>, <em>40</em>(30), 6743–6761. (<a
href="https://doi.org/10.1002/sim.9170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We outline a Bayesian model-averaged (BMA) meta-analysis for standardized mean differences in order to quantify evidence for both treatment effectiveness δ and across-study heterogeneity τ . We construct four competing models by orthogonally combining two present-absent assumptions, one for the treatment effect and one for across-study heterogeneity. To inform the choice of prior distributions for the model parameters, we used 50% of the Cochrane Database of Systematic Reviews to specify rival prior distributions for and . The relative predictive performance of the competing models and rival prior distributions was assessed using the remaining 50% of the Cochrane Database. On average, —the model that assumes the presence of a treatment effect as well as across-study heterogeneity—outpredicted the other models, but not by a large margin. Within , predictive adequacy was relatively constant across the rival prior distributions. We propose specific empirical prior distributions, both for the field in general and for each of 46 specific medical subdisciplines. An example from oral health demonstrates how the proposed prior distributions can be used to conduct a BMA meta-analysis in the open-source software R and JASP. The preregistered analysis plan is available at https://osf.io/zs3df/ .},
  archive      = {J_SIM},
  author       = {František Bartoš and Quentin F. Gronau and Bram Timmers and Willem M. Otte and Alexander Ly and Eric-Jan Wagenmakers},
  doi          = {10.1002/sim.9170},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {6743-6761},
  shortjournal = {Stat. Med.},
  title        = {Bayesian model-averaged meta-analysis in medicine},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On a new piecewise regression model with cure rate:
Diagnostics and application to medical data. <em>SIM</em>,
<em>40</em>(29), 6723–6742. (<a
href="https://doi.org/10.1002/sim.9208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss an extension of the classical negative binomial cure rate model with piecewise exponential distribution of the time to event for concurrent causes, which enables the modeling of monotonic and non-monotonic hazard functions (ie, the shape of the hazard function is not assumed as in traditional parametric models). This approach produces a flexible cure rate model, depending on the choice of time partition. We discuss local influence on this negative binomial power piecewise exponential model. We report on Monte Carlo simulation studies and application of the model to real melanoma and leukemia datasets.},
  archive      = {J_SIM},
  author       = {Yolanda M. Gómez and Diego I. Gallardo and Jeremias Leão and Vinicius F. Calsavara},
  doi          = {10.1002/sim.9208},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6723-6742},
  shortjournal = {Stat. Med.},
  title        = {On a new piecewise regression model with cure rate: Diagnostics and application to medical data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partially linear single-index generalized mean residual life
models. <em>SIM</em>, <em>40</em>(29), 6707–6722. (<a
href="https://doi.org/10.1002/sim.9207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean residual life (MRL) function defines the remaining life expectancy of a subject who has survived to a time point and is an important alternative to the hazard function for characterizing the distribution of a time-to-event variable. Existing MRL models primarily focus on studying the association between risk factors and disease risks using linear model specifications in multiplicative or additive scale. When risk factors have complex correlation structures, nonlinear effects, or interactions, the prefixed linearity assumption may be insufficient to capture the relationship. Single-index modeling framework offers flexibility in reducing dimensionality and modeling nonlinear effects. In this article, we propose a class of partially linear single-index generalized MRL models, the regression component of which consists of both a semiparametric single-index part and a linear regression part. Regression spline technique is employed to approximate the nonparametric single-index function, and parameters are estimated using an iterative algorithm. Double-robust estimators are also proposed to protect against the misspecification of censoring distribution or MRL models. A further contribution of this article is a nonparametric test proposed to formally evaluate the linearity of the single-index function. Asymptotic properties of the estimators are established, and the finite-sample performance is evaluated through extensive numerical simulations. The proposed models and inference approaches are demonstrated by a New York University Langone Health (NYULH) COVID-19 dataset.},
  archive      = {J_SIM},
  author       = {Peng Jin and Mengling Liu},
  doi          = {10.1002/sim.9207},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6707-6722},
  shortjournal = {Stat. Med.},
  title        = {Partially linear single-index generalized mean residual life models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint modeling of multivariate nonparametric longitudinal
data and survival data: A local smoothing approach. <em>SIM</em>,
<em>40</em>(29), 6689–6706. (<a
href="https://doi.org/10.1002/sim.9206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical studies, evaluating the association between longitudinal and survival outcomes is of primary concern. For analyzing data from such studies, joint modeling of longitudinal and survival data becomes an appealing approach. In some applications, there are multiple longitudinal outcomes whose longitudinal pattern is difficult to describe by a parametric form. For such applications, existing research on joint modeling is limited. In this article, we develop a novel joint modeling method to fill the gap. In the new method, a local polynomial mixed-effects model is used for describing the nonparametric longitudinal pattern of the multiple longitudinal outcomes. Two model estimation procedures, that is, the local EM algorithm and the local penalized quasi-likelihood estimation, are explored. Practical guidelines for choosing tuning parameters and for variable selection are provided. The new method is justified by some theoretical arguments and numerical studies.},
  archive      = {J_SIM},
  author       = {Lu You and Peihua Qiu},
  doi          = {10.1002/sim.9206},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6689-6706},
  shortjournal = {Stat. Med.},
  title        = {Joint modeling of multivariate nonparametric longitudinal data and survival data: A local smoothing approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Power calculation for analyses of cross-sectional
stepped-wedge cluster randomized trials with binary outcomes via
generalized estimating equations. <em>SIM</em>, <em>40</em>(29),
6674–6688. (<a href="https://doi.org/10.1002/sim.9205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power calculation for stepped-wedge cluster randomized trials (SW-CRTs) presents unique challenges, beyond those of standard parallel cluster randomized trials, due to the need to consider temporal within cluster correlations and background period effects. To date, power calculation methods specific to SW-CRTs have primarily been developed under a linear model. When the outcome is binary, the use of a linear model corresponds to assessing a prevalence difference; yet trial analysis often employs a nonlinear link function. We propose power calculation methods for cross-sectional SW-CRTs under a logistic model fitted by generalized estimating equations. Firstly, under an exchangeable correlation structure, we show the power based on a logistic model is lower than that from assuming a linear model in the absence of period effects. We then evaluate the impact of background prevalence changes over time on power. To allow the correlation among outcomes in the same cluster to change over time and with treatment status, we generalize the methods to more complex correlation structures. Our simulation studies demonstrate that the proposed power calculation methods perform well with the model-based variance under the true correlation structure and reveal that a working independence structure can result in substantial efficiency loss, while a working exchangeable structure performs well even when the underlying correlation structure deviates from exchangeable. An extension to our methods accounts for variable cluster sizes and reveals that unequal cluster sizes have a modest impact on power. We illustrate the approaches by application to a quality of care improvement trial for acute coronary syndrome.},
  archive      = {J_SIM},
  author       = {Linda J. Harrison and Rui Wang},
  doi          = {10.1002/sim.9205},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6674-6688},
  shortjournal = {Stat. Med.},
  title        = {Power calculation for analyses of cross-sectional stepped-wedge cluster randomized trials with binary outcomes via generalized estimating equations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interaction screening for high-dimensional heterogeneous
data via robust hybrid metrics. <em>SIM</em>, <em>40</em>(29),
6651–6673. (<a href="https://doi.org/10.1002/sim.9204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel model-free interaction screening approach called the hybrid metrics is introduced for high-dimensional heterogeneous data analysis. The metrics established based on the variation of conditional joint distribution function are measurements of interaction that include both size and direction. They are robust and can work with many types of response variables, including continuous, discrete, and categorical variables. We can apply the hybrid metrics to effective interaction selection for classification, response index models, and Poisson regression, among others. When dealing with classification, the hybrid metrics are capable of capturing both nonlinear category-general and category-specific interaction effects, providing us with a comprehensive overview and precise discovery of category information. When faced with a continuous response, the hybrid metrics perform fairly well even if the signal strength is weak, behaving as if the true interactions were known. To facilitate implementation, a fast two-stage procedure which naturally and efficiently enforces both strong and weak heredity is advocated. We further demonstrate their superior performances over popular competitors by exhaustive simulations and a SRBCT real data example. Supplementary materials for this article are available online.},
  archive      = {J_SIM},
  author       = {Wei Xiong and Han Pan},
  doi          = {10.1002/sim.9204},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6651-6673},
  shortjournal = {Stat. Med.},
  title        = {Interaction screening for high-dimensional heterogeneous data via robust hybrid metrics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of methods for analyzing a binary composite
endpoint with partially observed components in randomized controlled
trials. <em>SIM</em>, <em>40</em>(29), 6634–6650. (<a
href="https://doi.org/10.1002/sim.9203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite endpoints are commonly used to define primary outcomes in randomized controlled trials. A participant may be classified as meeting the endpoint if they experience an event in one or several components (eg, a favorable outcome based on a composite of being alive and attaining negative culture results in trials assessing tuberculosis treatments). Partially observed components that are not missing simultaneously complicate the analysis of the composite endpoint. An intuitive strategy frequently used in practice for handling missing values in the components is to derive the values of the composite endpoint from observed components when possible, and exclude from analysis participants whose composite endpoint cannot be derived. Alternatively, complete record analysis (CRA) (excluding participants with any missing components) or multiple imputation (MI) can be used. We compare a set of methods for analyzing a composite endpoint with partially observed components mathematically and by simulation, and apply these methods in a reanalysis of a published trial (TOPPS). We show that the derived composite endpoint can be missing not at random even when the components are missing completely at random. Consequently, the treatment effect estimated from the derived endpoint is biased while CRA results without the derived endpoint are valid. Missing at random mechanisms require MI of the components. We conclude that, although superficially attractive, deriving the composite endpoint from observed components should generally be avoided. Despite the potential risk of imputation model mis-specification, MI of missing components is the preferred approach in this study setting.},
  archive      = {J_SIM},
  author       = {Tra My Pham and Ian R. White and Brennan C. Kahan and Tim P. Morris and Simon J. Stanworth and Gordon Forbes},
  doi          = {10.1002/sim.9203},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6634-6650},
  shortjournal = {Stat. Med.},
  title        = {A comparison of methods for analyzing a binary composite endpoint with partially observed components in randomized controlled trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gene-gene interaction analysis incorporating network
information via a structured bayesian approach. <em>SIM</em>,
<em>40</em>(29), 6619–6633. (<a
href="https://doi.org/10.1002/sim.9202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing evidence has shown that gene-gene interactions have important effects in biological processes of human diseases. Due to the high dimensionality of genetic measurements, interaction analysis usually suffers from a lack of sufficient information and has unsatisfactory results. Biological network information has been massively accumulated, allowing researchers to identify biomarkers while taking a system perspective, conducting network selection (of functionally related biomarkers), and accommodating network structures. In main-effect-only analysis, network information has been incorporated. However, effort has been limited in interaction analysis. Recently, link networks that describe the relationships between genetic interactions have been demonstrated as effective for revealing multiscale hierarchical organizations in networks and providing interesting findings beyond node networks. In this study, we develop a novel structured Bayesian interaction analysis approach to effectively incorporate network information. This study is among the first to identify gene-gene interactions with the assistance of network selection, while simultaneously accommodating the underlying network structures of both main effects and interactions. It innovatively respects multiple hierarchies among main effects, interactions, and networks. The Bayesian technique is adopted, which may be more informative for estimation and prediction over some other techniques. An efficient variational Bayesian expectation-maximization algorithm is developed to explore the posterior distribution. Extensive simulation studies demonstrate the practical superiority of the proposed approach. The analysis of TCGA data on melanoma and lung cancer leads to biologically sensible findings with satisfactory prediction accuracy and selection stability.},
  archive      = {J_SIM},
  author       = {Xing Qin and Shuangge Ma and Mengyun Wu},
  doi          = {10.1002/sim.9202},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6619-6633},
  shortjournal = {Stat. Med.},
  title        = {Gene-gene interaction analysis incorporating network information via a structured bayesian approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating baseline covariates to validate surrogate
endpoints with a constant biomarker under control arm. <em>SIM</em>,
<em>40</em>(29), 6605–6618. (<a
href="https://doi.org/10.1002/sim.9201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A surrogate endpoint S in a clinical trial is an outcome that may be measured earlier or more easily than the true outcome of interest T . In this work, we extend causal inference approaches to validate such a surrogate using potential outcomes. The causal association paradigm assesses the relationship of the treatment effect on the surrogate with the treatment effect on the true endpoint. Using the principal surrogacy criteria, we utilize the joint conditional distribution of the potential outcomes T , given the potential outcomes S . In particular, our setting of interest allows us to assume the surrogate under the placebo, is zero-valued, and we incorporate baseline covariates in the setting of normally distributed endpoints. We develop Bayesian methods to incorporate conditional independence and other modeling assumptions and explore their impact on the assessment of surrogacy. We demonstrate our approach via simulation and data that mimics an ongoing study of a muscular dystrophy gene therapy.},
  archive      = {J_SIM},
  author       = {Emily K. Roberts and Michael R. Elliott and Jeremy M. G. Taylor},
  doi          = {10.1002/sim.9201},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6605-6618},
  shortjournal = {Stat. Med.},
  title        = {Incorporating baseline covariates to validate surrogate endpoints with a constant biomarker under control arm},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixture proportional hazards cure model with latent
variables. <em>SIM</em>, <em>40</em>(29), 6590–6604. (<a
href="https://doi.org/10.1002/sim.9200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture proportional hazards cure model with latent variables is proposed. The proposed model assesses the effects of the observed and latent risk factors on the hazards of uncured subjects and the cure rate through a proportional hazards model and a logistic model, respectively. Factor analysis is employed to measure the latent variables through correlated multiple indicators. Maximum likelihood estimation is performed through a Gaussian quadratic technique that approximates the integration over the latent variables. A piecewise constant function is used for the unspecified baseline hazard of uncured subjects. The proposed method can be conveniently implemented by using SAS Proc NLMIXED. Simulation studies are conducted to evaluate the performance of the proposed approach. An application to a study concerning the risk factors of chronic kidney disease for type 2 diabetic patients is provided.},
  archive      = {J_SIM},
  author       = {Haijin He and Dongxiao Han and Xinyuan Song and Liuquan Sun},
  doi          = {10.1002/sim.9200},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6590-6604},
  shortjournal = {Stat. Med.},
  title        = {Mixture proportional hazards cure model with latent variables},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging real-world evidence for determining performance
goals for medical device studies. <em>SIM</em>, <em>40</em>(29),
6577–6589. (<a href="https://doi.org/10.1002/sim.9199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance goals are numerical target values pertaining to effectiveness or safety endpoints in single-arm medical device clinical studies. Typically, performance goals are determined at the planning stage of the investigational study under consideration based on summarized outcome information from existing relevant clinical trials. In recent years, there is a growing interest in leveraging real-world evidence in medical product development. In this article, we introduce a new method for proposing performance goals by leveraging real-world evidence. The method applies entropy balancing to address possible patient dissimilarities between the study&#39;s target patient population and existing real-world patients, and can take into account operation differences between clinical studies and real-world clinical practice. An illustrative example is provided to demonstrate how to implement the proposed method for performance goal determination while leveraging real-world evidence.},
  archive      = {J_SIM},
  author       = {Chenguang Wang and Gary L. Rosner and Tingting Bao and Nelson Lu and Wei-Chen Chen and Heng Li and Ram Tiwari and Yunling Xu and Lilly Q. Yue},
  doi          = {10.1002/sim.9199},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6577-6589},
  shortjournal = {Stat. Med.},
  title        = {Leveraging real-world evidence for determining performance goals for medical device studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust method for optimal treatment decision making based on
survival data. <em>SIM</em>, <em>40</em>(29), 6558–6576. (<a
href="https://doi.org/10.1002/sim.9198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the optimal treatment decision rule, where the best treatment for an individual varies according to his/her characteristics, is of great importance when treatment effect heterogeneity exists. We develop methods for estimating the optimal treatment decision rule based on data with survival time as the primary endpoint. Our methods are based on a flexible semiparametric accelerated failure time model, where only the treatment contrast (ie, the difference in means between treatments) is parameterized and all other aspects are unspecified. An individual&#39;s treatment contrast is firstly estimated robustly by an augmented inverse probability weighted estimator (AIPWE). Then the optimal decision rule is estimated by minimizing the loss between the treatment contrast and the AIPWE contrast. Two loss functions with different strategies to account for censoring are proposed. The proposed loss functions distinguish from existing ones in that they are based on treatment contrasts, which completely determine the optimal treatment rule. Our methods can further incorporate a penalty term to select variables that are only important for treatment decision making, while taking advantage of all covariates predictive of outcomes to improve performance. Comprehensive simulation studies have been conducted to evaluate performances of the proposed methods relative to existing methods. The proposed methods are illustrated with an application to the ACTG 175 clinical trial on HIV-infected patients.},
  archive      = {J_SIM},
  author       = {Yuexin Fang and Baqun Zhang and Min Zhang},
  doi          = {10.1002/sim.9198},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6558-6576},
  shortjournal = {Stat. Med.},
  title        = {Robust method for optimal treatment decision making based on survival data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized variable selection for cause-specific hazard
frailty models with clustered competing-risks data. <em>SIM</em>,
<em>40</em>(29), 6541–6557. (<a
href="https://doi.org/10.1002/sim.9197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competing risks data usually arise when an occurrence of an event precludes other types of events from being observed. Such data are often encountered in a clustered clinical study such as a multi-center clinical trial. For the clustered competing-risks data which are correlated within a cluster, competing-risks models allowing for frailty terms have been recently studied. To the best of our knowledge, however, there is no literature on variable selection methods for cause-specific hazard frailty models. In this article, we propose a variable selection procedure for fixed effects in cause-specific competing risks frailty models using a penalized h-likelihood (HL). Here, we study three penalty functions, LASSO, SCAD, and HL. Simulation studies demonstrate that the proposed procedure using the HL penalty works well, providing a higher probability of choosing the true model than LASSO and SCAD methods without losing prediction accuracy. The proposed method is illustrated by using two kinds of clustered competing-risks cancer data sets.},
  archive      = {J_SIM},
  author       = {Trias W. Rakhmawati and Il Do Ha and Hangbin Lee and Youngjo Lee},
  doi          = {10.1002/sim.9197},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6541-6557},
  shortjournal = {Stat. Med.},
  title        = {Penalized variable selection for cause-specific hazard frailty models with clustered competing-risks data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification and inference for subgroups with differential
treatment efficacy from randomized controlled trials with survival
outcomes through multiple testing. <em>SIM</em>, <em>40</em>(29),
6523–6540. (<a href="https://doi.org/10.1002/sim.9196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the uptake of targeted therapies, instead of the “one-fits-all” approach, modern randomized controlled trials (RCTs) often aim to develop treatments that target a subgroup of patients. Motivated by analyzing the Age-Related Eye Disease Study (AREDS) data, a large RCT to study the efficacy of nutritional supplements in delaying the progression of an eye disease, age-related macular degeneration (AMD), we develop a simultaneous inference procedure to identify and infer subgroups with differential treatment efficacy in RCTs with time-to-event outcomes. Specifically, we formulate the multiple testing problem through contrasts and construct their simultaneous confidence intervals, which appropriately control both within- and across-marker multiplicity. Realistic simulations are conducted using real genotype data to evaluate the method performance under various scenarios. The method is then applied to AREDS to assess the efficacy of antioxidants and zinc combination in delaying AMD progression. Multiple gene regions including ESRRB-VASH1 on chromosome 14 have been identified with subgroups showing differential efficacy. We further validate our findings in an independent subsequent RCT, AREDS2, by discovering consistent differential treatment responses in the targeted and non-targeted subgroups identified from AREDS. This multiple-testing-based simultaneous inference approach provides a step forward to confidently identify and infer subgroups in modern drug development.},
  archive      = {J_SIM},
  author       = {Yue Wei and Jason C. Hsu and Wei Chen and Emily Y. Chew and Ying Ding},
  doi          = {10.1002/sim.9196},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6523-6540},
  shortjournal = {Stat. Med.},
  title        = {Identification and inference for subgroups with differential treatment efficacy from randomized controlled trials with survival outcomes through multiple testing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probability bound analysis: A novel approach for quantifying
parameter uncertainty in decision-analytic modeling and
cost-effectiveness analysis. <em>SIM</em>, <em>40</em>(29), 6501–6522.
(<a href="https://doi.org/10.1002/sim.9195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions about health interventions are often made using limited evidence. Mathematical models used to inform such decisions often include uncertainty analysis to account for the effect of uncertainty in the current evidence base on decision-relevant quantities. However, current uncertainty quantification methodologies, including probabilistic sensitivity analysis (PSA), require modelers to specify a precise probability distribution to represent the uncertainty of a model parameter. This study introduces a novel approach for representing and propagating parameter uncertainty, probability bounds analysis (PBA), where the uncertainty about the unknown probability distribution of a model parameter is expressed in terms of an interval bounded by lower and upper bounds on the unknown cumulative distribution function (p-box) and without assuming a particular form of the distribution function. We give the formulas of the p-boxes for common situations (given combinations of data on minimum, maximum, median, mean, or standard deviation), describe an approach to propagate p-boxes into a black-box mathematical model, and introduce an approach for decision-making based on the results of PBA. We demonstrate the characteristics and utility of PBA vs PSA using two case studies. In sum, this study provides modelers with practical tools to conduct parameter uncertainty quantification given the constraints of available data and with the fewest assumptions.},
  archive      = {J_SIM},
  author       = {Rowan Iskandar},
  doi          = {10.1002/sim.9195},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6501-6522},
  shortjournal = {Stat. Med.},
  title        = {Probability bound analysis: A novel approach for quantifying parameter uncertainty in decision-analytic modeling and cost-effectiveness analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial modeling of epidermal nerve fiber patterns.
<em>SIM</em>, <em>40</em>(29), 6479–6500. (<a
href="https://doi.org/10.1002/sim.9194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peripheral neuropathy is a condition associated with poor nerve functionality. Epidermal nerve fiber (ENF) counts per epidermal surface are dramatically reduced and the two-dimensional (2D) spatial structure of ENFs tends to become more clustered as neuropathy progresses. Therefore, studying the spatial structure of ENFs is essential to fully understand the mechanisms that guide those morphological changes. In this article, we compare ENF patterns of healthy controls and subjects suffering from mild diabetic neuropathy by using suction skin blister specimens obtained from the right foot. Previous analysis of these data has focused on the analysis and modeling of the spatial ENF patterns consisting of the points where the nerves enter the epidermis, base points, and the points where the nerve fibers terminate, end points, projected on a 2D plane, regarding the patterns as realizations of spatial point processes. Here, we include the first branching points, the points where the nerve trees branch for the first time, and model the three-dimensional (3D) patterns consisting of these three types of points. To analyze the patterns, spatial summary statistics are used and a new epidermal active territory that measures the volume in the epidermis that is covered by the individual nerve fibers is constructed. We developed a model for both the 2D and the 3D patterns including the branching points. Also, possible competitive behavior between individual nerves is examined. Our results indicate that changes in the ENFs spatial structure can more easily be detected in the later parts of the ENFs.},
  archive      = {J_SIM},
  author       = {Konstantinos Konstantinou and Aila Särkkä},
  doi          = {10.1002/sim.9194},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {6479-6500},
  shortjournal = {Stat. Med.},
  title        = {Spatial modeling of epidermal nerve fiber patterns},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint analysis of longitudinal measurements and spatially
clustered competing risks HIV/AIDS data. <em>SIM</em>, <em>40</em>(28),
6459–6477. (<a href="https://doi.org/10.1002/sim.9193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint modeling of repeated measurements and time-to-event provides a general framework to describe better the link between the progression of disease through longitudinal measurements and time-to-event outcome. In the survival data, a sample of individuals is frequently grouped into clusters. In some applications, these clusters could be arranged spatially, for example, based on geographical regions. There are two benefits of considering spatial variation in these data, enhancing the efficiency and accuracy of the parameters estimations, and investigating the survivorship spatial pattern. On the other hand, in survival data, there is a situation that subjects are supposed to experience more than one type of event potentially, but the occurrence of one type of event prevents the occurrence of the others. In this article, we considered a Bayesian joint model of longitudinal and competing risks outcomes for spatially clustered HIV/AIDS data. The data were from a registry-based study carried in Hamadan Province, Iran, from December 1997 to June 2020. In this joint model, a linear mixed effects model was used for the longitudinal submodel and a cause-specific hazard model with spatial and spatial-risk random effects was used for the survival submodel. Also, a latent structure was defined by random effects to link both event times and longitudinal processes. We used a univariate intrinsic conditional autoregressive (ICAR) distribution and a multivariate ICAR distribution for modeling the areal spatial and spatial-risk random effects, respectively. The performance of our proposed model using simulation studies and analysis of HIV/AIDS data were assessed.},
  archive      = {J_SIM},
  author       = {Somayeh Momenyan},
  doi          = {10.1002/sim.9193},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6459-6477},
  shortjournal = {Stat. Med.},
  title        = {Joint analysis of longitudinal measurements and spatially clustered competing risks HIV/AIDS data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matching on poset-based average rank for multiple treatments
to compare many unbalanced groups. <em>SIM</em>, <em>40</em>(28),
6443–6458. (<a href="https://doi.org/10.1002/sim.9192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an original matching procedure for multiple treatment frameworks based on partially ordered set theory (poset). In our proposal, called matching on poset-based average rank for multiple treatments (MARMoT), poset theory is used to summarize individuals&#39; confounders and the relative average rank is used to balance confounders and match individuals in different treatment groups. This approach proves to be particularly useful for balancing confounders when the number of treatments considered is high. We apply our approach to the estimation of neighborhood effect on the fractures among older people in Turin (a city in northern Italy).},
  archive      = {J_SIM},
  author       = {Margherita Silan and Giovanna Boccuzzo and Bruno Arpino},
  doi          = {10.1002/sim.9192},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6443-6458},
  shortjournal = {Stat. Med.},
  title        = {Matching on poset-based average rank for multiple treatments to compare many unbalanced groups},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating external data into the analysis of clinical
trials via bayesian additive regression trees. <em>SIM</em>,
<em>40</em>(28), 6421–6442. (<a
href="https://doi.org/10.1002/sim.9191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most clinical trials involve the comparison of a new treatment to a control arm (eg, the standard of care) and the estimation of a treatment effect. External data, including historical clinical trial data and real-world observational data, are commonly available for the control arm. With proper statistical adjustments, borrowing information from external data can potentially reduce the mean squared errors of treatment effect estimates and increase the power of detecting a meaningful treatment effect. In this article, we propose to use Bayesian additive regression trees (BART) for incorporating external data into the analysis of clinical trials, with a specific goal of estimating the conditional or population average treatment effect. BART naturally adjusts for patient-level covariates and captures potentially heterogeneous treatment effects across different data sources, achieving flexible borrowing. Simulation studies demonstrate that BART maintains desirable and robust performance across a variety of scenarios and compares favorably to alternatives. We illustrate the proposed method with an acupuncture trial and a colorectal cancer trial.},
  archive      = {J_SIM},
  author       = {Tianjian Zhou and Yuan Ji},
  doi          = {10.1002/sim.9191},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6421-6442},
  shortjournal = {Stat. Med.},
  title        = {Incorporating external data into the analysis of clinical trials via bayesian additive regression trees},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frailty modeling under a selective sampling protocol: An
application to type 1 diabetes related autoantibodies. <em>SIM</em>,
<em>40</em>(28), 6410–6420. (<a
href="https://doi.org/10.1002/sim.9190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies following selective sampling protocols for secondary outcomes, conventional analyses regarding their appearance could provide misguided information. In the large type 1 diabetes prevention and prediction (DIPP) cohort study monitoring type 1 diabetes-associated autoantibodies, we propose to model their appearance via a multivariate frailty model, which incorporates a correlation component that is important for unbiased estimation of the baseline hazards under the selective sampling mechanism. As further advantages, the frailty model allows for systematic evaluation of the association and the differences in regression parameters among the autoantibodies. We demonstrate the properties of the model by a simulation study and the analysis of the autoantibodies and their association with background factors in the DIPP study, in which we found that high genetic risk is associated with the appearance of all the autoantibodies, whereas the association with sex and urban municipality was evident for IA-2A and IAA autoantibodies.},
  archive      = {J_SIM},
  author       = {Jaakko Nevalainen and Somnath Datta and Jorma Toppari and Jorma Ilonen and Heikki Hyöty and Riitta Veijola and Mikael Knip and Suvi M. Virtanen},
  doi          = {10.1002/sim.9190},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6410-6420},
  shortjournal = {Stat. Med.},
  title        = {Frailty modeling under a selective sampling protocol: An application to type 1 diabetes related autoantibodies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simplified stochastic EM algorithm for cure rate model
with negative binomial competing risks: An application to breast cancer
data. <em>SIM</em>, <em>40</em>(28), 6387–6409. (<a
href="https://doi.org/10.1002/sim.9189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a long-term survival model under competing risks is considered. The unobserved number of competing risks is assumed to follow a negative binomial distribution that can capture both over- and under-dispersion. Considering the latent competing risks as missing data, a variation of the well-known expectation maximization (EM) algorithm, called the stochastic EM algorithm (SEM), is developed. It is shown that the SEM algorithm avoids calculation of complicated expectations, which is a major advantage of the SEM algorithm over the EM algorithm. The proposed procedure also allows the objective function to be split into two simpler functions, one corresponding to the parameters associated with the cure rate and the other corresponding to the parameters associated with the progression times. The advantage of this approach is that each simple function, with lower parameter dimension, can be maximized independently. An extensive Monte Carlo simulation study is carried out to compare the performances of the SEM and EM algorithms. Finally, a breast cancer survival data is analyzed and it is shown that the SEM algorithm performs better than the EM algorithm.},
  archive      = {J_SIM},
  author       = {Suvra Pal},
  doi          = {10.1002/sim.9189},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6387-6409},
  shortjournal = {Stat. Med.},
  title        = {A simplified stochastic EM algorithm for cure rate model with negative binomial competing risks: An application to breast cancer data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Use of the likelihood reduction factor in a path analysis
framework to quantify surrogacy in clinical trials. <em>SIM</em>,
<em>40</em>(28), 6373–6386. (<a
href="https://doi.org/10.1002/sim.9188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, surrogate endpoints are useful when the endpoint of interest is difficult to measure or requires a long follow-up time. Current methodology for validating surrogate endpoints encounters challenges in the presence of collinearity between the treatment and surrogate endpoint, which is often present in clinical trials. The proposed methods adapt current methodology in the structural framework of path analysis to quantify the validity of a surrogate endpoint. The path analysis framework provides an improved interpretation of treatment effect. Through derivation and simulation we show the proposed path likelihood reduction factor (LRF ), is less biased and more robust than current methodology in cases of collinearity between the treatment and surrogate endpoint, with notable improvement when surrogacy is weak or moderate. LRF can be expanded to evaluate multiple correlated surrogate endpoints, which as shown through simulation, is also less biased and more robust than current methodology in the case of collinearity between the treatment and surrogate endpoint.},
  archive      = {J_SIM},
  author       = {Katherine Bloore and Yang Song and Howard Cabral and Joseph Massaro and Michael LaValley},
  doi          = {10.1002/sim.9188},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6373-6386},
  shortjournal = {Stat. Med.},
  title        = {Use of the likelihood reduction factor in a path analysis framework to quantify surrogacy in clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel density estimation in mixture models with known
mixture proportions. <em>SIM</em>, <em>40</em>(28), 6360–6372. (<a
href="https://doi.org/10.1002/sim.9187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the density estimation for data with a mixture structure, where the component densities are assumed unknown, but for each observation, the probabilities of its membership to the subpopulations are known or estimable from other resources. Data of this kind arise from practice and have wide applications. Motivated from the classical kernel density estimation method for a single population, we propose a weighted kernel density estimation method to estimate the component density functions nonparametrically. Within the framework of the EM algorithm, we derive an algorithm that computes our proposed estimates effectively. Via extensive simulation studies, we demonstrate that our methods outperform the existing methods in most occasions. We further compare our methods with existing methods by real data examples.},
  archive      = {J_SIM},
  author       = {Siyun Liu and Tao Yu},
  doi          = {10.1002/sim.9187},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6360-6372},
  shortjournal = {Stat. Med.},
  title        = {Kernel density estimation in mixture models with known mixture proportions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian approach for event predictions in clinical trials
with time-to-event outcomes. <em>SIM</em>, <em>40</em>(28), 6344–6359.
(<a href="https://doi.org/10.1002/sim.9186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials with time-to-event outcome as the primary endpoint, the end of study date is often based on the number of observed events, which drives the statistical power and the sample size calculation. It is of great value for study sponsors to have a good understanding of the recruitment process and the event milestones to manage the logistical tasks, which require a considerable amount of resources. The objective of the proposed statistical approach is to predict, as accurately as possible, the timing of an analysis planned once a target number of events is collected. The method takes into account the enrollment, the time to event, and the time to censor processes, using Weibull models in a Bayesian framework. We also consider a possible delay in the event reporting by the investigators, and covariates may also be included. Several metrics can be obtained, such as the probability of study completion at specific timepoints or the credible interval of the date of study completion. The approach was applied to oncology trials, with progression-free survival as primary outcome. A retrospective analysis shows the accuracy of the approach on these examples, as well as the benefit of updating the predictive probability of study completion as data are accumulating or new information becomes available. We also evaluated the performances of the proposed method in a comprehensive simulation study.},
  archive      = {J_SIM},
  author       = {Paul Aubel and Marine Antigny and Ronan Fougeray and Frédéric Dubois and Gaëlle Saint-Hilary},
  doi          = {10.1002/sim.9186},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6344-6359},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach for event predictions in clinical trials with time-to-event outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying the feasibility of shortening clinical trial
duration using surrogate markers. <em>SIM</em>, <em>40</em>(28),
6321–6343. (<a href="https://doi.org/10.1002/sim.9185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential benefit of using a surrogate marker in place of a long-term primary outcome is very attractive in terms of the impact on study length and cost. Many available methods for quantifying the effectiveness of a surrogate endpoint either rely on strict parametric modeling assumptions or require that the primary outcome and surrogate marker are fully observed that is, not subject to censoring. Moreover, available methods for quantifying surrogacy typically provide a proportion of treatment effect explained (PTE) measure and do not directly address the important questions of whether and how the trial can be ended earlier using the surrogate marker. In this article, we specifically address these important questions by proposing a PTE measure to quantify the feasibility of ending trials early based on endpoint information collected at an earlier landmark point in a time-to-event outcome setting. We provide a framework for deriving an optimally predicted outcome for individual patients at based on a combination of surrogate marker and event time information in the presence of censoring. We propose a non-parametric estimator for the PTE measure and derive the asymptotic properties of our estimators. Finite sample performance of our estimators are illustrated via extensive simulation studies and a real data application examining the potential of hemoglobin A1c and fasting plasma glucose to predict treatment effects on long term diabetes risk based on the Diabetes Prevention Program study.},
  archive      = {J_SIM},
  author       = {Xuan Wang and Tianxi Cai and Lu Tian and Florence Bourgeois and Layla Parast},
  doi          = {10.1002/sim.9185},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6321-6343},
  shortjournal = {Stat. Med.},
  title        = {Quantifying the feasibility of shortening clinical trial duration using surrogate markers},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simulation-extrapolation approach for regression analysis
of misclassified current status data with the additive hazards model.
<em>SIM</em>, <em>40</em>(28), 6309–6320. (<a
href="https://doi.org/10.1002/sim.9184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current status data arise when each subject is observed only once and the failure time of interest is only known to be either smaller or larger than the observation time rather than observed exactly. For the situation, due to the use of imperfect diagnostic tests, the failure status could often suffer misclassification or one observes misclassified data, which may result in severely biased estimation if not taken into account. In this article, we discuss regression analysis of such misclassified current status data arising from the additive hazards model, and a simulation-extrapolation (SIMEX) approach is developed for the estimation. Furthermore, the asymptotic properties of the proposed estimators are established, and a simulation study is conducted to assess the empirical performance of the method, which indicates that the proposed procedure performs well. In particular, it can correct the estimation bias given by the naive method that ignores the existence of misclassification. An application to a medical study on gonorrhea is also provided.},
  archive      = {J_SIM},
  author       = {Shuwei Li and Tian Tian and Tao Hu and Jianguo Sun},
  doi          = {10.1002/sim.9184},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6309-6320},
  shortjournal = {Stat. Med.},
  title        = {A simulation-extrapolation approach for regression analysis of misclassified current status data with the additive hazards model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating survival data into case-control studies with
incident and prevalent cases. <em>SIM</em>, <em>40</em>(28), 6295–6308.
(<a href="https://doi.org/10.1002/sim.9183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typically, case-control studies to estimate odds-ratios associating risk factors with disease incidence only include newly diagnosed cases. Recently proposed methods allow incorporating information on prevalent cases, individuals who survived from disease diagnosis to sampling, into cross-sectionally sampled case-control studies under parametric assumptions for the survival time after diagnosis. Here we propose and study methods to additionally use prospectively observed survival times from prevalent and incident cases to adjust logistic models for the time between diagnosis and sampling, the backward time, for prevalent cases. This adjustment yields unbiased odds-ratio estimates from case-control studies that include prevalent cases. We propose a computationally simple two-step generalized method-of-moments estimation procedure. First, we estimate the survival distribution assuming a semiparametric Cox model using an expectation-maximization algorithm that yields fully efficient estimates and accommodates left truncation for prevalent cases and right censoring. Then, we use the estimated survival distribution in an extension of the logistic model to three groups (controls, incident, and prevalent cases), to adjust for the survival bias in prevalent cases. In simulations, under modest amounts of censoring, odds-ratios from the two-step procedure were equally efficient as those estimated from a joint logistic and survival data likelihood under parametric assumptions. This indicates that utilizing the cases&#39; prospective survival data lessens model dependencies and improves precision of association estimates for case-control studies with prevalent cases. We illustrate the methods by estimating associations between single nucleotide polymorphisms and breast cancer risk using controls, and incident and prevalent cases sampled from the US Radiologic Technologists Study cohort.},
  archive      = {J_SIM},
  author       = {Soutrik Mandal and Jing Qin and Ruth M. Pfeiffer},
  doi          = {10.1002/sim.9183},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6295-6308},
  shortjournal = {Stat. Med.},
  title        = {Incorporating survival data into case-control studies with incident and prevalent cases},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geographically weighted generalized farrington algorithm for
rapid outbreak detection over short data accumulation periods.
<em>SIM</em>, <em>40</em>(28), 6277–6294. (<a
href="https://doi.org/10.1002/sim.9182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for rapid surveillance and early detection of local outbreaks has been growing recently. The rapid surveillance can select timely and appropriate interventions toward controlling the spread of emerging infectious diseases, such as the coronavirus disease 2019 (COVID-19). The Farrington algorithm was originally proposed by Farrington et al (1996), extended by Noufaily et al (2012), and is commonly used to estimate excess death. However, one of the major challenges in implementing this algorithm is the lack of historical information required to train it, especially for emerging diseases. Without sufficient training data the estimation/prediction accuracy of this algorithm can suffer leading to poor outbreak detection. We propose a new statistical algorithm—the geographically weighted generalized Farrington (GWGF) algorithm—by incorporating both geographically varying and geographically invariant covariates, as well as geographical information to analyze time series count data sampled from a spatially correlated process for estimating excess death. The algorithm is a type of local quasi-likelihood-based regression with geographical weights and is designed to achieve a stable detection of outbreaks even when the number of time points is small. We validate the outbreak detection performance by using extensive numerical experiments and real-data analysis in Japan during COVID-19 pandemic. We show that the GWGF algorithm succeeds in improving recall without reducing the level of precision compared with the conventional Farrington algorithm.},
  archive      = {J_SIM},
  author       = {Daisuke Yoneoka and Takayuki Kawashima and Koji Makiyama and Yuta Tanoue and Shuhei Nomura and Akifumi Eguchi},
  doi          = {10.1002/sim.9182},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6277-6294},
  shortjournal = {Stat. Med.},
  title        = {Geographically weighted generalized farrington algorithm for rapid outbreak detection over short data accumulation periods},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Household transmission of influenza a and b within a
prospective cohort during the 2013-2014 and 2014-2015 seasons.
<em>SIM</em>, <em>40</em>(28), 6260–6276. (<a
href="https://doi.org/10.1002/sim.9181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People living within the same household as someone ill with influenza are at increased risk of infection. Here, we use Markov chain Monte Carlo methods to partition the hazard of influenza illness within a cohort into the hazard from the community and the hazard from the household. During the 2013-2014 influenza season, 49 (4.7%) of the 1044 people enrolled in a community surveillance cohort had an acute respiratory illness (ARI) attributable to influenza. During the 2014-2015 influenza season, 50 (4.7%) of the 1063 people in the cohort had an ARI attributable to influenza. The secondary attack rate from a household member was 2.3% for influenza A (H1) during 2013-2014, 5.3% for influenza B during 2013-2014, and 7.6% for influenza A (H3) during 2014-2015. Living in a household with a person ill with influenza increased the risk of an ARI attributable to influenza up to 350%, depending on the season and the influenza virus circulating within the household.},
  archive      = {J_SIM},
  author       = {F. Scott Dahlgren and Ivo M. Foppa and Melissa S. Stockwell and Celibell Y. Vargas and Philip LaRussa and Carrie Reed},
  doi          = {10.1002/sim.9181},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6260-6276},
  shortjournal = {Stat. Med.},
  title        = {Household transmission of influenza a and b within a prospective cohort during the 2013-2014 and 2014-2015 seasons},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive generalized varying-coefficient longitudinal
model. <em>SIM</em>, <em>40</em>(28), 6243–6259. (<a
href="https://doi.org/10.1002/sim.9180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nonparametric bivariate varying coefficient generalized linear model to predict a mean response trajectory in the future given an individual&#39;s characteristics at present or an earlier time point in a longitudinal study. Given the measurement time of the predictors, the coefficients vary as functions of the future time over which the prediction of the mean response is concerned and illustrate the dynamic association between the future response and the earlier measured predictors. We use a nonparametric approach that takes advantage of features of both the kernel and the spline methods for estimation. The resulting coefficient estimator is asymptotically consistent under mild regularity conditions. We also develop a new bootstrap approach to construct simultaneous confidence bands for statistical inference about the coefficients and the predicted response trajectory based on the coverage rate of bootstrap estimates. We use the Framingham Heart Study to illustrate the methodology. The proposed procedure is applied to predict the probability trajectory of hypertension risk given individuals&#39; health condition in early adulthood and to examine the impact of risk factors in early adulthood on a long-term risk of hypertension over several decades.},
  archive      = {J_SIM},
  author       = {Seonjin Kim and Hyunkeun Ryan Cho and Mi-Ok Kim},
  doi          = {10.1002/sim.9180},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6243-6259},
  shortjournal = {Stat. Med.},
  title        = {Predictive generalized varying-coefficient longitudinal model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Choosing clinically interpretable summary measures and
robust analytic procedures for quantifying the treatment difference in
comparative clinical studies. <em>SIM</em>, <em>40</em>(28), 6235–6242.
(<a href="https://doi.org/10.1002/sim.8971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Zachary R. McCaw and Lu Tian and Jiawei Wei and Brian Lee Claggett and Frank Bretz and Garrett Fitzmaurice and Lee-Jen Wei},
  doi          = {10.1002/sim.8971},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {6235-6242},
  shortjournal = {Stat. Med.},
  title        = {Choosing clinically interpretable summary measures and robust analytic procedures for quantifying the treatment difference in comparative clinical studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian workflow for disease transmission modeling in stan.
<em>SIM</em>, <em>40</em>(27), 6209–6234. (<a
href="https://doi.org/10.1002/sim.9164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This tutorial shows how to build, fit, and criticize disease transmission models in Stan, and should be useful to researchers interested in modeling the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic and other infectious diseases in a Bayesian framework. Bayesian modeling provides a principled way to quantify uncertainty and incorporate both data and prior knowledge into the model estimates. Stan is an expressive probabilistic programming language that abstracts the inference and allows users to focus on the modeling. As a result, Stan code is readable and easily extensible, which makes the modeler&#39;s work more transparent. Furthermore, Stan&#39;s main inference engine, Hamiltonian Monte Carlo sampling, is amiable to diagnostics, which means the user can verify whether the obtained inference is reliable. In this tutorial, we demonstrate how to formulate, fit, and diagnose a compartmental transmission model in Stan, first with a simple susceptible-infected-recovered model, then with a more elaborate transmission model used during the SARS-CoV-2 pandemic. We also cover advanced topics which can further help practitioners fit sophisticated models; notably, how to use simulations to probe the model and priors, and computational techniques to scale-up models based on ordinary differential equations.},
  archive      = {J_SIM},
  author       = {Léo Grinsztajn and Elizaveta Semenova and Charles C. Margossian and Julien Riou},
  doi          = {10.1002/sim.9164},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6209-6234},
  shortjournal = {Stat. Med.},
  title        = {Bayesian workflow for disease transmission modeling in stan},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Avoiding bias in self-controlled case series studies of
coronavirus disease 2019. <em>SIM</em>, <em>40</em>(27), 6197–6208. (<a
href="https://doi.org/10.1002/sim.9179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies, including self-controlled case series (SCCS) studies, are being undertaken to quantify the risks of complications following infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes coronavirus disease 2019 (COVID-19). One such SCCS study, based on all COVID-19 cases arising in Sweden over an 8-month period, has shown that SARS-CoV-2 infection increases the risks of AMI and ischemic stroke. Some features of SARS-CoV-2 infection and COVID-19, present in this study and likely in others, complicate the analysis and may introduce bias. In the present paper we describe these features, and explore the biases they may generate. Motivated by data-based simulations, we propose methods to reduce or remove these biases.},
  archive      = {J_SIM},
  author       = {Osvaldo Fonseca-Rodríguez and Anne-Marie Fors Connolly and Ioannis Katsoularis and Krister Lindmark and Paddy Farrington},
  doi          = {10.1002/sim.9179},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6197-6208},
  shortjournal = {Stat. Med.},
  title        = {Avoiding bias in self-controlled case series studies of coronavirus disease 2019},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized regression calibration: A method for the
prediction of survival outcomes using complex longitudinal and
high-dimensional data. <em>SIM</em>, <em>40</em>(27), 6178–6196. (<a
href="https://doi.org/10.1002/sim.9178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal and high-dimensional measurements have become increasingly common in biomedical research. However, methods to predict survival outcomes using covariates that are both longitudinal and high-dimensional are currently missing. In this article, we propose penalized regression calibration (PRC), a method that can be employed to predict survival in such situations. PRC comprises three modeling steps: First, the trajectories described by the longitudinal predictors are flexibly modeled through the specification of multivariate mixed effects models. Second, subject-specific summaries of the longitudinal trajectories are derived from the fitted mixed models. Third, the time to event outcome is predicted using the subject-specific summaries as covariates in a penalized Cox model. To ensure a proper internal validation of the fitted PRC models, we furthermore develop a cluster bootstrap optimism correction procedure that allows to correct for the optimistic bias of apparent measures of predictiveness. PRC and the CBOCP are implemented in the R package pencal , available from CRAN . After studying the behavior of PRC via simulations, we conclude by illustrating an application of PRC to data from an observational study that involved patients affected by Duchenne muscular dystrophy, where the goal is predict time to loss of ambulation using longitudinal blood biomarkers.},
  archive      = {J_SIM},
  author       = {Mirko Signorelli and Pietro Spitali and Cristina Al-Khalili Szigyarto and Roula Tsonaka},
  doi          = {10.1002/sim.9178},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6178-6196},
  shortjournal = {Stat. Med.},
  title        = {Penalized regression calibration: A method for the prediction of survival outcomes using complex longitudinal and high-dimensional data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Step-adjusted tree-based reinforcement learning for
evaluating nested dynamic treatment regimes using test-and-treat
observational data. <em>SIM</em>, <em>40</em>(27), 6164–6177. (<a
href="https://doi.org/10.1002/sim.9177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs) include a sequence of treatment decision rules, in which treatment is adapted over time in response to the changes in an individual&#39;s disease progression and health care history. In medical practice, nested test-and-treat strategies are common to improve cost-effectiveness. For example, for patients at risk of prostate cancer, only patients who have high prostate-specific antigen (PSA) need a biopsy, which is costly and invasive, to confirm the diagnosis and help determine the treatment if needed. A decision about treatment happens after the biopsy, and is thus nested within the decision of whether to do the test. However, current existing statistical methods are not able to accommodate such a naturally embedded property of the treatment decision within the test decision. Therefore, we developed a new statistical learning method, step-adjusted tree-based reinforcement learning, to evaluate DTRs within such a nested multistage dynamic decision framework using observational data. At each step within each stage, we combined the robust semiparametric estimation via augmented inverse probability weighting with a tree-based reinforcement learning method to deal with the counterfactual optimization. The simulation studies demonstrated robust performance of the proposed methods under different scenarios. We further applied our method to evaluate the necessity of prostate biopsy and identify the optimal test-and-treat regimes for prostate cancer patients using data from the Johns Hopkins University prostate cancer active surveillance dataset.},
  archive      = {J_SIM},
  author       = {Ming Tang and Lu Wang and Michael A. Gorin and Jeremy M. G. Taylor},
  doi          = {10.1002/sim.9177},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6164-6177},
  shortjournal = {Stat. Med.},
  title        = {Step-adjusted tree-based reinforcement learning for evaluating nested dynamic treatment regimes using test-and-treat observational data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Informing power and sample size calculations when using
inverse probability of treatment weighting using the propensity score.
<em>SIM</em>, <em>40</em>(27), 6150–6163. (<a
href="https://doi.org/10.1002/sim.9176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score weighting is increasingly being used in observational studies to estimate the effects of treatments. The use of such weights induces a within-person homogeneity in outcomes that must be accounted for when estimating the variance of the estimated treatment effect. Knowledge of the variance inflation factor (VIF), which describes the extent to which the effective sample size has been reduced by weighting, allows for conducting sample size and power calculations for observational studies that use propensity score weighting. However, estimation of the VIF requires knowledge of the weights, which are only known once the study has been conducted. We describe methods to estimate the VIF based on two characteristics of the observational study: the anticipated prevalence of treatment and the anticipated c-statistic of the propensity score model. We considered five different sets of weights: those for estimating the average treatment effect (ATE), the average treated effect in the treated (ATT), and three recently described sets of weights: overlap weights, matching weights, and entropy weights. The VIF was substantially smaller for the latter three sets of weights than for the first two sets of weights. Once the VIF has been estimated during the design phase of the study, sample size and power calculations can be done using calculations appropriate for a randomized controlled trial with similar prevalence of treatment and similar outcome variable, and then multiplying the requisite sample size by the estimated VIF. Implementation of these methods allows for improving the design and reporting of observational studies that use propensity score weighting.},
  archive      = {J_SIM},
  author       = {Peter C. Austin},
  doi          = {10.1002/sim.9176},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6150-6163},
  shortjournal = {Stat. Med.},
  title        = {Informing power and sample size calculations when using inverse probability of treatment weighting using the propensity score},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size re-estimation in clinical trials. <em>SIM</em>,
<em>40</em>(27), 6133–6149. (<a
href="https://doi.org/10.1002/sim.9175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, sample size re-estimation is often conducted at interim. The purpose is to determine whether the study will achieve study objectives if the observed treatment effect at interim preserves till end of the study. A traditional approach is to conduct a conditional power analysis for sample size only based on observed treatment effect. This approach, however, does not take into consideration the variabilities of (i) the observed (estimate) treatment effect and (ii) the observed (estimate) variability associated with the treatment effect. Thus, the resultant re-estimated sample sizes may not be robust and hence may not be reliable. In this article, a couple of methods are proposed, namely, adjusted effect size (AES) approach and iterated expectation/variance (IEV) approach, which can account for the variability associated with the observed responses at interim. The proposed methods provide interval estimates of sample size required for the intended trial, which is useful for making critical go/no go decision. Statistical properties of the proposed methods are evaluated in terms of controlling of type I error rate and statistical power. The results show that traditional approach performs poorly in controlling type I error inflation, whereas IEV approach has the best performance in most cases. Additionally, all re-estimation approaches can keep the statistical power over 80 ; especially, IEV approach&#39;s statistical power, using adjusted significance level, is over 95 . However, IEV approach may lead to a greater increment in sample size when detecting a smaller effect size. In general, IEV approach is effective when effect size is large; otherwise, AES approach is more suitable for controlling type I error rate and keep power over 80 with a more reasonable re-estimated sample size.},
  archive      = {J_SIM},
  author       = {Peijin Wang and Shein-Chung Chow},
  doi          = {10.1002/sim.9175},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6133-6149},
  shortjournal = {Stat. Med.},
  title        = {Sample size re-estimation in clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accounting for not-at-random missingness through imputation
stacking. <em>SIM</em>, <em>40</em>(27), 6118–6132. (<a
href="https://doi.org/10.1002/sim.9174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Not-at-random missingness presents a challenge in addressing missing data in many health research applications. In this article, we propose a new approach to account for not-at-random missingness after multiple imputation through weighted analysis of stacked multiple imputations. The weights are easily calculated as a function of the imputed data and assumptions about the not-at-random missingness. We demonstrate through simulation that the proposed method has excellent performance when the missingness model is correctly specified. In practice, the missingness mechanism will not be known. We show how we can use our approach in a sensitivity analysis framework to evaluate the robustness of model inference to different assumptions about the missingness mechanism, and we provide R package StackImpute to facilitate implementation as part of routine sensitivity analyses. We apply the proposed method to account for not-at-random missingness in human papillomavirus test results in a study of survival for patients diagnosed with oropharyngeal cancer.},
  archive      = {J_SIM},
  author       = {Lauren J. Beesley and Jeremy M. G. Taylor},
  doi          = {10.1002/sim.9174},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6118-6132},
  shortjournal = {Stat. Med.},
  title        = {Accounting for not-at-random missingness through imputation stacking},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The statistical properties of RCTs and a proposal for
shrinkage. <em>SIM</em>, <em>40</em>(27), 6107–6117. (<a
href="https://doi.org/10.1002/sim.9173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We abstract the concept of a randomized controlled trial as a triple ( β , b , s ) , where β is the primary efficacy parameter, b the estimate, and s the standard error ( s &gt; 0 ). If the parameter β is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z -value z = b / s and the signal-to-noise ratio SNR = β / s from a sample of pairs ( b i , s i ) . We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on ( β , b , s ) only through the pair ( z , SNR ) . We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13%. We also consider the exaggeration ratio which is the factor by which the magnitude of β is overestimated. We find that if the estimate is just significant at the 5% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner&#39;s curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
  archive      = {J_SIM},
  author       = {Erik van Zwet and Simon Schwab and Stephen Senn},
  doi          = {10.1002/sim.9173},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6107-6117},
  shortjournal = {Stat. Med.},
  title        = {The statistical properties of RCTs and a proposal for shrinkage},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple imputation of semi-continuous exposure variables
that are categorized for analysis. <em>SIM</em>, <em>40</em>(27),
6093–6106. (<a href="https://doi.org/10.1002/sim.9172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-continuous variables are characterized by a point mass at one value and a continuous range of values for remaining observations. An example is alcohol consumption quantity, with a spike of zeros representing non-drinkers and positive values for drinkers. If multiple imputation is used to handle missing values for semi-continuous variables, it is unclear how this should be implemented within the standard approaches of fully conditional specification (FCS) and multivariate normal imputation (MVNI). This question is brought into focus by the use of categorized versions of semi-continuous exposure variables in analyses (eg, no drinking, drinking below binge level, binge drinking, heavy binge drinking), raising the question of how best to achieve congeniality between imputation and analysis models. We performed a simulation study comparing nine approaches for imputing semi-continuous exposures requiring categorization for analysis. Three methods imputed the categories directly: ordinal logistic regression, and imputation of binary indicator variables representing the categories using MVNI (with two variants). Six methods (predictive mean matching, zero-inflated binomial imputation, and two-part imputation methods with variants in FCS and MVNI) imputed the semi-continuous variable, with categories derived after imputation. The ordinal and zero-inflated binomial methods had good performance across most scenarios, while MVNI methods requiring rounding after imputation did not perform well. There were mixed results for predictive mean matching and the two-part methods, depending on whether the estimands were proportions or regression coefficients. The results highlight the need to consider the parameter of interest when selecting an imputation procedure.},
  archive      = {J_SIM},
  author       = {Cattram D. Nguyen and Margarita Moreno-Betancur and Laura Rodwell and Helena Romaniuk and John B. Carlin and Katherine J. Lee},
  doi          = {10.1002/sim.9172},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6093-6106},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation of semi-continuous exposure variables that are categorized for analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inverse probability weighting and doubly robust
standardization in the relative survival framework. <em>SIM</em>,
<em>40</em>(27), 6069–6092. (<a
href="https://doi.org/10.1002/sim.9171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A commonly reported measure when interested in the survival of cancer patients is relative survival. Relative survival circumvents issues with inaccurate cause of death information by incorporating the expected mortality rates of cancer individuals from population lifetables of the general population. A summary of the cancer population prognosis can be obtained using the marginal relative survival. To explore differences between exposure groups, such as socioeconomic groups, the difference in marginal relative survival between exposed and unexposed can be obtained and under assumptions is interpreted as the average causal effect of exposure to survival. In a modeling context, this is usually estimated by applying regression standardization as the average of the individual-specific estimates after fitting a relative survival model. Regression standardization yields an estimator that consistently estimates the causal effect under standard causal inference assumptions and if the relative survival model is correctly specified. We extend inverse probability weighting (IPW) and doubly robust standardization methods in the relative survival framework as additional valuable tools for obtaining average causal effects when correct model specification might not hold for the relative survival model. IPW yields an unbiased estimate of the average causal effect if a correctly specified model has been fitted for the exposure (propensity score) whereas doubly robust standardization requires that at least one of the propensity score model or the relative survival model is correctly specified. An example using data on melanoma is provided and a simulation study is conducted to investigate how sensitive are the methods to model misspecification, including different ways for obtaining standard errors.},
  archive      = {J_SIM},
  author       = {Elisavet Syriopoulou and Mark J. Rutherford and Paul C. Lambert},
  doi          = {10.1002/sim.9171},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6069-6092},
  shortjournal = {Stat. Med.},
  title        = {Inverse probability weighting and doubly robust standardization in the relative survival framework},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Best practices in statistical computing. <em>SIM</em>,
<em>40</em>(27), 6057–6068. (<a
href="https://doi.org/10.1002/sim.9169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is becoming increasingly complex, both in terms of the rich sources of data we have access to and the statistical and computational methods we can use on data. These factors create an ever-increasing risk for errors in code and the sensitivity of findings to data preparation and the execution of complex statistical and computing methods. The consequences of coding and data mistakes can be substantial. In this paper, we describe the key steps for implementing a code quality assurance (QA) process that researchers can follow to improve their coding practices throughout a project to assure the quality of the final data, code, analyses, and results. These steps include: (i) adherence to principles for code writing and style that follow best practices; (ii) clear written documentation that describes code, workflow, and key analytic decisions; (iii) careful version control; (iv) good data management; and (v) regular testing and review. Following these steps will greatly improve the ability of a study to assure results are accurate and reproducible. The responsibility for code QA falls not only on individual researchers but institutions, journals, and funding agencies as well.},
  archive      = {J_SIM},
  author       = {Ricardo Sanchez and Beth Ann Griffin and Joseph Pane and Daniel F. McCaffrey},
  doi          = {10.1002/sim.9169},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6057-6068},
  shortjournal = {Stat. Med.},
  title        = {Best practices in statistical computing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian hierarchical models for high-dimensional mediation
analysis with coordinated selection of correlated mediators.
<em>SIM</em>, <em>40</em>(27), 6038–6056. (<a
href="https://doi.org/10.1002/sim.9168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Bayesian high-dimensional mediation analysis to identify among a large set of correlated potential mediators the active ones that mediate the effect from an exposure variable to an outcome of interest. Correlations among mediators are commonly observed in modern data analysis; examples include the activated voxels within connected regions in brain image data, regulatory signals driven by gene networks in genome data, and correlated exposure data from the same source. When correlations are present among active mediators, mediation analysis that fails to account for such correlation can be suboptimal and may lead to a loss of power in identifying active mediators. Building upon a recent high-dimensional mediation analysis framework, we propose two Bayesian hierarchical models, one with a Gaussian mixture prior that enables correlated mediator selection and the other with a Potts mixture prior that accounts for the correlation among active mediators in mediation analysis. We develop efficient sampling algorithms for both methods. Various simulations demonstrate that our methods enable effective identification of correlated active mediators, which could be missed by using existing methods that assume prior independence among active mediators. The proposed methods are applied to the LIFECODES birth cohort and the Multi-Ethnic Study of Atherosclerosis (MESA) and identified new active mediators with important biological implications.},
  archive      = {J_SIM},
  author       = {Yanyi Song and Xiang Zhou and Jian Kang and Max T. Aung and Min Zhang and Wei Zhao and Belinda L. Needham and Sharon L. R. Kardia and Yongmei Liu and John D. Meeker and Jennifer A. Smith and Bhramar Mukherjee},
  doi          = {10.1002/sim.9168},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6038-6056},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical models for high-dimensional mediation analysis with coordinated selection of correlated mediators},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian nonparametric approach to dynamic item-response
modeling: An application to the GUSTO cohort study. <em>SIM</em>,
<em>40</em>(27), 6021–6037. (<a
href="https://doi.org/10.1002/sim.9167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical analysis of questionnaire data is often performed employing techniques from item-response theory. In this framework, it is possible to differentiate respondent profiles and characterize the questions (items) included in the questionnaire via interpretable parameters. These models are often crosssectional and aim at evaluating the performance of the respondents. The motivating application of this work is the analysis of psychometric questionnaires taken by a group of mothers at different time points and by their children at one later time point. The data are available through the GUSTO cohort study. To this end, we propose a Bayesian semiparametric model and extend the current literature by: (i) introducing temporal dependence among questionnaires taken at different time points; (ii) jointly modeling the responses to questionnaires taken from different, but related, groups of subjects (in our case mothers and children), introducing a further dependency structure and therefore sharing of information; (iii) allowing clustering of subjects based on their latent response profile. The proposed model is able to identify three main groups of mother/child pairs characterized by their response profiles. Furthermore, we report an interesting maternal reporting bias effect strongly affecting the clustering structure of the mother/child dyads.},
  archive      = {J_SIM},
  author       = {Andrea Cremaschi and Maria De Iorio and Yap Seng Chong and Birit Broekman and Michael J. Meaney and Michelle Z. L. Kee},
  doi          = {10.1002/sim.9167},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6021-6037},
  shortjournal = {Stat. Med.},
  title        = {A bayesian nonparametric approach to dynamic item-response modeling: An application to the GUSTO cohort study},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple imputation for handling missing outcome data in
randomized trials involving a mixture of independent and paired data.
<em>SIM</em>, <em>40</em>(27), 6008–6020. (<a
href="https://doi.org/10.1002/sim.9166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized trials involving independent and paired observations occur in many areas of health research, for example in paediatrics, where studies can include infants from both single and twin births. Multiple imputation (MI) is often used to address missing outcome data in randomized trials, yet its performance in trials with independent and paired observations, where design effects can be less than or greater than one, remains to be explored. Using simulated data and through application to a trial dataset, we investigated the performance of different methods of MI for a continuous or binary outcome when followed by analysis using generalized estimating equations to account for clustering due to the pairs. We found that imputing data separately for independent and paired data, with paired data imputed in wide format, was the best performing MI method, producing unbiased point and standard error estimates for the treatment effect throughout. Ignoring clustering in the imputation model performed well in settings where the design effect due to the inclusion of paired data was close to one, but otherwise led to moderately biased variance estimates. Including a random cluster effect in the imputation model led to slightly biased point estimates for binary outcome data and variance estimates that were too small in some settings. Based on these results, we recommend researchers impute independent and paired data separately where feasible to do so. The exception is if the design effect due to the inclusion of paired data is close to one, where ignoring clustering may be appropriate.},
  archive      = {J_SIM},
  author       = {Thomas R. Sullivan and Lisa N. Yelland and Margarita Moreno-Betancur and Katherine J. Lee},
  doi          = {10.1002/sim.9166},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {6008-6020},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation for handling missing outcome data in randomized trials involving a mixture of independent and paired data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing vaccine durability in randomized trials following
placebo crossover. <em>SIM</em>, <em>40</em>(27), 5983–6007. (<a
href="https://doi.org/10.1002/sim.9001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized vaccine trials are used to assess vaccine efficacy (VE) and to characterize the durability of vaccine-induced protection. If efficacy is demonstrated, the treatment of placebo volunteers becomes an issue. For COVID-19 vaccine trials, there is broad consensus that placebo volunteers should be offered a vaccine once efficacy has been established. This will likely lead to most placebo volunteers crossing over to the vaccine arm, thus complicating the assessment of long term durability. We show how to analyze durability following placebo crossover and demonstrate that the VE profile that would be observed in a placebo controlled trial is recoverable in a trial with placebo crossover. This result holds no matter when the crossover occurs and with no assumptions about the form of the efficacy profile. We only require that the VE profile applies to the newly vaccinated irrespective of the timing of vaccination. We develop different methods to estimate efficacy within the context of a proportional hazards regression model and explore via simulation the implications of placebo crossover for estimation of VE under different efficacy dynamics and study designs. We apply our methods to simulated COVID-19 vaccine trials with durable and waning VE and a total follow-up of 2 years.},
  archive      = {J_SIM},
  author       = {Jonathan Fintzi and Dean Follmann},
  doi          = {10.1002/sim.9001},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5983-6007},
  shortjournal = {Stat. Med.},
  title        = {Assessing vaccine durability in randomized trials following placebo crossover},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tutorial on individualized treatment effect prediction
from randomized trials with a binary endpoint. <em>SIM</em>,
<em>40</em>(26), 5961–5981. (<a
href="https://doi.org/10.1002/sim.9154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized trials typically estimate average relative treatment effects, but decisions on the benefit of a treatment are possibly better informed by more individualized predictions of the absolute treatment effect. In case of a binary outcome, these predictions of absolute individualized treatment effect require knowledge of the individual&#39;s risk without treatment and incorporation of a possibly differential treatment effect (ie, varying with patient characteristics). In this article, we lay out the causal structure of individualized treatment effect in terms of potential outcomes and describe the required assumptions that underlie a causal interpretation of its prediction. Subsequently, we describe regression models and model estimation techniques that can be used to move from average to more individualized treatment effect predictions. We focus mainly on logistic regression-based methods that are both well-known and naturally provide the required probabilistic estimates. We incorporate key components from both causal inference and prediction research to arrive at individualized treatment effect predictions. While the separate components are well known, their successful amalgamation is very much an ongoing field of research. We cut the problem down to its essentials in the setting of a randomized trial, discuss the importance of a clear definition of the estimand of interest, provide insight into the required assumptions, and give guidance with respect to modeling and estimation options. Simulated data illustrate the potential of different modeling options across scenarios that vary both average treatment effect and treatment effect heterogeneity. Two applied examples illustrate individualized treatment effect prediction in randomized trial data.},
  archive      = {J_SIM},
  author       = {Jeroen Hoogland and Joanna IntHout and Michail Belias and Maroeska M. Rovers and Richard D. Riley and Frank E. Harrell Jr and Karel G. M. Moons and Thomas P. A. Debray and Johannes B. Reitsma},
  doi          = {10.1002/sim.9154},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5961-5981},
  shortjournal = {Stat. Med.},
  title        = {A tutorial on individualized treatment effect prediction from randomized trials with a binary endpoint},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian inference of dependent kappa for binary ratings.
<em>SIM</em>, <em>40</em>(26), 5947–5960. (<a
href="https://doi.org/10.1002/sim.9165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical and social science research, reliability of testing methods measured through inter- and intraobserver agreement is critical in disease diagnosis. Often comparison of agreement across multiple testing methods is sought in situations where testing is carried out on the same experimental units rendering the outcomes to be correlated. In this article, we first developed a Bayesian method for comparing dependent agreement measures under a grouped data setting. Simulation studies showed that the proposed methodology outperforms the competing methods in terms of power, while maintaining a decent type I error rate. We further developed a Bayesian joint model for comparing dependent agreement measures adjusting for subject and rater-level heterogeneity. Simulation studies indicate that our model outperforms a competing method that is used in this context. The developed methodology was implemented on a key measure on a dichotomous rating scale from a study with six raters evaluating three classification methods for chest radiographs for pneumoconiosis developed by the International Labor Office.},
  archive      = {J_SIM},
  author       = {Ananda Sen and Pin Li and Wen Ye and Alfred Franzblau},
  doi          = {10.1002/sim.9165},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5947-5960},
  shortjournal = {Stat. Med.},
  title        = {Bayesian inference of dependent kappa for binary ratings},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling conditional reference regions: Application to
glycemic markers. <em>SIM</em>, <em>40</em>(26), 5926–5946. (<a
href="https://doi.org/10.1002/sim.9163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical decisions are taken based on the results of continuous diagnostic tests. Usually, only the results of one single test is taken into consideration, the interpretation of which requires a reference range for the healthy population. However, the use of two different tests, can be necessary in the diagnosis of certain diseases. This obliges a bivariate reference region be available for their interpretation. It should also be remembered that reference regions may depend on patient variables (eg, age and sex) independent of the suspected disease. However, few proposals have been made regarding the statistical modeling of such reference regions, and those put forward have always assumed a Gaussian distribution, which can be rather restrictive. The present work describes a new statistical method that allows such reference regions to be estimated with no insistence on the results being normally distributed. The proposed method is based on a bivariate location-scale model that provides probabilistic regions covering a specific percentage of the bivariate data, dependent on certain covariates. The reference region is estimated nonparametrically and the nonlinear effects of continuous covariates via polynomial kernel smoothers in additive models. The bivariate model is estimated using a backfitting algorithm, and the optimal smoothing parameters of the kernel smoothers selected by cross-validation. The model performed satisfactorily in simulation studies under the assumption of non-Gaussian conditions. Finally, the proposed methodology was found to be useful in estimating a reference region for two continuous diagnostic tests for diabetes (fasting plasma glucose and glycated hemoglobin), taking into account the age of the patient.},
  archive      = {J_SIM},
  author       = {Óscar Lado-Baleato and Javier Roca-Pardiñas and Carmen Cadarso-Suárez and Francisco Gude},
  doi          = {10.1002/sim.9163},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5926-5946},
  shortjournal = {Stat. Med.},
  title        = {Modeling conditional reference regions: Application to glycemic markers},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible co-data learning for high-dimensional prediction.
<em>SIM</em>, <em>40</em>(26), 5910–5925. (<a
href="https://doi.org/10.1002/sim.9162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical research often focuses on complex traits in which many variables play a role in mechanisms driving, or curing, diseases. Clinical prediction is hard when data is high-dimensional, but additional information, like domain knowledge and previously published studies, may be helpful to improve predictions. Such complementary data, or co-data, provide information on the covariates, such as genomic location or P -values from external studies. We use multiple and various co-data to define possibly overlapping or hierarchically structured groups of covariates. These are then used to estimate adaptive multi-group ridge penalties for generalized linear and Cox models. Available group adaptive methods primarily target for settings with few groups, and therefore likely overfit for non-informative, correlated or many groups, and do not account for known structure on group level. To handle these issues, our method combines empirical Bayes estimation of the hyperparameters with an extra level of flexible shrinkage. This renders a uniquely flexible framework as any type of shrinkage can be used on the group level. We describe various types of co-data and propose suitable forms of hypershrinkage. The method is very versatile, as it allows for integration and weighting of multiple co-data sets, inclusion of unpenalized covariates and posterior variable selection. For three cancer genomics applications we demonstrate improvements compared to other models in terms of performance, variable selection stability and validation.},
  archive      = {J_SIM},
  author       = {Mirrelijn M. van Nee and Lodewyk F.A. Wessels and Mark A. van de Wiel},
  doi          = {10.1002/sim.9162},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5910-5925},
  shortjournal = {Stat. Med.},
  title        = {Flexible co-data learning for high-dimensional prediction},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bias correction method in meta-analysis of randomized
clinical trials with no adjustments for zero-inflated outcomes.
<em>SIM</em>, <em>40</em>(26), 5894–5909. (<a
href="https://doi.org/10.1002/sim.9161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical endpoint measures, such as the number of standard drinks consumed per week or the number of days that patients stayed in the hospital, are count data with excessive zeros. However, the zero-inflated nature of such outcomes is sometimes ignored in analyses of clinical trials. This leads to biased estimates of study-level intervention effect and, consequently, a biased estimate of the overall intervention effect in a meta-analysis. The current study proposes a novel statistical approach, the Zero-inflation Bias Correction (ZIBC) method, that can account for the bias introduced when using the Poisson regression model, despite a high rate of inflated zeros in the outcome distribution of a randomized clinical trial. This correction method only requires summary information from individual studies to correct intervention effect estimates as if they were appropriately estimated using the zero-inflated Poisson regression model, thus it is attractive for meta-analysis when individual participant-level data are not available in some studies. Simulation studies and real data analyses showed that the ZIBC method performed well in correcting zero-inflation bias in most situations.},
  archive      = {J_SIM},
  author       = {Zhengyang Zhou and Minge Xie and David Huh and Eun-Young Mun},
  doi          = {10.1002/sim.9161},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5894-5909},
  shortjournal = {Stat. Med.},
  title        = {A bias correction method in meta-analysis of randomized clinical trials with no adjustments for zero-inflated outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composite likelihood inference for ordinal periodontal data
with replicated spatial patterns. <em>SIM</em>, <em>40</em>(26),
5871–5893. (<a href="https://doi.org/10.1002/sim.9160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial ordinal data observed separately for multiple subjects are common in biomedical research, yet statistical methodology for such ordinal data analysis is limited. The existing methodology often assumes a single realization of spatial ordinal data without replications, a commonplace in disease mapping studies, and thus are not directly applicable. Motivated by a dataset evaluating periodontal disease (PD) status, we propose a multisubject spatial ordinal model that assumes a geostatistical spatial structure within a regression framework through an elegant latent variable representation. For achieving computational scalability within a classical inferential framework, we develop a maximum composite likelihood method for parameter estimation, and establish the asymptotic properties of the parameter estimates. Another major contribution is the development of model diagnostic measures for our dependent data scenario using generalized surrogate residuals. A simulation study suggests sound finite sample properties of the proposed methods. We also illustrate our proposed methodology via application to the motivating PD dataset. A companion R package clordr is available for easy implementation.},
  archive      = {J_SIM},
  author       = {Pingping Wang and Ting Fung Ma and Dipankar Bandyopadhyay and Yincai Tang and Jun Zhu},
  doi          = {10.1002/sim.9160},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5871-5893},
  shortjournal = {Stat. Med.},
  title        = {Composite likelihood inference for ordinal periodontal data with replicated spatial patterns},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian approach for estimating typhoid fever incidence
from large-scale facility-based passive surveillance data. <em>SIM</em>,
<em>40</em>(26), 5853–5870. (<a
href="https://doi.org/10.1002/sim.9159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions about typhoid fever prevention and control are based on estimates of typhoid incidence and their uncertainty. Lack of specific clinical diagnostic criteria, poorly sensitive diagnostic tests, and scarcity of accurate and complete datasets contribute to difficulties in calculating age-specific population-level typhoid incidence. Using data from the Strategic Typhoid Alliance across Africa and Asia program, we integrated demographic censuses, healthcare utilization surveys, facility-based surveillance, and serological surveillance from Malawi, Nepal, and Bangladesh to account for under-detection of cases. We developed a Bayesian approach that adjusts the count of reported blood-culture-positive cases for blood culture detection, blood culture collection, and healthcare seeking—and how these factors vary by age—while combining information from prior published studies. We validated the model using simulated data. The ratio of observed to adjusted incidence rates was 7.7 (95% credible interval [CrI]: 6.0-12.4) in Malawi, 14.4 (95% CrI: 9.3-24.9) in Nepal, and 7.0 (95% CrI: 5.6-9.2) in Bangladesh. The probability of blood culture collection led to the largest adjustment in Malawi, while the probability of seeking healthcare contributed the most in Nepal and Bangladesh; adjustment factors varied by age. Adjusted incidence rates were within or below the seroincidence rate limits of typhoid infection. Estimates of blood-culture-confirmed typhoid fever without these adjustments results in considerable underestimation of the true incidence of typhoid fever. Our approach allows each phase of the reporting process to be synthesized to estimate the adjusted incidence of typhoid fever while correctly characterizing uncertainty, which can inform decision-making for typhoid prevention and control.},
  archive      = {J_SIM},
  author       = {Maile T. Phillips and James E. Meiring and Merryn Voysey and Joshua L. Warren and Stephen Baker and Buddha Basnyat and John D. Clemens and Christiane Dolecek and Sarah J. Dunstan and Gordon Dougan and Melita A. Gordon and Deus Thindwa and Robert S. Heyderman and Kathryn E. Holt and Firdausi Qadri and Andrew J. Pollard and Virginia E. Pitzer},
  doi          = {10.1002/sim.9159},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5853-5870},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach for estimating typhoid fever incidence from large-scale facility-based passive surveillance data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias correction for estimates from linear excess relative
risk models in small case-control studies. <em>SIM</em>,
<em>40</em>(26), 5831–5852. (<a
href="https://doi.org/10.1002/sim.9158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiologic studies conducted to quantify the impact of radiation dose d on an outcome typically model the hazard ratio (HR) for association using a linear term, HR ( d ) = 1 + β ⁢ d , via a linear excess relative risk (ERR) model, based on biological considerations. To study associations of risk of a second cancer with radiation treatment for a first cancer, several nested case-control designs to estimate have been proposed that use refined doses received by different locations in the organ of interest. Here we first evaluated the small sample bias in maximum likelihood estimates of for the linear ERR model using location-specific radiation doses in simulations. As we found substantial upward bias for studies of realistic sample sizes (more than 50% relative bias for studies with 75 cases), we also proposed and investigated several approaches to correct this bias. We studied first and second order jackknife bias corrections and we derived a modified set of score functions under retrospective case-control sampling, from which we directly obtained bias-corrected estimates. In simulations based on doses from a study of stomach cancer among testicular cancer survivors and synthetically generated data, neither the first nor second order jackknife bias correction performed well. Estimates based on the modified score equations corrected the bias much better, albeit not completely, and were numerically much more stable.},
  archive      = {J_SIM},
  author       = {Sander Roberti and Flora E. van Leeuwen and Michael Hauptmann and Ruth M. Pfeiffer},
  doi          = {10.1002/sim.9158},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5831-5852},
  shortjournal = {Stat. Med.},
  title        = {Bias correction for estimates from linear excess relative risk models in small case-control studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pleiotropy robust methods for multivariable mendelian
randomization. <em>SIM</em>, <em>40</em>(26), 5813–5830. (<a
href="https://doi.org/10.1002/sim.9156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization is a powerful tool for inferring the presence, or otherwise, of causal effects from observational data. However, the nature of genetic variants is such that pleiotropy remains a barrier to valid causal effect estimation. There are many options in the literature for pleiotropy robust methods when studying the effects of a single risk factor on an outcome. However, there are few pleiotropy robust methods in the multivariable setting, that is, when there are multiple risk factors of interest. In this article we introduce three methods which build on common approaches in the univariable setting: MVMR-Robust; MVMR-Median; and MVMR-Lasso. We discuss the properties of each of these methods and examine their performance in comparison to existing approaches in a simulation study. MVMR-Robust is shown to outperform existing outlier robust approaches when there are low levels of pleiotropy. MVMR-Lasso provides the best estimation in terms of mean squared error for moderate to high levels of pleiotropy, and can provide valid inference in a three sample setting. MVMR-Median performs well in terms of estimation across all scenarios considered, and provides valid inference up to a moderate level of pleiotropy. We demonstrate the methods in an applied example looking at the effects of intelligence, education and household income on the risk of Alzheimer&#39;s disease.},
  archive      = {J_SIM},
  author       = {Andrew J. Grant and Stephen Burgess},
  doi          = {10.1002/sim.9156},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5813-5830},
  shortjournal = {Stat. Med.},
  title        = {Pleiotropy robust methods for multivariable mendelian randomization},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Restricted sub-tree learning to estimate an optimal dynamic
treatment regime using observational data. <em>SIM</em>,
<em>40</em>(26), 5796–5812. (<a
href="https://doi.org/10.1002/sim.9155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs), consisting of a sequence of tailored treatment decision rules that span multiple stages of care, present a unique opportunity in our drive toward personalized medicine. Given that estimation of optimal DTRs is often exploratory and communication with clinicians is vital, robust and flexible methods that yield interpretable results are needed. Tree-based methods utilizing a purity measure defined on the full set of covariates have enjoyed much success in meeting this goal. Often, however, it is necessary for clinical, practical, or ethical reasons to restrict certain covariates that should be used when making treatment decisions. Herein we present restricted sub-tree learning (ReST-L), a flexible and robust, sub-tree-based method to estimate an optimal multi-stage multi-treatment DTR that enables restrictions to the set of prespecified candidate tailoring variables. ReST-L employs a purity measure derived from an augmented inverse probability weighted estimator for the counterfactual mean outcome, using observational data to build multi-stage decision trees that are restricted in sub-tree spaces defined by the corresponding prescriptive covariates. We show that ReST-L is able to correctly estimate the optimal DTR searching over a large number of variables with relatively small sample sizes and improves upon competing estimation methods. We demonstrate the utility of ReST-L to estimate a two-stage fluid resuscitation strategy for patients admitted to an intensive care unit with acute emergent sepsis.},
  archive      = {J_SIM},
  author       = {Kelly Speth and Lu Wang},
  doi          = {10.1002/sim.9155},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5796-5812},
  shortjournal = {Stat. Med.},
  title        = {Restricted sub-tree learning to estimate an optimal dynamic treatment regime using observational data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and flexible inference for the covariate-specific
receiver operating characteristic curve. <em>SIM</em>, <em>40</em>(26),
5779–5795. (<a href="https://doi.org/10.1002/sim.9153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic tests are of critical importance in health care and medical research. Motivated by the impact that atypical and outlying test outcomes might have on the assessment of the discriminatory ability of a diagnostic test, we develop a robust and flexible model for conducting inference about the covariate-specific receiver operating characteristic (ROC) curve that safeguards against outlying test results while also accommodating for possible nonlinear effects of the covariates. Specifically, we postulate a location-scale regression model for the test outcomes in both the diseased and nondiseased populations, combining additive regression B-splines and M-estimation for the regression function, while the distribution of the error term is estimated via a weighted empirical distribution function of the standardized residuals. The results of the simulation study show that our approach successfully recovers the true covariate-specific area under the ROC curve on a variety of conceivable test outcomes contamination scenarios. Our method is applied to a dataset derived from a prostate cancer study where we seek to assess the ability of the Prostate Health Index to discriminate between men with and without Gleason 7 or above prostate cancer, and if and how such discriminatory capacity changes with age.},
  archive      = {J_SIM},
  author       = {Vanda Inácio and Vanda M. Lourenço and Miguel de Carvalho and Richard A. Parker and Vincent Gnanapragasam},
  doi          = {10.1002/sim.9153},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5779-5795},
  shortjournal = {Stat. Med.},
  title        = {Robust and flexible inference for the covariate-specific receiver operating characteristic curve},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When should matching be used in the design of cluster
randomized trials? <em>SIM</em>, <em>40</em>(26), 5765–5778. (<a
href="https://doi.org/10.1002/sim.9152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For cluster randomized trials (CRTs) with a small number of clusters, the matched-pair (MP) design, where clusters are paired before randomizing one to each trial arm, is often recommended to minimize imbalance on known prognostic factors, add face-validity to the study, and increase efficiency, provided the analysis recognizes the matching. Little evidence exists to guide decisions on when to use matching. We used simulation to compare the efficiency of the MP design with the stratified and simple designs, based on the mean confidence interval width of the estimated intervention effect. Matched and unmatched analyses were used for the MP design; a stratified analysis was used for the stratified design; and analyses without and with post-stratification adjustment for factors that would otherwise have been used for restricted allocation were used for the simple design. Results showed the MP design was generally the most efficient for CRTs with 10 or more pairs when the correlation between cluster-level outcomes within pairs (matching correlation) was moderate to strong (0.3-0.5). There was little gain in efficiency for the MP or stratified designs compared to simple randomization when the matching correlation was weak (0.05-0.1). For trials with four pairs of clusters, the simple and stratified designs were more efficient than the MP design because greater degrees of freedom were available for the analysis, although an unmatched analysis of the MP design recovered precision for weak matching correlations. Practical guidance on choosing between the MP, stratified, and simple designs is provided.},
  archive      = {J_SIM},
  author       = {Patty Chondros and Obioha C. Ukoumunne and Jane M. Gunn and John B. Carlin},
  doi          = {10.1002/sim.9152},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5765-5778},
  shortjournal = {Stat. Med.},
  title        = {When should matching be used in the design of cluster randomized trials?},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the marginal effect of a continuous exposure on
an ordinal outcome using data subject to covariate-driven treatment and
visit processes. <em>SIM</em>, <em>40</em>(26), 5746–5764. (<a
href="https://doi.org/10.1002/sim.9151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the statistical literature, a number of methods have been proposed to ensure valid inference about marginal effects of variables on a longitudinal outcome in settings with irregular monitoring times. However, the potential biases due to covariate-driven monitoring times and confounding have rarely been considered simultaneously, and never in a setting with an ordinal outcome and a continuous exposure. In this work, we propose and demonstrate a methodology for causal inference in such a setting, relying on a proportional odds model to study the effect of the exposure on the outcome. Irregular observation times are considered via a proportional rate model, and a generalization of inverse probability of treatment weights is used to account for the continuous exposure. We motivate our methodology by the estimation of the marginal (causal) effect of the time spent on video or computer games on suicide attempts in the Add Health study, a longitudinal study in the United States. Although in the Add Health data, observation times are prespecified, our proposed approach is applicable even in more general settings such as when analyzing data from electronic health records where observations are highly irregular. In simulation studies, we let observation times vary across individuals and demonstrate that not accounting for biasing imbalances due to the monitoring and the exposure schemes can bias the estimate for the marginal odds ratio of exposure.},
  archive      = {J_SIM},
  author       = {Janie Coulombe and Erica E. M. Moodie and Robert W. Platt},
  doi          = {10.1002/sim.9151},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5746-5764},
  shortjournal = {Stat. Med.},
  title        = {Estimating the marginal effect of a continuous exposure on an ordinal outcome using data subject to covariate-driven treatment and visit processes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective disease surveillance by using covariate
information. <em>SIM</em>, <em>40</em>(26), 5725–5745. (<a
href="https://doi.org/10.1002/sim.9150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective surveillance of infectious diseases, cancers, and other deadly diseases is critically important for public health and safety of our society. Incidence data of such diseases are often collected spatially from different clinics and hospitals through a regional, national or global disease reporting system. In such a system, new batches of data keep being collected over time, and a decision needs to be made immediately after new data are collected regarding whether there is a disease outbreak at the current time point. This is the disease surveillance problem that will be focused in this article. There are some existing methods for solving this problem, most of which use the disease incidence data only. In practice, however, disease incidence is often associated with some covariates, including the air temperature, humidity, and other weather or environmental conditions. In this article, we develop a new methodology for disease surveillance which can make use of helpful covariate information to improve its effectiveness. A novelty of this new method is behind the property that only those covariate information that is associated with a true disease outbreak can help trigger a signal. The new method can accommodate seasonality, spatio-temporal data correlation, and nonparametric data distribution. These features make it feasible to use in many real applications.},
  archive      = {J_SIM},
  author       = {Peihua Qiu and Kai Yang},
  doi          = {10.1002/sim.9150},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5725-5745},
  shortjournal = {Stat. Med.},
  title        = {Effective disease surveillance by using covariate information},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of semiparametric approaches to evaluate
composite endpoints in heart failure trials. <em>SIM</em>,
<em>40</em>(26), 5702–5724. (<a
href="https://doi.org/10.1002/sim.9149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heart failure (HF) trials efficacy is usually assessed by a composite endpoint including cardiovascular death (CVD) and heart failure hospitalizations (HFHs), which has traditionally been evaluated with a time-to-first-event analysis based on a Cox model. As a considerable fraction of events is ignored that way, methods for recurrent events were suggested, among others the semiparametric proportional rates models by Lin, Wei, Yang, and Ying (LWYY model) and Mao and Lin (Mao-Lin model). In our work we apply least false parameter theory to explain the behavior of the composite treatment effect estimates resulting from the Cox model, the LWYY model, and the Mao-Lin model in clinically relevant scenarios parameterized through joint frailty models. These account for both different treatment effects on the two outcomes (CVD, HFHs) and the positive correlation between their risk rates. For the important setting of beneficial outcome-specific treatment effects we show that the correlation results in composite treatment effect estimates, which are decreasing with trial duration. The estimate from the Cox model is affected more by the attenuation than the estimates from the recurrent event models, which both demonstrate very similar behavior. Since the Mao-Lin model turns out to be less sensitive to harmful effects on mortality, we conclude that, among the three investigated approaches, the LWYY model is the most appropriate one for the composite endpoint in HF trials. Our investigations are motivated and compared with empirical results from the PARADIGM-HF trial (ClinicalTrials.gov identifier: NCT01035255), a large multicenter trial including 8399 chronic HF patients.},
  archive      = {J_SIM},
  author       = {Gerrit Toenges and Tobias Mütze and Antje Jahn-Eimermacher},
  doi          = {10.1002/sim.9149},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5702-5724},
  shortjournal = {Stat. Med.},
  title        = {A comparison of semiparametric approaches to evaluate composite endpoints in heart failure trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence intervals of prediction accuracy measures for
multivariable prediction models based on the bootstrap-based optimism
correction methods. <em>SIM</em>, <em>40</em>(26), 5691–5701. (<a
href="https://doi.org/10.1002/sim.9148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In assessing prediction accuracy of multivariable prediction models, optimism corrections are essential for preventing biased results. However, in most published papers of clinical prediction models, the point estimates of the prediction accuracy measures are corrected by adequate bootstrap-based correction methods, but their confidence intervals are not corrected, for example, the DeLong&#39;s confidence interval is usually used for assessing the C -statistic. These naïve methods do not adjust for the optimism bias and do not account for statistical variability in the estimation of parameters in the prediction models. Therefore, their coverage probabilities of the true value of the prediction accuracy measure can be seriously below the nominal level (eg, 95%). In this article, we provide two generic bootstrap methods, namely, (1) location-shifted bootstrap confidence intervals and (2) two-stage bootstrap confidence intervals, that can be generally applied to the bootstrap-based optimism correction methods, that is, the Harrell&#39;s bias correction, 0.632, and 0.632+ methods. In addition, they can be widely applied to various methods for prediction model development involving modern shrinkage methods such as the ridge and lasso regressions. Through numerical evaluations by simulations, the proposed confidence intervals showed favorable coverage performances. Besides, the current standard practices based on the optimism-uncorrected methods showed serious undercoverage properties. To avoid erroneous results, the optimism-uncorrected confidence intervals should not be used in practice, and the adjusted methods are recommended instead. We also developed the R package predboot for implementing these methods ( https://github.com/nomahi/predboot ). The effectiveness of the proposed methods are illustrated via applications to the GUSTO-I clinical trial.},
  archive      = {J_SIM},
  author       = {Hisashi Noma and Tomohiro Shinozaki and Katsuhiro Iba and Satoshi Teramukai and Toshi A. Furukawa},
  doi          = {10.1002/sim.9148},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {5691-5701},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals of prediction accuracy measures for multivariable prediction models based on the bootstrap-based optimism correction methods},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayes estimate of primary threshold in clusterwise
functional magnetic resonance imaging inferences. <em>SIM</em>,
<em>40</em>(25), 5673–5689. (<a
href="https://doi.org/10.1002/sim.9147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clusterwise statistical inference is the most widely used technique for functional magnetic resonance imaging (fMRI) data analyses. Clusterwise statistical inference consists of two steps: (i) primary thresholding that excludes less significant voxels by a prespecified cut-off (eg, p &lt; . 001 ); and (ii) clusterwise thresholding that controls the familywise error rate caused by clusters consisting of false positive suprathreshold voxels. The selection of the primary threshold is critical because it determines both statistical power and false discovery rate (FDR). However, in most existing statistical packages, the primary threshold is selected based on prior knowledge (eg, ) without taking into account the information in the data. In this article, we propose a data-driven approach to algorithmically select the optimal primary threshold based on an empirical Bayes framework. We evaluate the proposed model using extensive simulation studies and real fMRI data. In the simulation, we show that our method can effectively increase statistical power by 20% to over 100% while effectively controlling the FDR. We then investigate the brain response to the dose-effect of chlorpromazine in patients with schizophrenia by analyzing fMRI scans and generate consistent results.},
  archive      = {J_SIM},
  author       = {Yunjiang Ge and Stephanie Hare and Gang Chen and James A. Waltz and Peter Kochunov and L. Elliot Hong and Shuo Chen},
  doi          = {10.1002/sim.9147},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5673-5689},
  shortjournal = {Stat. Med.},
  title        = {Bayes estimate of primary threshold in clusterwise functional magnetic resonance imaging inferences},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unit information prior for adaptive information borrowing
from multiple historical datasets. <em>SIM</em>, <em>40</em>(25),
5657–5672. (<a href="https://doi.org/10.1002/sim.9146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, there often exist multiple historical studies for the same or related treatment investigated in the current trial. Incorporating historical data in the analysis of the current study is of great importance, as it can help to gain more information, improve efficiency, and provide a more comprehensive evaluation of treatment. Enlightened by the unit information prior (UIP) concept in the reference Bayesian test, we propose a new informative prior called UIP from an information perspective that can adaptively borrow information from multiple historical datasets. We consider both binary and continuous data and also extend the new UIP to linear regression settings. Extensive simulation studies demonstrate that our method is comparable to other commonly used informative priors, while the interpretation of UIP is intuitive and its implementation is relatively easy. One distinctive feature of UIP is that its construction only requires summary statistics commonly reported in the literature rather than the patient-level data. By applying our UIP to phase III clinical trials for investigating the efficacy of memantine in Alzheimer&#39;s disease, we illustrate its ability to adaptively borrow information from multiple historical datasets. The Python codes for simulation studies and the real data application are available at https://github.com/JINhuaqing/UIP.},
  archive      = {J_SIM},
  author       = {Huaqing Jin and Guosheng Yin},
  doi          = {10.1002/sim.9146},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5657-5672},
  shortjournal = {Stat. Med.},
  title        = {Unit information prior for adaptive information borrowing from multiple historical datasets},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A forward search algorithm for detecting extreme study
effects in network meta-analysis. <em>SIM</em>, <em>40</em>(25),
5642–5656. (<a href="https://doi.org/10.1002/sim.9145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a quantitative synthesis of studies via meta-analysis, it is possible that some studies provide a markedly different relative treatment effect or have a large impact on the summary estimate and/or heterogeneity. Extreme study effects (outliers) can be detected visually with forest/funnel plots and by using statistical outlying detection methods. A forward search (FS) algorithm is a common outlying diagnostic tool recently extended to meta-analysis. FS starts by fitting the assumed model to a subset of the data which is gradually incremented by adding the remaining studies according to their closeness to the postulated data-generating model. At each step of the algorithm, parameter estimates, measures of fit (residuals, likelihood contributions), and test statistics are being monitored and their sharp changes are used as an indication for outliers. In this article, we extend the FS algorithm to network meta-analysis (NMA). In NMA, visualization of outliers is more challenging due to the multivariate nature of the data and the fact that studies contribute both directly and indirectly to the network estimates. Outliers are expected to contribute not only to heterogeneity but also to inconsistency, compromising the NMA results. The FS algorithm was applied to real and artificial networks of interventions that include outliers. We developed an R package (NMAoutlier) to allow replication and dissemination of the proposed method. We conclude that the FS algorithm is a visual diagnostic tool that helps to identify studies that are a potential source of heterogeneity and inconsistency.},
  archive      = {J_SIM},
  author       = {Maria Petropoulou and Georgia Salanti and Gerta Rücker and Guido Schwarzer and Irini Moustaki and Dimitris Mavridis},
  doi          = {10.1002/sim.9145},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5642-5656},
  shortjournal = {Stat. Med.},
  title        = {A forward search algorithm for detecting extreme study effects in network meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal mediation analysis with latent subgroups.
<em>SIM</em>, <em>40</em>(25), 5628–5641. (<a
href="https://doi.org/10.1002/sim.9144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, the causal mediation effect might be heterogeneous across individuals in the study population due to each study subject&#39;s unique characteristics. While individuals&#39; mediation effects may differ from each other, it is often reasonable and more interpretable to assume that individuals belong to several distinct latent subgroups with similar attributes. In this article, we first show that the subgroup-specific mediation effect can be identified under the group-specific sequential ignorability assumptions. Then, we propose a simple mixture modeling approach to account for the latent subgroup structure where each mixture component corresponds to one latent subgroup in the linear structural equation model framework. Model parameters can be estimated using the standard expectation-maximization (EM) algorithm. Each individual&#39;s subgroup membership can be inferred based on the posterior probability. We propose to use the singular Bayesian information criterion to consistently select the number of latent subgroups by recognizing that the Fisher information matrix for mixture models might be singular. We then propose to use nonparametric bootstrap method to compute standard errors and confidence intervals. We conducted simulation studies to evaluate the empirical performance of our proposed method named iMed . Finally, we reanalyzed a DNA methylation data set from the Normative Aging Study and found that the mediation effects of two well-documented DNA methylation CpG sites are heterogeneous across two latent subgroups in the causal pathway from smoking behavior to lung function. We also developed an R package iMed for public use.},
  archive      = {J_SIM},
  author       = {WenWu Wang and Jinfeng Xu and Joel Schwartz and Andrea Baccarelli and Zhonghua Liu},
  doi          = {10.1002/sim.9144},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5628-5641},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis with latent subgroups},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Connecting instrumental variable methods for causal
inference to the estimand framework. <em>SIM</em>, <em>40</em>(25),
5605–5627. (<a href="https://doi.org/10.1002/sim.9143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference methods are gaining increasing prominence in pharmaceutical drug development in light of the recently published addendum on estimands and sensitivity analysis in clinical trials to the E9 guideline of the International Council for Harmonisation. The E9 addendum emphasises the need to account for post-randomization or ‘intercurrent’ events that can potentially influence the interpretation of a treatment effect estimate at a trial&#39;s conclusion. Instrumental Variables (IV) methods have been used extensively in economics, epidemiology, and academic clinical studies for ‘causal inference,’ but less so in the pharmaceutical industry setting until now. In this tutorial article we review the basic tools for causal inference, including graphical diagrams and potential outcomes, as well as several conceptual frameworks that an IV analysis can sit within. We discuss in detail how to map these approaches to the Treatment Policy, Principal Stratum and Hypothetical ‘estimand strategies’ introduced in the E9 addendum, and provide details of their implementation using standard regression models. Specific attention is given to discussing the assumptions each estimation strategy relies on in order to be consistent, the extent to which they can be empirically tested and sensitivity analyses in which specific assumptions can be relaxed. We finish by applying the methods described to simulated data closely matching two recent pharmaceutical trials to further motivate and clarify the ideas.},
  archive      = {J_SIM},
  author       = {Jack Bowden and Björn Bornkamp and Ekkehard Glimm and Frank Bretz},
  doi          = {10.1002/sim.9143},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5605-5627},
  shortjournal = {Stat. Med.},
  title        = {Connecting instrumental variable methods for causal inference to the estimand framework},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A permutation-based approach for heterogeneous
meta-analyses of rare events. <em>SIM</em>, <em>40</em>(25), 5587–5604.
(<a href="https://doi.org/10.1002/sim.9142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly widespread use of meta-analysis has led to growing interest in meta-analytic methods for rare events and sparse data. Conventional approaches tend to perform very poorly in such settings. Recent work in this area has provided options for sparse data, but these are still often hampered when heterogeneity across the available studies differs based on treatment group. We propose a permutation-based approach based on conditional logistic regression that accommodates this common contingency, providing more reliable statistical tests when such patterns of heterogeneity are observed. We find that commonly used methods can yield highly inflated Type I error rates, low confidence interval coverage, and bias when events are rare and non-negligible heterogeneity is present. Our method often produces much lower Type I error rates and higher confidence interval coverage than traditional methods in these circumstances. We illustrate the utility of our method by comparing it to several other methods via a simulation study and analyzing an example data set, which assess the use of antibiotics to prevent acute rheumatic fever.},
  archive      = {J_SIM},
  author       = {Brinley N. Zabriskie and Chris Corcoran and Pralay Senchaudhuri},
  doi          = {10.1002/sim.9142},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5587-5604},
  shortjournal = {Stat. Med.},
  title        = {A permutation-based approach for heterogeneous meta-analyses of rare events},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating adjusted risk differences by multiply-imputing
missing control binary potential outcomes following propensity
score-matching. <em>SIM</em>, <em>40</em>(25), 5565–5586. (<a
href="https://doi.org/10.1002/sim.9141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a new method to combine propensity-score matching with regression adjustment in treatment-control studies when outcomes are binary by multiply imputing potential outcomes under control for the matched treated subjects. This enables the estimation of clinically meaningful measures of effect such as the risk difference. We used Monte Carlo simulation to explore the effect of the number of imputed potential outcomes under control for the matched treated subjects on inferences about the risk difference. We found that imputing potential outcomes under control (either single imputation or multiple imputation) resulted in a substantial reduction in bias compared with what was achieved using conventional nearest neighbor matching alone. Increasing the number of imputed potential outcomes under control resulted in more efficient estimation, with more efficient estimation of the estimated risk difference when increasing the number of the imputed potential outcomes. The greatest relative increase in efficiency was achieved by imputing five potential outcomes; once 20 outcomes under control were imputed for each matched treated subject, further improvements in efficiency were negligible. We also examined the effect of the number of these imputed potential outcomes on: (i) estimated standard errors; (ii) mean squared error; (iii) coverage of estimated confidence intervals. We illustrate the application of the method by estimating the effect on the risk of death within 1 year of prescribing beta-blockers to patients discharged from hospital with a diagnosis of heart failure.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Donald B. Rubin and Neal Thomas},
  doi          = {10.1002/sim.9141},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5565-5586},
  shortjournal = {Stat. Med.},
  title        = {Estimating adjusted risk differences by multiply-imputing missing control binary potential outcomes following propensity score-matching},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A graph convolutional neural network for gene expression
data analysis with multiple gene networks. <em>SIM</em>,
<em>40</em>(25), 5547–5564. (<a
href="https://doi.org/10.1002/sim.9140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral graph convolutional neural networks (GCN) are proposed to incorporate important information contained in graphs such as gene networks. In a standard spectral GCN, there is only one gene network to describe the relationships among genes. However, for genomic applications, due to condition- or tissue-specific gene function and regulation, multiple gene networks may be available; it is unclear how to apply GCNs to disease classification with multiple networks. Besides, which gene networks may provide more effective prior information for a given learning task is unknown a priori and is not straightforward to discover in many cases. A deep multiple graph convolutional neural network is therefore developed here to meet the challenge. The new approach not only computes a feature of a gene as the weighted average of those of itself and its neighbors through spectral GCNs, but also extracts features from gene-specific expression (or other feature) profiles via a feed-forward neural networks (FNN). We also provide two measures, the importance of a given gene and the relative importance score of each gene network, for the genes&#39; and gene networks&#39; contributions, respectively, to the learning task. To evaluate the new method, we conduct real data analyses using several breast cancer and diffuse large B-cell lymphoma datasets and incorporating multiple gene networks obtained from “GIANT 2.0” Compared with the standard FNN, GCN, and random forest, the new method not only yields high classification accuracy but also prioritizes the most important genes confirmed to be highly associated with cancer, strongly suggesting the usefulness of the new method in incorporating multiple gene networks.},
  archive      = {J_SIM},
  author       = {Hu Yang and Zhong Zhuang and Wei Pan},
  doi          = {10.1002/sim.9140},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5547-5564},
  shortjournal = {Stat. Med.},
  title        = {A graph convolutional neural network for gene expression data analysis with multiple gene networks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). K-resolution sequential randomization procedure to improve
covariates balance in a randomized experiment. <em>SIM</em>,
<em>40</em>(25), 5534–5546. (<a
href="https://doi.org/10.1002/sim.9139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing allocation of assigning units to two treatment groups to minimize the allocation differences is important in biomedical research. The complete randomization, rerandomization, and pairwise sequential randomization (PSR) procedures can be employed to balance the allocation. However, the first two do not allow a large number of covariates. In this article, we generalize the PSR procedure and propose a k -resolution sequential randomization ( k -RSR) procedure by minimizing the Mahalanobis distance between both groups with equal group size. The proposed method can be used to achieve adequate balance and obtain a reasonable estimate of treatment effect. Compared to PSR, k -RSR is more likely to achieve the optimal value theoretically. Extensive simulation studies are conducted to show the superiorities of k -RSR and applications to the clinical synthetic data and GAW16 data further illustrate the methods.},
  archive      = {J_SIM},
  author       = {Mingya Long and Liuquan Sun and Qizhai Li},
  doi          = {10.1002/sim.9139},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5534-5546},
  shortjournal = {Stat. Med.},
  title        = {K-resolution sequential randomization procedure to improve covariates balance in a randomized experiment},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Window mean survival time. <em>SIM</em>, <em>40</em>(25),
5521–5533. (<a href="https://doi.org/10.1002/sim.9138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a class of alternative estimates and tests to restricted mean survival time (RMST) which improves power in numerous survival scenarios while maintaining a level of interpretability. The industry standards for interpretable hypothesis tests in survival analysis, RMST and logrank tests (LRTs), can suffer from low power in cases where the proportional hazards assumption fails. In particular, when late differences occur between survival curves, our proposed estimate and class of tests, window mean survival time (WMST), outperforms both RMST and LRT without sacrificing interpretability, unlike weighted rank tests (WRTs). WMST has the added advantage of maintaining high power when the proportional hazards assumption is met, while WRTs do not. With testing methods often being chosen in advance of data collection, WMST can ensure adequate power without distributional assumptions and is robust to the choice of its restriction parameters. Functions for performing WMST analysis are provided in the survWM2 package in R.},
  archive      = {J_SIM},
  author       = {Mitchell Paukner and Richard Chappell},
  doi          = {10.1002/sim.9138},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5521-5533},
  shortjournal = {Stat. Med.},
  title        = {Window mean survival time},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted expectile regression for right-censored data.
<em>SIM</em>, <em>40</em>(25), 5501–5520. (<a
href="https://doi.org/10.1002/sim.9137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectile regression can be used to analyze the entire conditional distribution of a response, omitting all distributional assumptions. Among its benefits are computational simplicity, efficiency, and the possibility to incorporate a semiparametric predictor. Due to its advantages in full data settings, we propose an extension to right-censored data situations, where conventional methods typically focus only on mean effects. We propose to extend expectile regression with inverse probability weights. Estimates are easy to implement and computationally simple. Expectiles can be converted to more easily interpreted tail expectations, that is, the expected residual life. It provides a meaningful effect measure, similar to the hazard rate. The results from an extensive simulation study are presented, evaluating consistency and sensitivity to violations of assumptions. We use the proposed method to analyze survival times of colorectal cancer patients from a regional certified high volume cancer center.},
  archive      = {J_SIM},
  author       = {Alexander Seipp and Verena Uslar and Dirk Weyhe and Antje Timmer and Fabian Otto-Sobotka},
  doi          = {10.1002/sim.9137},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5501-5520},
  shortjournal = {Stat. Med.},
  title        = {Weighted expectile regression for right-censored data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized regression for left-truncated and right-censored
survival data. <em>SIM</em>, <em>40</em>(25), 5487–5500. (<a
href="https://doi.org/10.1002/sim.9136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data are becoming increasingly common in the medical field as large volumes of patient information are collected and processed by high-throughput screening, electronic health records, and comprehensive genomic testing. Statistical models that attempt to study the effects of many predictors on survival typically implement feature selection or penalized methods to mitigate the undesirable consequences of overfitting. In some cases survival data are also left-truncated which can give rise to an immortal time bias, but penalized survival methods that adjust for left truncation are not commonly implemented. To address these challenges, we apply a penalized Cox proportional hazards model for left-truncated and right-censored survival data and assess implications of left truncation adjustment on bias and interpretation. We use simulation studies and a high-dimensional, real-world clinico-genomic database to highlight the pitfalls of failing to account for left truncation in survival modeling.},
  archive      = {J_SIM},
  author       = {Sarah F. McGough and Devin Incerti and Svetlana Lyalina and Ryan Copping and Balasubramanian Narasimhan and Robert Tibshirani},
  doi          = {10.1002/sim.9136},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5487-5500},
  shortjournal = {Stat. Med.},
  title        = {Penalized regression for left-truncated and right-censored survival data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal design of cluster randomized trials allowing unequal
allocation of clusters and unequal cluster size between arms.
<em>SIM</em>, <em>40</em>(25), 5474–5486. (<a
href="https://doi.org/10.1002/sim.9135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are sometimes cost, scientific, or logistical reasons to allocate individuals unequally in an individually randomized trial. In cluster randomized trials we can allocate clusters unequally and/or allow different cluster size between trial arms. We consider parallel group designs with a continuous outcome, and optimal designs that require the smallest number of individuals to be measured given the number of clusters. Previous authors have derived the optimal allocation ratio for clusters under different variance and/or intracluster correlations (ICCs) between arms, allowing different but prespecified cluster sizes by arm. We derive closed-form expressions to identify the optimal proportions of clusters and of individuals measured for each arm, thereby defining optimal cluster sizes, when cluster size can be chosen freely. When ICCs differ between arms but the variance is equal, the optimal design allocates more than half the clusters to the arm with the higher ICC, but (typically only slightly) less than half the individuals and hence a smaller cluster size. We also describe optimal design under constraints on the number of clusters or cluster size in one or both arms. This methodology allows trialists to consider a range for the number of clusters in the trial and for each to identify the optimal design. Except if there is clear prior evidence for the ICC and variance by arm, a range of values will need to be considered. Researchers should choose a design with adequate power across the range, while also keeping enough clusters in each arm to permit the intended analysis method.},
  archive      = {J_SIM},
  author       = {Andrew J. Copas and Richard Hooper},
  doi          = {10.1002/sim.9135},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5474-5486},
  shortjournal = {Stat. Med.},
  title        = {Optimal design of cluster randomized trials allowing unequal allocation of clusters and unequal cluster size between arms},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using knockoffs for controlled predictive biomarker
identification. <em>SIM</em>, <em>40</em>(25), 5453–5473. (<a
href="https://doi.org/10.1002/sim.9134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key challenges of personalized medicine is to identify which patients will respond positively to a given treatment. The area of subgroup identification focuses on this challenge, that is, identifying groups of patients that experience desirable characteristics, such as an enhanced treatment effect. A crucial first step towards the subgroup identification is to identify the baseline variables (eg, biomarkers) that influence the treatment effect, which are known as predictive variables. Many subgroup discovery algorithms return importance scores that capture the variables&#39; predictive strength. However, a major limitation of these scores is that they do not answer the core question: “Which variables are actually predictive?” With our work we answer this question by using the knockoff framework, which is a general framework for controlling the false discovery rate when performing prognostic variable selection. In contrast, our work is the first that uses knockoffs for predictive variable selection. We introduce two novel knockoff filters: one parametric, building on variable importance scores derived from a penalized linear regression model, and one non-parametric, building on causal forest variable importance scores. We conduct extensive simulations to validate performance of the proposed methodology and we also apply the proposed methods to data from a randomized clinical trial.},
  archive      = {J_SIM},
  author       = {Konstantinos Sechidis and Matthias Kormaksson and David Ohlssen},
  doi          = {10.1002/sim.9134},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5453-5473},
  shortjournal = {Stat. Med.},
  title        = {Using knockoffs for controlled predictive biomarker identification},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing and correcting for weak and pleiotropic instruments
in two-sample multivariable mendelian randomization. <em>SIM</em>,
<em>40</em>(25), 5434–5452. (<a
href="https://doi.org/10.1002/sim.9133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariable Mendelian randomization (MVMR) is a form of instrumental variable analysis which estimates the direct effect of multiple exposures on an outcome using genetic variants as instruments. Mendelian randomization and MVMR are frequently conducted using two-sample summary data where the association of the genetic variants with the exposures and outcome are obtained from separate samples. If the genetic variants are only weakly associated with the exposures either individually or conditionally, given the other exposures in the model, then standard inverse variance weighting will yield biased estimates for the effect of each exposure. Here, we develop a two-sample conditional F -statistic to test whether the genetic variants strongly predict each exposure conditional on the other exposures included in a MVMR model. We show formally that this test is equivalent to the individual level data conditional F -statistic, indicating that conventional rule-of-thumb critical values of 10, can be used to test for weak instruments. We then demonstrate how reliable estimates of the causal effect of each exposure on the outcome can be obtained in the presence of weak instruments and pleiotropy, by repurposing a commonly used heterogeneity Q -statistic as an estimating equation. Furthermore, the minimized value of this Q -statistic yields an exact test for heterogeneity due to pleiotropy. We illustrate our methods with an application to estimate the causal effect of blood lipid fractions on age-related macular degeneration.},
  archive      = {J_SIM},
  author       = {Eleanor Sanderson and Wes Spiller and Jack Bowden},
  doi          = {10.1002/sim.9133},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5434-5452},
  shortjournal = {Stat. Med.},
  title        = {Testing and correcting for weak and pleiotropic instruments in two-sample multivariable mendelian randomization},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lasso estimation of hierarchical interactions for analyzing
heterogeneity of treatment effect. <em>SIM</em>, <em>40</em>(25),
5417–5433. (<a href="https://doi.org/10.1002/sim.9132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals differ in how they respond to a given treatment. In an effort to predict the treatment response and analyze the heterogeneity of treatment effect, we propose a general modeling framework by identifying treatment-covariate interactions honoring a hierarchical condition. We construct a single-step l 1 norm penalty procedure that maintains the hierarchical structure of interactions in the sense that a treatment-covariate interaction term is included in the model only when either the covariate or both the covariate and treatment have nonzero main effects. We developed a constrained Lasso approach with two parameterization schemes that enforce the hierarchical interaction restriction differently. We solved the resulting constrained optimization problem using a spectral projected gradient method. We compared our methods to the unstructured Lasso using simulation studies including a scenario that violates the hierarchical condition (misspecified model). The simulations showed that our methods yielded more parsimonious models and outperformed the unstructured Lasso for correctly identifying nonzero treatment-covariate interactions. The superior performance of our methods are also corroborated by an application to a large randomized clinical trial data investigating a drug for treating congestive heart failure (N = 2569). Our methods provide a well-suited approach for doing secondary analysis in clinical trials to analyze heterogeneous treatment effects and to identify predictive biomarkers.},
  archive      = {J_SIM},
  author       = {Yu Du and Huan Chen and Ravi Varadhan},
  doi          = {10.1002/sim.9132},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {5417-5433},
  shortjournal = {Stat. Med.},
  title        = {Lasso estimation of hierarchical interactions for analyzing heterogeneity of treatment effect},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size considerations for matched-pair cluster
randomization design with incomplete observations of binary outcomes.
<em>SIM</em>, <em>40</em>(24), 5397–5416. (<a
href="https://doi.org/10.1002/sim.9131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple public health and medical research studies have applied matched-pair cluster randomization design to the evaluation of the intervention and/or prevention effects. One of the most common and severe problems faced by researchers when conducting cluster randomized trials (CRTs) is incomplete observations, which are associated with various reasons causing the individuals to discontinue participating in the trials. Although statistical methods to remedy the problems of missing data have already been proposed, there are still methodological gaps in research concerning the determination of sample size in matched-pair CRTs with incomplete binary outcomes. One conventional method for adjusting for missing data in the sample size determination is to divide the sample size under complete data by the expected follow-up rate. However, such crude adjustment ignores the impact of the structure and strength of correlations regarding both outcome data and missing data mechanism. This article provides a closed-form sample size formula for matched-pair CRTs with incomplete binary outcomes, which appropriately accounts for different missing patterns and magnitudes as well as the effects of matching and clustering on the outcome and missing data. The generalized estimating equation (GEE) approach treats incomplete observations as missing data in a marginal logistic regression model, which flexibly accommodates various types of intraclass correlation, missing patterns, and missing proportions. In the presence of missing data, the proposed GEE sample size method provides higher accuracy as compared with the conventional method. The performance of the proposed method is assessed by simulation studies. This article also illustrates how the proposed method can be used to design a real-world matched-pair CRT to examine the effect of a team-based approach on controlling blood pressure (BP).},
  archive      = {J_SIM},
  author       = {Xiaohan Xu and Hong Zhu and Anh Q. Hoang and Chul Ahn},
  doi          = {10.1002/sim.9131},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5397-5416},
  shortjournal = {Stat. Med.},
  title        = {Sample size considerations for matched-pair cluster randomization design with incomplete observations of binary outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous positive airway pressure adherence trajectories
in sleep apnea: Clustering with summed discrete fréchet and dynamic time
warping dissimilarities. <em>SIM</em>, <em>40</em>(24), 5373–5396. (<a
href="https://doi.org/10.1002/sim.9130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Guillaume Bottaz-Bosson and Agnès Hamon and Jean-Louis Pépin and Sébastien Bailly and Adeline Samson},
  doi          = {10.1002/sim.9130},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5373-5396},
  shortjournal = {Stat. Med.},
  title        = {Continuous positive airway pressure adherence trajectories in sleep apnea: Clustering with summed discrete fréchet and dynamic time warping dissimilarities},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multivariate statistical approach to predict COVID-19
count data with epidemiological interpretation and uncertainty
quantification. <em>SIM</em>, <em>40</em>(24), 5351–5372. (<a
href="https://doi.org/10.1002/sim.9129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the analysis of COVID-19 pandemic data, we propose Bayesian multinomial and Dirichlet-multinomial autoregressive models for time-series of counts of patients in mutually exclusive and exhaustive observational categories, defined according to the severity of the patient status and the required treatment. Categories include hospitalized in regular wards (H) and in intensive care units (ICU), together with deceased (D) and recovered (R). These models explicitly formulate assumptions on the transition probabilities between these categories across time, thanks to a flexible formulation based on parameters that a priori follow normal distributions, possibly truncated to incorporate specific hypotheses having an epidemiological interpretation. The posterior distribution of model parameters and the transition matrices are estimated by a Markov chain Monte Carlo algorithm that also provides predictions and allows us to compute the reproduction number . All estimates and predictions are endowed with an accuracy measure obtained thanks to the Bayesian approach. We present results concerning data collected during the first wave of the pandemic in Italy and Lombardy and study the effect of nonpharmaceutical interventions. Suitable discrepancy measures defined to check and compare models show that the Dirichlet-multinomial model has an adequate fit and provides good predictive performance in particular for H and ICU patients.},
  archive      = {J_SIM},
  author       = {Francesco Bartolucci and Fulvia Pennoni and Antonietta Mira},
  doi          = {10.1002/sim.9129},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5351-5372},
  shortjournal = {Stat. Med.},
  title        = {A multivariate statistical approach to predict COVID-19 count data with epidemiological interpretation and uncertainty quantification},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group sequential holm and hochberg procedures. <em>SIM</em>,
<em>40</em>(24), 5333–5350. (<a
href="https://doi.org/10.1002/sim.9128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of testing multiple hypotheses using a group sequential procedure often arises in clinical trials. We review several group sequential Holm (GSHM) type procedures proposed in the literature and clarify the relationships between them. In particular, we show which procedures are equivalent or, if different, which are more powerful and what are their pros and cons. We propose a step-up group sequential Hochberg (GSHC) procedure as a reverse application of a particular step-down GSHM procedure. We conducted an extensive simulation study to evaluate the familywise error rate (FWER) and power properties of that GSHM procedure and the GSHC procedure and found that the GSHC procedure controls FWER more closely and is more powerful. All procedures are illustrated with a common numerical example, the data for which are chosen to bring out the differences between them. A real case study is also presented to illustrate application of these procedures. R programs for applying the proposed procedures, additional simulation results, and the proof of the FWER control of the GSHC procedure in a special case are provided in Supplementary Material.},
  archive      = {J_SIM},
  author       = {Ajit C. Tamhane and Dong Xi and Jiangtao Gou},
  doi          = {10.1002/sim.9128},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5333-5350},
  shortjournal = {Stat. Med.},
  title        = {Group sequential holm and hochberg procedures},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ridge-penalized adaptive mantel test and its application in
imaging genetics. <em>SIM</em>, <em>40</em>(24), 5313–5332. (<a
href="https://doi.org/10.1002/sim.9127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a ridge-penalized adaptive Mantel test (AdaMant) for evaluating the association of two high-dimensional sets of features. By introducing a ridge penalty, AdaMant tests the association across many metrics simultaneously. We demonstrate how ridge penalization bridges Euclidean and Mahalanobis distances and their corresponding linear models from the perspective of association measurement and testing. This result is not only theoretically interesting but also has important implications in penalized hypothesis testing, especially in high-dimensional settings such as imaging genetics. Applying the proposed method to an imaging genetic study of visual working memory in healthy adults, we identified interesting associations of brain connectivity (measured by electroencephalogram coherence) with selected genetic features.},
  archive      = {J_SIM},
  author       = {Dustin Pluta and Tong Shen and Gui Xue and Chuansheng Chen and Hernando Ombao and Zhaoxia Yu},
  doi          = {10.1002/sim.9127},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5313-5332},
  shortjournal = {Stat. Med.},
  title        = {Ridge-penalized adaptive mantel test and its application in imaging genetics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fitting marginal models in small samples: A simulation study
of marginalized multilevel models and generalized estimating equations.
<em>SIM</em>, <em>40</em>(24), 5298–5312. (<a
href="https://doi.org/10.1002/sim.9126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In correlated data settings, analysts typically choose between fitting conditional and marginal models, whose parameters come with distinct interpretations, and as such the choice between the two should be made on scientific grounds. For settings where interest lies in marginal—or population-averaged—parameters, the question of how best to estimate those parameters is a statistical one, and analysts have at their disposal two distinct modeling frameworks: generalized estimating equations (GEE) and marginalized multilevel models (MMMs). The two have been contrasted theoretically and in large sample settings, but asymptotic theory provides no guarantees in the small sample settings that are commonplace. In a comprehensive series of simulation studies, we shed light on the relative performance of GEE and MMMs in small-sample settings to help guide analysis decisions in practice. We find that both GEE and MMMs exhibit similar small-sample bias when the correct correlation structure is adopted (ie, when the random effects distribution is correctly specified or moderately misspecified)—but MMMs can be sensitive to misspecification of the correlation structure. When there are a small number of clusters, MMMs only slightly underestimate standard errors (SEs) for within-cluster associations but can severely underestimate SEs for between-cluster associations. By contrast, while GEE severely underestimates SEs, the Mancl and DeRouen correction provides approximately valid inference.},
  archive      = {J_SIM},
  author       = {Ruofan Bie and Sebastien Haneuse and Nathan Huey and Jonathan Schildcrout and Glen McGee},
  doi          = {10.1002/sim.9126},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5298-5312},
  shortjournal = {Stat. Med.},
  title        = {Fitting marginal models in small samples: A simulation study of marginalized multilevel models and generalized estimating equations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A comparison of confidence distribution approaches for rare
event meta-analysis. <em>SIM</em>, <em>40</em>(24), 5276–5297. (<a
href="https://doi.org/10.1002/sim.9125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis of rare event data has recently received increasing attention due to the challenging issues rare events pose to traditional meta-analytic methods. One specific way to combine information and analyze rare event meta-analysis data utilizes confidence distributions (CDs). While several CD methods exist, no comparisons have been made to determine which method is best suited for homogeneous or heterogeneous meta-analyses with rare events. In this article, we review several CD methods: Fisher&#39;s classic P -value combination method, one that combines P -value functions, another that combines confidence intervals, and one that combines confidence log-likelihood functions. We compare these CD approaches, and we propose and compare variations of these methods to determine which method produces reliable results for homogeneous or heterogeneous rare event meta-analyses. We find that for homogeneous rare event data, most CD methods perform very well. On the other hand, for heterogeneous rare event data, there is a clear split in performance between some CD methods, with some performing very poorly and others performing reasonably well.},
  archive      = {J_SIM},
  author       = {Brinley N. Zabriskie and Chris Corcoran and Pralay Senchaudhuri},
  doi          = {10.1002/sim.9125},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5276-5297},
  shortjournal = {Stat. Med.},
  title        = {A comparison of confidence distribution approaches for rare event meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the normalized power prior. <em>SIM</em>,
<em>40</em>(24), 5251–5275. (<a
href="https://doi.org/10.1002/sim.9124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power prior is a popular tool for constructing informative prior distributions based on historical data. The method consists of raising the likelihood to a discounting factor in order to control the amount of information borrowed from the historical data. However, one often wishes to assign this discounting factor a prior distribution and estimate it jointly with the parameters, which in turn necessitates the computation of a normalizing constant. In this article, we are concerned with how to approximately sample from joint posterior of the parameters and the discounting factor. We first show a few important properties of the normalizing constant and then use these results to motivate a bisection-type algorithm for computing it on a fixed budget of evaluations. We give a large array of illustrations and discuss cases where the normalizing constant is known in closed-form and where it is not. We show that the proposed method produces approximate posteriors that are very close to the exact distributions and also produces posteriors that cover the data-generating parameters with higher probability in the intractable case. Our results suggest that the proposed method is an accurate and easy to implement technique to include this normalization, being applicable to a large class of models. They also reinforce the notion that proper inclusion of the normalizing constant is crucial to the drawing of correct inferences and appropriate quantification of uncertainty.},
  archive      = {J_SIM},
  author       = {Luiz Max Carvalho and Joseph G. Ibrahim},
  doi          = {10.1002/sim.9124},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5251-5275},
  shortjournal = {Stat. Med.},
  title        = {On the normalized power prior},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adjusted logistic propensity weighting methods for
population inference using nonprobability volunteer-based epidemiologic
cohorts. <em>SIM</em>, <em>40</em>(24), 5237–5250. (<a
href="https://doi.org/10.1002/sim.9122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many epidemiologic studies forgo probability sampling and turn to nonprobability volunteer-based samples because of cost, response burden, and invasiveness of biological samples. However, finite population (FP) inference is difficult to make from the nonprobability sample due to the lack of population representativeness. Aiming for making inferences at the population level using nonprobability samples, various inverse propensity score weighting methods have been studied with the propensity defined by the participation rate of population units in the nonprobability sample. In this article, we propose an adjusted logistic propensity weighting (ALP) method to estimate the participation rates for nonprobability sample units. The proposed ALP method is easy to implement by ready-to-use software while producing approximately unbiased estimators for population quantities regardless of the nonprobability sample rate. The efficiency of the ALP estimator can be further improved by scaling the survey sample weights in propensity estimation. Taylor linearization variance estimators are proposed for ALP estimators of FP means that account for all sources of variability. The proposed ALP methods are evaluated numerically via simulation studies and empirically using the naïve unweighted National Health and Nutrition Examination Survey III sample, while taking the 1997 National Health Interview Survey as the reference, to estimate the 15-year mortality rates.},
  archive      = {J_SIM},
  author       = {Lingxiao Wang and Richard Valliant and Yan Li},
  doi          = {10.1002/sim.9122},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5237-5250},
  shortjournal = {Stat. Med.},
  title        = {Adjusted logistic propensity weighting methods for population inference using nonprobability volunteer-based epidemiologic cohorts},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Summary concordance index for meta-analysis of prognosis
studies with a survival outcome. <em>SIM</em>, <em>40</em>(24),
5218–5236. (<a href="https://doi.org/10.1002/sim.9121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prognosis studies to evaluate association between a continuous biomarker and a survival outcome, investigators often classify subjects into two subclasses of the high- and low-expression groups and apply simple survival analysis techniques of the Kaplan-Meier method and the logrank test. The high- and low-expressions are defined according to whether or not the observation of the biomarker is higher than the cut-off value, which is heterogeneous across studies. The heterogeneous definitions of the cut-off value make it difficult to apply the standard meta-analysis techniques. We propose a method to estimate the concordance index for a survival outcome synthesizing published prognosis studies, in which the Kaplan-Meier estimates for the high- and low-expression groups are reported. We illustrate our proposed method with a real dataset for meta-analysis of prognosis studies evaluating Ki-67 in early breast cancer and evaluate its performance with a simulation study.},
  archive      = {J_SIM},
  author       = {Satoshi Hattori and Xiao-Hua Zhou},
  doi          = {10.1002/sim.9121},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5218-5236},
  shortjournal = {Stat. Med.},
  title        = {Summary concordance index for meta-analysis of prognosis studies with a survival outcome},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precision bayesian phase i-II dose-finding based on
utilities tailored to prognostic subgroups. <em>SIM</em>,
<em>40</em>(24), 5199–5217. (<a
href="https://doi.org/10.1002/sim.9120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian phase I-II design is presented that optimizes the dose of a new agent within predefined prognostic subgroups. The design is motivated by a trial to evaluate targeted agents for treating metastatic clear cell renal carcinoma, where a prognostic risk score defined by clinical variables and biomarkers is well established. Two clinical outcomes are used for dose-finding, time-to-toxicity during a prespecified follow-up period, and efficacy characterized by ordinal disease status evaluated at the end of follow-up. A joint probability model is constructed for these outcomes as functions of dose and subgroup. The model performs adaptive clustering of adjacent subgroups having similar dose-outcome distributions to facilitate borrowing information across subgroups. To quantify toxicity-efficacy risk-benefit trade-offs that may differ between subgroups, the objective function is based on outcome utilities elicited separately for each subgroup. In the context of the renal cancer trial, a design is constructed and a simulation study is presented to evaluate the design&#39;s reliability, safety, and robustness, and to compare it to designs that either ignore subgroups or run a separate trial within each subgroup.},
  archive      = {J_SIM},
  author       = {Juhee Lee and Peter F. Thall and Pavlos Msaouel},
  doi          = {10.1002/sim.9120},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5199-5217},
  shortjournal = {Stat. Med.},
  title        = {Precision bayesian phase I-II dose-finding based on utilities tailored to prognostic subgroups},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing conditional causal effect via characteristic
score. <em>SIM</em>, <em>40</em>(24), 5188–5198. (<a
href="https://doi.org/10.1002/sim.9119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational studies usually include participants representing the wide heterogeneous population. The conditional causal effect, treatment effect conditional on baseline characteristics, is of practical importance. Its estimation is subject to two challenges. First, the causal effect is not observable in any individual due to counterfactuality. Second, high-dimensional baseline variables are involved to satisfy the ignorable treatment selection assumption and to attain better estimation efficiency. In this work, a nonparametric estimation procedure, along with a pseudo-response, is proposed to estimate the conditional treatment effect through “characteristic score”—a parsimonious representation of baseline variable influence on treatment benefit. Adopting sparse dimension reduction with variable prescreening in the proposed estimation, we aim to identify the key baseline variables that impact the conditional treatment effect and to uncover the characteristic score that best predicts the treatment effect. This approach is applied to an HIV study for assessing the benefit of antiretroviral regimens and identifying the beneficiary subpopulation.},
  archive      = {J_SIM},
  author       = {Zonghui Hu},
  doi          = {10.1002/sim.9119},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5188-5198},
  shortjournal = {Stat. Med.},
  title        = {Assessing conditional causal effect via characteristic score},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subgroup discovery in non-inferiority trials. <em>SIM</em>,
<em>40</em>(24), 5174–5187. (<a
href="https://doi.org/10.1002/sim.9118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approaches and guidelines for performing subgroup analysis to assess heterogeneity of treatment effect in clinical trials have been the topic of numerous papers in the statistical and clinical literature, but have been discussed predominantly in the context of conventional superiority trials. Concerns about treatment heterogeneity are the same if not greater in non-inferiority (NI) trials, especially since overall similarity between two treatment arms in a successful NI trial could be due to the existence of qualitative interactions that are more likely when comparing two active therapies. Even in unsuccessful NI trials, subgroup analyses can yield important insights about the potential reasons for failure to demonstrate non-inferiority of the experimental therapy. Recent NI trials have performed a priori subgroup analyses using standard statistical tests for interaction, but there is increasing interest in more flexible machine learning approaches for post-hoc subgroup discovery. The performance and practical application of such methods in NI trials have not been systematically explored, however. We considered the Virtual Twin method for the NI setting, an algorithm for subgroup identification that combines random forest with classification and regression trees, and conducted extensive simulation studies to examine its performance under different NI trial conditions and to devise decision rules for selecting the final subgroups. We illustrate the utility of the method with data from a NI trial that was conducted to compare two acupuncture treatments for chronic musculoskeletal pain.},
  archive      = {J_SIM},
  author       = {Melissa J. Fazzari and Mimi Y. Kim},
  doi          = {10.1002/sim.9118},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5174-5187},
  shortjournal = {Stat. Med.},
  title        = {Subgroup discovery in non-inferiority trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principal component analysis of hybrid functional and vector
data. <em>SIM</em>, <em>40</em>(24), 5152–5173. (<a
href="https://doi.org/10.1002/sim.9117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a practical principal component analysis (PCA) framework that provides a nonparametric means of simultaneously reducing the dimensions of and modeling functional and vector (multivariate) data. We first introduce a Hilbert space that combines functional and vector objects as a single hybrid object. The framework, termed a PCA of hybrid functional and vector data (HFV-PCA), is then based on the eigen-decomposition of a covariance operator that captures simultaneous variations of functional and vector data in the new space. This approach leads to interpretable principal components that have the same structure as each observation and a single set of scores that serves well as a low-dimensional proxy for hybrid functional and vector data. To support practical application of HFV-PCA, the explicit relationship between the hybrid PC decomposition and the functional and vector PC decompositions is established, leading to a simple and robust estimation scheme where components of HFV-PCA are calculated using the components estimated from the existing functional and classical PCA methods. This estimation strategy allows flexible incorporation of sparse and irregular functional data as well as multivariate functional data. We derive the consistency results and asymptotic convergence rates for the proposed estimators. We demonstrate the efficacy of the method through simulations and analysis of renal imaging data.},
  archive      = {J_SIM},
  author       = {Jeong Hoon Jang},
  doi          = {10.1002/sim.9117},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5152-5173},
  shortjournal = {Stat. Med.},
  title        = {Principal component analysis of hybrid functional and vector data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prospective individual patient data meta-analysis:
Evaluating convalescent plasma for COVID-19. <em>SIM</em>,
<em>40</em>(24), 5131–5151. (<a
href="https://doi.org/10.1002/sim.9115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the world faced the devastation of the COVID-19 pandemic in late 2019 and early 2020, numerous clinical trials were initiated in many locations in an effort to establish the efficacy (or lack thereof) of potential treatments. As the pandemic has been shifting locations rapidly, individual studies have been at risk of failing to meet recruitment targets because of declining numbers of eligible patients with COVID-19 encountered at participating sites. It has become clear that it might take several more COVID-19 surges at the same location to achieve full enrollment and to find answers about what treatments are effective for this disease. This paper proposes an innovative approach for pooling patient-level data from multiple ongoing randomized clinical trials (RCTs) that have not been configured as a network of sites. We present the statistical analysis plan of a prospective individual patient data (IPD) meta-analysis (MA) from ongoing RCTs of convalescent plasma (CP). We employ an adaptive Bayesian approach for continuously monitoring the accumulating pooled data via posterior probabilities for safety, efficacy, and harm. Although we focus on RCTs for CP and address specific challenges related to CP treatment for COVID-19, the proposed framework is generally applicable to pooling data from RCTs for other therapies and disease settings in order to find answers in weeks or months, rather than years.},
  archive      = {J_SIM},
  author       = {Keith S. Goldfeld and Danni Wu and Thaddeus Tarpey and Mengling Liu and Yinxiang Wu and Andrea B. Troxel and Eva Petkova},
  doi          = {10.1002/sim.9115},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5131-5151},
  shortjournal = {Stat. Med.},
  title        = {Prospective individual patient data meta-analysis: Evaluating convalescent plasma for COVID-19},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Borrowing from supplemental sources to estimate causal
effects from a primary data source. <em>SIM</em>, <em>40</em>(24),
5115–5130. (<a href="https://doi.org/10.1002/sim.9114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing multiplicity of data sources offers exciting possibilities in estimating the effects of a treatment, intervention, or exposure, particularly if observational and experimental sources could be used simultaneously. Borrowing between sources can potentially result in more efficient estimators, but it must be done in a principled manner to mitigate increased bias and Type I error. Furthermore, when the effect of treatment is confounded, as in observational sources or in clinical trials with noncompliance, causal effect estimators are needed to simultaneously adjust for confounding and to estimate effects across data sources. We consider the problem of estimating causal effects from a primary source and borrowing from any number of supplemental sources. We propose using regression-based estimators that borrow based on assuming exchangeability of the regression coefficients and parameters between data sources. Borrowing is accomplished with multisource exchangeability models and Bayesian model averaging. We show via simulation that a Bayesian linear model and Bayesian additive regression trees both have desirable properties and borrow under appropriate circumstances. We apply the estimators to recently completed trials of very low nicotine content cigarettes investigating their impact on smoking behavior.},
  archive      = {J_SIM},
  author       = {Jeffrey A. Boatman and David M. Vock and Joseph S. Koopmeiners},
  doi          = {10.1002/sim.9114},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {5115-5130},
  shortjournal = {Stat. Med.},
  title        = {Borrowing from supplemental sources to estimate causal effects from a primary data source},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian modeling of a bivariate toxicity outcome for early
phase oncology trials evaluating dose regimens. <em>SIM</em>,
<em>40</em>(23), 5096–5114. (<a
href="https://doi.org/10.1002/sim.9113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most phase I trials in oncology aim to find the maximum tolerated dose (MTD) based on the occurrence of dose limiting toxicities (DLT). Evaluating the schedule of administration in addition to the dose may improve drug tolerance. Moreover, for some molecules, a bivariate toxicity endpoint may be more appropriate than a single endpoint. However, standard dose-finding designs do not account for multiple dose regimens and bivariate toxicity endpoint within the same design. In this context, following a phase I motivating trial, we proposed modeling the first type of DLT, cytokine release syndrome, with the entire dose regimen using pharmacokinetics and pharmacodynamics (PK/PD), whereas the other DLT (DLT o ) was modeled with the cumulative dose. We developed three approaches to model the joint distribution of DLT, defining it as a bivariate binary outcome from the two toxicity types, under various assumptions about the correlation between toxicities: an independent model, a copula model and a conditional model. Our Bayesian approaches were developed to be applied at the end of the dose-allocation stage of the trial, once all data, including PK/PD measurements, were available. The approaches were evaluated through an extensive simulation study that showed that they can improve the performance of selecting the true MTD-regimen compared to the recommendation of the dose-allocation method implemented. Our joint approaches can also predict the DLT probabilities of new dose regimens that were not tested in the study and could be investigated in further stages of the trial.},
  archive      = {J_SIM},
  author       = {Emma Gerard and Sarah Zohar and Christelle Lorenzato and Moreno Ursino and Marie-Karelle Riviere},
  doi          = {10.1002/sim.9113},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {5096-5114},
  shortjournal = {Stat. Med.},
  title        = {Bayesian modeling of a bivariate toxicity outcome for early phase oncology trials evaluating dose regimens},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear mixed-effects modeling of longitudinal count data:
Bayesian inference about median counts based on the marginal
zero-inflated discrete weibull distribution. <em>SIM</em>,
<em>40</em>(23), 5078–5095. (<a
href="https://doi.org/10.1002/sim.9112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a Bayesian regression model for nonlinear zero-inflated longitudinal count data that models the median count as an alternative to the mean count. The nonlinear model generalizes a recently introduced linear mixed-effects model based on the zero-inflated discrete Weibull (ZIDW) distribution. The ZIDW distribution is more robust to severe skewness in the data than conventional zero-inflated count distributions such as the zero-inflated negative binomial (ZINB) distribution. Moreover, the ZIDW distribution is attractive because of its convenience to model the median counts given its closed-form quantile function. The median is a more robust measure of central tendency than the mean when the data, for instance, zero-inflated counts, are right-skewed. In an application of the model we consider a biphasic mixed-effects model consisting of an intercept term and two slope terms. Conventionally, the ZIDW model separately specifies the predictors for the zero-inflation probability and the counting process&#39;s median count. In our application, the two latent class interpretations are not clinically plausible. Therefore, we propose a marginal ZIDW model that directly models the biphasic median counts marginally. We also consider the marginal ZINB model to make inferences about the nonlinear mean counts over time. Our simulation study shows that the models have good properties in terms of accuracy and confidence interval coverage.},
  archive      = {J_SIM},
  author       = {Divan A. Burger and Emmanuel Lesaffre},
  doi          = {10.1002/sim.9112},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {5078-5095},
  shortjournal = {Stat. Med.},
  title        = {Nonlinear mixed-effects modeling of longitudinal count data: Bayesian inference about median counts based on the marginal zero-inflated discrete weibull distribution},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the time-varying predictive performance of longitudinal
biomarkers: Measure and estimation. <em>SIM</em>, <em>40</em>(23),
5065–5077. (<a href="https://doi.org/10.1002/sim.9111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many biomedical studies, participants are monitored at periodic visits until the occurrence of the failure event. Biomarkers are often measured repeatedly during these visits, and such measurements can facilitate updated disease prediction. In this work, we propose a two-dimensional incident dynamic area under curve (AUC), to capture the variability due to both the biomarker assessment time and the prediction time to comprehensively quantify the predictive performance of a longitudinal biomarker. We propose a pseudo partial-likelihood to achieve consistent estimation of the AUC under two realistic scenarios of visit schedules. Variance estimation methods are designed to facilitate inferential procedures. We examine the finite-sample performance of our method through extensive simulations. The methods are applied to a study of chronic myeloid leukemia to evaluate the predictive performance of longitudinally collected gene expression levels.},
  archive      = {J_SIM},
  author       = {Jing Zhang and Jing Ning and Xuelin Huang and Ruosha Li},
  doi          = {10.1002/sim.9111},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {5065-5077},
  shortjournal = {Stat. Med.},
  title        = {On the time-varying predictive performance of longitudinal biomarkers: Measure and estimation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection for censored data using modified
correlation adjusted coRrelation (MCAR) scores. <em>SIM</em>,
<em>40</em>(23), 5046–5064. (<a
href="https://doi.org/10.1002/sim.9110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with high-dimensional censored data is very challenging because of the complexities in data structure. This article focuses on developing a variable selection procedure for censored high-dimensional data with the AFT models using the Modified Correlation Adjusted coRrelation (MCAR) scores method. The latter is developed based on CAR scores method that provides a canonical ordering that encourages grouping of correlated predictors and down-weights antagonistic variables. The proposed MCAR scores method is developed as an extension of the CAR scores method using NOVEL integration of the sample and threshold estimator of the correlation matrix as suggested by Huang and Frylewicz. The proposed MCAR exhibits computationally more efficient estimates under model sparsity and can provide a canonical ordering among the predictors. The MCAR method is a greedy method that is also easy to understand and can perform estimation and variable selection simultaneously. Performances of variable selection by the MCAR method have been compared with other existing regularized techniques in literature—such as the lasso, elastic net and with a machine learning technique called boosting and with the censored CAR by a number of simulation studies and a real microarray data set called diffuse large-B-cell lymphoma. Results indicate that when correlation exists among the covariates, the MCAR method outperforms all five techniques while for uncorrelated data, the MCAR performs quite similar to the CAR method but clearly outperforms the other three methods. The empirical study further reveals that the MCAR method exhibits the best predictive performance among the methods.},
  archive      = {J_SIM},
  author       = {Afsana Mimi and Md Hasinur Rahaman Khan},
  doi          = {10.1002/sim.9110},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {5046-5064},
  shortjournal = {Stat. Med.},
  title        = {Variable selection for censored data using modified correlation adjusted coRrelation (MCAR) scores},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian variable selection with a pleiotropic loss function
in mendelian randomization. <em>SIM</em>, <em>40</em>(23), 5025–5045.
(<a href="https://doi.org/10.1002/sim.9109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization is the use of genetic variants as instruments to assess the existence of a causal relationship between a risk factor and an outcome. A Mendelian randomization analysis requires a set of genetic variants that are strongly associated with the risk factor and only associated with the outcome through their effect on the risk factor. We describe a novel variable selection algorithm for Mendelian randomization that can identify sets of genetic variants which are suitable in both these respects. Our algorithm is applicable in the context of two-sample summary-data Mendelian randomization and employs a recently proposed theoretical extension of the traditional Bayesian statistics framework, including a loss function to penalize genetic variants that exhibit pleiotropic effects. The algorithm offers robust inference through the use of model averaging, as we illustrate by running it on a range of simulation scenarios and comparing it against established pleiotropy-robust Mendelian randomization methods. In a real-data application, we study the effect of systolic and diastolic blood pressure on the risk of suffering from coronary heart disease (CHD). Based on a recent large-scale GWAS for blood pressure, we use 395 genetic variants for systolic and 391 variants for diastolic blood pressure. Both traits are shown to have significant risk-increasing effects on CHD risk.},
  archive      = {J_SIM},
  author       = {Apostolos Gkatzionis and Stephen Burgess and David V. Conti and Paul J. Newcombe},
  doi          = {10.1002/sim.9109},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {5025-5045},
  shortjournal = {Stat. Med.},
  title        = {Bayesian variable selection with a pleiotropic loss function in mendelian randomization},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approximate quasi-likelihood approach for error-prone
failure time outcomes and exposures. <em>SIM</em>, <em>40</em>(23),
5006–5024. (<a href="https://doi.org/10.1002/sim.9108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement error arises commonly in clinical research settings that rely on data from electronic health records or large observational cohorts. In particular, self-reported outcomes are typical in cohort studies for chronic diseases such as diabetes in order to avoid the burden of expensive diagnostic tests. Dietary intake, which is also commonly collected by self-report and subject to measurement error, is a major factor linked to diabetes and other chronic diseases. These errors can bias exposure-disease associations that ultimately can mislead clinical decision-making. We have extended an existing semiparametric likelihood-based method for handling error-prone, discrete failure time outcomes to also address covariate error. We conduct an extensive numerical study to compare the proposed method to the naive approach that ignores measurement error in terms of bias and efficiency in the estimation of the regression parameter of interest. In all settings considered, the proposed method showed minimal bias and maintained coverage probability, thus outperforming the naive analysis which showed extreme bias and low coverage. This method is applied to data from the Women&#39;s Health Initiative to assess the association between energy and protein intake and the risk of incident diabetes mellitus. Our results show that correcting for errors in both the self-reported outcome and dietary exposures leads to considerably different hazard ratio estimates than those from analyses that ignore measurement error, which demonstrates the importance of correcting for both outcome and covariate error.},
  archive      = {J_SIM},
  author       = {Lillian A. Boe and Lesley F. Tinker and Pamela A. Shaw},
  doi          = {10.1002/sim.9108},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {5006-5024},
  shortjournal = {Stat. Med.},
  title        = {An approximate quasi-likelihood approach for error-prone failure time outcomes and exposures},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating optimal dynamic treatment strategies under
resource constraints using dynamic marginal structural models.
<em>SIM</em>, <em>40</em>(23), 4996–5005. (<a
href="https://doi.org/10.1002/sim.9107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for estimating optimal treatment strategies typically assume unlimited access to resources. However, when a health system has resource constraints, such as limited funds, access to medication, or monitoring capabilities, medical decisions must account for competition between individuals in resource usage. The problem of incorporating resource constraints into optimal treatment strategies has been solved for point exposures (1), that is, treatment strategies entailing a decision at just one time point. However, attempts to directly generalize the point exposure solution to dynamic time-varying treatment strategies run into complications. We sidestep these complications by targeting the optimal strategy within a clinically defined subclass. Our approach is to employ dynamic marginal structural models to estimate (counterfactual) resource usage under the class of candidate treatment strategies and solve a constrained optimization problem to choose the optimal strategy for which expected resource usage is within acceptable limits. We apply this method to determine the optimal dynamic monitoring strategy for people living with HIV when resource limits on monitoring exist using observational data from the HIV-CAUSAL Collaboration.},
  archive      = {J_SIM},
  author       = {Ellen C. Caniglia and Eleanor J. Murray and Miguel A. Hernán and Zach Shahn},
  doi          = {10.1002/sim.9107},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4996-5005},
  shortjournal = {Stat. Med.},
  title        = {Estimating optimal dynamic treatment strategies under resource constraints using dynamic marginal structural models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graphical models for mean and covariance of multivariate
longitudinal data. <em>SIM</em>, <em>40</em>(23), 4977–4995. (<a
href="https://doi.org/10.1002/sim.9106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint mean-covariance modeling of multivariate longitudinal data helps to understand the relative changes among multiple longitudinally measured and correlated outcomes. A key challenge in the analysis of multivariate longitudinal data is the complex covariance structure. This is due to the contemporaneous and cross-temporal associations between multiple longitudinal outcomes. Graphical and data-driven tools that can aid in visualizing the dependence patterns among multiple longitudinal outcomes are not readily available. In this work, we show the role of graphical techniques: profile plots, and multivariate regressograms, in developing mean and covariance models for multivariate longitudinal data. We introduce an R package MLGM (Multivariate Longitudinal Graphical Models) to facilitate visualization and modeling mean and covariance patterns. Through two real studies, microarray data from the T-cell activation study and Mayo Clinic&#39;s primary biliary cirrhosis of the liver study, we show the key features of MLGM. We evaluate the finite sample performance of the proposed mean-covariance estimation approach through simulations.},
  archive      = {J_SIM},
  author       = {Priya Kohli and Xinyu Du and Haoyang Shen},
  doi          = {10.1002/sim.9106},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4977-4995},
  shortjournal = {Stat. Med.},
  title        = {Graphical models for mean and covariance of multivariate longitudinal data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Some new confidence intervals for kaplan-meier based
estimators from one and two sample survival data. <em>SIM</em>,
<em>40</em>(23), 4961–4976. (<a
href="https://doi.org/10.1002/sim.9105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) has been popularly used to assess the treatment effect in survival trials. Greenwood&#39;s formula is often used to estimate the variance of RMST, and the resulting Wald confidence interval (CI) tends to be liberal in small and moderate samples. We propose the empirical likelihood ratio, score-type, and loglog transformed CIs for RMST in a single sample. The method of variance estimates recovery technique is used to derive the CIs for the difference and ratio parameters in the two sample inference. A variance estimate, which assumes equal survival curves, but possibly different censoring rates in the two groups, is proposed for comparing two groups. The new variance estimate shows excellent performance in testing for superiority, and also works well for a noninferiority test with a small margin, and for the interval estimation when the two survival curves are close. We use similar techniques to construct CIs for comparing two milestone survival probabilities. Numerical examples are used to assess these interval estimation methods.},
  archive      = {J_SIM},
  author       = {Yongqiang Tang},
  doi          = {10.1002/sim.9105},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4961-4976},
  shortjournal = {Stat. Med.},
  title        = {Some new confidence intervals for kaplan-meier based estimators from one and two sample survival data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A practical response adaptive block randomization (RABR)
design with analytic type i error protection. <em>SIM</em>,
<em>40</em>(23), 4947–4960. (<a
href="https://doi.org/10.1002/sim.9104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response adaptive randomization (RAR) is appealing from methodological, ethical, and pragmatic perspectives in the sense that subjects are more likely to be randomized to better performing treatment groups based on accumulating data. However, applications of RAR in confirmatory drug clinical trials with multiple active arms are limited largely due to its complexity, and lack of control of randomization ratios to different treatment groups. To address the aforementioned issues, we propose a Response Adaptive Block Randomization (RABR) design allowing arbitrarily prespecified randomization ratios for the control and high-performing groups to meet clinical trial objectives. We show the validity of the conventional unweighted test in RABR with a controlled type I error rate based on the weighted combination test for sample size adaptive design invoking no large sample approximation. The advantages of the proposed RABR in terms of robustly reaching target final sample size to meet regulatory requirements and increasing statistical power as compared with the popular Doubly Adaptive Biased Coin Design are demonstrated by statistical simulations and a practical clinical trial design example.},
  archive      = {J_SIM},
  author       = {Tianyu Zhan and Lu Cui and Ziqian Geng and Lanju Zhang and Yihua Gu and Ivan S.F. Chan},
  doi          = {10.1002/sim.9104},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4947-4960},
  shortjournal = {Stat. Med.},
  title        = {A practical response adaptive block randomization (RABR) design with analytic type i error protection},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A copula-based approach for dynamic prediction of survival
with a binary time-dependent covariate. <em>SIM</em>, <em>40</em>(23),
4931–4946. (<a href="https://doi.org/10.1002/sim.9102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction methods incorporate longitudinal biomarker information to produce updated, more accurate predictions of conditional survival probability. There are two approaches for obtaining dynamic predictions: (1) a joint model of the longitudinal marker and survival process, and (2) an approximate approach that specifies a model for a specific component of the joint distribution. In the case of a binary marker, an illness-death model is an example of a joint modeling approach that is unified and produces consistent predictions. However, previous literature has shown that approximate approaches, such as landmarking, with additional flexibility can have good predictive performance. One such approach proposes using a Gaussian copula to model the joint distribution of conditional continuous marker and survival distributions. It has the advantage of specifying established, flexible models for the marginals for which goodness-of-fit can be assessed, and has easy estimation that can be implemented in standard software. In this article, we provide a Gaussian copula approach for dynamic prediction to accommodate a binary marker using a continuous latent variable formulation. We compare the predictive performance of this approach to joint modeling and landmarking using simulations and demonstrate its use for obtaining dynamic predictions in an application to a prostate cancer study.},
  archive      = {J_SIM},
  author       = {Krithika Suresh and Jeremy M.G. Taylor and Alexander Tsodikov},
  doi          = {10.1002/sim.9102},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4931-4946},
  shortjournal = {Stat. Med.},
  title        = {A copula-based approach for dynamic prediction of survival with a binary time-dependent covariate},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthesizing external aggregated information in the
penalized cox regression under population heterogeneity. <em>SIM</em>,
<em>40</em>(23), 4915–4930. (<a
href="https://doi.org/10.1002/sim.9101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing external aggregated information has been proven useful in improving estimation efficiency when conducting statistical analysis using a limited amount of data. In this paper, we develop a unified framework for combining information from high-dimensional individual-level data and potentially low-dimensional external aggregate data under the Cox model. We summarize various forms of external aggregated information by population estimating equations and propose a penalized empirical likelihood approach to borrow information from these estimating equations. The proposed methods possess the flexibility to handle the case where individual-level data and external aggregate data are from heterogeneous populations. Specifically, a penalized empirical likelihood ratio test is developed to check for the potential heterogeneity, and a semiparametric density ratio model is postulated to account for the heterogeneity. Moreover, we study the impact of uncertainty in the auxiliary information on the efficiency gain and propose a modified variance estimator to adjust for the uncertainty. The proposed estimators enjoy the oracle property and are asymptotically more efficient than the penalized partial likelihood estimator that does not exploit the external aggregated information. Simulation studies show improvement in both estimation efficiency and variable selection over the competitors. The proposed approaches are applied to the analysis of a pediatric kidney transplant study for illustration.},
  archive      = {J_SIM},
  author       = {Ying Sheng and Yifei Sun and Chiung-Yu Huang and Mi-Ok Kim},
  doi          = {10.1002/sim.9101},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4915-4930},
  shortjournal = {Stat. Med.},
  title        = {Synthesizing external aggregated information in the penalized cox regression under population heterogeneity},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exact sequential test for clinical trials and post-market
drug and vaccine safety surveillance with poisson and binary data.
<em>SIM</em>, <em>40</em>(22), 4890–4913. (<a
href="https://doi.org/10.1002/sim.9094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sequential analysis, hypothesis testing is performed repeatedly in a prospective manner as data accrue over time to quickly arrive at an accurate conclusion or decision. In this tutorial paper, detailed explanations are given for both designing and operating sequential testing. We describe the calculation of exact thresholds for stopping or signaling, statistical power, expected time to signal, and expected sample sizes for sequential analysis with Poisson and binary type data. The calculations are run using the package Sequential , constructed in R language. Real data examples are inspired on clinical trials practice, such as the current efforts to develop treatments to face the COVID-19 pandemic, and the comparison of treatments of osteoporosis. In addition, we mimic the monitoring of adverse events following influenza vaccination and Pediarix vaccination.},
  archive      = {J_SIM},
  author       = {Ivair R. Silva and Judith Maro and Martin Kulldorff},
  doi          = {10.1002/sim.9094},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4890-4913},
  shortjournal = {Stat. Med.},
  title        = {Exact sequential test for clinical trials and post-market drug and vaccine safety surveillance with poisson and binary data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning and confirming a class of treatment responders in
clinical trials. <em>SIM</em>, <em>40</em>(22), 4872–4889. (<a
href="https://doi.org/10.1002/sim.9100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials require substantial effort and time to complete, and regulatory agencies may require two successful efficacy trials before approving a new drug. One way to improve the chance of follow-up success is to identify a subpopulation among whom treatment effects are estimated to be beneficial, and enrolling future studies from this subpopulation. In this article we study confirmable responder class (CRC) learning, where the objective is to learn in a random half of the dataset (training set) a subpopulation among whom the predicted conditional ATE (CATE) suggests clinically meaningful benefit, with maximum power of being confirmed via hypothesis test in the other half (test set). We studied a set of CRC learners across simulated datasets in which either all patients benefited, or only some did. Performance metrics included the rate of confirmation in the test set, and the classification accuracy of the CRC compared with the group with true CATE&gt;0. In trials where all patients benefitted, only two learners were able to consistently identify most of the population as the CRC. One of these also performed especially well when only some patients benefitted, having relatively high confirmation rates, and showing robustness to the dimension of the covariate vector and population characteristics. This learner is based on cross-validation, and is a possible avenue for further development of model selection criteria for CRC learning. We also show that the performance of all methods can be improved by using both halves of the original dataset as training and test sets in turn.},
  archive      = {J_SIM},
  author       = {Victor B. Talisa and Chung-Chou H. Chang},
  doi          = {10.1002/sim.9100},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4872-4889},
  shortjournal = {Stat. Med.},
  title        = {Learning and confirming a class of treatment responders in clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian variable selection for understanding mixtures in
environmental exposures. <em>SIM</em>, <em>40</em>(22), 4850–4871. (<a
href="https://doi.org/10.1002/sim.9099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social and environmental stressors are crucial factors in child development. However, there exists a multitude of measurable social and environmental factors—the effects of which may be cumulative, interactive, or null. Using a comprehensive cohort of children in North Carolina, we study the impact of social and environmental variables on 4th end-of-grade exam scores in reading and mathematics. To identify the essential factors that predict these educational outcomes, we design new tools for Bayesian linear variable selection using decision analysis. We extract a predictive optimal subset of explanatory variables by coupling a loss function with a novel model-based penalization scheme, which leads to coherent Bayesian decision analysis and empirically improves variable selection, estimation, and prediction on simulated data. The Bayesian linear model propagates uncertainty quantification to all predictive evaluations, which is important for interpretable and robust model comparisons. These predictive comparisons are conducted out-of-sample with a customized approximation algorithm that avoids computationally intensive model refitting. We apply our variable selection techniques to identify the joint collection of social and environmental stressors—and their interactions—that offer clear and quantifiable improvements in prediction of reading and mathematics exam scores.},
  archive      = {J_SIM},
  author       = {Daniel R. Kowal and Mercedes Bravo and Henry Leong and Alexander Bui and Robert J. Griffin and Katherine B. Ensor and Marie Lynn Miranda},
  doi          = {10.1002/sim.9099},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4850-4871},
  shortjournal = {Stat. Med.},
  title        = {Bayesian variable selection for understanding mixtures in environmental exposures},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The reciprocal bayesian LASSO. <em>SIM</em>,
<em>40</em>(22), 4830–4849. (<a
href="https://doi.org/10.1002/sim.9098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reciprocal LASSO (rLASSO) regularization employs a decreasing penalty function as opposed to conventional penalization approaches that use increasing penalties on the coefficients, leading to stronger parsimony and superior model selection relative to traditional shrinkage methods. Here we consider a fully Bayesian formulation of the rLASSO problem, which is based on the observation that the rLASSO estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters are assigned independent inverse Laplace priors. Bayesian inference from this posterior is possible using an expanded hierarchy motivated by a scale mixture of double Pareto or truncated normal distributions. On simulated and real datasets, we show that the Bayesian formulation outperforms its classical cousin in estimation, prediction, and variable selection across a wide range of scenarios while offering the advantage of posterior inference. Finally, we discuss other variants of this new approach and provide a unified framework for variable selection using flexible reciprocal penalties. All methods described in this article are publicly available as an R package at: https://github.com/himelmallick/BayesRecipe.},
  archive      = {J_SIM},
  author       = {Himel Mallick and Rahim Alhamzawi and Erina Paul and Vladimir Svetnik},
  doi          = {10.1002/sim.9098},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4830-4849},
  shortjournal = {Stat. Med.},
  title        = {The reciprocal bayesian LASSO},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of diagnostic test accuracy: A “rule of three”
for data with repeated observations but without a gold standard.
<em>SIM</em>, <em>40</em>(22), 4815–4829. (<a
href="https://doi.org/10.1002/sim.9097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers how to estimate the accuracy of a diagnostic test when there are repeated observations, but without the availability of a gold standard or reference test. We identify conditions under which the structure of the observed data is rich enough to provide sufficient degrees of freedom, such that a suitable latent class model can be fitted with identifiable accuracy parameters. We show that a Rule of Three applies, specifying that accuracy can be evaluated as long as there are at least three observations per individual with the given test. This rule also applies if the three observations arise from combinations of different test methods, or from a sequential design in which individuals are tested for a maximum number of times with the same test but stopping if a positive (or negative) result occurs. The rule pertains to tests having an arbitrary number of response categories. Accuracy is evaluated by parameters reflecting rates of misclassification among the response categories, and the model also provides estimates of the underlying distribution of the true disease state. These ideas are illustrated by data from two medical studies. Issues discussed include the advantages and disadvantages of analyzing the response variable as binary or multinomial, as well as the feasibility of testing goodness of fit when the model incorporates a large number of parameters. Comparisons are possible between models that do or do not assume equal accuracy rates for the observations, and between models where certain misclassification parameters are or are not assumed to be zero.},
  archive      = {J_SIM},
  author       = {Stephen D. Walter},
  doi          = {10.1002/sim.9097},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4815-4829},
  shortjournal = {Stat. Med.},
  title        = {Estimation of diagnostic test accuracy: A “Rule of three” for data with repeated observations but without a gold standard},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size adaptations and evaluation of pediatric study
interpretability. <em>SIM</em>, <em>40</em>(22), 4809–4814. (<a
href="https://doi.org/10.1002/sim.9096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To extend marketing exclusivity, a drug manufacture is required by the Food and Drug Administration to conduct an interpretable study of approved drugs that may benefit pediatric patients. In return for conducting an interpretable study, the drug manufacturer is rewarded with an additional 6 months of marketing exclusivity. If the study design uses a fixed sample size and nothing is changed during the study, it is relatively easy to determine whether an interpretable study was conducted. However, if the sample size can be changed based on an unblinded interim analysis, it is difficult to answer the question of whether the study was interpretable. We investigate the optimal design from the manufacturers perspective and find the strategy for sample size adaptation that optimizes the utility including the chance of an interpretable study.},
  archive      = {J_SIM},
  author       = {John Lawrence},
  doi          = {10.1002/sim.9096},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4809-4814},
  shortjournal = {Stat. Med.},
  title        = {Sample size adaptations and evaluation of pediatric study interpretability},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propensity-score-based meta-analytic predictive prior for
incorporating real-world and historical data. <em>SIM</em>,
<em>40</em>(22), 4794–4808. (<a
href="https://doi.org/10.1002/sim.9095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the availability of real-world data sources (eg, EHRs, claims data, registries) and historical data has rapidly surged in recent years, there is an increasing interest and need from investigators and health authorities to leverage all available information to reduce patient burden and accelerate both drug development and regulatory decision making. Bayesian meta-analytic approaches are a popular historical borrowing method that has been developed to leverage such data using robust hierarchical models. The model structure accounts for various degrees of between-trial heterogeneity, resulting in adaptively discounting the external information in the case of data conflict. In this article, we propose to integrate the propensity score method and Bayesian meta-analytic-predictive (MAP) prior to leverage external real-world and historical data. The propensity score methodology is applied to select a subset of patients from external data that are similar to those in the current study with regards to key baseline covariates and to stratify the selected patients together with those in the current study into more homogeneous strata. The MAP prior approach is used to obtain stratum-specific MAP prior and derive the overall propensity score integrated meta-analytic predictive (PS-MAP) prior. Additionally, we allow for tuning the prior effective sample size for the proposed PS-MAP prior, which quantifies the amount of information borrowed from external data. We evaluate the performance of the proposed PS-MAP prior by comparing it to the existing propensity score-integrated power prior approach in a simulation study and illustrate its implementation with an example of a single-arm phase II trial.},
  archive      = {J_SIM},
  author       = {Meizi Liu and Veronica Bunn and Bradley Hupf and Junjing Lin and Jianchang Lin},
  doi          = {10.1002/sim.9095},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4794-4808},
  shortjournal = {Stat. Med.},
  title        = {Propensity-score-based meta-analytic predictive prior for incorporating real-world and historical data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian multiple imputation approach to bivariate
functional data with missing components. <em>SIM</em>, <em>40</em>(22),
4772–4793. (<a href="https://doi.org/10.1002/sim.9093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing missing data methods for functional data mainly focus on reconstructing missing measurements along a single function—a univariate functional data setting. Motivated by a renal study, we focus on a bivariate functional data setting, where each sampling unit is a collection of two distinct component functions, one of which may be missing. Specifically, we propose a Bayesian multiple imputation approach based on a bivariate functional latent factor model that exploits the joint changing patterns of the component functions to allow accurate and stable imputation of one component given the other. We further extend the framework to address multilevel bivariate functional data with missing components by modeling and exploiting inter-component and intra-subject correlations. We develop a Gibbs sampling algorithm that simultaneously generates multiple imputations of missing component functions and posterior samples of model parameters. For multilevel bivariate functional data, a partially collapsed Gibbs sampler is implemented to improve computational efficiency. Our simulation study demonstrates that our methods outperform other competing methods for imputing missing components of bivariate functional data under various designs and missingness rates. The motivating renal study aims to investigate the distribution and pharmacokinetic properties of baseline and post-furosemide renogram curves that provide further insights into the underlying mechanism of renal obstruction, with post-furosemide renogram curves missing for some subjects. We apply the proposed methods to impute missing post-furosemide renogram curves and obtain more refined insights.},
  archive      = {J_SIM},
  author       = {Jeong Hoon Jang and Amita K. Manatunga and Changgee Chang and Qi Long},
  doi          = {10.1002/sim.9093},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4772-4793},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multiple imputation approach to bivariate functional data with missing components},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Rejoinder to “on the robustness of latent class models for
diagnostic testing with no gold standard.” <em>SIM</em>,
<em>40</em>(22), 4770–4771. (<a
href="https://doi.org/10.1002/sim.9157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Matthew R. Schofield and Michael J. Maze and John A. Crump and Matthew P. Rubach and Renee L. Galloway and Katrina J. Sharples},
  doi          = {10.1002/sim.9157},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4770-4771},
  shortjournal = {Stat. Med.},
  title        = {Rejoinder to “On the robustness of latent class models for diagnostic testing with no gold standard”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Commentary on “on the robustness of latent class models for
diagnostic testing with no gold-standard” by schofield et
al. <em>SIM</em>, <em>40</em>(22), 4766–4769. (<a
href="https://doi.org/10.1002/sim.9086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Nandini Dendukuri},
  doi          = {10.1002/sim.9086},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4766-4769},
  shortjournal = {Stat. Med.},
  title        = {Commentary on “On the robustness of latent class models for diagnostic testing with no gold-standard” by schofield et al.},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continued controversy in using latent class models for
estimating diagnostic accuracy without a gold standard. <em>SIM</em>,
<em>40</em>(22), 4764–4765. (<a
href="https://doi.org/10.1002/sim.9085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Paul S. Albert},
  doi          = {10.1002/sim.9085},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4764-4765},
  shortjournal = {Stat. Med.},
  title        = {Continued controversy in using latent class models for estimating diagnostic accuracy without a gold standard},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). On the robustness of latent class models for diagnostic
testing with no gold standard. <em>SIM</em>, <em>40</em>(22), 4751–4763.
(<a href="https://doi.org/10.1002/sim.8999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult to estimate sensitivity and specificity of diagnostic tests when there is no gold standard. Latent class models have been proposed as a potential solution as they provide estimates without the need for a gold standard. Using a motivating example of the evaluation of point of care tests for leptospirosis in Tanzania, we show how a realistic violation of assumptions underpinning the latent class model can lead directly to substantial bias in the estimates of the parameters of interest. In particular, we consider the robustness of estimates of sensitivity, specificity, and prevalence, to the presence of additional latent states when fitting a two-state latent class model. The violation is minor in the sense that it cannot be routinely detected with goodness-of-fit procedures, but is major with regard to the resulting bias.},
  archive      = {J_SIM},
  author       = {Matthew R. Schofield and Michael J. Maze and John A. Crump and Matthew P. Rubach and Renee Galloway and Katrina J. Sharples},
  doi          = {10.1002/sim.8999},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4751-4763},
  shortjournal = {Stat. Med.},
  title        = {On the robustness of latent class models for diagnostic testing with no gold standard},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of covariance under variance heteroscedasticity in
general factorial designs. <em>SIM</em>, <em>40</em>(21), 4732–4749. (<a
href="https://doi.org/10.1002/sim.9092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjusting for baseline values and covariates is a recurrent statistical problem in medical science. In particular, variance heteroscedasticity is non-negligible in experimental designs and ignoring it might result in false conclusions. Approximate inference methods are developed to test null hypotheses formulated in terms of adjusted treatment effects and regression parameters in general analysis of covariance designs with arbitrary numbers of factors. Variance homoscedasticity is not assumed. The distributions of the test statistics are approximated using Box-type approximation methods. Extensive simulation studies show that the procedures are particularly suitable when sample sizes are rather small. A real data set illustrates the application of the methods.},
  archive      = {J_SIM},
  author       = {Frank Konietschke and Cong Cao and Asanka Gunawardana and Georg Zimmermann},
  doi          = {10.1002/sim.9092},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4732-4749},
  shortjournal = {Stat. Med.},
  title        = {Analysis of covariance under variance heteroscedasticity in general factorial designs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hypothesis test of feasibility for external pilot trials
assessing recruitment, follow-up, and adherence rates. <em>SIM</em>,
<em>40</em>(21), 4714–4731. (<a
href="https://doi.org/10.1002/sim.9091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power of a large clinical trial can be adversely affected by low recruitment, follow-up, and adherence rates. External pilot trials estimate these rates and use them, via prespecified decision rules, to determine if the definitive trial is feasible and should go ahead. There is little methodological research underpinning how these decision rules, or the sample size of the pilot, should be chosen. In this article we propose a hypothesis test of the feasibility of a definitive trial, to be applied to the external pilot data and used to make progression decisions. We quantify feasibility by the power of the planned trial, as a function of recruitment, follow-up, and adherence rates. We use this measure to define hypotheses to test in the pilot, propose a test statistic, and show how the error rates of this test can be calculated for the common scenario of a two-arm parallel group definitive trial with a single normally distributed primary endpoint. We use our method to redesign TIGA-CUB, an external pilot trial comparing a psychotherapy with treatment as usual for children with conduct disorders. We then extend our formulation to include using the pilot data to estimate the standard deviation of the primary endpoint and incorporate this into the progression decision.},
  archive      = {J_SIM},
  author       = {Duncan T. Wilson and Julia Brown and Amanda J. Farrin and Rebecca E. A. Walwyn},
  doi          = {10.1002/sim.9091},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4714-4731},
  shortjournal = {Stat. Med.},
  title        = {A hypothesis test of feasibility for external pilot trials assessing recruitment, follow-up, and adherence rates},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating heterogeneous survival treatment effect in
observational data using machine learning. <em>SIM</em>,
<em>40</em>(21), 4691–4713. (<a
href="https://doi.org/10.1002/sim.9090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for estimating heterogeneous treatment effect in observational data have largely focused on continuous or binary outcomes, and have been relatively less vetted with survival outcomes. Using flexible machine learning methods in the counterfactual framework is a promising approach to address challenges due to complex individual characteristics, to which treatments need to be tailored. To evaluate the operating characteristics of recent survival machine learning methods for the estimation of treatment effect heterogeneity and inform better practice, we carry out a comprehensive simulation study presenting a wide range of settings describing confounded heterogeneous survival treatment effects and varying degrees of covariate overlap. Our results suggest that the nonparametric Bayesian Additive Regression Trees within the framework of accelerated failure time model (AFT-BART-NP) consistently yields the best performance, in terms of bias, precision, and expected regret. Moreover, the credible interval estimators from AFT-BART-NP provide close to nominal frequentist coverage for the individual survival treatment effect when the covariate overlap is at least moderate. Including a nonparametrically estimated propensity score as an additional fixed covariate in the AFT-BART-NP model formulation can further improve its efficiency and frequentist coverage. Finally, we demonstrate the application of flexible causal machine learning estimators through a comprehensive case study examining the heterogeneous survival effects of two radiotherapy approaches for localized high-risk prostate cancer.},
  archive      = {J_SIM},
  author       = {Liangyuan Hu and Jiayi Ji and Fan Li},
  doi          = {10.1002/sim.9090},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4691-4713},
  shortjournal = {Stat. Med.},
  title        = {Estimating heterogeneous survival treatment effect in observational data using machine learning},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing categorical time series in the presence of missing
observations. <em>SIM</em>, <em>40</em>(21), 4675–4690. (<a
href="https://doi.org/10.1002/sim.9089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real applications, time series often exhibit missing observations such that standard analytical tools cannot be applied. While there are approaches of how to handle missing data in quantitative time series, the case of categorical time series seems not to have been treated so far. Both for the case of ordinal and nominal time series, solutions are developed that allow to analyze their marginal and serial properties in the presence of missing observations. This is achieved by adapting the concept of amplitude modulation, which allows to obtain closed-form asymptotic expressions for the derived statistics&#39; distribution (assuming that missingness happens independently of the actual process). The proposed methods are investigated with simulations, and they are applied in a project on migraine patients, where the monitored qualitative time series are often incomplete.},
  archive      = {J_SIM},
  author       = {Christian H. Weiß},
  doi          = {10.1002/sim.9089},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4675-4690},
  shortjournal = {Stat. Med.},
  title        = {Analyzing categorical time series in the presence of missing observations},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of multiple imputation strategies for handling
missing data in multi-item scales: Guidance for longitudinal studies.
<em>SIM</em>, <em>40</em>(21), 4660–4674. (<a
href="https://doi.org/10.1002/sim.9088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical research often involves using multi-item scales to assess individual characteristics, disease severity, and other health-related outcomes. It is common to observe missing data in the scale scores, due to missing data in one or more items that make up that score. Multiple imputation (MI) is a popular method for handling missing data. However, it is not clear how best to use MI in the context of scale scores, particularly when they are assessed at multiple waves of data collection resulting in large numbers of items. The aim of this article is to provide practical advice on how to impute missing values in a repeatedly measured multi-item scale using MI when inference on the scale score is of interest. We evaluated the performance of five MI strategies for imputing missing data at either the item or scale level using simulated data and a case study based on four waves of the Longitudinal Study of Australian Children (LSAC). MI was implemented using both multivariate normal imputation and fully conditional specification, with two rules for calculating the scale score. A complete case analysis was also performed for comparison. Based on our results, we caution against the use of a MI strategy that does not include the scale score in the imputation model(s) when the scale score is required for analysis.},
  archive      = {J_SIM},
  author       = {Rheanna Mainzer and Jemishabye Apajee and Cattram D. Nguyen and John B. Carlin and Katherine J. Lee},
  doi          = {10.1002/sim.9088},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4660-4674},
  shortjournal = {Stat. Med.},
  title        = {A comparison of multiple imputation strategies for handling missing data in multi-item scales: Guidance for longitudinal studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference in functional mixed regression models with
applications to positron emission tomography imaging data. <em>SIM</em>,
<em>40</em>(21), 4640–4659. (<a
href="https://doi.org/10.1002/sim.9087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a function-on-scalar regression framework, we present some modeling strategies for functional mixed models and also some approaches for making inference about various aspects of the fixed effects. This is presented in the context of modeling positron emission tomography (PET) data in order to explore the density of various proteins of interest throughout the human brain. For this application, information about the density of the target protein in a given brain region is encapsulated in the impulse response function (IRF) of the region. Previous work on nonparametric estimation of the IRF is limited in that it is only able to model a single brain region at a time. We propose an extension, based on principles of functional data analysis, that will allow modeling of multiple brain regions simultaneously. Applicable more broadly to functional mixed regression modeling, we discuss two general approaches for permutation testing and describe valid strategies for identifying exchangeable units within the model and building corresponding permutation tests. We illustrate our methods with an application to PET data and explore the effects of depression and sex on the IRF.},
  archive      = {J_SIM},
  author       = {Baoyi Shi and R. Todd Ogden},
  doi          = {10.1002/sim.9087},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4640-4659},
  shortjournal = {Stat. Med.},
  title        = {Inference in functional mixed regression models with applications to positron emission tomography imaging data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian hierarchical monitoring design for phase II
cancer clinical trials: Incorporating information on response duration
into monitoring rules. <em>SIM</em>, <em>40</em>(21), 4629–4639. (<a
href="https://doi.org/10.1002/sim.9084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian hierarchical monitoring design for single-arm phase II clinical trials of cancer treatments that incorporates the information on the duration of response (DOR) into the monitoring rules. To screen a new treatment by evaluating its preliminary therapeutic effect, futility monitoring rules are commonly used in phase II clinical trials to make “go/no-go” decisions timely and efficiently. These futility monitoring rules are usually focused on a single outcome (eg, response rate), although a single outcome may not adequately determine the efficacy of the experimental treatment. For example, targeted agents with a long response duration but a similar response rate may be worth further evaluation in cancer research. To address this issue, we propose Bayesian hierarchical futility monitoring rules to consider both the response rate and duration. The first level of monitoring evaluates whether the response rate provides evidence that the experimental treatment is worthy of further evaluation. If the evidence from the response rate does not support continuing the trial, the second level monitoring rule, which is based on the DOR, will be triggered. If both stopping rules are satisfied, the trial will be stopped for futility. We conducted simulation studies to evaluate the operating characteristics of the proposed monitoring rules and compared them to those of standard method. We illustrated the proposed design with a single-arm phase II cancer clinical trial to assess the safety and efficacy of combined treatment of nivolumab and azacitidine in patients with relapsed/refractory acute myeloid leukemia. The proposed design avoids an aggressive early termination for futility when the experimental treatment substantially prolongs the DOR but fails to improve the response rate.},
  archive      = {J_SIM},
  author       = {Jian Wang and Junsheng Ma and Chunyan Cai and Naval Daver and Jing Ning},
  doi          = {10.1002/sim.9084},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4629-4639},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical monitoring design for phase II cancer clinical trials: Incorporating information on response duration into monitoring rules},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian sensitivity analyses for longitudinal data with
dropouts that are potentially missing not at random: A high dimensional
pattern-mixture model. <em>SIM</em>, <em>40</em>(21), 4609–4628. (<a
href="https://doi.org/10.1002/sim.9083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized clinical trials with outcome measured longitudinally are frequently analyzed using either random effect models or generalized estimating equations. Both approaches assume that the dropout mechanism is missing at random (MAR) or missing completely at random (MCAR). We propose a Bayesian pattern-mixture model to incorporate missingness mechanisms that might be missing not at random (MNAR), where the distribution of the outcome measure at the follow-up time , conditional on the prior history, differs across the patterns of missing data. We then perform sensitivity analysis on estimates of the parameters of interest. The sensitivity parameters relate the distribution of the outcome of interest between subjects from a missing-data pattern at time with that of the observed subjects at time . The large number of the sensitivity parameters is reduced by treating them as random with a prior distribution having some pre-specified mean and variance, which are varied to explore the sensitivity of inferences. The missing at random (MAR) mechanism is a special case of the proposed model, allowing a sensitivity analysis of deviations from MAR. The proposed approach is applied to data from the Trial of Preventing Hypertension.},
  archive      = {J_SIM},
  author       = {Niko A. Kaciroti and Roderick J.A. Little},
  doi          = {10.1002/sim.9083},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4609-4628},
  shortjournal = {Stat. Med.},
  title        = {Bayesian sensitivity analyses for longitudinal data with dropouts that are potentially missing not at random: A high dimensional pattern-mixture model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A ROC-based test for evaluating the group difference with an
application to neonatal audiology screening. <em>SIM</em>,
<em>40</em>(21), 4597–4608. (<a
href="https://doi.org/10.1002/sim.9082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a powerful method to compare two samples. The proposed method handles comparison of data by drawing inference from ROC curve model parameters. The method estimates parameters from a linear model framework on the empirical sensitivities and specificities. The consistent ROC parameters are then used to give a more powerful test than existing methods in several situations. In addition, we present a comprehensive statistic based on the Cauchy combination, which works well in all scenarios considered in this article. We also offer an efficient one-layer wild permutation procedure to calculate the P -value of our statistic. The method is particularly useful when the underlying continuous biomarker results are non-normal. We illustrate the proposed methods in a neonatal audiology diagnostic example.},
  archive      = {J_SIM},
  author       = {Larry L. Tang and Zhen Meng and Qizhai Li},
  doi          = {10.1002/sim.9082},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4597-4608},
  shortjournal = {Stat. Med.},
  title        = {A ROC-based test for evaluating the group difference with an application to neonatal audiology screening},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint modeling of longitudinal data with informative cluster
size adjusted for zero-inflation and a dependent terminal event.
<em>SIM</em>, <em>40</em>(21), 4582–4596. (<a
href="https://doi.org/10.1002/sim.9081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repeated measures are often collected in longitudinal follow-up from clinical trials and observational studies. In many situations, these measures are adherent to some specific event and are only available when it occurs; an example is serum creatinine from laboratory tests for hospitalized acute kidney injuries. The frequency of event recurrences is potentially correlated with overall health condition and hence may influence the distribution of the outcome measure of interest, leading to informative cluster size. In particular, there may be a large portion of subjects without any events, thus no longitudinal measures are available, which may be due to insusceptibility to such events or censoring before any events, and this zero-inflation nature of the data needs to be taken into account. On the other hand, there often exists a terminal event that may be correlated with the recurrent events. Previous work in this area suffered from the limitation that not all these issues were handled simultaneously. To address this deficiency, we propose a novel joint modeling approach for longitudinal data adjusting for zero-inflated and informative cluster size as well as a terminal event. A three-stage semiparametric likelihood-based approach is applied for parameter estimation and inference. Extensive simulations are conducted to evaluate the performance of our proposal. Finally, we utilize the Assessment, Serial Evaluation, and Subsequent Sequelae of Acute Kidney Injury (ASSESS-AKI) study for illustration.},
  archive      = {J_SIM},
  author       = {Biyi Shen and Chixiang Chen and Danping Liu and Somnath Datta and Nasrollah Ghahramani and Vernon M. Chinchilli and Ming Wang},
  doi          = {10.1002/sim.9081},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4582-4596},
  shortjournal = {Stat. Med.},
  title        = {Joint modeling of longitudinal data with informative cluster size adjusted for zero-inflation and a dependent terminal event},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Seamless phase i/II design for novel anticancer agents with
competing disease progression. <em>SIM</em>, <em>40</em>(21), 4568–4581.
(<a href="https://doi.org/10.1002/sim.9080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecularly targeted agents and immunotherapies have prolonged administration and complicated toxicity and efficacy profiles requiring longer toxicity observation windows and the inclusion of efficacy information to identify the optimal dose. Methods have been proposed to either jointly model toxicity and efficacy, or for prolonged observation windows. However, it is inappropriate to address these issues individually in the setting of dose-finding because longer toxicity windows increase the risk of patients experiencing disease progression and discontinuing the trial, with progression defining a competing event to toxicity, and progression-free survival being a commonly used efficacy endpoint. No method has been proposed to address this issue in a competing risk framework. We propose a seamless phase I/II design, namely the competing risks continual reassessment method (CR-CRM). Given an observation window, the objective is to recommend doses that minimize the progression probability, among a set of tolerable doses in terms of toxicity risk. In toxicity-centered stage of the design, doses are assigned based on toxicity alone, and in optimization stage of the design, doses are assigned integrating both toxicity and progression information. Design operating characteristics were examined in a simulation study compared with benchmark performances, including sensitivity to time-varying hazards and correlated events. The method performs well in selecting doses with acceptable toxicity risk and minimum progression risk across a wide range of scenarios.},
  archive      = {J_SIM},
  author       = {Lucie Biard and Shing M. Lee and Bin Cheng},
  doi          = {10.1002/sim.9080},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4568-4581},
  shortjournal = {Stat. Med.},
  title        = {Seamless phase I/II design for novel anticancer agents with competing disease progression},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrated multiple mediation analysis: A
robustness-specificity trade-off in causal structure. <em>SIM</em>,
<em>40</em>(21), 4541–4567. (<a
href="https://doi.org/10.1002/sim.9079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methodological developments in causal mediation analysis have addressed several issues regarding multiple mediators. However, these developed methods differ in their definitions of causal parameters, assumptions for identification, and interpretations of causal effects, making it unclear which method ought to be selected when investigating a given causal effect. Thus, in this study, we construct an integrated framework, which unifies all existing methodologies, as a standard for mediation analysis with multiple mediators. To clarify the relationship between existing methods, we propose four strategies for effect decomposition: two-way, partially forward, partially backward, and complete decompositions. This study reveals how the direct and indirect effects of each strategy are explicitly and correctly interpreted as path-specific effects under different causal mediation structures. In the integrated framework, we further verify the utility of the interventional analogues of direct and indirect effects, especially when natural direct and indirect effects cannot be identified or when crossworld exchangeability is invalid. Consequently, this study yields a robustness-specificity trade-off in the choice of strategies. Inverse probability weighting is considered for estimation. The four strategies are further applied to a simulation study for performance evaluation and for analyzing the Risk Evaluation of Viral Load Elevation and Associated Liver Disease/Cancer dataset from Taiwan to investigate the causal effect of hepatitis C virus infection on mortality.},
  archive      = {J_SIM},
  author       = {An-Shun Tai and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.9079},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {4541-4567},
  shortjournal = {Stat. Med.},
  title        = {Integrated multiple mediation analysis: A robustness-specificity trade-off in causal structure},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and construction of confidence intervals for
biomarker cutoff-points under the shortest euclidean distance from the
ROC surface to the perfection corner. <em>SIM</em>, <em>40</em>(20),
4522–4539. (<a href="https://doi.org/10.1002/sim.9077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic ductal adenocarcinoma (PDAC) is an aggressive type of cancer with a 5-year survival rate of less than 5%. As in many other diseases, its diagnosis might involve progressive stages. It is common that in biomarker studies referring to PDAC, recruitment involves three groups: healthy individuals, patients that suffer from chronic pancreatitis, and PDAC patients. Early detection and accurate classification of the state of the disease are crucial for patients&#39; successful treatment. ROC analysis is the most popular way to evaluate the performance of a biomarker and the Youden index is commonly employed for cutoff derivation. The so-called generalized Youden index has a drawback in the three-class case of not accommodating the full data set when estimating the optimal cutoffs. In this article, we explore the use of the Euclidean distance of the ROC to the perfection corner for the derivation of cutoffs in trichotomous settings. We construct an inferential framework that involves both parametric and nonparametric techniques. Our methods can accommodate the full information of a given data set and thus provide more accurate estimates in terms of the decision-making cutoffs compared with a Youden-based strategy. We evaluate our approaches through extensive simulations and illustrate them on a PDAC biomarker study.},
  archive      = {J_SIM},
  author       = {Brian R. Mosier and Leonidas E. Bantis},
  doi          = {10.1002/sim.9077},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4522-4539},
  shortjournal = {Stat. Med.},
  title        = {Estimation and construction of confidence intervals for biomarker cutoff-points under the shortest euclidean distance from the ROC surface to the perfection corner},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How vague is vague? How informative is informative?
Reference analysis for bayesian meta-analysis. <em>SIM</em>,
<em>40</em>(20), 4505–4521. (<a
href="https://doi.org/10.1002/sim.9076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis provides important insights for evidence-based medicine by synthesizing evidence from multiple studies which address the same research question. Within the Bayesian framework, meta-analysis is frequently expressed by a Bayesian normal-normal hierarchical model (NNHM). Recently, several publications have discussed the choice of the prior distribution for the between-study heterogeneity in the Bayesian NNHM and used several “vague” priors. However, no approach exists to quantify the informativeness of such priors, and thus, we develop a principled reference analysis framework for the Bayesian NNHM acting at the posterior level. The posterior reference analysis (post-RA) is based on two posterior benchmarks: one induced by the improper reference prior, which is minimally informative for the data, and the other induced by a highly anticonservative proper prior. This approach applies the Hellinger distance to quantify the informativeness of a heterogeneity prior of interest by comparing the corresponding marginal posteriors with both posterior benchmarks. The post-RA is implemented in the freely accessible R package ra4bayesmeta and is applied to two medical case studies. Our findings show that anticonservative heterogeneity priors produce platykurtic posteriors compared with the reference posterior, and they produce shorter 95% credible intervals (CrI) and optimistic inference compared with the reference prior. Conservative heterogeneity priors produce leptokurtic posteriors, longer 95% CrI and cautious inference. The novel post-RA framework could support numerous Bayesian meta-analyses in many research fields, as it determines how informative a heterogeneity prior is for the actual data as compared with the minimally informative reference prior.},
  archive      = {J_SIM},
  author       = {Manuela Ott and Martyn Plummer and Małgorzata Roos},
  doi          = {10.1002/sim.9076},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4505-4521},
  shortjournal = {Stat. Med.},
  title        = {How vague is vague? how informative is informative? reference analysis for bayesian meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The mechanistic analysis of founder virus data in challenge
models. <em>SIM</em>, <em>40</em>(20), 4492–4504. (<a
href="https://doi.org/10.1002/sim.9075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repeated low-dose challenge studies provide valuable information when evaluating candidate vaccines since they resemble the typical exposure of natural transmission and inform on the number of exposures prior to infection. Traditionally, the number of challenges to infection has been used as the outcome. This work uses the number of infecting viruses, or founder viruses at the time of infection, to more efficiently characterize a vaccine&#39;s mechanism of action. The vaccine mechanisms of action we consider are a Null mechanism (the vaccine offers no protection), a Leaky mechanism in which the number of founder viruses is reduced by some factor in vaccinated subjects, the All-or-None mechanism in which the vaccine randomly provides either complete protection or no protection in vaccinated subjects, and a Combination mechanism with both Leaky and All-or-None components. We consider two discrete marked survival models where the number of founder viruses follows a Poisson distribution with either a fixed mean parameter (Poisson model), or a random mean parameter that follows a Gamma distribution (negative binomial model). We estimate the models using maximum likelihood and derive likelihood ratio testing procedures that are accurate for small samples with boundary parameters. We illustrate the performance of these methodologies with a data example of simian immunodeficiency virus on nonhuman primates and a simulation study.},
  archive      = {J_SIM},
  author       = {Ana M. Ortega-Villa and Martha C. Nason and Dean Follmann},
  doi          = {10.1002/sim.9075},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4492-4504},
  shortjournal = {Stat. Med.},
  title        = {The mechanistic analysis of founder virus data in challenge models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust estimation and variable selection for the accelerated
failure time model. <em>SIM</em>, <em>40</em>(20), 4473–4491. (<a
href="https://doi.org/10.1002/sim.9042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns robust modeling of the survival time for cancer patients. Accurate prediction of patient survival time is crucial to the development of effective therapeutic strategies. To this goal, we propose a unified Expectation-Maximization approach combined with the L 1 -norm penalty to perform variable selection and parameter estimation simultaneously in the accelerated failure time model with right-censored survival data of moderate sizes. Our approach accommodates general loss functions, and reduces to the well-known Buckley-James method when the squared-error loss is used without regularization. To mitigate the effects of outliers and heavy-tailed noise in real applications, we recommend the use of robust loss functions under the general framework. Furthermore, our approach can be extended to incorporate group structure among covariates. We conduct extensive simulation studies to assess the performance of the proposed methods with different loss functions and apply them to an ovarian carcinoma study as an illustration.},
  archive      = {J_SIM},
  author       = {Yi Li and Muxuan Liang and Lu Mao and Sijian Wang},
  doi          = {10.1002/sim.9042},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4473-4491},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation and variable selection for the accelerated failure time model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian phase II clinical trial design with noncompliance.
<em>SIM</em>, <em>40</em>(20), 4457–4472. (<a
href="https://doi.org/10.1002/sim.9041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noncompliance issue is common in early phase clinical trials; and may lead to biased estimation of the intent-to-treat effect and incorrect conclusions for the clinical trial. In this work, we propose a Bayesian approach for sequentially monitoring the phase II randomized clinical trials that takes account for the noncompliance information. We adopt the principal stratification framework and propose to use Bayesian additive regression trees for selecting useful baseline covariates and estimating the complier average causal effect (CACE) for both efficacy and toxicity outcomes. The decision of early termination or not is then made adaptively based on the estimated CACE from the accumulated data. Simulation studies have confirmed the excellent performance of the proposed design in the presence of noncompliance.},
  archive      = {J_SIM},
  author       = {Tingyang Ren and Weining Shen and Liwen Zhang and Haibing Zhao},
  doi          = {10.1002/sim.9041},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4457-4472},
  shortjournal = {Stat. Med.},
  title        = {Bayesian phase II clinical trial design with noncompliance},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomization-based inference for a marginal treatment
effect in stepped wedge cluster randomized trials. <em>SIM</em>,
<em>40</em>(20), 4442–4456. (<a
href="https://doi.org/10.1002/sim.9040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a cross-sectional stepped wedge cluster randomized trial (SWT), clusters are randomized to crossover from control to intervention at different time periods and outcomes are assessed for a different set of individuals in each cluster-period. Randomization-based inference is an attractive analysis strategy for SWTs because it does not require full parametric specification of the outcome distribution or correlation structure and its validity does not rely on having a large number of clusters. Existing randomization-based approaches for SWTs, however, either focus on hypothesis testing and omit technical details on confidence interval (CI) calculation with noncontinuous outcomes, or employ weighted cluster-period summary statistics for p -value and CI calculation, which can result in suboptimal efficiency if weights do not incorporate information on varying cluster-period sizes. In this article, we propose a framework for calculating randomization-based p -values and CIs for a marginal treatment effect in SWTs by using test statistics derived from individual-level generalized linear models. We also investigate how study design features, such as stratified randomization, subsequently impact various SWT analysis methods including the proposed approach. Data from the XpertMTB/RIF tuberculosis trial are reanalyzed to illustrate our method and compare it to alternatives.},
  archive      = {J_SIM},
  author       = {Dustin J. Rabideau and Rui Wang},
  doi          = {10.1002/sim.9040},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4442-4456},
  shortjournal = {Stat. Med.},
  title        = {Randomization-based inference for a marginal treatment effect in stepped wedge cluster randomized trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical insights for crude-rate-based operational
measures of misdiagnosis-related harms. <em>SIM</em>, <em>40</em>(20),
4430–4441. (<a href="https://doi.org/10.1002/sim.9039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal event data, a crude rate is a simple quantification of the event rate, defined as the number of events during an evaluation window, divided by the at-risk population size at the beginning or mid-time point of that window. The crude rate recently received revitalizing interest from medical researchers who aimed to improve measurement of misdiagnosis-related harms using administrative or billing data by tracking unexpected adverse events following a “benign” diagnosis. The simplicity of these measures makes them attractive for implementation and routine operational monitoring at hospital or health system level. However, relevant statistical inference procedures have not been systematically summarized. Moreover, it is unclear to what extent the temporal changes of the at-risk population size would bias analyses and affect important conclusions concerning misdiagnosis-related harms. In this article, we present statistical inference tools for using crude-rate based harm measures, as well as formulas and simulation results that quantify the deviation of such measures from those based on the more sophisticated Nelson-Aalen estimator. Moreover, we present results for a generalized multibin version of the crude rate, for which the usual crude rate is a single-bin special case. The generalized multibin crude rate is more straightforward to compute than the Nelson-Aalen estimator and can reduce potential biases of the single-bin crude rate. For studies that seek to use multibin measures, we provide simulations to guide the choice regarding number of bins. We further bolster these results using a worked example of stroke after “benign” dizziness from a large data set.},
  archive      = {J_SIM},
  author       = {Yuxin Zhu and Zheyu Wang and Ava L. Liberman and Tzu-Pu Chang and David Newman-Toker},
  doi          = {10.1002/sim.9039},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4430-4441},
  shortjournal = {Stat. Med.},
  title        = {Statistical insights for crude-rate-based operational measures of misdiagnosis-related harms},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic latent variable models for the analysis of cognitive
abilities in the elderly population. <em>SIM</em>, <em>40</em>(20),
4410–4429. (<a href="https://doi.org/10.1002/sim.9038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive functioning is a key indicator of overall individual health. Identifying factors related to cognitive status, especially in later life, is of major importance. We concentrate on the analysis of the temporal evolution of cognitive abilities in the elderly population. We propose to model the individual cognitive functioning as a multidimensional latent process that accounts also for the effects of individual-specific characteristics (gender, age, and years of education). The proposed model is specified within the generalized linear latent variable framework, and its efficient estimation is obtained using a recent approximation technique, called dimensionwise quadrature. It provides a fast and streamlined approximate inference for complex models, with better or no degradation in accuracy compared with standard techniques. The methodology is applied to the cognitive assessment data from the Health and Retirement Study combined with the Asset and Health Dynamic study in the years between 2006 and 2010. We evaluate the temporal relationship between two dimensions of cognitive functioning, that is, episodic memory and general mental status. We find a substantial influence of the former on the evolution of the latter, as well as evidence of severe consequences on both cognitive abilities among less-educated and older individuals.},
  archive      = {J_SIM},
  author       = {Silvia Bianconcini and Silvia Cagnone},
  doi          = {10.1002/sim.9038},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4410-4429},
  shortjournal = {Stat. Med.},
  title        = {Dynamic latent variable models for the analysis of cognitive abilities in the elderly population},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Backward joint model and dynamic prediction of survival with
multivariate longitudinal data. <em>SIM</em>, <em>40</em>(20),
4395–4409. (<a href="https://doi.org/10.1002/sim.9037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important approach to dynamic prediction of time-to-event outcomes using longitudinal data is based on modeling the joint distribution of longitudinal and time-to-event data. The widely used joint model for this purpose is the shared random effect model. Presumably, adding more longitudinal predictors improves the predictive accuracy. However, the shared random effect model can be computationally difficult or prohibitive when a large number of longitudinal variables are used. In this paper, we study an alternative way of modeling the joint distribution of longitudinal and time-to-event data. Under this formulation, the log-likelihood involves no more than one-dimensional integration, regardless of the number of longitudinal variables in the model. Therefore, this model is particularly suitable in dynamic prediction problems with large number of longitudinal predictors. The model fitting can be implemented with tractable and stable computation by using a combination of pseudo maximum likelihood estimation, Expectation-Maximization algorithm, and convex optimization. We evaluate the proposed methodology and its predictive accuracy with varying number of longitudinal variables using simulations and data from a primary biliary cirrhosis study.},
  archive      = {J_SIM},
  author       = {Fan Shen and Liang Li},
  doi          = {10.1002/sim.9037},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4395-4409},
  shortjournal = {Stat. Med.},
  title        = {Backward joint model and dynamic prediction of survival with multivariate longitudinal data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric regression analysis of partly
interval-censored failure time data with application to an AIDS clinical
trial. <em>SIM</em>, <em>40</em>(20), 4376–4394. (<a
href="https://doi.org/10.1002/sim.9035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure time data subject to various types of censoring commonly arise in epidemiological and biomedical studies. Motivated by an AIDS clinical trial, we consider regression analysis of failure time data that include exact and left-, interval-, and/or right-censored observations, which are often referred to as partly interval-censored failure time data. We study the effects of potentially time-dependent covariates on partly interval-censored failure time via a class of semiparametric transformation models that includes the widely used proportional hazards model and the proportional odds model as special cases. We propose an EM algorithm for the nonparametric maximum likelihood estimation and show that it unifies some existing approaches developed for traditional right-censored data or purely interval-censored data. In particular, the proposed method reduces to the partial likelihood approach in the case of right-censored data under the proportional hazards model. We establish that the resulting estimator is consistent and asymptotically normal. In addition, we investigate the proposed method via simulation studies and apply it to the motivating AIDS clinical trial.},
  archive      = {J_SIM},
  author       = {Qingning Zhou and Yanqing Sun and Peter B. Gilbert},
  doi          = {10.1002/sim.9035},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4376-4394},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric regression analysis of partly interval-censored failure time data with application to an AIDS clinical trial},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage prediction model for heterogeneous effects of
treatments. <em>SIM</em>, <em>40</em>(20), 4362–4375. (<a
href="https://doi.org/10.1002/sim.9034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment effects vary across different patients, and estimation of this variability is essential for clinical decision-making. We aimed to develop a model estimating the benefit of alternative treatment options for individual patients, extending a risk modeling approach in a network meta-analysis framework. We propose a two-stage prediction model for heterogeneous treatment effects by combining prognosis research and network meta-analysis methods where individual patient data are available. In the first stage, a prognostic model to predict the baseline risk of the outcome. In the second stage, we use the baseline risk score from the first stage as a single prognostic factor and effect modifier in a network meta-regression model. We apply the approach to a network meta-analysis of three randomized clinical trials comparing the relapses in Natalizumab, Glatiramer Acetate, and Dimethyl Fumarate, including 3590 patients diagnosed with relapsing-remitting multiple sclerosis. We find that the baseline risk score modifies the relative and absolute treatment effects. Several patient characteristics, such as age and disability status, impact the baseline risk of relapse, which in turn moderates the benefit expected for each of the treatments. For high-risk patients, the treatment that minimizes the risk of relapse in 2 years is Natalizumab, whereas Dimethyl Fumarate might be a better option for low-risk patients. Our approach can be easily extended to all outcomes of interest and has the potential to inform a personalized treatment approach.},
  archive      = {J_SIM},
  author       = {Konstantina Chalkou and Ewout Steyerberg and Matthias Egger and Andrea Manca and Fabio Pellegrini and Georgia Salanti},
  doi          = {10.1002/sim.9034},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4362-4375},
  shortjournal = {Stat. Med.},
  title        = {A two-stage prediction model for heterogeneous effects of treatments},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using population crossover trials to improve the decision
process regarding treatment individualization in n-of-1 trials.
<em>SIM</em>, <em>40</em>(20), 4345–4361. (<a
href="https://doi.org/10.1002/sim.9030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare researchers are showing renewed interest in the utilization of N-of-1 clinical trials for the individualization of pharmacological treatments. Here, we propose a frequentist approach to conducting treatment individualization in N-of-1 trials that we call “partial empirical Bayes.” We infer the most beneficial treatment for the patient from combining the information provided by a previously conducted population crossover trial with individual patient data. We propose a method for estimating an optimal number of treatment cycles and investigate the statistical conditions under which N-of-1 trials are more beneficial than traditional clinical approaches. We represent the patient population with a random-coefficients linear model and calculate estimators of posttreatment individual disease severities. We show the estimators&#39; consistency under the most common N-of-1 designs and examine their prediction errors and performance with small numbers of patient&#39;s responses. We demonstrate by simulating new patients that our approach is equivalent or superior to both the common clinical practice of recommending the on-average best treatment for all patients and the common individualization method that simply compares average responses to the tested treatments. We conclude that some situations exist in which individualization with N-of-1 trials is highly beneficial while other situations exist in which individualization may be unfruitful.},
  archive      = {J_SIM},
  author       = {Francisco J. Diaz},
  doi          = {10.1002/sim.9030},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {4345-4361},
  shortjournal = {Stat. Med.},
  title        = {Using population crossover trials to improve the decision process regarding treatment individualization in N-of-1 trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survival analysis using a 5-step stratified testing and
amalgamation routine (5-STAR) in randomized clinical trials.
<em>SIM</em>, <em>40</em>(19), 4341–4343. (<a
href="https://doi.org/10.1002/sim.9116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Devan V. Mehrotra and Rachel Marceau West},
  doi          = {10.1002/sim.9116},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4341-4343},
  shortjournal = {Stat. Med.},
  title        = {Survival analysis using a 5-step stratified testing and amalgamation routine (5-STAR) in randomized clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient study design to estimate population means with
multiple measurement instruments. <em>SIM</em>, <em>40</em>(19),
4327–4340. (<a href="https://doi.org/10.1002/sim.9032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outcomes from studies assessing exposure often use multiple measurements. In previous work, using a model first proposed by Buonoccorsi (1991), we showed that combining direct (eg, biomarkers) and indirect (eg, self-report) measurements provides a more accurate picture of true exposure than estimates obtained when using a single type of measurement. In this article, we propose a tool for efficient design of studies that include both direct and indirect measurements of a relevant outcome. Based on data from a pilot or preliminary study, the tool, which is available online as a shiny app at https://michalbitan.shinyapps.io/shinyApp/ , can be used to compute: (1) the sample size required for a statistical power analysis, while optimizing the percent of participants who should provide direct measures of exposure (biomarkers) in addition to the indirect (self-report) measures provided by all participants; (2) the ideal number of replicates; and (3) the allocation of resources to intervention and control arms. In addition we show how to examine the sensitivity of results to underlying assumptions. We illustrate our analysis using studies of tobacco smoke exposure and nutrition. In these examples, a near-optimal allocation of the resources can be found even if the assumptions are not precise.},
  archive      = {J_SIM},
  author       = {Michal Bitan and Malka Gorfine and Laura Rosen and David M. Steinberg},
  doi          = {10.1002/sim.9032},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4327-4340},
  shortjournal = {Stat. Med.},
  title        = {Efficient study design to estimate population means with multiple measurement instruments},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transporting experimental results with entropy balancing.
<em>SIM</em>, <em>40</em>(19), 4310–4326. (<a
href="https://doi.org/10.1002/sim.9031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show how entropy balancing can be used for transporting experimental treatment effects from a trial population onto a target population. This method is doubly robust in the sense that if either the outcome model or the probability of trial participation is correctly specified, then the estimate of the target population average treatment effect is consistent. Furthermore, we only require the sample moments of the effect modifiers drawn from the target population to consistently estimate the target population average treatment effect. We compared the finite-sample performance of entropy balancing with several alternative methods for transporting treatment effects between populations. Entropy balancing techniques are efficient and robust to violations of model misspecification. We also examine the results of our proposed method in an applied analysis of the Action to Control Cardiovascular Risk in Diabetes Blood Pressure trial transported to a sample of US adults with diabetes taken from the National Health and Nutrition Examination Survey cohort.},
  archive      = {J_SIM},
  author       = {Kevin P. Josey and Seth A. Berkowitz and Debashis Ghosh and Sridharan Raghavan},
  doi          = {10.1002/sim.9031},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4310-4326},
  shortjournal = {Stat. Med.},
  title        = {Transporting experimental results with entropy balancing},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propensity score weighting for causal subgroup analysis.
<em>SIM</em>, <em>40</em>(19), 4294–4309. (<a
href="https://doi.org/10.1002/sim.9029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common goal in comparative effectiveness research is to estimate treatment effects on prespecified subpopulations of patients. Though widely used in medical research, causal inference methods for such subgroup analysis (SGA) remain underdeveloped, particularly in observational studies. In this article, we develop a suite of analytical methods and visualization tools for causal SGA. First, we introduce the estimand of subgroup weighted average treatment effect and provide the corresponding propensity score weighting estimator. We show that balancing covariates within a subgroup bounds the bias of the estimator of subgroup causal effects. Second, we propose to use the overlap weighting (OW) method to achieve exact balance within subgroups. We further propose a method that combines OW and LASSO, to balance the bias-variance tradeoff in SGA. Finally, we design a new diagnostic graph—the Connect-S plot—for visualizing the subgroup covariate balance. Extensive simulation studies are presented to compare the proposed method with several existing methods. We apply the proposed methods to the patient-centered results for uterine fibroids (COMPARE-UF) registry data to evaluate alternative management options for uterine fibroids for relief of symptoms and quality of life.},
  archive      = {J_SIM},
  author       = {Siyun Yang and Elizabeth Lorenzi and Georgia Papadogeorgou and Daniel M. Wojdyla and Fan Li and Laine E. Thomas},
  doi          = {10.1002/sim.9029},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4294-4309},
  shortjournal = {Stat. Med.},
  title        = {Propensity score weighting for causal subgroup analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized estimation of the gaussian graphical model from
data with replicates. <em>SIM</em>, <em>40</em>(19), 4279–4293. (<a
href="https://doi.org/10.1002/sim.9028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models are usually estimated from unreplicated data. The data are, however, likely to comprise signal and noise. These two cannot be deconvoluted from unreplicated data. Pragmatically, the noise is then ignored in practice. We point out the consequences of this practice for the reconstruction of the conditional independence graph of the signal. Replicated data allow for the deconvolution of signal and noise and the reconstruction of former&#39;s conditional independence graph. Hereto we present a penalized Expectation-Maximization algorithm. The penalty parameter is chosen to maximize the F -fold cross-validated log-likelihood. Sampling schemes of the folds from replicated data are discussed. By simulation we investigate the effect of replicates on the reconstruction of the signal&#39;s conditional independence graph. Moreover, we compare the proposed method to several obvious competitors. In an application we use data from oncogenomic studies with replicates to reconstruct the gene-gene interaction networks, operationalized as conditional independence graphs. This yields a realistic portrait of the effect of ignoring other sources but sampling variation. In addition, it bears implications on the reproducibility of inferred gene-gene interaction networks reported in literature.},
  archive      = {J_SIM},
  author       = {Wessel N. van Wieringen and Yao Chen},
  doi          = {10.1002/sim.9028},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4279-4293},
  shortjournal = {Stat. Med.},
  title        = {Penalized estimation of the gaussian graphical model from data with replicates},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vaccine adverse event enrichment tests. <em>SIM</em>,
<em>40</em>(19), 4269–4278. (<a
href="https://doi.org/10.1002/sim.9027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vaccination safety is critical for individual and public health. Many existing methods have been used to conduct safety studies with the VAERS (Vaccine Adverse Event Reporting System) database. However, these methods frequently identify many adverse event (AE) signals and they are often hard to interpret in a biological context. The AE ontology introduces biologically meaningful structures to the Vaccine Adverse Event Reporting System (VAERS) database by connecting similar AEs, which provides meaningful interpretation for the underlying safety issues. In this paper, we develop rigorous statistical methods to identify “interesting&quot; AE groups by performing AE enrichment analysis. We extend existing gene enrichment tests to perform AE enrichment analysis, while incorporating the special features of the AE data. The proposed methods were evaluated using simulation studies and were further illustrated on two studies using VAERS data. The proposed methods were implemented in R package AEenrich and can be installed from the Comprehensive R Archive Network, CRAN, and source code are available at https://github.com/umich-biostatistics/AEenrich .},
  archive      = {J_SIM},
  author       = {Shuoran Li and Lili Zhao},
  doi          = {10.1002/sim.9027},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4269-4278},
  shortjournal = {Stat. Med.},
  title        = {Vaccine adverse event enrichment tests},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced empirical likelihood estimation of incubation
period of COVID-19 by integrating published information. <em>SIM</em>,
<em>40</em>(19), 4252–4268. (<a
href="https://doi.org/10.1002/sim.9026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the outbreak of the new coronavirus disease (COVID-19), a large number of scientific studies and data analysis reports have been published in the International Journal of Medicine and Statistics . Taking the estimation of the incubation period as an example, we propose a low-cost method to integrate external research results and available internal data together. By using empirical likelihood method, we can effectively incorporate summarized information even if it may be derived from a misspecified model. Taking the possible uncertainty in summarized information into account, we augment a logarithm of the normal density in the log empirical likelihood. We show that the augmented log-empirical likelihood can produce enhanced estimates for the underlying parameters compared with the method without utilizing auxiliary information. Moreover, the Wilks&#39; theorem is proved to be true. We illustrate our methodology by analyzing a COVID-19 incubation period data set retrieved from Zhejiang Province and summarized information from a similar study in Shenzhen, China.},
  archive      = {J_SIM},
  author       = {Zhongfeng Jiang and Baoying Yang and Jing Qin and Yong Zhou},
  doi          = {10.1002/sim.9026},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4252-4268},
  shortjournal = {Stat. Med.},
  title        = {Enhanced empirical likelihood estimation of incubation period of COVID-19 by integrating published information},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimum sample size for external validation of a clinical
prediction model with a binary outcome. <em>SIM</em>, <em>40</em>(19),
4230–4251. (<a href="https://doi.org/10.1002/sim.9025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prediction model research, external validation is needed to examine an existing model&#39;s performance using data independent to that for model development. Current external validation studies often suffer from small sample sizes and consequently imprecise predictive performance estimates. To address this, we propose how to determine the minimum sample size needed for a new external validation study of a prediction model for a binary outcome. Our calculations aim to precisely estimate calibration (Observed/Expected and calibration slope), discrimination (C-statistic), and clinical utility (net benefit). For each measure, we propose closed-form and iterative solutions for calculating the minimum sample size required. These require specifying: (i) target SEs (confidence interval widths) for each estimate of interest, (ii) the anticipated outcome event proportion in the validation population, (iii) the prediction model&#39;s anticipated (mis)calibration and variance of linear predictor values in the validation population, and (iv) potential risk thresholds for clinical decision-making. The calculations can also be used to inform whether the sample size of an existing (already collected) dataset is adequate for external validation. We illustrate our proposal for external validation of a prediction model for mechanical heart valve failure with an expected outcome event proportion of 0.018. Calculations suggest at least 9835 participants (177 events) are required to precisely estimate the calibration and discrimination measures, with this number driven by the calibration slope criterion, which we anticipate will often be the case. Also, 6443 participants (116 events) are required to precisely estimate net benefit at a risk threshold of 8%. Software code is provided.},
  archive      = {J_SIM},
  author       = {Richard D. Riley and Thomas P. A. Debray and Gary S. Collins and Lucinda Archer and Joie Ensor and Maarten van Smeden and Kym I. E. Snell},
  doi          = {10.1002/sim.9025},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4230-4251},
  shortjournal = {Stat. Med.},
  title        = {Minimum sample size for external validation of a clinical prediction model with a binary outcome},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tractable bayesian joint model for longitudinal and
survival data. <em>SIM</em>, <em>40</em>(19), 4213–4229. (<a
href="https://doi.org/10.1002/sim.9024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a numerically tractable formulation of Bayesian joint models for longitudinal and survival data. The longitudinal process is modeled using generalized linear mixed models, while the survival process is modeled using a parametric general hazard structure. The two processes are linked by sharing fixed and random effects, separating the effects that play a role at the time scale from those that affect the hazard scale. This strategy allows for the inclusion of nonlinear and time-dependent effects while avoiding the need for numerical integration, which facilitates the implementation of the proposed joint model. We explore the use of flexible parametric distributions for modeling the baseline hazard function which can capture the basic shapes of interest in practice. We discuss prior elicitation based on the interpretation of the parameters. We present an extensive simulation study, where we analyze the inferential properties of the proposed models, and illustrate the trade-off between flexibility, sample size, and censoring. We also apply our proposal to two real data applications in order to demonstrate the adaptability of our formulation both in univariate time-to-event data and in a competing risks framework. The methodology is implemented in rstan .},
  archive      = {J_SIM},
  author       = {Danilo Alvares and Francisco J. Rubio},
  doi          = {10.1002/sim.9024},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4213-4229},
  shortjournal = {Stat. Med.},
  title        = {A tractable bayesian joint model for longitudinal and survival data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-gray subdistribution hazard models to simultaneously
estimate the absolute risk of different event types: Cumulative total
failure probability may exceed 1. <em>SIM</em>, <em>40</em>(19),
4200–4212. (<a href="https://doi.org/10.1002/sim.9023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fine-Gray subdistribution hazard model has become the default method to estimate the incidence of outcomes over time in the presence of competing risks. This model is attractive because it directly relates covariates to the cumulative incidence function (CIF) of the event of interest. An alternative is to combine the different cause-specific hazard functions to obtain the different CIFs. A limitation of the subdistribution hazard approach is that the sum of the cause-specific CIFs can exceed 1 (100%) for some covariate patterns. Using data on 9479 patients hospitalized with acute myocardial infarction, we estimated the cumulative incidence of both cardiovascular death and non-cardiovascular death for each patient. We found that when using subdistribution hazard models, approximately 5% of subjects had an estimated risk of 5-year all-cause death (obtained by combining the two cause-specific CIFs obtained from subdistribution hazard models) that exceeded 1. This phenomenon was avoided by using the two cause-specific hazard models. We provide a proof that the sum of predictions exceeds 1 is a fundamental problem with the Fine-Gray subdistribution hazard model. We further explored this issue using simulations based on two different types of data-generating process, one based on subdistribution hazard models and other based on cause-specific hazard models. We conclude that care should be taken when using the Fine-Gray subdistribution hazard model in situations with wide risk distributions or a high cumulative incidence, and if one is interested in the risk of failure from each of the different event types.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Ewout W. Steyerberg and Hein Putter},
  doi          = {10.1002/sim.9023},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4200-4212},
  shortjournal = {Stat. Med.},
  title        = {Fine-gray subdistribution hazard models to simultaneously estimate the absolute risk of different event types: Cumulative total failure probability may exceed 1},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction-driven pooled testing methods: Application to HIV
treatment monitoring in rakai, uganda. <em>SIM</em>, <em>40</em>(19),
4185–4199. (<a href="https://doi.org/10.1002/sim.9022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic medical conditions often necessitate regular testing for proper treatment. Regular testing of all afflicted individuals may not be feasible due to limited resources, as is true with HIV monitoring in resource-limited settings. Pooled testing methods have been developed in order to allow regular testing for all while reducing resource burden. However, the most commonly used methods do not make use of covariate information predictive of treatment failure, which could improve performance. We propose and evaluate four prediction-driven pooled testing methods that incorporate covariate information to improve pooled testing performance. We then compare these methods in the HIV treatment management setting to current methods with respect to testing efficiency, sensitivity, and number of testing rounds using simulated data and data collected in Rakai, Uganda. Results show that the prediction-driven methods increase efficiency by up to 20% compared with current methods while maintaining equivalent sensitivity and reducing number of testing rounds by up to 70%. When predictions were incorrect, the performance of prediction-based matrix methods remained robust. The best performing method using our motivating data from Rakai was a prediction-driven hybrid method, maintaining sensitivity over 96% and efficiency over 75% in likely scenarios. If these methods perform similarly in the field, they may contribute to improving mortality and reducing transmission in resource-limited settings. Although we evaluate our proposed pooling methods in the HIV treatment setting, they can be applied to any setting that necessitates testing of a quantitative biomarker for a threshold-based decision.},
  archive      = {J_SIM},
  author       = {Adam Brand and Susanne May and James P. Hughes and Gertrude Nakigozi and Steven J. Reynolds and Erin E. Gabriel},
  doi          = {10.1002/sim.9022},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4185-4199},
  shortjournal = {Stat. Med.},
  title        = {Prediction-driven pooled testing methods: Application to HIV treatment monitoring in rakai, uganda},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian adaptive design for clinical trials in duchenne
muscular dystrophy. <em>SIM</em>, <em>40</em>(19), 4167–4184. (<a
href="https://doi.org/10.1002/sim.9021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian adaptive design is proposed for a clinical trial in Duchenne muscular dystrophy. The trial was designed to demonstrate treatment efficacy on an ambulatory-based clinical endpoint and to identify early success on a biomarker (dystrophin protein levels) that can serve as a basis for accelerated approval in the United States. The trial incorporates placebo augmentation using placebo data from past clinical trials. A thorough simulation study was conducted to understand the operating characteristics of the trial. This trial design was selected for the US FDA Complex Innovative Trial Design Pilot Meeting Program and the experience in that program is summarized.},
  archive      = {J_SIM},
  author       = {Stephen L. Lake and Melanie A. Quintana and Kristine Broglio and Jennifer Panagoulias and Scott M. Berry and Michael A. Panzara},
  doi          = {10.1002/sim.9021},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {4167-4184},
  shortjournal = {Stat. Med.},
  title        = {Bayesian adaptive design for clinical trials in duchenne muscular dystrophy},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction: Quantifying how diagnostic test accuracy depends
on threshold in a meta-analysis. <em>SIM</em>, <em>40</em>(18), 4166.
(<a href="https://doi.org/10.1002/sim.9103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Hayley E. Jones},
  doi          = {10.1002/sim.9103},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4166},
  shortjournal = {Stat. Med.},
  title        = {Correction: Quantifying how diagnostic test accuracy depends on threshold in a meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Authors’ reply to “comments on identifying inconsistency in
network meta-analysis: Is the net heat plot a reliable method?”
<em>SIM</em>, <em>40</em>(18), 4164–4165. (<a
href="https://doi.org/10.1002/sim.9073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Suzanne C. Freeman and David Fisher and Ian R. White and James R. Carpenter},
  doi          = {10.1002/sim.9073},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4164-4165},
  shortjournal = {Stat. Med.},
  title        = {Authors&#39; reply to “Comments on identifying inconsistency in network meta-analysis: Is the net heat plot a reliable method?”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments on “identifying inconsistency in network
meta-analysis: Is the net heat plot a reliable method?” <em>SIM</em>,
<em>40</em>(18), 4161–4163. (<a
href="https://doi.org/10.1002/sim.9074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Ulrike Krahn and Harald Binder and Gerta Rücker and Jochem König},
  doi          = {10.1002/sim.9074},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4161-4163},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Identifying inconsistency in network meta-analysis: Is the net heat plot a reliable method?”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring COVID-19 contagion growth. <em>SIM</em>,
<em>40</em>(18), 4150–4160. (<a
href="https://doi.org/10.1002/sim.9020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a statistical model that can be employed to monitor the time evolution of the COVID-19 contagion curve and the associated reproduction rate. The model is a Poisson autoregression of the daily new observed cases and dynamically adapt its estimates to explain the evolution of contagion in terms of a short-term and long-term dependence of case counts, allowing for a comparative evaluation of health policy measures. We have applied the model to 2020 data from the countries most hit by the virus. Our empirical findings show that the proposed model describes the evolution of contagion dynamics and determines whether contagion growth can be affected by health policies. Based on our findings, we can draw two health policy conclusions that can be useful for all countries in the world. First, policy measures aimed at reducing contagion are very useful when contagion is at its peak to reduce the reproduction rate. Second, the contagion curve should be accurately monitored over time to apply policy measures that are cost-effective.},
  archive      = {J_SIM},
  author       = {Arianna Agosto and Alexandra Campmas and Paolo Giudici and Andrea Renda},
  doi          = {10.1002/sim.9020},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4150-4160},
  shortjournal = {Stat. Med.},
  title        = {Monitoring COVID-19 contagion growth},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel application of a discrete time-to-event model for
randomized oral immunotherapy clinical trials with repeat food
challenges. <em>SIM</em>, <em>40</em>(18), 4136–4149. (<a
href="https://doi.org/10.1002/sim.9019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of double-blind, placebo-controlled food challenges (DBPCFC) generally focuses on a participant passing a challenge at a predetermined dose, and does not consider the dose of reaction for those who fail or are censored due to study discontinuation. Further, a number of food allergy trials have incorporated multiple DBPCFCs throughout the duration of the study in order to evaluate changes in reaction over time including sustained unresponsiveness from treatment. Outcomes arising from these trials are commonly modeled using Chi-squared or Fisher&#39;s exact tests at each time point. We propose applying time-to-event methodology to food allergy trials in order to exploit the inherent granularity of challenge outcomes that additionally accommodates repeated DBPCFCs. Specifically, we consider dose-to-failure for each study challenge and extend the cumulative tolerated dose across challenges to result in a dose-time axis. A discrete time-to-event framework is applied to the dose-time outcome to assess the efficacy of treatment across the entire study period. We illustrate ideas with data from the Peanut Oral Immunotherapy Study: Safety, Efficacy and Discovery (POISED) trial, conducted at Stanford University, which evaluated the efficacy of oral immunotherapy on desensitization and sustained unresponsiveness in peanut allergic children and adults. We demonstrate the advantages of time-to-event approaches for assessing the efficacy of treatment over time and incorporating information for those who failed or were lost to follow up. Further, we introduce a dose-time outcome that is interpretable to clinicians and allows for examination of such outcomes over time.},
  archive      = {J_SIM},
  author       = {Natasha Purington and Sandra Andorf and Bryan Bunning and Sharon Chinthrajah and Kari Nadeau and Manisha Desai},
  doi          = {10.1002/sim.9019},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4136-4149},
  shortjournal = {Stat. Med.},
  title        = {Novel application of a discrete time-to-event model for randomized oral immunotherapy clinical trials with repeat food challenges},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of phase III trials with long-term survival outcomes
based on short-term binary results. <em>SIM</em>, <em>40</em>(18),
4122–4135. (<a href="https://doi.org/10.1002/sim.9018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathologic complete response (pCR) is a common primary endpoint for a phase II trial or even accelerated approval of neoadjuvant cancer therapy. If granted, a two-arm confirmatory trial is often required to demonstrate the efficacy with a time-to-event outcome such as overall survival. However, the design of a subsequent phase III trial based on prior information on the pCR effect is not straightforward. Aiming at designing such phase III trials with overall survival as primary endpoint using pCR information from previous trials, we consider a mixture model that incorporates both the survival and the binary endpoints. We propose to base the comparison between arms on the difference of the restricted mean survival times, and show how the effect size and sample size for overall survival rely on the probability of the binary response and the survival distribution by response status, both for each treatment arm. Moreover, we provide the sample size calculation under different scenarios and accompany them with the R package survmixer where all the computations have been implemented. We evaluate our proposal with a simulation study, and illustrate its application through a neoadjuvant breast cancer trial.},
  archive      = {J_SIM},
  author       = {Marta Bofill Roig and Yu Shen and Guadalupe Gómez Melis},
  doi          = {10.1002/sim.9018},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4122-4135},
  shortjournal = {Stat. Med.},
  title        = {Design of phase III trials with long-term survival outcomes based on short-term binary results},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Principled selection of baseline covariates to account for
censoring in randomized trials with a survival endpoint. <em>SIM</em>,
<em>40</em>(18), 4108–4121. (<a
href="https://doi.org/10.1002/sim.9017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of randomized trials with time-to-event endpoints is nearly always plagued by the problem of censoring. In practice, such analyses typically invoke the assumption of noninformative censoring. While this assumption usually becomes more plausible as more baseline covariates are being adjusted for, such adjustment also raises concerns. Prespecification of which covariates will be adjusted for (and how) is difficult, thus prompting the use of data-driven variable selection procedures, which may impede valid inferences to be drawn. The adjustment for covariates moreover adds concerns about model misspecification, and the fact that each change in adjustment set also changes the censoring assumption and the treatment effect estimand. In this article, we discuss these concerns and propose a simple variable selection strategy designed to produce a valid test of the null in large samples. The proposal can be implemented using off-the-shelf software for (penalized) Cox regression, and is empirically found to work well in simulation studies and real data analyses.},
  archive      = {J_SIM},
  author       = {Kelly Van Lancker and Oliver Dukes and Stijn Vansteelandt},
  doi          = {10.1002/sim.9017},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4108-4121},
  shortjournal = {Stat. Med.},
  title        = {Principled selection of baseline covariates to account for censoring in randomized trials with a survival endpoint},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal allocation in stratified cluster-based
outcome-dependent sampling designs. <em>SIM</em>, <em>40</em>(18),
4090–4107. (<a href="https://doi.org/10.1002/sim.9016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In public health research, finite resources often require that decisions be made at the study design stage regarding which individuals to sample for detailed data collection. At the same time, when study units are naturally clustered, as patients are in clinics, it may be preferable to sample clusters rather than the study units, especially when the costs associated with travel between clusters are high. In this setting, aggregated data on the outcome and select covariates are sometimes routinely available through, for example, a country&#39;s Health Management Information System. If used wisely, this information can be used to guide decisions regarding which clusters to sample, and potentially obtain gains in efficiency over simple random sampling. In this article, we derive a series of formulas for optimal allocation of resources when a single-stage stratified cluster-based outcome-dependent sampling design is to be used and a marginal mean model is specified to answer the question of interest. Specifically, we consider two settings: (i) when a particular parameter in the mean model is of primary interest; and, (ii) when multiple parameters are of interest. We investigate the finite population performance of the optimal allocation framework through a comprehensive simulation study. Our results show that there are trade-offs that must be considered at the design stage: optimizing for one parameter yields efficiency gains over balanced and simple random sampling, while resulting in losses for the other parameters in the model. Optimizing for all parameters simultaneously yields smaller gains in efficiency, but mitigates the losses for the other parameters in the model.},
  archive      = {J_SIM},
  author       = {Sara Sauer and Bethany Hedt-Gauthier and Sebastien Haneuse},
  doi          = {10.1002/sim.9016},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4090-4107},
  shortjournal = {Stat. Med.},
  title        = {Optimal allocation in stratified cluster-based outcome-dependent sampling designs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Category encoding method to select feature genes for the
classification of bulk and single-cell RNA-seq data. <em>SIM</em>,
<em>40</em>(18), 4077–4089. (<a
href="https://doi.org/10.1002/sim.9015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bulk and single-cell RNA-seq (scRNA-seq) data are being used as alternatives to traditional technology in biology and medicine research. These data are used, for example, for the detection of differentially expressed (DE) genes. Several statistical methods have been developed for the classification of bulk and single-cell RNA-seq data. These feature genes are vitally important for the classification of bulk and single-cell RNA-seq data. The majority of genes are not DE and they are thus irrelevant for class distinction. To improve the classification performance and save the computation time, removal of irrelevant genes is necessary. Removal will aid the detection of the important feature genes. Widely used schemes in the literature, such as the BSS/WSS (BW) method, assume that data are normally distributed and may not be suitable for bulk and single-cell RNA-seq data. In this article, a category encoding (CAEN) method is proposed to select feature genes for bulk and single-cell RNA-seq data classification. This novel method encodes categories by employing the rank of sequence samples for each gene in each class. Correlation coefficients are considered for gene and class with the rank of sample and a new rank of category. The highest gene correlation coefficients are considered feature genes, which are the most effective for classifying bulk and single-cell RNA-seq dataset. The sure screening method was also established for rank consistency properties of the proposed CAEN method. Simulation studies show that the classifier using the proposed CAEN method performs better than, or at least as well as, the existing methods in most settings. Existing real datasets were analyzed, with the results demonstrating superior performance of the proposed method over current competitors. The application has been coded into an R package named “CAEN” to facilitate wide use.},
  archive      = {J_SIM},
  author       = {Yan Zhou and Li Zhang and Jinfeng Xu and Jun Zhang and Xiaodong Yan},
  doi          = {10.1002/sim.9015},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4077-4089},
  shortjournal = {Stat. Med.},
  title        = {Category encoding method to select feature genes for the classification of bulk and single-cell RNA-seq data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Replicability of studies following a dual-criterion design.
<em>SIM</em>, <em>40</em>(18), 4068–4076. (<a
href="https://doi.org/10.1002/sim.9014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Replicability of results is regarded as the corner stone of science. Recent research seems to raise doubts about whether this requirement is generally fulfilled. Often, replicability of results is defined as repeating a statistically significant result. However, since significance may not imply scientific relevance, dual-criterion study designs that take both aspects into account have been proposed and investigated during the last decade. Originally developed for proof-of-concept trials, the design could be appropriate for phase III trials as well. In fact, a dual-criterion design has been requested for COVID-19 vaccine applications by major health authorities. In this article, replicability of dual-criterion designs is investigated. It turns out that the probability to replicate a significant and relevant result can become as low as 0.5. The replication probability increases if the effect estimator exceeds the minimum relevant effect in the original study by an extra amount.},
  archive      = {J_SIM},
  author       = {Gerd K. Rosenkranz},
  doi          = {10.1002/sim.9014},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4068-4076},
  shortjournal = {Stat. Med.},
  title        = {Replicability of studies following a dual-criterion design},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric analysis of zero-inflated recurrent events
with a terminal event. <em>SIM</em>, <em>40</em>(18), 4053–4067. (<a
href="https://doi.org/10.1002/sim.9013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event data frequently arise in longitudinal studies and observations on recurrent events could be terminated by a major failure event such as death. In many situations, there exist a large fraction of subjects without any recurrent events of interest. Among these subjects, some are unsusceptible to recurrent events, while others are susceptible but have no recurrent events being observed due to censoring. In this article, we propose a zero-inflated generalized joint frailty model and a sieve maximum likelihood approach to analyze zero-inflated recurrent events with a terminal event. The model provides a considerable flexibility in formulating the effects of covariates on both recurrent events and the terminal event by specifying various transformation functions. In addition, Bernstein polynomials are employed to approximate the unknown cumulative baseline hazard (intensity) function. The estimation procedure can be easily implemented and is computationally fast. Extensive simulation studies are conducted and demonstrate that our proposed method works well for practical situations. Finally, we apply the method to analyze myocardial infarction recurrences in the presence of death in a clinical trial with cardiovascular outcomes.},
  archive      = {J_SIM},
  author       = {Chenchen Ma and Tao Hu and Zhantao Lin},
  doi          = {10.1002/sim.9013},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4053-4067},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric analysis of zero-inflated recurrent events with a terminal event},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biomarker evaluation under imperfect nested case-control
design. <em>SIM</em>, <em>40</em>(18), 4035–4052. (<a
href="https://doi.org/10.1002/sim.9012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nested case-control (NCC) design has been widely adopted as a cost-effective sampling design for biomarker research. Under the NCC design, markers are only measured for the NCC subcohort consisting of all cases and a fraction of the controls selected randomly from the matched risk sets of the cases. Robust methods for evaluating prediction performance of risk models have been derived under the inverse probability weighting framework. The probabilities of samples being included in the NCC cohort can be calculated based on the study design ``a previous study&#39;&#39; or estimated non-parametrically ``a previous study&#39;&#39;. Neither strategy works well due to model mis-specification and the curse of dimensionality in practical settings where the sampling does not entirely follow the study design or depends on many factors. In this paper, we propose an alternative strategy to estimate the sampling probabilities based on a varying coefficient model, which attains a balance between robustness and the curse of dimensionality. The complex correlation structure induced by repeated finite risk set sampling makes the standard resampling procedure for variance estimation fail. We propose a perturbation resampling procedure that provides valid interval estimation for the proposed estimators. Simulation studies show that the proposed method performs well in finite samples. We apply the proposed method to the Nurses&#39; Health Study II to develop and evaluate prediction models using clinical biomarkers for cardiovascular risk.},
  archive      = {J_SIM},
  author       = {Xuan Wang and Yingye Zheng and Majken Karoline Jensen and Zeling He and Tianxi Cai},
  doi          = {10.1002/sim.9012},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {4035-4052},
  shortjournal = {Stat. Med.},
  title        = {Biomarker evaluation under imperfect nested case-control design},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric mixture cure model analysis with competing
risks data: Application to vascular access thrombosis data.
<em>SIM</em>, <em>40</em>(17), 4034. (<a
href="https://doi.org/10.1002/sim.9036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Chyong-Mei Chen and Pao-sheng Shen and Chih-Ching Lin and Chih-Cheng Wu},
  doi          = {10.1002/sim.9036},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {4034},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric mixture cure model analysis with competing risks data: Application to vascular access thrombosis data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring rater bias in diagnostic tests with ordinal
ratings. <em>SIM</em>, <em>40</em>(17), 4014–4033. (<a
href="https://doi.org/10.1002/sim.9011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic tests are frequently reliant upon the interpretation of images by skilled raters. In many clinical settings, however, the variability observed between experts&#39; ratings plays a detrimental role in the degree of confidence in these interpretations, leading to uncertainty in the diagnostic process. For example, in breast cancer testing, radiologists interpret mammographic images, while breast biopsy results are examined by pathologists. Each of these procedures involves elements of subjectivity. We propose here a flexible two-stage Bayesian latent variable model to investigate how the skills of individual raters impact the diagnostic accuracy of image-related testing in large-scale medical testing studies. A strength of the proposed model is that the true disease status of a patient within a reasonable time frame may or may not be known. In these studies, many raters each contribute classifications on a large sample of patients using a defined ordinal grading scale, leading to a complex correlation structure between ratings. Our modeling approach considers the different sources of variability contributed by experts and patients while accounting for correlations present between ratings and patients, in contrast to currently available methods. We propose a novel measure of a rater&#39;s ability (magnifier) that, in contrast to conventional measures of sensitivity and specificity, is robust to the underlying prevalence of disease in the population, providing an alternative measure of diagnostic accuracy across patient populations. Extensive simulation studies demonstrate lower bias in estimation of parameters and measures of accuracy, and illustrate outperformance of the proposed model when compared with existing models. Receiver operator characteristic curves are derived to assess the diagnostic accuracy of individual experts and their overall performance. Our proposed modeling approach is applied to a large breast imaging study for known disease status and a uterine cancer dataset for unknown disease status.},
  archive      = {J_SIM},
  author       = {Chanmin Kim and Xiaoyan Lin and Kerrie P. Nelson},
  doi          = {10.1002/sim.9011},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {4014-4033},
  shortjournal = {Stat. Med.},
  title        = {Measuring rater bias in diagnostic tests with ordinal ratings},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessment of heterogeneous treatment effect estimation
accuracy via matching. <em>SIM</em>, <em>40</em>(17), 3990–4013. (<a
href="https://doi.org/10.1002/sim.9010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the assessment of the accuracy of heterogeneous treatment effect (HTE) estimation, where the HTE is not directly observable so standard computation of prediction errors is not applicable. To tackle the difficulty, we propose an assessment approach by constructing pseudo-observations of the HTE based on matching. Our contributions are three-fold: first, we introduce a novel matching distance derived from proximity scores in random forests; second, we formulate the matching problem as an average minimum-cost flow problem and provide an efficient algorithm; third, we propose a match-then-split principle for the assessment with cross-validation. We demonstrate the efficacy of the assessment approach using simulations and a real dataset.},
  archive      = {J_SIM},
  author       = {Zijun Gao and Trevor Hastie and Robert Tibshirani},
  doi          = {10.1002/sim.9010},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3990-4013},
  shortjournal = {Stat. Med.},
  title        = {Assessment of heterogeneous treatment effect estimation accuracy via matching},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal mediation analysis with sure outcomes of random
events model. <em>SIM</em>, <em>40</em>(17), 3975–3989. (<a
href="https://doi.org/10.1002/sim.9009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is a useful tool in randomized trials for understanding how a treatment works, in particular how much of the treatment&#39;s effect on an outcome is explained by a mediator variable. The traditional approach to mediation analysis makes sequential ignorability assumption which precludes the existence of unobserved confounders between the mediator and outcome variables. Since the randomized experiment does not randomize the mediator, sequential ignorability may not be plausible. In this article, based on a statistical model termed sure outcomes of random events model, we propose an alternative approach to causal mediation analysis without relying on the sequential ignorability assumption for the case of binary treatment and mediator variables. When the outcome is also binary, we establish the identifiability of the average natural direct and indirect effects in the presence of an unobserved confounder between mediator and outcome variables. More importantly, if the identifiability conditions are violated, we provide new bounds that are narrower than those in the previous studies, and these bound results are extended to the case of an arbitrary bounded outcome. Simulation studies show good performance for the proposed estimators in finite samples. Finally, we use a job training intervention on the mental health study to illustrate our approach.},
  archive      = {J_SIM},
  author       = {Wei Li and Zhi Geng and Xiao-Hua Zhou},
  doi          = {10.1002/sim.9009},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3975-3989},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis with sure outcomes of random events model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survival mediation analysis with the death-truncated
mediator: The completeness of the survival mediation parameter.
<em>SIM</em>, <em>40</em>(17), 3953–3974. (<a
href="https://doi.org/10.1002/sim.9008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, the development of mediation analysis with a survival outcome has facilitated investigation into causal mechanisms. However, studies have not discussed the death-truncation problem for mediators, the problem being that conventional mediation parameters cannot be well defined in the presence of a truncated mediator. In the present study, we systematically defined the completeness of causal effects to uncover the gap, in conventional causal definitions, between the survival and nonsurvival settings. We propose a novel approach to redefining natural direct and indirect effects, which are generalized forms of conventional causal effects for survival outcomes. Furthermore, we developed three statistical methods for the binary outcome of survival status and formulated a Cox model for survival time. We performed simulations to demonstrate that the proposed methods are unbiased and robust. We also applied the proposed method to explore the effect of hepatitis C virus infection on mortality, as mediated through hepatitis B viral load.},
  archive      = {J_SIM},
  author       = {An-Shun Tai and Chun-An Tsai and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.9008},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3953-3974},
  shortjournal = {Stat. Med.},
  title        = {Survival mediation analysis with the death-truncated mediator: The completeness of the survival mediation parameter},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilevel modeling of spatially nested functional data:
Spatiotemporal patterns of hospitalization rates in the US dialysis
population. <em>SIM</em>, <em>40</em>(17), 3937–3952. (<a
href="https://doi.org/10.1002/sim.9007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-stage renal disease patients on dialysis experience frequent hospitalizations. In addition to known temporal patterns of hospitalizations over the life span on dialysis, where poor outcomes are typically exacerbated during the first year on dialysis, variations in hospitalizations among dialysis facilities across the US contribute to spatial variation. Utilizing national data from the United States Renal Data System (USRDS), we propose a novel multilevel spatiotemporal functional model to study spatiotemporal patterns of hospitalization rates among dialysis facilities. Hospitalization rates of dialysis facilities are considered as spatially nested functional data (FD) with longitudinal hospitalizations nested in dialysis facilities and dialysis facilities nested in geographic regions. A multilevel Karhunen-Loéve expansion is utilized to model the two-level (facility and region) FD, where spatial correlations are induced among region-specific principal component scores accounting for regional variation. A new efficient algorithm based on functional principal component analysis and Markov Chain Monte Carlo is proposed for estimation and inference. We report a novel application using USRDS data to characterize spatiotemporal patterns of hospitalization rates for over 400 health service areas across the US and over the posttransition time on dialysis. Finite sample performance of the proposed method is studied through simulations.},
  archive      = {J_SIM},
  author       = {Yihao Li and Danh V. Nguyen and Sudipto Banerjee and Connie M. Rhee and Kamyar Kalantar-Zadeh and Esra Kürüm and Damla Şentürk},
  doi          = {10.1002/sim.9007},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3937-3952},
  shortjournal = {Stat. Med.},
  title        = {Multilevel modeling of spatially nested functional data: Spatiotemporal patterns of hospitalization rates in the US dialysis population},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Biomarker-guided heterogeneity analysis of genetic
regulations via multivariate sparse fusion. <em>SIM</em>,
<em>40</em>(17), 3915–3936. (<a
href="https://doi.org/10.1002/sim.9006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity is a hallmark of many complex diseases. There are multiple ways of defining heterogeneity, among which the heterogeneity in genetic regulations, for example, gene expressions (GEs) by copy number variations (CNVs), and methylation, has been suggested but little investigated. Heterogeneity in genetic regulations can be linked with disease severity, progression, and other traits and is biologically important. However, the analysis can be very challenging with the high dimensionality of both sides of regulation as well as sparse and weak signals. In this article, we consider the scenario where subjects form unknown subgroups, and each subgroup has unique genetic regulation relationships. Further, such heterogeneity is “guided” by a known biomarker. We develop a multivariate sparse fusion (MSF) approach, which innovatively applies the penalized fusion technique to simultaneously determine the number and structure of subgroups and regulation relationships within each subgroup. An effective computational algorithm is developed, and extensive simulations are conducted. The analysis of heterogeneity in the GE-CNV regulations in melanoma and GE-methylation regulations in stomach cancer using the TCGA data leads to interesting findings.},
  archive      = {J_SIM},
  author       = {Sanguo Zhang and Xiaonan Hu and Ziye Luo and Yu Jiang and Yifan Sun and Shuangge Ma},
  doi          = {10.1002/sim.9006},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3915-3936},
  shortjournal = {Stat. Med.},
  title        = {Biomarker-guided heterogeneity analysis of genetic regulations via multivariate sparse fusion},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new regression model for overdispersed binomial data
accounting for outliers and an excess of zeros. <em>SIM</em>,
<em>40</em>(17), 3895–3914. (<a
href="https://doi.org/10.1002/sim.9005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary outcomes are extremely common in biomedical research. Despite its popularity, binomial regression often fails to model this kind of data accurately due to the overdispersion problem. Many alternatives can be found in the literature, the beta-binomial (BB) regression model being one of the most popular. The additional parameter of this model enables a better fit to overdispersed data. It also exhibits an attractive interpretation in terms of the intraclass correlation coefficient. Nonetheless, in many real data applications, a single additional parameter cannot handle the entire excess of variability. In this study, we propose a new finite mixture distribution with BB components, namely, the flexible beta-binomial (FBB), which is characterized by a richer parameterization. This allows us to enhance the variance structure to account for multiple causes of overdispersion while also preserving the intraclass correlation interpretation. The novel regression model, based on the FBB distribution, exploits the flexibility and large variety of the distribution&#39;s possible shapes (which includes bimodality and various tail behaviors). Thus, it succeeds in accounting for several (possibly concomitant) sources of overdispersion stemming from the presence of latent groups in the population, outliers, and excessive zero observations. Adopting a Bayesian approach to inference, we perform an intensive simulation study that shows the superiority of the new regression model over that of the existing ones. Its better performance is also confirmed by three applications to real datasets extensively studied in the biomedical literature, namely, bacteria data, atomic bomb radiation data, and control mice data.},
  archive      = {J_SIM},
  author       = {Roberto Ascari and Sonia Migliorati},
  doi          = {10.1002/sim.9005},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3895-3914},
  shortjournal = {Stat. Med.},
  title        = {A new regression model for overdispersed binomial data accounting for outliers and an excess of zeros},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder to discussion on is group testing ready for
prime-time in disease identification? <em>SIM</em>, <em>40</em>(17),
3892–3894. (<a href="https://doi.org/10.1002/sim.9033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Gregory Haber and Yaakov Malinovsky and Paul S. Albert},
  doi          = {10.1002/sim.9033},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3892-3894},
  shortjournal = {Stat. Med.},
  title        = {Rejoinder to discussion on is group testing ready for prime-time in disease identification?},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on “is group testing ready for prime time in disease
identification?” <em>SIM</em>, <em>40</em>(17), 3889–3891. (<a
href="https://doi.org/10.1002/sim.9078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Wesley O. Johnson},
  doi          = {10.1002/sim.9078},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3889-3891},
  shortjournal = {Stat. Med.},
  title        = {Comment on “Is group testing ready for prime time in disease identification?”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of “is group testing ready for prime-time in
disease identification?” By haber, malinovsky, and albert, statistics in
medicine, 2021. <em>SIM</em>, <em>40</em>(17), 3887–3888. (<a
href="https://doi.org/10.1002/sim.8989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Brad J. Biggerstaff},
  doi          = {10.1002/sim.8989},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3887-3888},
  shortjournal = {Stat. Med.},
  title        = {Discussion of “Is group testing ready for prime-time in disease identification?” by haber, malinovsky, and albert, statistics in medicine, 2021},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Discussion on “is group testing ready for prime-time in
disease identification.” <em>SIM</em>, <em>40</em>(17), 3881–3886. (<a
href="https://doi.org/10.1002/sim.8988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Christopher R. Bilder and Joshua M. Tebbs and Christopher S. McMahan},
  doi          = {10.1002/sim.8988},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3881-3886},
  shortjournal = {Stat. Med.},
  title        = {Discussion on “Is group testing ready for prime-time in disease identification”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Is group testing ready for prime-time in disease
identification? <em>SIM</em>, <em>40</em>(17), 3865–3880. (<a
href="https://doi.org/10.1002/sim.9003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale disease screening is a complicated process in which high costs must be balanced against pressing public health needs. When the goal is screening for infectious disease, one approach is group testing in which samples are initially tested in pools and individual samples are retested only if the initial pooled test was positive. Intuitively, if the prevalence of infection is small, this could result in a large reduction of the total number of tests required. Despite this, the use of group testing in medical studies has been limited, largely due to skepticism about the impact of pooling on the accuracy of a given assay. While there is a large body of research addressing the issue of testing errors in group testing studies, it is customary to assume that the misclassification parameters are known from an external population and/or that the values do not change with the group size. Both of these assumptions are highly questionable for many medical practitioners considering group testing in their study design. In this article, we explore how the failure of these assumptions might impact the efficacy of a group testing design and, consequently, whether group testing is currently feasible for medical screening. Specifically, we look at how incorrect assumptions about the sensitivity function at the design stage can lead to poor estimation of a procedure&#39;s overall sensitivity and expected number of tests. Furthermore, if a validation study is used to estimate the pooled misclassification parameters of a given assay, we show that the sample sizes required are so large as to be prohibitive in all but the largest screening programs.},
  archive      = {J_SIM},
  author       = {Gregory Haber and Yaakov Malinovsky and Paul S. Albert},
  doi          = {10.1002/sim.9003},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3865-3880},
  shortjournal = {Stat. Med.},
  title        = {Is group testing ready for prime-time in disease identification?},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nowcasting COVID-19 incidence indicators during the italian
first outbreak. <em>SIM</em>, <em>40</em>(16), 3843–3864. (<a
href="https://doi.org/10.1002/sim.9004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel parametric regression model is proposed to fit incidence data typically collected during epidemics. The proposal is motivated by real-time monitoring and short-term forecasting of the main epidemiological indicators within the first outbreak of COVID-19 in Italy. Accurate short-term predictions, including the potential effect of exogenous or external variables are provided. This ensures to accurately predict important characteristics of the epidemic (e.g., peak time and height), allowing for a better allocation of health resources over time. Parameter estimation is carried out in a maximum likelihood framework. All computational details required to reproduce the approach and replicate the results are provided.},
  archive      = {J_SIM},
  author       = {Pierfrancesco Alaimo Di Loro and Fabio Divino and Alessio Farcomeni and Giovanna Jona Lasinio and Gianfranco Lovison and Antonello Maruotti and Marco Mingione},
  doi          = {10.1002/sim.9004},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3843-3864},
  shortjournal = {Stat. Med.},
  title        = {Nowcasting COVID-19 incidence indicators during the italian first outbreak},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Semiparametric recurrent event vs time-to-first-event
analyses in randomized trials: Estimands and model misspecification.
<em>SIM</em>, <em>40</em>(16), 3823–3842. (<a
href="https://doi.org/10.1002/sim.9002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insights regarding the merits of recurrent event and time-to-first-event analyses are needed to provide guidance on strategies for analyzing intervention effects in randomized trials involving recurrent event responses. Using established asymptotic results we introduce a framework for studying the large sample properties of estimators arising from semiparametric proportional rate function models and Cox regression under model misspecification. The asymptotic biases and power implications are investigated for different data generating models, and we study the impact of dependent censoring on these findings. Illustrative applications are given involving data from a cystic fibrosis trial and a carcinogenicity experiment, following which we summarize findings and discuss implications for clinical trial design.},
  archive      = {J_SIM},
  author       = {Yujie Zhong and Richard J. Cook},
  doi          = {10.1002/sim.9002},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3823-3842},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric recurrent event vs time-to-first-event analyses in randomized trials: Estimands and model misspecification},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Score tests for scale effects, with application to genomic
analysis. <em>SIM</em>, <em>40</em>(16), 3808–3822. (<a
href="https://doi.org/10.1002/sim.9000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tests for variance or scale effects due to covariates are used in many areas and recently, in genomic and genetic association studies. We study score tests based on location-scale models with arbitrary error distributions that allow incorporation of additional adjustment covariates. Tests based on Gaussian and Laplacian double generalized linear models are examined in some detail. Numerical properties of the tests under Gaussian and other error distributions are examined. Our results show that the use of model-based asymptotic distributions with score tests for scale effects does not control type 1 error well in many settings of practical relevance. We consider simple statistics based on permutation distribution approximations, which correspond to well-known statistics derived by another approach. They are shown to give good type 1 error control under different error distributions and under covariate distribution imbalance. The methods are illustrated through a differential gene expression analysis involving breast cancer tumor samples.},
  archive      = {J_SIM},
  author       = {David Soave and Jerald F. Lawless and Philip Awadalla},
  doi          = {10.1002/sim.9000},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3808-3822},
  shortjournal = {Stat. Med.},
  title        = {Score tests for scale effects, with application to genomic analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multistate survival model of the natural history of cancer
using data from screened and unscreened population. <em>SIM</em>,
<em>40</em>(16), 3791–3807. (<a
href="https://doi.org/10.1002/sim.8998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main aims of models using cancer screening data is to determine the time between the onset of preclinical screen-detectable cancer and the onset of the clinical state of the cancer. This time is called the sojourn time. One problem in using screening data is that an individual can be observed in preclinical phase or clinically diagnosed but not both. Multistate survival models provide a method of modeling the natural history of cancer. The natural history model allows for the calculation of the sojourn time. We developed a continuous-time Markov model and the corresponding likelihood function. The model allows for the use of interval-censored, left-truncated and right-censored data. The model uses data of clinically diagnosed cancers from both screened and nonscreened individuals. Parameters of age-varying hazards and age-varying misclassification are estimated simultaneously. The mean sojourn time is calculated from a micro-simulation using model parameters. The model is applied to data from a prostate screening trial. The simulation study showed that the model parameters could be estimated accurately.},
  archive      = {J_SIM},
  author       = {Rikesh Bhatt and Ardo van den Hout and Nora Pashayan},
  doi          = {10.1002/sim.8998},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3791-3807},
  shortjournal = {Stat. Med.},
  title        = {A multistate survival model of the natural history of cancer using data from screened and unscreened population},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using generalized linear models to implement g-estimation
for survival data with time-varying confounding. <em>SIM</em>,
<em>40</em>(16), 3779–3790. (<a
href="https://doi.org/10.1002/sim.8997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using data from observational studies to estimate the causal effect of a time-varying exposure, repeatedly measured over time, on an outcome of interest requires careful adjustment for confounding. Standard regression adjustment for observed time-varying confounders is unsuitable, as it can eliminate part of the causal effect and induce bias. Inverse probability weighting, g-computation, and g-estimation have been proposed as being more suitable methods. G-estimation has some advantages over the other two methods, but until recently there has been a lack of flexible g-estimation methods for a survival time outcome. The recently proposed Structural Nested Cumulative Survival Time Model (SNCSTM) is such a method. Efficient estimation of the parameters of this model required bespoke software. In this article we show how the SNCSTM can be fitted efficiently via g-estimation using standard software for fitting generalised linear models. The ability to implement g-estimation for a survival outcome using standard statistical software greatly increases the potential uptake of this method. We illustrate the use of this method of fitting the SNCSTM by reanalyzing data from the UK Cystic Fibrosis Registry, and provide example R code to facilitate the use of this approach by other researchers.},
  archive      = {J_SIM},
  author       = {Shaun R. Seaman and Ruth H. Keogh and Oliver Dukes and Stijn Vansteelandt},
  doi          = {10.1002/sim.8997},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3779-3790},
  shortjournal = {Stat. Med.},
  title        = {Using generalized linear models to implement g-estimation for survival data with time-varying confounding},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian semiparametric mixed effects models for
meta-analysis of the literature data: An application to cadmium toxicity
studies. <em>SIM</em>, <em>40</em>(16), 3762–3778. (<a
href="https://doi.org/10.1002/sim.8996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Bayesian semiparametric mixed effects models with measurement error to analyze the literature data collected from multiple studies in a meta-analytic framework. We explore this methodology for risk assessment in cadmium toxicity studies, where the primary objective is to investigate dose-response relationships between urinary cadmium concentrations and β 2 -microglobulin. In the proposed model, a nonlinear association between exposure and response is described by a Gaussian process with shape restrictions, and study-specific random effects are modeled to have either normal or unknown distributions with Dirichlet process mixture priors. In addition, nonparametric Bayesian measurement error models are incorporated to flexibly account for the uncertainty resulting from the usage of a surrogate measurement of a true exposure. We apply the proposed model to analyze cadmium toxicity data imposing shape constraints along with measurement errors and study-specific random effects across varying characteristics, such as population gender, age, or ethnicity.},
  archive      = {J_SIM},
  author       = {Seongil Jo and Beomjo Park and Yeonseung Chung and Jeongseon Kim and Eunji Lee and Jangwon Lee and Taeryon Choi},
  doi          = {10.1002/sim.8996},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3762-3778},
  shortjournal = {Stat. Med.},
  title        = {Bayesian semiparametric mixed effects models for meta-analysis of the literature data: An application to cadmium toxicity studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring association among censored antibody titer data.
<em>SIM</em>, <em>40</em>(16), 3740–3761. (<a
href="https://doi.org/10.1002/sim.8995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censoring due to a limit of detection or limit of quantification happens quite often in many medical studies. Conventional approaches to deal with censoring when analyzing these data include, for example, the substitution method and the complete case (CC) analysis. More recently, maximum likelihood estimation (MLE) has been increasingly used. While the CC analysis and the substitution method usually lead to biased estimates, the MLE approach appears to perform well in many situations. This article proposes an MLE approach to estimate the association between two measurements in the presence of censoring in one or both quantities. The central idea is to use a copula function to join the marginal distributions of the two measurements. In various simulation studies, we show that our approach outperforms existing conventional methods (CC and substitution analyses). In addition, rank-based measures of global association such as Kendall&#39;s tau or Spearman&#39;s rho can be studied, hence, attention is not only confined to Pearson&#39;s product-moment correlation coefficient capturing solely linear association. We have shown in our simulations that our approach is robust to misspecification of the copula function or marginal distributions given a small association. Furthermore, we propose a straightforward MLE method to fit a (multiple) linear regression model in the presence of censoring in a covariate or both the covariate and the response. Given the marginal distribution of the censored covariate, our method outperforms conventional approaches. We also compare and discuss the performance of our method with multiple imputation and missing indicator model approaches.},
  archive      = {J_SIM},
  author       = {Thao M. P. Tran and Steven Abrams and Marc Aerts and Kirsten Maertens and Niel Hens},
  doi          = {10.1002/sim.8995},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3740-3761},
  shortjournal = {Stat. Med.},
  title        = {Measuring association among censored antibody titer data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression analysis of arbitrarily censored survival data
under the proportional odds model. <em>SIM</em>, <em>40</em>(16),
3724–3739. (<a href="https://doi.org/10.1002/sim.8994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrarily censored data are referred to as the survival data that contain a mixture of exactly observed, left-censored, interval-censored, and right-censored observations. Existing research work on regression analysis on arbitrarily censored data is relatively sparse and mainly focused on the proportional hazards model and the accelerated failure time model. This article studies the proportional odds (PO) model and proposes a novel estimation approach through an expectation-maximization (EM) algorithm for analyzing such data. The proposed EM algorithm has many appealing properties such as being robust to initial values, easy to implement, converging fast, and providing the variance estimate of the regression parameter estimate in closed form. An informal diagnosis plot is developed for checking the PO model assumption. Our method has shown excellent performance in estimating the regression parameters as well as the baseline survival function in a simulation study. A real-life dataset about metastatic colorectal cancer is analyzed for illustration. An R package regPO has been created for practitioners to implement our method.},
  archive      = {J_SIM},
  author       = {Lu Wang and Lianming Wang},
  doi          = {10.1002/sim.8994},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3724-3739},
  shortjournal = {Stat. Med.},
  title        = {Regression analysis of arbitrarily censored survival data under the proportional odds model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence interval estimation for sensitivity and
difference between two sensitivities at a given specificity under tree
ordering. <em>SIM</em>, <em>40</em>(16), 3695–3723. (<a
href="https://doi.org/10.1002/sim.8993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers a setting in diagnostic studies (or biomarker study) which involves a healthy class and a diseased class and the latter consists of several subclasses. The problem of interest is to evaluate the accuracy of a biomarker (or a diagnostic test) measured on a continuous scale correctly identifying healthy subjects from diseased subjects without requiring specification of an ordering in terms of marker values for subclasses relative to each other within the diseased class. Such setting is quite common in practice and it falls in the framework of tree ordering or umbrella ordering. This article explores several parametric and nonparametric approaches for estimating confidence intervals of sensitivity of single biomarker and difference between sensitivities of two correlated biomarkers under tree ordering at a given specificity. The performances of all the methods are evaluated and compared by a comprehensive simulation study. A published microarray data set is analyzed using the proposed methods.},
  archive      = {J_SIM},
  author       = {Yi Gao and Lili Tian},
  doi          = {10.1002/sim.8993},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3695-3723},
  shortjournal = {Stat. Med.},
  title        = {Confidence interval estimation for sensitivity and difference between two sensitivities at a given specificity under tree ordering},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-efficient clinical studies with continuous time
survival outcomes. <em>SIM</em>, <em>40</em>(16), 3682–3694. (<a
href="https://doi.org/10.1002/sim.8992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event outcomes are common in clinical studies. For example, the time to a first major adverse cardiovascular event (MACE, defined as CVD death, nonfatal myocardial infarction, or stroke) is a commonly used outcome in cardiovascular outcome trials. Owing to the lengthy time frame and other factors, the high costs of conducting such studies has been identified as one of the major obstacles in conducting clinical trials in the United States. However, typical approaches for designing clinical trials with time-to-event outcomes do not consider study costs. For a given effect size (eg, hazard ratio), the power to detect differences between two groups is typically a function of the total number of events observed in the study. Therefore, the same level of power will be achieved based on various combinations of the total number of participants, length of enrollment and total follow-up times, and group allocation probability. Herein, we provide a general framework for designing cost-efficient studies comparing treatments with respect to continuous time-to-event outcomes. Among the various designs that achieve the desired level of power to detect a given effect size for a fixed type-I error level, the optimal cost-efficient design is the design that minimizes the expected total study cost. The method is general and can be used for Cox proportional hazards models or Aalen additive models, and under various recruitment and censoring assumptions. The proposed approach for designing cost-efficient studies is illustrated for a Weibull time-to-event outcome with uniform recruitment and exponentially distributed censoring time. The case of an additive hazards model is also described. A Shiny web application implementation of the proposed methods is presented.},
  archive      = {J_SIM},
  author       = {Grecio J. Sandoval and Ionut Bebu and John M. Lachin},
  doi          = {10.1002/sim.8992},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3682-3694},
  shortjournal = {Stat. Med.},
  title        = {Cost-efficient clinical studies with continuous time survival outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing the polytomous discrimination index. <em>SIM</em>,
<em>40</em>(16), 3667–3681. (<a
href="https://doi.org/10.1002/sim.8991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polytomous regression models generalize logistic models for the case of a categorical outcome variable with more than two distinct categories. These models are currently used in clinical research, and it is essential to measure their abilities to distinguish between the categories of the outcome. In 2012, van Calster et al proposed the polytomous discrimination index (PDI) as an extension of the binary discrimination c-statistic to unordered polytomous regression. The PDI is a summary of the simultaneous discrimination between all outcome categories. Previous implementations of the PDI are not capable of running on “big data.” This article shows that the PDI formula can be manipulated to depend only on the distributions of the predicted probabilities evaluated for each outcome category and within each observed level of the outcome, which substantially improves the computation time. We present a SAS macro and R function that can rapidly evaluate the PDI and its components. The routines are evaluated on several simulated datasets after varying the number of categories of the outcome and size of the data and two real-world large administrative health datasets. We compare PDI with two other discrimination indices: M-index and hypervolume under the manifold (HUM) on simulated examples. We describe situations where the PDI and HUM, indices based on multiple comparisons, are superior to the M-index, an index based on pairwise comparisons, to detect predictions that are no different than random selection or erroneous due to incorrect ranking.},
  archive      = {J_SIM},
  author       = {Douglas C. Dover and Sunjidatul Islam and Cynthia M. Westerhout and Linn E. Moore and Padma Kaul and Anamaria Savu},
  doi          = {10.1002/sim.8991},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3667-3681},
  shortjournal = {Stat. Med.},
  title        = {Computing the polytomous discrimination index},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection of within-run quality control rules for laboratory
biomarkers. <em>SIM</em>, <em>40</em>(16), 3645–3666. (<a
href="https://doi.org/10.1002/sim.8987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to release correct biomarker results of a laboratory test, it is a regulatory requirement to apply quality control standards for controlling analytical errors. Releasing an incorrect test result might lead to wrong diagnosis or treatment of a patient in medical decision-making. In laboratory medicine, one of the means to control analytical errors is statistical process control procedures proposed by James O. Westgard and his coworkers nowadays known as “Westgard rules.” To judge their performance for discriminating in-control from out-of-control processes, power curves are used. In this article, we describe functions for the power curves of the within-run Westgard rules. Based on these power curves, we use a benchmark approach for selecting a quality control procedure out of the set of Westgard rules. It is shown that two graphical procedures proposed by Westgard and his coworkers can be reduced to this benchmark approach. Besides, a commonly used measure in laboratory medicine for describing out-of-control processes is critically examined revealing the threat of selecting too optimistic quality control rules.},
  archive      = {J_SIM},
  author       = {Ruediger Paul Laubender and Andrea Geistanger},
  doi          = {10.1002/sim.8987},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {3645-3666},
  shortjournal = {Stat. Med.},
  title        = {Selection of within-run quality control rules for laboratory biomarkers},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating partial adherence into the principal
stratification analysis framework. <em>SIM</em>, <em>40</em>(15),
3625–3644. (<a href="https://doi.org/10.1002/sim.8986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Participants in pragmatic clinical trials often partially adhere to treatment. However, to simplify the analysis, most studies dichotomize adherence (supposing that subjects received either full or no treatment), which can introduce biases in the results. For example, the popular approach of principal stratification is based on the concept that the population can be separated into strata based on how they will react to treatment assignment, but this framework does not include strata in which a partially adhering participant would belong. We expanded the principal stratification framework to allow partial adherers to have their own principal stratum and treatment level. The expanded approach is feasible in pragmatic settings. We have designed a Monte Carlo posterior sampling method to obtain the relevant parameter estimates. Simulations were completed under a range of settings where participants partially adhered to treatment, including a hypothetical setting from a published simulation trial on the topic of partial adherence. The inference method is additionally applied to data from a real randomized clinical trial that features partial adherence. Comparison of the simulation results indicated that our method is superior in most cases to the biased estimators obtained through standard principal stratification. Simulation results further suggest that our proposed method may lead to increased accuracy of inference in settings where study participants only partially adhere to assigned treatment.},
  archive      = {J_SIM},
  author       = {Eric Sanders and Paul Gustafson and Mohammad Ehsanul Karim},
  doi          = {10.1002/sim.8986},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3625-3644},
  shortjournal = {Stat. Med.},
  title        = {Incorporating partial adherence into the principal stratification analysis framework},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational bayesian partially linear mean shift models for
high-dimensional alzheimer’s disease neuroimaging data. <em>SIM</em>,
<em>40</em>(15), 3604–3624. (<a
href="https://doi.org/10.1002/sim.8985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease can be diagnosed by analyzing brain images (eg, magnetic resonance imaging, MRI) and neuropsychological tests (eg, mini-mental state examination, MMSE). A partially linear mean shift model (PLMSM) is here proposed to investigate the relationship between MMSE score and high-dimensional regions of interest in MRI, and detect the outliers. In the presence of high-dimensional data, existing Bayesian approaches (eg, Markov chain Monte Carlo) to analyze a PLMSM take intensive computational cost and require huge memory, and have low convergence rate. To address these issues, a variational Bayesian inference is developed to simultaneously estimate parameters and nonparametric functions and identify outliers in a PLMSM. A Bayesian P-splines method is presented to approximate nonparametric functions, a Bayesian adaptive Lasso approach is employed to select predictors, and outliers are detected by the classification variable. Two simulation studies are conducted to assess the finite sample performance of the proposed method. An MRI dataset with elderly cognitive ability is provided to corroborate the proposed method.},
  archive      = {J_SIM},
  author       = {Ying Wu and Niansheng Tang},
  doi          = {10.1002/sim.8985},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3604-3624},
  shortjournal = {Stat. Med.},
  title        = {Variational bayesian partially linear mean shift models for high-dimensional alzheimer&#39;s disease neuroimaging data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian network meta-regression hierarchical models using
heavy-tailed multivariate random effects with covariate-dependent
variances. <em>SIM</em>, <em>40</em>(15), 3582–3603. (<a
href="https://doi.org/10.1002/sim.8983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) is gaining popularity in evidence synthesis and network meta-regression allows us to incorporate potentially important covariates into network meta-analysis. In this article, we propose a Bayesian network meta-regression hierarchical model and assume a general multivariate t distribution for the random treatment effects. The multivariate t distribution is desired for heavy-tailed random effects and converges to the multivariate normal distribution when the degrees of freedom go to infinity. Moreover, in NMA, some treatments are compared only in a single study. To overcome such sparsity, we propose a log-linear regression model for the variances of the random effects and incorporate aggregate covariates into modeling the variance components. We develop a Markov chain Monte Carlo sampling algorithm to sample from the posterior distribution via the collapsed Gibbs technique. We further use the deviance information criterion and the logarithm of the pseudo-marginal likelihood for model comparison. A simulation study is conducted and a detailed analysis from our motivating case study is carried out to further demonstrate the proposed methodology.},
  archive      = {J_SIM},
  author       = {Hao Li and Daeyoung Lim and Ming-Hui Chen and Joseph G. Ibrahim and Sungduk Kim and Arvind K. Shah and Jianxin Lin},
  doi          = {10.1002/sim.8983},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3582-3603},
  shortjournal = {Stat. Med.},
  title        = {Bayesian network meta-regression hierarchical models using heavy-tailed multivariate random effects with covariate-dependent variances},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New partition based measures for data compatibility and
information gain. <em>SIM</em>, <em>40</em>(15), 3560–3581. (<a
href="https://doi.org/10.1002/sim.8982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of great practical importance to compare and combine data from different studies in order to carry out appropriate and more powerful statistical inference. We propose a partition based measure to quantify the compatibility of two datasets using their respective posterior distributions. We further propose an information gain measure to quantify the information increase (or decrease) in combining two datasets. These measures are well calibrated and efficient computational algorithms are provided for their calculations. We use examples in a benchmark dose toxicology study, a six cities pollution data and a melanoma clinical trial to illustrate how these two measures are useful in combining current data with historical data and missing data.},
  archive      = {J_SIM},
  author       = {Daoyuan Shi and Ming-Hui Chen and Lynn Kuo and Paul O. Lewis},
  doi          = {10.1002/sim.8982},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3560-3581},
  shortjournal = {Stat. Med.},
  title        = {New partition based measures for data compatibility and information gain},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing more generalizable prediction models from pooled
studies and large clustered data sets. <em>SIM</em>, <em>40</em>(15),
3533–3559. (<a href="https://doi.org/10.1002/sim.8981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction models often yield inaccurate predictions for new individuals. Large data sets from pooled studies or electronic healthcare records may alleviate this with an increased sample size and variability in sample characteristics. However, existing strategies for prediction model development generally do not account for heterogeneity in predictor-outcome associations between different settings and populations. This limits the generalizability of developed models (even from large, combined, clustered data sets) and necessitates local revisions. We aim to develop methodology for producing prediction models that require less tailoring to different settings and populations. We adopt internal-external cross-validation to assess and reduce heterogeneity in models&#39; predictive performance during the development. We propose a predictor selection algorithm that optimizes the (weighted) average performance while minimizing its variability across the hold-out clusters (or studies). Predictors are added iteratively until the estimated generalizability is optimized. We illustrate this by developing a model for predicting the risk of atrial fibrillation and updating an existing one for diagnosing deep vein thrombosis, using individual participant data from 20 cohorts (N = 10 873) and 11 diagnostic studies (N = 10 014), respectively. Meta-analysis of calibration and discrimination performance in each hold-out cluster shows that trade-offs between average and heterogeneity of performance occurred. Our methodology enables the assessment of heterogeneity of prediction model performance during model development in multiple or clustered data sets, thereby informing researchers on predictor selection to improve the generalizability to different settings and populations, and reduce the need for model tailoring. Our methodology has been implemented in the R package metamisc .},
  archive      = {J_SIM},
  author       = {Valentijn M. T. de Jong and Karel G. M. Moons and Marinus J. C. Eijkemans and Richard D. Riley and Thomas P. A. Debray},
  doi          = {10.1002/sim.8981},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3533-3559},
  shortjournal = {Stat. Med.},
  title        = {Developing more generalizable prediction models from pooled studies and large clustered data sets},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Promotion time cure rate model with a neural network
estimated nonparametric component. <em>SIM</em>, <em>40</em>(15),
3516–3532. (<a href="https://doi.org/10.1002/sim.8980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Promotion time cure rate models (PCM) are often used to model the survival data with a cure fraction. Medical images or biomarkers derived from medical images can be the key predictors in survival models. However, incorporating images in the PCM is challenging using traditional nonparametric methods such as splines. We propose to use neural network to model the nonparametric or unstructured predictors&#39; effect in the PCM context. Expectation-maximization algorithm with neural network for the M-step is used for parameter estimation. Asymptotic properties of the proposed estimates are derived. Simulation studies show good performance in terms of both prediction and estimation. We finally apply our methods to analyze the brain images from open access series of imaging studies data.},
  archive      = {J_SIM},
  author       = {Yujing Xie and Zhangsheng Yu},
  doi          = {10.1002/sim.8980},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3516-3532},
  shortjournal = {Stat. Med.},
  title        = {Promotion time cure rate model with a neural network estimated nonparametric component},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust covariance estimation for high-dimensional
compositional data with application to microbial communities analysis.
<em>SIM</em>, <em>40</em>(15), 3499–3515. (<a
href="https://doi.org/10.1002/sim.8979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbial communities analysis is drawing growing attention due to the rapid development fire of high-throughput sequencing techniques nowadays. The observed data has the following typical characteristics: it is high-dimensional, compositional (lying in a simplex) and even would be leptokurtic and highly skewed due to the existence of overly abundant taxa, which makes the conventional correlation analysis infeasible to study the co-occurrence and co-exclusion relationship between microbial taxa. In this article, we address the challenges of covariance estimation for this kind of data. Assuming the basis covariance matrix lying in a well-recognized class of sparse covariance matrices, we adopt a proxy matrix known as centered log-ratio covariance matrix in the literature. We construct a Median-of-Means estimator for the centered log-ratio covariance matrix and propose a thresholding procedure that is adaptive to the variability of individual entries. By imposing a much weaker finite fourth moment condition compared with the sub-Gaussianity condition in the literature, we derive the optimal rate of convergence under the spectral norm. In addition, we also provide theoretical guarantee on support recovery. The adaptive thresholding procedure of the MOM estimator is easy to implement and gains robustness when outliers or heavy-tailedness exist. Thorough simulation studies are conducted to show the advantages of the proposed procedure over some state-of-the-arts methods. At last, we apply the proposed method to analyze a microbiome dataset in human gut.},
  archive      = {J_SIM},
  author       = {Yong He and Pengfei Liu and Xinsheng Zhang and Wang Zhou},
  doi          = {10.1002/sim.8979},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3499-3515},
  shortjournal = {Stat. Med.},
  title        = {Robust covariance estimation for high-dimensional compositional data with application to microbial communities analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of predictive model performance of an existing
model in the presence of missing data. <em>SIM</em>, <em>40</em>(15),
3477–3498. (<a href="https://doi.org/10.1002/sim.8978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, the Brier score (BS) and the area under the receiver operating characteristic (ROC) curves (AUC) are two common metrics used to evaluate prediction models of a binary outcome, such as using biomarkers to predict the risk of developing a disease in the future. The assessment of an existing prediction models using data with missing covariate values is challenging. In this article, we propose inverse probability weighted (IPW) and augmented inverse probability weighted (AIPW) estimates of AUC and BS to handle the missing data. An alternative approach uses multiple imputation (MI), which requires a model for the distribution of the missing variable. We evaluated the performance of IPW and AIPW in comparison with MI in simulation studies under missing completely at random, missing at random, and missing not at random scenarios. When there are missing observations in the data, MI and IPW can be used to obtain unbiased estimates of BS and AUC if the imputation model for the missing variable or the model for the missingness is correctly specified. MI is more efficient than IPW. Our simulation results suggest that AIPW can be more efficient than IPW, and also achieves double robustness from miss-specification of either the missingness model or the imputation model. The outcome variable should be included in the model for the missing variable under all scenarios, while it only needs to be included in missingness model if the missingness depends on the outcome. We illustrate these methods using an example from prostate cancer.},
  archive      = {J_SIM},
  author       = {Pin Li and Jeremy M. G. Taylor and Daniel E. Spratt and R. Jeffery Karnes and Matthew J. Schipper},
  doi          = {10.1002/sim.8978},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3477-3498},
  shortjournal = {Stat. Med.},
  title        = {Evaluation of predictive model performance of an existing model in the presence of missing data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hidden mover-stayer model for disease progression accounting
for misclassified and partially observed diagnostic tests: Application
to the natural history of human papillomavirus and cervical precancer.
<em>SIM</em>, <em>40</em>(15), 3460–3476. (<a
href="https://doi.org/10.1002/sim.8977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) have been proposed to model the natural history of diseases while accounting for misclassification in state identification. We introduce a discrete time HMM for human papillomavirus (HPV) and cervical precancer/cancer where the hidden and observed state spaces are defined by all possible combinations of HPV, cytology, and colposcopy results. Because the population of women undergoing cervical cancer screening is heterogeneous with respect to sexual behavior, and therefore risk of HPV acquisition and subsequent precancers, we use a mover-stayer mixture model that assumes a proportion of the population will stay in the healthy state and are not subject to disease progression. As each state is a combination of three distinct tests that characterize the cervix, partially observed data arise when at least one but not every test is observed. The standard forward-backward algorithm, used for evaluating the E-step within the E-M algorithm for maximum-likelihood estimation of HMMs, cannot incorporate time points with partially observed data. We propose a new forward-backward algorithm that considers all possible fully observed states that could have occurred across a participant&#39;s follow-up visits. We apply our method to data from a large management trial for women with low-grade cervical abnormalities. Our simulation study found that our method has relatively little bias and out preforms simpler methods that resulted in larger bias.},
  archive      = {J_SIM},
  author       = {Jordan Aron and Paul S. Albert and Nicolas Wentzensen and Li C. Cheung},
  doi          = {10.1002/sim.8977},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3460-3476},
  shortjournal = {Stat. Med.},
  title        = {Hidden mover-stayer model for disease progression accounting for misclassified and partially observed diagnostic tests: Application to the natural history of human papillomavirus and cervical precancer},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multithreshold change plane model: Estimation theory and
applications in subgroup identification. <em>SIM</em>, <em>40</em>(15),
3440–3459. (<a href="https://doi.org/10.1002/sim.8976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multithreshold change plane regression model which naturally partitions the observed subjects into subgroups with different covariate effects. The underlying grouping variable is a linear function of observed covariates and thus multiple thresholds produce change planes in the covariate space. We contribute a novel two-stage estimation approach to determine the number of subgroups, the location of thresholds, and all other regression parameters. In the first stage we adopt a group selection principle to consistently identify the number of subgroups, while in the second stage change point locations and model parameter estimates are refined by a penalized induced smoothing technique. Our procedure allows sparse solutions for relatively moderate- or high-dimensional covariates. We further establish the asymptotic properties of our proposed estimators under appropriate technical conditions. We evaluate the performance of the proposed methods by simulation studies and provide illustrations using two medical data examples. Our proposal for subgroup identification may lead to an immediate application in personalized medicine.},
  archive      = {J_SIM},
  author       = {Jialiang Li and Yaguang Li and Baisuo Jin and Michael R. Kosorok},
  doi          = {10.1002/sim.8976},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3440-3459},
  shortjournal = {Stat. Med.},
  title        = {Multithreshold change plane model: Estimation theory and applications in subgroup identification},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory clustering using mixed classification models.
<em>SIM</em>, <em>40</em>(15), 3425–3439. (<a
href="https://doi.org/10.1002/sim.8975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory classification has become frequent in clinical research to understand the heterogeneity of individual trajectories. The standard classification model for trajectories assumes no between-individual variance within groups. However, this assumption is often not appropriate, which may overestimate the error variance of the model, leading to a biased classification. Hence, two extensions of the standard classification model were developed through a mixed model. The first one considers an equal between-individual variance across groups, and the second one considers unequal between-individual variance. Simulations were performed to evaluate the impact of these considerations on the classification. The simulation results showed that the first extended model gives a lower misclassification percentage (with differences up to 50%) than the standard one in case of presence of a true variance between individuals inside groups. The second model decreases the misclassification percentage compared with the first one (up to 11%) when the between-individual variance is unequal between groups. However, these two extensions require high number of repeated measurements to be adjusted correctly. Using human chorionic gonadotropin trajectories after curettage for hydatidiform mole, the standard classification model classified trajectories mainly according to their levels whereas the two extended models classified them according to their patterns, which provided more clinically relevant groups. In conclusion, for studies with a nonnegligible number of repeated measurements, the use, in first instance, of a classification model that considers equal between-individual variance across groups rather than a standard classification model, appears more appropriate. A model that considers unequal between-individual variance may find its place thereafter.},
  archive      = {J_SIM},
  author       = {Amna Klich and René Ecochard and Fabien Subtil},
  doi          = {10.1002/sim.8975},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3425-3439},
  shortjournal = {Stat. Med.},
  title        = {Trajectory clustering using mixed classification models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Slamming the sham: A bayesian model for adaptive adjustment
with noisy control data. <em>SIM</em>, <em>40</em>(15), 3403–3424. (<a
href="https://doi.org/10.1002/sim.8973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is not always clear how to adjust for control data in causal inference, balancing the goals of reducing bias and variance. We show how, in a setting with repeated experiments, Bayesian hierarchical modeling yields an adaptive procedure that uses the data to determine how much adjustment to perform. The result is a novel analysis with increased statistical efficiency compared with the default analysis based on difference estimates. We demonstrate this procedure on two real examples, as well as on a series of simulated datasets. We show that the increased efficiency can have real-world consequences in terms of the conclusions that can be drawn from the experiments. We also discuss the relevance of this work to causal inference and statistical design and analysis more generally.},
  archive      = {J_SIM},
  author       = {Andrew Gelman and Matthijs Vákár},
  doi          = {10.1002/sim.8973},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {3403-3424},
  shortjournal = {Stat. Med.},
  title        = {Slamming the sham: A bayesian model for adaptive adjustment with noisy control data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction: Mixed-effects models for slope-based endpoints
in clinical trials of chronic kidney disease. <em>SIM</em>,
<em>40</em>(14), 3400–3401. (<a
href="https://doi.org/10.1002/sim.8974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Edward F. Vonesh and Tom Greene},
  doi          = {10.1002/sim.8974},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3400-3401},
  shortjournal = {Stat. Med.},
  title        = {Correction: Mixed-effects models for slope-based endpoints in clinical trials of chronic kidney disease},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian semiparametric meta-analytic-predictive prior for
historical control borrowing in clinical trials. <em>SIM</em>,
<em>40</em>(14), 3385–3399. (<a
href="https://doi.org/10.1002/sim.8970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing a clinical trial, borrowing historical control information can provide a more efficient approach by reducing the necessary control arm sample size while still yielding increased power. Several Bayesian methods for incorporating historical information via a prior distribution have been proposed, for example, (modified) power prior, (robust) meta-analytic predictive prior. When utilizing historical control borrowing, the prior parameter(s) must be specified to determine the magnitude of borrowing before the current data are observed. Thus, a flexible prior is needed in case of heterogeneity between historic trials or prior data conflict with the current trial. To incorporate the ability to selectively borrow historic information, we propose a Bayesian semiparametric meta-analytic-predictive prior. Using a Dirichlet process mixture prior allows for relaxation of parametric assumptions, and lets the model adaptively learn the relationship between the historic and current control data. Additionally, we generalize a method for estimating the prior effective sample size (ESS) for the proposed prior. This gives an intuitive quantification of the amount of information borrowed from historical trials, and aids in tuning the prior to the specific task at hand. We illustrate the effectiveness of the proposed methodology by comparing performance between existing methods in an extensive simulation study and a phase II proof-of-concept trial in ankylosing spondylitis. In summary, our proposed robustification of the meta-analytic-predictive prior alleviates the need for prespecifying the amount of borrowing, providing a more flexible and robust method to integrate historical data from multiple study sources in the design and analysis of clinical trials.},
  archive      = {J_SIM},
  author       = {Bradley Hupf and Veronica Bunn and Jianchang Lin and Cheng Dong},
  doi          = {10.1002/sim.8970},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3385-3399},
  shortjournal = {Stat. Med.},
  title        = {Bayesian semiparametric meta-analytic-predictive prior for historical control borrowing in clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Win odds: An adaptation of the win ratio to include ties.
<em>SIM</em>, <em>40</em>(14), 3367–3384. (<a
href="https://doi.org/10.1002/sim.8967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The win ratio, a recently proposed measure for comparing the benefit of two treatment groups, allows ties in the data but ignores ties in the inference. In this article, we highlight some difficulties that this can lead to, and we propose to focus on the win odds instead, a modification of the win ratio which takes ties into account. We construct hypothesis tests and confidence intervals for the win odds, and we investigate their properties through simulations and in a case study. We conclude that the win odds should be preferred over the win ratio.},
  archive      = {J_SIM},
  author       = {Edgar Brunner and Marc Vandemeulebroecke and Tobias Mütze},
  doi          = {10.1002/sim.8967},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3367-3384},
  shortjournal = {Stat. Med.},
  title        = {Win odds: An adaptation of the win ratio to include ties},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Degrees of necessity and of sufficiency: Further results and
extensions, with an application to covid-19 mortality in austria.
<em>SIM</em>, <em>40</em>(14), 3352–3366. (<a
href="https://doi.org/10.1002/sim.8961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to extend to ordinal and nominal outcomes the measures of degree of necessity and of sufficiency defined by the authors for dichotomous and survival outcomes in a previous paper. A cause, represented by certain values of prognostic factors, is considered necessary for an event if, without the cause, the event cannot develop. It is considered sufficient for an event if the event is unavoidable in the presence of the cause. The degrees of necessity and sufficiency, ranging from zero to one, are simple, intuitive functions of unconditional and conditional probabilities of an event such as disease or death. These probabilities often will be derived from logistic regression models; the measures, however, do not require any particular model. In addition, we study in detail the relationship between the proposed measures and the related explained variation summary for dichotomous outcomes, which are the common root for the developments for ordinal, nominal, and survival outcomes. We introduce and analyze the Austrian covid-19 data, with the aim of quantifying effects of age and other potentially prognostic factors on covid-19 mortality. This is achieved by standard regression methods but also in terms of the newly proposed measures. It is shown how they complement the toolbox of prognostic factor studies, in particular when comparing the importance of prognostic factors of different types. While the full model&#39;s degree of necessity is extremely high (0.933), its low degree of sufficiency (0.179) is responsible for the low proportion of explained variation (0.193).},
  archive      = {J_SIM},
  author       = {Andreas Gleiss and Robin Henderson and Michael Schemper},
  doi          = {10.1002/sim.8961},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3352-3366},
  shortjournal = {Stat. Med.},
  title        = {Degrees of necessity and of sufficiency: Further results and extensions, with an application to covid-19 mortality in austria},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contamination: How much can an individually randomized trial
tolerate? <em>SIM</em>, <em>40</em>(14), 3329–3351. (<a
href="https://doi.org/10.1002/sim.8958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomization results in an increase in sample size compared to individual randomization, referred to as an efficiency loss. This efficiency loss is typically presented under an assumption of no contamination in the individually randomized trial. An alternative comparator is the sample size needed under individual randomization to detect the attenuated treatment effect due to contamination. A general framework is provided for determining the extent of contamination that can be tolerated in an individually randomized trial before a cluster randomized design yields a larger sample size. Results are presented for a variety of cluster trial designs including parallel arm, stepped-wedge and cluster crossover trials. Results reinforce what is expected: individually randomized trials can tolerate a surprisingly large amount of contamination before they become less efficient than cluster designs. We determine the point at which the contamination means an individual randomized design to detect an attenuated effect requires a larger sample size than cluster randomization under a nonattenuated effect. This critical rate is a simple function of the design effect for clustering and the design effect for multiple periods as well as design effects for stratification or repeated measures under individual randomization. These findings are important for pragmatic comparisons between a novel treatment and usual care as any bias due to contamination will only attenuate the true treatment effect. This is a bias that operates in a predictable direction. Yet, cluster randomized designs with post-randomization recruitment without blinding, are at high risk of bias due to the differential recruitment across treatment arms. This sort of bias operates in an unpredictable direction. Thus, with knowledge that cluster randomized trials are generally at a greater risk of biases that can operate in a nonpredictable direction, results presented here suggest that even in situations where there is a risk of contamination, individual randomization might still be the design of choice.},
  archive      = {J_SIM},
  author       = {Karla Hemming and Monica Taljaard and Mirjam Moerbeek and Andrew Forbes},
  doi          = {10.1002/sim.8958},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3329-3351},
  shortjournal = {Stat. Med.},
  title        = {Contamination: How much can an individually randomized trial tolerate?},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential knockoffs for continuous and categorical
predictors: With application to a large psoriatic arthritis clinical
trial pool. <em>SIM</em>, <em>40</em>(14), 3313–3328. (<a
href="https://doi.org/10.1002/sim.8955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knockoffs provide a general framework for controlling the false discovery rate when performing variable selection. Much of the Knockoffs literature focuses on theoretical challenges and we recognize a need for bringing some of the current ideas into practice. In this paper we propose a sequential algorithm for generating knockoffs when underlying data consists of both continuous and categorical (factor) variables. Further, we present a heuristic multiple knockoffs approach that offers a practical assessment of how robust the knockoff selection process is for a given dataset. We conduct extensive simulations to validate performance of the proposed methodology. Finally, we demonstrate the utility of the methods on a large clinical data pool of more than 2000 patients with psoriatic arthritis evaluated in four clinical trials with an IL-17A inhibitor, secukinumab (Cosentyx), where we determine prognostic factors of a well established clinical outcome. The analyses presented in this paper could provide a wide range of applications to commonly encountered datasets in medical practice and other fields where variable selection is of particular interest.},
  archive      = {J_SIM},
  author       = {Matthias Kormaksson and Luke J. Kelly and Xuan Zhu and Sibylle Haemmerle and Luminita Pricop and David Ohlssen},
  doi          = {10.1002/sim.8955},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3313-3328},
  shortjournal = {Stat. Med.},
  title        = {Sequential knockoffs for continuous and categorical predictors: With application to a large psoriatic arthritis clinical trial pool},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulating the dynamics of atherosclerosis to the incidence
of myocardial infarction, applied to the KORA population. <em>SIM</em>,
<em>40</em>(14), 3299–3312. (<a
href="https://doi.org/10.1002/sim.8951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing epidemiological data with simplified mathematical models of disease development provides a link between the time-course of incidence and the underlying biological processes. Here we point out that considerable modeling flexibility is gained if the model is solved by simulation only. To this aim, a model of atherosclerosis is proposed: a Markov Chain with continuous state space which represents the coronary artery intimal surface area involved with atherosclerotic lesions of increasing severity. Myocardial infarction rates are assumed to be proportional to the area of most severe lesions. The model can be fitted simultaneously to infarction incidence rates observed in the KORA registry, and to the age-dependent prevalence and extent of atherosclerotic lesions in the PDAY study. Moreover, the simulation approach allows for non-linear transition rates, and to consider at the same time randomness and inter-individual heterogeneity. Interestingly, the fit revealed significant age dependence of parameters in females around the age of menopause, qualitatively reproducing the known vascular effects of female sex hormones. For males, the incidence curve flattens for higher ages. According to the model, frailty explains this flattening only partially, and saturation of the disease process plays also an important role. This study shows the feasibility of simulating subclinical and epidemiological data with the same mathematical model. The approach is very general and may be extended to investigate the effects of risk factors or interventions. Moreover, it offers an interface to integrate quantitative individual health data as assessed, for example, by imaging.},
  archive      = {J_SIM},
  author       = {Cristoforo Simonetto and Susanne Rospleszcz and Margit Heier and Christa Meisinger and Annette Peters and Jan Christian Kaiser},
  doi          = {10.1002/sim.8951},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3299-3312},
  shortjournal = {Stat. Med.},
  title        = {Simulating the dynamics of atherosclerosis to the incidence of myocardial infarction, applied to the KORA population},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of statistical methods for allocating disease
costs in the presence of interactions. <em>SIM</em>, <em>40</em>(14),
3286–3298. (<a href="https://doi.org/10.1002/sim.8950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the non-trivial problem of estimating a health cost repartition among diseases from patients&#39; hospital stays&#39; global costs in the presence of multimorbidity, that is, when the patients may suffer from more than one disease. The problem is even harder in the presence of interactions among the disease costs, that is, when the costs of having, for example, two diseases simultaneously do not match the sum of the basic costs of having each disease alone, generating an extra cost which might be either positive or negative. In such a situation, there might be no “true solution” and the choice of the method to be used to solve the problem will depend on how one wishes to allocate the extra costs among the diseases. In this article, we study mathematically how different methods proceed in this regard, namely ordinary least squares (OLS), generalized linear models (GLM), and an iterative proportional repartition (IPR) algorithm, in a simple case with only two diseases. It turned out that only IPR allowed to retrieve the total costs and the unambiguous solution that one would have in a setting without interaction, that is, when no extra cost has to be allocated, while OLS and GLM may produce some negative health costs. Also, contrary to OLS, IPR is taking into account the basic costs of the diseases for the allocation of the extra cost. We conclude that IPR seems to be the most natural method to solve the problem, at least among those considered.},
  archive      = {J_SIM},
  author       = {Jean-Benoît Rossel and Valentin Rousson and Yves Eggli},
  doi          = {10.1002/sim.8950},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3286-3298},
  shortjournal = {Stat. Med.},
  title        = {A comparison of statistical methods for allocating disease costs in the presence of interactions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse group regularization for semi-continuous
transportation data. <em>SIM</em>, <em>40</em>(14), 3267–3285. (<a
href="https://doi.org/10.1002/sim.8942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor vehicle crashes are a global public health concern. Most analysis have used zero-inflated count models for examining crash counts. However, few methods are available to account for safety metrics that have semi-continuous observations. This article considers the problem of variable selection for the semi-continuous zero-inflated (SCZI) models. These models include two parts: a zero-inflated part and a nonzero continuous part. A special group regularization is designed to accommodate the unique structure of two-part SCZI models, and a type of Bayesian information criterion is proposed to select tuning parameters. We illustrate the variable selection process of the proposed model using lane position data from a driving simulator study. In the study, drivers stay in the intended lane for the majority of their drive (zero-inflated part). On occasion, some drivers do drift out of their intended driving lane (nonzero continuous part). Our findings show that individual differences can be captured with the proposed model, which has implications for driving safety and the design of in-vehicle alerting systems.},
  archive      = {J_SIM},
  author       = {Tianshu Feng and Linda Ng Boyle},
  doi          = {10.1002/sim.8942},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3267-3285},
  shortjournal = {Stat. Med.},
  title        = {Sparse group regularization for semi-continuous transportation data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous modeling of alzheimer’s disease progression via
multiple cognitive scales. <em>SIM</em>, <em>40</em>(14), 3251–3266. (<a
href="https://doi.org/10.1002/sim.8932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the progression of Alzheimer&#39;s disease (AD) is challenging due to lacking sensitivity in currently available measures. AD stages are typically defined based on cognitive cut-offs, but this results in heterogeneous patient groups. More accurate modeling of the continuous progression of the disease would enable more accurate patient prognosis. To address these issues, we propose a new multivariate continuous-time disease progression (MCDP) model. The model is formulated as a nonlinear mixed-effects model that aligns patients based on their predicted disease progression along a continuous latent disease timeline. The model is evaluated using long-term follow-up data from 2152 participants in the Alzheimer&#39;s Disease Neuroimaging Initiative. The MCDP model was used to simultaneously model three cognitive scales; the Alzheimer&#39;s Disease Assessment Scale-cognitive subscale, the Mini-Mental State Examination, and the Clinical Dementia Rating scale—sum of boxes. Compared with univariate modeling and previously proposed multivariate disease progression models, the MCDP model showed superior ability to predict future patient trajectories. Finally, based on the multivariate disease timeline estimated using the MCDP model, the sensitivity of the individual items of the cognitive scales along the different stages of disease was analyzed. The analysis showed that delayed memory recall items had the highest sensitivity in the early stages of disease, whereas language and attention items were sensitive later in disease.},
  archive      = {J_SIM},
  author       = {Line Kühnel and Anna-Karin Berger and Bo Markussen and Lars L. Raket},
  doi          = {10.1002/sim.8932},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3251-3266},
  shortjournal = {Stat. Med.},
  title        = {Simultaneous modeling of alzheimer&#39;s disease progression via multiple cognitive scales},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence interval estimation for treatment effects in
cluster randomization trials based on ranks. <em>SIM</em>,
<em>40</em>(14), 3227–3250. (<a
href="https://doi.org/10.1002/sim.8918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cluster randomization trial is one in which clusters of individuals are randomly allocated to different intervention arms. This design has become the standard for the evaluation of health care and educational strategies. To assess treatment effect, many cluster randomization trials involve outcomes that are lack meaningful units, making interpretation difficult. This difficulty may be dealt with by estimating the Mann-Whitney probability, which quantifies the probability that a typical response from one treatment arm is larger (or smaller) than a typical response from the other arm. In this work, we propose procedures for estimating this probability in cluster randomization trials. Primary emphasis is given to confidence interval estimation in trials with a small number of large clusters. The essence of the procedures is to obtain placement values based on overall ranks and arm-specific ranks prior to application of the ratio estimator, cluster-size-weighted means and mixed models for adjusting clustering effects. Nine confidence intervals were developed by applying three interval methods each based on the three variance estimators. The proposed methods can be applied to studies with binary, ordinal or continuous outcomes without making parametric assumptions. Simulation results demonstrated that the three variance estimators performed equally well, with the confidence interval procedures based on logit and inverse hyperbolic sine transformations performing better in terms of coverage and average interval width, even when the numbers of clusters are as small as 3 to 5 clusters per arm. The methods are illustrated using data from three published cluster randomization trials with SAS code provided.},
  archive      = {J_SIM},
  author       = {Guangyong Zou},
  doi          = {10.1002/sim.8918},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3227-3250},
  shortjournal = {Stat. Med.},
  title        = {Confidence interval estimation for treatment effects in cluster randomization trials based on ranks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Early completion of phase i cancer clinical trials with
bayesian optimal interval design. <em>SIM</em>, <em>40</em>(14),
3215–3226. (<a href="https://doi.org/10.1002/sim.8886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase I cancer clinical trials have been proposed novel designs such as algorithm-based, model-based, and model-assisted designs. Model-based and model-assisted designs have a higher identification rate of maximum tolerated dose (MTD) than algorithm-based designs, but are limited by the fact that the sample size is fixed. Hence, it would be very attractive to estimate the MTD with sufficient accuracy and complete the trial early. O&#39;Quigley proposed the early completion of a trial with the continual reassessment method among model-based designs when the MTD is estimated with sufficient accuracy. However, the proposed early completion method based on the binary outcome trees has a problem that the calculation cost is high when the number of remaining patients is large. Among model-assisted designs, the Bayesian optimal interval (BOIN) design provides the simplest approach for dose adjustment. We propose the novel early completion method for the clinical trials with the BOIN design when the MTD is estimated with sufficient accuracy. This completion method can be easily calculated. In addition, the method does not require many more patients treated for the determination of early completion. We confirm that the BOIN design applying the early completion method has almost the same MTD identification rate compared to the BOIN design through simulations conducted based on over 30 000 scenarios.},
  archive      = {J_SIM},
  author       = {Masahiro Kojima},
  doi          = {10.1002/sim.8886},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {3215-3226},
  shortjournal = {Stat. Med.},
  title        = {Early completion of phase i cancer clinical trials with bayesian optimal interval design},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal planning of adaptive two-stage designs.
<em>SIM</em>, <em>40</em>(13), 3196–3213. (<a
href="https://doi.org/10.1002/sim.8953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive designs are playing an increasingly important role in the planning of clinical trials. While there exists various research on the optimal determination of a two-stage design, non-optimal versions still are frequently applied in clinical research. In this article, we strive to motivate the application of optimal adaptive designs and give guidance on how to determine them. It is demonstrated that optimizing a trial design with respect to particular objective criteria can have a substantial benefit over the application of conventional adaptive sample size recalculation rules. Furthermore, we show that in many practical situations, optimal group-sequential designs show an almost negligible performance loss compared to optimal adaptive designs. Finally, we illustrate how optimal designs can be tailored to specific operational requirements by customizing the underlying optimization problem.},
  archive      = {J_SIM},
  author       = {Maximilian Pilz and Kevin Kunzmann and Carolin Herrmann and Geraldine Rauch and Meinhard Kieser},
  doi          = {10.1002/sim.8953},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3196-3213},
  shortjournal = {Stat. Med.},
  title        = {Optimal planning of adaptive two-stage designs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring latent heterogeneity using many feature variables
supervised by survival outcome. <em>SIM</em>, <em>40</em>(13),
3181–3195. (<a href="https://doi.org/10.1002/sim.8972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cancer studies, it is important to understand disease heterogeneity among patients so that precision medicine can particularly target high-risk patients at the right time. Many feature variables such as demographic variables and biomarkers, combined with a patient&#39;s survival outcome, can be used to infer such latent heterogeneity. In this work, we propose a mixture model to model each patient&#39;s latent survival pattern, where the mixing probabilities for latent groups are modeled through a multinomial distribution. The Bayesian information criterion is used for selecting the number of latent groups. Furthermore, we incorporate variable selection with the adaptive lasso into inference so that only a few feature variables will be selected to characterize the latent heterogeneity. We show that our adaptive lasso estimator has oracle properties when the number of parameters diverges with the sample size. The finite sample performance is evaluated by the simulation study, and the proposed method is illustrated by two datasets.},
  archive      = {J_SIM},
  author       = {Beilin Jia and Donglin Zeng and Jason J. Z. Liao and Guanghan F. Liu and Xianming Tan and Guoqing Diao and Joseph G. Ibrahim},
  doi          = {10.1002/sim.8972},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3181-3195},
  shortjournal = {Stat. Med.},
  title        = {Inferring latent heterogeneity using many feature variables supervised by survival outcome},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigation of patient-sharing networks using a bayesian
network model selection approach for congruence class models.
<em>SIM</em>, <em>40</em>(13), 3167–3180. (<a
href="https://doi.org/10.1002/sim.8969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian approach to conduct network model selection is presented for a general class of network models referred to as the congruence class models (CCMs). CCMs form a broad class that includes as special cases several common network models, such as the Erdős-Rényi-Gilbert model, stochastic block model, and many exponential random graph models. Due to the range of models that can be specified as CCMs, our proposed method is better able to select models consistent with generative mechanisms associated with observed networks than are current approaches. In addition, our approach allows for incorporation of prior information. We illustrate the use of this approach to select among several different proposed mechanisms for the structure of patient-sharing networks; such networks have been found to be associated with the cost and quality of medical care. We found evidence in support of heterogeneity in sociality but not selective mixing by provider type or degree.},
  archive      = {J_SIM},
  author       = {Ravi Goyal and Victor De Gruttola},
  doi          = {10.1002/sim.8969},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3167-3180},
  shortjournal = {Stat. Med.},
  title        = {Investigation of patient-sharing networks using a bayesian network model selection approach for congruence class models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple yet powerful test for assessing goodness-of-fit of
high-dimensional linear models. <em>SIM</em>, <em>40</em>(13),
3153–3166. (<a href="https://doi.org/10.1002/sim.8968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We evaluate the validity of a projection-based test checking linear models when the number of covariates tends to infinity, and analyze two gene expression datasets. We show that the test is still consistent and derive the asymptotic distributions under the null and alternative hypotheses. The asymptotic properties are almost the same as those when the number of covariates is fixed as long as p / n → 0 with additional mild assumptions. The test dramatically gains dimension reduction, and its numerical performance is remarkable.},
  archive      = {J_SIM},
  author       = {Qi Zhang and Feifei Chen and Shunyao Wu and Hua Liang},
  doi          = {10.1002/sim.8968},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3153-3166},
  shortjournal = {Stat. Med.},
  title        = {A simple yet powerful test for assessing goodness-of-fit of high-dimensional linear models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiparameter one-sided tests for nonlinear mixed effects
models with censored responses. <em>SIM</em>, <em>40</em>(13),
3138–3152. (<a href="https://doi.org/10.1002/sim.8966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear mixed-effects (NLME) models are commonly used in longitudinal studies such as pharmacokinetics and HIV viral dynamics studies. NLME models are often derived based on underlying data-generating mechanisms, therefore the parameters in these models often have natural physical interpretations that may suggest reasonable constraints on certain parameters. For example, the HIV viral decay rates for populations receiving anti-HIV treatments may be reasonably expected to be nonnegative. Hypothesis testing for these parameters should incorporate practically reasonable constraints to increase statistical power. Motivated from HIV viral dynamic models, in this article we propose multiparameter one-sided or constrained tests for NLME models with censored responses, for example, viral dynamic models with viral loads subject to lower detection limits. We propose approximate likelihood-based tests that are computationally efficient. We evaluate the tests via simulations and show that the proposed tests are more powerful than the corresponding two-sided or unrestricted tests. We apply the proposed tests to two AIDS datasets with new findings.},
  archive      = {J_SIM},
  author       = {Guohai Zhou and Lang Wu},
  doi          = {10.1002/sim.8966},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3138-3152},
  shortjournal = {Stat. Med.},
  title        = {Multiparameter one-sided tests for nonlinear mixed effects models with censored responses},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fusion designs and estimators for treatment effects.
<em>SIM</em>, <em>40</em>(13), 3124–3137. (<a
href="https://doi.org/10.1002/sim.8963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While randomized trials remain the best evidence for treatment effectiveness, lack of generalizability often remains an important concern. Additionally, when new treatments are compared against existing standards of care, the potentially small benefit of the new treatment may be difficult to detect in a trial without extremely large sample sizes and long follow-up times. Recent advances in “data fusion” provide a framework to combine results across studies that are applicable to a given population of interest and allow treatment comparisons that may not be feasible with traditional study designs. We propose a data fusion-based estimator that can be used to combine information from two studies: (1) a study comparing a new treatment to the standard of care in the local population of interest, and (2) a study comparing the standard of care to placebo in a separate, distal population. We provide conditions under which the parameter of interest can be identified from the two studies described and explore properties of the estimator through simulation. Finally, we apply the estimator to estimate the effect of triple- vs monotherapy for the treatment of HIV using data from two randomized trials. The proposed estimator can account for underlying population structures that induce differences in case mix, adherence, and outcome prevalence between the local and distal populations, and the estimator can also account for potentially informative loss to follow-up. Approaches like those detailed here are increasingly important to speed the approval and adoption of effective new therapies by leveraging multiple sources of information.},
  archive      = {J_SIM},
  author       = {Alexander Breskin and Stephen R. Cole and Jessie K. Edwards and Ron Brookmeyer and Joseph J. Eron and Adimora A. Adimora},
  doi          = {10.1002/sim.8963},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3124-3137},
  shortjournal = {Stat. Med.},
  title        = {Fusion designs and estimators for treatment effects},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric regression analysis of case-cohort studies
with multiple interval-censored disease outcomes. <em>SIM</em>,
<em>40</em>(13), 3106–3123. (<a
href="https://doi.org/10.1002/sim.8962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored failure time data commonly arise in epidemiological and biomedical studies where the occurrence of an event or a disease is determined via periodic examinations. Subject to interval-censoring, available information on the failure time can be quite limited. Cost-effective sampling designs are desirable to enhance the study power, especially when the disease rate is low and the covariates are expensive to obtain. In this work, we formulate the case-cohort design with multiple interval-censored disease outcomes and also generalize it to nonrare diseases where only a portion of diseased subjects are sampled. We develop a marginal sieve weighted likelihood approach, which assumes that the failure times marginally follow the proportional hazards model. We consider two types of weights to account for the sampling bias, and adopt a sieve method with Bernstein polynomials to handle the unknown baseline functions. We employ a weighted bootstrap procedure to obtain a variance estimate that is robust to the dependence structure between failure times. The proposed method is examined via simulation studies and illustrated with a dataset on incident diabetes and hypertension from the Atherosclerosis Risk in Communities study.},
  archive      = {J_SIM},
  author       = {Qingning Zhou and Jianwen Cai and Haibo Zhou},
  doi          = {10.1002/sim.8962},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3106-3123},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric regression analysis of case-cohort studies with multiple interval-censored disease outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial skew-normal/independent models for nonrandomly
missing clustered data. <em>SIM</em>, <em>40</em>(13), 3085–3105. (<a
href="https://doi.org/10.1002/sim.8960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical studies on periodontal disease (PD) often lead to data collected which are clustered in nature (viz. clinical attachment level, or CAL, measured at tooth-sites and clustered within subjects) that are routinely analyzed under a linear mixed model framework, with underlying normality assumptions of the random effects and random errors. However, a careful look reveals that these data might exhibit skewness and tail behavior, and hence the usual normality assumptions might be questionable. Besides, PD progression is often hypothesized to be spatially associated, that is, a diseased tooth-site may influence the disease status of a set of neighboring sites. Also, the presence/absence of a tooth is informative, as the number and location of missing teeth informs about the periodontal health in that region. In this paper, we develop a (shared) random effects model for site-level CAL and binary presence/absence status of a tooth under a Bayesian paradigm. The random effects are modeled using a spatial skew-normal/independent (S-SNI) distribution, whose dependence structure is conditionally autoregressive (CAR). Our S-SNI density presents an attractive parametric tool to model spatially referenced asymmetric thick-tailed structures. Both simulation studies and application to a clinical dataset recording PD status reveal the advantages of our proposition in providing a significantly improved fit, over models that do not consider these features in a unified way.},
  archive      = {J_SIM},
  author       = {Dipankar Bandyopadhyay and Marcos O. Prates and Xiaoyue Zhao and Victor H. Lachos},
  doi          = {10.1002/sim.8960},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3085-3105},
  shortjournal = {Stat. Med.},
  title        = {Spatial skew-normal/independent models for nonrandomly missing clustered data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Individual participant data meta-analysis for external
validation, recalibration, and updating of a flexible parametric
prognostic model. <em>SIM</em>, <em>40</em>(13), 3066–3084. (<a
href="https://doi.org/10.1002/sim.8959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual participant data (IPD) from multiple sources allows external validation of a prognostic model across multiple populations. Often this reveals poor calibration, potentially causing poor predictive performance in some populations. However, rather than discarding the model outright, it may be possible to modify the model to improve performance using recalibration techniques. We use IPD meta-analysis to identify the simplest method to achieve good model performance. We examine four options for recalibrating an existing time-to-event model across multiple populations: (i) shifting the baseline hazard by a constant, (ii) re-estimating the shape of the baseline hazard, (iii) adjusting the prognostic index as a whole, and (iv) adjusting individual predictor effects. For each strategy, IPD meta-analysis examines (heterogeneity in) model performance across populations. Additionally, the probability of achieving good performance in a new population can be calculated allowing ranking of recalibration methods. In an applied example, IPD meta-analysis reveals that the existing model had poor calibration in some populations, and large heterogeneity across populations. However, re-estimation of the intercept substantially improved the expected calibration in new populations, and reduced between-population heterogeneity. Comparing recalibration strategies showed that re-estimating both the magnitude and shape of the baseline hazard gave the highest predicted probability of good performance in a new population. In conclusion, IPD meta-analysis allows a prognostic model to be externally validated in multiple settings, and enables recalibration strategies to be compared and ranked to decide on the least aggressive recalibration strategy to achieve acceptable external model performance without discarding existing model information.},
  archive      = {J_SIM},
  author       = {Joie Ensor and Kym I. E. Snell and Thomas P. A. Debray and Paul C. Lambert and Maxime P. Look and Mamas A. Mamas and Karel G. M. Moons and Richard D. Riley},
  doi          = {10.1002/sim.8959},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3066-3084},
  shortjournal = {Stat. Med.},
  title        = {Individual participant data meta-analysis for external validation, recalibration, and updating of a flexible parametric prognostic model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pathway testing for longitudinal metabolomics. <em>SIM</em>,
<em>40</em>(13), 3053–3065. (<a
href="https://doi.org/10.1002/sim.8957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a top-down approach for pathway analysis of longitudinal metabolite data. We apply a score test based on a shared latent process mixed model which can identify pathways with differentially progressing metabolites. The strength of our approach is that it can handle unbalanced designs, deals with potential missing values in the longitudinal markers, and gives valid results even with small sample sizes. Contrary to bottom-up approaches, correlations between metabolites are explicitly modeled leveraging power gains. For large pathway sizes, a computationally efficient solution is proposed based on pseudo-likelihood methodology. We demonstrate the advantages of the proposed method in identification of differentially expressed pathways through simulation studies. Finally, longitudinal metabolite data from a mice experiment is analyzed to demonstrate our methodology.},
  archive      = {J_SIM},
  author       = {Mitra Ebrahimpoor and Pietro Spitali and Jelle J. Goeman and Roula Tsonaka},
  doi          = {10.1002/sim.8957},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3053-3065},
  shortjournal = {Stat. Med.},
  title        = {Pathway testing for longitudinal metabolomics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A binary hidden markov model on spatial network for
amyotrophic lateral sclerosis disease spreading pattern analysis.
<em>SIM</em>, <em>40</em>(13), 3035–3052. (<a
href="https://doi.org/10.1002/sim.8956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amyotrophic lateral sclerosis (ALS) is a neurological disease that starts at a focal point and gradually spreads to other parts of the nervous system. One of the main clinical symptoms of ALS is muscle weakness. To study spreading patterns of muscle weakness, we analyze spatiotemporal binary muscle strength data, which indicates whether observed muscle strengths are impaired or healthy. We propose a hidden Markov model-based approach that assumes the observed disease status depends on two latent disease states. The model enables us to estimate the incidence rate of ALS disease and the probability of disease state transition. Specifically, the latter is modeled by a logistic autoregression in that the spatial network of susceptible muscles follows a Markov process. The proposed model is flexible to allow both historical muscle conditions and their spatial relationships to be included in the analysis. To estimate the model parameters, we provide an iterative algorithm to maximize sparse-penalized likelihood with bias correction, and use the Viterbi algorithm to label hidden disease states. We apply the proposed approach to analyze the ALS patients&#39; data from EMPOWER Study.},
  archive      = {J_SIM},
  author       = {Yei Eun Shin and Dawei Liu and Huiyan Sang and Toby A. Ferguson and Peter X. K. Song},
  doi          = {10.1002/sim.8956},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3035-3052},
  shortjournal = {Stat. Med.},
  title        = {A binary hidden markov model on spatial network for amyotrophic lateral sclerosis disease spreading pattern analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Informative array testing with multiplex assays.
<em>SIM</em>, <em>40</em>(13), 3021–3034. (<a
href="https://doi.org/10.1002/sim.8954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-volume testing of clinical specimens for sexually transmitted diseases is performed frequently by a process known as group testing. This algorithmic process involves testing portions of specimens from separate individuals together as one unit (or “group”) to detect diseases. Retesting is performed on groups that test positively in order to differentiate between positive and negative individual specimens. The overall goal is to use the least number of tests possible across all individuals without sacrificing diagnostic accuracy. One of the most efficient group testing algorithms is array testing. In its simplest form, specimens are arranged into a grid-like structure so that row and column groups can be formed. Positive-testing rows/columns indicate which specimens to retest. With the growing use of multiplex assays, the increasing number of diseases tested by these assays, and the availability of subject-specific risk information, opportunities exist to make this testing process even more efficient. We propose specific specimen arrangements within an array that can reduce the number of retests needed when compared with other array testing algorithms. We examine how to calculate operating characteristics, including the expected number of tests and the SD for the number of tests, and then subsequently find a best arrangement. Our methods are illustrated for chlamydia and gonorrhea detection with the Aptima Combo 2 Assay. We also provide R functions to make our research accessible to laboratories.},
  archive      = {J_SIM},
  author       = {Christopher R. Bilder and Joshua M. Tebbs and Christopher S. McMahan},
  doi          = {10.1002/sim.8954},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {3021-3034},
  shortjournal = {Stat. Med.},
  title        = {Informative array testing with multiplex assays},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian survival analysis with BUGS. <em>SIM</em>,
<em>40</em>(12), 2975–3020. (<a
href="https://doi.org/10.1002/sim.8933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis is one of the most important fields of statistics in medicine and biological sciences. In addition, the computational advances in the last decades have favored the use of Bayesian methods in this context, providing a flexible and powerful alternative to the traditional frequentist approach. The objective of this article is to summarize some of the most popular Bayesian survival models, such as accelerated failure time, proportional hazards, mixture cure, competing risks, multi-state, frailty, and joint models of longitudinal and survival data. Moreover, an implementation of each presented model is provided using a BUGS syntax that can be run with JAGS from the R programming language. Reference to other Bayesian R-packages is also discussed.},
  archive      = {J_SIM},
  author       = {Danilo Alvares and Elena Lázaro and Virgilio Gómez-Rubio and Carmen Armero},
  doi          = {10.1002/sim.8933},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2975-3020},
  shortjournal = {Stat. Med.},
  title        = {Bayesian survival analysis with BUGS},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combined assessment of early and late-phase outcomes in
orphan drug development. <em>SIM</em>, <em>40</em>(12), 2957–2974. (<a
href="https://doi.org/10.1002/sim.8952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In drug development programs, proof-of-concept Phase II clinical trials typically have a biomarker as a primary outcome, or an outcome that can be observed with relatively short follow-up. Subsequently, the Phase III clinical trials aim to demonstrate the treatment effect based on a clinical outcome that often needs a longer follow-up to be assessed. Early-phase outcomes or biomarkers are typically associated with late-phase outcomes and they are often included in Phase III trials. The decision to proceed to Phase III development is based on analysis of the early-Phase II outcome data. In rare diseases, it is likely that only one Phase II trial and one Phase III trial are available. In such cases and before drug marketing authorization requests, positive results of the early-phase outcome of Phase II trials are then likely seen as supporting (or even replicating) positive Phase III results on the late-phase outcome, without a formal retrospective combined assessment and without accounting for between-study differences. We used double-regression modeling applied to the Phase II and Phase III results to numerically mimic this informal retrospective assessment. We provide an analytical solution for the bias and mean square error of the overall effect that leads to a corrected double-regression. We further propose a flexible Bayesian double-regression approach that minimizes the bias by accounting for between-study differences via discounting the Phase II early-phase outcome when they are not in line with the Phase III biomarker outcome results. We illustrate all methods with an orphan drug example for Fabry disease.},
  archive      = {J_SIM},
  author       = {Konstantinos Pateras and Stavros Nikolakopoulos and Kit C. B. Roes},
  doi          = {10.1002/sim.8952},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2957-2974},
  shortjournal = {Stat. Med.},
  title        = {Combined assessment of early and late-phase outcomes in orphan drug development},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing subgroup selection in two-stage adaptive
enrichment and umbrella designs. <em>SIM</em>, <em>40</em>(12),
2939–2956. (<a href="https://doi.org/10.1002/sim.8949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design two-stage confirmatory clinical trials that use adaptation to find the subgroup of patients who will benefit from a new treatment, testing for a treatment effect in each of two disjoint subgroups. Our proposal allows aspects of the trial, such as recruitment probabilities of each group, to be altered at an interim analysis. We use the conditional error rate approach to implement these adaptations with protection of overall error rates. Applying a Bayesian decision-theoretic framework, we optimize design parameters by maximizing a utility function that takes the population prevalence of the subgroups into account. We show results for traditional trials with familywise error rate control (using a closed testing procedure) as well as for umbrella trials in which only the per-comparison type 1 error rate is controlled. We present numerical examples to illustrate the optimization process and the effectiveness of the proposed designs.},
  archive      = {J_SIM},
  author       = {Nicolás M. Ballarini and Thomas Burnett and Thomas Jaki and Christoper Jennison and Franz König and Martin Posch},
  doi          = {10.1002/sim.8949},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2939-2956},
  shortjournal = {Stat. Med.},
  title        = {Optimizing subgroup selection in two-stage adaptive enrichment and umbrella designs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian approach for estimating age-adjusted rates for
low-prevalence diseases over space and time. <em>SIM</em>,
<em>40</em>(12), 2922–2938. (<a
href="https://doi.org/10.1002/sim.8948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age-adjusted rates are frequently used by epidemiologists to compare disease incidence and mortality across populations. In small geographic regions, age-adjusted rates computed directly from the data are subject to considerable variability and are generally unreliable. Therefore, we desire an approach that accounts for the excessive number of zero counts in disease mapping datasets, which are naturally present for low-prevalence diseases and are further innated when stratifying by age group. Bayesian modeling approaches are naturally suited to employ spatial and temporal smoothing to produce more stable estimates of age-adjusted rates for small areas. We propose a Bayesian hierarchical spatio-temporal hurdle model for counts and demonstrate how age-adjusted rates can be estimated from the hurdle model. We perform a simulation study to evaluate the performance of the proposed model vs a traditional Poisson model on datasets with varying characteristics. The approach is illustrated using two applications to cancer mortality at the county level.},
  archive      = {J_SIM},
  author       = {Melissa Jay and Jacob Oleson and Mary Charlton and Ali Arab},
  doi          = {10.1002/sim.8948},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2922-2938},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach for estimating age-adjusted rates for low-prevalence diseases over space and time},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical bayesian clustering design of multiple
biomarker subgroups (HCOMBS). <em>SIM</em>, <em>40</em>(12), 2893–2921.
(<a href="https://doi.org/10.1002/sim.8946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the Food and Drug Administration&#39;s (FDA&#39;s) acceptance of master protocol designs in recent guidance documents, the oncology field is rapidly moving to address the paradigm shift to molecular subtype focused studies. Identifying new “marker-based” treatments requires new methodologies to address the growing demand to conduct clinical trials in smaller molecular subpopulations, identify effective treatment and marker interactions, and control for false positives. We introduce our methodology, Hierarchical Bayesian Clustering Design of Multiple Biomarker Subgroups (HCOMBS), a two-stage umbrella Phase II design with effect size clustering and information borrowing across multiple biomarker-treatment pairs. HCOMBS was designed to reduce required sample size, differentiate between varying effect sizes, and control for operating characteristics in the multi-arm setting. When compared to independently applied Simon&#39;s Optimal two-stage design, we showed through simulations that HCOMBS required less participants per treatment arm with a well-controlled family-wise error rate and desirable marginal power. Additionally, HCOMBS features a statistical approach that simultaneously conducts clustering and hypothesis testing in one step. We also applied the proposed design on the alliance brain metastases umbrella trial.},
  archive      = {J_SIM},
  author       = {Daniel Kang and Christopher S Coffey and Brian J Smith and Ying Yuan and Qian Shi and Jun Yin},
  doi          = {10.1002/sim.8946},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2893-2921},
  shortjournal = {Stat. Med.},
  title        = {Hierarchical bayesian clustering design of multiple biomarker subgroups (HCOMBS)},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian design and analysis of external pilot trials for
complex interventions. <em>SIM</em>, <em>40</em>(12), 2877–2892. (<a
href="https://doi.org/10.1002/sim.8941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {External pilot trials of complex interventions are used to help determine if and how a confirmatory trial should be undertaken, providing estimates of parameters such as recruitment, retention, and adherence rates. The decision to progress to the confirmatory trial is typically made by comparing these estimates to pre-specified thresholds known as progression criteria, although the statistical properties of such decision rules are rarely assessed. Such assessment is complicated by several methodological challenges, including the simultaneous evaluation of multiple endpoints, complex multi-level models, small sample sizes, and uncertainty in nuisance parameters. In response to these challenges, we describe a Bayesian approach to the design and analysis of external pilot trials. We show how progression decisions can be made by minimizing the expected value of a loss function, defined over the whole parameter space to allow for preferences and trade-offs between multiple parameters to be articulated and used in the decision-making process. The assessment of preferences is kept feasible by using a piecewise constant parametrization of the loss function, the parameters of which are chosen at the design stage to lead to desirable operating characteristics. We describe a flexible, yet computationally intensive, nested Monte Carlo algorithm for estimating operating characteristics. The method is used to revisit the design of an external pilot trial of a complex intervention designed to increase the physical activity of care home residents.},
  archive      = {J_SIM},
  author       = {Duncan T. Wilson and James M. S. Wason and Julia Brown and Amanda J. Farrin and Rebecca E. A. Walwyn},
  doi          = {10.1002/sim.8941},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2877-2892},
  shortjournal = {Stat. Med.},
  title        = {Bayesian design and analysis of external pilot trials for complex interventions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-analysis methods for multiple related markers:
Applications to microbiome studies with the results on multiple
α-diversity indices. <em>SIM</em>, <em>40</em>(12), 2859–2876. (<a
href="https://doi.org/10.1002/sim.8940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is a practical and powerful analytic tool that enables a unified statistical inference across the results from multiple studies. Notably, researchers often report the results on multiple related markers in each study (eg, various α -diversity indices in microbiome studies). However, univariate meta-analyses are limited to combining the results on a single common marker at a time, whereas existing multivariate meta-analyses are limited to the situations where marker-by-marker correlations are given in each study. Thus, here we introduce two meta-analysis methods, multi-marker meta-analysis ( mMeta ) and adaptive multi-marker meta-analysis ( aMeta ), to combine multiple studies throughout multiple related markers with no priori results on marker-by-marker correlations. mMeta is a statistical estimator for a pooled estimate and its SE across all the studies and markers, whereas aMeta is a statistical test based on the test statistic of the minimum P -value among marker-specific meta-analyses. mMeta conducts both effect estimation and hypothesis testing based on a weighted average of marker-specific pooled estimates while estimating marker-by-marker correlations non-parametrically via permutations, yet its power is only moderate. In contrast, aMeta closely approaches the highest power among marker-specific meta-analyses, yet it is limited to hypothesis testing. While their applications can be broader, we illustrate the use of mMeta and aMeta to combine microbiome studies throughout multiple α -diversity indices. We evaluate mMeta and aMeta in silico and apply them to real microbiome studies on the disparity in α -diversity by the status of human immunodeficiency virus (HIV) infection. The R package for mMeta and aMeta is freely available at https://github.com/hk1785/mMeta .},
  archive      = {J_SIM},
  author       = {Hyunwook Koh and Susan Tuddenham and Cynthia L Sears and Ni Zhao},
  doi          = {10.1002/sim.8940},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2859-2876},
  shortjournal = {Stat. Med.},
  title        = {Meta-analysis methods for multiple related markers: Applications to microbiome studies with the results on multiple α-diversity indices},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample size re-estimation for covariate-adaptive randomized
clinical trials. <em>SIM</em>, <em>40</em>(12), 2839–2858. (<a
href="https://doi.org/10.1002/sim.8939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adaptive randomization (CAR) procedures have been developed in clinical trials to mitigate the imbalance of treatments among covariates. In recent years, an increasing number of trials have started to use CAR for the advantages in statistical efficiency and enhancing credibility. At the same time, sample size re-estimation (SSR) has become a common technique in industry to reduce time and cost while maintaining a good probability of success. Despite the widespread popularity of combining CAR designs with SSR, few researchers have investigated this combination theoretically. More importantly, the existing statistical inference must be adjusted to protect the desired type I error rate when a model that omits some covariates is used. In this article, we give a framework for the application of SSR in CAR trials and study the underlying theoretical properties. We give the adjusted test statistic and derive the sample size calculation formula under the CAR setting. We can tackle the difficulties caused by the adaptive features in CAR and prove the asymptotic independence between stages. Numerical studies are conducted under multiple parameter settings and scenarios that are commonly encountered in practice. The results show that all advantages of CAR and SSR can be preserved and further improved in terms of power and sample size.},
  archive      = {J_SIM},
  author       = {Xin Li and Wei Ma and Feifang Hu},
  doi          = {10.1002/sim.8939},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2839-2858},
  shortjournal = {Stat. Med.},
  title        = {Sample size re-estimation for covariate-adaptive randomized clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Censored functional data for incomplete follow-up studies.
<em>SIM</em>, <em>40</em>(12), 2821–2838. (<a
href="https://doi.org/10.1002/sim.8930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis plays an increasingly important role in medical research because patients are followed over time. Thus, the measurements of a particular biomarker for each patient are often registered as curves. Hence, it is of interest to estimate the mean function under certain conditions as an average of the observed functional data over a given period. However, this is often difficult as this type of follow-up studies are confronted with the challenge of some individuals dropping-out before study completion. Therefore, for these individuals, only a partial functional observation is available. In this study, we propose an estimator for the functional mean when the functions may be censored from the right, and thus, only partly observed. Unlike sparse functional data, the censored curves are observed until some (random) time and this censoring time may depend on the trajectory of the functional observations. Our approach is model-free and fully nonparametric, although the proposed methods can also be incorporated into regression models. The use of the functional structure of the data distinguishes our approach from the longitudinal data approaches. In addition, in this study, we propose a bootstrap-based confidence band for the mean function, examine the estimation of the covariance function, and apply our new approach to functional principal component analysis. Employing an extensive simulation study, we demonstrate that our method outperforms the only two existing approaches. Furthermore, we apply our new estimator to a real data example on lung growth, measured by changes in pulmonary function for girls in the United States.},
  archive      = {J_SIM},
  author       = {Ewa Strzalkowska-Kominiak and Juan Romo},
  doi          = {10.1002/sim.8930},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2821-2838},
  shortjournal = {Stat. Med.},
  title        = {Censored functional data for incomplete follow-up studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent class mediator for multiple indicators of mediation.
<em>SIM</em>, <em>40</em>(12), 2800–2820. (<a
href="https://doi.org/10.1002/sim.8929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper demonstrates the utility of latent classes in evaluating the effect of an intervention on an outcome through multiple indicators of mediation. These indicators are observed intermediate variables that identify an underlying latent class mediator, with each class representing a different mediating pathway. The use of a latent class mediator allows us to avoid modeling the complex interactions between the multiple indicators and ensures the decomposition of the total mediating effects into additive effects from individual mediating pathways, a desirable feature for evaluating multiple indicators of mediation. This method is suitable when the goal is to estimate the total mediating effects that can be decomposed into the additive effects of distinct mediating pathways. Each indicator may be involved in multiple mediation pathways and at the same time multiple indicators may contribute to a single mediating pathway. The relative importance of each pathway may vary across subjects. We applied this method to the analysis of the first 6 months of data from a 2-year clustered randomized trial for adults in their first episode of schizophrenia. Four indicators of mediation are considered: individual resiliency training; family psychoeducation; supported education and employment; and a structural assessment for medication. The improvement in symptoms was found to be mediated by the latent class mediator derived from these four service indicators. Simulation studies were conducted to assess the performance of the proposed model and showed that the simultaneous estimation through the maximum likelihood yielded little bias when the entropy of the indicators was high.},
  archive      = {J_SIM},
  author       = {Kyaw Sint and Robert Rosenheck and Haiqun Lin},
  doi          = {10.1002/sim.8929},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2800-2820},
  shortjournal = {Stat. Med.},
  title        = {Latent class mediator for multiple indicators of mediation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the design and the analysis of stratified biomarker
trials in the presence of measurement error. <em>SIM</em>,
<em>40</em>(12), 2783–2799. (<a
href="https://doi.org/10.1002/sim.8928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major emphasis in precision medicine is to optimally treat subgroups of patients who may benefit from certain therapeutic agents. And as such, enormous resources and innovative clinical trials designs in oncology are devoted to identifying predictive biomarkers. Predictive biomarkers are ones that will identify patients that are more likely to respond to specific therapies and they are usually discovered through retrospective analysis from large randomized phase II or phase III trials. One important design to consider is the stratified biomarker design, where patients will have their specimens obtained at baseline and the biomarker status will be assessed prior to random assignment. Regardless of their biomarker status, patients will be randomized to either an experimental arm or the standard of care arm. The stratified biomarker design can be used to test for a treatment-biomarker interaction in predicting a time-to event outcome. Many biomarkers, however, are derived from tissues from patients, and their levels may be heterogeneous. As a result, biomarker levels may be measured with error and this would have an adverse impact on the power of a stratified biomarker clinical trial. We present a trial design and an analysis framework for the stratified biomarker design. We show that the naïve test is biased and provide bias-corrected estimators for computing the sample size and the 95% confidence interval when testing for a treatment-biomarker interaction in predicting a time to event outcome. We propose a sample size formula that adjusts for misclassification and apply it in the design of a phase III clinical trial in renal cancer.},
  archive      = {J_SIM},
  author       = {Susan Halabi and Chen-Yen Lin and Aiyi Liu},
  doi          = {10.1002/sim.8928},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2783-2799},
  shortjournal = {Stat. Med.},
  title        = {On the design and the analysis of stratified biomarker trials in the presence of measurement error},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inverse probability weighted estimation for recurrent events
data with missing category. <em>SIM</em>, <em>40</em>(12), 2765–2782.
(<a href="https://doi.org/10.1002/sim.8927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling recurrent event data with multiple event types has drawn interest in recent biomedical studies due to its flexibility for understanding different risk factors for multiple recurrent event processes. However, in such data type, missing event type appears frequently because of various reasons such as recording ignorance or resource limitation. In this study, we aim to propose an inverse probability weighted estimation that is commonly used in the missing data literature to correct possibly biased estimation by a complete-case analysis. This approach is not limited to a specific form of the recurrent event model. We derive the large sample theory in a general form. We demonstrate that our approach can be applied to either multiplicative or additive rates model with practical sample size via comprehensive simulations. Nonmucoid and mucoid Pseudomonas aeruginosa infections of 14 888 patients in 2016 Cystic Fibrosis Foundation Patient Registry data are analyzed to show that, without including 12% events with missing event type in the analysis, several factors may be misidentified as risk factors for the nonmucoid type of infections.},
  archive      = {J_SIM},
  author       = {Feng-Chang Lin and Jianwen Cai and Yu Deng and Charles R. Esther Jr},
  doi          = {10.1002/sim.8927},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2765-2782},
  shortjournal = {Stat. Med.},
  title        = {Inverse probability weighted estimation for recurrent events data with missing category},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Target estimands for efficient decision making: Response to
comments on “assessing the performance of population adjustment methods
for anchored indirect comparisons: A simulation study.” <em>SIM</em>,
<em>40</em>(11), 2759–2763. (<a
href="https://doi.org/10.1002/sim.8965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {David M. Phillippo and Sofia Dias and Anthony E. Ades and Nicky J. Welton},
  doi          = {10.1002/sim.8965},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2759-2763},
  shortjournal = {Stat. Med.},
  title        = {Target estimands for efficient decision making: response to comments on “Assessing the performance of population adjustment methods for anchored indirect comparisons: a simulation study”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conflating marginal and conditional treatment effects:
Comments on “assessing the performance of population adjustment methods
for anchored indirect comparisons: A simulation study.” <em>SIM</em>,
<em>40</em>(11), 2753–2758. (<a
href="https://doi.org/10.1002/sim.8857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this commentary, we highlight the importance of: (1) carefully considering and clarifying whether a marginal or conditional treatment effect is of interest in a population-adjusted indirect treatment comparison; and (2) developing distinct methodologies for estimating the different measures of effect. The appropriateness of each methodology depends on the preferred target of inference.},
  archive      = {J_SIM},
  author       = {Antonio Remiro-Azócar and Anna Heath and Gianluca Baio},
  doi          = {10.1002/sim.8857},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2753-2758},
  shortjournal = {Stat. Med.},
  title        = {Conflating marginal and conditional treatment effects: comments on “Assessing the performance of population adjustment methods for anchored indirect comparisons: a simulation study”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust estimation of heterogeneous treatment effects using
electronic health record data. <em>SIM</em>, <em>40</em>(11), 2713–2752.
(<a href="https://doi.org/10.1002/sim.8926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of heterogeneous treatment effects is an essential component of precision medicine. Model and algorithm-based methods have been developed within the causal inference framework to achieve valid estimation and inference. Existing methods such as the A-learner, R-learner, modified covariates method (with and without efficiency augmentation), inverse propensity score weighting, and augmented inverse propensity score weighting have been proposed mostly under the square error loss function. The performance of these methods in the presence of data irregularity and high dimensionality, such as that encountered in electronic health record (EHR) data analysis, has been less studied. In this research, we describe a general formulation that unifies many of the existing learners through a common score function. The new formulation allows the incorporation of least absolute deviation (LAD) regression and dimension reduction techniques to counter the challenges in EHR data analysis. We show that under a set of mild regularity conditions, the resultant estimator has an asymptotic normal distribution. Within this framework, we proposed two specific estimators for EHR analysis based on weighted LAD with penalties for sparsity and smoothness simultaneously. Our simulation studies show that the proposed methods are more robust to outliers under various circumstances. We use these methods to assess the blood pressure-lowering effects of two commonly used antihypertensive therapies.},
  archive      = {J_SIM},
  author       = {Ruohong Li and Honglang Wang and Wanzhu Tu},
  doi          = {10.1002/sim.8926},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2713-2752},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation of heterogeneous treatment effects using electronic health record data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corrected score methods for estimating bayesian networks
with error-prone nodes. <em>SIM</em>, <em>40</em>(11), 2692–2712. (<a
href="https://doi.org/10.1002/sim.8925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by inferring cellular signaling networks using noisy flow cytometry data, we develop procedures to draw inference for Bayesian networks based on error-prone data. Two methods for inferring causal relationships between nodes in a network are proposed based on penalized estimation methods that account for measurement error and encourage sparsity. We discuss consistency of the proposed network estimators and develop an approach for selecting the tuning parameter in the penalized estimation methods. Empirical studies are carried out to compare the proposed methods with a naive method that ignores measurement error. Finally, we apply these methods to infer signaling networks using single cell flow cytometry data.},
  archive      = {J_SIM},
  author       = {Xianzheng Huang and Hongmei Zhang},
  doi          = {10.1002/sim.8925},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2692-2712},
  shortjournal = {Stat. Med.},
  title        = {Corrected score methods for estimating bayesian networks with error-prone nodes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonparametric machine learning for precision medicine with
longitudinal clinical trials and bayesian additive regression trees with
mixed models. <em>SIM</em>, <em>40</em>(11), 2665–2691. (<a
href="https://doi.org/10.1002/sim.8924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is an active area of research that could offer an analytic paradigm shift for clinical trials and the subsequent treatment decisions based on them. Clinical trials are typically analyzed with the intent of discovering beneficial treatments if the same treatment is applied to the entire population under study. But, such a treatment strategy could be suboptimal if subsets of the population exhibit varying treatment effects. Identifying subsets of the population experiencing differential treatment effect and forming individualized treatment rules is a task well-suited to modern machine learning methods such as tree-based ensemble predictive models. Specifically, Bayesian additive regression trees (BART) has shown promise in this regard because of its exceptional performance in out-of-sample prediction. Due to the unique inferential needs of precision medicine for clinical trials, we have proposed the BART extensions explicated here. We incorporate random effects for longitudinal repeated measures and subject clustering within medical centers. The addition of a novel interaction detection prior to identify treatment heterogeneity among clinical trial patients and its association with patient characteristics. These extensions are unified under a framework that we call mixedBART. Simulation studies and applications of precision medicine based on real randomized clinical trials data examples are presented.},
  archive      = {J_SIM},
  author       = {Charles Spanbauer and Rodney Sparapani},
  doi          = {10.1002/sim.8924},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2665-2691},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric machine learning for precision medicine with longitudinal clinical trials and bayesian additive regression trees with mixed models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reduced rank multinomial logistic regression in markov
chains with application to cognitive data. <em>SIM</em>,
<em>40</em>(11), 2650–2664. (<a
href="https://doi.org/10.1002/sim.8923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite Markov chains are useful tools for studying transitions among health states; these chains can be complex consisting of a mix of transient and absorbing states. The transition probabilities, which are often affected by covariates, can be difficult to estimate due to the presence of many covariates and/or a subset of transitions that are rarely observed. The purpose of this article is to show how to estimate the effect of a subset of covariates of interest after adjusting for the presence of multiple other covariates by applying multidimensional dimension reduction to the latter. The case in which transitions within each row of the one-step transition probability matrix are estimated by multinomial logistic regression is discussed in detail. Dimension reduction for the adjustment covariates involves estimating the effect of the covariates by a product of matrices iteratively; at each iteration one matrix in the product is fixed while the second is estimated using either standard software or nonlinear estimation, depending on which of the matrices in the product is fixed. The algorithm is illustrated by an application where the effect of at least one Apolipoprotein-E (APOE) gene allele on transition probability is estimated in a Markov Chain that includes adjustment for eight covariates and focuses on transitions from normal cognition to several forms of mild cognitive impairment, with possible absorption into dementia. Data were drawn from annual cognitive assessments of 649 participants enrolled in the BRAiNS cohort at the University of Kentucky&#39;s Alzheimer&#39;s Disease Research Center.},
  archive      = {J_SIM},
  author       = {Pei Wang and Erin L. Abner and David W. Fardo and Frederick A. Schmitt and Gregory A. Jicha and Linda J. Van Eldik and Richard J. Kryscio},
  doi          = {10.1002/sim.8923},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2650-2664},
  shortjournal = {Stat. Med.},
  title        = {Reduced rank multinomial logistic regression in markov chains with application to cognitive data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UTPI: A utility-based toxicity probability interval design
for phase i/II dose-finding trials. <em>SIM</em>, <em>40</em>(11),
2626–2649. (<a href="https://doi.org/10.1002/sim.8922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike chemotherapy, the maximum tolerated dose (MTD) of molecularly targeted agents and immunotherapy may not pose significant clinical benefit over the lower doses. By simultaneously considering both toxicity and efficacy endpoints, phase I/II trials can identify a more clinically meaningful dose for subsequent phase II trials than traditional toxicity-based phase I trials in terms of risk-benefit tradeoff. To strengthen and simplify the current practice of phase I/II trials, we propose a utility-based toxicity probability interval (uTPI) design for finding the optimal biological dose, based on a numerical utility that provides a clinically meaningful, one-dimensional summary representation of the patient&#39;s bivariate toxicity and efficacy outcome. The uTPI design does not rely on any parametric specification of the dose-response relationship, and it directly models the dose desirability through a quasi binomial likelihood. Toxicity probability intervals are used to screen out overly toxic dose levels, and then the dose escalation/de-escalation decisions are made adaptively by comparing the posterior desirability distributions of the adjacent levels of the current dose. The uTPI design is flexible in accommodating various dose desirability formulations, while only requiring minimum design parameters. It has a clear decision structure such that a dose-assignment decision table can be calculated before the trial starts and can be used throughout the trial, which simplifies the practical implementation of the design. Extensive simulation studies demonstrate that the proposed uTPI design yields desirable as well as robust performance under various scenarios.},
  archive      = {J_SIM},
  author       = {Haolun Shi and Jiguo Cao and Ying Yuan and Ruitao Lin},
  doi          = {10.1002/sim.8922},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2626-2649},
  shortjournal = {Stat. Med.},
  title        = {UTPI: A utility-based toxicity probability interval design for phase I/II dose-finding trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accounting for selection bias due to death in estimating the
effect of wealth shock on cognition for the health and retirement study.
<em>SIM</em>, <em>40</em>(11), 2613–2625. (<a
href="https://doi.org/10.1002/sim.8921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Health and Retirement Study (HRS) is a longitudinal study of U.S. adults enrolled at age 50 and older. We were interested in investigating the effect of a sudden large decline in wealth on the cognitive ability of subjects measured using a dataset provided composite score. However, our analysis was complicated by the lack of randomization, time-dependent confounding, and a substantial fraction of the sample and population will die during follow-up leading to some of our outcomes being censored. The common method to handle this type of problem is marginal structural models (MSM). Although MSM produces valid estimates, this may not be the most appropriate method to reflect a useful real-world situation because MSM upweights subjects who are more likely to die to obtain a hypothetical population that over time, resembles that would have been obtained in the absence of death. A more refined and practical framework, principal stratification (PS), would be to restrict analysis to the strata of the population that would survive regardless of negative wealth shock experience. In this work, we propose a new algorithm for the estimation of the treatment effect under PS by imputing the counterfactual survival status and outcomes. Simulation studies suggest that our algorithm works well in various scenarios. We found no evidence that a negative wealth shock experience would affect the cognitive score of HRS subjects.},
  archive      = {J_SIM},
  author       = {Yaoyuan Vincent Tan and Carol A. C. Flannagan and Lindsay R. Pool and Michael R. Elliott},
  doi          = {10.1002/sim.8921},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2613-2625},
  shortjournal = {Stat. Med.},
  title        = {Accounting for selection bias due to death in estimating the effect of wealth shock on cognition for the health and retirement study},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A top scoring pairs classifier for recent HIV infections.
<em>SIM</em>, <em>40</em>(11), 2604–2612. (<a
href="https://doi.org/10.1002/sim.8920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate incidence estimation of HIV infection from cross-sectional biomarker data is crucial for monitoring the epidemic and determining the impact of HIV prevention interventions. A key feature of cross-sectional incidence testing methods is the mean window period, defined as the average duration that infected individuals are classified as recently infected. Two assays available for cross-sectional incidence estimation, the BED capture immunoassay, and the Limiting Antigen (LAg) Avidity assay, measure a general characteristic of antibody response; performance of these assays can be affected and biased by factors such as viral suppression, resulting in sample misclassification and overestimation of HIV incidence. As availability and use of antiretroviral treatment increase worldwide, algorithms that do not include HIV viral load and are not impacted by viral suppression are needed for cross-sectional HIV incidence estimation. Using a phage display system to quantify antibody binding to over 3300 HIV peptides, we present a classifier based on top scoring peptide pairs that identifies recent infections using HIV antibody responses alone. Based on plasma samples from individuals with known dates of seroconversion, we estimated the mean window period for our classifier to be 217 days (95% confidence interval 183 to 257 days), compared to the estimated mean window period for the LAg-Avidity protocol of 106 days (76 to 146 days). Moreover, each of the four peptide pairs correctly classified more of the recent samples than the LAg-Avidity assay alone at the same classification accuracy for non-recent samples.},
  archive      = {J_SIM},
  author       = {Athena Chen and Oliver Laeyendecker and Susan H. Eshleman and Daniel R. Monaco and Kai Kammers and Harry Benjamin Larman and Ingo Ruczinski},
  doi          = {10.1002/sim.8920},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2604-2612},
  shortjournal = {Stat. Med.},
  title        = {A top scoring pairs classifier for recent HIV infections},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal and ethical designs for hypothesis testing in
multi-arm exponential trials. <em>SIM</em>, <em>40</em>(11), 2578–2603.
(<a href="https://doi.org/10.1002/sim.8919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-arm clinical trials are complex experiments which involve several objectives. The demand for unequal allocations in a multi-treatment context is growing and adaptive designs are being increasingly used in several areas of medical research. For uncensored and censored exponential responses, we propose a constrained optimization approach in order to derive the design maximizing the power of the multivariate test of homogeneity, under a suitable ethical constraint. In the absence of censoring, we obtain a very simple closed-form solution that dominates the balanced design in terms of power and ethics. Our suggestion can also accommodate delayed responses and staggered entries, and can be implemented via response adaptive rules. While other targets proposed in the literature could present an unethical behavior, the suggested optimal allocation is frequently unbalanced by assigning more patients to the best treatment, both in the absence and presence of censoring. We evaluate the operating characteristics of our proposal theoretically and by simulations, also redesigning a real lung cancer trial, showing that the constrained optimal target guarantees very good performances in terms of ethical demands, power and estimation precision. Therefore, it is a valid and useful tool in designing clinical trials, especially oncological trials and clinical experiments for grave and novel infectious diseases, where the ethical concern is of primary importance.},
  archive      = {J_SIM},
  author       = {Rosamarie Frieri and Maroussa Zagoraiou},
  doi          = {10.1002/sim.8919},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2578-2603},
  shortjournal = {Stat. Med.},
  title        = {Optimal and ethical designs for hypothesis testing in multi-arm exponential trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of treatment effect in 2-in-1 adaptive design and
some of its extensions. <em>SIM</em>, <em>40</em>(11), 2556–2577. (<a
href="https://doi.org/10.1002/sim.8917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2-in-1 adaptive design allows seamless expansion of an ongoing Phase II trial into a Phase III trial to expedite a drug development program. Since its publication, it has generated a lot of interest. So far, most of the related research focused on type I error control. Similar to most adaptive designs, 2-in-1 design could also pose a great challenge on estimation of treatment effect due to the data-driven adaptation. In addition, the use of intermediate endpoint for interim adaptive decision-making is a less well-studied field. In this paper, we investigate the bias and variances in estimation for 2-in-1 design and some of its extensions, and propose some bias-adjusted estimators for 2-in-1 design. The properties of the proposed estimators are further studied theoretically and/or numerically, so as to provide guidance on how to interpret the estimated treatment effect of 2-in-1 design.},
  archive      = {J_SIM},
  author       = {Wen Li and Xiaofei Bai and Qiqi Deng and Fang Liu and Cong Chen},
  doi          = {10.1002/sim.8917},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2556-2577},
  shortjournal = {Stat. Med.},
  title        = {Estimation of treatment effect in 2-in-1 adaptive design and some of its extensions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating the dilution effect in group testing
regression. <em>SIM</em>, <em>40</em>(11), 2540–2555. (<a
href="https://doi.org/10.1002/sim.8916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When screening for infectious diseases, group testing has proven to be a cost efficient alternative to individual level testing. Cost savings are realized by testing pools of individual specimens (eg, blood, urine, saliva, and so on) rather than by testing the specimens separately. However, a common concern that arises in group testing is the so-called “dilution effect.” This occurs if the signal from a positive individual&#39;s specimen is diluted past an assay&#39;s threshold of detection when it is pooled with multiple negative specimens. In this article, we propose a new statistical framework for group testing data that merges estimation and case identification, which are often treated separately in the literature. Our approach considers analyzing continuous biomarker levels (eg, antibody levels, antigen concentrations, and so on) from pooled samples to estimate both a binary regression model for the probability of disease and the biomarker distributions for cases and controls. To increase case identification accuracy, we then show how estimates of the biomarker distributions can be used to select diagnostic thresholds on a pool-by-pool basis. Our proposals are evaluated through numerical studies and are illustrated using hepatitis B virus data collected on a prison population in Ireland.},
  archive      = {J_SIM},
  author       = {Stefani C. Mokalled and Christopher S. McMahan and Joshua M. Tebbs and Derek Andrew Brown and Christopher R. Bilder},
  doi          = {10.1002/sim.8916},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2540-2555},
  shortjournal = {Stat. Med.},
  title        = {Incorporating the dilution effect in group testing regression},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Rejoinder. <em>SIM</em>, <em>40</em>(11), 2536–2539. (<a
href="https://doi.org/10.1002/sim.8984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Susan S. Ellenberg and Jeffrey S. Morris},
  doi          = {10.1002/sim.8984},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2536-2539},
  shortjournal = {Stat. Med.},
  title        = {Rejoinder},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the role of statisticians and modelers in responding to
AIDS and COVID-19. <em>SIM</em>, <em>40</em>(11), 2530–2535. (<a
href="https://doi.org/10.1002/sim.8943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Britta L. Jewell and Nicholas P. Jewell},
  doi          = {10.1002/sim.8943},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2530-2535},
  shortjournal = {Stat. Med.},
  title        = {On the role of statisticians and modelers in responding to AIDS and COVID-19},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on ellenberg and morris: The role of statisticians
in vaccine surveillance. <em>SIM</em>, <em>40</em>(11), 2528–2529. (<a
href="https://doi.org/10.1002/sim.8944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Robert W. Platt},
  doi          = {10.1002/sim.8944},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2528-2529},
  shortjournal = {Stat. Med.},
  title        = {Comment on ellenberg and morris: The role of statisticians in vaccine surveillance},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment. <em>SIM</em>, <em>40</em>(11), 2526–2527. (<a
href="https://doi.org/10.1002/sim.8947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common themes from the AIDS and COVID-19 pandemics are reviewed from the vantage point of a statistician at the National Institutes of Health.},
  archive      = {J_SIM},
  author       = {Dean Follmann},
  doi          = {10.1002/sim.8947},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2526-2527},
  shortjournal = {Stat. Med.},
  title        = {Comment},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comment on AIDS and COVID-19: A tale of two pandemics and
the role of statisticians. <em>SIM</em>, <em>40</em>(11), 2524–2525. (<a
href="https://doi.org/10.1002/sim.8937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {M. Elizabeth Halloran},
  doi          = {10.1002/sim.8937},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2524-2525},
  shortjournal = {Stat. Med.},
  title        = {Comment on AIDS and COVID-19: A tale of two pandemics and the role of statisticians},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Commentary on the role of statisticians in pandemics.
<em>SIM</em>, <em>40</em>(11), 2521–2523. (<a
href="https://doi.org/10.1002/sim.8935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Ron Brookmeyer},
  doi          = {10.1002/sim.8935},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2521-2523},
  shortjournal = {Stat. Med.},
  title        = {Commentary on the role of statisticians in pandemics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contribution to the discussion of AIDS and covid-19: A tale
of two pandemics and the role of statisticians by ellenberg and morris.
<em>SIM</em>, <em>40</em>(11), 2518–2520. (<a
href="https://doi.org/10.1002/sim.8938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Valerie Isham},
  doi          = {10.1002/sim.8938},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2518-2520},
  shortjournal = {Stat. Med.},
  title        = {Contribution to the discussion of AIDS and covid-19: A tale of two pandemics and the role of statisticians by ellenberg and morris},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical successes and failures during the COVID-19
pandemic: Comments on ellenberg and morris. <em>SIM</em>,
<em>40</em>(11), 2515–2517. (<a
href="https://doi.org/10.1002/sim.8934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Natalie Dean},
  doi          = {10.1002/sim.8934},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2515-2517},
  shortjournal = {Stat. Med.},
  title        = {Statistical successes and failures during the COVID-19 pandemic: Comments on ellenberg and morris},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thoughts on “AIDS and COVID-19: A tale of two pandemics and
the role of statisticians” by susan s. Ellenberg and jeffrey s. morris.
<em>SIM</em>, <em>40</em>(11), 2513–2514. (<a
href="https://doi.org/10.1002/sim.8931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human immunodeficiency virus and Covid-19 (or SARS-CoV-2) differ in their incubation distributions and in their susceptibility to immunologic defense. These features affect our ability to predict the course of these epidemics and to control them.},
  archive      = {J_SIM},
  author       = {Mitchell H. Gail},
  doi          = {10.1002/sim.8931},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2513-2514},
  shortjournal = {Stat. Med.},
  title        = {Thoughts on “AIDS and COVID-19: A tale of two pandemics and the role of statisticians” by susan s. ellenberg and jeffrey s. morris},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of article by ellenberg and morris. <em>SIM</em>,
<em>40</em>(11), 2511–2512. (<a
href="https://doi.org/10.1002/sim.8945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Victor De Gruttola and Ravi Goyal and Natasha K. Martin},
  doi          = {10.1002/sim.8945},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2511-2512},
  shortjournal = {Stat. Med.},
  title        = {Discussion of article by ellenberg and morris},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). AIDS and COVID: A tale of two pandemics and the role of
statisticians. <em>SIM</em>, <em>40</em>(11), 2499–2510. (<a
href="https://doi.org/10.1002/sim.8936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world has experienced three global pandemics over the last half-century: HIV/AIDS, H1N1, and COVID-19. HIV/AIDS and COVID-19 are still with us and have wrought extensive havoc worldwide. There are many differences between these two infections and their global impacts, but one thing they have in common is the mobilization of scientific resources to both understand the infection and develop ways to combat it. As was the case with HIV, statisticians have been in the forefront of scientists working to understand transmission dynamics and the natural history of infection, determine prognostic factors for severe disease, and develop optimal study designs to assess therapeutics and vaccines.},
  archive      = {J_SIM},
  author       = {Susan S. Ellenberg and Jeffrey S. Morris},
  doi          = {10.1002/sim.8936},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {2499-2510},
  shortjournal = {Stat. Med.},
  title        = {AIDS and COVID: A tale of two pandemics and the role of statisticians},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selecting the model for multiple imputation of missing data:
Just use an IC! <em>SIM</em>, <em>40</em>(10), 2467–2497. (<a
href="https://doi.org/10.1002/sim.8915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation and maximum likelihood estimation (via the expectation-maximization algorithm) are two well-known methods readily used for analyzing data with missing values. While these two methods are often considered as being distinct from one another, multiple imputation (when using improper imputation) is actually equivalent to a stochastic expectation-maximization approximation to the likelihood. In this article, we exploit this key result to show that familiar likelihood-based approaches to model selection, such as Akaike&#39;s information criterion (AIC) and the Bayesian information criterion (BIC), can be used to choose the imputation model that best fits the observed data. Poor choice of imputation model is known to bias inference, and while sensitivity analysis has often been used to explore the implications of different imputation models, we show that the data can be used to choose an appropriate imputation model via conventional model selection tools. We show that BIC can be consistent for selecting the correct imputation model in the presence of missing data. We verify these results empirically through simulation studies, and demonstrate their practicality on two classical missing data examples. An interesting result we saw in simulations was that not only can parameter estimates be biased by misspecifying the imputation model, but also by overfitting the imputation model. This emphasizes the importance of using model selection not just to choose the appropriate type of imputation model, but also to decide on the appropriate level of imputation model complexity.},
  archive      = {J_SIM},
  author       = {Firouzeh Noghrehchi and Jakub Stoklosa and Spiridon Penev and David I. Warton},
  doi          = {10.1002/sim.8915},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2467-2497},
  shortjournal = {Stat. Med.},
  title        = {Selecting the model for multiple imputation of missing data: Just use an IC!},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative poisson clinical trials of multiple experimental
treatments vs a single control using the negative multinomial
distribution. <em>SIM</em>, <em>40</em>(10), 2452–2466. (<a
href="https://doi.org/10.1002/sim.8914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a method which conditions on the number of events that occur in the control group to determine rejection regions and power for comparative Poisson trials with multiple experimental treatment arms that are each compared to one control arm. This leads to the negative multinomial as the statistical distribution used for testing. For one experimental treatment and one control with curtailed sampling, this is equivalent to Gail&#39;s (1974) approach. We provide formulas to calculate exact one-sided overall Type I error and pointwise power for tests of treatment superiority and inferiority (vs the control). Tables of trial design parameters for combinations of one-sided overall Type I error = 0.05, 0.01 and pointwise power = 0.90, 0.80 are provided. Curtailment approaches are presented to stop follow-up of experimental treatment arms or to stop the study entirely once the final outcomes for each arm are known.},
  archive      = {J_SIM},
  author       = {Joseph A. Chiarappa and Donald R. Hoover},
  doi          = {10.1002/sim.8914},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2452-2466},
  shortjournal = {Stat. Med.},
  title        = {Comparative poisson clinical trials of multiple experimental treatments vs a single control using the negative multinomial distribution},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLRT-mod: An efficient methodology for pharmacometric
model-based analysis of longitudinal phase II dose finding studies under
model uncertainty. <em>SIM</em>, <em>40</em>(10), 2435–2451. (<a
href="https://doi.org/10.1002/sim.8913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the challenging context of phase II dose-finding trials, longitudinal analyses may increase drug effect detection power compared to an end-of-treatment analysis. This work proposes cLRT-Mod, a pharmacometric adaptation of the MCP-Mod methodology, which allows the use of nonlinear mixed effect models to first detect a dose-response signal and then identify the doses for the confirmatory phase while accounting for model structure uncertainty. The method was evaluated through extensive clinical trial simulations of a hypothetical phase II dose-finding trial using different scenarios and comparing different methods such as MCP-Mod. The results show an increase in power using cLRT with longitudinal data compared to an EOT multiple contrast tests for scenarios with small sample size and weak drug effect while maintaining pre-specifiability of the models prior to data analysis and the nominal type I error. This work shows how model averaging provides better coverage probability of the drug effect in the prediction step, and avoids under-estimation of the size of the confidence interval. Finally, for illustration purpose cLRT-Mod was applied to the analysis of a real phase II dose-finding trial.},
  archive      = {J_SIM},
  author       = {Simon Buatois and Sebastian Ueckert and Nicolas Frey and Sylvie Retout and France Mentré},
  doi          = {10.1002/sim.8913},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2435-2451},
  shortjournal = {Stat. Med.},
  title        = {CLRT-mod: An efficient methodology for pharmacometric model-based analysis of longitudinal phase II dose finding studies under model uncertainty},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel test by combining the maximum and minimum values
among a large number of dependent z-scores with application to genome
wide association study. <em>SIM</em>, <em>40</em>(10), 2422–2434. (<a
href="https://doi.org/10.1002/sim.8912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel test via combining the maximum and minimum values among a large number of dependent Z -scores for testing the hypothesis with sparse signals. The proposed test employs the information about different signs of maximum and minimum Z -scores and thus power is gained. Its asymptotic null distribution is derived under the null hypothesis and some regular conditions. Extensive simulation studies are conducted to show the advantages of the proposed test by comparing with two existing ones. A real application to the lipids genome wide association study further shows its performances.},
  archive      = {J_SIM},
  author       = {Zhengbang Li and Sanan Qin and Qizhai Li},
  doi          = {10.1002/sim.8912},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2422-2434},
  shortjournal = {Stat. Med.},
  title        = {A novel test by combining the maximum and minimum values among a large number of dependent Z-scores with application to genome wide association study},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting study duration in clinical trials with a
time-to-event endpoint. <em>SIM</em>, <em>40</em>(10), 2413–2421. (<a
href="https://doi.org/10.1002/sim.8911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In event-driven clinical trials comparing the survival functions of two groups, the number of events required to achieve the desired power is usually calculated using the Freedman formula or the Schoenfeld formula. Then, the sample size and the study duration derived from the required number of events are considered; however, their combination is not uniquely determined. In practice, various combinations are examined considering the enrollment speed, study duration, and the cost of enrollment. However, effective methods for visually representing their relationships and evaluating the uncertainty in study duration are insufficient. We developed a graphical approach for examining the relationship between sample size and study duration. To evaluate the uncertainty in study duration under a given sample size, we also derived the probability density function of the study duration and a method for updating the probability density function according to the observed number of events (ie, information time). The proposed methods are expected to improve the operation and management of clinical trials with a time-to-event endpoint.},
  archive      = {J_SIM},
  author       = {Ryunosuke Machida and Yosuke Fujii and Takashi Sozu},
  doi          = {10.1002/sim.8911},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2413-2421},
  shortjournal = {Stat. Med.},
  title        = {Predicting study duration in clinical trials with a time-to-event endpoint},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marginal analysis of current status data with informative
cluster size using a class of semiparametric transformation cure models.
<em>SIM</em>, <em>40</em>(10), 2400–2412. (<a
href="https://doi.org/10.1002/sim.8910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research is motivated by a periodontal disease dataset that possesses certain special features. The dataset consists of clustered current status time-to-event observations with large and varying cluster sizes, where the cluster size is associated with the disease outcome. Also, heavy censoring is present in the data even with long follow-up time, suggesting the presence of a cured subpopulation. In this paper, we propose a computationally efficient marginal approach, namely the cluster-weighted generalized estimating equation approach, to analyze the data based on a class of semiparametric transformation cure models. The parametric and nonparametric components of the model are estimated using a Bernstein-polynomial based sieve maximum pseudo-likelihood approach. The asymptotic properties of the proposed estimators are studied. Simulation studies are conducted to evaluate the performance of the proposed estimators in scenarios with different degree of informative clustering and within-cluster dependence. The proposed method is applied to the motivating periodontal disease data for illustration.},
  archive      = {J_SIM},
  author       = {Kwok Fai Lam and Chun Yin Lee and Kin Yau Wong and Dipankar Bandyopadhyay},
  doi          = {10.1002/sim.8910},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2400-2412},
  shortjournal = {Stat. Med.},
  title        = {Marginal analysis of current status data with informative cluster size using a class of semiparametric transformation cure models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exact confidence limits after a group sequential single arm
binary trial. <em>SIM</em>, <em>40</em>(10), 2389–2399. (<a
href="https://doi.org/10.1002/sim.8909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential single arm designs are common in phase II trials as well as attribute testing and acceptance sampling. After the trial is completed, especially if the recommendation is to proceed to further testing, there is interest in full inference on treatment efficacy. For a binary response, there is the potential to construct exact upper and lower confidence limits, the first published method for which is Jennison and Turnbull (1983). We place their method within the modern theory of exact confidence limits and provide a new general result that ensures that the exact limits are consistent with the test result, an issue that has been largely ignored in the literature. Amongst methods based on the minimal sufficient statistic, we propose two exact methods that out-perform Jennison and Turnbull&#39;s method across 10 selected designs. One of these we prefer and recommend for practical and theoretical reasons. We also investigate a method based on inverting Fisher&#39;s combination test, as well as a pure tie-breaking variant of it. For the range of designs considered, neither of these methods result in large enough improvements in efficiency to justify violation of the sufficiency principle. For any nonadaptive sequential design, an R-package is provided to select a method and compute the inference from a given realization.},
  archive      = {J_SIM},
  author       = {Chris J. Lloyd},
  doi          = {10.1002/sim.8909},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2389-2399},
  shortjournal = {Stat. Med.},
  title        = {Exact confidence limits after a group sequential single arm binary trial},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized linear mixed hidden semi-markov models in
longitudinal settings: A bayesian approach. <em>SIM</em>,
<em>40</em>(10), 2373–2388. (<a
href="https://doi.org/10.1002/sim.8908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov and semi-Markov models (H(S)MMs) constitute useful tools for modeling observations subject to certain dependency structures. The hidden states render these models very flexible and allow them to capture many different types of latent patterns and dynamics present in the data. This has led to the increased popularity of these models, which have been applied to a variety of problems in various domains and settings, including longitudinal data. In many longitudinal studies, the response variable is categorical or count-type. Generalized linear mixed models (GLMMs) can be used to analyze a wide range of variables, including categorical and count. The present study proposes a model that combines HSMMs with GLMMs, leading to generalized linear mixed hidden semi-Markov models (GLM-HSMMs). These models can account for time-varying unobserved heterogeneity and handle different response types. Parameter estimation is achieved using a Monte Carlo Newton-Raphson (MCNR)-like algorithm. In our proposed model, the distribution of the random effects depends on hidden states. We illustrate the applicability of GLM-HSMMs with an example in the field of occupational health, where the response variable consists of count values. Furthermore, we assess the performance of our MCNR-like algorithm through a simulation study.},
  archive      = {J_SIM},
  author       = {Saiedeh Haji-Maghsoudi and Jan Bulla and Majid Sadeghifar and Ghodratollah Roshanaei and Hossein Mahjub},
  doi          = {10.1002/sim.8908},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2373-2388},
  shortjournal = {Stat. Med.},
  title        = {Generalized linear mixed hidden semi-markov models in longitudinal settings: A bayesian approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Qualifying drug dosing regimens in pediatrics using gaussian
processes. <em>SIM</em>, <em>40</em>(10), 2355–2372. (<a
href="https://doi.org/10.1002/sim.8907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug development commonly studies an adult population first and then the pediatric population. The knowledge from the adult population is taken advantage of for the design of the pediatric trials. Adjusted drug doses for these are often derived from adult pharmacokinetic (PK) models which are extrapolated to patients with smaller body size. This extrapolation is based on scaling physiologic model parameters with a body size measure accounting for organ size differences. The inherent assumption is that children are merely small adults. However, children can be subject to additional effects such as organ maturation. These effects are not present in the adult population and are possibly overlooked at the design stage of the pediatric trials. It is thus crucial to qualify the extrapolation assumptions once the pediatric trial data are available. In this work, we propose a model based on a non-parametric regression method called Gaussian process (GP) to detect deviations from the made extrapolation assumptions. We introduce the theoretical background of this model and compare its performance to a parametric expansion of the adult model. The comparison includes simulations and a clinical study data example. The results show that the GP approach can reliably detect maturation trends from sparse pediatric data.},
  archive      = {J_SIM},
  author       = {Eero Siivola and Sebastian Weber and Aki Vehtari},
  doi          = {10.1002/sim.8907},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2355-2372},
  shortjournal = {Stat. Med.},
  title        = {Qualifying drug dosing regimens in pediatrics using gaussian processes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impact of adjusting for pure predictors of exposure,
mediator, and outcome on the variance of natural direct and indirect
effect estimators. <em>SIM</em>, <em>40</em>(10), 2339–2354. (<a
href="https://doi.org/10.1002/sim.8906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is now well established that adjusting for pure predictors of the outcome, in addition to confounders, allows unbiased estimation of the total exposure effect on an outcome with generally reduced standard errors (SEs). However, no analogous results have been derived for mediation analysis. Considering the simplest linear regression setting and the ordinary least square estimator, we obtained theoretical results showing that adjusting for pure predictors of the outcome, in addition to confounders, allows unbiased estimation of the natural indirect effect (NIE) and the natural direct effect (NDE) on the difference scale with reduced SEs. Adjusting for pure predictors of the mediator increases the SE of the NDE&#39;s estimator, but may increase or decrease the variance of the NIE&#39;s estimator. Adjusting for pure predictors of the exposure increases the variance of estimators of the NIE and NDE. Simulation studies were used to confirm and extend these results to the case where the mediator or the outcome is binary. Additional simulations were conducted to explore scenarios featuring an exposure-mediator interaction as well as the relative risk and odds ratio scales for the case of binary mediator and outcome. Both a regression approach and an inverse probability weighting approach were considered in the simulation study. A real-data illustration employing data from the Canadian Study of Health and Aging is provided. This analysis is concerned with the mediating effect of vitamin D in the effect of physical activity on dementia and its results are overall consistent with the theoretical and empirical findings.},
  archive      = {J_SIM},
  author       = {Awa Diop and Geneviève Lefebvre and Caroline S. Duchaine and Danielle Laurin and Denis Talbot},
  doi          = {10.1002/sim.8906},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2339-2354},
  shortjournal = {Stat. Med.},
  title        = {The impact of adjusting for pure predictors of exposure, mediator, and outcome on the variance of natural direct and indirect effect estimators},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On information fraction for fleming-harrington type weighted
log-rank tests in a group-sequential clinical trial design.
<em>SIM</em>, <em>40</em>(10), 2321–2338. (<a
href="https://doi.org/10.1002/sim.8905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When comparing survival times of treatment and control groups under a more realistic nonproportional hazards scenario, the standard log-rank (SLR) test may be replaced by a more efficient weighted log-rank (WLR) test, such as the Fleming-Harrington (FH) test. Designing a group-sequential clinical trial with one or more interim looks during which a FH test will be performed, necessitates correctly quantifying the information fraction (IF). For SLR test, IF is defined simply as the ratio of interim to final numbers of events; but for FH test, it can deviate substantially from this ratio. In this article, we separate the effect of weight function (of FH test) alone on IF from the effect of censoring. We have shown that, without considering the effect of censoring, IF can be derived analytically for FH test using information available at the design stage and the additional effect due to censoring is relatively smaller. This article intends to serve two major purposes: first, to emphasize and rationalize the deviation of IF in weighted log-rank test from that of SLR test which is often overlooked (Jiménez, Stalbovskaya, and Jones); second, although it is impossible to predict IF for a weighted log-rank test at the design stage, our decomposition of effects on IF provides a reasonable and practically feasible range of IF to work with. We illustrate our approach with an example and provide simulation results to evaluate operating characteristics.},
  archive      = {J_SIM},
  author       = {Madan G. Kundu and Jyotirmoy Sarkar},
  doi          = {10.1002/sim.8905},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2321-2338},
  shortjournal = {Stat. Med.},
  title        = {On information fraction for fleming-harrington type weighted log-rank tests in a group-sequential clinical trial design},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). More robust estimation of average treatment effects using
kernel optimal matching in an observational study of spine surgical
interventions. <em>SIM</em>, <em>40</em>(10), 2305–2320. (<a
href="https://doi.org/10.1002/sim.8904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability of treatment weighting (IPTW), which has been used to estimate average treatment effects (ATE) using observational data, tenuously relies on the positivity assumption and the correct specification of the treatment assignment model, both of which are problematic assumptions in many observational studies. Various methods have been proposed to overcome these challenges, including truncation, covariate-balancing propensity scores, and stable balancing weights. Motivated by an observational study in spine surgery, in which positivity is violated and the true treatment assignment model is unknown, we present the use of optimal balancing by kernel optimal matching (KOM) to estimate ATE. By uniformly controlling the conditional mean squared error of a weighted estimator over a class of models, KOM simultaneously mitigates issues of possible misspecification of the treatment assignment model and is able to handle practical violations of the positivity assumption, as shown in our simulation study. Using data from a clinical registry, we apply KOM to compare two spine surgical interventions and demonstrate how the result matches the conclusions of clinical trials that IPTW estimates spuriously refute.},
  archive      = {J_SIM},
  author       = {Nathan Kallus and Brenton Pennicooke and Michele Santacatterina},
  doi          = {10.1002/sim.8904},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {2305-2320},
  shortjournal = {Stat. Med.},
  title        = {More robust estimation of average treatment effects using kernel optimal matching in an observational study of spine surgical interventions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiparametric linear transformation models for indirectly
observed outcomes. <em>SIM</em>, <em>40</em>(9), 2286–2303. (<a
href="https://doi.org/10.1002/sim.8903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a regression framework to analyze outcomes that are indirectly observed via one or multiple proxies. Semiparametric transformation models, including Cox proportional hazards regression, turn out to be well suited to model the association between the covariates and the unobserved outcome. By coupling this regression model to a semiparametric measurement model, we can estimate these associations without requiring calibration data and without imposing strong functional assumptions on the relationship between the unobserved outcome and its proxy. When multiple proxies are available, we propose a data-driven aggregation resulting in an improved proxy. We empirically validate the proposed methodology in a simulation study, revealing good finite sample properties, especially when multiple proxies are aggregated. The methods are demonstrated on two case studies.},
  archive      = {J_SIM},
  author       = {Jan De Neve and Heidelinde Dehaene},
  doi          = {10.1002/sim.8903},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2286-2303},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric linear transformation models for indirectly observed outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proportional likelihood ratio mixed model for discrete
longitudinal data. <em>SIM</em>, <em>40</em>(9), 2272–2285. (<a
href="https://doi.org/10.1002/sim.8902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rathouz and Gao [2] and Luo and Tsai [3] proposed valuable extensions to the generalized linear model for modeling a nonlinear monotonic relationship between the mean response and a set of covariates. In their extensions for discrete data the baseline response distribution is unspecified and is estimated from the data. We propose to extend this model for the analysis of longitudinal data by incorporating random effects into the linear predictor, and using maximum likelihood for estimation and inference. Motivated in particular by longitudinal studies of clinical scale outcomes, we developed an estimation procedure for a finite-support response using a generalized expectation-maximization algorithm where Gauss-Hermite quadrature is employed to approximate the integrals in the E step of the algorithm. Upon convergence, the observed information matrix is estimated through second-order numerical differentiation of the log-likelihood function. Asymptotic properties of the maximum likelihood estimates follow under the usual regularity conditions. Simulation studies are conducted to assess its finite-sample properties and compare the proposed model to the generalized linear mixed model. The proposed method is illustrated in an analysis of data from a longitudinal study of Huntington&#39;s disease.},
  archive      = {J_SIM},
  author       = {Hongqian Wu and Michael P. Jones},
  doi          = {10.1002/sim.8902},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2272-2285},
  shortjournal = {Stat. Med.},
  title        = {Proportional likelihood ratio mixed model for discrete longitudinal data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel estimand to adjust for rescue treatment in
randomized clinical trials. <em>SIM</em>, <em>40</em>(9), 2257–2271. (<a
href="https://doi.org/10.1002/sim.8901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretation of randomized clinical trial results is often complicated by intercurrent events. For instance, rescue medication is sometimes given to patients in response to worsening of their disease, either in addition to the randomized treatment or in its place. The use of such medication complicates the interpretation of the intention-to-treat analysis. In view of this, we propose a novel estimand defined as the intention-to-treat effect that would have been observed, had patients on the active arm been switched to rescue medication if and only if they would have been switched when randomized to control. This enables us to disentangle the treatment effect from the effect of rescue medication on a patient&#39;s outcome, while tempering the strong extrapolations that are typically needed when inferring what the intention-to-treat effect would have been in the absence of rescue medication. We propose a novel inverse probability weighting method for estimating this effect in settings where the decision to initiate rescue medication is made at one prespecified time point. This estimator relies on specific untestable assumptions, in view of which we propose a sensitivity analysis. We use the method for the analysis of a clinical trial conducted by Janssen Pharmaceuticals, in which patients with type 2 diabetes mellitus can switch to rescue medication for ethical reasons. Monte Carlo simulations confirm that the proposed estimator is unbiased in moderate sample sizes.},
  archive      = {J_SIM},
  author       = {Hege Michiels and Cristina Sotto and An Vandebosch and Stijn Vansteelandt},
  doi          = {10.1002/sim.8901},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2257-2271},
  shortjournal = {Stat. Med.},
  title        = {A novel estimand to adjust for rescue treatment in randomized clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrative sparse partial least squares. <em>SIM</em>,
<em>40</em>(9), 2239–2256. (<a
href="https://doi.org/10.1002/sim.8900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial least squares, as a dimension reduction technique, has become increasingly important for its ability to deal with problems with a large number of variables. Since noisy variables may weaken estimation performance, the sparse partial least squares (SPLS) technique has been proposed to identify important variables and generate more interpretable results. However, the small sample size of a single dataset limits the performance of conventional methods. An effective solution comes from gathering information from multiple comparable studies. Integrative analysis has essential importance in multidatasets analysis. The main idea is to improve performance by assembling raw data from multiple independent datasets and analyzing them jointly. In this article, we develop an integrative SPLS (iSPLS) method using penalization based on the SPLS technique. The proposed approach consists of two penalties. The first penalty conducts variable selection under the context of integrative analysis. The second penalty, a contrasted penalty, is imposed to encourage the similarity of estimates across datasets and generate more sensible and accurate results. Computational algorithms are developed. Simulation experiments are conducted to compare iSPLS with alternative approaches. The practical utility of iSPLS is shown in the analysis of two TCGA gene expression data.},
  archive      = {J_SIM},
  author       = {Weijuan Liang and Shuangge Ma and Qingzhao Zhang and Tingyu Zhu},
  doi          = {10.1002/sim.8900},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2239-2256},
  shortjournal = {Stat. Med.},
  title        = {Integrative sparse partial least squares},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized confidence interval for an agreement between
raters. <em>SIM</em>, <em>40</em>(9), 2230–2238. (<a
href="https://doi.org/10.1002/sim.8899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation and inference are two key components toward the solution of any statistical problem; however, the inferential issues of statistical assessment of agreement among two or more raters have not been well developed as compared to the development of estimation procedures in this area. The fundamental reason for this gap is the complex expression of the concordance correlation coefficient (CCC) that is frequently used in assessing agreement among raters. Large sample-based statistical tests for CCC often fail to produce desired results for small samples. Hence, inferential procedures for small samples are urgently needed to evaluate agreement between raters. We argue that hypothesis testing of CCC has little value in practice due to the absence of a gold standard of agreement. In this article, we construct the generalized confidence interval (GCI) for CCC utilizing a bivariate normal distribution of measurements, and also develop a large sample-based confidence interval (LSCI). We establish satisfactory performance of GCI by providing the desired coverage probability (CP) via simulation. Results of GCI and LSCI are illustrated and compared with a data set of a recent study performed at U.S. Department of Veterans Affairs, Hines.},
  archive      = {J_SIM},
  author       = {Dulal K. Bhaumik and Hairong Shi and Domenic J. Reda and Bikas K. Sinha},
  doi          = {10.1002/sim.8899},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2230-2238},
  shortjournal = {Stat. Med.},
  title        = {Generalized confidence interval for an agreement between raters},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Randomization-based inference in the presence of selection
bias. <em>SIM</em>, <em>40</em>(9), 2212–2229. (<a
href="https://doi.org/10.1002/sim.8898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the analysis of clinical trials, the study participants are usually assumed to be representative sample of a target population. This assumption is rarely fulfilled in clinical trials, and particularly not if the sample size is small. In addition, covariate imbalances may affect the trial. Randomization tests provide a nonparametric analysis method of the treatment effect that does not rely on population-based assumptions. We propose a nonparametric statistical model that yields a formal basis for randomization tests. We adapt the model for the presence of covariate imbalance in the form of selection bias and investigate the effects of bias on the rejection probability of the randomization test using Monte Carlo simulations. Finally, we show that ancillary statistics can be used to control for the influence of bias. We show that covariate imbalance leads to an inflation of the type I error probability. The proposed nonparametric model allows for the use of ancillary statistics that yield an unbiased adjusted randomization test.},
  archive      = {J_SIM},
  author       = {Diane Uschner},
  doi          = {10.1002/sim.8898},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2212-2229},
  shortjournal = {Stat. Med.},
  title        = {Randomization-based inference in the presence of selection bias},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). District-level estimation of vaccination coverage: Discrete
vs continuous spatial models. <em>SIM</em>, <em>40</em>(9), 2197–2211.
(<a href="https://doi.org/10.1002/sim.8897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health and development indicators (HDIs) such as vaccination coverage are regularly measured in many low- and middle-income countries using household surveys, often due to the unreliability or incompleteness of routine data collection systems. Recently, the development of model-based approaches for producing subnational estimates of HDIs using survey data, particularly cluster-level data, has been an active area of research. This is mostly driven by the increasing demand for estimates at certain administrative levels, for example, districts, at which many development goals are set and evaluated. In this study, we explore spatial modeling approaches for producing district-level estimates of vaccination coverage. Specifically, we compare discrete spatial smoothing models which directly model district-level data with continuous Gaussian process (GP) models that utilize geolocated cluster-level data. We adopt a fully Bayesian framework, implemented using the INLA and SPDE approaches. We compare the predictive performance of the models by analyzing vaccination coverage using data from two Demographic and Health Surveys (DHS), namely the 2014 Kenya DHS and the 2015-16 Malawi DHS. We find that the continuous GP models performed well, offering a credible alternative to traditional discrete spatial smoothing models. Our analysis also revealed that accounting for between-cluster variation in the continuous GP models did not have any real effect on the district-level estimates. Our results provide guidance to practitioners on the reliability of these model-based approaches for producing estimates of vaccination coverage and other HDIs.},
  archive      = {J_SIM},
  author       = {C. Edson Utazi and Kristine Nilsen and Oliver Pannell and Winfred Dotse-Gborgbortsi and Andrew J. Tatem},
  doi          = {10.1002/sim.8897},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2197-2211},
  shortjournal = {Stat. Med.},
  title        = {District-level estimation of vaccination coverage: Discrete vs continuous spatial models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and modeling of the restricted mean time lost in
the presence of competing risks. <em>SIM</em>, <em>40</em>(9),
2177–2196. (<a href="https://doi.org/10.1002/sim.8896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival data with competing or semi-competing risks are common in observational studies. As an alternative to cause-specific and subdistribution hazard ratios, the between-group difference in cause-specific restricted mean times lost (RMTL) gives the mean difference in life expectancy lost to a specific cause of death or in disease-free time lost, in the case of a nonfatal outcome, over a prespecified period. To adjust for covariates, we introduce an inverse probability weighted estimator and its variance for the marginal difference in RMTL. We also introduce an inverse probability of censoring weighted regression model for the RMTL. In simulation studies, we examined the finite sample performance of the proposed methods under proportional and nonproportional subdistribution hazards scenarios. We illustrated both methods with competing risks data from the Framingham Heart Study. We estimated sex differences in atrial fibrillation (AF)-free times lost over 40 years. We also estimated sex differences in mean lifetime lost to cardiovascular disease (CVD) and non-CVD death over 10 years among individuals with AF.},
  archive      = {J_SIM},
  author       = {Sarah C. Conner and Ludovic Trinquart},
  doi          = {10.1002/sim.8896},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2177-2196},
  shortjournal = {Stat. Med.},
  title        = {Estimation and modeling of the restricted mean time lost in the presence of competing risks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric and nonparametric improvements in bland and
altman’s assessment of agreement method. <em>SIM</em>, <em>40</em>(9),
2155–2176. (<a href="https://doi.org/10.1002/sim.8895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bland-Altman method, which assesses agreement via an assessment set constructed by the difference of the measurement variables, has received great attention. Other assessment approaches have been proposed following the same difference-based framework. However, the exact assessment set constructed by the difference is achievable only for measurements with certain joint distributions. To provide a more general assessment framework, we propose two approaches. First, when the measurement distribution is known, we propose a parametric approach that constructs the assessment set through a measure of closeness corresponding to the distribution. Second, when the measurement distribution is unknown, we propose a nonparametric approach that constructs the assessment set through quantile regression. Both approaches quantify the degree of agreement with the presence of both systematic and random measurement errors, and enable one to go beyond the difference-based approach. Results of simulation and data analyses are presented to compare the two approaches.},
  archive      = {J_SIM},
  author       = {Lin-An Chen and Chu-Lan Kao},
  doi          = {10.1002/sim.8895},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2155-2176},
  shortjournal = {Stat. Med.},
  title        = {Parametric and nonparametric improvements in bland and altman&#39;s assessment of agreement method},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multistate model incorporating estimation of excess
hazards and multiple time scales. <em>SIM</em>, <em>40</em>(9),
2139–2154. (<a href="https://doi.org/10.1002/sim.8894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cancer patient survival improves, late effects from treatment are becoming the next clinical challenge. Chemotherapy and radiotherapy, for example, potentially increase the risk of both morbidity and mortality from second malignancies and cardiovascular disease. To provide clinically relevant population-level measures of late effects, it is of importance to (1) simultaneously estimate the risks of both morbidity and mortality, (2) partition these risks into the component expected in the absence of cancer and the component due to the cancer and its treatment, and (3) incorporate the multiple time scales of attained age, calendar time, and time since diagnosis. Multistate models provide a framework for simultaneously studying morbidity and mortality, but do not solve the problem of partitioning the risks. However, this partitioning can be achieved by applying a relative survival framework, allowing us to directly quantify the excess risk. This article proposes a combination of these two frameworks, providing one approach to address (1) to (3). Using recently developed methods in multistate modeling, we incorporate estimation of excess hazards into a multistate model. Both intermediate and absorbing state risks can be partitioned and different transitions are allowed to have different and/or multiple time scales. We illustrate our approach using data on Hodgkin lymphoma patients and excess risk of diseases of the circulatory system, and provide user-friendly Stata software with accompanying example code.},
  archive      = {J_SIM},
  author       = {Caroline E. Weibull and Paul C. Lambert and Sandra Eloranta and Therese M. L. Andersson and Paul W. Dickman and Michael J. Crowther},
  doi          = {10.1002/sim.8894},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2139-2154},
  shortjournal = {Stat. Med.},
  title        = {A multistate model incorporating estimation of excess hazards and multiple time scales},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Change-point joint model for identification of plateau of
activity in early phase trials. <em>SIM</em>, <em>40</em>(9), 2113–2138.
(<a href="https://doi.org/10.1002/sim.8889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a phase I/II trial design for targeted therapies and immunotherapies, with the objective of identifying the optimal dose (OD). We employ a joint modeling technique for discrete time-to-event toxicity data and repeated and continuous biomarker measurements. For the biomarker measurements, we implement a change point linear mixed effects skeleton model. This model can accommodate both plateauing and nonplateauing dose-activity relationships. For each new cohort of patients, we estimate the maximum tolerated dose (MTD) taking toxicity as a cumulative endpoint, over six treatment cycles. Then, we select the OD using two different criteria. The OD is a dose that is equally active to the MTD or a dose located on the beginning of the plateau of the dose-activity relationship. Joint modeling allows us to take into account informative censoring due to toxicities or lack of activity and we also consider consent withdrawal and intermittent missing responses. Model estimation relies on likelihood inference.},
  archive      = {J_SIM},
  author       = {Maria-Athina Altzerinakou and Xavier Paoletti},
  doi          = {10.1002/sim.8889},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2113-2138},
  shortjournal = {Stat. Med.},
  title        = {Change-point joint model for identification of plateau of activity in early phase trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propensity score trimming mitigates bias due to covariate
measurement error in inverse probability of treatment weighted analyses:
A plasmode simulation. <em>SIM</em>, <em>40</em>(9), 2101–2112. (<a
href="https://doi.org/10.1002/sim.8887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Mitchell M. Conover and Kenneth J. Rothman and Til Stürmer and Alan R. Ellis and Charles Poole and Michele Jonsson Funk},
  doi          = {10.1002/sim.8887},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {2101-2112},
  shortjournal = {Stat. Med.},
  title        = {Propensity score trimming mitigates bias due to covariate measurement error in inverse probability of treatment weighted analyses: A plasmode simulation},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human disease clinical treatment network for the elderly:
The analysis of medicare inpatient length of stay data. <em>SIM</em>,
<em>40</em>(8), 2083–2099. (<a
href="https://doi.org/10.1002/sim.8893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease clinical treatment measures, such as inpatient length of stay (LOS), have been examined for most if not all diseases. Such analysis has important implications for the management and planning of health care, financial, and human resources. In addition, clinical treatment measures can also informatively reflect intrinsic disease properties such as severity. The existing studies mostly focus on either a single disease (or a few pre-selected and closely related diseases) or all diseases combined. In this study, we take a new and innovative perspective, examine the interconnections in length of stay (LOS) among diseases, and construct the very first disease clinical treatment network on LOS. To accommodate uniquely challenging data distributions, a new conditional network construction approach is developed. Based on the constructed network, the analysis of important network properties is conducted. The Medicare data on 100 000 randomly selected subjects for the period of January 2008 to December 2018 is analyzed. The network structure and key properties are found to have sensible biomedical interpretations. Being the very first of its kind, this study can be informative to disease clinical management, advance our understanding of disease interconnections, and foster complex network analysis.},
  archive      = {J_SIM},
  author       = {Hao Mei and Ruofan Jia and Guanzhong Qiao and Zhenqiu Lin and Shuangge Ma},
  doi          = {10.1002/sim.8893},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {2083-2099},
  shortjournal = {Stat. Med.},
  title        = {Human disease clinical treatment network for the elderly: The analysis of medicare inpatient length of stay data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistency of the CRM when the dose-toxicity curve is not
monotone and its application to the POCRM. <em>SIM</em>, <em>40</em>(8),
2073–2082. (<a href="https://doi.org/10.1002/sim.8892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continual reassessment method (CRM) is a well-known design for dose-finding trials with the goal of estimating the maximum tolerated dose (MTD), the dose with a given probability of toxicity. The standard assumption is that the probability of toxicity monotonically increases with dose. We show that the CRM can still be consistent and correctly identify the MTD even when the dose-toxicity curve is not monotone as long as there is monotonicity of the true toxicity probabilities right below and right above the true MTD. In the case of multiple therapies, where it is unclear how to order combinations of dose levels of multiple therapies, our findings provide insight into the performance of the partial order CRM (POCRM). To select the correct dose combination at the end of a trial, the POCRM does not have to select a monotone ordering of drug combinations. We illustrate the connection between our results for the CRM with a nonmonotone dose-toxicity curve and the POCRM via simulations.},
  archive      = {J_SIM},
  author       = {Pooja T. Saha and Jason P. Fine and Anastasia Ivanova},
  doi          = {10.1002/sim.8892},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {2073-2082},
  shortjournal = {Stat. Med.},
  title        = {Consistency of the CRM when the dose-toxicity curve is not monotone and its application to the POCRM},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point process models for sweat gland activation observed
with noise. <em>SIM</em>, <em>40</em>(8), 2055–2072. (<a
href="https://doi.org/10.1002/sim.8891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to construct spatial models for the activation of sweat glands for healthy subjects and subjects suffering from peripheral neuropathy by using videos of sweating recorded from the subjects. The sweat patterns are regarded as realizations of spatial point processes and two point process models for the sweat gland activation and two methods for inference are proposed. Several image analysis steps are needed to extract the point patterns from the videos and some incorrectly identified sweat gland locations may be present in the data. To take into account the errors, we either include an error term in the point process model or use an estimation procedure that is robust with respect to the errors.},
  archive      = {J_SIM},
  author       = {Mikko Kuronen and Mari Myllymäki and Adam Loavenbruck and Aila Särkkä},
  doi          = {10.1002/sim.8891},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {2055-2072},
  shortjournal = {Stat. Med.},
  title        = {Point process models for sweat gland activation observed with noise},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid design evaluating new biomarkers when there is an
existing screening test. <em>SIM</em>, <em>40</em>(8), 2037–2054. (<a
href="https://doi.org/10.1002/sim.8890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of cancer screening biomarkers usually follows the Early Detection Research Network 5-Phase guideline in Pepe et al. A key feature of this guide is that the phased development follows a sequential order, moving to the next phase only when the current phase study is complete and has met its target performance. Motivated by a newly funded Newly onset Diabetes cohort study, we propose a design evaluating new biomarkers to discriminate between cases and controls in the presence of an existing screening test. The proposed design achieves two goals: (1) avoiding bias in estimating sensitivity or specificity in predicting cancer at a given time period prior to clinical diagnosis, using data from both screening detected cancers in Phase IV study and clinically diagnosed cancers in Phase III study; and (2) building a panel with biomarkers for Phase III and IV studies based on all data. A simulation study shows that the proposed design outperforms both a conventional method using data in Phase III arm only and a naive method using data in Phase III and IV arms ignoring the difference between the time of screening the detected cancer and the time of clinical diagnosis. The proposed design yields a smaller standard error of the estimation and increases the statistical power to confirm biomarker performance. This proposed method has the potential to shorten the cancer screening biomarker development process, use resources more effectively, and bring benefits to patients quickly.},
  archive      = {J_SIM},
  author       = {Yeonhee Park and Ying Yuan and Jing Ning and Suyu Liu and Ziding Feng},
  doi          = {10.1002/sim.8890},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {2037-2054},
  shortjournal = {Stat. Med.},
  title        = {Hybrid design evaluating new biomarkers when there is an existing screening test},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Displaying survival of patient groups defined by covariate
paths: Extensions of the kaplan-meier estimator. <em>SIM</em>,
<em>40</em>(8), 2024–2036. (<a
href="https://doi.org/10.1002/sim.8888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensions of the Kaplan-Meier estimator have been developed to illustrate the relationship between a time-varying covariate of interest and survival. In particular, Snapinn et al and Xu et al developed estimators to display survival for patients who always have a certain value of a time-varying covariate. These estimators properly handle time-varying covariates, but their clinical interpretation is limited. It is of greater clinical interest to display survival for patients whose covariates lie along certain defined paths. In this article, we propose extensions of Snapinn et al and Xu et al&#39;s estimators, providing crude and covariate-adjusted estimates of the survival function for patients defined by covariate paths. We also derive analytical variance estimators. We demonstrate the utility of these estimators with medical examples and a simulation study.},
  archive      = {J_SIM},
  author       = {Melissa Jay and Rebecca A. Betensky},
  doi          = {10.1002/sim.8888},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {2024-2036},
  shortjournal = {Stat. Med.},
  title        = {Displaying survival of patient groups defined by covariate paths: Extensions of the kaplan-meier estimator},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic prediction of disease processes based on recurrent
history and functional principal component analysis of longitudinal
biomarkers: Application for ovarian epithelial cancer. <em>SIM</em>,
<em>40</em>(8), 2006–2023. (<a
href="https://doi.org/10.1002/sim.8885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ovarian epithelial cancer is a gynecological tumor with a high risk of recurrence and death. In the clinical diagnosis of ovarian epithelial cancer, CA125 has become an important indicator of disease burden. To account for patient recurrence and death, a proper method is needed to integrate information from biomarkers and recurrence simultaneously. In the past 10 years, many methods have been proposed for joint modeling of longitudinal biomarkers and survival data, but few of them are applicable to longitudinal data and disease processes, including recurrence and death. In this article, we proposed a new joint frailty model based on functional principal component analysis for dynamic prediction of survival probabilities on the total time scale, which took recurrent history and longitudinal data into account simultaneously. The estimation of the joint frailty model is achieved by maximizing the penalized log-likelihood function. The simulation results demonstrated the advantages of our method in both discrimination and accuracy under different scenarios. To indicate the method&#39;s practicality, it is applied to an actual dataset of patients with ovarian epithelial cancer to predict survival dynamically using longitudinal data of biomarker CA125 and recurrent history data.},
  archive      = {J_SIM},
  author       = {Yizhou Hong and Liwen Su and Siyi Song and Fangrong Yan},
  doi          = {10.1002/sim.8885},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {2006-2023},
  shortjournal = {Stat. Med.},
  title        = {Dynamic prediction of disease processes based on recurrent history and functional principal component analysis of longitudinal biomarkers: Application for ovarian epithelial cancer},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional adaptive bayesian spectral analysis of
replicated multivariate time series. <em>SIM</em>, <em>40</em>(8),
1989–2005. (<a href="https://doi.org/10.1002/sim.8884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a flexible nonparametric approach for analyzing the association between covariates and power spectra of multivariate time series observed across multiple subjects, which we refer to as multivariate conditional adaptive Bayesian power spectrum analysis (MultiCABS). The proposed procedure adaptively collects time series with similar covariate values into an unknown number of groups and nonparametrically estimates group-specific power spectra through penalized splines. A fully Bayesian framework is developed in which the number of groups and the covariate partition defining the groups are random and fit using Markov chain Monte Carlo techniques. MultiCABS offers accurate estimation and inference on power spectra of multivariate time series with both smooth and abrupt dynamics across covariate by averaging over the distribution of covariate partitions. Performance of the proposed method compared with existing methods is evaluated in simulation studies. The proposed methodology is used to analyze the association between fear of falling and power spectra of center-of-pressure trajectories of postural control while standing in people with Parkinson&#39;s disease.},
  archive      = {J_SIM},
  author       = {Zeda Li and Scott A. Bruce and Clinton J. Wutzke and Yang Long},
  doi          = {10.1002/sim.8884},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1989-2005},
  shortjournal = {Stat. Med.},
  title        = {Conditional adaptive bayesian spectral analysis of replicated multivariate time series},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The first-order markov conditional linear expectation
approach for analysis of longitudinal data. <em>SIM</em>,
<em>40</em>(8), 1972–1988. (<a
href="https://doi.org/10.1002/sim.8883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider longitudinal discrete data that may be unequally spaced in time and may exhibit overdispersion, so that the variance of the outcome variable is inflated relative to its assumed distribution. We implement an approach that extends generalized linear models for analysis of longitudinal data and is likelihood based, in contrast to generalized estimating equations (GEE) that are semiparametric. The method assumes independence between subjects; first-order antedependence within subjects; exponential family distributions for the first outcome on each subject and for the subsequent conditional distributions; and linearity of the expectations of the conditional distributions. We demonstrate application of the method in an analysis of seizure counts and in a study to evaluate the performance of transplant centers. Simulations for both studies demonstrate the benefits of the proposed likelihood based approach; however, they also demonstrate better than anticipated performance for GEE.},
  archive      = {J_SIM},
  author       = {Shaun Bender and Victoria Gamerman and Peter P. Reese and Daniel Lloyd Gray and Yimei Li and Justine Shults},
  doi          = {10.1002/sim.8883},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1972-1988},
  shortjournal = {Stat. Med.},
  title        = {The first-order markov conditional linear expectation approach for analysis of longitudinal data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Power and sample size for multistate model analysis of
longitudinal discrete outcomes in disease prevention trials.
<em>SIM</em>, <em>40</em>(8), 1960–1971. (<a
href="https://doi.org/10.1002/sim.8882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For clinical trials where participants pass through a number of discrete health states resulting in longitudinal measures over time, there are several potential primary estimands for the treatment effect. Incidence or time to a particular health state are commonly used outcomes but the choice of health state may not be obvious and these estimands do not make full use of the longitudinal assessments. Multistate models have been developed for some diseases and conditions with the purpose of understanding their natural history and have been used for secondary analysis to understand mechanisms of action of treatments. There is little published on the use of multistate models as the primary analysis method and potential implications on design features, such as assessment schedules. We illustrate methods via analysis of data from a motivating example; a Phase III clinical trial of pressure ulcer prevention strategies. We clarify some of the possible estimands that might be considered and we show, via a simulation study, that under some circumstances the sample size could be reduced by half using a multistate model based analysis, without adversely affecting the power of the trial.},
  archive      = {J_SIM},
  author       = {Isabelle L. Smith and Jane E. Nixon and Linda Sharples},
  doi          = {10.1002/sim.8882},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1960-1971},
  shortjournal = {Stat. Med.},
  title        = {Power and sample size for multistate model analysis of longitudinal discrete outcomes in disease prevention trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A powerful test for the maximum treatment effect in thorough
QT/QTc studies. <em>SIM</em>, <em>40</em>(8), 1947–1959. (<a
href="https://doi.org/10.1002/sim.8881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel-group thorough QT/QTc studies focus on the change of QT/QTc values at several time-matched points from a pretreatment day (baseline) to a posttreatment day for different groups of treatment. The International Council for Harmonisation E14 stresses that QTc prolongation beyond a threshold represents high cardiac risk and calls for a test on the largest time-matched treatment effect (QTc prolongation). QT/QTc analysis usually assumes a jointly multivariate normal (MVN) distribution of pretreatment and posttreatment QT/QTc values, with a blocked compound symmetry covariance matrix. Existing methods use an analysis of covariance (ANCOVA) model including day-averaged baseline as a covariate to deal with the MVN model. However, the ANCOVA model tends to underestimate the variation of the estimator for treatment effects, resulting in the inflation of empirical type I error rate when testing whether the largest QTc prolongation is beyond a threshold. In this article, we propose two new methods to estimate the time-matched treatment effects under the MVN model, including maximum likelihood estimation and ordinary-least-square-based two-stage estimation. These two methods take advantage of the covariance structure and are asymptotically efficient. Based on these estimators, powerful tests for QT/QTc prolongation are constructed. Simulation shows that the proposed estimators have smaller mean square error, and the tests can control the type I error rate with high power. The proposed methods are applied on testing the carryover effect of diltiazem to inhibit dofetilide in a randomized phase 1 trial.},
  archive      = {J_SIM},
  author       = {Yuhao Deng and Fangyi Chen and Yang Li and Kaihuan Qian and Rui Wang and Xiao-Hua Zhou},
  doi          = {10.1002/sim.8881},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1947-1959},
  shortjournal = {Stat. Med.},
  title        = {A powerful test for the maximum treatment effect in thorough QT/QTc studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning latent heterogeneity for type 2 diabetes patients
using longitudinal health markers in electronic health records.
<em>SIM</em>, <em>40</em>(8), 1930–1946. (<a
href="https://doi.org/10.1002/sim.8880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) from type 2 diabetes (T2D) patients consist of longitudinally and sparsely measured health markers at clinical encounters. Our goal is to use such data to learn latent patterns that can inform patient&#39;s health status related to T2D while accounting for challenges in retrospectively collected EHRs. To handle challenges such as correlated longitudinal measurements, irregular and informative encounter times, and mixed marker types, we propose multivariate generalized linear models to learn latent patient subgroups. In our model, covariate effects were time-dependent and latent Gaussian processes were introduced to model between-marker correlations over time. Using inferred latent processes, we integrated the irregularly measured health markers of mixed types into composite scores and applied hierarchical clustering to learn latent subgroup structures among T2D patients. Application to an EHR dataset of T2D patients showed different trends of age, sex, and race effects on hypertension/high blood pressure, total cholesterol, glycated hemoglobin, high-density lipoprotein, and medications. The associations among these markers varied over time during the study window. Clustering results revealed four subgroups, each with distinct health status. The same patterns were further confirmed using new EHR records of the same cohort. We developed a novel latent model to integrate longitudinal health markers in EHRs and characterize patient latent heterogeneities. Analysis indicated that there were distinct subgroups of T2D patients, suggesting that effective healthcare managements for these patients should be performed separately for each subgroup.},
  archive      = {J_SIM},
  author       = {Jitong Lou and Yuanjia Wang and Lang Li and Donglin Zeng},
  doi          = {10.1002/sim.8880},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1930-1946},
  shortjournal = {Stat. Med.},
  title        = {Learning latent heterogeneity for type 2 diabetes patients using longitudinal health markers in electronic health records},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple imputation strategies for a bounded outcome
variable in a competing risks analysis. <em>SIM</em>, <em>40</em>(8),
1917–1929. (<a href="https://doi.org/10.1002/sim.8879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In patient follow-up studies, events of interest may take place between periodic clinical assessments and so the exact time of onset is not observed. Such events are known as “bounded” or “interval-censored.” Methods for handling such events can be categorized as either (i) applying multiple imputation (MI) strategies or (ii) taking a full likelihood-based (LB) approach. We focused on MI strategies, rather than LB methods, because of their flexibility. We evaluated MI strategies for bounded event times in a competing risks analysis, examining the extent to which interval boundaries, features of the data distribution and substantive analysis model are accounted for in the imputation model. Candidate imputation models were predictive mean matching (PMM); log-normal regression with postimputation back-transformation; normal regression with and without restrictions on the imputed values and Delord and Genin&#39;s method based on sampling from the cumulative incidence function. We used a simulation study to compare MI methods and one LB method when data were missing at random and missing not at random, also varying the proportion of missing data, and then applied the methods to a hematopoietic stem cell transplantation dataset. We found that cumulative incidence and median event time estimation were sensitive to model misspecification. In a competing risks analysis, we found that it is more important to account for features of the data distribution than to restrict imputed values based on interval boundaries or to ensure compatibility with the substantive analysis by sampling from the cumulative incidence function. We recommend MI by type 1 PMM.},
  archive      = {J_SIM},
  author       = {Elinor Curnow and Rachael A. Hughes and Kate Birnie and Michael J. Crowther and Margaret T. May and Kate Tilling},
  doi          = {10.1002/sim.8879},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1917-1929},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation strategies for a bounded outcome variable in a competing risks analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capturing heterogeneity in repeated measures data by fusion
penalty. <em>SIM</em>, <em>40</em>(8), 1901–1916. (<a
href="https://doi.org/10.1002/sim.8878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we are interested in capturing heterogeneity in clustered or longitudinal data. Traditionally such heterogeneity is modeled by either fixed effects (FE) or random effects (RE). In FE models, the degree of freedom for the heterogeneity equals the number of clusters/subjects minus 1, which could result in less efficiency. In RE models, the heterogeneity across different clusters/subjects is described by, for example, a random intercept with 1 parameter (for the variance of the random intercept), which could lead to oversimplification and biases (for the estimates of subject-specific effects). Our “fused effects” model stands in between these two approaches: we assume that there are unknown number of distinct levels of heterogeneity, and use the fusion penalty approach for estimation and inference. We evaluate and compare the performance of our method to the FE and RE models by simulation studies. We apply our method to the Ocular Hypertension Treatment Study to capture the heterogeneity in the progression rate of primary open-angle glaucoma of left and right eyes of different subjects.},
  archive      = {J_SIM},
  author       = {Lili Liu and Mae Gordon and J. Philip Miller and Michael Kass and Lu Lin and Shujie Ma and Lei Liu},
  doi          = {10.1002/sim.8878},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1901-1916},
  shortjournal = {Stat. Med.},
  title        = {Capturing heterogeneity in repeated measures data by fusion penalty},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting rare haplotype association with two correlated
phenotypes of binary and continuous types. <em>SIM</em>, <em>40</em>(8),
1877–1900. (<a href="https://doi.org/10.1002/sim.8877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple correlated traits/phenotypes are often collected in genetic association studies and they may share a common genetic mechanism. Joint analysis of correlated phenotypes has well-known advantages over one-at-a-time analysis including gain in power and better understanding of genetic etiology. However, when the phenotypes are of discordant types such as binary and continuous, the joint modeling is more challenging. Another research area of current interest is discovery of rare genetic variants. Currently there is no method available for detecting association of rare (or common) haplotypes with multiple discordant phenotypes jointly. Our goal is to fill this gap specifically for two discordant phenotypes. We consider a rare haplotype association method for a binary phenotype, logistic Bayesian LASSO (univariate LBL) and its extension for two correlated binary phenotypes (bivariate LBL-2B). Under this framework, we propose a haplotype association test with binary and continuous phenotypes jointly (bivariate LBL-BC). Specifically, we use a latent variable to induce correlation between the two phenotypes. We carry out extensive simulations to investigate bivariate LBL-BC and compare it with univariate LBL and bivariate LBL-2B. In most settings, bivariate LBL-BC performs the best. In only two situations, bivariate LBL-BC has similar performance—when the two phenotypes are (1) weakly or not correlated and the target haplotype affects the binary phenotype only and (2) strongly positively correlated and the target haplotype affects both phenotypes in positive direction. Finally, we apply the method to a data set on lung cancer and nicotine dependence and detect several haplotypes including a rare one.},
  archive      = {J_SIM},
  author       = {Xiaochen Yuan and Swati Biswas},
  doi          = {10.1002/sim.8877},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1877-1900},
  shortjournal = {Stat. Med.},
  title        = {Detecting rare haplotype association with two correlated phenotypes of binary and continuous types},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-wave two-phase outcome-dependent sampling designs, with
applications to longitudinal binary data. <em>SIM</em>, <em>40</em>(8),
1863–1876. (<a href="https://doi.org/10.1002/sim.8876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase outcome-dependent sampling (ODS) designs are useful when resource constraints prohibit expensive exposure ascertainment on all study subjects. One class of ODS designs for longitudinal binary data stratifies subjects into three strata according to those who experience the event at none, some, or all follow-up times. For time-varying covariate effects, exclusively selecting subjects with response variation can yield highly efficient estimates. However, if interest lies in the association of a time-invariant covariate, or the joint associations of time-varying and time-invariant covariates with the outcome, then the optimal design is unknown. Therefore, we propose a class of two-wave two-phase ODS designs for longitudinal binary data. We split the second-phase sample selection into two waves, between which an interim design evaluation analysis is conducted. The interim design evaluation analysis uses first-wave data to conduct a simulation-based search for the optimal second-wave design that will improve the likelihood of study success. Although we focus on longitudinal binary response data, the proposed design is general and can be applied to other response distributions. We believe that the proposed designs can be useful in settings where (1) the expected second-phase sample size is fixed and one must tailor stratum-specific sampling probabilities to maximize estimation efficiency, or (2) relative sampling probabilities are fixed across sampling strata and one must tailor sample size to achieve a desired precision. We describe the class of designs, examine finite sampling operating characteristics, and apply the designs to an exemplar longitudinal cohort study, the Lung Health Study.},
  archive      = {J_SIM},
  author       = {Ran Tao and Nathaniel D. Mercaldo and Sebastien Haneuse and Jacob M. Maronge and Paul J. Rathouz and Patrick J. Heagerty and Jonathan S. Schildcrout},
  doi          = {10.1002/sim.8876},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1863-1876},
  shortjournal = {Stat. Med.},
  title        = {Two-wave two-phase outcome-dependent sampling designs, with applications to longitudinal binary data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Authors’ reply to sabour and ghajari “clinical prediction
models to predict the risk of multiple binary outcomes: Methodological
issues.” <em>SIM</em>, <em>40</em>(7), 1861–1862. (<a
href="https://doi.org/10.1002/sim.8872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Glen Philip Martin and Matthew Sperrin and Kym I. E. Snell and Iain Buchan and Richard D. Riley},
  doi          = {10.1002/sim.8872},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1861-1862},
  shortjournal = {Stat. Med.},
  title        = {Authors&#39; reply to sabour and ghajari “Clinical prediction models to predict the risk of multiple binary outcomes: Methodological issues”},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clinical prediction models to predict the risk of multiple
binary outcomes: Methodological issues. <em>SIM</em>, <em>40</em>(7),
1859–1860. (<a href="https://doi.org/10.1002/sim.8874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Siamak Sabour and Hadis Ghajari},
  doi          = {10.1002/sim.8874},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1859-1860},
  shortjournal = {Stat. Med.},
  title        = {Clinical prediction models to predict the risk of multiple binary outcomes: Methodological issues},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible multivariate joint model of longitudinal intensity
and binary process for medical monitoring of frequently collected data.
<em>SIM</em>, <em>40</em>(7), 1845–1858. (<a
href="https://doi.org/10.1002/sim.8875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A frequent problem in longitudinal studies is that data may be assessed at subject-selected, irregularly spaced time-points, resulting in highly unbalanced outcome data, inducing bias, especially if availability of data is directly related to outcome. Our aim was to develop a multivariate joint model in a mixed outcomes framework to minimize irregular sampling bias. We demonstrate using blood glucose monitoring throughout pregnancy and risk of preterm birth among women with type 1 diabetes mellitus. Blood glucose measurements were unequally spaced and intensity of sampling varied between and within individuals over time. Multivariate linear mixed effects submodel for the longitudinal outcome (blood glucose), Poisson model for the intensity of glucose sampling, and logistic regression model for binary process (preterm birth) were specified. Association between models is captured through shared random effects. Markov chain Monte Carlo methods were used to fit the model. The multivariate joint model provided better prediction, compared with a joint model with a multivariate linear mixed effects submodel (ignoring intensity of glucose sampling) and a two-stage model. Most association parameters were significant in the preterm birth outcome model, signifying improvement of predictive ability of the binary endpoint by sharing random effects between glucose monitoring and preterm birth. A simulation study is presented to illustrate the effectiveness of the multivariate joint modeling approach.},
  archive      = {J_SIM},
  author       = {Resmi Gupta and Jane C. Khoury and Mekibib Altaye and Roman Jandarov and Rhonda D. Szczesniak},
  doi          = {10.1002/sim.8875},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1845-1858},
  shortjournal = {Stat. Med.},
  title        = {Flexible multivariate joint model of longitudinal intensity and binary process for medical monitoring of frequently collected data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian-bandit adaptive design for n-of-1 clinical
trials. <em>SIM</em>, <em>40</em>(7), 1825–1844. (<a
href="https://doi.org/10.1002/sim.8873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N-of-1 trials, which are randomized, double-blinded, controlled, multiperiod, crossover trials on a single subject, have been applied to determine the heterogeneity of the individual&#39;s treatment effect in precision medicine settings. An aggregated N-of-1 design, which can estimate the population effect from these individual trials, is a pragmatic alternative when a randomized controlled trial (RCT) is infeasible. We propose a Bayesian adaptive design for both the individual and aggregated N-of-1 trials using a multiarmed bandit framework that is estimated via efficient Markov chain Monte Carlo. A Bayesian hierarchical structure is used to jointly model the individual and population treatment effects. Our proposed adaptive trial design is based on Thompson sampling, which randomly allocates individuals to treatments based on the Bayesian posterior probability of each treatment being optimal. While we use a subject-specific treatment effect and Bayesian posterior probability estimates to determine an individual&#39;s treatment allocation, our hierarchical model facilitates these individual estimates to borrow strength from the population estimates via shrinkage to the population mean. We present the design&#39;s operating characteristics and performance via a simulation study motivated by a recently completed N-of-1 clinical trial. We demonstrate that from a patient-centered perspective, subjects are likely to benefit from our adaptive design, in particular, for those individuals that deviate from the overall population effect.},
  archive      = {J_SIM},
  author       = {Sama Shrestha and Sonia Jain},
  doi          = {10.1002/sim.8873},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1825-1844},
  shortjournal = {Stat. Med.},
  title        = {A bayesian-bandit adaptive design for N-of-1 clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cause-specific quantile regression on inactivity time.
<em>SIM</em>, <em>40</em>(7), 1811–1824. (<a
href="https://doi.org/10.1002/sim.8871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In time-to-event analysis, the traditional summary measures have been based on the hazard function, survival function, quantile event time, restricted mean event time, and residual lifetime. Under competing risks, furthermore, typical summary measures have been the cause-specific hazard function and cumulative incidence function. Recently inactivity time has recaptured attention in the literature, being interpreted as life lost. In this paper, we further interpret it as quality of life reduced and time period after transition to a drug, and propose a quantile regression model to associate the inactivity time with potential predictors under competing risks. We define the proper cumulative distribution function of the inactivity time distribution for each specific event type among those subjects who experience the same type of events during a follow-up period. A score function-type estimating equation is developed and asymptotic properties of the regression coefficient estimators are derived by assuming that competing events are censored at their occurrence times as in the cause-specific hazard analysis. The proposed approach reduces to a regular quantile regression on the inactivity time without competing risks when all types of competing events are collapsed into the same type. Due to difficulty in estimating the improper probability density function of the cause-specific inactivity distribution to evaluate the variance of the quantiles, a computationally efficient perturbation method is adopted to infer the regression coefficients. Simulation results show that our proposed method works well under the assumed finite sample settings. The proposed method is illustrated with a real dataset from a breast cancer study.},
  archive      = {J_SIM},
  author       = {Yichen Jia and Jong-Hyeon Jeong},
  doi          = {10.1002/sim.8871},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1811-1824},
  shortjournal = {Stat. Med.},
  title        = {Cause-specific quantile regression on inactivity time},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scale mixture of skew-normal linear mixed models with
within-subject serial dependence. <em>SIM</em>, <em>40</em>(7),
1790–1810. (<a href="https://doi.org/10.1002/sim.8870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal studies, repeated measures are collected over time and hence they tend to be serially correlated. These studies are commonly analyzed using linear mixed models (LMMs), and in this article we consider an extension of the skew-normal/independent LMM, where the error term has a dependence structure, such as damped exponential correlation or autoregressive correlation of order p . The proposed model provides flexibility in capturing the effects of skewness and heavy tails simultaneously when continuous repeated measures are serially correlated. For this robust model, we present an efficient EM-type algorithm for parameters estimation via maximum likelihood and the observed information matrix is derived analytically to account for standard errors. The methodology is illustrated through an application to schizophrenia data and some simulation studies. The proposed algorithm and methods are implemented in the new R package skewlmm .},
  archive      = {J_SIM},
  author       = {Fernanda L. Schumacher and Victor H. Lachos and Larissa A. Matos},
  doi          = {10.1002/sim.8870},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1790-1810},
  shortjournal = {Stat. Med.},
  title        = {Scale mixture of skew-normal linear mixed models with within-subject serial dependence},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The length of the receiver operating characteristic curve
and the two cutoff youden index within a robust framework for discovery,
evaluation, and cutoff estimation in biomarker studies involving
improper receiver operating characteristic curves. <em>SIM</em>,
<em>40</em>(7), 1767–1789. (<a
href="https://doi.org/10.1002/sim.8869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the early stage of biomarker discovery, high throughput technologies allow for simultaneous input of thousands of biomarkers that attempt to discriminate between healthy and diseased subjects. In such cases, proper ranking of biomarkers is highly important. Common measures, such as the area under the receiver operating characteristic (ROC) curve (AUC), as well as affordable sensitivity and specificity levels, are often taken into consideration. Strictly speaking, such measures are appropriate under a stochastic ordering assumption, which implies, without loss of generality, that higher measurements are more indicative for the disease. Such an assumption is not always plausible and may lead to rejection of extremely useful biomarkers at this early discovery stage. We explore the length of a smooth ROC curve as a measure for biomarker ranking, which is not subject to directionality. We show that the length corresponds to a divergence, is identical to the corresponding length of the optimal (likelihood ratio) ROC curve, and is an appropriate measure for ranking biomarkers. We explore the relationship between the length measure and the AUC of the optimal ROC curve. We then provide a complete framework for the evaluation of a biomarker in terms of sensitivity and specificity through a proposed ROC analogue for use in improper settings. In the absence of any clinical insight regarding the appropriate cutoffs, we estimate the sensitivity and specificity under a two-cutoff extension of the Youden index and we further take into account the implied costs. We apply our approaches on two biomarker studies that relate to pancreatic and esophageal cancer.},
  archive      = {J_SIM},
  author       = {Leonidas E. Bantis and John V. Tsimikas and Gregory R. Chambers and Michela Capello and Samir Hanash and Ziding Feng},
  doi          = {10.1002/sim.8869},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1767-1789},
  shortjournal = {Stat. Med.},
  title        = {The length of the receiver operating characteristic curve and the two cutoff youden index within a robust framework for discovery, evaluation, and cutoff estimation in biomarker studies involving improper receiver operating characteristic curves},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The optimal design of clinical trials with potential
biomarker effects: A novel computational approach. <em>SIM</em>,
<em>40</em>(7), 1752–1766. (<a
href="https://doi.org/10.1002/sim.8868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a future trend of healthcare, personalized medicine tailors medical treatments to individual patients. It requires to identify a subset of patients with the best response to treatment. The subset can be defined by a biomarker (eg, expression of a gene) and its cutoff value. Topics on subset identification have received massive attention. There are over two million hits by keyword searches on Google Scholar. However, designing clinical trials that utilize the discovered uncertain subsets/biomarkers is not trivial and rarely discussed in the literature. This leads to a gap between research results and real-world drug development. To fill in this gap, we formulate the problem of clinical trial design into an optimization problem involving high-dimensional integration, and propose a novel computational solution based on Monte Carlo and smoothing methods. Our method utilizes the modern techniques of general purpose computing on graphics processing units for large-scale parallel computing. Compared to a published method in three-dimensional problems, our approach is more accurate and 133 times faster. This advantage increases when dimensionality increases. Our method is scalable to higher dimensional problems since the precision bound of our estimated study power is a finite number not affected by dimensionality. To design clinical trials incorporating the potential biomarkers, users can use our software &quot;DesignCTPB&quot;. This software can be found on Github and will be available as an R package on CRAN. Although our research is motivated by the design of clinical trials, the method can be used widely to solve other optimization problems involving high-dimensional integration.},
  archive      = {J_SIM},
  author       = {Yitao Lu and Julie Zhou and Li Xing and Xuekui Zhang},
  doi          = {10.1002/sim.8868},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1752-1766},
  shortjournal = {Stat. Med.},
  title        = {The optimal design of clinical trials with potential biomarker effects: A novel computational approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information content of stepped wedge designs with unequal
cluster-period sizes in linear mixed models: Informing incomplete
designs. <em>SIM</em>, <em>40</em>(7), 1736–1751. (<a
href="https://doi.org/10.1002/sim.8867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, stepped wedge trials frequently include clusters of differing sizes. However, investigations into the theoretical aspects of stepped wedge designs have, until recently, typically assumed equal numbers of subjects in each cluster and in each period. The information content of the cluster-period cells, clusters, and periods of stepped wedge designs has previously been investigated assuming equal cluster-period sizes, and has shown that incomplete stepped wedge designs may be efficient alternatives to the full stepped wedge. How this changes when cluster-period sizes are not equal is unknown, and we investigate this here. Working within the linear mixed model framework, we show that the information contributed by design components (clusters, sequences, and periods) does depend on the sizes of each cluster-period. Using a particular trial that assessed the impact of an individual education intervention on log-length of stay in rehabilitation units, we demonstrate how strongly the efficiency of incomplete designs depends on which cells are excluded: smaller incomplete designs may be more powerful than alternative incomplete designs that include a greater total number of participants. This also serves to demonstrate how the pattern of information content can be used to inform a set of incomplete designs to be considered as alternatives to the complete stepped wedge design. Our theoretical results for the information content can be extended to a broad class of longitudinal (ie, multiple period) cluster randomized trial designs.},
  archive      = {J_SIM},
  author       = {Jessica Kasza and Rhys Bowden and Andrew B. Forbes},
  doi          = {10.1002/sim.8867},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1736-1751},
  shortjournal = {Stat. Med.},
  title        = {Information content of stepped wedge designs with unequal cluster-period sizes in linear mixed models: Informing incomplete designs},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using propensity scores to estimate effects of treatment
initiation decisions: State of the science. <em>SIM</em>,
<em>40</em>(7), 1718–1735. (<a
href="https://doi.org/10.1002/sim.8866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confounding can cause substantial bias in nonexperimental studies that aim to estimate causal effects. Propensity score methods allow researchers to reduce bias from measured confounding by summarizing the distributions of many measured confounders in a single score based on the probability of receiving treatment. This score can then be used to mitigate imbalances in the distributions of these measured confounders between those who received the treatment of interest and those in the comparator population, resulting in less biased treatment effect estimates. This methodology was formalized by Rosenbaum and Rubin in 1983 and, since then, has been used increasingly often across a wide variety of scientific disciplines. In this review article, we provide an overview of propensity scores in the context of real-world evidence generation with a focus on their use in the setting of single treatment decisions, that is, choosing between two therapeutic options. We describe five aspects of propensity score analysis: alignment with the potential outcomes framework, implications for study design, estimation procedures, implementation options, and reporting. We add context to these concepts by highlighting how the types of comparator used, the implementation method, and balance assessment techniques have changed over time. Finally, we discuss evolving applications of propensity scores.},
  archive      = {J_SIM},
  author       = {Michael Webster-Clark and Til Stürmer and Tiansheng Wang and Kenneth Man and Danica Marinac-Dabic and Kenneth J. Rothman and Alan R. Ellis and Mugdha Gokhale and Mark Lunt and Cynthia Girman and Robert J. Glynn},
  doi          = {10.1002/sim.8866},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1718-1735},
  shortjournal = {Stat. Med.},
  title        = {Using propensity scores to estimate effects of treatment initiation decisions: State of the science},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending the mann-whitney-wilcoxon rank sum test to survey
data for comparing mean ranks. <em>SIM</em>, <em>40</em>(7), 1705–1717.
(<a href="https://doi.org/10.1002/sim.8865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods for analysis of survey data have been developed to facilitate research. More recently, Lumley and Scott (2013) developed an approach to extend the Mann-Whitney-Wilcoxon (MWW) rank sum test to survey data. Their approach focuses on the null of equal distribution. In many studies, the MWW test is called for when two-sample t-tests (with or without equal variance assumed) fail to provide meaningful results, as they are highly sensitive to outliers. In such situations, the null of equal distribution is too restrictive, as interest lies in comparing centers of groups. In this article, we develop an approach to extend the MWW test to survey data to test the null of equal mean rank. Although not as popular as the mean and median, the mean rank is also a meaningful measure of the center of a distribution and is the same as the median for a symmetric distribution. We illustrate the proposed approach and show major differences with Lumley and Scott&#39;s alternative using both real and simulated data.},
  archive      = {J_SIM},
  author       = {Tuo Lin and Tian Chen and Jinyuan Liu and Xin M. Tu},
  doi          = {10.1002/sim.8865},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1705-1717},
  shortjournal = {Stat. Med.},
  title        = {Extending the mann-whitney-wilcoxon rank sum test to survey data for comparing mean ranks},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial modeling of individual-level infectious disease
transmission: Tuberculosis data in manitoba, canada. <em>SIM</em>,
<em>40</em>(7), 1678–1704. (<a
href="https://doi.org/10.1002/sim.8863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically dependent individual level models (GD-ILMs) are a class of statistical models that can be used to study the spread of infectious disease through a population in discrete-time in which covariates can be measured both at individual and area levels. The typical ILMs to illustrate spatial data are based on the distance between susceptible and infectious individuals. A key feature of GD-ILMs is that they take into account the spatial location of the individuals in addition to the distance between susceptible and infectious individuals. As a motivation of this article, we consider tuberculosis (TB) data which is an infectious disease which can be transmitted through individuals. It is also known that certain areas/demographics/communities have higher prevalent of TB (see Section 4 for more details). It is also of interest of policy makers to identify those areas with higher infectivity rate of TB for possible preventions. Therefore, we need to analyze this data properly to address those concerns. In this article, the expectation conditional maximization algorithm is proposed for estimating the parameters of GD-ILMs to be able to predict the areas with the highest average infectivity rates of TB. We also evaluate the performance of our proposed approach through some simulations. Our simulation results indicate that the proposed method provides reliable estimates of parameters which confirms accuracy of the infectivity rates.},
  archive      = {J_SIM},
  author       = {Leila Amiri and Mahmoud Torabi and Rob Deardon and Michael Pickles},
  doi          = {10.1002/sim.8863},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1678-1704},
  shortjournal = {Stat. Med.},
  title        = {Spatial modeling of individual-level infectious disease transmission: Tuberculosis data in manitoba, canada},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of parametric propensity score-based methods
for causal inference with multiple treatments and a binary outcome.
<em>SIM</em>, <em>40</em>(7), 1653–1677. (<a
href="https://doi.org/10.1002/sim.8862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider comparative effectiveness research (CER) from observational data with two or more treatments. In observational studies, the estimation of causal effects is prone to bias due to confounders related to both treatment and outcome. Methods based on propensity scores are routinely used to correct for such confounding biases. A large fraction of propensity score methods in the current literature consider the case of either two treatments or continuous outcome. There has been extensive literature with multiple treatment and binary outcome, but interest often lies in the intersection, for which the literature is still evolving. The contribution of this article is to focus on this intersection and compare across methods, some of which are fairly recent. We describe propensity-based methods when more than two treatments are being compared, and the outcome is binary. We assess the relative performance of these methods through a set of simulation studies. The methods are applied to assess the effect of four common therapies for castration-resistant advanced-stage prostate cancer. The data consist of medical and pharmacy claims from a large national private health insurance network, with the adverse outcome being admission to the emergency room within a short time window of treatment initiation.},
  archive      = {J_SIM},
  author       = {Youfei Yu and Min Zhang and Xu Shi and Megan E. V. Caram and Roderick J. A. Little and Bhramar Mukherjee},
  doi          = {10.1002/sim.8862},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1653-1677},
  shortjournal = {Stat. Med.},
  title        = {A comparison of parametric propensity score-based methods for causal inference with multiple treatments and a binary outcome},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A depth-based global envelope test for comparing two groups
of functions with applications to biomedical data. <em>SIM</em>,
<em>40</em>(7), 1639–1652. (<a
href="https://doi.org/10.1002/sim.8861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data are commonly observed in many emerging biomedical fields and their analysis is an exciting developing area in statistics. Numerous statistical methods, such as principal components, analysis of variance, and linear regression, have been extended to functional data. The statistical analysis of functions can be significantly improved using nonparametric and robust estimators. New ideas of depth for functional data have been proposed in recent years and can be extended to image data. They provide a way of ordering curves or images from center-outward, and of defining robust order statistics in a functional context. In this paper we develop depth-based global envelope tests for comparing two groups of functions or images. In addition to providing global P -values, the proposed envelope test can be displayed graphically and indicates the specific portion(s) of the functional data (eg, in pixels or in time) that may have led to rejection of the null hypothesis. We show in a simulation study the performance of the envelope test in terms of empirical power and size in different scenarios. The proposed depth-based global approach has good power even for small differences and is robust to outliers. The methodology introduced is applied to test whether children with normal and low birth weight have similar growth pattern. We also analyzed a brain image dataset consisting of positron emission tomography scans of severe depressed patients and healthy controls. The global envelope test was used to find and visualize differences between the two groups.},
  archive      = {J_SIM},
  author       = {Sara Lopez-Pintado and Kun Qian},
  doi          = {10.1002/sim.8861},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1639-1652},
  shortjournal = {Stat. Med.},
  title        = {A depth-based global envelope test for comparing two groups of functions with applications to biomedical data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Space-time modeling of child mortality at the admin-2 level
in a low and middle income countries context. <em>SIM</em>,
<em>40</em>(7), 1593–1638. (<a
href="https://doi.org/10.1002/sim.8854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sustainable Development Goals call for a total reduction of preventable child mortality before 2030. Further, the goals state the desirability to have subnational mortality estimates. Estimates at this level are required for health interventions at the subnational level. In a low and middle income countries context, the data on mortality typically consist of household surveys, which are carried out with a stratified, cluster design, and census microsamples. Most household surveys collect full birth history (FBH) data on birth and death dates of a mother&#39;s children, but censuses collect summary birth history (SBH) data which consist only of the number of children born and the number that died. In previous work, direct (survey-weighted) estimates with associated variances were derived from FBH data and smoothed in space and time. Unfortunately, the FBH data from household surveys are usually not sufficiently abundant to obtain yearly estimates at the Admin-2 level (at which interventions are often made). In this paper we describe four extensions to previous work: (i) combining SBH data with FBH data, (ii) modeling on a yearly scale, to combine data on a yearly scale with data at coarser time scales, (iii) adjusting direct estimates in Admin-2 areas where we do not observe any deaths due to small sample sizes, (iv) acknowledge differences in data sources by modeling potential bias arising from the various data sources. The methods are illustrated using household survey and census data from Kenya and Malawi, to produce mortality estimates from 1980 to the time of the most recent survey, and predictions to 2020.},
  archive      = {J_SIM},
  author       = {Jessica Godwin and Jon Wakefield},
  doi          = {10.1002/sim.8854},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1593-1638},
  shortjournal = {Stat. Med.},
  title        = {Space-time modeling of child mortality at the admin-2 level in a low and middle income countries context},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An alternative formulation of coxian phase-type
distributions with covariates: Application to emergency department
length of stay. <em>SIM</em>, <em>40</em>(6), 1574–1592. (<a
href="https://doi.org/10.1002/sim.8860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a new methodology to model patient transitions and length of stay in the emergency department using a series of conditional Coxian phase-type distributions, with covariates. We reformulate the Coxian models (standard Coxian, Coxian with multiple absorbing states, joint Coxian, and conditional Coxian) to take into account heterogeneity in patient characteristics such as arrival mode, time of admission, and age. The approach differs from previous research in that it reduces the computational time, and it allows the inclusion of patient covariate information directly into the model. The model is applied to emergency department data from University Hospital Limerick in Ireland, where we find broad agreement with a number of commonly used survival models (parametric Weibull and log-normal regression models and the semiparametric Cox proportional hazards model).},
  archive      = {J_SIM},
  author       = {Jean Rizk and Cathal Walsh and Kevin Burke},
  doi          = {10.1002/sim.8860},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1574-1592},
  shortjournal = {Stat. Med.},
  title        = {An alternative formulation of coxian phase-type distributions with covariates: Application to emergency department length of stay},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing methods for estimating patient-specific treatment
effects in individual patient data meta-analysis. <em>SIM</em>,
<em>40</em>(6), 1553–1573. (<a
href="https://doi.org/10.1002/sim.8859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis of individual patient data (IPD) is increasingly used to synthesize data from multiple trials. IPD meta-analysis offers several advantages over meta-analyzing aggregate data, including the capacity to individualize treatment recommendations. Trials usually collect information on many patient characteristics. Some of these covariates may strongly interact with treatment (and thus be associated with treatment effect modification) while others may have little effect. It is currently unclear whether a systematic approach to the selection of treatment-covariate interactions in an IPD meta-analysis can lead to better estimates of patient-specific treatment effects. We aimed to answer this question by comparing in simulations the standard approach to IPD meta-analysis (no variable selection, all treatment-covariate interactions included in the model) with six alternative methods: stepwise regression, and five regression methods that perform shrinkage on treatment-covariate interactions, that is, least absolute shrinkage and selection operator (LASSO), ridge, adaptive LASSO, Bayesian LASSO, and stochastic search variable selection. Exploring a range of scenarios, we found that shrinkage methods performed well for both continuous and dichotomous outcomes, for a variety of settings. In most scenarios, these methods gave lower mean squared error of the patient-specific treatment effect as compared with the standard approach and stepwise regression. We illustrate the application of these methods in two datasets from cardiology and psychiatry. We recommend that future IPD meta-analysis that aim to estimate patient-specific treatment effects using multiple effect modifiers should use shrinkage methods, whereas stepwise regression should be avoided.},
  archive      = {J_SIM},
  author       = {Michael Seo and Ian R. White and Toshi A. Furukawa and Hissei Imai and Marco Valgimigli and Matthias Egger and Marcel Zwahlen and Orestis Efthimiou},
  doi          = {10.1002/sim.8859},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1553-1573},
  shortjournal = {Stat. Med.},
  title        = {Comparing methods for estimating patient-specific treatment effects in individual patient data meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the properties of the toxicity index and its statistical
efficiency. <em>SIM</em>, <em>40</em>(6), 1535–1552. (<a
href="https://doi.org/10.1002/sim.8858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer clinical trials typically generate detailed patient toxicity data. The most common measure used to summarize patient toxicity is the maximum grade among all toxicities and it does not fully represent the toxicity burden experienced by patients. In this article, we study the mathematical and statistical properties of the toxicity index (TI), in an effort to address this deficiency. We introduce a total ordering, (T-rank), that allows us to fully rank the patients according to how frequently they exhibit toxicities, and show that TI is the only measure that preserves the T-rank among its competitors. Moreover, we propose a Poisson-Limit model for sparse toxicity data. Under this model, we develop a general two-sample test, which can be applied to any summary measure for detecting differences among two population of toxicity data. We derive the asymptotic power function of this class as well as the asymptotic relative efficiency (ARE) of the members of the class. We evaluate the ARE formula empirically and show that if the data are drawn from a random Poisson-Limit model, the TI is more efficient, with high probability, than the maximum and the average summary measures. Finally, we evaluate our method on clinical trial toxicity data and show that TI has a higher power in detecting the differences in toxicity profile among treatments. The results of this article can be applied beyond toxicity modeling, to any problem where one observes a sparse array of scores on subjects and a ranking based on extreme scores is desirable.},
  archive      = {J_SIM},
  author       = {Zahra S. Razaee and Arash A. Amini and Márcio A. Diniz and Mourad Tighiouart and Greg Yothers and André Rogatko},
  doi          = {10.1002/sim.8858},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1535-1552},
  shortjournal = {Stat. Med.},
  title        = {On the properties of the toxicity index and its statistical efficiency},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Link predictions for incomplete network data with outcome
misclassification. <em>SIM</em>, <em>40</em>(6), 1519–1534. (<a
href="https://doi.org/10.1002/sim.8856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction is a fundamental problem in network analysis. In a complex network, links can be unreported and/or under detection limits due to heterogeneous sources of noise and technical challenges during data collection. The incomplete network data can lead to an inaccurate inference of network based data analysis. We propose a parametric link prediction model and consider latent links as misclassified binary outcomes. We develop new algorithms to optimize model parameters and yield robust predictions of unobserved links. Theoretical properties of the predictive model are also discussed. We apply the new method to a partially observed social network data and incomplete brain network data. The results demonstrate that our method outperforms the existing latent-link prediction methods.},
  archive      = {J_SIM},
  author       = {Qiong Wu and Zhen Zhang and Tianzhou Ma and James Waltz and Donald Milton and Shuo Chen},
  doi          = {10.1002/sim.8856},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1519-1534},
  shortjournal = {Stat. Med.},
  title        = {Link predictions for incomplete network data with outcome misclassification},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian meta-analysis models for cross cancer genomic
investigation of pleiotropic effects using group structure.
<em>SIM</em>, <em>40</em>(6), 1498–1518. (<a
href="https://doi.org/10.1002/sim.8855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of genome-wide association studies (GWAS) summary statistics is made available to the scientific community. Exploiting these results from multiple phenotypes would permit identification of novel pleiotropic associations. In addition, incorporating prior biological information in GWAS such as group structure information (gene or pathway) has shown some success in classical GWAS approaches. However, this has not been widely explored in the context of pleiotropy. We propose a Bayesian meta-analysis approach (termed GCPBayes) that uses summary-level GWAS data across multiple phenotypes to detect pleiotropy at both group-level (gene or pathway) and within group (eg, at the SNP level). We consider both continuous and Dirac spike and slab priors for group selection. We also use a Bayesian sparse group selection approach with hierarchical spike and slab priors that enables us to select important variables both at the group level and within group. GCPBayes uses a Bayesian statistical framework based on Markov chain Monte Carlo (MCMC) Gibbs sampling. It can be applied to multiple types of phenotypes for studies with overlapping or nonoverlapping subjects, and takes into account heterogeneity in the effect size and allows for the opposite direction of the genetic effects across traits. Simulations show that the proposed methods outperform benchmark approaches such as ASSET and CPBayes in the ability to retrieve pleiotropic associations at both SNP and gene-levels. To illustrate the GCPBayes method, we investigate the shared genetic effects between thyroid cancer and breast cancer in candidate pathways.},
  archive      = {J_SIM},
  author       = {Taban Baghfalaki and Pierre-Emmanuel Sugier and Therese Truong and Anthony N. Pettitt and Kerrie Mengersen and Benoit Liquet},
  doi          = {10.1002/sim.8855},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1498-1518},
  shortjournal = {Stat. Med.},
  title        = {Bayesian meta-analysis models for cross cancer genomic investigation of pleiotropic effects using group structure},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model diagnostics for censored regression via randomized
survival probabilities. <em>SIM</em>, <em>40</em>(6), 1482–1497. (<a
href="https://doi.org/10.1002/sim.8852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residuals in normal regression are used to assess a model&#39;s goodness-of-fit (GOF) and discover directions for improving the model. However, there is a lack of residuals with a characterized reference distribution for censored regression. In this article, we propose to diagnose censored regression with normalized randomized survival probabilities (RSP). The key idea of RSP is to replace the survival probability (SP) of a censored failure time with a uniform random number between 0 and the SP of the censored time. We prove that RSPs always have the uniform distribution on (0, 1) under the true model with the true generating parameters. Therefore, we can transform RSPs into normally distributed residuals with the normal quantile function. We call such residuals by normalized RSP (NRSP residuals). We conduct simulation studies to investigate the sizes and powers of statistical tests based on NRSP residuals in detecting the incorrect choice of distribution family and nonlinear effect in covariates. Our simulation studies show that, although the GOF tests with NRSP residuals are not as powerful as a traditional GOF test method, a nonlinear test based on NRSP residuals has significantly higher power in detecting nonlinearity. We also compared these model diagnostics methods with a breast-cancer recurrent-free time dataset. The results show that the NRSP residual diagnostics successfully captures a subtle nonlinear relationship in the dataset, which is not detected by the graphical diagnostics with CS residuals and existing GOF tests.},
  archive      = {J_SIM},
  author       = {Longhai Li and Tingxuan Wu and Cindy Feng},
  doi          = {10.1002/sim.8852},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1482-1497},
  shortjournal = {Stat. Med.},
  title        = {Model diagnostics for censored regression via randomized survival probabilities},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian penalized cumulative logit model for
high-dimensional data with an ordinal response. <em>SIM</em>,
<em>40</em>(6), 1453–1481. (<a
href="https://doi.org/10.1002/sim.8851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many previous studies have identified associations between gene expression, measured using high-throughput genomic platforms, and quantitative or dichotomous traits. However, we note that health outcome and disease status measurements frequently appear on an ordinal scale, that is, the outcome is categorical but has inherent ordering. Identification of important genes may be useful for developing novel diagnostic and prognostic tools to predict or classify stage of disease. Gene expression data are usually high-dimensional, meaning that the number of genes is much larger than the sample size or number of patients. Herein we describe some existing frequentist methods for modeling an ordinal response in a high-dimensional predictor space. Following Tibshirani (1996), who described the LASSO estimate as the Bayesian posterior mode when the regression coefficients have independent Laplace priors, we propose a new approach for high-dimensional data with an ordinal response that is rooted in the Bayesian paradigm. We show that our proposed Bayesian approach outperforms existing frequentist methods through simulation studies. We then compare the performance of frequentist and Bayesian approaches using a study evaluating progression to hepatocellular carcinoma in hepatitis C infected patients.},
  archive      = {J_SIM},
  author       = {Yiran Zhang and Kellie J. Archer},
  doi          = {10.1002/sim.8851},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1453-1481},
  shortjournal = {Stat. Med.},
  title        = {Bayesian penalized cumulative logit model for high-dimensional data with an ordinal response},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Longitudinal multivariate normative comparisons.
<em>SIM</em>, <em>40</em>(6), 1440–1452. (<a
href="https://doi.org/10.1002/sim.8850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the Multicenter AIDS Cohort Study (MACS), we develop classification procedures for cognitive impairment based on longitudinal measures. To control family-wise error, we adapt the cross-sectional multivariate normative comparisons (MNC) method to the longitudinal setting. The cross-sectional MNC was proposed to control family-wise error by measuring the distance between multiple domain scores of a participant and the norms of healthy controls and specifically accounting for intercorrelations among all domain scores. However, in a longitudinal setting where domain scores are recorded multiple times, applying the cross-sectional MNC at each visit will still have inflated family-wise error rate due to multiple testing over repeated visits. Thus, we propose longitudinal MNC procedures that are constructed based on multivariate mixed effects models. A test procedure is adapted from the cross-sectional MNC to classify impairment on longitudinal multivariate normal data. Meanwhile, a permutation procedure is proposed to handle skewed data. Through simulations we show that our methods can effectively control family-wise error at a predetermined level. A dataset from a neuropsychological substudy of the MACS is used to illustrate the applications of our proposed classification procedures.},
  archive      = {J_SIM},
  author       = {Zheng Wang and Yu Cheng and Eric C. Seaberg and Leah H. Rubin and Andrew J. Levine and James T. Becker},
  doi          = {10.1002/sim.8850},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1440-1452},
  shortjournal = {Stat. Med.},
  title        = {Longitudinal multivariate normative comparisons},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling the mean time to interval cancer after negative
results of periodic cancer screening. <em>SIM</em>, <em>40</em>(6),
1429–1439. (<a href="https://doi.org/10.1002/sim.8849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval cancers are cancers detected symptomatically between screens or after the last screen. A mathematical model for the development of interval cancers can provide useful information for evaluating cancer screening. In this regard a useful quantity is MIC, the mean duration in years of progressive preclinical cancer (PPC) that leads to interval cancers. Estimation of MIC involved extending a previous model to include three negative screens, invoking the multinomial-Poisson transformation to avoid estimating background cancer trends, and varying screening test sensitivity. Simulations show that when the true MIC is 0.5, the method yields a reasonably narrow range of estimated MICs over the range of screening test sensitivities from 0.5 to 1.0. If the lower bound on the screening test sensitivity is 0.7, the method performs considerably better even for larger MICs. The application of the method involved annual lung cancer screening in the Prostate, Lung, Colorectal, and Ovarian trial. Assuming a normal distribution for PPC duration, the estimated MIC (95% confidence interval) ranged from 0.00 (0.00 to 0.34) at a screening test sensitivity of 1.0 to 0.54 (0.03, 1.00) at a screening test sensitivity of 0.5 Assuming an exponential distribution for PPC duration, which did not fit as well, the estimated MIC ranged from 0.27 (0.08, 0.49) at a screening test sensitivity of 0.5 to 0.73 (0.32, 1.26) at a screen test sensitivity of 1.0 Based on these results, investigators may wish to investigate more frequent lung cancer screening.},
  archive      = {J_SIM},
  author       = {Stuart G. Baker},
  doi          = {10.1002/sim.8849},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1429-1439},
  shortjournal = {Stat. Med.},
  title        = {Modeling the mean time to interval cancer after negative results of periodic cancer screening},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mean comparisons and power calculations to ensure
reproducibility in preclinical drug discovery. <em>SIM</em>,
<em>40</em>(6), 1414–1428. (<a
href="https://doi.org/10.1002/sim.8848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pharmaceutical industry, in vivo animal experiments are conducted to test the effects of novel preclinical drug compounds. Well-planned animal studies involve a sample size and statistical power analysis to provide a basis for the number of animals allocated into comparator arms of a future study. These calculations require approximate values for the parameters of a statistical model that will be applied to the future data and used to test for differences via statistical hypotheses. If the prestudy parameter estimates are nearly correct, the power analysis guarantees that a difference will be detected from the study data, up to a prespecified probability. Traditional power computations, however, are not calculated with reproducibility in mind. In this work, the issue of reproducibility in drug discovery is tackled from the point of view that study-to-study variability is not included in a typical sample size and power analysis. Three proposed methods that yield a reproducible mean-comparison analysis are derived and compared.},
  archive      = {J_SIM},
  author       = {Steven Novick and Tianhui Zhang},
  doi          = {10.1002/sim.8848},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1414-1428},
  shortjournal = {Stat. Med.},
  title        = {Mean comparisons and power calculations to ensure reproducibility in preclinical drug discovery},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An overview and critique of the use of cumulative sum
methods with surgical learning curve data. <em>SIM</em>, <em>40</em>(6),
1400–1413. (<a href="https://doi.org/10.1002/sim.8847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cumulative sum (CUSUM) plots and methods have wide-ranging applications in healthcare. We review and discuss some issues related to the analysis of surgical learning curve (LC) data with a focus on three types of CUSUM statistical approaches. The underlying assumptions, benefits, and weaknesses of each approach are given. Our primary conclusion is that two types of CUSUM methods are useful in providing visual aids, but are subject to overinterpretation due to the lack of well-defined decision rules and performance metrics. The third type is based on plotting the CUSUM of the differences between observations and their average value. We show that this commonly applied retrospective method is frequently interpreted incorrectly and is thus unhelpful in the LC application. Curve-fitting methods are more suitable for meeting many of the goals associated with the study of surgical LCs.},
  archive      = {J_SIM},
  author       = {William H. Woodall and George Rakovich and Stefan H. Steiner},
  doi          = {10.1002/sim.8847},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1400-1413},
  shortjournal = {Stat. Med.},
  title        = {An overview and critique of the use of cumulative sum methods with surgical learning curve data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A machine learning compatible method for ordinal propensity
score stratification and matching. <em>SIM</em>, <em>40</em>(6),
1383–1399. (<a href="https://doi.org/10.1002/sim.8846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although machine learning techniques that estimate propensity scores for observational studies with multivalued treatments have advanced rapidly in recent years, the development of propensity score adjustment techniques has not kept pace. While machine learning propensity models provide numerous benefits, they do not produce a single variable balancing score that can be used for propensity score stratification and matching. This issue motivates the development of a flexible ordinal propensity scoring methodology that does not require parametric assumptions for the propensity model. The proposed method fits a one-parameter power function to the cumulative distribution function (CDF) of the generalized propensity score (GPS) vector resulting from any machine learning propensity model, and is henceforth called the GPS-CDF method. The estimated parameter from the GPS-CDF method, is a scalar balancing score that can be used to group similar subjects in outcome analyses. Specifically, subjects who received different levels of the treatment are stratified or matched based on their value to produce unbiased estimates of the average treatment effect (ATE). Simulation studies presented show remediation of covariate balance, minimal bias in ATEs, and maintain coverage probability. The proposed method is applied to the Mexican-American Tobacco use in Children (MATCh) study to determine whether an ordinal treatment of exposure to smoking imagery in movies causes cigarette experimentation in Mexican-American adolescents.},
  archive      = {J_SIM},
  author       = {Thomas J. Greene and Stacia M. DeSantis and Derek W. Brown and Anna V. Wilkinson and Michael D. Swartz},
  doi          = {10.1002/sim.8846},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1383-1399},
  shortjournal = {Stat. Med.},
  title        = {A machine learning compatible method for ordinal propensity score stratification and matching},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heritability curves: A local measure of heritability in
family models. <em>SIM</em>, <em>40</em>(6), 1357–1382. (<a
href="https://doi.org/10.1002/sim.8845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical heritability models for family data split the phenotype variance into genetic and environmental components. For instance, the ACE model in twin studies assumes the phenotype variance decomposes as a 2 + c 2 + e 2 , representing (additive) genetic effects, common (shared) environment, and residual environment, respectively. However, for some phenotypes it is biologically plausible that the genetic and environmental components may vary over the range of the phenotype. For instance, very large or small values of the phenotype may be caused by “sporadic” environmental factors, whereas the mid-range phenotype variation may be more under the control of common genetic factors. This article introduces a “local” measure of heritability, where the genetic and environmental components are allowed to depend on the value of the phenotype itself. Our starting point is a general formula for local correlation between two random variables. For estimation purposes, we use a multivariate Gaussian mixture, which is able to capture nonlinear dependence and respects certain distributional constraints. We derive an analytical expression for the associated correlation curve, and show how to decompose the correlation curve into genetic and environmental parts, for instance, a 2 ( y ) + c 2 ( y ) + e 2 ( y ) for the ACE model, where we estimate the components as functions of the phenotype y . Furthermore, our model allows switching, for instance, from the ACE model to the ADE model within the range of the same phenotype. When applied to birth weight (BW) data on Norwegian mother-father-child trios, we conclude from the model that low and high BW are less heritable traits than medium BW. We also demonstrate switching between the ACE and ADE model when studying body mass index in adult monozygotic and dizygotic twins.},
  archive      = {J_SIM},
  author       = {Geir D. Berentsen and Francesca Azzolini and Hans J. Skaug and Rolv T. Lie and Håkon K. Gjessing},
  doi          = {10.1002/sim.8845},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1357-1382},
  shortjournal = {Stat. Med.},
  title        = {Heritability curves: A local measure of heritability in family models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A longitudinal bayesian mixed effects model with hurdle
conway-maxwell-poisson distribution. <em>SIM</em>, <em>40</em>(6),
1336–1356. (<a href="https://doi.org/10.1002/sim.8844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dental caries (i.e., cavities) is one of the most common chronic childhood diseases and may continue to progress throughout a person&#39;s lifetime. The Iowa Fluoride Study (IFS) was designed to investigate the effects of various fluoride, dietary and nondietary factors on the progression of dental caries among a cohort of Iowa school children. We develop a mixed effects model to perform a comprehensive analysis of the longitudinal clustered data of IFS at ages 5, 9, 13, and 17. We combine a Bayesian hurdle framework with the Conway-Maxwell-Poisson regression model, which can account for both excessive zeros and various levels of dispersion. A hierarchical shrinkage prior distribution is used to share the temporal information for predictors in the fixed-effects model. The dependence among teeth of each individual child is modeled through a sparse covariance structure of the random effects across time. Moreover, we obtain the parameter estimates and credible intervals from a Gibbs sampler. Simulation studies are conducted to assess the accuracy and effectiveness of our statistical methodology. The results of this article provide novel tools to statistical practitioners and offer fresh insights to dental researchers on effects of various risk and protective factors on caries progression.},
  archive      = {J_SIM},
  author       = {Tong Kang and Jeremy Gaskins and Steven Levy and Somnath Datta},
  doi          = {10.1002/sim.8844},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1336-1356},
  shortjournal = {Stat. Med.},
  title        = {A longitudinal bayesian mixed effects model with hurdle conway-maxwell-poisson distribution},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing environmental epidemiology questions in practice
with a causal inference pipeline: An investigation of the air
pollution-multiple sclerosis relapses relationship. <em>SIM</em>,
<em>40</em>(6), 1321–1335. (<a
href="https://doi.org/10.1002/sim.8843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When addressing environmental health-related questions, most often, only observational data are collected for ethical or practical reasons. However, the lack of randomized exposure often prevents the comparison of similar groups of exposed and unexposed units. This design barrier leads the environmental epidemiology field to mainly estimate associations between environmental exposures and health outcomes. A recently developed causal inference pipeline was developed to guide researchers interested in estimating the effects of plausible hypothetical interventions for policy recommendations. This article illustrates how this multistaged pipeline can help environmental epidemiologists reconstruct and analyze hypothetical randomized experiments by investigating whether an air pollution reduction intervention decreases the risk of multiple sclerosis relapses in Alsace region, France. The epidemiology literature reports conflicted findings on the relationship between air pollution and multiple sclerosis. Some studies found significant associations, whereas others did not. Two case-crossover studies reported significant associations between the risk of multiple sclerosis relapses and the exposure to air pollutants in the Alsace region. We use the same study population as these epidemiological studies to illustrate how appealing this causal inference approach is to estimate the effects of hypothetical, but plausible, environmental interventions.},
  archive      = {J_SIM},
  author       = {Alice J. Sommer and Emmanuelle Leray and Young Lee and Marie-Abèle C. Bind},
  doi          = {10.1002/sim.8843},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {1321-1335},
  shortjournal = {Stat. Med.},
  title        = {Assessing environmental epidemiology questions in practice with a causal inference pipeline: An investigation of the air pollution-multiple sclerosis relapses relationship},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of ascertainment bias and its effect on power in
clinical trials with time-to-event outcomes. <em>SIM</em>,
<em>40</em>(5), 1306–1320. (<a
href="https://doi.org/10.1002/sim.8842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the gold standard for clinical trials is to blind all parties—participants, researchers, and evaluators—to treatment assignment, this is not always a possibility. When some or all of the above individuals know the treatment assignment, this leaves the study open to the introduction of postrandomization biases. In the Strategies to Reduce Injuries and Develop Confidence in Elders (STRIDE) trial, we were presented with the potential for the unblinded clinicians administering the treatment, as well as the individuals enrolled in the study, to introduce ascertainment bias into some but not all events comprising the primary outcome. In this article, we present ways to estimate the ascertainment bias for a time-to-event outcome, and discuss its impact on the overall power of a trial vs changing of the outcome definition to a more stringent unbiased definition that restricts attention to measurements less subject to potentially differential assessment. We found that for the majority of situations, it is better to revise the definition to a more stringent definition, as was done in STRIDE, even though fewer events may be observed.},
  archive      = {J_SIM},
  author       = {Erich J. Greene and Peter Peduzzi and James Dziura and Can Meng and Michael E. Miller and Thomas G. Travison and Denise Esserman},
  doi          = {10.1002/sim.8842},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1306-1320},
  shortjournal = {Stat. Med.},
  title        = {Estimation of ascertainment bias and its effect on power in clinical trials with time-to-event outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust wald-type tests under random censoring. <em>SIM</em>,
<em>40</em>(5), 1285–1305. (<a
href="https://doi.org/10.1002/sim.8841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomly censored survival data are frequently encountered in biomedical or reliability applications and clinical trial analyses. Testing the significance of statistical hypotheses is crucial in such analyses to get conclusive inference but the existing likelihood-based tests, under a fully parametric model, are extremely nonrobust against outliers in the data. Although there exists a few robust estimators given randomly censored data, there is hardly any robust testing procedure available in the literature in this context. One of the major difficulties here is the construction of a suitable consistent estimator of the asymptotic variance of robust estimators, since the latter is a function of the unknown censoring distribution. In this article, we take the first step in this direction by proposing a consistent estimator of asymptotic variance of the M-estimators based on randomly censored data without any assumption on the censoring scheme. We then describe and study a class of robust Wald-type tests for parametric statistical hypothesis, both simple as well as composite, under such a set-up. Robust tests for comparing two independent randomly censored samples and robust tests against one sided alternatives are also discussed. Their advantages and usefulness are demonstrated for the tests based on the minimum density power divergence estimators and illustrated with clinical trials and other medical data.},
  archive      = {J_SIM},
  author       = {Abhik Ghosh and Ayanendranath Basu and Leandro Pardo},
  doi          = {10.1002/sim.8841},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1285-1305},
  shortjournal = {Stat. Med.},
  title        = {Robust wald-type tests under random censoring},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint analysis of mixed types of outcomes with latent
variables. <em>SIM</em>, <em>40</em>(5), 1272–1284. (<a
href="https://doi.org/10.1002/sim.8840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a joint modeling approach to investigating the observed and latent risk factors of mixed types of outcomes. The proposed model comprises three parts. The first part is an exploratory factor analysis model that summarizes latent factors through multiple observed variables. The second part is a proportional hazards model that examines the observed and latent risk factors of multivariate time-to-event outcomes. The third part is a linear regression model that investigates the determinants of a continuous outcome. We develop a Bayesian approach coupled with MCMC methods to determine the number of latent factors, the association between latent and observed variables, and the important risk factors of different types of outcomes. A modified stochastic search item selection algorithm, which introduces normal-mixture-inverse gamma priors to factor loadings and regression coefficients, is developed for simultaneous model selection and parameter estimation. The proposed method is subjected to simulation studies for empirical performance assessment and then applied to a study concerning the risk factors of type 2 diabetes and the associated complications.},
  archive      = {J_SIM},
  author       = {Deng Pan and Yingying Wei and Xinyuan Song},
  doi          = {10.1002/sim.8840},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1272-1284},
  shortjournal = {Stat. Med.},
  title        = {Joint analysis of mixed types of outcomes with latent variables},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression analysis of mixed panel count data with
informative indicator processes. <em>SIM</em>, <em>40</em>(5),
1262–1271. (<a href="https://doi.org/10.1002/sim.8839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panel count data occur often in event history studies and in these situations, one observes only incomplete information, the number of events rather than the occurrence times of each event, about the point processes of interest. 2 Sometimes one may have to face a more complicated type of panel count data, mixed panel count data in which instead of the number of events, one only knows if there is an occurrence of an event. 3 Furthermore, this may depend on the underlying point process of interest or in other words, the point process of interest and the observation type process may be related. To address this, a sieve maximum likelihood estimation approach is proposed with the use of Bernstein polynomials, and for the implementation, an EM algorithm is developed. To assess the finite sample performance of the proposed approach, a simulation study is conducted and suggests that it works well for practical situations. The method is then applied to a motivating example about cancer survivors.},
  archive      = {J_SIM},
  author       = {Lei Ge and Liang Zhu and Jianguo Sun},
  doi          = {10.1002/sim.8839},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1262-1271},
  shortjournal = {Stat. Med.},
  title        = {Regression analysis of mixed panel count data with informative indicator processes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial process decomposition for quantitative imaging
biomarkers using multiple images of varying shapes. <em>SIM</em>,
<em>40</em>(5), 1243–1261. (<a
href="https://doi.org/10.1002/sim.8838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative imaging biomarkers (QIB) are extracted from medical images in radiomics for a variety of purposes including noninvasive disease detection, cancer monitoring, and precision medicine. The existing methods for QIB extraction tend to be ad hoc and not reproducible. In this article, a general and flexible statistical approach is proposed for handling up to three-dimensional medical images and reasonably capturing features with respect to specific spatial patterns. In particular, a model-based spatial process decomposition is developed where the random weights are unique to individual patients for component functions common across patients. Model fitting and selection are based on maximum likelihood, while feature extractions are via optimal prediction of the underlying true image. Simulation studies are conducted to investigate the properties of the proposed methodology. For illustration, a cancer image data set is analyzed and QIBs are extracted in association with a clinical endpoint.},
  archive      = {J_SIM},
  author       = {ShengLi Tzeng and Jun Zhu and Amy J. Weisman and Tyler J. Bradshaw and Robert Jeraj},
  doi          = {10.1002/sim.8838},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1243-1261},
  shortjournal = {Stat. Med.},
  title        = {Spatial process decomposition for quantitative imaging biomarkers using multiple images of varying shapes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the marginal hazard ratio by simultaneously using
a set of propensity score models: A multiply robust approach.
<em>SIM</em>, <em>40</em>(5), 1224–1242. (<a
href="https://doi.org/10.1002/sim.8837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inverse probability weighted Cox model is frequently used to estimate the marginal hazard ratio. Its validity requires a crucial condition that the propensity score model be correctly specified. To provide protection against misspecification of the propensity score model, we propose a weighted estimation method rooted in the empirical likelihood theory. The proposed estimator is multiply robust in that it is guaranteed to be consistent when a set of postulated propensity score models contains a correctly specified model. Our simulation studies demonstrate satisfactory finite sample performance of the proposed method in terms of consistency and efficiency. We apply the proposed method to compare the risk of postoperative hospitalization between sleeve gastrectomy and Roux-en-Y gastric bypass using data from a large medical claims and billing database. We further extend the development to multisite studies to enable each site to postulate multiple site-specific propensity score models.},
  archive      = {J_SIM},
  author       = {Di Shu and Peisong Han and Rui Wang and Sengwee Toh},
  doi          = {10.1002/sim.8837},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1224-1242},
  shortjournal = {Stat. Med.},
  title        = {Estimating the marginal hazard ratio by simultaneously using a set of propensity score models: A multiply robust approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector-based kernel weighting: A simple estimator for
improving precision and bias of average treatment effects in multiple
treatment settings. <em>SIM</em>, <em>40</em>(5), 1204–1223. (<a
href="https://doi.org/10.1002/sim.8836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment effect estimation must account for observed confounding, in which factors affect treatment assignment and outcomes simultaneously. Ignoring observed confounding risks concluding that a helpful treatment is not beneficial or that a treatment is safe when actually harmful. Propensity score matching or weighting adjusts for observed confounding, but the best way to use propensity scores for multiple treatments is unknown. It is unclear when choice of a different weighting or matching strategy leads to divergent inferences. We used Monte Carlo simulations (1000 replications) to examine sensitivity of multivalued treatment inferences to propensity score weighting or matching strategies. We consider five variants of propensity score adjustment: inverse probability of treatment weights, generalized propensity score matching, kernel weights (KW), vector matching, and a new hybrid that is easily implemented—vector-based kernel weighting (VBKW). VBKW matches observations with similar propensity score vectors, assigning greater KW to observations with similar probabilities within a given bandwidth. We varied degree of propensity score model misspecification, sample size, treatment effect heterogeneity, initial covariate imbalance, and sample distribution across treatment groups. We evaluated sensitivity of results to propensity score estimation technique (multinomial logit or multinomial probit). Across simulations, VBKW performed equally or better than the other methods in terms of bias, efficiency, and covariate balance measured via prognostic scores. Our simulations suggest that VBKW is amenable to full automation and is less sensitive to PS model misspecification than other methods used to account for observed confounding in multivalued treatment analyses.},
  archive      = {J_SIM},
  author       = {Melissa M. Garrido and Jessica Lum and Steven D. Pizer},
  doi          = {10.1002/sim.8836},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1204-1223},
  shortjournal = {Stat. Med.},
  title        = {Vector-based kernel weighting: A simple estimator for improving precision and bias of average treatment effects in multiple treatment settings},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propensity score stratification methods for continuous
treatments. <em>SIM</em>, <em>40</em>(5), 1189–1203. (<a
href="https://doi.org/10.1002/sim.8835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous treatments propensity scoring remains understudied as the majority of methods are focused on the binary treatment setting. Current propensity score methods for continuous treatments typically rely on weighting in order to produce causal estimates. It has been shown that in some continuous treatment settings, weighting methods can result in worse covariate balance than had no adjustments been made to the data. Furthermore, weighting is not always stable, and resultant estimates may be unreliable due to extreme weights. These issues motivate the current development of novel propensity score stratification techniques to be used with continuous treatments. Specifically, the generalized propensity score cumulative distribution function (GPS-CDF) and the nonparametric GPS-CDF approaches are introduced. Empirical CDFs are used to stratify subjects based on pretreatment confounders in order to produce causal estimates. A detailed simulation study shows superiority of these new stratification methods based on the empirical CDF, when compared with standard weighting techniques. The proposed methods are applied to the “Mexican-American Tobacco use in Children” study to determine the causal relationship between continuous exposure to smoking imagery in movies, and smoking behavior among Mexican-American adolescents. These promising results provide investigators with new options for implementing continuous treatment propensity scoring.},
  archive      = {J_SIM},
  author       = {Derek W. Brown and Thomas J. Greene and Michael D. Swartz and Anna V. Wilkinson and Stacia M. DeSantis},
  doi          = {10.1002/sim.8835},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1189-1203},
  shortjournal = {Stat. Med.},
  title        = {Propensity score stratification methods for continuous treatments},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulation model of disease incidence driven by diagnostic
activity. <em>SIM</em>, <em>40</em>(5), 1172–1188. (<a
href="https://doi.org/10.1002/sim.8833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is imperative to understand the effects of early detection and treatment of chronic diseases, such as prostate cancer, regarding incidence, overtreatment and mortality. Previous simulation models have emulated clinical trials, and relied on extensive assumptions on the natural history of the disease. In addition, model parameters were typically calibrated to a variety of data sources. We propose a model designed to emulate real-life scenarios of chronic disease using a proxy for the diagnostic activity without explicitly modeling the natural history of the disease and properties of clinical tests. Our model was applied to Swedish nation-wide population-based prostate cancer data, and demonstrated good performance in terms of reconstructing observed incidence and mortality. The model was used to predict the number of prostate cancer diagnoses with a high or limited diagnostic activity between 2017 and 2060. In the long term, high diagnostic activity resulted in a substantial increase in the number of men diagnosed with lower risk disease, fewer men with metastatic disease, and decreased prostate cancer mortality. The model can be used for prediction of outcome, to guide decision-making, and to evaluate diagnostic activity in real-life settings with respect to overdiagnosis and prostate cancer mortality.},
  archive      = {J_SIM},
  author       = {Marcus Westerberg and Rolf Larsson and Lars Holmberg and Pär Stattin and Hans Garmo},
  doi          = {10.1002/sim.8833},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1172-1188},
  shortjournal = {Stat. Med.},
  title        = {Simulation model of disease incidence driven by diagnostic activity},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simplified approach for establishing estimable functions
in fixed effect age-period-cohort multiple classification models.
<em>SIM</em>, <em>40</em>(5), 1160–1171. (<a
href="https://doi.org/10.1002/sim.8831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimable functions play an important role in learning about certain aspects of the impact of ages, periods, and cohorts in age-period-cohort multiple classification (APCMC) models. The advantage of these estimates is that they are unbiased estimates of, for example, the deviations of age, period, and cohort effects from their linear trends, or changes in the linear trends of cohort effects within cohorts, or the residuals of fixed effect APCMC models. If the fixed effect APCMC model contains the relevant variables (is well specified), these estimable functions are unbiased estimates of functions of the parameters that generated the dependent variable data, even though the parameters that generated that data are not identified. I provide a simplified approach to establishing which functions are estimable in fixed effect APCMC models that provides an intuitive understanding of estimable functions by showing clearly and simply why they are estimable. This approach involves the partitioning of the age, period, and cohort effects into linear components and deviations from the linear components; the use of the “line of solutions”; and of the “extended null vector.”},
  archive      = {J_SIM},
  author       = {Robert M. O&#39;Brien},
  doi          = {10.1002/sim.8831},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1160-1171},
  shortjournal = {Stat. Med.},
  title        = {A simplified approach for establishing estimable functions in fixed effect age-period-cohort multiple classification models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence intervals for difference in proportions for
matched pairs compatible with exact McNemar’s or sign tests.
<em>SIM</em>, <em>40</em>(5), 1147–1159. (<a
href="https://doi.org/10.1002/sim.8829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For testing with paired data (eg, twins randomized between two treatments), a simple test is the sign test, where we test if the distribution of the sign of the differences in responses between the two treatments within pairs is more often positive (favoring one treatment) or negative (favoring the other). When the responses are binary, this reduces to a McNemar-type test, and the calculations are the same. Although it is easy to calculate an exact P -value by conditioning on the total number of discordant pairs, the accompanying confidence interval on a parameter of interest (proportion positive minus proportion negative) is not straightforward. Effect estimates and confidence intervals are important for interpretation because it is possible that the treatment helps a very small proportion of the population yet gives a highly significant effect. We construct a confidence interval that is compatible with an exact sign test, meaning the 100 interval excludes the null hypothesis of equality of proportions if and only if the associated exact sign test rejects at level . We conjecture that the proposed confidence intervals guarantee nominal coverage, and we support that conjecture with extensive numerical calculations, but we have no mathematical proof to show guaranteed coverage. We have written and made available the function mcnemarExactDP in the exact2x2 R package and the function signTest in the asht R package to perform the methods described in this article.},
  archive      = {J_SIM},
  author       = {Michael P. Fay and Keith Lumbard},
  doi          = {10.1002/sim.8829},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1147-1159},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for difference in proportions for matched pairs compatible with exact McNemar&#39;s or sign tests},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis of three-arm parallel cluster randomized
trials with small numbers of clusters. <em>SIM</em>, <em>40</em>(5),
1133–1146. (<a href="https://doi.org/10.1002/sim.8828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we review and evaluate a number of methods used in the design and analysis of small three-arm parallel cluster randomized trials. We conduct a simulation-based study to evaluate restricted randomization methods including covariate-constrained randomization and a novel method for matched-group cluster randomization. We also evaluate the appropriate modelling of the data and small sample inferential methods for a variety of treatment effects relevant to three-arm trials. Our results indicate that small-sample corrections are required for high (0.05) but not low (0.001) values of the intraclass correlation coefficient and their performance can depend on trial design, number of clusters, and the nature of the hypothesis being tested. The Satterthwaite correction generally performed best at an ICC of 0.05 with a nominal type I error rate for single-period trials, and in trials with repeated measures type I error rates were between 0.04 and 0.06. Restricted randomization methods produce little benefit in trials with repeated measures but in trials with single post-intervention design can provide relatively large gains in power when compared to the most unbalanced possible allocations. Matched-group randomization improves power but is not as effective as covariate-constrained randomization. For model-based analysis, adjusting for fewer covariates than were used in a restricted randomization process under any design can produce non-nominal type I error rates and reductions in power. Where comparisons to two-arm cluster trials are possible, the performance of the methods is qualitatively very similar.},
  archive      = {J_SIM},
  author       = {Samuel I. Watson and Alan Girling and Karla Hemming},
  doi          = {10.1002/sim.8828},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1133-1146},
  shortjournal = {Stat. Med.},
  title        = {Design and analysis of three-arm parallel cluster randomized trials with small numbers of clusters},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified approach to sample size and power determination
for testing parameters in generalized linear and time-to-event
regression models. <em>SIM</em>, <em>40</em>(5), 1121–1132. (<a
href="https://doi.org/10.1002/sim.8823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure that a study can properly address its research aims, the sample size and power must be determined appropriately. Covariate adjustment via regression modeling permits more precise estimation of the effect of a primary variable of interest at the expense of increased complexity in sample size/power calculation. The presence of correlation between the main variable and other covariates, commonly seen in observational studies and non-randomized clinical trials, further complicates this process. Though sample size and power specification methods have been obtained to accommodate specific covariate distributions and models, most existing approaches rely on either simple approximations lacking theoretical support or complex procedures that are difficult to apply at the design stage. The current literature lacks a general, coherent theory applicable to a broader class of regression models and covariate distributions. We introduce succinct formulas for sample size and power determination with the generalized linear, Cox, and Fine-Gray models that account for correlation between a main effect and other covariates. Extensive simulations demonstrate that this method produces studies that are appropriately sized to meet their type I error rate and power specifications, particularly offering accurate sample size/power estimation in the presence of correlated covariates.},
  archive      = {J_SIM},
  author       = {Michael J. Martens and Brent R. Logan},
  doi          = {10.1002/sim.8823},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1121-1132},
  shortjournal = {Stat. Med.},
  title        = {A unified approach to sample size and power determination for testing parameters in generalized linear and time-to-event regression models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalizing randomized trial findings to a target
population using complex survey population data. <em>SIM</em>,
<em>40</em>(5), 1101–1120. (<a
href="https://doi.org/10.1002/sim.8822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized trials are considered the gold standard for estimating causal effects. Trial findings are often used to inform policy and programming efforts, yet their results may not generalize well to a relevant target population due to potential differences in effect moderators between the trial and population. Statistical methods have been developed to improve generalizability by combining trials and population data, and weighting the trial to resemble the population on baseline covariates. Large-scale surveys in fields such as health and education with complex survey designs are a logical source for population data; however, there is currently no best practice for incorporating survey weights when generalizing trial findings to a complex survey. We propose and investigate ways to incorporate survey weights in this context. We examine the performance of our proposed estimator through simulations in comparison to estimators that ignore the complex survey design. We then apply the methods to generalize findings from two trials—a lifestyle intervention for blood pressure reduction and a web-based intervention to treat substance use disorders—to their respective target populations using population data from complex surveys. The work highlights the importance in properly accounting for the complex survey design when generalizing trial findings to a population represented by a complex survey sample.},
  archive      = {J_SIM},
  author       = {Benjamin Ackerman and Catherine R. Lesko and Juned Siddique and Ryoko Susukida and Elizabeth A. Stuart},
  doi          = {10.1002/sim.8822},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1101-1120},
  shortjournal = {Stat. Med.},
  title        = {Generalizing randomized trial findings to a target population using complex survey population data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparisons of zero-augmented continuous regression models
from a bayesian perspective. <em>SIM</em>, <em>40</em>(5), 1073–1100.
(<a href="https://doi.org/10.1002/sim.8795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-part model and the Tweedie model are two essential methods to analyze the positive continuous and zero-augmented responses. Compared with other continuous zero-augmented models, the zero-augmented gamma model (ZAG) demonstrates its performance on the mass zeros data. In this article, we compare the Bayesian model for continuous data of excess zeros by considering the ZAG and Tweedie model. We model the mean of both models in a logarithmic scale and the probability of zero within the zero-augmented model in a logit scale. As previous researchers employed different priors in Bayesian settings for the Tweedie model, by conducting a sensitivity analysis, we select the optimal priors for Tweedie model. Furthermore, we present a simulation study to evaluate the performance of two models in the comparison and apply them to a dataset about the daily fish intake and blood mercury levels from National Health and Nutrition Examination Survey. According to the Watanabe-Akaike information criterion and leave-one-out cross-validation criterion, the Tweedie model provides higher predictive accuracy for the positive continuous and zero-augmented data.},
  archive      = {J_SIM},
  author       = {Tairan Ye and Victor H. Lachos and Xiaojing Wang and Dipak K. Dey},
  doi          = {10.1002/sim.8795},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {1073-1100},
  shortjournal = {Stat. Med.},
  title        = {Comparisons of zero-augmented continuous regression models from a bayesian perspective},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal two-phase sampling for estimating the area under the
receiver operating characteristic curve. <em>SIM</em>, <em>40</em>(4),
1059–1071. (<a href="https://doi.org/10.1002/sim.8819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods are well developed for estimating the area under the receiver operating characteristic curve (AUC) based on a random sample where the gold standard is available for every subject in the sample, or a two-phase sample where the gold standard is ascertained only at the second phase for a subset of subjects sampled using fixed sampling probabilities. However, the methods based on a two-phase sample do not attempt to optimize the sampling probabilities to minimize the variance of AUC estimator. In this paper, we consider the optimal two-phase sampling design for evaluating the performance of an ordinal test in classifying disease status. We derived the analytic variance formula for the AUC estimator and used it to obtain the optimal sampling probabilities. The efficiency of the two-phase sampling under the optimal sampling probabilities (OA) is evaluated by a simulation study, which indicates that two-phase sampling under OA achieves a substantial amount of variance reduction with an over-sample of subjects with low and high ordinal levels, compared with two-phase sampling under proportional allocation (PA). Furthermore, in comparison with an one-phase random sampling, two-phase sampling under OA or PA have a clear advantage in reducing the variance of AUC estimator when the variance of diagnostic test results in the disease population is small relative to its counterpart in nondisease population. Finally, we applied the optimal two-phase sampling design to a real-world example to evaluate the performance of a questionnaire score in screening for childhood asthma.},
  archive      = {J_SIM},
  author       = {Yougui Wu},
  doi          = {10.1002/sim.8819},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {1059-1071},
  shortjournal = {Stat. Med.},
  title        = {Optimal two-phase sampling for estimating the area under the receiver operating characteristic curve},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error-corrected estimation of a diagnostic accuracy index of
a biomarker against a continuous gold standard. <em>SIM</em>,
<em>40</em>(4), 1034–1058. (<a
href="https://doi.org/10.1002/sim.8818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns evaluating the effectiveness of a continuous diagnostic biomarker against a continuous gold standard that is measured with error. Extending the work of Obuchowski (2005, 2016), Wu et al (2016) suggested an accuracy index and proposed an estimator for the index with error-prone standard when the reliability coefficient is known. Combining with additional measurements (without measurement errors) on the continuous gold standard collected from some subjects, this article proposes two adaptive estimators of the accuracy index when the reliability coefficient is unknown, and further establish the consistency and asymptotic normality of these estimators. Simulation studies are conducted to compare various estimators. Data from an intervention trial on glycemic control among children with type 1 diabetes are used to illustrate the proposed methods.},
  archive      = {J_SIM},
  author       = {Mixia Wu and Xiaoyu Zhang and Wei Zhang and Xu Zhang and Aiyi Liu},
  doi          = {10.1002/sim.8818},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {1034-1058},
  shortjournal = {Stat. Med.},
  title        = {Error-corrected estimation of a diagnostic accuracy index of a biomarker against a continuous gold standard},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal analysis of misaligned burden of disease
data using a geo-statistical approach. <em>SIM</em>, <em>40</em>(4),
1021–1033. (<a href="https://doi.org/10.1002/sim.8817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data used to estimate the burden of diseases (BOD) are usually sparse, noisy, and heterogeneous. These data are collected from surveys, registries, and systematic reviews that have different areal units, are conducted at different times, and are reported for different age groups. In this study, we developed a Bayesian geo-statistical model to combine aggregated sparse, noisy BOD data from different sources with misaligned areal units. Our model incorporates the correlation of space, time, and age to estimate health indicators for areas with no data or a small number of observations. The model also considers the heterogeneity of data sources and the measurement errors of input data in the final estimates and uncertainty intervals. We applied the model to combine data from nine different sources of body mass index in a national and sub-national BOD study. The cross-validation results confirmed a high out-of-sample predictive ability in sparse and noisy data. The proposed model can be used by other BOD studies especially at the sub-national level when the areal units are subject to misalignment.},
  archive      = {J_SIM},
  author       = {Mahboubeh Parsaeian and Majid Jafari Khaledi and Farshad Farzadfar and Mahdi Mahdavi and Hojjat Zeraati and Mahmood Mahmoudi and Ardeshir Khosravi and Kazem Mohammad},
  doi          = {10.1002/sim.8817},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {1021-1033},
  shortjournal = {Stat. Med.},
  title        = {Spatio-temporal analysis of misaligned burden of disease data using a geo-statistical approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survival analysis under the cox proportional hazards model
with pooled covariates. <em>SIM</em>, <em>40</em>(4), 998–1020. (<a
href="https://doi.org/10.1002/sim.8816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a continuous time-to-event outcome and an expensive-to-measure exposure, we develop a pooling design and propose a likelihood-based approach to estimate the hazard ratios (HRs) of a Cox proportional hazards (PH) model. Our proposed approach fits a PH model based on pooled exposures with individually observed time-to-event outcomes. The design and estimation exploits the equivalence of the conditional logistic likelihood functions arising from a matched case-control study and the partial likelihood function of a riskset-matched, nested case-control (NCC) subset of a cohort study. To create the pools, we first focus on an NCC subcohort. Pools are formed at random while keeping the matching intact. Pool-level exposure and confounders are then evaluated and used in the likelihood to estimate the HR and the standard error of the estimates. The estimators are MLEs, provide consistent estimates of the individual-level HRs, and are asymptotically normal. Our simulation results indicate that the pooled estimates are comparable to the estimates obtained from the NCC subcohort. The units of analysis for the pooled design are the pools and not the individual participants. Hence the effective sample size is reduced. Therefore, the variance of the HR estimate increases with increasing poolsize. However, this variance inflation in small samples can be offset by including more matched controls per case within the NCC subcohort. An application is demonstrated with the Second Manifestations of ARTerial disease (SMART) study.},
  archive      = {J_SIM},
  author       = {Paramita Saha-Chaudhuri and Lamin Juwara},
  doi          = {10.1002/sim.8816},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {998-1020},
  shortjournal = {Stat. Med.},
  title        = {Survival analysis under the cox proportional hazards model with pooled covariates},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Determination of correlations in multivariate longitudinal
data with modified cholesky and hypersphere decomposition using bayesian
variable selection approach. <em>SIM</em>, <em>40</em>(4), 978–997. (<a
href="https://doi.org/10.1002/sim.8815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a Bayesian framework for multivariate longitudinal data analysis with a focus on selection of important elements in the generalized autoregressive matrix. An efficient Gibbs sampling algorithm was developed for the proposed model and its implementation in a comprehensive R package called MLModelSelection is available on the comprehensive R archive network. The performance of the proposed approach was studied via a comprehensive simulation study. The effectiveness of the methodology was illustrated using a nonalcoholic fatty liver disease dataset to study correlations in multiple responses over time to explain the joint variability of lung functions and body mass index. Supplementary materials for this article, including a standardized description of the materials needed to reproduce the work, are available as an online supplement.},
  archive      = {J_SIM},
  author       = {Kuo-Jung Lee and Ray-Bing Chen and Min-Sun Kwak and Keunbaik Lee},
  doi          = {10.1002/sim.8815},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {978-997},
  shortjournal = {Stat. Med.},
  title        = {Determination of correlations in multivariate longitudinal data with modified cholesky and hypersphere decomposition using bayesian variable selection approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian methods to compare dose levels with placebo in a
small n, sequential, multiple assignment, randomized trial.
<em>SIM</em>, <em>40</em>(4), 963–977. (<a
href="https://doi.org/10.1002/sim.8813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials studying treatments for rare diseases are challenging to design and conduct due to the limited number of patients eligible for the trial. One design used to address this challenge is the small n, sequential, multiple assignment, randomized trial (snSMART). We propose a new snSMART design that investigates the response rates of a drug tested at a low and high dose compared with placebo. Patients are randomized to an initial treatment (stage 1). In stage 2, patients are rerandomized, depending on their initial treatment and their response to that treatment in stage 1, to either the same or a different dose of treatment. Data from both stages are used to determine the efficacy of the active treatment. We present a Bayesian approach where information is borrowed between stage 1 and stage 2. We compare our approach to standard methods using only stage 1 data and a log-linear Poisson model that uses data from both stages where parameters are estimated using generalized estimating equations. We observe that the Bayesian method has smaller root-mean-square-error and 95% credible interval widths than standard methods in the tested scenarios. We conclude that it is advantageous to utilize data from both stages for a primary efficacy analysis and that the specific snSMART design shown here can be used in the registration of a drug for the treatment of rare diseases.},
  archive      = {J_SIM},
  author       = {Fang Fang and Kimberly A. Hochstedler and Roy N. Tamura and Thomas M. Braun and Kelley M. Kidwell},
  doi          = {10.1002/sim.8813},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {963-977},
  shortjournal = {Stat. Med.},
  title        = {Bayesian methods to compare dose levels with placebo in a small n, sequential, multiple assignment, randomized trial},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complexity and bias in cross-sectional data with binary
disease outcome in observational studies. <em>SIM</em>, <em>40</em>(4),
950–962. (<a href="https://doi.org/10.1002/sim.8812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cross sectional population is defined as a population of living individuals at the sampling or observational time. Cross-sectionally sampled data with binary disease outcome are commonly analyzed in observational studies for identifying how covariates correlate with disease occurrence. It is generally understood that cross-sectional binary outcome is not as informative as longitudinally collected time-to-event data, but there is insufficient understanding as to whether bias can possibly exist in cross-sectional data and how the bias is related to the population risk of interest. As the progression of a disease typically involves both time and disease status, we consider how the binary disease outcome from the cross-sectional population is connected to birth-illness-death process in the target population. We argue that the distribution of cross-sectional binary outcome is different from the risk distribution from the target population and that bias would typically arise when using cross-sectional data to draw inference for population risk. In general, the cross-sectional risk probability is determined jointly by the population risk probability and the ratio of duration of diseased state to the duration of disease-free state. Through explicit formulas we conclude that bias can almost never be avoided from cross-sectional data. We present age-specific risk probability (ARP) and argue that models based on ARP offers a compromised but still biased approach to understand the population risk. An analysis based on Alzheimer&#39;s disease data is presented to illustrate the ARP model and possible critiques for the analysis results.},
  archive      = {J_SIM},
  author       = {Mei-Cheng Wang and Yuchen Yang},
  doi          = {10.1002/sim.8812},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {950-962},
  shortjournal = {Stat. Med.},
  title        = {Complexity and bias in cross-sectional data with binary disease outcome in observational studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust inference in the joint modeling of multilevel
zero-inflated poisson and cox models. <em>SIM</em>, <em>40</em>(4),
933–949. (<a href="https://doi.org/10.1002/sim.8811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular method for simultaneously modeling of correlated count response with excess zeros and time to event is by means of the joint models. In these models, the likelihood-based methods (such as expectation-maximization algorithm and Newton-Raphson) are used for estimating the parameters, but in the presence of contaminations, these methods are unstable. To overcome this challenge, we extend the M-estimator methods and propose a robust estimator approach to obtain a robust estimation of the regression parameters in the joint model. Our proposed algorithm has two steps (Expectation and Solution). In the expectation step, the likelihood function is expected by conditioning on the observed data and in the solution step, the parameters are computed, with solving robust estimating equations. Therefore, this algorithm achieves robustness by applying robust estimating equations and weighted likelihood in the S-step. Simulation studies under various situations of contaminations show that the robust algorithm gives us consistent estimates with a smaller bias than likelihood-based methods. The application section uses data on factors affecting fertility and birth spacing.},
  archive      = {J_SIM},
  author       = {Eghbal Zandkarimi and Abbas Moghimbeigi and Hossein Mahjub},
  doi          = {10.1002/sim.8811},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {933-949},
  shortjournal = {Stat. Med.},
  title        = {Robust inference in the joint modeling of multilevel zero-inflated poisson and cox models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian latent factor on image regression with nonignorable
missing data. <em>SIM</em>, <em>40</em>(4), 920–932. (<a
href="https://doi.org/10.1002/sim.8810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging data have been widely used in modern health care, particularly in the prognosis, screening, diagnosis, and treatment of various diseases. In this study, we consider a latent factor-on-image (LoI) regression model that regresses a latent factor on ultrahigh dimensional imaging covariates. The latent factor is characterized by multiple manifest variables through a factor analysis model, while the manifest variables are subject to nonignorable missingness. We propose a two-stage approach for statistical inference. At the first stage, an efficient functional principal component analysis method is applied to reduce the dimension and extract useful features/eigenimages. At the second stage, a factor analysis mode is proposed to characterize the latent response variable. Moreover, an LoI model is used to detect influential risk factors, and an exponential tiling model applied to accommodate nonignoreable nonresponses. A fully Bayesian method with an adjust spike-and-slab absolute shrinkage and selection operator (lasso) procedure is developed for the estimation and selection of influential features/eigenimages. Simulation studies show the proposed method exhibits satisfactory performance. The proposed methodology is applied to a study on the Alzheimer&#39;s Disease Neuroimaging Initiative data set.},
  archive      = {J_SIM},
  author       = {Xiaoqing Wang and Xinyuan Song and Hongtu Zhu},
  doi          = {10.1002/sim.8810},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {920-932},
  shortjournal = {Stat. Med.},
  title        = {Bayesian latent factor on image regression with nonignorable missing data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SuRF: A new method for sparse variable selection, with
application in microbiome data analysis. <em>SIM</em>, <em>40</em>(4),
897–919. (<a href="https://doi.org/10.1002/sim.8809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a new variable selection method for regression and classification purposes, particularly for microbiome analysis. Our method, called subsampling ranking forward selection (SuRF), is based on LASSO penalized regression, subsampling and forward-selection methods. SuRF offers major advantages over existing variable selection methods in terms of both sparsity of selected models and model inference. We provide an R package that can implement our method for generalized linear models. We apply our method to classification problems from microbiome data, using a novel agglomeration approach to deal with the special tree-like correlation structure of the variables. Existing methods arbitrarily choose a taxonomic level a priori before performing the analysis, whereas by combining SuRF with these aggregated variables, we are able to identify the key biomarkers at the appropriate taxonomic level, as suggested by the data. We present simulations in multiple sparse settings to demonstrate that our approach performs better than several other popularly used existing approaches in recovering the true variables. We apply SuRF to two microbiome datasets: one about prediction of pouchitis and another for identifying samples from two healthy individuals. We find that SuRF can provide a better or comparable prediction with other methods while controlling the false positive rate of variable selection.},
  archive      = {J_SIM},
  author       = {Lihui Liu and Hong Gu and Johan Van Limbergen and Toby Kenney},
  doi          = {10.1002/sim.8809},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {897-919},
  shortjournal = {Stat. Med.},
  title        = {SuRF: A new method for sparse variable selection, with application in microbiome data analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mediation effect selection in high-dimensional and
compositional microbiome data. <em>SIM</em>, <em>40</em>(4), 885–896.
(<a href="https://doi.org/10.1002/sim.8808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The microbiome plays an important role in human health by mediating the path from environmental exposures to health outcomes. The relative abundances of the high-dimensional microbiome data have an unit-sum restriction, rendering standard statistical methods in the Euclidean space invalid. To address this problem, we use the isometric log-ratio transformations of the relative abundances as the mediator variables. To select significant mediators, we consider a closed testing-based selection procedure with desirable confidence. Simulations are provided to verify the effectiveness of our method. As an illustrative example, we apply the proposed method to study the mediation effects of murine gut microbiome between subtherapeutic antibiotic treatment and body weight gain, and identify Coprobacillus and Adlercreutzia as two significant mediators.},
  archive      = {J_SIM},
  author       = {Haixiang Zhang and Jun Chen and Yang Feng and Chan Wang and Huilin Li and Lei Liu},
  doi          = {10.1002/sim.8808},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {885-896},
  shortjournal = {Stat. Med.},
  title        = {Mediation effect selection in high-dimensional and compositional microbiome data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Elasticity as a measure for online determination of
remission points in ongoing epidemics. <em>SIM</em>, <em>40</em>(4),
865–884. (<a href="https://doi.org/10.1002/sim.8807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correct identification of change-points during ongoing outbreak investigations of infectious diseases is a matter of paramount importance in epidemiology, with major implications for the management of health care resources, public health and, as the COVID-19 pandemic has shown, social live. Onsets, peaks, and inflexion points are some of them. An onset is the moment when the epidemic starts. A &quot;peak&quot; indicates a moment at which the incorporated values, both before and after, are lower: a maximum. The inflexion points identify moments in which the rate of growth of the incorporation of new cases changes intensity. In this study, after interpreting the concept of elasticity of a random variable in an innovative way, we propose using it as a new simpler tool for anticipating epidemic remission change-points. In particular, we propose that the &quot;remission point of change&quot; will occur just at the instant when the speed in the accumulation of new cases is lower than the average speed of accumulation of cases up to that moment. This gives stability and robustness to the estimation in the event of possible remission variations. This descriptive measure, which is very easy to calculate and interpret, is revealed as informative and adequate, has the advantage of being distribution-free and can be estimated in real time, while the data is being collected. We use the 2014-2016 Western Africa Ebola virus epidemic to demonstrate this new approach. A couple of examples analyzing COVID-19 data are also included.},
  archive      = {J_SIM},
  author       = {Ernesto J. Veres-Ferrer and Jose M. Pavía},
  doi          = {10.1002/sim.8807},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {865-884},
  shortjournal = {Stat. Med.},
  title        = {Elasticity as a measure for online determination of remission points in ongoing epidemics},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A note on estimating the cox-snell r2 from a reported c
statistic (AUROC) to inform sample size calculations for developing a
prediction model with a binary outcome. <em>SIM</em>, <em>40</em>(4),
859–864. (<a href="https://doi.org/10.1002/sim.8806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2019 we published a pair of articles in Statistics in Medicine that describe how to calculate the minimum sample size for developing a multivariable prediction model with a continuous outcome, or with a binary or time-to-event outcome. As for any sample size calculation, the approach requires the user to specify anticipated values for key parameters. In particular, for a prediction model with a binary outcome, the outcome proportion and a conservative estimate for the overall fit of the developed model as measured by the Cox-Snell R 2 (proportion of variance explained) must be specified. This proposal raises the question of how to identify a plausible value for R 2 in advance of model development. Our articles suggest researchers should identify R 2 from closely related models already published in their field. In this letter, we present details on how to derive R 2 using the reported C statistic (AUROC) for such existing prediction models with a binary outcome. The C statistic is commonly reported, and so our approach allows researchers to obtain R 2 for subsequent sample size calculations for new models. Stata and R code is provided, and a small simulation study.},
  archive      = {J_SIM},
  author       = {Richard D. Riley and Ben Van Calster and Gary S. Collins},
  doi          = {10.1002/sim.8806},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {859-864},
  shortjournal = {Stat. Med.},
  title        = {A note on estimating the cox-snell r2 from a reported c statistic (AUROC) to inform sample size calculations for developing a prediction model with a binary outcome},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Propensity score weighting for covariate adjustment in
randomized clinical trials. <em>SIM</em>, <em>40</em>(4), 842–858. (<a
href="https://doi.org/10.1002/sim.8805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chance imbalance in baseline characteristics is common in randomized clinical trials. Regression adjustment such as the analysis of covariance (ANCOVA) is often used to account for imbalance and increase precision of the treatment effect estimate. An objective alternative is through inverse probability weighting (IPW) of the propensity scores. Although IPW and ANCOVA are asymptotically equivalent, the former may demonstrate inferior performance in finite samples. In this article, we point out that IPW is a special case of the general class of balancing weights, and advocate to use overlap weighting (OW) for covariate adjustment. The OW method has a unique advantage of completely removing chance imbalance when the propensity score is estimated by logistic regression. We show that the OW estimator attains the same semiparametric variance lower bound as the most efficient ANCOVA estimator and the IPW estimator for a continuous outcome, and derive closed-form variance estimators for OW when estimating additive and ratio estimands. Through extensive simulations, we demonstrate OW consistently outperforms IPW in finite samples and improves the efficiency over ANCOVA and augmented IPW when the degree of treatment effect heterogeneity is moderate or when the outcome model is incorrectly specified. We apply the proposed OW estimator to the Best Apnea Interventions for Research (BestAIR) randomized trial to evaluate the effect of continuous positive airway pressure on patient health outcomes. All the discussed propensity score weighting methods are implemented in the R package PSweight .},
  archive      = {J_SIM},
  author       = {Shuxi Zeng and Fan Li and Rui Wang},
  doi          = {10.1002/sim.8805},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {842-858},
  shortjournal = {Stat. Med.},
  title        = {Propensity score weighting for covariate adjustment in randomized clinical trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic cause-of-disease assignment using case-control
diagnostic tests: A latent variable regression approach. <em>SIM</em>,
<em>40</em>(4), 823–841. (<a
href="https://doi.org/10.1002/sim.8804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal prevention and treatment strategies for a disease of multiple causes, such as pneumonia, must be informed by the population distribution of causes among cases, or cause-specific case fractions (CSCFs). CSCFs may further depend on additional explanatory variables. Existing methodological literature in disease etiology research does not fully address the regression problem, particularly under a case-control design. Based on multivariate binary non-gold-standard diagnostic data and additional covariate information, this article proposes a novel and unified regression modeling framework for estimating covariate-dependent CSCF functions in case-control disease etiology studies. The model leverages critical control data for valid probabilistic cause assignment for cases. We derive an efficient Markov chain Monte Carlo algorithm for flexible posterior inference. We illustrate the inference of CSCF functions using extensive simulations and show that the proposed model produces less biased estimates and more valid inference of the overall CSCFs than analyses that omit covariates. A regression analysis of pediatric pneumonia data reveals the dependence of CSCFs upon season, age, human immunodeficiency virus status and disease severity. The article concludes with a brief discussion on model extensions that may further enhance the utility of the regression model in broader disease etiology research.},
  archive      = {J_SIM},
  author       = {Zhenke Wu and Irena Chen},
  doi          = {10.1002/sim.8804},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {823-841},
  shortjournal = {Stat. Med.},
  title        = {Probabilistic cause-of-disease assignment using case-control diagnostic tests: A latent variable regression approach},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binary genetic algorithm for optimal joinpoint detection:
Application to cancer trend analysis. <em>SIM</em>, <em>40</em>(3),
799–822. (<a href="https://doi.org/10.1002/sim.8803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joinpoint regression model (JRM) is used to describe trend changes in many applications and relies on the detection of joinpoints (changepoints). However, the existing joinpoint detection methods, namely, the grid search (GS)-based methods, are computationally demanding, and hence, the maximum number of computable joinpoints is limited. Herein, we developed a genetic algorithm-based joinpoint (GAJP) model in which an explicitly decoupled computing procedure for optimization and regression is used to embed a binary genetic algorithm into the JRM for optimal joinpoint detection. The combinations of joinpoints were represented as binary chromosomes, and genetic operations were performed to determine the optimum solution by minimizing the fitness function, the Bayesian information criterion (BIC) and BIC 3 . The accuracy and computational performance of the GAJP model were evaluated via intensive simulation studies and compared with those of the GS-based methods using BIC, BIC 3 , and permutation test. The proposed method showed an outstanding computational efficiency in detecting multiple joinpoints. Finally, the suitability of the GAJP model for the analysis of cancer incidence trends was demonstrated by applying this model to data on the incidence of colorectal cancer in the United States from 1975 to 2016 from the National Cancer Institute&#39;s Surveillance, Epidemiology, and End Results program. Thus, the GAJP model was concluded to be practically feasible to detect multiple joinpoints up to the number of grids without requirement to preassign the number of joinpoints and be easily extendable to cancer trend analysis utilizing large datasets.},
  archive      = {J_SIM},
  author       = {Seongyoon Kim and Sanghee Lee and Jung-Il Choi and Hyunsoon Cho},
  doi          = {10.1002/sim.8803},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {799-822},
  shortjournal = {Stat. Med.},
  title        = {Binary genetic algorithm for optimal joinpoint detection: Application to cancer trend analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new statistical test for latent class in censored data due
to detection limit. <em>SIM</em>, <em>40</em>(3), 779–798. (<a
href="https://doi.org/10.1002/sim.8802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomarkers of interest in urine, serum, or other biological matrices often have an assay limit of detection. When concentration levels of the biomarkers for some subjects fall below the limit, the measures for those subjects are censored. Censored data due to detection limits are very common in public health and medical research. If censored data from a single exposure group follow a normal distribution or follow a normal distribution after some transformations, Tobit regression models can be applied. Given a Tobit regression model and a detection limit, the proportion of censored data can be determined. However, in practice, it is common that the data can exhibit excessive censored observations beyond what would be expected under a Tobit regression model. One common cause is heterogeneity of the study population, that is, there exists a subpopulation who lack such biomarkers and their values are always under the detection limit, and hence are censored. In this article, we develop a new test for testing such latent class under a Tobit regression model by directly comparing the amount of observed censored data with what would be expected under the Tobit regression model. A closed form of the test statistic as well as its asymptotic properties are derived based on estimating equations. Simulation studies are conducted to investigate the performance of the new test and compare the new one with the existing ones including the Wald test, likelihood ratio test, and score test. Two real data examples are also included for illustrative purpose.},
  archive      = {J_SIM},
  author       = {Yuhan Zou and Zuoxiang Peng and Jerry Cornell and Peng Ye and Hua He},
  doi          = {10.1002/sim.8802},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {779-798},
  shortjournal = {Stat. Med.},
  title        = {A new statistical test for latent class in censored data due to detection limit},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum approximate bernstein likelihood estimation in
proportional hazard model for interval-censored data. <em>SIM</em>,
<em>40</em>(3), 758–778. (<a
href="https://doi.org/10.1002/sim.8801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum approximate Bernstein likelihood estimates of the baseline density function and the regression coefficients in the proportional hazard regression models based on interval-censored event time data result in smooth estimates of the survival functions which enjoys an almost n 1/2 -rate of convergence faster than the n 1/3 -rate for the existing estimates. The proposed method was shown by a simulation to have better finite sample performance than its main competitors. Some examples including real data are used to illustrate the usage of the proposed method.},
  archive      = {J_SIM},
  author       = {Zhong Guan},
  doi          = {10.1002/sim.8801},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {758-778},
  shortjournal = {Stat. Med.},
  title        = {Maximum approximate bernstein likelihood estimation in proportional hazard model for interval-censored data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subgroup analysis in the heterogeneous cox model.
<em>SIM</em>, <em>40</em>(3), 739–757. (<a
href="https://doi.org/10.1002/sim.8800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of censored survival data, to avoid a biased inference of treatment effects on the hazard function of the survival time, it is important to consider the treatment heterogeneity. Without requiring any prior knowledge about the subgroup structure, we propose a data driven subgroup analysis procedure for the heterogeneous Cox model by constructing a pairwise fusion penalized partial likelihood-based objective function. The proposed method can determine the number of subgroups, identify the group structure, and estimate the treatment effect simultaneously and automatically. A majorized alternating direction method of multipliers algorithm is then developed to deal with the numerically challenging high-dimensional problems. We also establish the oracle properties and the model selection consistency for the proposed penalized estimator. Our proposed method is evaluated by simulation studies and further illustrated by the analysis of the breast cancer data.},
  archive      = {J_SIM},
  author       = {Xiangbin Hu and Jian Huang and Li Liu and Defeng Sun and Xingqiu Zhao},
  doi          = {10.1002/sim.8800},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {739-757},
  shortjournal = {Stat. Med.},
  title        = {Subgroup analysis in the heterogeneous cox model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient semiparametric inference for two-phase studies
with outcome and covariate measurement errors. <em>SIM</em>,
<em>40</em>(3), 725–738. (<a
href="https://doi.org/10.1002/sim.8799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern observational studies using electronic health records or other routinely collected data, both the outcome and covariates of interest can be error-prone and their errors often correlated. A cost-effective solution is the two-phase design, under which the error-prone outcome and covariates are observed for all subjects during the first phase and that information is used to select a validation subsample for accurate measurements of these variables in the second phase. Previous research on two-phase measurement error problems largely focused on scenarios where there are errors in covariates only or the validation sample is a simple random sample of study subjects. Herein, we propose a semiparametric approach to general two-phase measurement error problems with a quantitative outcome, allowing for correlated errors in the outcome and covariates and arbitrary second-phase selection. We devise a computationally efficient and numerically stable expectation-maximization algorithm to maximize the nonparametric likelihood function. The resulting estimators possess desired statistical properties. We demonstrate the superiority of the proposed methods over existing approaches through extensive simulation studies, and we illustrate their use in an observational HIV study.},
  archive      = {J_SIM},
  author       = {Ran Tao and Sarah C. Lotspeich and Gustavo Amorim and Pamela A. Shaw and Bryan E. Shepherd},
  doi          = {10.1002/sim.8799},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {725-738},
  shortjournal = {Stat. Med.},
  title        = {Efficient semiparametric inference for two-phase studies with outcome and covariate measurement errors},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional principal component analysis for longitudinal
data with informative dropout. <em>SIM</em>, <em>40</em>(3), 712–724.
(<a href="https://doi.org/10.1002/sim.8798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal studies, the values of biomarkers are often informatively missing due to dropout. The conventional functional principal component analysis typically disregards the missing information and simply treats the unobserved data points as missing completely at random. As a result, the estimation of the mean function and the covariance surface might be biased, resulting in a biased estimation of the functional principal components. We propose the informatively missing functional principal component analysis (imFunPCA), which is well suited for cases where the longitudinal trajectories are subject to informative missingness. Computation of the functional principal components in our approach is based on the likelihood of the data, where information of both the observed and missing data points are incorporated. We adopt a regression-based orthogonal approximation method to decompose the latent stochastic process based on a set of orthonormal empirical basis functions. Under the case of informative missingness, we show via simulation studies that the performance of our approach is superior to that of the conventional ones. We apply our method on a longitudinal dataset of kidney glomerular filtration rates for patients post renal transplantation.},
  archive      = {J_SIM},
  author       = {Haolun Shi and Jianghu Dong and Liangliang Wang and Jiguo Cao},
  doi          = {10.1002/sim.8798},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {712-724},
  shortjournal = {Stat. Med.},
  title        = {Functional principal component analysis for longitudinal data with informative dropout},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive enrichment trials: What are the benefits?
<em>SIM</em>, <em>40</em>(3), 690–711. (<a
href="https://doi.org/10.1002/sim.8797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning a Phase III clinical trial, suppose a certain subset of patients is expected to respond particularly well to the new treatment. Adaptive enrichment designs make use of interim data in selecting the target population for the remainder of the trial, either continuing with the full population or restricting recruitment to the subset of patients. We define a multiple testing procedure that maintains strong control of the familywise error rate, while allowing for the adaptive sampling procedure. We derive the Bayes optimal rule for deciding whether or not to restrict recruitment to the subset after the interim analysis and present an efficient algorithm to facilitate simulation-based optimisation, enabling the construction of Bayes optimal rules in a wide variety of problem formulations. We compare adaptive enrichment designs with traditional nonadaptive designs in a broad range of examples and draw clear conclusions about the potential benefits of adaptive enrichment.},
  archive      = {J_SIM},
  author       = {Thomas Burnett and Christopher Jennison},
  doi          = {10.1002/sim.8797},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {690-711},
  shortjournal = {Stat. Med.},
  title        = {Adaptive enrichment trials: What are the benefits?},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A likelihood ratio test on temporal trends in
age-period-cohort models with applications to the disparities of heart
disease mortality among US populations and comparison with japan.
<em>SIM</em>, <em>40</em>(3), 668–689. (<a
href="https://doi.org/10.1002/sim.8796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce the recently developed intrinsic estimator method in the age-period-cohort (APC) models in examining disease incidence and mortality data, further develop a likelihood ratio (L-R) test for testing differences in temporal trends across populations, and apply the methods to examining temporal trends in the age, period or calendar time, and birth cohort of the US heart disease mortality across racial and sex groups. The temporal trends are estimated with the intrinsic estimator method to address the model identification problem, in which multiple sets of parameter estimates yield the same fitted values for a given dataset, making it difficult to conduct comparison of and hypothesis testing on the temporal trends in the age, period, and cohort across populations. We employ a penalized profile log-likelihood approach in developing the L-R test to deal with the issues of multiple estimators and the diverging number of model parameters. The identification problem also induces overparametrization of the APC model, which requires a correction of the degree of freedom of the L-R test. Monte Carlo simulation studies demonstrate that the L-R test performs well in the Type I error calculation and is powerful to detect differences in the age or period trends. The L-R test further reveals disparities of heart disease mortality among the US populations and between the US and Japanese populations.},
  archive      = {J_SIM},
  author       = {Wenjiang Fu and Junyu Ding and Kuikui Gao and Shuangge Ma and Lu Tian},
  doi          = {10.1002/sim.8796},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {668-689},
  shortjournal = {Stat. Med.},
  title        = {A likelihood ratio test on temporal trends in age-period-cohort models with applications to the disparities of heart disease mortality among US populations and comparison with japan},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Functional principal component based landmark analysis for
the effects of longitudinal cholesterol profiles on the risk of coronary
heart disease. <em>SIM</em>, <em>40</em>(3), 650–667. (<a
href="https://doi.org/10.1002/sim.8794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients&#39; longitudinal biomarker changing patterns are crucial factors for their disease progression. In this research, we apply functional principal component analysis techniques to extract these changing patterns and use them as predictors in landmark models for dynamic prediction. The time-varying effects of risk factors along a sequence of landmark times are smoothed by a supermodel to borrow information from neighbor time intervals. This results in more stable estimation and more clear demonstration of the time-varying effects. Compared with the traditional landmark analysis, simulation studies show our proposed approach results in lower prediction error rates and higher area under receiver operating characteristic curve (AUC) values, which indicate better ability to discriminate between subjects with different risk levels. We apply our method to data from the Framingham Heart Study, using longitudinal total cholesterol (TC) levels to predict future coronary heart disease (CHD) risk profiles. Our approach not only obtains the overall trend of biomarker-related risk profiles, but also reveals different risk patterns that are not available from the traditional landmark analyses. Our results show that high cholesterol levels during young ages are more harmful than those in old ages. This demonstrates the importance of analyzing the age-dependent effects of TC on CHD risk.},
  archive      = {J_SIM},
  author       = {Bin Shi and Peng Wei and Xuelin Huang},
  doi          = {10.1002/sim.8794},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {650-667},
  shortjournal = {Stat. Med.},
  title        = {Functional principal component based landmark analysis for the effects of longitudinal cholesterol profiles on the risk of coronary heart disease},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Raking and regression calibration: Methods to address bias
from correlated covariate and time-to-event error. <em>SIM</em>,
<em>40</em>(3), 631–649. (<a
href="https://doi.org/10.1002/sim.8793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical studies that depend on electronic health records (EHR) data are often subject to measurement error, as the data are not collected to support research questions under study. These data errors, if not accounted for in study analyses, can obscure or cause spurious associations between patient exposures and disease risk. Methodology to address covariate measurement error has been well developed; however, time-to-event error has also been shown to cause significant bias, but methods to address it are relatively underdeveloped. More generally, it is possible to observe errors in both the covariate and the time-to-event outcome that are correlated. We propose regression calibration (RC) estimators to simultaneously address correlated error in the covariates and the censored event time. Although RC can perform well in many settings with covariate measurement error, it is biased for nonlinear regression models, such as the Cox model. Thus, we additionally propose raking estimators which are consistent estimators of the parameter defined by the population estimating equation. Raking can improve upon RC in certain settings with failure-time data, require no explicit modeling of the error structure, and can be utilized under outcome-dependent sampling designs. We discuss features of the underlying estimation problem that affect the degree of improvement the raking estimator has over the RC approach. Detailed simulation studies are presented to examine the performance of the proposed estimators under varying levels of signal, error, and censoring. The methodology is illustrated on observational EHR data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.},
  archive      = {J_SIM},
  author       = {Eric J. Oh and Bryan E. Shepherd and Thomas Lumley and Pamela A. Shaw},
  doi          = {10.1002/sim.8793},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {631-649},
  shortjournal = {Stat. Med.},
  title        = {Raking and regression calibration: Methods to address bias from correlated covariate and time-to-event error},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confounder selection strategies targeting stable treatment
effect estimators. <em>SIM</em>, <em>40</em>(3), 607–630. (<a
href="https://doi.org/10.1002/sim.8792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring the causal effect of a treatment on an outcome in an observational study requires adjusting for observed baseline confounders to avoid bias. However, adjusting for all observed baseline covariates, when only a subset are confounders of the effect of interest, is known to yield potentially inefficient and unstable estimators of the treatment effect. Furthermore, it raises the risk of finite-sample bias and bias due to model misspecification. For these stated reasons, confounder (or covariate) selection is commonly used to determine a subset of the available covariates that is sufficient for confounding adjustment. In this article, we propose a confounder selection strategy that focuses on stable estimation of the treatment effect. In particular, when the propensity score (PS) model already includes covariates that are sufficient to adjust for confounding, then the addition of covariates that are associated with either treatment or outcome alone, but not both, should not systematically change the effect estimator. The proposal, therefore, entails first prioritizing covariates for inclusion in the PS model, then using a change-in-estimate approach to select the smallest adjustment set that yields a stable effect estimate. The ability of the proposal to correctly select confounders, and to ensure valid inference of the treatment effect following data-driven covariate selection, is assessed empirically and compared with existing methods using simulation studies. We demonstrate the procedure using three different publicly available datasets commonly used for causal inference.},
  archive      = {J_SIM},
  author       = {Wen Wei Loh and Stijn Vansteelandt},
  doi          = {10.1002/sim.8792},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {607-630},
  shortjournal = {Stat. Med.},
  title        = {Confounder selection strategies targeting stable treatment effect estimators},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A likelihood-based approach to assessing frequency of
pathogenicity among variants of unknown significance in susceptibility
genes. <em>SIM</em>, <em>40</em>(3), 593–606. (<a
href="https://doi.org/10.1002/sim.8791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercialized multigene panel testing brings unprecedented opportunities to understand germline genetic contributions to hereditary cancers. Most genetic testing companies classify the pathogenicity of variants as pathogenic, benign, or variants of unknown significance (VUSs). The unknown pathogenicity of VUSs poses serious challenges to clinical decision-making. This study aims to assess the frequency of VUSs that are likely pathogenic in disease-susceptibility genes. Using estimates of probands&#39; probability of having a pathogenic mutation (ie, the carrier score) based on a family history probabilistic risk prediction model, we assume the carrier score distribution for probands with VUSs is a mixture of the carrier score distribution for probands with positive results and the carrier score distribution for probands with negative results. Under this mixture model, we propose a likelihood-based approach to assess the frequency of pathogenicity among probands with VUSs, while accounting for the existence of possible pathogenic mutations on genes not tested. We conducted simulations to assess the performance of the approach and show that under various settings, the approach performs well with very little bias in the estimated proportion of VUSs that are likely pathogenic. We also estimate the positive predictive value across the entire range of carrier scores. We apply our approach to the USC-Stanford Hereditary Cancer Panel Testing cohort, and estimate the proportion of probands that have VUSs in BRCA1/2 that are likely pathogenic to be 10.12% [95%CI: 0%, 43.04%]. This approach will enable clinicians to target high-risk patients who have VUSs, allowing for early prevention interventions.},
  archive      = {J_SIM},
  author       = {Yunqi Yang and Christine Hong and Jane W. Liang and Stephen Gruber and Giovanni Parmigiani and Gregory Idos and Danielle Braun},
  doi          = {10.1002/sim.8791},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {593-606},
  shortjournal = {Stat. Med.},
  title        = {A likelihood-based approach to assessing frequency of pathogenicity among variants of unknown significance in susceptibility genes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Serial correlation structures in latent linear mixed models
for analysis of multivariate longitudinal ordinal responses.
<em>SIM</em>, <em>40</em>(3), 578–592. (<a
href="https://doi.org/10.1002/sim.8790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a latent linear mixed model to analyze multivariate longitudinal data of multiple ordinal variables, which are manifestations of fewer continuous latent variables. We focus on the latent level where the effects of observed covariates on the latent variables are of interest. We incorporate serial correlation into the variance component rather than assuming independent residuals. We show that misleading inference may be drawn when misspecifying the variance component. Furthermore, we provide a graphical tool depicting latent empirical semi-variograms to detect serial correlation for latent stationary linear mixed models. We apply our proposed model to examine the treatment effect on patients having the amyotrophic lateral sclerosis disease. The result shows that the treatment can slow down progression of latent cervical and lumbar functions.},
  archive      = {J_SIM},
  author       = {Trung Dung Tran and Emmanuel Lesaffre and Geert Verbeke and Geert Molenberghs},
  doi          = {10.1002/sim.8790},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {578-592},
  shortjournal = {Stat. Med.},
  title        = {Serial correlation structures in latent linear mixed models for analysis of multivariate longitudinal ordinal responses},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matching-adjusted indirect comparisons: Application to
time-to-event data. <em>SIM</em>, <em>40</em>(3), 566–577. (<a
href="https://doi.org/10.1002/sim.8789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Matching-Adjusted Indirect Comparison method (MAIC) is a recent methodology that allows to perform indirect comparisons between two drugs assessed in two different studies, where individual patients data are available in only one of the two studies, the data of the other one being available in an aggregate format only. In this work, we have assessed the properties of the MAIC method and compared, through simulations, several ways of practical implementation of the method. We conclude that it is more efficient to match the treatment arms separately (match the two drugs to compare on one hand, and the control arms on the other hand) and use the Lasso technique to select the covariates for the matching step is better than matching a maximal set of covariates.},
  archive      = {J_SIM},
  author       = {Jihane Aouni and Nadia Gaudel-Dedieu and Bernard Sebastien},
  doi          = {10.1002/sim.8789},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {566-577},
  shortjournal = {Stat. Med.},
  title        = {Matching-adjusted indirect comparisons: Application to time-to-event data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Net benefit in the presence of correlated prioritized
outcomes using generalized pairwise comparisons: A simulation study.
<em>SIM</em>, <em>40</em>(3), 553–565. (<a
href="https://doi.org/10.1002/sim.8788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Joris Giai and Delphine Maucort-Boulch and Brice Ozenne and Jean-Christophe Chiêm and Marc Buyse and Julien Péron},
  doi          = {10.1002/sim.8788},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {553-565},
  shortjournal = {Stat. Med.},
  title        = {Net benefit in the presence of correlated prioritized outcomes using generalized pairwise comparisons: A simulation study},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A practical introduction to bayesian estimation of causal
effects: Parametric and nonparametric approaches. <em>SIM</em>,
<em>40</em>(2), 518–551. (<a
href="https://doi.org/10.1002/sim.8761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Substantial advances in Bayesian methods for causal inference have been made in recent years. We provide an introduction to Bayesian inference for causal effects for practicing statisticians who have some familiarity with Bayesian models and would like an overview of what it can add to causal estimation in practical settings. In the paper, we demonstrate how priors can induce shrinkage and sparsity in parametric models and be used to perform probabilistic sensitivity analyses around causal assumptions. We provide an overview of nonparametric Bayesian estimation and survey their applications in the causal inference literature. Inference in the point-treatment and time-varying treatment settings are considered. For the latter, we explore both static and dynamic treatment regimes. Throughout, we illustrate implementation using off-the-shelf open source software. We hope to leave the reader with implementation-level knowledge of Bayesian causal inference using both parametric and nonparametric models. All synthetic examples and code used in the paper are publicly available on a companion GitHub repository.},
  archive      = {J_SIM},
  author       = {Arman Oganisian and Jason A. Roy},
  doi          = {10.1002/sim.8761},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {518-551},
  shortjournal = {Stat. Med.},
  title        = {A practical introduction to bayesian estimation of causal effects: Parametric and nonparametric approaches},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Clinical prediction models to predict the risk of multiple
binary outcomes: A comparison of approaches. <em>SIM</em>,
<em>40</em>(2), 498–517. (<a
href="https://doi.org/10.1002/sim.8787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical prediction models (CPMs) can predict clinically relevant outcomes or events. Typically, prognostic CPMs are derived to predict the risk of a single future outcome. However, there are many medical applications where two or more outcomes are of interest, meaning this should be more widely reflected in CPMs so they can accurately estimate the joint risk of multiple outcomes simultaneously. A potentially naïve approach to multi-outcome risk prediction is to derive a CPM for each outcome separately, then multiply the predicted risks. This approach is only valid if the outcomes are conditionally independent given the covariates, and it fails to exploit the potential relationships between the outcomes. This paper outlines several approaches that could be used to develop CPMs for multiple binary outcomes. We consider four methods, ranging in complexity and conditional independence assumptions: namely, probabilistic classifier chain, multinomial logistic regression, multivariate logistic regression, and a Bayesian probit model. These are compared with methods that rely on conditional independence: separate univariate CPMs and stacked regression. Employing a simulation study and real-world example, we illustrate that CPMs for joint risk prediction of multiple outcomes should only be derived using methods that model the residual correlation between outcomes. In such a situation, our results suggest that probabilistic classification chains, multinomial logistic regression or the Bayesian probit model are all appropriate choices. We call into question the development of CPMs for each outcome in isolation when multiple correlated or structurally related outcomes are of interest and recommend more multivariate approaches to risk prediction.},
  archive      = {J_SIM},
  author       = {Glen P. Martin and Matthew Sperrin and Kym I. E. Snell and Iain Buchan and Richard D. Riley},
  doi          = {10.1002/sim.8787},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {498-517},
  shortjournal = {Stat. Med.},
  title        = {Clinical prediction models to predict the risk of multiple binary outcomes: A comparison of approaches},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spline-based accelerated failure time model. <em>SIM</em>,
<em>40</em>(2), 481–497. (<a
href="https://doi.org/10.1002/sim.8786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated failure time (AFT) model has been suggested as an alternative to the Cox proportional hazards model. However, a parametric AFT model requires the specification of an appropriate distribution for the event time, which is often difficult to identify in real-life studies and may limit applications. A semiparametric AFT model was developed by Komárek et al based on smoothed error distribution that does not require such specification. In this article, we develop a spline-based AFT model that also does not require specification of the parametric family of event time distribution. The baseline hazard function is modeled by regression B-splines, allowing for the estimation of a variety of smooth and flexible shapes. In comprehensive simulations, we validate the performance of our approach and compare with the results from parametric AFT models and the approach of Komárek. Both the proposed spline-based AFT model and the approach of Komárek provided unbiased estimates of covariate effects and survival curves for a variety of scenarios in which the event time followed different distributions, including both simple and complex cases. Spline-based estimates of the baseline hazard showed also a satisfactory numerical stability. As expected, the baseline hazard and survival probabilities estimated by the misspecified parametric AFT models deviated from the truth. We illustrated the application of the proposed model in a study of colon cancer.},
  archive      = {J_SIM},
  author       = {Menglan Pang and Robert W. Platt and Tibor Schuster and Michal Abrahamowicz},
  doi          = {10.1002/sim.8786},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {481-497},
  shortjournal = {Stat. Med.},
  title        = {Spline-based accelerated failure time model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustered spatio-temporal varying coefficient regression
model. <em>SIM</em>, <em>40</em>(2), 465–480. (<a
href="https://doi.org/10.1002/sim.8785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In regression analysis for spatio-temporal data, identifying clusters of spatial units over time in a regression coefficient could provide insight into the unique relationship between a response and covariates in certain subdomains of space and time windows relative to the background in other parts of the spatial domain and the time period of interest. In this article, we propose a varying coefficient regression method for spatial data repeatedly sampled over time, with heterogeneity in regression coefficients across both space and over time. In particular, we extend a varying coefficient regression model for spatial-only data to spatio-temporal data with flexible temporal patterns. We consider the detection of a potential cylindrical cluster of regression coefficients based on testing whether the regression coefficient is the same or not over the entire spatial domain for each time point. For multiple clusters, we develop a sequential identification approach. We assess the power and identification of known clusters via a simulation study. Our proposed methodology is illustrated by the analysis of a cancer mortality dataset in the Southeast of the U.S.},
  archive      = {J_SIM},
  author       = {Junho Lee and Maria E. Kamenetsky and Ronald E. Gangnon and Jun Zhu},
  doi          = {10.1002/sim.8785},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {465-480},
  shortjournal = {Stat. Med.},
  title        = {Clustered spatio-temporal varying coefficient regression model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A markov chain approach for ranking treatments in network
meta-analysis. <em>SIM</em>, <em>40</em>(2), 451–464. (<a
href="https://doi.org/10.1002/sim.8784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When interpreting the relative effects from a network meta-analysis (NMA), researchers are usually aware of the potential limitations that may render the results for some comparisons less useful or meaningless. In the presence of sufficient and appropriate data, some of these limitations (eg, risk of bias, small-study effects, publication bias) can be taken into account in the statistical analysis. Very often, though, the necessary data for applying these methods are missing and data limitations cannot be formally integrated into ranking. In addition, there are other important characteristics of the treatment comparisons that cannot be addressed within a statistical model but only through qualitative judgments; for example, the relevance of data to the research question, the plausibility of the assumptions, and so on. Here, we propose a new measure for treatment ranking called the Probability of Selecting a Treatment to Recommend (POST-R). We suggest that the order of treatments should represent the process of considering treatments for selection in clinical practice and we assign to each treatment a probability of being selected. This process can be considered as a Markov chain model that allows the end-users of NMA to select the most appropriate treatments based not only on the NMA results but also to information external to the NMA. In this way, we obtain rankings that can inform decision-making more efficiently as they represent not only the relative effects but also their potential limitations. We illustrate our approach using a NMA comparing treatments for chronic plaque psoriasis and we provide the Stata commands.},
  archive      = {J_SIM},
  author       = {Anna Chaimani and Raphaël Porcher and Émilie Sbidian and Dimitris Mavridis},
  doi          = {10.1002/sim.8784},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {451-464},
  shortjournal = {Stat. Med.},
  title        = {A markov chain approach for ranking treatments in network meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampling-based estimation for massive survival data with
additive hazards model. <em>SIM</em>, <em>40</em>(2), 441–450. (<a
href="https://doi.org/10.1002/sim.8783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For massive survival data, we propose a subsampling algorithm to efficiently approximate the estimates of regression parameters in the additive hazards model. We establish consistency and asymptotic normality of the subsample-based estimator given the full data. The optimal subsampling probabilities are obtained via minimizing asymptotic variance of the resulting estimator. The subsample-based procedure can largely reduce the computational cost compared with the full data method. In numerical simulations, our method has low bias and satisfactory coverage probabilities. We provide an illustrative example on the survival analysis of patients with lymphoma cancer from the Surveillance, Epidemiology, and End Results Program.},
  archive      = {J_SIM},
  author       = {Lulu Zuo and Haixiang Zhang and HaiYing Wang and Lei Liu},
  doi          = {10.1002/sim.8783},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {441-450},
  shortjournal = {Stat. Med.},
  title        = {Sampling-based estimation for massive survival data with additive hazards model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A class of generalized linear mixed models adjusted for
marginal interpretability. <em>SIM</em>, <em>40</em>(2), 427–440. (<a
href="https://doi.org/10.1002/sim.8782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two popular approaches for relating correlated measurements of a non-Gaussian response variable to a set of predictors are to fit a marginal model using generalized estimating equations and to fit a generalized linear mixed model (GLMM) by introducing latent random variables. The first approach is effective for parameter estimation, but leaves one without a formal model for the data with which to assess quality of fit or make individual-level predictions for future observations. The second approach overcomes these deficiencies, but leads to parameter estimates that must be interpreted conditional on the latent variables. To obtain marginal summaries, one needs to evaluate an analytically intractable integral or use attenuation factors as an approximation. Further, we note an unpalatable implication of the standard GLMM. To resolve these issues, we turn to a class of marginally interpretable GLMMs that lead to parameter estimates with a marginal interpretation while maintaining the desirable statistical properties of a conditionally specified model and avoiding problematic implications. We establish the form of these models under the most commonly used link functions and address computational issues. For logistic mixed effects models, we introduce an accurate and efficient method for evaluating the logistic-normal integral.},
  archive      = {J_SIM},
  author       = {Jeffrey J. Gory and Peter F. Craigmile and Steven N. MacEachern},
  doi          = {10.1002/sim.8782},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {427-440},
  shortjournal = {Stat. Med.},
  title        = {A class of generalized linear mixed models adjusted for marginal interpretability},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of various estimators for standardized mean
difference in meta-analysis. <em>SIM</em>, <em>40</em>(2), 403–426. (<a
href="https://doi.org/10.1002/sim.8781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analyses of a treatment&#39;s effect compared with a control frequently calculate the meta-effect from standardized mean differences (SMDs). SMDs are usually estimated by Cohen&#39;s d or Hedges&#39; g . Cohen&#39;s d divides the difference between sample means of a continuous response by the pooled standard deviation, but is subject to nonnegligible bias for small sample sizes. Hedges&#39; g removes this bias with a correction factor. The current literature (including meta-analysis books and software packages) is confusingly inconsistent about methods for synthesizing SMDs, potentially making reproducibility a problem. Using conventional methods, the variance estimate of SMD is associated with the point estimate of SMD, so Hedges&#39; g is not guaranteed to be unbiased in meta-analyses. This article comprehensively reviews and evaluates available methods for synthesizing SMDs. Their performance is compared using extensive simulation studies and analyses of actual datasets. We find that because of the intrinsic association between point estimates and standard errors, the usual version of Hedges&#39; g can result in more biased meta-estimation than Cohen&#39;s d . We recommend using average-adjusted variance estimators to obtain an unbiased meta-estimate, and the Hartung-Knapp-Sidik-Jonkman method for accurate estimation of its confidence interval.},
  archive      = {J_SIM},
  author       = {Lifeng Lin and Ariel M. Aloe},
  doi          = {10.1002/sim.8781},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {403-426},
  shortjournal = {Stat. Med.},
  title        = {Evaluation of various estimators for standardized mean difference in meta-analysis},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian adaptive phase i/II platform trial design for
pediatric immunotherapy trials. <em>SIM</em>, <em>40</em>(2), 382–402.
(<a href="https://doi.org/10.1002/sim.8780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapy is the most promising new cancer treatment for various pediatric tumors and has resulted in an unprecedented surge in the number of novel immunotherapeutic treatments that need to be evaluated in clinical trials. Most phase I/II trial designs have been developed for evaluating only one candidate treatment at a time, and are thus not optimal for this task. To address these issues, we propose a Bayesian phase I/II platform trial design, which accounts for the unique features of immunotherapy, thereby allowing investigators to continuously screen a large number of immunotherapeutic treatments in an efficient and seamless manner. The elicited numerical utility is adopted to account for the risk-benefit trade-off and to quantify the desirability of the dose. During the trial, inefficacious or overly toxic treatments are adaptively dropped from the trial and the promising treatments are graduated from the trial to the next stage of development. Once an experimental treatment is dropped or graduated, the next available new treatment can be immediately added and tested. Extensive simulation studies have demonstrated the desirable operating characteristics of the proposed design.},
  archive      = {J_SIM},
  author       = {Rongji Mu and Haitao Pan and Guoying Xu},
  doi          = {10.1002/sim.8780},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {382-402},
  shortjournal = {Stat. Med.},
  title        = {A bayesian adaptive phase I/II platform trial design for pediatric immunotherapy trials},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection of variables for multivariable models:
Opportunities and limitations in quantifying model stability by
resampling. <em>SIM</em>, <em>40</em>(2), 369–381. (<a
href="https://doi.org/10.1002/sim.8779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models are often fitted to obtain a concise description of the association of an outcome variable with some covariates. Even if background knowledge is available to guide preselection of covariates, stepwise variable selection is commonly applied to remove irrelevant ones. This practice may introduce additional variability and selection is rarely certain. However, these issues are often ignored and model stability is not questioned. Several resampling-based measures were proposed to describe model stability, including variable inclusion frequencies (VIFs), model selection frequencies, relative conditional bias (RCB), and root mean squared difference ratio (RMSDR). The latter two were recently proposed to assess bias and variance inflation induced by variable selection. Here, we study the consistency and accuracy of resampling estimates of these measures and the optimal choice of the resampling technique. In particular, we compare subsampling and bootstrapping for assessing stability of linear, logistic, and Cox models obtained by backward elimination in a simulation study. Moreover, we exemplify the estimation and interpretation of all suggested measures in a study on cardiovascular risk. The VIF and the model selection frequency are only consistently estimated in the subsampling approach. By contrast, the bootstrap is advantageous in terms of bias and precision for estimating the RCB as well as the RMSDR. Though, unbiased estimation of the latter quantity requires independence of covariates, which is rarely encountered in practice. Our study stresses the importance of addressing model stability after variable selection and shows how to cope with it.},
  archive      = {J_SIM},
  author       = {Christine Wallisch and Daniela Dunkler and Geraldine Rauch and Riccardo de Bin and Georg Heinze},
  doi          = {10.1002/sim.8779},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {369-381},
  shortjournal = {Stat. Med.},
  title        = {Selection of variables for multivariable models: Opportunities and limitations in quantifying model stability by resampling},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The implications of noncompliance for randomized trials with
partial nesting due to group treatment. <em>SIM</em>, <em>40</em>(2),
349–368. (<a href="https://doi.org/10.1002/sim.8778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyses of trials of group administered treatments require an identifier for therapy group to account for clustering by group. All patients randomized to receive the group administered treatment could be assigned an intended group identifier following randomization. Alternatively, an actual group could be based on those patients that comply with group therapy. We investigate the implications for intention-to-treat (ITT) analyses of using either the intended or actual group to adjust for the clustering effect. We also consider causal models using the actual group. A simulation study showed that ITT estimates based on random effects models or GEE with an exchangeable correlation matrix performed much better when using the intended group than the actual group. OLS with robust standard errors performed well with both. Most compliance average causal effect (CACE) models performed well. While practical constraints of the clinical setting may determine the choice between an intended or actual group analyses, it is desirable to record both. An ITT analysis using mixed models can then be fitted using the intended group with data generation assumptions checked by a causal model using the actual group. Where an ITT analysis is based on the actual group, worse outcome for never-takers than compliers may allow one to infer that some estimators are biased toward no treatment effect. The work here is motivated and illustrated by a trial of a group therapy, but also has relevance to trials with treatment related clustering due to therapist examples of which include physical and talking therapies or surgery.},
  archive      = {J_SIM},
  author       = {Chris Roberts},
  doi          = {10.1002/sim.8778},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {349-368},
  shortjournal = {Stat. Med.},
  title        = {The implications of noncompliance for randomized trials with partial nesting due to group treatment},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for considering the risk-benefit trade-off in
designing noninferiority trials using composite outcome approaches.
<em>SIM</em>, <em>40</em>(2), 327–348. (<a
href="https://doi.org/10.1002/sim.8777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a new treatment regimen is expected to have comparable or slightly worse efficacy to that of the control regimen but has benefits in other domains such as safety and tolerability, a noninferiority (NI) trial may be appropriate but is fraught with difficulty in justifying an acceptable NI margin that is based on both clinical and statistical input. To overcome this, we propose to utilize composite risk-benefit outcomes that combine elements from domains of importance (eg, efficacy, safety, and tolerability). The composite outcome itself may be analyzed using a superiority framework, or it can be used as a tool at the design stage of a NI trial for selecting an NI margin for efficacy that balances changes in risks and benefits. In the latter case, the choice of NI margin may be based on a novel quantity called the maximum allowable decrease in efficacy (MADE), defined as the marginal difference in efficacy between arms that would yield a null treatment effect for the composite outcome given an assumed distribution for the composite outcome. We observe that MADE: (1) is larger when the safety improvement for the experimental arm is larger, (2) depends on the association between the efficacy and safety outcomes, and (3) depends on the control arm efficacy rate. We use a numerical example for power comparisons between a superiority test for the composite outcome vs a noninferiority test for efficacy using the MADE as the NI margin, and apply the methods to a TB treatment trial.},
  archive      = {J_SIM},
  author       = {Grace Montepiedra and Ritesh Ramchandani and Sachiko Miyahara and Soyeon Kim},
  doi          = {10.1002/sim.8777},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {327-348},
  shortjournal = {Stat. Med.},
  title        = {A framework for considering the risk-benefit trade-off in designing noninferiority trials using composite outcome approaches},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis considerations for utilizing a mapping
function in a small sample, sequential, multiple assignment, randomized
trials with continuous outcomes. <em>SIM</em>, <em>40</em>(2), 312–326.
(<a href="https://doi.org/10.1002/sim.8776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small sample, sequential, multiple assignment, randomized trials (snSMARTs) are multistage trials with the overall goal of determining the best treatment after a fixed amount of time. In snSMART trials, patients are first randomized to one of three treatments and a binary (e.g. response/nonresponse) outcome is measured at the end of the first stage. Responders to first stage treatment continue their treatment. Nonresponders to first stage treatment are rerandomized to one of the remaining treatments. The same binary outcome is measured at the end of the first and second stages, and data from both stages are pooled together to find the best first stage treatment. However, in many settings the primary endpoint may be continuous, and dichotomizing this continuous variable may reduce statistical efficiency. In this article, we extend the snSMART design and methods to allow for continuous outcomes. Instead of requiring a binary outcome at the first stage for rerandomization, the probability of staying on the same treatment or switching treatment is a function of the first stage outcome. Rerandomization based on a mapping function of a continuous outcome allows for snSMART designs without requiring a binary outcome. We perform simulation studies to compare the proposed design with continuous outcomes to standard snSMART designs with binary outcomes. The proposed design results in more efficient treatment effect estimates and similar outcomes for trial patients.},
  archive      = {J_SIM},
  author       = {Holly Hartman and Roy N. Tamura and Matthew J Schipper and Kelley M. Kidwell},
  doi          = {10.1002/sim.8776},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {312-326},
  shortjournal = {Stat. Med.},
  title        = {Design and analysis considerations for utilizing a mapping function in a small sample, sequential, multiple assignment, randomized trials with continuous outcomes},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Robust estimation in the nested case-control design under a
misspecified covariate functional form. <em>SIM</em>, <em>40</em>(2),
299–311. (<a href="https://doi.org/10.1002/sim.8775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazards model is typically used to analyze time-to-event data. If the event of interest is rare and covariates are difficult or expensive to collect, the nested case-control (NCC) design provides consistent estimates at reduced costs with minimal impact on precision if the model is specified correctly. If our scientific goal is to conduct inference regarding an association of interest, it is essential that we specify the model a priori to avoid multiple testing bias. We cannot, however, be certain that all assumptions will be satisfied so it is important to consider robustness of the NCC design under model misspecification. In this manuscript, we show that in finite sample settings where the functional form of a covariate of interest is misspecified, the estimates resulting from the partial likelihood estimator under the NCC design depend on the number of controls sampled at each event time. To account for this dependency, we propose an estimator that recovers the results obtained using using the full cohort, where full covariate information is available for all study participants. We present the utility of our estimator using simulation studies and show the theoretical properties. We end by applying our estimator to motivating data from the Alzheimer&#39;s Disease Neuroimaging Initiative.},
  archive      = {J_SIM},
  author       = {Michelle M. Nuño and Daniel L. Gillen},
  doi          = {10.1002/sim.8775},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {299-311},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation in the nested case-control design under a misspecified covariate functional form},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing left-truncated and right-censored infectious
disease cohort data with interval-censored infection onset.
<em>SIM</em>, <em>40</em>(2), 287–298. (<a
href="https://doi.org/10.1002/sim.8774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an infectious disease cohort study, individuals who have been infected with a pathogen are often recruited for follow up. The period between infection and the onset of symptomatic disease, referred to as the incubation period, is of interest because of its importance on disease surveillance and control. However, the incubation period is often difficult to ascertain due to the uncertainty associated with asymptomatic infection onset time. An additional complication is that the observed infected subjects are likely to have longer incubation periods due to the prevalent sampling. In this article, we demonstrate how to estimate the distribution of the incubation period with the uncertain infection onset, subject to left-truncation and right-censoring. We employ a family of sufficiently general parametric models, the generalized odds-rate class of regression models, for the underlying incubation period and its correlation with covariates. In simulation studies, we assess the finite sample performance of the model fitting and hazard function estimation. The proposed method is illustrated on data from the HIV/AIDS study on injection drug users admitted to a detoxification program in Badalona, Spain.},
  archive      = {J_SIM},
  author       = {Daewoo Pak and Jun Liu and Jing Ning and Guadalupe Gómez and Yu Shen},
  doi          = {10.1002/sim.8774},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {287-298},
  shortjournal = {Stat. Med.},
  title        = {Analyzing left-truncated and right-censored infectious disease cohort data with interval-censored infection onset},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regression calibration to correct correlated errors in
outcome and exposure. <em>SIM</em>, <em>40</em>(2), 271–286. (<a
href="https://doi.org/10.1002/sim.8773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement error arises through a variety of mechanisms. A rich literature exists on the bias introduced by covariate measurement error and on methods of analysis to address this bias. By comparison, less attention has been given to errors in outcome assessment and nonclassical covariate measurement error. We consider an extension of the regression calibration method to settings with errors in a continuous outcome, where the errors may be correlated with prognostic covariates or with covariate measurement error. This method adjusts for the measurement error in the data and can be applied with either a validation subset, on which the true data are also observed (eg, a study audit), or a reliability subset, where a second observation of error prone measurements are available. For each case, we provide conditions under which the proposed method is identifiable and leads to consistent estimates of the regression parameter. When the second measurement on the reliability subset has no error or classical unbiased measurement error, the proposed method is consistent even when the primary outcome and exposures of interest are subject to both systematic and random error. We examine the performance of the method with simulations for a variety of measurement error scenarios and sizes of the reliability subset. We illustrate the method&#39;s application using data from the Women&#39;s Health Initiative Dietary Modification Trial.},
  archive      = {J_SIM},
  author       = {Pamela A. Shaw and Jiwei He and Bryan E. Shepherd},
  doi          = {10.1002/sim.8773},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {271-286},
  shortjournal = {Stat. Med.},
  title        = {Regression calibration to correct correlated errors in outcome and exposure},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Selection models for efficient two-phase design of family
studies. <em>SIM</em>, <em>40</em>(2), 254–270. (<a
href="https://doi.org/10.1002/sim.8772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Family studies routinely employ biased sampling schemes in which individuals are randomly chosen from a disease registry and genetic and phenotypic data are obtained from their consenting relatives. We view this as a two-phase study and propose the use of an efficient selection model for the recruitment of families to form a phase II sample subject to budgetary constraints. Simple random sampling, balanced sampling and use of an approximately optimal selection model are considered where the latter is chosen to minimize the variance of parameters of interest. We consider the setting where family members provide current status data with respect to the disease and use copula models to address within-family dependence. The efficiency gains from the use of an optimal selection model over simple random sampling and balanced sampling schemes are investigated as is the robustness of optimal sampling to model misspecification. An application to a family study on psoriatic arthritis is given for illustration.},
  archive      = {J_SIM},
  author       = {Yujie Zhong and Richard J. Cook},
  doi          = {10.1002/sim.8772},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {254-270},
  shortjournal = {Stat. Med.},
  title        = {Selection models for efficient two-phase design of family studies},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Early phase dose-finding trials in virology. <em>SIM</em>,
<em>40</em>(2), 240–253. (<a
href="https://doi.org/10.1002/sim.8771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Little has been published in terms of dose-finding methodology in virology. Aside from a few papers focusing on HIV, the considerable progress in dose-finding methodology of the last 25 years has focused almost entirely on oncology. While adverse reactions to cytotoxic drugs may be life threatening, for anti-viral agents we anticipate something different: side effects that provoke the cessation of treatment. This would correspond to treatment failure. On the other hand, success would not be yes/no but would correspond to a range of responses, from small, no more than say 20% reduction in viral load to the complete elimination of the virus. Less than total success matters since this may allow the patient to achieve immune-mediated clearance. The motivation for this article is an upcoming dose-finding trial in chronic norovirus infection. We propose a novel methodology whose goal is twofold: first, to identify the dose that provides the most favorable distribution of treatment outcomes, and, second, to do this in a way that maximizes the treatment benefit for the patients included in the study.},
  archive      = {J_SIM},
  author       = {Hakim-Moulay Dehbi and David M. Lowe and John O&#39;Quigley},
  doi          = {10.1002/sim.8771},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {240-253},
  shortjournal = {Stat. Med.},
  title        = {Early phase dose-finding trials in virology},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploratory assessment of treatment-dependent random-effects
distribution using gradient functions. <em>SIM</em>, <em>40</em>(2),
226–239. (<a href="https://doi.org/10.1002/sim.8770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In analyzing repeated measurements from randomized controlled trials with mixed-effects models, it is important to carefully examine the conventional normality assumption regarding the random-effects distribution and its dependence on treatment allocation in order to avoid biased estimation and correctly interpret the estimated random-effects distribution. In this article, we propose the use of a gradient function method in modeling with the different random-effects distributions depending on the treatment allocation. This method can be effective for considering in advance whether a proper fit requires a model that allows dependence of the random-effects distribution on covariates, or for finding the subpopulations in the random effects.},
  archive      = {J_SIM},
  author       = {Takumi Imai and Shiro Tanaka and Koji Kawakami},
  doi          = {10.1002/sim.8770},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {226-239},
  shortjournal = {Stat. Med.},
  title        = {Exploratory assessment of treatment-dependent random-effects distribution using gradient functions},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and visualizing two-way contingency tables using
compositional data analysis: A case-study on individual self-prediction
of migraine days. <em>SIM</em>, <em>40</em>(2), 213–225. (<a
href="https://doi.org/10.1002/sim.8769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-way contingency tables arise in many fields, such as in medical studies, where the relation between two discrete random variables or responses is to be assessed. We propose to analyze and visualize a sample of 2 × 2 tables in the context of single-subject repeated measurements design by means of compositional data (CoDa) methods. First, we propose to visualize the tables in a quaternary diagram. Second, we show how to represent these tables by means of logratios indicating the relationship between the two variables as well as their strength and direction of dependency. Finally, we describe a technique to model those tables with a simplicial regression model. Data from a real-world study of self-prediction of migraine attack onset is used to illustrate this methodology. For each individual, the 2 × 2 table of their migraine expectation vs next day migraine occurrence is computed, generating a sample of tables. Then we visualize and interpret the prediction ability of individuals both in the simplex and in terms of logratios of components. Finally, we model the self-prediction ability with respect to demographic variables, days tracked and disease characteristics. Our application demonstrates that CoDa can be a useful tool for visualizing, modeling, and interpreting the components of 2 × 2 tables.},
  archive      = {J_SIM},
  author       = {Marina Vives-Mestres and Amparo Casanova},
  doi          = {10.1002/sim.8769},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {213-225},
  shortjournal = {Stat. Med.},
  title        = {Modeling and visualizing two-way contingency tables using compositional data analysis: A case-study on individual self-prediction of migraine days},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of time-to-event for observational studies:
Guidance to the use of intensity models. <em>SIM</em>, <em>40</em>(1),
185–211. (<a href="https://doi.org/10.1002/sim.8757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides guidance for researchers with some mathematical background on the conduct of time-to-event analysis in observational studies based on intensity (hazard) models. Discussions of basic concepts like time axis, event definition and censoring are given. Hazard models are introduced, with special emphasis on the Cox proportional hazards regression model. We provide check lists that may be useful both when fitting the model and assessing its goodness of fit and when interpreting the results. Special attention is paid to how to avoid problems with immortal time bias by introducing time-dependent covariates. We discuss prediction based on hazard models and difficulties when attempting to draw proper causal conclusions from such models. Finally, we present a series of examples where the methods and check lists are exemplified. Computational details and implementation using the freely available R software are documented in Supplementary Material. The paper was prepared as part of the STRATOS initiative.},
  archive      = {J_SIM},
  author       = {Per Kragh Andersen and Maja Pohar Perme and Hans C. van Houwelingen and Richard J. Cook and Pierre Joly and Torben Martinussen and Jeremy M. G. Taylor and Michal Abrahamowicz and Terry M. Therneau},
  doi          = {10.1002/sim.8757},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {185-211},
  shortjournal = {Stat. Med.},
  title        = {Analysis of time-to-event for observational studies: Guidance to the use of intensity models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parameter clustering in bayesian functional principal
component analysis of neuroscientific data. <em>SIM</em>,
<em>40</em>(1), 167–184. (<a
href="https://doi.org/10.1002/sim.8768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraordinary advancements in neuroscientific technology for brain recordings over the last decades have led to increasingly complex spatiotemporal data sets. To reduce oversimplifications, new models have been developed to be able to identify meaningful patterns and new insights within a highly demanding data environment. To this extent, we propose a new model called parameter clustering functional principal component analysis (PCl-fPCA) that merges ideas from functional data analysis and Bayesian nonparametrics to obtain a flexible and computationally feasible signal reconstruction and exploration of spatiotemporal neuroscientific data. In particular, we use a Dirichlet process Gaussian mixture model to cluster functional principal component scores within the standard Bayesian functional PCA framework. This approach captures the spatial dependence structure among smoothed time series (curves) and its interaction with the time domain without imposing a prior spatial structure on the data. Moreover, by moving the mixture from data to functional principal component scores, we obtain a more general clustering procedure, thus allowing a higher level of intricate insight and understanding of the data. We present results from a simulation study showing improvements in curve and correlation reconstruction compared with different Bayesian and frequentist fPCA models and we apply our method to functional magnetic resonance imaging and electroencephalogram data analyses providing a rich exploration of the spatiotemporal dependence in brain time series.},
  archive      = {J_SIM},
  author       = {Nicolò Margaritella and Vanda Inácio and Ruth King},
  doi          = {10.1002/sim.8768},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {167-184},
  shortjournal = {Stat. Med.},
  title        = {Parameter clustering in bayesian functional principal component analysis of neuroscientific data},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian joint model for zero-inflated integers and
left-truncated event times with a time-varying association: Applications
to senior health care. <em>SIM</em>, <em>40</em>(1), 147–166. (<a
href="https://doi.org/10.1002/sim.8767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population aging in most industrialized societies has led to a dramatic increase in emergency medical demand among the elderly. In the context of private health care, an optimal allocation of the medical resources for seniors is commonly done by forecasting their life spans. Accounting for each subject&#39;s particularities is therefore indispensable, so the available data must be processed at an individual level. We use a large and unique dataset of insured parties aged 65 and older to appropriately relate the emergency care usage with mortality risk. Longitudinal and time-to-event processes are jointly modeled, and their underlying relationship can therefore be assessed. Such an application, however, requires some special features to also be considered. First, longitudinal demand for emergency services exhibits a nonnegative integer response with an excess of zeros due to the very nature of the data. These subject-specific responses are handled by a zero-inflated version of the hierarchical negative binomial model. Second, event times must account for the left truncation derived from the fact that policyholders must reach the age of 65 before they may begin to be observed. Consequently, a delayed entry bias arises for those individuals entering the study after this age threshold. Third, and as the main challenge of our analysis, the association parameter between both processes is expected to be age-dependent, with an unspecified association structure. This is well-approximated through a flexible functional specification provided by penalized B-splines. The parameter estimation of the joint model is derived under a Bayesian scheme.},
  archive      = {J_SIM},
  author       = {Xavier Piulachs and Eleni-Rosalina Andrinopoulou and Montserrat Guillén and Dimitris Rizopoulos},
  doi          = {10.1002/sim.8767},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {147-166},
  shortjournal = {Stat. Med.},
  title        = {A bayesian joint model for zero-inflated integers and left-truncated event times with a time-varying association: Applications to senior health care},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimum sample size for external validation of a clinical
prediction model with a continuous outcome. <em>SIM</em>,
<em>40</em>(1), 133–146. (<a
href="https://doi.org/10.1002/sim.8766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical prediction models provide individualized outcome predictions to inform patient counseling and clinical decision making. External validation is the process of examining a prediction model&#39;s performance in data independent to that used for model development. Current external validation studies often suffer from small sample sizes, and subsequently imprecise estimates of a model&#39;s predictive performance. To address this, we propose how to determine the minimum sample size needed for external validation of a clinical prediction model with a continuous outcome. Four criteria are proposed, that target precise estimates of (i) R 2 (the proportion of variance explained), (ii) calibration-in-the-large (agreement between predicted and observed outcome values on average), (iii) calibration slope (agreement between predicted and observed values across the range of predicted values), and (iv) the variance of observed outcome values. Closed-form sample size solutions are derived for each criterion, which require the user to specify anticipated values of the model&#39;s performance (in particular R 2 ) and the outcome variance in the external validation dataset. A sensible starting point is to base values on those for the model development study, as obtained from the publication or study authors. The largest sample size required to meet all four criteria is the recommended minimum sample size needed in the external validation dataset. The calculations can also be applied to estimate expected precision when an existing dataset with a fixed sample size is available, to help gauge if it is adequate. We illustrate the proposed methods on a case-study predicting fat-free mass in children.},
  archive      = {J_SIM},
  author       = {Lucinda Archer and Kym I. E. Snell and Joie Ensor and Mohammed T. Hudda and Gary S. Collins and Richard D. Riley},
  doi          = {10.1002/sim.8766},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {133-146},
  shortjournal = {Stat. Med.},
  title        = {Minimum sample size for external validation of a clinical prediction model with a continuous outcome},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel density-based likelihood ratio tests for linear
regression models. <em>SIM</em>, <em>40</em>(1), 119–132. (<a
href="https://doi.org/10.1002/sim.8765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a so-called profile likelihood ratio test (PLRT) based on the estimated error density for the multiple linear regression model. Unlike the existing likelihood ratio test (LRT), our proposed PLRT does not require any specification on the error distribution. The asymptotic properties are developed and the Wilks phenomenon is studied. Simulation studies are conducted to examine the performance of the PLRT. It is observed that our proposed PLRT generally outperforms the existing LRT, empirical likelihood ratio test and the weighted profile likelihood ratio test in sense that (i) its type I error rates are closer to the prespecified nominal level; (ii) it generally has higher powers; (iii) it performs satisfactorily when moments of the error do not exist (eg, Cauchy distribution); and (iv) it has higher probability of correctly selecting the correct model in the multiple testing problem. A mammalian eye gene expression dataset and a concrete compressive strength dataset are analyzed to illustrate our methodologies.},
  archive      = {J_SIM},
  author       = {Feifei Yan and Qing-Song Xu and Man-Lai Tang and Ziqi Chen},
  doi          = {10.1002/sim.8765},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {119-132},
  shortjournal = {Stat. Med.},
  title        = {Kernel density-based likelihood ratio tests for linear regression models},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effect of a constraint on the maximum number of controls
matched to each treated subject on the performance of full matching on
the propensity score when estimating risk differences. <em>SIM</em>,
<em>40</em>(1), 101–118. (<a
href="https://doi.org/10.1002/sim.8764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many observational studies estimate causal effects using methods based on matching on the propensity score. Full matching on the propensity score is an effective and flexible method for utilizing all available data and for creating well-balanced treatment and control groups. An important component of the full matching algorithm is the decision about whether to impose a restriction on the maximum ratio of controls matched to each treated subject. Despite the possible effect of this restriction on subsequent inferences, this issue has not been examined. We used a series of Monte Carlo simulations to evaluate the effect of imposing a restriction on the maximum ratio of controls matched to each treated subject when estimating risk differences. We considered full matching both with and without a caliper restriction. When using full matching with a caliper restriction, the imposition of a subsequent constraint on the maximum ratio of the number of controls matched to each treated subject had no effect on the quality of inferences. However, when using full matching without a caliper restriction, the imposition of a constraint on the maximum ratio of the number of controls matched to each treated subject tended to result in an increase in bias in the estimated risk difference. However, this increase in bias tended to be accompanied by a corresponding decrease in the sampling variability of the estimated risk difference. We illustrate the consequences of these restrictions using observational data to estimate the effect of medication prescribing on survival following hospitalization for a heart attack.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Elizabeth A. Stuart},
  doi          = {10.1002/sim.8764},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {101-118},
  shortjournal = {Stat. Med.},
  title        = {The effect of a constraint on the maximum number of controls matched to each treated subject on the performance of full matching on the propensity score when estimating risk differences},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explained variation under the additive hazards model.
<em>SIM</em>, <em>40</em>(1), 85–100. (<a
href="https://doi.org/10.1002/sim.8763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study explained variation under the additive hazards regression model for right-censored data. We consider different approaches for developing such a measure, and focus on one that estimates the proportion of variation in the failure time explained by the covariates. We study the properties of the measure both analytically, and through extensive simulations. We apply the measure to a well-known survival dataset as well as the linked surveillance, epidemiology, and end results-Medicare database for prediction of mortality in early stage prostate cancer patients using high-dimensional claims codes.},
  archive      = {J_SIM},
  author       = {Denise Rava and Ronghui Xu},
  doi          = {10.1002/sim.8763},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {85-100},
  shortjournal = {Stat. Med.},
  title        = {Explained variation under the additive hazards model},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of crossover designs with nonignorable dropout.
<em>SIM</em>, <em>40</em>(1), 64–84. (<a
href="https://doi.org/10.1002/sim.8762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the analysis of crossover designs with nonignorable dropout. We study nonreplicated crossover designs and replicated designs separately. With a primary objective of comparing the treatment mean effects, we jointly model the longitudinal measures and discrete time to dropout. We propose shared-parameter models and mixed-effects selection models. We adapt a linear-mixed effects model as the conditional model for the longitudinal outcomes. We invoke a discrete-time hazards model with a complementary log-log link function for the conditional distribution of time to dropout. We apply maximum likelihood for parameter estimation. We perform simulation studies to investigate the robustness of our proposed approaches under various missing data mechanisms. We then apply the approaches to two examples with a continuous outcome and one example with a binary outcome using existing software. We also implement the controlled multiple imputation methods as a sensitivity analysis of the missing data assumption.},
  archive      = {J_SIM},
  author       = {Xi Wang and Vernon M. Chinchilli},
  doi          = {10.1002/sim.8762},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {64-84},
  shortjournal = {Stat. Med.},
  title        = {Analysis of crossover designs with nonignorable dropout},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A method to aid statistical judgment on outliers: Comment on
hill’s the statistician in medicine. <em>SIM</em>, <em>40</em>(1),
58–63. (<a href="https://doi.org/10.1002/sim.8853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Jeffrey Zhang and Bo Zhang and Dylan S. Small},
  doi          = {10.1002/sim.8853},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {58-63},
  shortjournal = {Stat. Med.},
  title        = {A method to aid statistical judgment on outliers: Comment on hill&#39;s the statistician in medicine},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thoughts on a.b. Hill’s watson lecture. <em>SIM</em>,
<em>40</em>(1), 55–57. (<a
href="https://doi.org/10.1002/sim.8820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Janet T. Wittes},
  doi          = {10.1002/sim.8820},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {55-57},
  shortjournal = {Stat. Med.},
  title        = {Thoughts on A.B. hill&#39;s watson lecture},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comments on “the statistician in medicine” by austin
bradford hill. <em>SIM</em>, <em>40</em>(1), 52–54. (<a
href="https://doi.org/10.1002/sim.8824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Louise M. Ryan},
  doi          = {10.1002/sim.8824},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {52-54},
  shortjournal = {Stat. Med.},
  title        = {Comments on “The statistician in medicine” by austin bradford hill},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A.b. Hill’s 1962 watson lecture: The statistical consultant
as consensus maker. <em>SIM</em>, <em>40</em>(1), 49–51. (<a
href="https://doi.org/10.1002/sim.8827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Christopher J. Phillips},
  doi          = {10.1002/sim.8827},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {49-51},
  shortjournal = {Stat. Med.},
  title        = {A.B. hill&#39;s 1962 watson lecture: The statistical consultant as consensus maker},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reflecting on “a statistician in medicine” in 2020.
<em>SIM</em>, <em>40</em>(1), 42–48. (<a
href="https://doi.org/10.1002/sim.8830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this commentary, we revisit Sir Austin Bradford Hill&#39;s seminal Alfred Watson Memorial Lecture in 1962 through the eyes of two practicing biostatisticians of the current era. We summarize some eternal takeaway messages from Hill&#39;s lecture regarding observations and experiments translated through the modern lexicon of causal inference. Finally, we pose a series of questions that we would have liked to pose to Sir Austin Bradford Hill if he were to deliver the lecture in 2020.},
  archive      = {J_SIM},
  author       = {Walter Dempsey and Bhramar Mukherjee},
  doi          = {10.1002/sim.8830},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {42-48},
  shortjournal = {Stat. Med.},
  title        = {Reflecting on “A statistician in medicine” in 2020},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Commentary on “the statistician in medicine” by professor
sir austin bradford hill. <em>SIM</em>, <em>40</em>(1), 37–41. (<a
href="https://doi.org/10.1002/sim.8825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Erica E. M. Moodie and David A. Stephens},
  doi          = {10.1002/sim.8825},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {37-41},
  shortjournal = {Stat. Med.},
  title        = {Commentary on “The statistician in medicine” by professor sir austin bradford hill},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion of the statistician in medicine, by professor sir
austin bradford hill. <em>SIM</em>, <em>40</em>(1), 35–36. (<a
href="https://doi.org/10.1002/sim.8821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Thomas A. Louis},
  doi          = {10.1002/sim.8821},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {35-36},
  shortjournal = {Stat. Med.},
  title        = {Discussion of the statistician in medicine, by professor sir austin bradford hill},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reflections on the 1962 paper “the statistician in medicine”
by sir austin bradford hill. <em>SIM</em>, <em>40</em>(1), 32–34. (<a
href="https://doi.org/10.1002/sim.8834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides reflections on the 1962 paper by Sir Austin Bradford Hill, entitled “The Statistician in Medicine.” It discusses several key takeaways of this paper, including causal inference for big data, reproducibility and replicability in science, and integration of statistics and data science with domain science.},
  archive      = {J_SIM},
  author       = {Xihong Lin},
  doi          = {10.1002/sim.8834},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {32-34},
  shortjournal = {Stat. Med.},
  title        = {Reflections on the 1962 paper “The statistician in medicine” by sir austin bradford hill},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Commentary on professor austin bradford hill’s alfred watson
memorial lecture. <em>SIM</em>, <em>40</em>(1), 29–31. (<a
href="https://doi.org/10.1002/sim.8826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As availability of health care data for research opens up new frontiers in medical statistics, keeping a focus on the science behind the data is more important than ever to promote sound research and protect the validity of research results. Though the electronic databases currently amassed for research far exceed in scale and scope the observational research Professor Hill likely conceived of, his guidance to statisticians to ground our work in the biological and medical processes behind the data remains salient across the decades.},
  archive      = {J_SIM},
  author       = {Rebecca A. Hubbard},
  doi          = {10.1002/sim.8826},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {29-31},
  shortjournal = {Stat. Med.},
  title        = {Commentary on professor austin bradford hill&#39;s alfred watson memorial lecture},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Medical statistics, austin bradford hill, and a celebration
of 40 years of statistics in medicine. <em>SIM</em>, <em>40</em>(1),
17–28. (<a href="https://doi.org/10.1002/sim.8832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Vern Farewell and Tony Johnson},
  doi          = {10.1002/sim.8832},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {17-28},
  shortjournal = {Stat. Med.},
  title        = {Medical statistics, austin bradford hill, and a celebration of 40 years of statistics in medicine},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Alfred watson memorial lecture: The statistician in
medicine. <em>SIM</em>, <em>40</em>(1), 3–16. (<a
href="https://doi.org/10.1002/sim.8814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {A.B. Hill},
  doi          = {10.1002/sim.8814},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {3-16},
  shortjournal = {Stat. Med.},
  title        = {Alfred watson memorial lecture: The statistician in medicine.},
  volume       = {40},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
