<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SADM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sadm---48">SADM - 48</h2>
<ul>
<li><details>
<summary>
(2021). Retracted: Multi-model penalized regression. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>14</em>(6), 698. (<a
href="https://doi.org/10.1002/sam.11496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Wendelberger, LJ, Reich, BJ, Wilson, AG. Multi-model penalized regression. Stat Anal Data Min: The ASA Data Sci Journal. 2021; 1 - 25. https://doi.org/10.1002/sam.11496 The above article from Statistical Analysis and Data Mining, published online on 13 January 2021 in Wiley Online Library (wileyonlinelibrary.com), has been retracted by agreement between the authors, the journal Editor-in-Chief and Wiley Periodicals, LLC. The retraction has been agreed due to inadvertent but substantial overlap with a previously published article in the journal Technometrics.},
  archive  = {J},
  author   = {Laura J. Wendelberger and Brian J. Reich and Alyson G. Wilson},
  doi      = {10.1002/sam.11496},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {698},
  title    = {Retracted: Multi-model penalized regression},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precision aggregated local models. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>14</em>(6),
676–697. (<a href="https://doi.org/10.1002/sam.11547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large-scale Gaussian process (GP) regression is infeasible for large training data due to cubic scaling of flops and quadratic storage involved in working with covariance matrices. Remedies in recent literature focus on divide-and-conquer, for example, partitioning into subproblems and inducing functional (and thus computational) independence. Such approximations can be speedy, accurate, and sometimes even more flexible than ordinary GPs. However, a big downside is loss of continuity at partition boundaries. Modern methods like local approximate GPs (LAGPs) imply effectively infinite partitioning and are thus both good and bad in this regard. Model averaging, an alternative to divide-and-conquer, can maintain absolute continuity but often over-smooths, diminishing accuracy. Here we propose putting LAGP-like methods into a local experts-like framework, blending partition-based speed with model-averaging continuity, as a flagship example of what we call precision aggregated local models (PALM). Using LAGPs, each selecting from total data pairs, our scheme is at most cubic in , quadratic in , and linear in . Extensive empirical illustration shows how PALM is at least as accurate as LAGP, can be much faster, and furnishes continuous predictions. Finally, we propose sequential updating scheme that greedily refines a PALM predictor up to a computational budget.},
  archive  = {J},
  author   = {Adam M. Edwards and Robert B. Gramacy},
  doi      = {10.1002/sam.11547},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {676-697},
  title    = {Precision aggregated local models},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Power grid frequency prediction using spatiotemporal
modeling. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(6), 662–675. (<a
href="https://doi.org/10.1002/sam.11535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Understanding power system dynamics is essential for interarea oscillation analysis and the detection of grid instabilities. The FNET/GridEye is a GPS-synchronized wide-area frequency measurement network that provides an accurate picture of the normal real-time operational condition of the power system dynamics, giving rise to new and intricate spatiotemporal patterns of power loads. We propose to model FNET/GridEye grid frequency data from the U.S. Eastern Interconnection with a spatiotemporal statistical model. We predict the frequency data at locations without observations, a critical need during disruption events where measurement data are inaccessible. Spatial information is accounted for either as neighboring measurements in the form of covariates or with a spatiotemporal correlation model captured by a latent Gaussian field. The proposed method is useful in estimating power system dynamic response from limited phasor measurements and holds promise for predicting instability that may lead to undesirable effects such as cascading outages.},
  archive  = {J},
  author   = {Amanda Lenzi and Julie Bessac and Mihai Anitescu},
  doi      = {10.1002/sam.11535},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {662-675},
  title    = {Power grid frequency prediction using spatiotemporal modeling},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fourier neural networks as function approximators and
differential equation solvers. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>14</em>(6), 647–661. (<a
href="https://doi.org/10.1002/sam.11531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a Fourier neural network (FNN) that can be mapped directly to the Fourier decomposition. The choice of activation and loss function yields results that replicate a Fourier series expansion closely while preserving a straightforward architecture with a single hidden layer. The simplicity of this network architecture facilitates the integration with any other higher-complexity networks, at a data pre- or postprocessing stage. We validate this FNN on naturally periodic smooth functions and on piecewise continuous periodic functions. We showcase the use of this FNN for modeling or solving partial differential equations with periodic boundary conditions. The main advantages of the current approach are the validity of the solution outside the training region, interpretability of the trained model, and simplicity of use.},
  archive  = {J},
  author   = {Marieme Ngom and Oana Marin},
  doi      = {10.1002/sam.11531},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {647-661},
  title    = {Fourier neural networks as function approximators and differential equation solvers},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A practical extension of the recursive multi-fidelity model
for the emulation of hole closure experiments. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>14</em>(6),
636–646. (<a href="https://doi.org/10.1002/sam.11513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In regimes of high strain rate, the strength of materials often cannot be measured directly in experiments. Instead, the strength is inferred based on an experimental observable, such as a change in shape, that is matched by simulations supported by a known strength model. In hole closure experiments, the rate and degree to which a central hole in a plate of material closes during a dynamic loading event are used to infer material strength parameters. Due to the complexity of the experiment, many computationally expensive, three-dimensional simulations are necessary to train an emulator for calibration or other analyses. These simulations can be run at multiple grid resolutions, where dense grids are slower but more accurate. In an effort to reduce the computational cost, a combination of simulations with different resolutions can be combined to develop an accurate emulator within a limited training time. We explore the novel design and construction of an appropriate functional recursive multi-fidelity emulator of a strength model for tantalum in hole closure experiments that can be applied to arbitrarily large training data. Hence, by formulating a multi-fidelity model to employ low-fidelity simulations, we were able to reduce the error of our emulator by approximately 81% with only an approximately 1.6% increase in computing resource utilization.},
  archive  = {J},
  author   = {Amanda Muyskens and Kathleen Schmidt and Matthew Nelms and Nathan Barton and Jeffrey Florando and Ana Kupresanin and David Rivera},
  doi      = {10.1002/sam.11513},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {636-646},
  title    = {A practical extension of the recursive multi-fidelity model for the emulation of hole closure experiments},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating causal-based feature selection for fuel property
prediction models. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>14</em>(6), 624–635. (<a
href="https://doi.org/10.1002/sam.11511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In-silico screening of novel biofuel molecules based on chemical and fuel properties is a critical first step in the biofuel evaluation process due to the significant volumes of samples required for experimental testing, the destructive nature of engine tests, and the costs associated with bench-scale synthesis of novel fuels. Predictive models are limited by training sets of few existing measurements, often containing similar classes of molecules that represent just a subset of the potential molecular fuel space. Software tools can be used to generate every possible molecular descriptor for use as input features, but most of these features are largely irrelevant and training models on datasets with higher dimensionality than size tends to yield poor predictive performance. Feature selection has been shown to improve machine learning models, but correlation-based feature selection fails to provide scientific insight into the underlying mechanisms that determine structure–property relationships. The implementation of causal discovery in feature selection could potentially inform the biofuel design process while also improving model prediction accuracy and robustness to new data. In this study, we investigate the benefits causal-based feature selection might have on both model performance and identification of key molecular substructures. We found that causal-based feature selection performed on par with alternative filtration methods, and that a structural causal model provides valuable scientific insights into the relationships between molecular substructures and fuel properties.},
  archive  = {J},
  author   = {Bernard Nguyen and Leanne S. Whitmore and Anthe George and Corey M. Hudson},
  doi      = {10.1002/sam.11511},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {624-635},
  title    = {Evaluating causal-based feature selection for fuel property prediction models},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of gaussian processes and neural networks for
computer model emulation and calibration. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>14</em>(6), 606–623.
(<a href="https://doi.org/10.1002/sam.11507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Department of Energy relies on complex physics simulations for prediction in domains like cosmology, nuclear theory, and materials science. These simulations are often extremely computationally intensive, with some requiring days or weeks for a single simulation. In order to assure their accuracy, these models are calibrated against observational data in order to estimate inputs and systematic biases. Because of their great computational complexity, this process typically requires the construction of an emulator , a fast approximation to the simulation. In this paper, two emulator approaches are compared: Gaussian process regression and neural networks. Their emulation accuracy and calibration performance on three real problems of Department of Energy interest is considered. On these problems, the Gaussian process emulator tends to be more accurate with narrower, but still well-calibrated uncertainty estimates. The neural network emulator is accurate, but tends to have large uncertainty on its predictions. As a result, calibration with the Gaussian process emulator produces more constrained posteriors that still perform well in prediction.},
  archive  = {J},
  author   = {Samuel Myren and Earl Lawrence},
  doi      = {10.1002/sam.11507},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {606-623},
  title    = {A comparison of gaussian processes and neural networks for computer model emulation and calibration},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An initial exploration of bayesian model calibration for
estimating the composition of rocks and soils on mars. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>14</em>(6), 596–605. (<a
href="https://doi.org/10.1002/sam.11503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Mars Curiosity rover carries an instrument, ChemCam, designed to measure the composition of surface rocks and soil using laser-induced breakdown spectroscopy (LIBS). The measured spectra from this instrument must be analyzed to identify the component elements in the target sample, as well as their relative proportions. This process, which we call disaggregation, is complicated by so-called matrix effects, which describe nonlinear changes in the relative heights of emission lines as an unknown function of composition due to atomic interactions within the LIBS plasma. In this work, we explore the use of the plasma physics code ATOMIC, developed at Los Alamos National Laboratory, for the disaggregation task. ATOMIC has recently been used to model LIBS spectra and can robustly reproduce matrix effects from first principles. The ability of ATOMIC to predict LIBS spectra presents an exciting opportunity to perform disaggregation in a manner not yet tried in the LIBS community, namely via Bayesian model calibration. However, using it directly to solve our inverse problem is computationally intractable due to the large parameter space and the computation time required to produce a single output. Therefore, we also explore the use of emulators as a fast solution for this analysis. We discuss a proof of concept Gaussian process emulator for disaggregating two-element compounds of sodium and copper. The training and test datasets were simulated with ATOMIC using a Latin hypercube design. After testing the performance of the emulator, we successfully recover the composition of 25 test spectra with Bayesian model calibration.},
  archive  = {J},
  author   = {Claire-Alice Hébert and Earl Lawrence and Kary Myers and James P. Colgan and Elizabeth J. Judge},
  doi      = {10.1002/sam.11503},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {596-605},
  title    = {An initial exploration of bayesian model calibration for estimating the composition of rocks and soils on mars},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approach to characterizing spatial aspects of image
system blur. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>14</em>(6), 583–595. (<a
href="https://doi.org/10.1002/sam.11501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Quantitative X-ray radiographic imaging systems that utilize a charged couple device (CCD) camera connected to a thick, monolithic scintillator can exhibit blur that varies spatially across the field of view, especially for thick scintillators used in pulse-power radiography of dynamically compressed objects. A three-point approach to estimating and accounting for this effect is demonstrated by (a) using a local estimation technique to measure the effect of blurring a calibration object at key locations across the field of view, (b) combining each of the local estimates into a spatially varying blurring function via partitions of unity interpolation, and (c) resolving the effects of that blur on the image by solving an ill-posed inverse problem using a spatially varying regularization term. The technique is demonstrated on synthetic examples and actual radiographs collected at the Naval Research Laboratory&#39;s (NRL) Mercury pulsed power facility.},
  archive  = {J},
  author   = {Jesse Adams and Jessica Pillow and Kevin Joyce and Michael Brennan and Malena I. Español and Matthias Morzfeld and Sean Breckling and Daniel Champion and Eric Clarkson and Ryan Coffee and Amanda Gehring and Margaret Lund and Duane Smalley and Ajanae Williams and Jacob Zier and Daniel Frayer and Marylesa Howard and Eric Machorro},
  doi      = {10.1002/sam.11501},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {583-595},
  title    = {An approach to characterizing spatial aspects of image system blur},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying build orientation of 3D-printed materials using
convolutional neural networks. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>14</em>(6), 575–582. (<a
href="https://doi.org/10.1002/sam.11497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The advent of additive manufacturing (AM) processes brought with it intense research into various materials and manufacturing processes. At the same time, the need for validation of material properties, as well as study and forecasting of aging, has arisen. Modern imaging techniques, like X-ray computed tomography (XCT), are a convenient vehicle for such studies; however, the large datasets they produce require novel analysis techniques to efficiently extract critical information. In this paper, we present our work on developing a 3D extension of the ResNet architecture to distinguish between two build orientations of tensile bars produced by AM. Using only information from XCT, our method achieves a 99.3% correct classification at a misidentification of 1%.},
  archive  = {J},
  author   = {Jan Strube and Malachi Schram and Sabiha Rustam and Zachary Kennedy and Tamas Varga},
  doi      = {10.1002/sam.11497},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {575-582},
  title    = {Identifying build orientation of 3D-printed materials using convolutional neural networks},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding the merits of winning data competition
solutions for varied sets of objectives. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>14</em>(6), 556–574.
(<a href="https://doi.org/10.1002/sam.11494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data competitions provide an efficient cost-effective way to obtain diverse solutions for challenging problems across a wide variety of applications. The competition leaderboard, by necessity, must combine multiple objectives into a single scoring formula to determine winners and allocate prize money. However, after the competition concludes, the host may wish to choose a best solution for a particular scenario that focuses on only a subset of all the competition objectives. Through the use of Pareto fronts and graphical summaries, we describe how top solutions for a specific scenario can be identified and compared. The strategy uses intentional tie-handling, thresholds to eliminate undesirable solutions and Pareto fronts to identify objectively superior solutions for a subset of objectives. Then the strengths and weaknesses of different alternatives can be compared to find the ideal solution for the problem. The methods are illustrated with a real Topcoder data competition hosted by Los Alamos National Laboratory that used 16 different objectives to evaluate the quality of solutions for urban radiation search.},
  archive  = {J},
  author   = {Lu Lu and Christine M. Anderson-Cook and Miaolu Zhang},
  doi      = {10.1002/sam.11494},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {556-574},
  title    = {Understanding the merits of winning data competition solutions for varied sets of objectives},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of machine learning approaches used to identify
the drivers of bakken oil well productivity. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>14</em>(6),
536–555. (<a href="https://doi.org/10.1002/sam.11487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Geologists and petroleum engineers have struggled to identify the mechanisms that drive productivity in horizontal hydraulically fractured oil wells. The machine learning algorithms of Random Forest (RF), gradient boosting trees (GBT) and extreme gradient boosting (XGBoost) were applied to a dataset containing 7311 horizontal hydraulically fractured wells drilled into the middle member of the Bakken Formation from 2010 through 2017. The initial goal is to use these data-driven machine learning algorithms to identify the most important explanatory predictors of well productivity within nine subareas and the composite area. Predictor variables representing initial gas production, the initial 180-day water cut, and vertical depth vary spatially and are identified with geologically favorable areas. Well-completion predictors include the well lateral length, number of fracture stages, volume of proppant per stage, and the volume of injected fluids per stage. The performance of methods is compared based on a common test sample. The analysis then examines the comparative predictive performance of the three algorithms for 1330 wells that had initiated production after the initial 7311 well sample had been producing. The computations of predictor importance identified the initial 180-day water cut and the 30-day initial gas production predictors as having a dominant influence in most subareas and for the composite area. The relative importance of well completion predictor variables, that is, the number of fracture stages per well, volume of injected proppant per stage, volume of injected fluids per stage, and lateral length, varied considerably across the subareas. For the common test or holdout sample, the models calibrated with the XGBoost algorithm had superior predictive power. The predictive power of all the algorithms trained on the data from the original sample suffered some loss when tested with a sample of wells that had started production after the end of that period. Implications of the empirical findings and strategies to mitigate loss of predictive power are discussed in the concluding section.},
  archive  = {J},
  author   = {Emil D. Attanasi and Philip A. Freeman and Timothy C. Coburn},
  doi      = {10.1002/sam.11487},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {536-555},
  title    = {Comparison of machine learning approaches used to identify the drivers of bakken oil well productivity},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning compact physics-aware delayed photocurrent models
using dynamic mode decomposition. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>14</em>(6), 521–535. (<a
href="https://doi.org/10.1002/sam.11485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Radiation-induced photocurrent in semiconductor devices can be simulated using complex physics-based models, which are accurate, but computationally expensive. This presents a challenge for implementing device characteristics in high-level circuit simulations where it is computationally infeasible to evaluate detailed models for multiple individual circuit elements. In this work we demonstrate a procedure for learning compact delayed photocurrent models that are efficient enough to implement in large-scale circuit simulations, but remain faithful to the underlying physics. Our approach utilizes dynamic mode decomposition (DMD), a system identification technique for learning reduced-order discrete-time dynamical systems from time series data based on singular value decomposition. To obtain physics-aware device models, we simulate the excess carrier density induced by radiation pulses by solving numerically the ambipolar diffusion equation, then use the simulated internal state as training data for the DMD algorithm. Our results show that the significantly reduced-order delayed photocurrent models obtained via this method accurately approximate the dynamics of the internal excess carrier density—which can be used to calculate the induced current at the device boundaries—while remaining compact enough to incorporate into larger circuit simulations.},
  archive  = {J},
  author   = {Joshua Hanson and Pavel Bochev and Biliana Paskaleva},
  doi      = {10.1002/sam.11485},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {521-535},
  title    = {Learning compact physics-aware delayed photocurrent models using dynamic mode decomposition},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel coordinate order for high-dimensional data.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(5), 501–515. (<a
href="https://doi.org/10.1002/sam.11543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visualization of high-dimensional data is counter-intuitive using conventional graphs. Parallel coordinates are proposed as an alternative to explore multivariate data more effectively. However, it is difficult to extract relevant information through the parallel coordinates when the data are high-dimensional with thousands of overlapping lines. The order of the axes determines the perception of information on parallel coordinates. Thus, the information between attributes remains hidden if coordinates are improperly ordered. Here we propose a general framework to reorder the coordinates. This framework is general enough to cover a wide range of data visualization objectives. It is also flexible enough to contain many conventional ordering measures. Consequently, we present the coordinate ordering binary optimization problem and enhance it to achieve a computationally efficient greedy approach that suits high-dimensional data. Our approach is applied to wine data and genetic data. The purpose of dimension reordering of wine data is to highlight attributes&#39; dependence. Genetic data are reordered to enhance cluster detection. The proposed framework shows that it is able to adapt the criteria for the visualization objective.},
  archive  = {J},
  author   = {Shaima Tilouche and Vahid Partovi Nia and Samuel Bassetto},
  doi      = {10.1002/sam.11543},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {501-515},
  title    = {Parallel coordinate order for high-dimensional data},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate gaussian RBF-net for smooth function estimation
and variable selection. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>14</em>(5), 484–500. (<a
href="https://doi.org/10.1002/sam.11540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Neural networks are routinely used for nonparametric regression modeling. The interest in these models is growing with ever-increasing complexities in modern datasets. With modern technological advancements, the number of predictors frequently exceeds the sample size in many application areas. Thus, selecting important predictors from the huge pool is an extremely important task for judicious inference. This paper proposes a novel flexible class of single-layer radial basis functions (RBF) networks. The proposed architecture can estimate smooth unknown regression functions and also perform variable selection. We primarily focus on Gaussian RBF-net due to its attractive properties. The extensions to other choices of RBF are fairly straightforward. The proposed architecture is also shown to be effective in identifying relevant predictors in a low-dimensional setting using the posterior samples without imposing any sparse estimation scheme. We develop an efficient Markov chain Monte Carlo algorithm to generate posterior samples of the parameters. We illustrate the proposed method&#39;s empirical efficacy through simulation experiments, both in high and low dimensional regression problems. The posterior contraction rate is established with respect to empirical distance assuming that the error variance is unknown, and the true function belongs to a Hölder ball. We illustrate our method in a Human Connectome Project dataset to predict vocabulary comprehension and to identify important edges of the structural connectome.},
  archive  = {J},
  author   = {Arkaprava Roy},
  doi      = {10.1002/sam.11540},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {484-500},
  title    = {Multivariate gaussian RBF-net for smooth function estimation and variable selection},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survival trees based on heterogeneity in time-to-event and
censoring distributions using parameter instability test.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(5), 466–483. (<a
href="https://doi.org/10.1002/sam.11539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Survival analysis of right-censored data often arises in many areas of research including medical research. The effect of covariates (and their interactions) on survival distribution can be studied through existing methods that require pre-specifying the functional form of the covariates including their interactions. Survival trees offer a relatively flexible approach when the form of covariates&#39; effects is unknown. We have proposed the SurvCART algorithm to construct a survival tree. There are two features that distinguish the SurvCART algorithm from the rest. First, most of the currently available survival tree construction techniques are not based on a formal test of significance and, hence, prone to spurious findings. The proposed SurvCART algorithm utilizes the “conditional inference” framework that selects splitting variable via parameter instability test and subsequently finds the optimal split based on some maximally chosen statistic. We used likelihood score-based parameter instability tests that converge to distribution with known distribution function so that the p -value can be obtained easily without any approximation. Second, the SurvCART algorithm has the flexibility to extend the concept of heterogeneity to the censoring time distribution as well, a feature that can be useful when censoring distribution is influenced by baseline covariates. We evaluated the operating characteristics of the parameter instability test and compared the performance of the SurvCART algorithm with other survival tree algorithms via simulation. Finally, the SurvCART algorithm was applied to a real data setting. The proposed method is implemented in R package LongCART available on CRAN.},
  archive  = {J},
  author   = {Madan Gopal Kundu and Samiran Ghosh},
  doi      = {10.1002/sam.11539},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {466-483},
  title    = {Survival trees based on heterogeneity in time-to-event and censoring distributions using parameter instability test},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Negative binomial graphical model with excess zeros.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(5), 449–465. (<a
href="https://doi.org/10.1002/sam.11536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Markov random field or undirected graphical models (GM) are a popular class of GM useful in various fields because they provide an intuitive and interpretable graph expressing the complex relationship between random variables. The zero-inflated local Poisson graphical model has been proposed as a graphical model for count data with excess zeros. However, as count data are often characterized by over-dispersion, the local Poisson graphical model may suffer from a poor fit to data. In this paper, we propose a zero-inflated local negative binomial (NB) graphical model. Due to the dependencies of parameters in our models, a direct optimization of the objective function is difficult. Instead, we devise expectation-minimization algorithms based on two different parametrizations for the NB distribution. Through a simulation study, we illustrate the effectiveness of our method for learning network structure from over-dispersed count data with excess zeros. We further apply our method to real data to estimate its network structure.},
  archive  = {J},
  author   = {Beomjin Park and Hosik Choi and Changyi Park},
  doi      = {10.1002/sam.11536},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {449-465},
  title    = {Negative binomial graphical model with excess zeros},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating basis functions in massive fields under the
spatial mixed effects model. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>14</em>(5), 430–448. (<a
href="https://doi.org/10.1002/sam.11537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spatial prediction is commonly achieved under the assumption of a Gaussian random field by obtaining maximum likelihood estimates of parameters, and then using the kriging equations to arrive at predicted values. For massive datasets, fixed rank kriging using the expectation–maximization algorithm for estimation has been proposed as an alternative to the usual but computationally prohibitive kriging method. The method reduces computation cost of estimation by redefining the spatial process as a linear combination of basis functions and spatial random effects. A disadvantage of this method is that it imposes constraints on the relationship between the observed locations and the knots. We develop an alternative method that utilizes the spatial mixed effects model, but allows for additional flexibility by estimating the range of the spatial dependence between the observations and the knots via an alternating expectation conditional maximization algorithm. Experiments show that our methodology improves estimation without sacrificing prediction accuracy while also minimizing the additional computational burden of extra parameter estimation. The methodology is applied to a temperature dataset archived by the United States National Climate Data Center, with improved results over previous methodology.},
  archive  = {J},
  author   = {Karl Pazdernik and Ranjan Maitra},
  doi      = {10.1002/sam.11537},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {430-448},
  title    = {Estimating basis functions in massive fields under the spatial mixed effects model},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coefficient tree regression for generalized linear models.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(5), 407–429. (<a
href="https://doi.org/10.1002/sam.11534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large regression data sets are now commonplace, with so many predictors that they cannot or should not all be included individually. In practice, derived predictors are relevant as meaningful features or, at the very least, as a form of regularized approximation of the true coefficients. We consider derived predictors that are the sum of some groups of individual predictors, which is equivalent to predictors within a group sharing the same coefficient. However, the groups of predictors are usually not known in advance and must be discovered from the data. In this paper we develop a coefficient tree regression algorithm for generalized linear models to discover the group structure from the data. The approach results in simple and highly interpretable models, and we demonstrated with real examples that it can provide a clear and concise interpretation of the data. Via simulation studies under different scenarios we showed that our approach performs better than existing competitors in terms of computing time and predictive accuracy.},
  archive  = {J},
  author   = {Özge Sürer and Daniel W. Apley and Edward C. Malthouse},
  doi      = {10.1002/sam.11534},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {407-429},
  title    = {Coefficient tree regression for generalized linear models},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imbalanced classification: A paradigm-based review.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(5), 383–406. (<a
href="https://doi.org/10.1002/sam.11538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A common issue for classification in scientific research and industry is the existence of imbalanced classes. When sample sizes of different classes are imbalanced in training data, naively implementing a classification method often leads to unsatisfactory prediction results on test data. Multiple resampling techniques have been proposed to address the class imbalance issues. Yet, there is no general guidance on when to use each technique. In this article, we provide a paradigm-based review of the common resampling techniques for binary classification under imbalanced class sizes. The paradigms we consider include the classical paradigm that minimizes the overall classification error, the cost-sensitive learning paradigm that minimizes a cost-adjusted weighted type I and type II errors, and the Neyman–Pearson paradigm that minimizes the type II error subject to a type I error constraint. Under each paradigm, we investigate the combination of the resampling techniques and a few state-of-the-art classification methods. For each pair of resampling techniques and classification methods, we use simulation studies and a real dataset on credit card fraud to study the performance under different evaluation metrics. From these extensive numerical experiments, we demonstrate under each classification paradigm, the complex dynamics among resampling techniques, base classification methods, evaluation metrics, and imbalance ratios. We also summarize a few takeaway messages regarding the choices of resampling techniques and base classification methods, which could be helpful for practitioners.},
  archive  = {J},
  author   = {Yang Feng and Min Zhou and Xin Tong},
  doi      = {10.1002/sam.11538},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {383-406},
  title    = {Imbalanced classification: A paradigm-based review},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized composite likelihood for colored graphical
gaussian models. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>14</em>(4), 366–378. (<a
href="https://doi.org/10.1002/sam.11530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article proposes a penalized composite likelihood method for model selection in colored graphical Gaussian models. The method provides a sparse and symmetry-constrained estimator of the precision matrix and thus conducts model selection and precision matrix estimation simultaneously. In particular, the method uses penalty terms to constrain the elements of the precision matrix, which enables us to transform the model selection problem into a constrained optimization problem. Further, computer experiments are conducted to illustrate the performance of the proposed new methodology. It is shown that the proposed method performs well in both the selection of nonzero elements in the precision matrix and the identification of symmetry structures in graphical models. The feasibility and potential clinical application of the proposed method are demonstrated on a microarray gene expression dataset.},
  archive  = {J},
  author   = {Qiong Li and Xiaoying Sun and Nanwei Wang and Xin Gao},
  doi      = {10.1002/sam.11530},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {366-378},
  title    = {Penalized composite likelihood for colored graphical gaussian models},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and inference for mixtures of simple symmetric
exponential families of -dimensional distributions for vectors with
binary coordinates. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>14</em>(4), 352–365. (<a
href="https://doi.org/10.1002/sam.11528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose tractable symmetric exponential families of distributions for multivariate vectors of 0&#39;s and 1&#39;s in dimensions, or what are referred to in this paper as binary vectors, that allow for nontrivial amounts of variation around some central value . We note that more or less standard asymptotics provides likelihood-based inference in the one-sample problem. We then consider mixture models where component distributions are of this form. Bayes analysis based on Dirichlet processes and Jeffreys priors for the exponential family parameters prove tractable and informative in problems where relevant distributions for a vector of binary variables are clearly not symmetric. We also extend our proposed Bayesian mixture model analysis to datasets with missing entries. Performance is illustrated through simulation studies and application to real datasets.},
  archive  = {J},
  author   = {Abhishek Chakraborty and Stephen B. Vardeman},
  doi      = {10.1002/sam.11528},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {352-365},
  title    = {Modeling and inference for mixtures of simple symmetric exponential families of -dimensional distributions for vectors with binary coordinates},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of the cox proportional hazards model and
competing risks models to critical illness insurance data.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(4), 342–351. (<a
href="https://doi.org/10.1002/sam.11532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A commercial insurance company in the Czech Republic provided data on critical illness insurance. The survival analysis was used to study the influence of the gender of an insured person, the age at which the person entered into an insurance contract, and the region where the insured person lived on the occurrence of an insured event. The main goal of the research was to investigate whether the influence of explanatory variables is estimated differently when two different approaches of analysis are used. The two approaches used were (1) the Cox proportional hazard model that does not assign a specific cause, such as a certain diagnosis, to a critical illness insured event and (2) the competing risks models. Regression models related to these approaches were estimated by R software. The results, which are discussed and compared in the paper, show that insurance companies might benefit from offering policies that consider specific diagnoses as the cause of insured events. They also show that in addition to age, the gender of the client plays a key role in the occurrence of such insured events.},
  archive  = {J},
  author   = {David Zapletal},
  doi      = {10.1002/sam.11532},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {342-351},
  title    = {Application of the cox proportional hazards model and competing risks models to critical illness insurance data},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Markov chain to analyze web usability of a university
website using eye tracking data. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>14</em>(4), 331–341. (<a
href="https://doi.org/10.1002/sam.11512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Web usability is a crucial feature of a website, allowing users to easily find information in a short time. Eye tracking data registered during the execution of tasks allow to measure web usability in a more objective way compared to questionnaires. In this work, we evaluated the web usability of the website of the University of Cagliari through the analysis of eye tracking data with qualitative and quantitative methods. Performances of two groups of students (i.e., high school and university students) across 10 different tasks were compared in terms of time to completion, number of fixations and difficulty ratio. Transitions between different areas of interest (AOI) were analyzed in the two groups using Markov chain. For the majority of tasks, we did not observe significant differences in the performances of the two groups, suggesting that the information needed to complete the tasks could easily be retrieved by students with little previous experience in using the website. For a specific task, high school students showed a worse performance based on the number of fixations and a different Markov chain stationary distribution compared to university students. These results allowed to highlight elements of the pages that can be modified to improve web usability.},
  archive  = {J},
  author   = {Gianpaolo Zammarchi and Luca Frigau and Francesco Mola},
  doi      = {10.1002/sam.11512},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {331-341},
  title    = {Markov chain to analyze web usability of a university website using eye tracking data},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted pivot coordinates for partial least squares-based
marker discovery in high-throughput compositional data. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>14</em>(4), 315–330. (<a
href="https://doi.org/10.1002/sam.11514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-throughput data representing large mixtures of chemical or biological signals are ordinarily produced in the molecular sciences. Given a number of samples, partial least squares (PLS) regression is a well-established statistical method to investigate associations between them and any continuous response variables of interest. However, technical artifacts generally make the raw signals not directly comparable between samples. Thus, data normalization is required before any meaningful scientific information can be drawn. This often allows to characterize the processed signals as compositional data where the relevant information is contained in the pairwise log-ratios between the components of the mixture. The (log-ratio) pivot coordinate approach facilitates the aggregation into single variables of the pairwise log-ratios of a component to all the remaining components. This simplifies interpretability and the investigation of their relative importance but, particularly in a high-dimensional context, the aggregated log-ratios can easily mix up information from different underlaying processes. In this context, we propose a weighting strategy for the construction of pivot coordinates for PLS regression which draws on the correlation between response variable and pairwise log-ratios. Using real and simulated data sets, we demonstrate that this proposal enhances the discovery of biological markers in high-throughput compositional data.},
  archive  = {J},
  author   = {Nikola Štefelová and Javier Palarea-Albaladejo and Karel Hron},
  doi      = {10.1002/sam.11514},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {315-330},
  title    = {Weighted pivot coordinates for partial least squares-based marker discovery in high-throughput compositional data},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast and efficient modal EM algorithm for gaussian
mixtures. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(4), 305–314. (<a
href="https://doi.org/10.1002/sam.11527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the modal approach to clustering, clusters are defined as the local maxima of the underlying probability density function, where the latter can be estimated either nonparametrically or using finite mixture models. Thus, clusters are closely related to certain regions around the density modes, and every cluster corresponds to a bump of the density. The Modal Expectation-Maximization (MEM) algorithm is an iterative procedure that can identify the local maxima of any density function. In this contribution, we propose a fast and efficient MEM algorithm to be used when the density function is estimated through a finite mixture of Gaussian distributions with parsimonious component-covariance structures. After describing the procedure, we apply the proposed MEM algorithm on both simulated and real data examples, showing its high flexibility in several contexts.},
  archive  = {J},
  author   = {Luca Scrucca},
  doi      = {10.1002/sam.11527},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {305-314},
  title    = {A fast and efficient modal EM algorithm for gaussian mixtures},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-node expectation–maximization algorithm for finite
mixture models. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>14</em>(4), 297–304. (<a
href="https://doi.org/10.1002/sam.11529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Finite mixture models are powerful tools for modeling and analyzing heterogeneous data. Parameter estimation is typically carried out using maximum likelihood estimation via the Expectation–Maximization (EM) algorithm. Recently, the adoption of flexible distributions as component densities has become increasingly popular. Often, the EM algorithm for these models involves complicated expressions that are time-consuming to evaluate numerically. In this paper, we describe a parallel implementation of the EM algorithm suitable for both single-threaded and multi-threaded processors and for both single machine and multiple-node systems. Numerical experiments are performed to demonstrate the potential performance gain in different settings. Comparison is also made across two commonly used platforms—R and MATLAB. For illustration, a fairly general mixture model is used in the comparison.},
  archive  = {J},
  author   = {Sharon X. Lee and Geoffrey J. McLachlan and Kaleb L. Leemaqz},
  doi      = {10.1002/sam.11529},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {297-304},
  title    = {Multi-node Expectation–Maximization algorithm for finite mixture models},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLADAG 2019 special issue: Selected papers on classification
and data analysis. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>14</em>(4), 295–296. (<a
href="https://doi.org/10.1002/sam.11533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Francesca Greselin and Thomas Brendan Murphy and Giovanni C. Porzio and Domenico Vistocco},
  doi     = {10.1002/sam.11533},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month   = {8},
  number  = {4},
  pages   = {295-296},
  title   = {CLADAG 2019 special issue: Selected papers on classification and data analysis},
  volume  = {14},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Erratum to “data-driven dimension reduction in functional
principal component analysis identifying the change-point in functional
data.” <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(3), 290. (<a
href="https://doi.org/10.1002/sam.11510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1002/sam.11510},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month   = {6},
  number  = {3},
  pages   = {290},
  title   = {Erratum to “Data-driven dimension reduction in functional principal component analysis identifying the change-point in functional data”},
  volume  = {14},
  year    = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential calibration for correlation coefficient with
additive distortion measurement errors. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>14</em>(3), 271–289.
(<a href="https://doi.org/10.1002/sam.11509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper studies the estimation of correlation coefficient between unobserved variables of interest. These unobservable variables are distorted in an additive fashion by an observed confounding variable. We propose a new identifiability condition by using the exponential calibration to obtain calibrated variables and propose a direct-plug-in estimator for the correlation coefficient. We show that the direct-plug-in estimator is asymptotically efficient. Next, we suggest an asymptotic normal approximation and an empirical likelihood-based statistic to construct the confidence intervals. Last, we propose several test statistics for testing whether the true correlation coefficient is zero or not. The asymptotic properties of the proposed test statistics are examined. We conduct Monte Carlo simulation experiments to examine the performance of the proposed estimators and test statistics. These methods are applied to analyze a temperature forecast data set for an illustration.},
  archive  = {J},
  author   = {Jun Zhang and Zhuoer Xu},
  doi      = {10.1002/sam.11509},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {271-289},
  title    = {Exponential calibration for correlation coefficient with additive distortion measurement errors},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximation error of fourier neural networks.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(3), 258–270. (<a
href="https://doi.org/10.1002/sam.11506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper investigates approximation error of two-layer feedforward Fourier Neural Networks (FNNs). Such networks are motivated by the approximation properties of Fourier series. Several implementations of FNNs were proposed since 1980s: by Gallant and White, Silvescu, Tan, Zuo and Cai, and Liu. The main focus of our work is Silvescu&#39;s FNN, because its activation function does not fit into the category of networks, where the linearly transformed input is exposed to activation. The latter ones were extensively described by Hornik. In regard to non-trivial Silvescu&#39;s FNN, its convergence rate is proven to be of order O (1/ n ) . The paper continues investigating classes of functions approximated by Silvescu FNN, which appeared to be from Schwartz space and space of positive definite functions.},
  archive  = {J},
  author   = {Abylay Zhumekenov and Rustem Takhanov and Alejandro J. Castro and Zhenisbek Assylbekov},
  doi      = {10.1002/sam.11506},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {258-270},
  title    = {Approximation error of fourier neural networks},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized mixed-effects random forest: A flexible approach
to predict university student dropout. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>14</em>(3), 241–257. (<a
href="https://doi.org/10.1002/sam.11505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a new statistical method, called generalized mixed-effects random forest (GMERF), that extends the use of random forest to the analysis of hierarchical data, for any type of response variable in the exponential family. The method maintains the flexibility and the ability of modeling complex patterns within the data, typical of tree-based ensemble methods, and it can handle both continuous and discrete covariates. At the same time, GMERF takes into account the nested structure of hierarchical data, modeling the dependence structure that exists at the highest level of the hierarchy and allowing statistical inference on this structure. In the case study, we apply GMERF to Higher Education data to analyze the university student dropout phenomenon. We predict engineering student dropout probability by means of student-level information and considering the degree program students are enrolled in as grouping factor.},
  archive  = {J},
  author   = {Massimo Pellagatti and Chiara Masci and Francesca Ieva and Anna M. Paganoni},
  doi      = {10.1002/sam.11505},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {241-257},
  title    = {Generalized mixed-effects random forest: A flexible approach to predict university student dropout},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based clustering of time-dependent categorical
sequences with application to the analysis of major life event patterns.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(3), 230–240. (<a
href="https://doi.org/10.1002/sam.11502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Clustering categorical sequences is a problem that arises in many fields. There is a few techniques available in this framework but none of them take into account the possible temporal character of transitions from one state to another. A mixture of Markov models is proposed, where transition probabilities are represented as functions of time. The corresponding expectation–maximization algorithm is discussed along with related computational challenges. The effectiveness of the proposed procedure is illustrated on the set of simulation studies, in which it outperforms four alternative approaches. The method is applied to major life event sequences from the British Household Panel Survey. As reflected by Bayesian Information Criterion, the proposed model demonstrates substantially better performance than its competitors. The analysis of obtained results and related transition probability plots reveals two groups of individuals: people with a conventional development of life course and those encountering some challenges.},
  archive  = {J},
  author   = {Yingying Zhang and Volodymyr Melnykov and Xuwen Zhu},
  doi      = {10.1002/sam.11502},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {230-240},
  title    = {Model-based clustering of time-dependent categorical sequences with application to the analysis of major life event patterns},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised compression of big data. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>14</em>(3),
217–229. (<a href="https://doi.org/10.1002/sam.11508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The phenomenon of big data has become ubiquitous in nearly all disciplines, from science to engineering. A key challenge is the use of such data for fitting statistical and machine learning models, which can incur high computational and storage costs. One solution is to perform model fitting on a carefully selected subset of the data. Various data reduction methods have been proposed in the literature, ranging from random subsampling to optimal experimental design-based methods. However, when the goal is to learn the underlying input–output relationship, such reduction methods may not be ideal, since it does not make use of information contained in the output. To this end, we propose a supervised data compression method called supercompress , which integrates output information by sampling data from regions most important for modeling the desired input–output relationship. An advantage of supercompress is that it is nonparametric—the compression method does not rely on parametric modeling assumptions between inputs and output. As a result, the proposed method is robust to a wide range of modeling choices. We demonstrate the usefulness of supercompress over existing data reduction methods, in both simulations and a taxicab predictive modeling application.},
  archive  = {J},
  author   = {V. Roshan Joseph and Simon Mak},
  doi      = {10.1002/sam.11508},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {217-229},
  title    = {Supervised compression of big data},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emulated order identification for models of big time series
data. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(2), 201–212. (<a
href="https://doi.org/10.1002/sam.11504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This interdisciplinary research includes elements of computing, optimization, and statistics for big data. Specifically, it addresses model order identification aspects of big time series data. Computing and minimizing information criteria, such as BIC, on a grid of integer orders becomes prohibitive for time series recorded at a large number of time points. We propose to compute information criteria only for a sample of integer orders and use kriging-based methods to emulate the information criteria on the rest of the grid. Then we use an efficient global optimization (EGO) algorithm to identify the orders. The method is applied to both ARMA and ARMA-GARCH models. We simulated times series from each type of model of prespecified orders and applied the method to identify the orders. We also used real big time series with tens of thousands of time points to illustrate the method. In particular, we used sentiment scores for news headlines on the economy for ARMA models, and the NASDAQ daily returns for ARMA-GARCH models, from the beginning in 1971 to mid-April 2020 in the early stages of the COVID-19 pandemic. The proposed method identifies efficiently and accurately the orders of models for big time series data.},
  archive  = {J},
  author   = {Brian Wu and Dorin Drignei},
  doi      = {10.1002/sam.11504},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {201-212},
  title    = {Emulated order identification for models of big time series data},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual diagnostics of an explainer model: Tools for the
assessment of LIME explanations. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>14</em>(2), 185–200. (<a
href="https://doi.org/10.1002/sam.11500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The importance of providing explanations for predictions made by black-box models has led to the development of explainer model methods such as LIME (local interpretable model-agnostic explanations). LIME uses a surrogate model to explain the relationship between predictor variables and predictions from a black-box model in a local region around a prediction of interest. However, the quality of the resulting explanations relies on how well the explainer model captures the black-box model in a specified local region. Here we introduce three visual diagnostics to assess the quality of LIME explanations: (1) explanation scatterplots, (2) assessment metric plots, and (3) feature heatmaps. We apply the visual diagnostics to a forensics bullet matching dataset to show examples where LIME explanations depend on the tuning parameter values and the explainer model oversimplifies the black-box model. Our examples raise concerns about claims made of LIME that are similar to other criticisms in the literature.},
  archive  = {J},
  author   = {Katherine Goode and Heike Hofmann},
  doi      = {10.1002/sam.11500},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {185-200},
  title    = {Visual diagnostics of an explainer model: Tools for the assessment of LIME explanations},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subsampling from features in large regression to find
“winning features.” <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>14</em>(2), 168–184. (<a
href="https://doi.org/10.1002/sam.11499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Feature (or variable) selection from a large number of p features continuously challenges data science, especially for ever-enlarging data and in discovering scientifically important features in a regression setting. For example, to develop valid drug targets for ovarian cancer, we must control the false-discovery rate (FDR) of a selection procedure. The popular approach to feature selection in large- p regression uses a penalized likelihood or a shrinkage estimation, such as a LASSO, SCAD, Elastic Net, or MCP procedure. We present a different approach called the Subsampling Winner algorithm (SWA), which subsamples from p features. The idea of SWA is analogous to selecting US national merit scholars&#39; that selects semifinalists based on student&#39;s performance in tests done at local schools (a.k.a. subsample analyses), and then determine the finalists (a.k.a. winning features) from the semifinalists. Due to its subsampling nature, SWA can scale to data of any dimension. SWA also has the best-controlled FDR compared to the penalized and Random Forest procedures while having a competitive true-feature discovery rate. Our application of SWA to an ovarian cancer data revealed functionally important genes and pathways.},
  archive  = {J},
  author   = {Yiying Fan and Jiayang Sun},
  doi      = {10.1002/sam.11499},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {168-184},
  title    = {Subsampling from features in large regression to find “winning features”},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised random forests. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>14</em>(2), 144–167.
(<a href="https://doi.org/10.1002/sam.11498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {sidClustering is a new random forests unsupervised machine learning algorithm. The first step in sidClustering involves what is called sidification of the features: staggering the features to have mutually exclusive ranges (called the staggered interaction data [SID] main features) and then forming all pairwise interactions (called the SID interaction features). Then a multivariate random forest (able to handle both continuous and categorical variables) is used to predict the SID main features. We establish uniqueness of sidification and show how multivariate impurity splitting is able to identify clusters. The proposed sidClustering method is adept at finding clusters arising from categorical and continuous variables and retains all the important advantages of random forests. The method is illustrated using simulated and real data as well as two in depth case studies, one from a large multi-institutional study of esophageal cancer, and the other involving hospital charges for cardiovascular patients.},
  archive  = {J},
  author   = {Alejandro Mantero and Hemant Ishwaran},
  doi      = {10.1002/sam.11498},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {144-167},
  title    = {Unsupervised random forests},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for stability-based module detection in
correlation graphs. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>14</em>(2), 129–143. (<a
href="https://doi.org/10.1002/sam.11495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graphs can be used to represent the direct and indirect relationships between variables, and elucidate complex relationships and interdependencies. Detecting structure within a graph is a challenging problem. This problem is studied over a range of fields and is sometimes termed community detection, module detection, or graph partitioning. A popular class of algorithms for module detection relies on optimizing a function of modularity to identify the structure. In practice, graphs are often learned from the data, and thus prone to uncertainty. In these settings, the uncertainty of the network structure can become exaggerated by giving unreliable estimates of the module structure. In this work, we begin to address this challenge through the use of a nonparametric bootstrap approach to assessing the stability of module detection in a graph. Estimates of stability are presented at the level of the individual node, the inferred modules, and as an overall measure of performance for module detection in a given graph. Furthermore, bootstrap stability estimates are derived for complexity parameter selection that ultimately defines a graph from data in a way that optimizes stability. This approach is utilized in connection with correlation graphs but is generalizable to other graphs that are defined through the use of dissimilarity measures. We demonstrate our approach using a broad range of simulations and on a metabolomics dataset from the Beijing Olympics Air Pollution study. These approaches are implemented using bootcluster package that is available in the R programming language.},
  archive  = {J},
  author   = {Mingmei Tian and Rachael Hageman Blair and Lina Mu and Matthew Bonner and Richard Browne and Han Yu},
  doi      = {10.1002/sam.11495},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {129-143},
  title    = {A framework for stability-based module detection in correlation graphs},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extreme ensemble of extreme learning machines.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(2), 116–128. (<a
href="https://doi.org/10.1002/sam.11493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Extreme learning machine (ELM) has attracted attentions in pattern classification problems due to its preferences in low computations and high generalization. To overcome its drawbacks, caused by the randomness of input weights and biases, the ensemble of ELMs was proposed. The diversity of ELMs forming the ensemble was studied broadly in the literature, via using different activation functions and/or different number of hidden neurons. However, less attention was paid to aggregation mechanism in ensemble of ELMs. To speed up this aggregation process, we propose an ensemble framework for ELMs, called extreme ensemble of ELMs (EEoELMs) because of its extreme speed in ensemble process. In this framework, the input weights of each ELM are randomly pre-assigned as usual. The ELMs make use of the same/distinct activation functions to increase the diversity of classifiers and so the generalization of ensemble. The output weights of each ELM are set using Moore–Penrose inverse method. However, the aggregation mechanism in EEoELM is novel. Instead of using majority/weighted voting on the prediction results of ELMs, their output neurons are combined in a new decision/ensemble layer. The output of this layer determines the ensemble output. As the weights of this decision layer are also computed using Moore–Penrose inverse method, the ensemble is extremely fast. Experimental results on synthetic and real-world datasets indicate the acceptable classification performance of EEoELM in much less computational efforts.},
  archive  = {J},
  author   = {Eghbal G. Mansoori and Massar Sara},
  doi      = {10.1002/sam.11493},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {116-128},
  title    = {Extreme ensemble of extreme learning machines},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A network model that combines latent factors and sparse
graphs. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(2), 97–115. (<a
href="https://doi.org/10.1002/sam.11492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a combined model, which integrates the latent factor model and a sparse graphical model, for network data. It is noticed that neither a latent factor model nor a sparse graphical model alone may be sufficient to capture the structure of the data. The proposed model has a latent (i.e., factor analysis) model to represent the main trends (a.k.a., factors), and a sparse graphical component that captures the remaining ad-hoc dependence. Model selection and parameter estimation are carried out simultaneously via a penalized likelihood approach. The convexity of the objective function allows us to develop an efficient algorithm, while the penalty terms push towards low-dimensional latent components and a sparse graphical structure. The effectiveness of our model is demonstrated via simulation studies, and the model is also applied to four real datasets: Zachary&#39;s Karate club data, Kreb&#39;s U.S. political book dataset ( http://www.orgnet.com ), U.S. political blog dataset , and citation network of statisticians; showing meaningful performances in practical situations.},
  archive  = {J},
  author   = {Namjoon Suh and Xiaoming Huo and Eric Heim and Lee Seversky},
  doi      = {10.1002/sam.11492},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {97-115},
  title    = {A network model that combines latent factors and sparse graphs},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A note on marginal correlation based screening.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>14</em>(1), 88–92. (<a
href="https://doi.org/10.1002/sam.11491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Independence screening methods such as the two-sample t -test and the marginal correlation based ranking are among the most widely used techniques for variable selection in ultrahigh-dimensional data sets. In this short note, simple examples are used to demonstrate potential problems with the independence screening methods in the presence of correlated predictors. Also, an example is considered where all important variables are independent among themselves and all but one important variables are independent with the unimportant variables. Furthermore, a real data example from a genome-wide association study is used to illustrate inferior performance of marginal correlation screening compared to another screening method.},
  archive  = {J},
  author   = {Run Wang and Somak Dutta and Vivekananda Roy},
  doi      = {10.1002/sam.11491},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {88-92},
  title    = {A note on marginal correlation based screening},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive nonparametric exponentially weighted moving
average control chart with dynamic sampling intervals. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>14</em>(1), 74–87. (<a
href="https://doi.org/10.1002/sam.11490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nonparametric statistical control charts have been widely studied over the past decade. The variable sampling intervals (VSIs) methods have been found to exhibit superior performance compared to the traditional fixed sampling policy in this area. In previous VSI studies, the sampling interval function d (·) realized only two distinct values. Limiting the selection of the sampling interval to two fixed values may not be reasonable, especially when the monitoring statistic value is at the edge or near the middle of these two intervals so that the advantages of VSI methods cannot be fully reflected. Therefore, we propose a nonparametric exponentially weighed moving average control chart with a dynamic sampling interval in this article, in which the dynamic sampling interval is an extension of two fixed sampling intervals. The dynamic sampling interval can be continuous valued and is determined by an indicator of the magnitude of the unknown shift. This chart can efficiently detect various magnitudes of shifts and performs robustly for different distributions according to simulation studies. The small computational cost indicates that the method is applicable for monitoring large data streams. Data from an aluminum electrolytic capacitor manufacturing process are used to illustrate the implementation of this chart.},
  archive  = {J},
  author   = {Liu Liu and Qing Peng and Xin Lai and Zepei Deng},
  doi      = {10.1002/sam.11490},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {74-87},
  title    = {An adaptive nonparametric exponentially weighted moving average control chart with dynamic sampling intervals},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning network event sequences using long short-term
memory and second-order statistic loss. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>14</em>(1), 61–73.
(<a href="https://doi.org/10.1002/sam.11489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modeling temporal event sequences on the vertices of a network is an important problem with widespread applications; examples include modeling influences in social networks, preventing crimes by modeling their space–time occurrences, and forecasting earthquakes. Existing solutions for this problem use a parametric approach, whose applicability is limited to event sequences following some well-known distributions, which is not true for many real life event datasets. To overcome this limitation, in this work, we propose a composite recurrent neural network model for learning events occurring in the vertices of a network over time. Our proposed model combines two long short-term memory units to capture base intensity and conditional intensity of an event sequence. We also introduce a second-order statistic loss that penalizes higher divergence between the generated and the target sequence&#39;s distribution of hop count distance of consecutive events. Given a sequence of vertices of a network in which an event has occurred, the proposed model predicts the vertex where the next event would most likely occur. Experimental results on synthetic and real-world datasets validate the superiority of our proposed model in comparison to various baseline methods.},
  archive  = {J},
  author   = {Hao Sha and Mohammad Al Hasan and George Mohler},
  doi      = {10.1002/sam.11489},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {61-73},
  title    = {Learning network event sequences using long short-term memory and second-order statistic loss},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A clustering method for graphical handwriting components and
statistical writership analysis. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>14</em>(1), 41–60. (<a
href="https://doi.org/10.1002/sam.11488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Handwritten documents can be characterized by their content or by the shape of the written characters. We focus on the problem of comparing a person&#39;s handwriting to a document of unknown provenance using the shape of the writing, as is done in forensic applications. To do so, we first propose a method for processing scanned handwritten documents to decompose the writing into small graphical structures, often corresponding to letters. We then introduce a measure of distance between two such structures that is inspired by the graph edit distance, and a measure of center for a collection of the graphs. These measurements are the basis for an outlier tolerant K -means algorithm to cluster the graphs based on structural attributes, thus creating a template for sorting new documents. Finally, we present a Bayesian hierarchical model to capture the propensity of a writer for producing graphs that are assigned to certain clusters. We illustrate the methods using documents from the Computer Vision Lab dataset. We show results of the identification task under the cluster assignments and compare to the same modeling, but with a less flexible grouping method that is not tolerant of incidental strokes or outliers.},
  archive  = {J},
  author   = {Amy M. Crawford and Nicholas S. Berry and Alicia L. Carriquiry},
  doi      = {10.1002/sam.11488},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {41-60},
  title    = {A clustering method for graphical handwriting components and statistical writership analysis},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complementary dimension reduction. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>14</em>(1),
31–40. (<a href="https://doi.org/10.1002/sam.11484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The goal of supervised dimension reduction (SDR) is to find a compact yet informative representation of the feature vector. Most SDR algorithms are formulated to solve sequential optimization problems with objective functions being linear functions of the L 2 norm of the data, for example, the well-known Fisher&#39;s discriminant analysis (FDA). A drawback of such objective functions is that they favor directions that result in large between-class distances; however, if the large between-class distance is mainly from classes that have already been well separated by prior directions, the new direction leads to a negligible improvement over classification accuracy. To address this issue, we introduce an objective function that directly quantifies classification accuracy, and present an efficient algorithm that retrieves directions sequentially from this nonlinear objective function. A key feature of our algorithm is that each sequentially added direction works complementarily with the previous sequentially-solved directions to boost the discriminative power of the reduced space as a whole. So we name our new algorithm “Complementary Dimension Analysis” (CDA). We have further generalized CDA to retrieve sparse directions that involve only a small fraction of the features. Finally we demonstrate the utility of our algorithms on several simulated and real datasets.},
  archive  = {J},
  author   = {Na Cui and Jianjun Hu and Feng Liang},
  doi      = {10.1002/sam.11484},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {31-40},
  title    = {Complementary dimension reduction},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable network estimation with l0 penalty. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>14</em>(1), 18–30. (<a
href="https://doi.org/10.1002/sam.11483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the advent of high-throughput sequencing, an efficient computing strategy is required to deal with large genomic data sets. The challenge of estimating a large precision matrix has garnered substantial research attention for its direct application to discriminant analyses and graphical models. Most existing methods either use a lasso-type penalty that may lead to biased estimators or are computationally intensive, which prevents their applications to very large graphs. We propose using an L 0 penalty to estimate an ultra-large precision matrix ( scalnetL0 ). We apply scalnetL0 to RNA-seq data from breast cancer patients represented in The Cancer Genome Atlas and find improved accuracy of classifications for survival times. The estimated precision matrix provides information about a large-scale co-expression network in breast cancer. Simulation studies demonstrate that scalnetL0 provides more accurate and efficient estimators, yielding shorter CPU time and less Frobenius loss on sparse learning for large-scale precision matrix estimation.},
  archive  = {J},
  author   = {Junghi Kim and Hongtu Zhu and Xiao Wang and Kim-Anh Do},
  doi      = {10.1002/sam.11483},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {18-30},
  title    = {Scalable network estimation with l0 penalty},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Next waves in veridical network embedding*. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>14</em>(1), 5–17. (<a
href="https://doi.org/10.1002/sam.11486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Embedding nodes of a large network into a metric (e.g., Euclidean) space has become an area of active research in statistical machine learning, which has found applications in natural and social sciences. Generally, a representation of a network object is learned in a Euclidean geometry and is then used for subsequent tasks regarding the nodes and/or edges of the network, such as community detection, node classification and link prediction. Network embedding algorithms have been proposed in multiple disciplines, often with domain-specific notations and details. In addition, different measures and tools have been adopted to evaluate and compare the methods proposed under different settings, often dependent of the downstream tasks. As a result, it is challenging to study these algorithms in the literature systematically. Motivated by the recently proposed PCS framework for Veridical Data Science, we propose a framework for network embedding algorithms and discuss how the principles of predictability , computability , and stability (PCS) apply in this context. The utilization of this framework in network embedding holds the potential to motivate and point to new directions for future research.},
  archive  = {J},
  author   = {Owen G. Ward and Zhen Huang and Andrew Davison and Tian Zheng},
  doi      = {10.1002/sam.11486},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {5-17},
  title    = {Next waves in veridical network embedding*},
  volume   = {14},
  year     = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
