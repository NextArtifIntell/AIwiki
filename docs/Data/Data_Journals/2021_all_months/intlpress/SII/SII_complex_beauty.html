<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SII_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sii---44">SII - 44</h2>
<ul>
<li><details>
<summary>
(2021). Demystify lindley’s paradox by connecting <span
class="math inline"><em>p</em></span>-value and posterior probability.
<em>SII</em>, <em>14</em>(4), 489–502. (<a
href="https://doi.org/10.4310/21-SII668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the hypothesis testing framework, $p$‑value is often computed to determine whether to reject the null hypothesis or not. On the other hand, Bayesian approaches typically compute the posterior probability of the null hypothesis to evaluate its plausibility. We revisit Lindley’s paradox and demystify the conflicting results between Bayesian and frequentist hypothesis testing procedures by casting a two-sided hypothesis as a combination of two one-sided hypotheses along the opposite directions. This formulation can naturally circumvent the ambiguities of assigning a point mass to the null and choices of using local or non-local prior distributions. As $p$‑value solely depends on the observed data without incorporating any prior information, we consider non-informative prior distributions for fair comparisons with $p$‑value. The equivalence of $p$‑value and the Bayesian posterior probability of the null hypothesis can be established to reconcile Lindley’s paradox. More complicated settings, such as multivariate cases, random effects models and non-normal data, are also explored for generalization of our results to various hypothesis tests.},
  archive      = {J_SII},
  author       = {Yin, Guosheng and Shi, Haolun},
  doi          = {10.4310/21-SII668},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {489-502},
  shortjournal = {Stat. Interface},
  title        = {Demystify lindley’s paradox by connecting $p$-value and posterior probability},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained estimation in cox model under failure-time
outcome-dependent sampling design. <em>SII</em>, <em>14</em>(4),
475–488. (<a href="https://doi.org/10.4310/21-SII667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The failure-time outcome-dependent sampling (ODS) design is a cost-effective sampling scheme, which can improve the efficiency of the studies by selectively including certain failures to enrich the observed sample. In modeling process, taking some prior constraints on parameters into account may lead to more powerful and efficient inferences. In this paper, we study how to fit the proportional hazards model with parameter constraints to data from a failure-time ODS design. We propose constrained weighted estimation by conducting an optimization problem on a working likelihood function. The asymptotic properties of the proposed estimator are established.We develop a restricted minorization-maximization (MM) algorithm for the numerical calculation of the proposed estimator. Simulation studies are conducted to evaluate the finite-sample performance of the proposed estimator. An application to a data set from a Wilms tumor study is illustrated for the utility of the proposed method.},
  archive      = {J_SII},
  author       = {Yin, Jie and Yang, Changming and Ding, Jieli and Liu, Yanyan},
  doi          = {10.4310/21-SII667},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {475-488},
  shortjournal = {Stat. Interface},
  title        = {Constrained estimation in cox model under failure-time outcome-dependent sampling design},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prior conditioned on scale parameter for bayesian quantile
LASSO and its generalizations. <em>SII</em>, <em>14</em>(4), 459–474.
(<a href="https://doi.org/10.4310/21-SII666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several undesirable issues exist in the Bayesian quantile LASSO and its two generalizations, quantile group LASSO and bridge quantile regression (Alhamzawi et al. [1]; Alhamzawi and Algamal [2]; Li et al. [21]). In this paper, we numerically show that, the joint posterior may be multimodal using unconditional prior for the regression coefficients and the posterior estimates may be sensitive to the hyperparameters in Gamma prior frequently used for the scale parameter. We also theoretically illustrate that the joint posterior may be improper when an invariant prior is used for the scale parameter, especially when predictors outnumber observations. To resolve the issues in a unified framework, we propose applying the priors conditioned on the scale parameter for the coefficients along with invariant prior to the scale parameter. We justify the prior choice under one general likelihood including asymmetric Laplace density and the common class of conditioned priors by establishing the corresponding sufficient and necessary condition of the posterior propriety. In addition, we develop ready-to-use partially collapsed Gibbs sampling algorithms for all methods to aid computations. Simulation studies and a real data example demonstrate that our methods usually outperform the original Bayesian approaches.},
  archive      = {J_SII},
  author       = {Cai, Zhongheng and Sun, Dongchu},
  doi          = {10.4310/21-SII666},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {459-474},
  shortjournal = {Stat. Interface},
  title        = {Prior conditioned on scale parameter for bayesian quantile LASSO and its generalizations},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularized multiple mediation analysis. <em>SII</em>,
<em>14</em>(4), 449–458. (<a
href="https://doi.org/10.4310/21-SII664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is used to explore how an established exposure-outcome relationship is influenced by a third variable (mediator). Multiple mediation analysis refers to the mediation analysis with multiple mediators. We propose to use the elastic net regularized linear regression in multiple mediation analysis when the number of potential mediators is large. In exploring the exposure-mediator-outcome relationship, we regularize coefficients of mediators in predicting the outcome. The penalization on the coefficient is inversely proportional to the association between the exposure variable and each mediator. Therefore, in estimating the effect of a mediator, the exposure-mediator and the mediatoroutcome associations are jointly considered. An R package, mmabig , is compiled for the proposed method. We perform a series of sensitivity and specificity analysis to examine factors that can influence the power of identifying important mediators. Further, we illustrate how to consider potential nonlinear associations among variables in the mediation analysis. Simulation studies have shown that the proposed mediation analysis method consistently obtain larger power when compared with its main competitors. The method is used with a real data set to explore factors that contribute to the racial disparity in survival rates among breast-cancer patients.},
  archive      = {J_SII},
  author       = {Li, Bin and Yu, Qingzhao and Zhang, Lu and Hsieh, Meichin},
  doi          = {10.4310/21-SII664},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {449-458},
  shortjournal = {Stat. Interface},
  title        = {Regularized multiple mediation analysis},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group variable selection for recurrent event model with a
diverging number of covariates. <em>SII</em>, <em>14</em>(4), 431–447.
(<a href="https://doi.org/10.4310/21-SII663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the high-dimensional data, the number of covariates can be large and diverge with the sample size. In this work, we propose an adaptive bi-level penalized method to solve the group variable selection problem for the recurrent event model with a diverging number of covariates. Comparing with the classical group variable selection methods, the adaptive bi-level penalized method can select the important group variables and individual variables simultaneously. For the case of diverging a number of covariates, we demonstrate that the proposed method has selection consistency and the penalized estimators have asymptotic normality. Simulation studies show that the proposed method performs well and the results are consistent with the theoretical properties. The proposed method is illustrated by analyzing a real life data set.},
  archive      = {J_SII},
  author       = {Cai, Kaida and Shen, Hua and Lu, Xuewen},
  doi          = {10.4310/21-SII663},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {431-447},
  shortjournal = {Stat. Interface},
  title        = {Group variable selection for recurrent event model with a diverging number of covariates},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature screening via bergsma–dassios sign correlation
learning. <em>SII</em>, <em>14</em>(4), 417–430. (<a
href="https://doi.org/10.4310/20-SII662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust rank correlation screening (RRCS) procedure that is built on Kendall $\tau$, has been suggested by Li, Peng, Zhang and Zhu (2012) as a robust alternative to the sure independence screening (SIS) method that is based on the Pearson’s correlation. However, as a drawback for certain applications is that $\tau$ may be zero even if there is an association between two random variables, RRCS is not omnibus, only having an ability to detect monotonic effects. In this paper, we use the Bergsma–Dassios sign correlation (Bergsma and Dassios, 2014, $\tau^\ast_b$) to introduce a new SIS procedure.We advocate using the $\tau^\ast_b$‑SIS for three reasons. First, as $\tau^\ast_b$ possesses the necessary and intuitive properties as a correlation index, the $\tau^\ast_b$‑SIS has a better screening ability for nonlinear effects including interactions and heterogeneity compared with the RRCS. Second, as $\tau^\ast_b$ is a natural extension of $\tau$, the $\tau^\ast_b$‑SIS is conceptually simple, easy to implement and robust to the presence of extreme values and outliers in the observations. Third, without assuming any moment condition on the response and predictors, the $\tau^\ast_b$‑SIS enjoys several appealing properties, such as the sure screening property, ranking consistency property and the characteristic of minimum model size. We demonstrate the merits of the $\tau^\ast_b$‑SIS procedure through extensive Monte Carlo experiments and illustrate the method through a real-data example.},
  archive      = {J_SII},
  author       = {He, Daojiang and Hao, Xinxin and Xu, Kai and He, Lei and Liu, Youxin},
  doi          = {10.4310/20-SII662},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {417-430},
  shortjournal = {Stat. Interface},
  title        = {Feature screening via Bergsma–Dassios sign correlation learning},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online multiple learning with working sufficient statistics
for generalized linear models in big data. <em>SII</em>, <em>14</em>(4),
403–416. (<a href="https://doi.org/10.4310/20-SII661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes an online multiple learning approach to generalized linear models (GLMs) in big data. The approach relies on a new concept called working sufficient statistics (WSS), formulated under traditional iteratively reweighted least squares (IRWLS) for maximum likelihood of GLMs. Because the algorithm needs to access the entire data set multiple times, it is impossible to directly apply traditional IRWLS to big data. To overcome the difficulty, a new approach, called one-step IRWLS, is proposed under the framework of the online setting. The work investigates two methods. The first only uses the current data to formulate the objective function. The second also uses the information of the previous data. The simulation studies show that the results given by the second method can be as precise and accurate as those given by the exact maximum likelihood. A nice property is that one-step IRWLS successfully avoids the memory and computational efficiency barriers caused by the volume of big data. As the size of the WSS does not vary with the sample size, the proposed approach can be used even if the size of big data is much higher than the memory size of the computing system.},
  archive      = {J_SII},
  author       = {Zhang, Tonglin and Yang, Baijian},
  doi          = {10.4310/20-SII661},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {403-416},
  shortjournal = {Stat. Interface},
  title        = {Online multiple learning with working sufficient statistics for generalized linear models in big data},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A residual-based approach for robust random forest
regression. <em>SII</em>, <em>14</em>(4), 389–402. (<a
href="https://doi.org/10.4310/20-SII660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel robust approach for random forest regression that is useful when the conditional distribution of the response variable, given predictor values, is contaminated. Residual analysis is used to identify unusual response values in training data, and the contributions of these values are down-weighted accordingly. This approach is motivated by a robust fitting procedure first proposed in the context of locally weighted polynomial regression and scatterplot smoothing. We demonstrate that tuning the parameter in the robustness algorithm using a weighted crossvalidation approach is advantageous when contamination is suspected in training data responses. We conduct extensive simulations, comparing our method to existing robust approaches, some of which have not been compared to one another in prior studies. Our approach outperforms existing techniques on noisy training datasets with response contamination. While no approach is uniformly optimal, ours is consistently competitive with the best existing approaches for robust random forest regression.},
  archive      = {J_SII},
  author       = {Sage, Andrew J. and Genschel, Ulrike and Nettleton, Dan},
  doi          = {10.4310/20-SII660},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {389-402},
  shortjournal = {Stat. Interface},
  title        = {A residual-based approach for robust random forest regression},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-drug combination designs with experiments in silico.
<em>SII</em>, <em>14</em>(4), 373–388. (<a
href="https://doi.org/10.4310/20-SII659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become evident to medical and statistical scientists treating complex diseases that satisfactory efficacy is more likely to be achieved by using combinations of drugs. Experimental design for drug combination in pre-clinical studies is an important stage to move new combination therapies rapidly into clinical trials. The existing design methods for pre-clinical studies are primarily applied to combination experiments with two or three combined drugs. However, as the research of systems biology advancing it is becoming more desire to consider combinations with multiple drugs. In this paper, we propose efficient experimental designs for multi-drug combination studies. The aim of the proposed design is to establish a good quality and high dimensional dose-response model which provides a basis for future developments on statistical analysis for complex multi-drug dose-finding problems. By borrowing the strength of experiments in silico , it turns out that the uniform design measure is the optimal design with respect to model prediction accuracy. Methods for sample size determination and how to construct uniform designs are given. Since the proposed uniform designs are constructed in regular dose regions, they are convenient to be applied to multi-drug combination experiments. The usefulness of the proposed design is illustrated by simulations and an application with multiple combined drugs.},
  archive      = {J_SII},
  author       = {Hengzhen, Huang},
  doi          = {10.4310/20-SII659},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {373-388},
  shortjournal = {Stat. Interface},
  title        = {Multi-drug combination designs with experiments in silico},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and inference for covariate adjusted partially
functional linear regression models. <em>SII</em>, <em>14</em>(4),
359–371. (<a href="https://doi.org/10.4310/20-SII656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce covariate adjusted partially functional linear regression models, in which both the response and the covariates in the non-functional linear component can only be observed after being distorted by some multiplicative factors. We first estimate the distorting functions by nonparametrically regressing the response variables and covariates on the distorting covariate, and then the estimators of the slope function and the partially linear coefficient are obtained using the estimated response variables and covariates and functional principal component analysis based on corrected profile least-squares. We establish the asymptotic properties of the proposed estimators. In addition, using empirical likelihood and functional principal component analysis, we construct confidence intervals and bands for the coefficient parameters and the slope function, respectively. Finally, some simulation studies and an empirical analysis of a real dataset are conducted to illustrate the finite sample performance of the proposed method.},
  archive      = {J_SII},
  author       = {Jiang, Zhiqiang and Huang, Zhensheng and Zhu, Hanbing},
  doi          = {10.4310/20-SII656},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {359-371},
  shortjournal = {Stat. Interface},
  title        = {Estimation and inference for covariate adjusted partially functional linear regression models},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-dimensional correlation matrix estimation for gaussian
data: A bayesian perspective. <em>SII</em>, <em>14</em>(3), 351–358. (<a
href="https://doi.org/10.4310/20-SII655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian covariance or precision matrix estimation is a classical problem in high-dimensional data analyses. For precision matrix estimation, the graphical lasso provides an efficient approach by optimizing the log-likelihood function with $L_1$-norm penalty. Inspired by the success of graphical lasso, researchers pursue analogous outcomes for covariance matrix estimation. However, it suffers from the difficulty of non-convex optimization and a degeneration problem when $p \gt n$ due to the singularity of the sample covariance matrix. In this paper, we fix the degeneration problem by adding an extra constraint on diagonal elements. From the Bayesian perspective, a grid-point gradient descent (GPGD) algorithm together with the block Gibbs sampler is developed to sample from the posterior distribution of the correlation matrix. The algorithm provides an effective approach to draw samples under the positive-definite constraint, and can explore the whole feasible region to attain the mode of the posterior distribution. Simulation studies and a real application demonstrate that our method is competitive with other existing methods in various settings.},
  archive      = {J_SII},
  author       = {Wang, Chaojie and Fan, Xiaodan},
  doi          = {10.4310/20-SII655},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {351-358},
  shortjournal = {Stat. Interface},
  title        = {High-dimensional correlation matrix estimation for gaussian data: A bayesian perspective},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized newton–raphson algorithm for high dimensional
LASSO regression. <em>SII</em>, <em>14</em>(3), 339–350. (<a
href="https://doi.org/10.4310/20-SII643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The least absolute shrinkage and selection operator (LASSO) penalized regression is a state-of-the-art statistical method in high dimensional data analysis, when the number of predictors exceeds the number of observations. The commonly used Newton–Raphson algorithm is not very successful in solving the non-smooth optimization in LASSO. In this paper, we propose a fast generalized Newton–Raphson (GNR) algorithm for LASSO-type problems. The proposed algorithm, derived from a suitable Karush–Kuhn–Tucker (KKT) conditions based on generalized Newton derivatives, is a non-smooth Newton-type method. We first establish the local one-step convergence of GNR and then show that it is very efficient and accurate when coupled with a constinuation strategy. We also develop a novel parameter selection method. Numerical studies of simulated and real data analysis suggest that the GNR algorithm, with better (or comparable) accuracy, is faster than the algorithm implemented in the popular glmnet package.},
  archive      = {J_SII},
  author       = {Shi, Yueyong and Huang, Jian and Jiao, Yuling and Kang, Yicheng and Zhang, Hu},
  doi          = {10.4310/20-SII643},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {339-350},
  shortjournal = {Stat. Interface},
  title        = {Generalized Newton–Raphson algorithm for high dimensional LASSO regression},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference in a mixture additive hazards cure model.
<em>SII</em>, <em>14</em>(3), 323–338. (<a
href="https://doi.org/10.4310/20-SII642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mixture additive hazards (AH) cure model for survival data with a cure fraction. The proposed model integrates a logistic regression model for the proportion of patients cured of disease and an AH model for the uncured patients. Generalized estimating equations are developed for parameter estimation, and the asymptotic properties of the resulting estimators are established. In addition, model-checking methods are presented to assess the adequacy of the model. The finite-sample performance of the proposed method is evaluated through simulation studies. An application to a human papillomavirus positive oropharyngeal cancer study is conducted to illustrate the proposed method.},
  archive      = {J_SII},
  author       = {Han, Dongxiao and He, Haijin and Sun, Liuquan and Song, Xinyuan and Xu, Wei},
  doi          = {10.4310/20-SII642},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {323-338},
  shortjournal = {Stat. Interface},
  title        = {Inference in a mixture additive hazards cure model},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A model checking method for the additive hazards model with
multivariate current status data. <em>SII</em>, <em>14</em>(3), 309–321.
(<a href="https://doi.org/10.4310/20-SII639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a class of graphical and numerical techniques to check the overall fitting adequacy of the marginal additive hazards model to multivariate current status data. The proposed testing methods are based on the supremum of the stochastic processes derived from the cumulative sum of martingale-based residuals over time and covariates. The distributions of the proposed stochastic processes can be approximated via a simulation technique. A series of simulation studies are conducted to assess the finite-sample performance of the proposed methods. An application to a data set from a tumorigenicity study is provided.},
  archive      = {J_SII},
  author       = {Feng, Yanqin and Zhang, Cheng and Ding, Jieli},
  doi          = {10.4310/20-SII639},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {309-321},
  shortjournal = {Stat. Interface},
  title        = {A model checking method for the additive hazards model with multivariate current status data},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual-based tree for clustered binary data. <em>SII</em>,
<em>14</em>(3), 295–308. (<a
href="https://doi.org/10.4310/20-SII638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-based methods are widely used for classification in health sciences research, where data are often clustered. In this paper, we propose a variant of the standard classification and regression tree paradigm (CART) to handle clustered binary outcomes. Using residuals from a null generalized linear mixed model as the response, we build a regression tree to partition the covariate space into rectangles. This circumvents modeling the correlation structure explicitly while still accounting for the cluster-correlated design, thereby allowing us to adopt the standard CART machinery in tree growing, pruning, and cross-validation. Class predictions for each terminal node in the final tree are estimated based on the success probabilities within the specific node. Our method also allows easy extension to ensemble of trees and random forest. Using extensive simulations, we compare our residual-based trees to the standard classification tree. Finally, the methods are illustrated using data from a study of kidney cancer and a study of surgical mortality after colectomy.},
  archive      = {J_SII},
  author       = {Xia, Rong and Friese, Christopher R. and Banerjee, Mousumi},
  doi          = {10.4310/20-SII638},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {295-308},
  shortjournal = {Stat. Interface},
  title        = {Residual-based tree for clustered binary data},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Community detection for statistical citation network by
d-SCORE. <em>SII</em>, <em>14</em>(3), 279–294. (<a
href="https://doi.org/10.4310/20-SII636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide application of statistics, it is important to identify research trends and the development of statistics. In this paper, we analyze a citation network of the top 4 statistical journals from 2001 to 2018, applying the directed spectral clustering on the ratio-of-eigenvectors (D-SCORE) method to detect the community structure of citation network. We find that statistical researchers are becoming more and more collaborative. The number of influential papers which account for the majority of citations is small. High betweenness centrality and high closeness centrality papers are concentrated in Annals of Statistics (AoS). Furthermore, we detect 4 communities and 11 sub-communities such as “High-dimensional Model”, “Variable Selection”, and “Covariance Matrix Analysis”. Then, we compare the results of D-SCORE with three other methods and find that D-SCORE is more suitable for our citation network. Finally, we identify the dynamic nature of the communities. Our findings present trends and topological patterns of statistical papers, and the data set provides a fertile ground for future research on social networks.},
  archive      = {J_SII},
  author       = {Gao, Tianchen and Pan, Rui and Wang, Siyu and Yang, Yuehan and Zhang, Yan},
  doi          = {10.4310/20-SII636},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {279-294},
  shortjournal = {Stat. Interface},
  title        = {Community detection for statistical citation network by D-SCORE},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A test against the stratified additive hazards model.
<em>SII</em>, <em>14</em>(3), 267–277. (<a
href="https://doi.org/10.4310/20-SII635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stratified data are commonly encountered in practice. An interesting problem for this type of data is to test the existence of stratum effect. This paper discusses hypothesis testing of stratum effect for interval-censored data with informative observation times under the stratified additive hazards model. We construct a test statistic for this test problem, and show that the test statistic is asymptotically distributed as a chi-squared distribution. Finite sample performance of the proposed method is assessed through an extensive simulation study, which indicates the procedure works well. A real data set from a hemophilia study is analyzed to illustrate the proposed method.},
  archive      = {J_SII},
  author       = {Feng, Yanqin and Yuan, Xin and Zhao, Shishun},
  doi          = {10.4310/20-SII635},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {267-277},
  shortjournal = {Stat. Interface},
  title        = {A test against the stratified additive hazards model},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extracting scalar measures from functional data with
applications to placebo response. <em>SII</em>, <em>14</em>(3), 255–265.
(<a href="https://doi.org/10.4310/20-SII633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In controlled and observational studies, outcome measures are often observed longitudinally. Such data are difficult to compare among units directly because there is no natural ordering of curves. This is relevant not only in clinical trials, where typically the goal is to evaluate the relative efficacy of treatments on average, but also in the growing and increasingly important area of personalized medicine, where treatment decisions are optimized with respect to a relevant patient outcome. In personalized medicine, there are no methods for optimizing treatment decision rules using longitudinal outcomes, e.g., symptom trajectories, because of the lack of a natural ordering of curves. A typical practice is to summarize the longitudinal response by a scalar outcome that can then be compared across patients, treatments, etc. We describe some of the summaries that are in common use, especially in clinical trials. We consider a general summary measure (weighted average tangent slope) with weights that can be chosen to optimize specific inference depending on the application. We illustrate the methodology on a study of depression treatment, in which it is difficult to separate placebo effects from the specific effects of the antidepressant. We argue that this approach provides a better summary for estimating the benefits of an active treatment than traditional non-weighted averages.},
  archive      = {J_SII},
  author       = {Tarpey, Thaddeus and Petkova, Eva and Ciarleglio, Adam and Ogden, Robert Todd},
  doi          = {10.4310/20-SII633},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {255-265},
  shortjournal = {Stat. Interface},
  title        = {Extracting scalar measures from functional data with applications to placebo response},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust regression model for ordinal response. <em>SII</em>,
<em>14</em>(3), 243–254. (<a
href="https://doi.org/10.4310/20-SII631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal outcome data with covariates occur frequently in statistical practice including applications from biomedicine to marketing research. Most existing methods for this type of data have relied on subjectively specified models that allow order restriction. There are also some semiparametric ordinal models which are more flexible than parametric ones, with fixed link function, they are still not flexible enough to capture the true link or the relationship between the response and covariates. We propose a broadly applicable robust semiparametric ordinal regression model, in which the relationship between the response and covariates is modelled with a nonparametric monotone increasing link function and parametric regression coefficients. This model is more robust and flexible than existing semiparametric and parametric models for this problem. The semiparametric maximum likelihood estimate is used to estimate the model parameters, and the asymptotic properties of the estimates are derived. Simulation studies show clear advantages of the proposed model over existing parametric models, and a real data analysis illustrates the utility of the proposed method.},
  archive      = {J_SII},
  author       = {Yuan, Ao and Duan, Chongyang and Tan, Ming T.},
  doi          = {10.4310/20-SII631},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {243-254},
  shortjournal = {Stat. Interface},
  title        = {Robust regression model for ordinal response},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian confidence intervals for variance of
delta-lognormal distribution with an application to rainfall dispersion.
<em>SII</em>, <em>14</em>(3), 229–241. (<a
href="https://doi.org/10.4310/20-SII630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For climate studies in agriculture, rainfall records often involve data which contain zeros and highly non-zero skewness. This is mostly used in models for prediction or that use the mean for approximation. Rainfall dispersion is also important in evaluations as it can vary enormously, and it is a natural phenomenon which can lead to drought or flood. Herein, the goal of this paper is to propose a variational approximation computed with interval estimator based on Bayesian approach for delta-lognormal variance consisting of the highest posterior density interval based on vague prior (HPD-V) and the method of variance estimates recovery (MOVER). By way of comparison, the performances of these intervals were evaluated in terms of coverage probability and relative average length via a Monte Carlo simulation. The numerical results show that HPD-V was much more likely to outperform the other methods in many situations even large variance, although MOVER became the recommended method when both of variance and the probability of having zero were small. Our methods were then be utilized to analyze the variability in Nan province’s daily rainfall dataset in a comparison with the other methods.},
  archive      = {J_SII},
  author       = {Maneerat, Patcharee and Niwitpong, Suparat and Niwitpong, Sa-Aat},
  doi          = {10.4310/20-SII630},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {229-241},
  shortjournal = {Stat. Interface},
  title        = {Bayesian confidence intervals for variance of delta-lognormal distribution with an application to rainfall dispersion},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grouped variable selection with prior information via the
prior group bridge method. <em>SII</em>, <em>14</em>(2), 211–227. (<a
href="https://doi.org/10.4310/20-SII629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a multiple regression with grouped predictors, it is usually desired to select important groups as well as to select important variables within a group simultaneously. To achieve this so-called “bi-level selection,” group bridge has been developed as a combination of group-level bridge and variable-level lasso penalties. However, in many scientific areas, prior knowledge is available about the importance of certain groups of predictors, leading to the necessity of methodological development to incorporate such valuable information. For a prior-informative group, we propose a new penalty called “group ridge” as a combination of grouplevel ridge and variable-level lasso penalties, which always preserves this group while selects important variables in it. Then, we propose a composite group penalization named “prior group bridge” by applying group ridge and group bridge to prior-informative groups and groups with no prior information, respectively. We prove that prior group bridge achieves estimation and group selection consistencies given that the prior information is correct. In addition, we demonstrate the empirical advantage of prior group bridge over group bridge in terms of estimation, group and variable selection, and prediction through simulation studies. Finally, we apply prior group bridge to a genetic association study of bipolar disorder to illustrate its applicability and efficacy in real applications.},
  archive      = {J_SII},
  author       = {Li, Kai and Mei, Meng and Jiang, Yuan},
  doi          = {10.4310/20-SII629},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {211-227},
  shortjournal = {Stat. Interface},
  title        = {Grouped variable selection with prior information via the prior group bridge method},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-marginal variable screening method for the varying
coefficient cox model. <em>SII</em>, <em>14</em>(2), 197–209. (<a
href="https://doi.org/10.4310/20-SII628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The varying coefficient model has become a very popular statistical tool for describing the dynamic effects of covariates on the response. In this article, we develop a new variable screening method for the varying coefficient Cox model based on the kernel smoothing and group learning methods. The sure screening property is established for ultra-high-dimensional settings. In addition, an iterative groupwise hard-thresholding algorithm is developed to implement our method. Simulation studies are conducted to evaluate the finite sample performances of the proposed method. An application to an ovarian cancer dataset is provided.},
  archive      = {J_SII},
  author       = {Qu, Lianqiang and Sun, Liuquan},
  doi          = {10.4310/20-SII628},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {197-209},
  shortjournal = {Stat. Interface},
  title        = {A non-marginal variable screening method for the varying coefficient cox model},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating equation estimators of quantile differences for
one sample with length-biased and right-censored data. <em>SII</em>,
<em>14</em>(2), 183–195. (<a
href="https://doi.org/10.4310/20-SII626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper estimates quantile differences for one sample with length-biased and right-censored (LBRC) data. To ensure the asymptotic unbiasedness of the estimator, the estimating equation method is adopted. To improve the efficiency of the estimator, in the sense of having a lower mean squared error, the kernel-smoothed approach is employed. To make full use of the features of LBRC data, the augmented inverse probability complete case weight is investigated in detail. Moreover, the consistency and asymptotic normality of the proposed estimators are established. The numerical simulations are conducted to examine the performance of the estimators.},
  archive      = {J_SII},
  author       = {Xun, Li and Zhang, Guangchao and Wang, Dehui and Zhou, Yong},
  doi          = {10.4310/20-SII626},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {183-195},
  shortjournal = {Stat. Interface},
  title        = {Estimating equation estimators of quantile differences for one sample with length-biased and right-censored data},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation and diagnostics for partially linear censored
regression models based on heavy-tailed distributions. <em>SII</em>,
<em>14</em>(2), 165–182. (<a
href="https://doi.org/10.4310/20-SII624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many studies, limited or censored data are collected. This occurs, in several practical situations, for reasons such as limitations of measuring instruments or due to experimental design. So, the responses can be either left, interval or right censored. On the other hand, partially linear models are considered as a flexible generalizations of linear regression models by including a nonparametric component of some covariates in the linear predictor. In this paper, we discuss estimation and diagnostic procedures in partially linear censored regression models with errors following a scale mixture of normal (SMN) distributions. This family of distributions contains a group of well-known heavy-tailed distributions that are often used for robust inference of symmetrical data, such as Student‑t, slash and contaminated normal, among others. A simple EM-type algorithm for iteratively computing maximum penalized likelihood (MPL) estimates of the parameters is presented. To examine the performance of the proposed model, case-deletion and local influence techniques are developed to show its robustness against outlying and influential observations. This is performed by sensitivity analysis of the maximum penalized likelihood estimates under some usual perturbation schemes, either in the model or in the data, and by inspecting some proposed diagnostic graphs. We evaluate the finite sample performance of the algorithm and the asymptotic properties of the MPL estimates through empirical experiments. An application to a real dataset is presented to illustrate the effectiveness of the proposed methods. Both estimation procedure and diagnostic tools were implemented in the $\texttt{RPartCensReg}$ package.},
  archive      = {J_SII},
  author       = {Nuñez Lemus, Marcela and Lachos, Victor H. and Galarza, Christian E. and Matos, Larissa A.},
  doi          = {10.4310/20-SII624},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {165-182},
  shortjournal = {Stat. Interface},
  title        = {Estimation and diagnostics for partially linear censored regression models based on heavy-tailed distributions},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian zero-inflated growth mixture models with
application to health risk behavior data. <em>SII</em>, <em>14</em>(2),
151–163. (<a href="https://doi.org/10.4310/20-SII623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on developing latent class models for longitudinal data with zero-inflated count response variables. The goals are to model discrete longitudinal patterns of rare events counts (for instance, health-risky behavior), and to identify individual-specific covariates associated with latent class probabilities. Two discrete latent structures are present in this type of model: a latent categorical variable that classifies subgroups with distinct developmental trajectories and a latent binary variable that identifies whether an observation is from a zero-inflation process or a regular count process. Within each class, two sets of covariates are used to separately model the probability of structural zeros and the mean trajectories of the count process. The estimation of the latent variables and regression parameters are carried jointly in a hierarchical Bayesian framework. Our methods are validated through a simulation study and then applied to cigarette smoking data, obtained from the National Longitudinal Study of Adolescent Health.},
  archive      = {J_SII},
  author       = {Yang, Si and Puggioni, Gavino},
  doi          = {10.4310/20-SII623},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {151-163},
  shortjournal = {Stat. Interface},
  title        = {Bayesian zero-inflated growth mixture models with application to health risk behavior data},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lead time distribution for individuals with a screening
history. <em>SII</em>, <em>14</em>(2), 131–149. (<a
href="https://doi.org/10.4310/20-SII622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derived the distribution of lead time for periodic screening in the future when an individual has a screening history with negative results. It is a mixture of a point mass at zero and a positive sub-PDF. The motivation comes from the reality that for people in older age, they may already have some screening exams for targeted cancer before and still look healthy and are asymptomatic at their current age. How to evaluate their future screening result is a challenge. We explored how the screening history would affect the lead time if one would be diagnosed with cancer in the future. Simulations were carried out on combinations of different initial screening age, current age, sensitivity, mean sojourn time, and screening schedule in the past and in the future. The method developed can be applied to periodic exams for any kind of chronic disease, such as cancer. We applied our new method of evaluating the lead time distribution for male and female heavy smokers using low-dose computed tomography in the National Lung Screening Trial.},
  archive      = {J_SII},
  author       = {Liu, Ruiqi and Wu, Dongfeng and Rai, Shesh N.},
  doi          = {10.4310/20-SII622},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {131-149},
  shortjournal = {Stat. Interface},
  title        = {Lead time distribution for individuals with a screening history},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information diffusion with network structures. <em>SII</em>,
<em>14</em>(2), 115–129. (<a
href="https://doi.org/10.4310/20-SII619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information diffusion refers to the process about passing certain information from one subject to another. It is a typical and critical phenomenon observed in large scale social networks. To statistically model such a phenomenon, a network diffusion model is proposed and studied. The diffusion process is then investigated under the modeling framework from both the short term and long term perspectives. To estimate the model, a maximum likelihood estimator and a moment estimator are proposed, whose asymptotic properties are further established. The resulting estimators are manifested to have a reliable finite sample performance through a number of numerical studies. Lastly, the diffusion of earthquake news on Sina Weibo is analyzed to illustrate the practical usefulness.},
  archive      = {J_SII},
  author       = {Xuening, Zhu and Pan, Rui and Zhang, Yuxuan and Chen, Yu and Mi, Wenquan and Wang, Hansheng},
  doi          = {10.4310/20-SII619},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {115-129},
  shortjournal = {Stat. Interface},
  title        = {Information diffusion with network structures},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relating parameters in conditional, marginalized, and
marginal logistic models when the mediator is binary. <em>SII</em>,
<em>14</em>(2), 109–114. (<a
href="https://doi.org/10.4310/20-SII618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stanghellini and Doretti (2019) studied the exact formulae relating parameters in conditional and marginalized logistic models when the mediator is binary. Those formulae generally do not hold for the reduced model as the reduced model is generally not the same as the marginalized model. For a conditional model that allows for treatmentmediator interaction, I present 1) alternative exact formulae relating parameters in the conditional model to those in the marginalized logistic model. They are equivalent to but simpler and easier to interpret than those given in Stanghellini and Doretti (2019); 2) a decomposition of the total treatment effect into the natural direct effect and the natural indirect effect without assuming the outcome is rare; 3) exact formulae relating parameters in the conditional model to those in the reduced logistic model by using likelihood equations; 4) a bound on the size of the natural direct effect regardless of whether the treatment is numeric or discrete; and 5) a numerical assessment of the bias of the approximate formulae reported in Valeri and VanderWeele (2013). The relative bias can be greater than 15% even when the prevalence is less than 10%.},
  archive      = {J_SII},
  author       = {Wang, Kai},
  doi          = {10.4310/20-SII618},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {109-114},
  shortjournal = {Stat. Interface},
  title        = {Relating parameters in conditional, marginalized, and marginal logistic models when the mediator is binary},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial regression models for bounded response variables
with evaluation of the degree of dependence. <em>SII</em>,
<em>14</em>(2), 95–107. (<a
href="https://doi.org/10.4310/20-SII617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bounded response variables such as percentages, proportions, or rates are common in applications involving social and educational datasets, including rates of poverty or rates of achievement by municipalities, counties or provinces. New regression models have been proposed in recent years by considering distributions such as the Beta, Simplex and Kumaraswamy models for this type of data. However, to this type of dataset, it is common to observe the spatial dependence of units. For instance, municipalities or counties are organized into states. For this case, the supposition of independence among observations in the same state removes relevant relations between neighboring provinces. In this paper, we present a model of spatially bounded distribution regression with a Bayesian estimation approach where spatial relations are modeled by a spatial random variable with a particular dependence structure, such as the intrinsic conditional autoregressive model or the Leroux definition. Additionally, the Bayesian inferential method and model comparison criteria are discussed. Simulation studies and an application in reading comprehension spatial data are used to illustrate the performance of the proposed model and the estimation method adopted.},
  archive      = {J_SII},
  author       = {Flores, Sandra E. and Prates, Marcos O. and Bazán, Jorge L. and Bolfarine, Heleno B.},
  doi          = {10.4310/20-SII617},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {95-107},
  shortjournal = {Stat. Interface},
  title        = {Spatial regression models for bounded response variables with evaluation of the degree of dependence},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized empirical likelihood for high-dimensional
generalized linear models. <em>SII</em>, <em>14</em>(2), 83–94. (<a
href="https://doi.org/10.4310/20-SII615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop penalized empirical likelihood for parameter estimation and variable selection in high-dimensional generalized linear models. By using adaptive lasso penalty function, we show that the proposed estimator has the oracle property. Also, we consider the problem of testing hypothesis, and show that the nonparametric profiled empirical likelihood ratio statistic has asymptotic chi-square distribution. Some simulations and an application are given to illustrate the performance of the proposed method.},
  archive      = {J_SII},
  author       = {Chen, Xia and Mao, Liyue},
  doi          = {10.4310/20-SII615},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {83-94},
  shortjournal = {Stat. Interface},
  title        = {Penalized empirical likelihood for high-dimensional generalized linear models},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneity learning for SIRS model: An application to the
COVID-19. <em>SII</em>, <em>14</em>(1), 73–81. (<a
href="https://doi.org/10.4310/20-SII644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian Heterogeneity Learning approach for Susceptible-Infected-Removal-Susceptible (SIRS) model that allows underlying clustering patterns for transmission rate, recovery rate, and loss of immunity rate for the latest corona virus (COVID-19) among different regions. Our proposed method provides simultaneously inference on parameter estimation and clustering information which contains both number of clusters and cluster configurations. Specifically, our key idea is to formulates the SIRS model into a hierarchical form and assign the Mixture of Finite mixtures priors for heterogeneity learning. The properties of the proposed models are examined and a Markov chain Monte Carlo sampling algorithm is used to sample from the posterior distribution. Extensive simulation studies are carried out to examine empirical performance of the proposed methods. We further apply the proposed methodology to analyze the state level COVID-19 data in U.S.},
  archive      = {J_SII},
  author       = {Hu, Guanyu and Geng, Junxian},
  doi          = {10.4310/20-SII644},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {73-81},
  shortjournal = {Stat. Interface},
  title        = {Heterogeneity learning for SIRS model: An application to the COVID-19},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting confirmed cases of the COVID-19 pandemic with a
migration-based epidemiological model. <em>SII</em>, <em>14</em>(1),
59–71. (<a href="https://doi.org/10.4310/20-SII641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unprecedented coronavirus disease 2019 (COVID-19) pandemic is still a worldwide threat to human life since its invasion into the daily lives of the public in the first several months of 2020. Predicting the size of confirmed cases is important for countries and communities to make proper prevention and control policies so as to effectively curb the spread of COVID-19. Different from the 2003 SARS epidemic and the worldwide 2009 H1N1 influenza pandemic, COVID-19 has unique epidemiological characteristics in its infectious and recovered compartments. This drives us to formulate a new infectious dynamic model for forecasting the COVID-19 pandemic within the human mobility network, named the SaucIR-model in the sense that the new compartmental model extends the benchmark SIR model by dividing the flow of people in the infected state into asymptomatic, pathologically infected but unconfirmed, and confirmed. Furthermore, we employ dynamic modeling of population flow in the model in order that spatial effects can be incorporated effectively. We forecast the spread of accumulated confirmed cases in some provinces of mainland China and other countries that experienced severe infection during the time period from late February to early May 2020. The novelty of incorporating the geographic spread of the pandemic leads to a surprisingly good agreement with published confirmed case reports. The numerical analysis validates the high degree of predictability of our proposed SaucIR model compared to existing resemblance. The proposed forecasting SaucIR model is implemented in Python. A web-based application is also developed by Dash (under construction).},
  archive      = {J_SII},
  author       = {Wang, Xinyu and Yang, Lu and Zhang, Hong and Yang, Zhouwang and Liu, Catherine},
  doi          = {10.4310/20-SII641},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {59-71},
  shortjournal = {Stat. Interface},
  title        = {Forecasting confirmed cases of the COVID-19 pandemic with a migration-based epidemiological model},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust nonlinear mixed-effects model for COVID-19 death
data. <em>SII</em>, <em>14</em>(1), 49–57. (<a
href="https://doi.org/10.4310/20-SII637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of complex longitudinal data such as COVID-19 deaths is challenging due to several inherent features: (i) similarly-shaped profiles with different decay patterns; (ii) unexplained variation among repeated measurements within each country, possibly interpreted as clustered data since they are obtained from the same country at roughly the same time; and (iii) skewness, outliers or skewed heavy-tailed noises possibly embodied within response variables. This article formulates a robust nonlinear mixed effects model based on the class of scale mixtures of skew-normal distributions to model COVID-19 deaths, which allows analysts to model such data in the presence of the above described features simultaneously. An efficient EM-type algorithm is proposed to carry out maximum likelihood estimation of model parameters. The bootstrap method is used to determine inherent characteristics of the individual nonlinear profiles, such as confidence intervals of the predicted deaths and fitted curves. The specific target is to model COVID-19 death curves from some Latin American countries since this region is the new epicenter of the disease. Moreover, since a mixed-effect framework borrows information from the population-average effects, in our analysis we include some countries from Europe and North America that are in a more advanced stage of the COVID-19 death curve.},
  archive      = {J_SII},
  author       = {Schumacher, Fernanda L. and Ferreira, Clécio S. and Prates, Marcos O. and Lachos, Alberto and Lachos, Victor H.},
  doi          = {10.4310/20-SII637},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {49-57},
  shortjournal = {Stat. Interface},
  title        = {A robust nonlinear mixed-effects model for COVID-19 death data},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel intervention recurrent autoencoder for real time
forecasting and non-pharmaceutical intervention selection to curb the
spread of covid-19 in the world. <em>SII</em>, <em>14</em>(1), 37–47.
(<a href="https://doi.org/10.4310/SII.2021.v14.n1.a10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Covid-19 pandemic soars around the world, there is urgent need to forecast the number of cases worldwide at its peak, the length of the pandemic before receding and implement public health interventions to significantly stop the spread of Covid-19. Widely used statistical and computer methods for modeling and forecasting the trajectory of Covid-19 are epidemiological models. Although these epidemiological models are useful for estimating the dynamics of transmission od epidemics, their prediction accuracies are quite low. To overcome this limitation, we formulated the real-time forecasting and evaluating multiple public health intervention problem into forecasting treatment response problem and developed recurrent neural network (RNN) for modeling the transmission dynamics of the epidemics and Counterfactual-RNN (CRNN) for evaluating and exploring public health intervention strategies to slow down the spread of Covid-19 worldwide. We applied the developed methods to the real data collected from January 22, 2020 to May 8, 2020 for real-time forecasting the confirmed cases of Covid-19 across the world.},
  archive      = {J_SII},
  author       = {Ge, Qiyang and Hu, Zixin and Li, Shudi and Lin, Wei and Jin, Li and Xiong, Momiao},
  doi          = {10.4310/SII.2021.v14.n1.a10},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {37-47},
  shortjournal = {Stat. Interface},
  title        = {A novel intervention recurrent autoencoder for real time forecasting and non-pharmaceutical intervention selection to curb the spread of covid-19 in the world},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Rejoinder of “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method.” <em>SII</em>, <em>14</em>(1), 33–36. (<a
href="https://doi.org/10.4310/20-SII654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SII},
  author       = {Tian, Ting and Luo, Wenxiang and Tan, Jianbin and Jiang, Yukang and Chen, Minqiong and Pan, Wenliang and Yang, Songpan and Zhao, Jiashu and Wang, Xueqin and Zhang, Heping},
  doi          = {10.4310/20-SII654},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {33-36},
  shortjournal = {Stat. Interface},
  title        = {Rejoinder of “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method”},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The need to incorporate communities in compartmental models.
<em>SII</em>, <em>14</em>(1), 29–32. (<a
href="https://doi.org/10.4310/20-SII647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tian et al. provide a framework for assessing populationlevel interventions of disease outbreaks through the construction of counterfactuals in a large-scale, natural experiment assessing the efficacy of mild, but early interventions compared to delayed interventions. The technique is applied to the recent SARS-CoV-2 outbreak with the population of Shenzhen, China acting as the mild-but-early treatment group and a combination of several US counties resembling Shenzhen but enacting a delayed intervention acting as the control. To help further the development of this framework and identify an avenue for further enhancement, we focus on the use and potential limitations of compartmental models. In particular, compartmental models make assumptions about the communicability of a disease that may not perform well when they are used for large areas with multiple communities where movement is restricted. To illustrate this phenomena, we provide a simulation of a directed percolation (outbreak) process on a simple stochastic block model with two blocks. The simulations show that when transmissibility between two communities is severely restricted an outbreak in two communities resembles a primary and secondary outbreak potentially causing policy and decision makers to mistake effective intervention strategies with noncompliance or inefficacy of an intervention.},
  archive      = {J_SII},
  author       = {Kane, Michael J. and Gilani, Owais},
  doi          = {10.4310/20-SII647},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {29-32},
  shortjournal = {Stat. Interface},
  title        = {The need to incorporate communities in compartmental models},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method” by tian et al. <em>SII</em>, <em>14</em>(1),
25–28. (<a href="https://doi.org/10.4310/20-SII653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tian et al. ought to be commended for their approach of using synthetic control methodology (SCM) to evaluate effectiveness of mild intervention strategies (e.g. wearing masks, isolation of overseas travelers, etc.) in controlling the spread of COVID-19 in industrial regions. The authors use Shenzhen in the Guangdong province of China as an example and compare it with several control counties in the United States. While SCM is often used for causal inference based on observational data in economics and social science literature, it is a relatively new tool in public health research (Bouttell et al. , 2018; Rehkopf &amp; Basu, 2018). In this discussion article, we comment on the imperfect data and the resultant biases one needs to be mindful of; and briefly describe the inferential framework of this new epidemiologic tool, its usefulness and potential concerns.We also comment on what could have been done differently.},
  archive      = {J_SII},
  author       = {Ray, Debashree and Bhattacharyya, Rupam and Mukherjee, Bhramar},
  doi          = {10.4310/20-SII653},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {25-28},
  shortjournal = {Stat. Interface},
  title        = {Discussion on “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method” by tian et al.},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method” by tian et al. <em>SII</em>, <em>14</em>(1),
23–24. (<a href="https://doi.org/10.4310/20-SII649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SII},
  author       = {Chen, Song Xi and Zheng, Xianyu},
  doi          = {10.4310/20-SII649},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {23-24},
  shortjournal = {Stat. Interface},
  title        = {Discussion on “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method” by tian et al.},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method” by tian et al. <em>SII</em>, <em>14</em>(1),
21–22. (<a href="https://doi.org/10.4310/20-SII652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a pleasure to have the opportunity to comment on this contribution by [6]. The authors use a matching technique called the synthetic control method (SCM) [1] to compare the spread of the COVID-19 pandemic in Shenzhen, China with a synthetic reference population in the USA that matches certain characteristics of Shenzhen, as chosen by the authors. The primary goal of this analysis is to examine the effectiveness of early interventions in the containment of the infectious disease. The basic idea of the SCM is to create and compare a ‘control region’ versus the ‘treatment region’, which in this case is Shenzhen, where a policy change has taken place. The invocation of the SCM in a counterfactual framework to the study of intervention policies for the infectious disease is interesting, although there are certain challenging technical issues involved. In this discussion, we will focus on the following domains of challenges in this type of ‘case-and-control’ analysis.},
  archive      = {J_SII},
  author       = {Purkayastha, Soumik and Song, Peter},
  doi          = {10.4310/20-SII652},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {21-22},
  shortjournal = {Stat. Interface},
  title        = {Discussion on “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method” by tian et al.},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method” by tian et al. <em>SII</em>, <em>14</em>(1),
19–20. (<a href="https://doi.org/10.4310/20-SII645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides an overview and discussion of the recent published paper from Tian et al. on modeling the differences of COVID-19 outbreak between Shenzhen and a synthetic population constructed from 68 US counties.},
  archive      = {J_SII},
  author       = {Zhu, Yifan},
  doi          = {10.4310/20-SII645},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {19-20},
  shortjournal = {Stat. Interface},
  title        = {Discussion on “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method” by tian et al.},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method” by tian et al. <em>SII</em>, <em>14</em>(1),
15–17. (<a href="https://doi.org/10.4310/20-SII648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ongoing pandemic of the novel coronavirus disease 2019 (COVID-19) has impacted tens of millions of people and caused a huge economic loss. Most of the impacted countries have implemented different non-pharmaceutical interventions (NPIs) to control and prevent the spreading of SARS-Cov-2, which is the virus causing COVID-19. With the coming flu season in the northern hemisphere, many countries are preparing for the potential second or third wave of COVID-19. Therefore it is crucial to understand the differential timing and effectiveness of these NPIs. We congratulate the authors for a very stimulating paper on this timely and crucial topic. The paper tackles several important questions regarding the evaluation of the effects of mild intervention policies for reducing the transmission of SARSCov- 2, with available observational data amid the ongoing pandemic. The proposed approach combines a variety of statistical tools and practical wisdom in an intriguing manner. There’s no doubt about the importance and the potential impact of this paper. In the following, we would like to further discuss several related aspects.},
  archive      = {J_SII},
  author       = {Chen, Kun and Wang, Fei},
  doi          = {10.4310/20-SII648},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {15-17},
  shortjournal = {Stat. Interface},
  title        = {Discussion on “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method” by tian et al.},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discussion on “the timing and effectiveness of implementing
mild interventions of COVID-19 in large industrial regions via a
synthetic control method” by tian et al. <em>SII</em>, <em>14</em>(1),
13–14. (<a href="https://doi.org/10.4310/20-SII640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SII},
  author       = {Tang, Lu},
  doi          = {10.4310/20-SII640},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {13-14},
  shortjournal = {Stat. Interface},
  title        = {Discussion on “The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method” by tian et al.},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). The timing and effectiveness of implementing mild
interventions of COVID-19 in large industrial regions via a synthetic
control method. <em>SII</em>, <em>14</em>(1), 3–12. (<a
href="https://doi.org/10.4310/20-SII634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of novel coronavirus disease (COVID-19) has spread around the world since it was detected in December 2019. The Chinese government executed a series of interventions to curb the pandemic. The “battle” against COVID-19 in Shenzhen, China is valuable because populated industrial cities are the epic centres of COVID-19 in many regions. We made use of synthetic control methods to create a reference population matching specific characteristics of Shenzhen. With both the synthetic and observed data, we introduced an epidemic compartmental model to compare the spread of COVID-19 between Shenzhen and its counterpart regions in the United States that didn’t implement interventions for policy evaluation. Once the effects of policy interventions adopted in Shenzhen were estimated, the delay effects of those interventions were referred to provide the further control degree of interventions. Thus, the hypothetical epidemic situations in Shenzhen were inferred by using time-varying reproduction numbers in the proposed SIHR (Susceptible, Infectious, Hospitalized, Removed) model and considering if the interventions were delayed by 0 day to 5 days. The expected cumulative confirmed cases would be 1546, which is 5.75 times of the observed cumulative confirmed cases of 269 in Shenzhen on February 3, 2020, based on the data from the counterpart counties (mainly from Broward, New York, Santa Clara, Pinellas, and Westchester) in the United States. If the interventions were delayed by 5 days from the day when the interventions started, the expected cumulative confirmed cases of COVID-19 in Shenzhen on February 3, 2020 would be 676 with 95% credible interval (303,1959). Early implementation of mild interventions can subdue the epidemic of COVID-19. The later the interventions were implemented, the more severe the epidemic was in the hard-hit areas. Mild interventions are less damaging to the society but can be effective when implemented early.},
  archive      = {J_SII},
  author       = {Tian, Ting and Luo, Wenxiang and Tan, Jianbin and Jiang, Yukang and Chen, Minqiong and Pan, Wenliang and Yang, Songpan and Zhao, Jiashu and Wang, Xueqin and Zhang, Heping},
  doi          = {10.4310/20-SII634},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {3-12},
  shortjournal = {Stat. Interface},
  title        = {The timing and effectiveness of implementing mild interventions of COVID-19 in large industrial regions via a synthetic control method},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Editorial. <em>SII</em>, <em>14</em>(1), 1–2. (<a
href="https://intlpress.com/site/pub/pages/journals/items/sii/content/vols/0014/0001/f001/index.php">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SII},
  author       = {de Castro, Mário and Heng, Lian and Liu, Catherine and Tong, Tiejun and Wang, Xia and Chen, Ming-Hui and Wang, Yuedong},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Stat. Interface},
  title        = {Editorial},
  url          = {https://intlpress.com/site/pub/pages/journals/items/sii/content/vols/0014/0001/f001/index.php},
  volume       = {14},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
