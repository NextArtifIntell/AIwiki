<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv---168">IJCV - 168</h2>
<ul>
<li><details>
<summary>
(2021). DeMoCap: Low-cost marker-based motion capture.
<em>IJCV</em>, <em>129</em>(12), 3338–3366. (<a
href="https://doi.org/10.1007/s11263-021-01526-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical marker-based motion capture (MoCap) remains the predominant way to acquire high-fidelity articulated body motions. We introduce DeMoCap, the first data-driven approach for end-to-end marker-based MoCap, using only a sparse setup of spatio-temporally aligned, consumer-grade infrared-depth cameras. Trading off some of their typical features, our approach is the sole robust option for far lower-cost marker-based MoCap than high-end solutions. We introduce an end-to-end differentiable markers-to-pose model to solve a set of challenges such as under-constrained position estimates, noisy input data and spatial configuration invariance. We simultaneously handle depth and marker detection noise, label and localize the markers, and estimate the 3D pose by introducing a novel spatial 3D coordinate regression technique under a multi-view rendering and supervision concept. DeMoCap is driven by a special dataset captured with 4 spatio-temporally aligned low-cost Intel RealSense D415 sensors and a 24 MXT40S camera professional MoCap system, used as input and ground truth, respectively.},
  archive      = {J_IJCV},
  author       = {Chatzitofis, Anargyros and Zarpalas, Dimitrios and Daras, Petros and Kollias, Stefanos},
  doi          = {10.1007/s11263-021-01526-z},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3338-3366},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DeMoCap: Low-cost marker-based motion capture},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D-FUTURE: 3D furniture shape with TextURE. <em>IJCV</em>,
<em>129</em>(12), 3313–3337. (<a
href="https://doi.org/10.1007/s11263-021-01534-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 9992 modern 3D furniture shapes with high-resolution textures and detailed attributes. To support the studies of 3D modeling from images, we couple the CAD models with 20,240 scene images. The room scenes are designed by professional designers or generated by an industrial scene creating system. Given the well-organized 3D-FUTURE and its characteristics, we provide a package of baseline experiments, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, texture recovery for 3D shapes, and furniture composition, to facilitate related future researches on our database.},
  archive      = {J_IJCV},
  author       = {Fu, Huan and Jia, Rongfei and Gao, Lin and Gong, Mingming and Zhao, Binqiang and Maybank, Steve and Tao, Dacheng},
  doi          = {10.1007/s11263-021-01534-z},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3313-3337},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {3D-FUTURE: 3D furniture shape with TextURE},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NAS-FCOS: Efficient search for object detection
architectures. <em>IJCV</em>, <em>129</em>(12), 3299–3312. (<a
href="https://doi.org/10.1007/s11263-021-01523-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has shown great potential in effectively reducing manual effort in network design by automatically discovering optimal architectures. What is noteworthy is that as of now, object detection is less touched by NAS algorithms despite its significant importance in computer vision. To the best of our knowledge, most of the recent NAS studies on object detection tasks fail to satisfactorily strike a balance between performance and efficiency of the resulting models, let alone the excessive amount of computational resources cost by those algorithms. Here we propose an efficient method to obtain better object detectors by searching for the feature pyramid network as well as the prediction head of a simple anchor-free object detector, namely, FCOS (Tian et al. in FCOS: Fully convolutional one-stage object detection, 2019), using a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms, and strategies for evaluating network quality, we are able to find top-performing detection architectures within 4 days using 8 V100 GPUs. The discovered architectures surpass state-of-the-art object detection models (such as Faster R-CNN, RetinaNet and, FCOS) by 1.0 to 5.4\% points in AP on the COCO dataset, with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS method for object detection. Code is available at https://github.com/Lausannen/NAS-FCOS .},
  archive      = {J_IJCV},
  author       = {Wang, Ning and Gao, Yang and Chen, Hao and Wang, Peng and Tian, Zhi and Shen, Chunhua and Zhang, Yanning},
  doi          = {10.1007/s11263-021-01523-2},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3299-3312},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {NAS-FCOS: Efficient search for object detection architectures},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-view snapshot compressive imaging via optical flow
aided recurrent neural network. <em>IJCV</em>, <em>129</em>(12),
3279–3298. (<a
href="https://doi.org/10.1007/s11263-021-01532-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual-view snapshot compressive imaging (SCI) aims to capture videos from two field-of-views (FoVs) using a 2D sensor (detector) in a single snapshot, achieving joint FoV and temporal compressive sensing, and thus enjoying the advantages of low-bandwidth, low-power and low-cost. However, it is challenging for existing model-based decoding algorithms to reconstruct each individual scene, which usually require exhaustive parameter tuning with extremely long running time for large scale data. In this paper, we propose an optical flow-aided recurrent neural network for dual video SCI systems, which provides high-quality decoding in seconds. Firstly, we develop a diversity amplification method to enlarge the differences between scenes of two FoVs, and design a deep convolutional neural network with dual branches to separate different scenes from the single measurement. Secondly, we integrate the bidirectional optical flow extracted from adjacent frames with the recurrent neural network to jointly reconstruct each video in a sequential manner. Extensive results on both simulation and real data demonstrate the superior performance of our proposed model in short inference time. The code and data are available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI .},
  archive      = {J_IJCV},
  author       = {Lu, Ruiying and Chen, Bo and Liu, Guanliang and Cheng, Ziheng and Qiao, Mu and Yuan, Xin},
  doi          = {10.1007/s11263-021-01532-1},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3279-3298},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dual-view snapshot compressive imaging via optical flow aided recurrent neural network},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Deep trajectory post-processing and position projection for
single &amp; multiple camera multiple object tracking. <em>IJCV</em>,
<em>129</em>(12), 3255–3278. (<a
href="https://doi.org/10.1007/s11263-021-01527-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Object Tracking (MOT) has attracted increasing interests in recent years, which plays a significant role in video analysis. MOT aims to track the specific targets as whole trajectories and locate the positions of the trajectory at different times. These trajectories are usually applied in Action Recognition, Anomaly Detection, Crowd Analysis and Multiple-Camera Tracking, etc. However, existing methods are still a challenge in complex scene. Generating false (impure, incomplete) tracklets directly affects the performance of subsequent tasks. Therefore, we propose a novel architecture, Siamese Bi-directional GRU, to construct Cleaving Network and Re-connection Network as trajectory post-processing. Cleaving Network is able to split the impure tracklets as several pure sub-tracklets, and Re-connection Network aims to re-connect the tracklets which belong to same person as whole trajectory. In addition, our methods are extended to Multiple-Camera Tracking, however, current methods rarely consider the spatial-temporal constraint, which increases redundant trajectory matching. Therefore, we present Position Projection Network (PPN) to convert trajectory position from local camera-coordinate to global world-coodrinate, which provides adequate and accurate temporal-spatial information for trajectory association. The proposed technique is evaluated over two widely used datasets MOT16 and Duke-MTMCT, and experiments demonstrate its superior effectiveness as compared with the state-of-the-arts.},
  archive      = {J_IJCV},
  author       = {Ma, Cong and Yang, Fan and Li, Yuan and Jia, Huizhu and Xie, Xiaodong and Gao, Wen},
  doi          = {10.1007/s11263-021-01527-y},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3255-3278},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep trajectory post-processing and position projection for single &amp; multiple camera multiple object tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-constrained deep semi-supervised coupled factorization
network with enriched prior. <em>IJCV</em>, <em>129</em>(12), 3233–3254.
(<a href="https://doi.org/10.1007/s11263-021-01524-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegativity based matrix factorization is usually powerful for learning the parts-based “shallow” representation, however it fails to discover deep hidden information within both the basis concept and representation spaces. In this paper, we therefore propose a new dual-constrained deep semi-supervised coupled factorization network (DS2CF-Net) for learning hierarchical representations. DS2CF-Net is formulated as the joint partial-label and structure-constrained deep factorization network using multi-layers of linear transformations, which coupled updates the basic concepts and new representations in each layer. An error correction mechanism with feature fusion strategy is also integrated between consecutive layers to improve the representation ability of features. To improve the discriminating abilities of both representation and coefficients in feature space, we clearly consider how to enrich the prior knowledge by the coefficients-based label prediction, and incorporate the enriched prior knowledge as the additional label and structure constraints. To be specific, the label constraint enables the intra-class samples to have the same coordinate in the feature space, while the structure constraint forces the coefficients in each layer to be block-diagonal so that the enriched prior knowledge are more accurate. Besides, we integrate the adaptive dual-graph learning to retain the locality structures of both the data manifold and feature manifold in each layer. Finally, a fine-tuning process is performed to refine the structure-constrained matrix and data weight matrix in each layer using the predicted labels for more accurate representations. Extensive simulations on public databases show that our method can obtain state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Zhang, Yan and Zhang, Zhao and Wang, Yang and Zhang, Zheng and Zhang, Li and Yan, Shuicheng and Wang, Meng},
  doi          = {10.1007/s11263-021-01524-1},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3233-3254},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dual-constrained deep semi-supervised coupled factorization network with enriched prior},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical domain-adapted feature learning for video
saliency prediction. <em>IJCV</em>, <em>129</em>(12), 3216–3232. (<a
href="https://doi.org/10.1007/s11263-021-01519-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a 3D fully convolutional architecture for video saliency prediction that employs hierarchical supervision on intermediate maps (referred to as conspicuity maps) generated using features extracted at different abstraction levels. We provide the base hierarchical learning mechanism with two techniques for domain adaptation and domain-specific learning. For the former, we encourage the model to unsupervisedly learn hierarchical general features using gradient reversal at multiple scales, to enhance generalization capabilities on datasets for which no annotations are provided during training. As for domain specialization, we employ domain-specific operations (namely, priors, smoothing and batch normalization) by specializing the learned features on individual datasets in order to maximize performance. The results of our experiments show that the proposed model yields state-of-the-art accuracy on supervised saliency prediction. When the base hierarchical model is empowered with domain-specific modules, performance improves, outperforming state-of-the-art models on three out of five metrics on the DHF1K benchmark and reaching the second-best results on the other two. When, instead, we test it in an unsupervised domain adaptation setting, by enabling hierarchical gradient reversal layers, we obtain performance comparable to supervised state-of-the-art. Source code, trained models and example outputs are publicly available at https://github.com/perceivelab/hd2s .},
  archive      = {J_IJCV},
  author       = {Bellitto, G. and Proietto Salanitri, F. and Palazzo, S. and Rundo, F. and Giordano, D. and Spampinato, C.},
  doi          = {10.1007/s11263-021-01519-y},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3216-3232},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical domain-adapted feature learning for video saliency prediction},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visiting the invisible: Layer-by-layer completed scene
decomposition. <em>IJCV</em>, <em>129</em>(12), 3195–3215. (<a
href="https://doi.org/10.1007/s11263-021-01517-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing scene understanding systems mainly focus on recognizing the visible parts of a scene, ignoring the intact appearance of physical objects in the real-world. Concurrently, image completion has aimed to create plausible appearance for the invisible regions, but requires a manual mask as input. In this work, we propose a higher-level scene understanding system to tackle both visible and invisible parts of objects and backgrounds in a given scene. Particularly, we built a system to decompose a scene into individual objects, infer their underlying occlusion relationships, and even automatically learn which parts of the objects are occluded that need to be completed. In order to disentangle the occluded relationships of all objects in a complex scene, we use the fact that the front object without being occluded is easy to be identified, detected, and segmented. Our system interleaves the two tasks of instance segmentation and scene completion through multiple iterations, solving for objects layer-by-layer. We first provide a thorough experiment using a new realistically rendered dataset with ground-truths for all invisible regions. To bridge the domain gap to real imagery where ground-truths are unavailable, we then train another model with the pseudo-ground-truths generated from our trained synthesis model. We demonstrate results on a wide variety of datasets and show significant improvement over the state-of-the-art.},
  archive      = {J_IJCV},
  author       = {Zheng, Chuanxia and Dao, Duy-Son and Song, Guoxian and Cham, Tat-Jen and Cai, Jianfei},
  doi          = {10.1007/s11263-021-01517-0},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3195-3215},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Visiting the invisible: Layer-by-layer completed scene decomposition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel-in-pixel net: Towards efficient facial landmark
detection in the wild. <em>IJCV</em>, <em>129</em>(12), 3174–3194. (<a
href="https://doi.org/10.1007/s11263-021-01521-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, heatmap regression models have become popular due to their superior performance in locating facial landmarks. However, three major problems still exist among these models: (1) they are computationally expensive; (2) they usually lack explicit constraints on global shapes; (3) domain gaps are commonly present. To address these problems, we propose Pixel-in-Pixel Net (PIPNet) for facial landmark detection. The proposed model is equipped with a novel detection head based on heatmap regression, which conducts score and offset predictions simultaneously on low-resolution feature maps. By doing so, repeated upsampling layers are no longer necessary, enabling the inference time to be largely reduced without sacrificing model accuracy. Besides, a simple but effective neighbor regression module is proposed to enforce local constraints by fusing predictions from neighboring landmarks, which enhances the robustness of the new detection head. To further improve the cross-domain generalization capability of PIPNet, we propose self-training with curriculum. This training strategy is able to mine more reliable pseudo-labels from unlabeled data across domains by starting with an easier task, then gradually increasing the difficulty to provide more precise labels. Extensive experiments demonstrate the superiority of PIPNet, which obtains new state-of-the-art results on three out of six popular benchmarks under the supervised setting. The results on two cross-domain test sets are also consistently improved compared to the baselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200 FPS on CPU and GPU, respectively, while still maintaining a competitive accuracy to state-of-the-art methods. The code of PIPNet is available at https://github.com/jhb86253817/PIPNet .},
  archive      = {J_IJCV},
  author       = {Jin, Haibo and Liao, Shengcai and Shao, Ling},
  doi          = {10.1007/s11263-021-01521-4},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3174-3194},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pixel-in-pixel net: Towards efficient facial landmark detection in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on deep learning for video
analysis and compression. <em>IJCV</em>, <em>129</em>(12), 3171–3173.
(<a href="https://doi.org/10.1007/s11263-021-01530-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Xu, Dong and Chellappa, Rama and Van Gool, Luc and Lu, Guo},
  doi          = {10.1007/s11263-021-01530-3},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3171-3173},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on deep learning for video analysis and compression},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer vision and pattern recognition 2020. <em>IJCV</em>,
<em>129</em>(12), 3169–3170. (<a
href="https://doi.org/10.1007/s11263-021-01522-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Akata, Zeynep and Geiger, Andreas and Sattler, Torsten},
  doi          = {10.1007/s11263-021-01522-3},
  journal      = {International Journal of Computer Vision},
  number       = {12},
  pages        = {3169-3170},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Computer vision and pattern recognition 2020},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Norm-aware embedding for efficient person search and
tracking. <em>IJCV</em>, <em>129</em>(11), 3154–3168. (<a
href="https://doi.org/10.1007/s11263-021-01512-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person detection and Re-identification are two well-defined support tasks for practically relevant tasks such as Person Search and Multiple Person Tracking. Person Search aims to find and locate all instances with the same identity as the query person in a set of panoramic gallery images. Similarly, Multiple Person Tracking, especially when using the tracking-by-detection pipeline, requires to detect and associate all appeared persons in consecutive video frames. One major challenge shared by the two tasks comes from the contradictory goals of detection and re-identification, i.e, person detection focuses on finding the commonness of all persons while person re-ID handles the differences among multiple identities. Therefore, it is crucial to reconcile the relationship between the two support tasks in a joint model. To this end, we present a novel approach called Norm-Aware Embedding to disentangle the person embedding into norm and angle for detection and re-ID respectively, allowing for both effective and efficient multi-task training. We further extend the proposal-level person embedding to pixel-level, whose discrimination ability is less affected by misalignment. Our Norm-Aware Embedding achieves remarkable performance on both person search and multiple person tracking benchmarks, with the merit of being easy to train and resource-friendly.},
  archive      = {J_IJCV},
  author       = {Chen, Di and Zhang, Shanshan and Yang, Jian and Schiele, Bernt},
  doi          = {10.1007/s11263-021-01512-5},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3154-3168},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Norm-aware embedding for efficient person search and tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic bottlenecks: Quantifying and improving
inspectability of deep representations. <em>IJCV</em>, <em>129</em>(11),
3136–3153. (<a
href="https://doi.org/10.1007/s11263-021-01498-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s deep learning systems deliver high performance based on end-to-end training but are notoriously hard to inspect. We argue that there are at least two reasons making inspectability challenging: (i) representations are distributed across hundreds of channels and (ii) a unifying metric quantifying inspectability is lacking. In this paper, we address both issues by proposing Semantic Bottlenecks (SB), which can be integrated into pretrained networks, to align channel outputs with individual visual concepts and introduce the model agnostic Area Under inspectability Curve (AUiC) metric to measure the alignment. We present a case study on semantic segmentation to demonstrate that SBs improve the AUiC up to six-fold over regular network outputs. We explore two types of SB-layers in this work. First, concept-supervised SB-layers (SSB), which offer inspectability w.r.t. predefined concepts that the model is demanded to rely on. And second, unsupervised SBs (USB), which offer equally strong AUiC improvements by restricting distributedness of representations across channels. Importantly, for both SB types, we can recover state of the art segmentation performance across two different models despite a drastic dimensionality reduction from 1000s of non aligned channels to 10s of semantics-aligned channels that all downstream results are based on.},
  archive      = {J_IJCV},
  author       = {Losch, Max and Fritz, Mario and Schiele, Bernt},
  doi          = {10.1007/s11263-021-01498-0},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3136-3153},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic bottlenecks: Quantifying and improving inspectability of deep representations},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The fishyscapes benchmark: Measuring blind spots in semantic
segmentation. <em>IJCV</em>, <em>129</em>(11), 3119–3135. (<a
href="https://doi.org/10.1007/s11263-021-01511-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has enabled impressive progress in the accuracy of semantic segmentation. Yet, the ability to estimate uncertainty and detect failure is key for safety-critical applications like autonomous driving. Existing uncertainty estimates have mostly been evaluated on simple tasks, and it is unclear whether these methods generalize to more complex scenarios. We present Fishyscapes, the first public benchmark for anomaly detection in a real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects. We adapt state-of-the-art methods to recent semantic segmentation models and compare uncertainty estimation approaches based on softmax confidence, Bayesian learning, density estimation, image resynthesis, as well as supervised anomaly detection methods. Our results show that anomaly detection is far from solved even for ordinary situations, while our benchmark allows measuring advancements beyond the state-of-the-art. Results, data and submission information can be found at https://fishyscapes.com/ .},
  archive      = {J_IJCV},
  author       = {Blum, Hermann and Sarlin, Paul-Edouard and Nieto, Juan and Siegwart, Roland and Cadena, Cesar},
  doi          = {10.1007/s11263-021-01511-6},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3119-3135},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {The fishyscapes benchmark: Measuring blind spots in semantic segmentation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assignment flow for order-constrained OCT segmentation.
<em>IJCV</em>, <em>129</em>(11), 3088–3118. (<a
href="https://doi.org/10.1007/s11263-021-01520-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the present time optical coherence tomography (OCT) is among the most commonly used non-invasive imaging methods for the acquisition of large volumetric scans of human retinal tissues and vasculature. The substantial increase of accessible highly resolved 3D samples at the optic nerve head and the macula is directly linked to medical advancements in early detection of eye diseases. To resolve decisive information from extracted OCT volumes and to make it applicable for further diagnostic analysis, the exact measurement of retinal layer thicknesses serves as an essential task be done for each patient separately. However, manual examination of OCT scans is a demanding and time consuming task, which is typically made difficult by the presence of tissue-dependent speckle noise. Therefore, the elaboration of automated segmentation models has become an important task in the field of medical image processing. We propose a novel, purely data driven geometric approach to order-constrained 3D OCT retinal cell layer segmentation which takes as input data in any metric space and can be implemented using only simple, highly parallelizable operations. As opposed to many established retinal layer segmentation methods, we use only locally extracted features as input and do not employ any global shape prior. The physiological order of retinal cell layers and membranes is achieved through the introduction of a smoothed energy term. This is combined with additional regularization of local smoothness to yield highly accurate 3D segmentations. The approach thereby systematically avoid bias pertaining to global shape and is hence suited for the detection of anatomical changes of retinal tissue structure. To demonstrate its robustness, we compare two different choices of features on a data set of manually annotated 3D OCT volumes of healthy human retina. The quality of computed segmentations is compared to the state of the art in automatic retinal layer segmention as well as to manually annotated ground truth data in terms of mean absolute error and Dice similarity coefficient. Visualizations of segmented volumes are also provided.},
  archive      = {J_IJCV},
  author       = {Sitenko, Dmitrij and Boll, Bastian and Schnörr, Christoph},
  doi          = {10.1007/s11263-021-01520-5},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3088-3118},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Assignment flow for order-constrained OCT segmentation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FairMOT: On the fairness of detection and re-identification
in multiple object tracking. <em>IJCV</em>, <em>129</em>(11), 3069–3087.
(<a href="https://doi.org/10.1007/s11263-021-01513-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT .},
  archive      = {J_IJCV},
  author       = {Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},
  doi          = {10.1007/s11263-021-01513-4},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3069-3087},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FairMOT: On the fairness of detection and re-identification in multiple object tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BiSeNet v2: Bilateral network with guided aggregation for
real-time semantic segmentation. <em>IJCV</em>, <em>129</em>(11),
3051–3068. (<a
href="https://doi.org/10.1007/s11263-021-01515-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-level details and high-level semantics are both essential to the semantic segmentation task. However, to speed up the model inference, current approaches almost always sacrifice the low-level details, leading to a considerable decrease in accuracy. We propose to treat these spatial details and categorical semantics separately to achieve high accuracy and high efficiency for real-time semantic segmentation. For this purpose, we propose an efficient and effective architecture with a good trade-off between speed and accuracy, termed Bilateral Segmentation Network (BiSeNet V2). This architecture involves the following: (i) A detail branch, with wide channels and shallow layers to capture low-level details and generate high-resolution feature representation; (ii) A semantics branch, with narrow channels and deep layers to obtain high-level semantic context. The detail branch has wide channel dimensions and shallow layers, while the semantics branch has narrow channel dimensions and deep layers. Due to the reduction in the channel capacity and the use of a fast-downsampling strategy, the semantics branch is lightweight and can be implemented by any efficient model. We design a guided aggregation layer to enhance mutual connections and fuse both types of feature representation. Moreover, a booster training strategy is designed to improve the segmentation performance without any extra inference cost. Extensive quantitative and qualitative evaluations demonstrate that the proposed architecture shows favorable performance compared to several state-of-the-art real-time semantic segmentation approaches. Specifically, for a $$2048\times 1024$$ input, we achieve 72.6\% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than existing methods, yet we achieve better segmentation accuracy. The code and trained models are available online at https://git.io/BiSeNet .},
  archive      = {J_IJCV},
  author       = {Yu, Changqian and Gao, Changxin and Wang, Jingbo and Yu, Gang and Shen, Chunhua and Sang, Nong},
  doi          = {10.1007/s11263-021-01515-2},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3051-3068},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BiSeNet v2: Bilateral network with guided aggregation for real-time semantic segmentation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical conditional relation networks for multimodal
video question answering. <em>IJCV</em>, <em>129</em>(11), 3027–3050.
(<a href="https://doi.org/10.1007/s11263-021-01514-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Question Answering (Video QA) challenges modelers in multiple fronts. Modeling video necessitates building not only spatio-temporal models for the dynamic visual channel but also multimodal structures for associated information channels such as subtitles or audio. Video QA adds at least two more layers of complexity – selecting relevant content for each channel in the context of the linguistic query, and composing spatio-temporal concepts and relations hidden in the data in response to the query. To address these requirements, we start with two insights: (a) content selection and relation construction can be jointly encapsulated into a conditional computational structure, and (b) video-length structures can be composed hierarchically. For (a) this paper introduces a general-reusable reusable neural unit dubbed Conditional Relation Network (CRN) taking as input a set of tensorial objects and translating into a new set of objects that encode relations of the inputs. The generic design of CRN helps ease the common complex model building process of Video QA by simple block stacking and rearrangements with flexibility in accommodating diverse input modalities and conditioning features across both visual and linguistic domains. As a result, we realize insight (b) by introducing Hierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN primarily aims at exploiting intrinsic properties of the visual content of a video as well as its accompanying channels in terms of compositionality, hierarchy, and near-term and far-term relation. HCRN is then applied for Video QA in two forms, short-form where answers are reasoned solely from the visual content of a video, and long-form where an additional associated information channel, such as movie subtitles, presented. Our rigorous evaluations show consistent improvements over state-of-the-art methods on well-studied benchmarks including large-scale real-world datasets such as TGIF-QA and TVQA, demonstrating the strong capabilities of our CRN unit and the HCRN for complex domains such as Video QA. To the best of our knowledge, the HCRN is the very first method attempting to handle long and short-form multimodal Video QA at the same time.},
  archive      = {J_IJCV},
  author       = {Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
  doi          = {10.1007/s11263-021-01514-3},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3027-3050},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical conditional relation networks for multimodal video question answering},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D scene reconstruction with an un-calibrated light field
camera. <em>IJCV</em>, <em>129</em>(11), 3006–3026. (<a
href="https://doi.org/10.1007/s11263-021-01516-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the problem of multi-view 3D reconstruction with an un-calibrated micro-lens array based light field camera. To acquire 3D Euclidean reconstruction, existing approaches commonly apply the calibration with a checkerboard and motion estimation from static scenes in two steps. Self-calibration is the process of simultaneously estimating intrinsic and extrinsic parameters directly from un-calibrated light fields without the help of a checkerboard. While the self-calibration technique for conventional (pinhole) camera is well understood, how to extend it to light field camera remains a challenging task. This is primarily due to the ultra-small baseline of the light field camera. We propose an effective self-calibration method for a light field camera for automatic metric reconstruction without a laborious pre-calibration process. In contrast to conventional self-calibration, we show how such a self-calibration method can be made numerically stable, by exploiting the regularity and measurement redundancies unique for the light field camera. The proposed method is built upon the derivation of a novel ray-space homography constraint (RSHC) using Plücker parameterization as well as a ray-space infinity homography (RSIH). We also propose a new concept of “rays of the absolute conic (RAC)” defined as a special quadric in 5D projective space $${\mathbb {P}}^5$$ . A set of new equations are established and solved for self-calibration and 3D metric reconstruction specifically designed for a light field camera . We validate the efficacy of the proposed method on both synthetic and real light fields, and have obtained superior results in both accuracy and robustness.},
  archive      = {J_IJCV},
  author       = {Zhang, Qi and Li, Hongdong and Wang, Xue and Wang, Qing},
  doi          = {10.1007/s11263-021-01516-1},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3006-3026},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {3D scene reconstruction with an un-calibrated light field camera},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue: Computer vision and pattern
recognition (DAGM GCPR 2019). <em>IJCV</em>, <em>129</em>(11),
3004–3005. (<a
href="https://doi.org/10.1007/s11263-021-01509-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Frintrop, Simone and Fink, Gernot A. and Jiang, Xiaoyi},
  doi          = {10.1007/s11263-021-01509-0},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {3004-3005},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: special issue: computer vision and pattern recognition (DAGM GCPR 2019)},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting visual political bias using webly supervised data
and an auxiliary task. <em>IJCV</em>, <em>129</em>(11), 2978–3003. (<a
href="https://doi.org/10.1007/s11263-021-01506-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The news media shape public opinion, and often, the visual bias they contain is evident for careful human observers. This bias can be inferred from how different media sources portray different subjects or topics. In this paper, we model visual political bias in contemporary media sources at scale, using webly supervised data. We collect a dataset of over one million unique images and associated news articles from left- and right-leaning news sources, and develop a method to predict the image’s political leaning. This problem is particularly challenging because of the enormous intra-class visual and semantic diversity of our data. We propose two stages of training to tackle this problem. In the first stage, the model is forced to learn relevant visual concepts that, when joined with document embeddings computed from articles paired with the images, enable the model to predict bias. In the second stage, we remove the requirement of the text domain and train a visual classifier from the features of the former model. We show this two-stage approach that relies on an auxiliary task leveraging text, facilitates learning and outperforms several strong baselines. We present extensive quantitative and qualitative results analyzing our dataset. Our results reveal disparities in how different sides of the political spectrum portray individuals, groups, and topics.},
  archive      = {J_IJCV},
  author       = {Thomas, Christopher and Kovashka, Adriana},
  doi          = {10.1007/s11263-021-01506-3},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2978-3003},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Predicting visual political bias using webly supervised data and an auxiliary task},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A coarse-to-fine framework for resource efficient video
recognition. <em>IJCV</em>, <em>129</em>(11), 2965–2977. (<a
href="https://doi.org/10.1007/s11263-021-01508-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have demonstrated remarkable recognition results on video classification, however great improvements in accuracies come at the expense of large amounts of computational resources. In this paper, we introduce LiteEval for resource efficient video recognition. LiteEval is a coarse-to-fine framework that dynamically allocates computation on a per-video basis, and can be deployed in both online and offline settings. Operating by default on low-cost features that are computed with images at a coarse scale, LiteEval adaptively determines on-the-fly when to read in more discriminative yet computationally expensive features. This is achieved by the interactions of a coarse RNN and a fine RNN, together with a conditional gating module that automatically learns when to use more computation conditioned on incoming frames. We conduct extensive experiments on three large-scale video benchmarks, FCVID, ActivityNet and Kinetics, and demonstrate, among other things, that LiteEval offers impressive recognition performance while using significantly less computation for both online and offline settings.},
  archive      = {J_IJCV},
  author       = {Wu, Zuxuan and Li, Hengduo and Zheng, Yingbin and Xiong, Caiming and Jiang, Yu-Gang and Davis, Larry S},
  doi          = {10.1007/s11263-021-01508-1},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2965-2977},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A coarse-to-fine framework for resource efficient video recognition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning geometry compression artifacts removal for
video-based point cloud compression. <em>IJCV</em>, <em>129</em>(11),
2947–2964. (<a
href="https://doi.org/10.1007/s11263-021-01503-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is an essential format for three-dimensional (3-D) object modelling and interaction in Augmented Reality and Virtual Reality applications. In the current state of the art video-based point cloud compression (V-PCC), a dynamic point cloud is projected onto geometry and attribute videos patch by patch, each represented by its texture, depth, and occupancy map for reconstruction. To deal with occlusion, each patch is projected onto near and far depth fields in the geometry video. Once there are artifacts on the compressed two-dimensional (2-D) geometry video, they would be propagated to the 3-D point-cloud frames. In addition, in the lossy compression, there always exists a tradeoff between the rate of bitstream and distortion. Although some geometry-related methods were proposed to attenuate these artifacts and improve the coding efficiency, the interactive correlation between projected near and far depth fields has been ignored. Moreover, the non-linear representation ability of Convolutional Neural Network has not been fully considered. Therefore, we propose a learning-based approach to remove the geometry artifacts and improve the compressing efficiency. We have the following contributions. We devise a two-step method working on the near and far depth fields decomposed from geometry. The first stage is learning-based Pseudo-Motion Compensation. The second stage exploits the potential of the strong correlations between near and far depth fields. Our proposed algorithm is embedded in the V-PCC reference software. To the best of our knowledge, this is the first learning-based solution of the geometry artifacts removal in V-PCC. The extensive experimental results show that the proposed approach achieves significant gains on geometry artifacts removal and quality improvement of 3-D point-cloud reconstruction compared to the state-of-the-art schemes.},
  archive      = {J_IJCV},
  author       = {Jia, Wei and Li, Li and Li, Zhu and Liu, Shan},
  doi          = {10.1007/s11263-021-01503-6},
  journal      = {International Journal of Computer Vision},
  number       = {11},
  pages        = {2947-2964},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep learning geometry compression artifacts removal for video-based point cloud compression},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context and structure mining network for video object
detection. <em>IJCV</em>, <em>129</em>(10), 2927–2946. (<a
href="https://doi.org/10.1007/s11263-021-01507-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregating temporal features from other frames is verified to be very effective for video object detection to overcome the challenges in still images, such as occlusion, motion blur, and rare pose. Currently, proposal-level feature aggregation dominates this direction. However, there are two main problems for the holistic proposal-level feature aggregation. First, the object proposals generated by the region proposal network ignore the useful context information around the object which is proved to be helpful for object classification. Second, the traditional proposal-level feature aggregation regards the proposal as a whole without considering the important object structure information, which makes the similarity comparison between two proposals less effective when occlusion or pose misalignment occurs on proposal objects. To deal with these problems, we propose the Context and Structure Mining Network to better aggregate features for video object detection. In our method, we first encode the spatial-temporal context information into object features in a global manner, which can benefit the object classification. In addition, the holistic proposal is divided into several patches to capture the structure information of the object, and cross patch matching is conducted to alleviate the pose misalignment between objects in target and support proposals. Moreover, an importance weight is learned for each target proposal patch to indicate how informative this patch is for the final feature aggregation, by which the occluded patches can be neglected. This enables the aggregation module to leverage the most important and informative patches to obtain the final feature aggregation. The proposed framework outperforms all the latest state-of-the-art methods on the ImageNet VID dataset with a large margin. This project is publicly available https://github.com/LiangHann/Context-and-Structure-Mining-Network-for-Video-Object-Detection .},
  archive      = {J_IJCV},
  author       = {Han, Liang and Wang, Pichao and Yin, Zhaozheng and Wang, Fan and Li, Hao},
  doi          = {10.1007/s11263-021-01507-2},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2927-2946},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Context and structure mining network for video object detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive dimension-discriminative low-rank tensor recovery
for computational hyperspectral imaging. <em>IJCV</em>,
<em>129</em>(10), 2907–2926. (<a
href="https://doi.org/10.1007/s11263-021-01481-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting the prior information is fundamental for image reconstruction in computational hyperspectral imaging (CHI). Existing methods usually unfold the 3D signal as a 1D vector and then handle the prior information among different dimensions in an indiscriminative manner, which inevitably ignores the high-dimensionality nature of the hyperspectral image (HSI) and thus results in poor reconstruction performance. In this paper, we propose a high-order tensor optimization based reconstruction method to boost the quality of CHI. Specifically, we first propose an adaptive dimension-discriminative low-rank tensor recovery (ADLTR) model to exploit the high-dimensionality prior of HSI faithfully. In the ADLTR model, we utilize the 3D tensors as the basic elements to fundamentally preserve the structure information in the spatial and spectral dimensions, introduce a dimension-discriminative low-rankness model to fully characterize the prior in the basic elements, and propose a weight estimation strategy by adaptively exploiting the diversity in each dimension. Then, we develop an optimization framework for the CHI reconstruction by integrating the structure prior in ADLTR with the system imaging principle, which is finally solved via the alternating minimization scheme. Extensive experiments on both synthetic and real data demonstrate that our method outperforms state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Wang, Lizhi and Zhang, Shipeng and Huang, Hua},
  doi          = {10.1007/s11263-021-01481-9},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2907-2926},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive dimension-discriminative low-rank tensor recovery for computational hyperspectral imaging},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Just recognizable distortion for machine vision oriented
image and video coding. <em>IJCV</em>, <em>129</em>(10), 2889–2906. (<a
href="https://doi.org/10.1007/s11263-021-01505-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine visual intelligence has exploded in recent years. Large-scale, high-quality image and video datasets significantly empower learning-based machine vision models, especially deep-learning models. However, images and videos are usually compressed before being analyzed in practical situations where transmission or storage is limited, leading to a noticeable performance loss of vision models. In this work, we broadly investigate the impact on the performance of machine vision from image and video coding. Based on the investigation, we propose Just Recognizable Distortion (JRD) to present the maximum distortion caused by data compression that will reduce the machine vision model performance to an unacceptable level. A large-scale JRD-annotated dataset containing over 340,000 images is built for various machine vision tasks, where the factors for different JRDs are studied. Furthermore, an ensemble-learning-based framework is established to predict the JRDs for diverse vision tasks under few- and non-reference conditions, which consists of multiple binary classifiers to improve the prediction accuracy. Experiments prove the effectiveness of the proposed JRD-guided image and video coding to significantly improve compression and machine vision performance. Applying predicted JRD is able to achieve remarkably better machine vision task accuracy and save a large number of bits.},
  archive      = {J_IJCV},
  author       = {Zhang, Qi and Wang, Shanshe and Zhang, Xinfeng and Ma, Siwei and Gao, Wen},
  doi          = {10.1007/s11263-021-01505-4},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2889-2906},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Just recognizable distortion for machine vision oriented image and video coding},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DLOW: Domain flow and applications. <em>IJCV</em>,
<em>129</em>(10), 2865–2888. (<a
href="https://doi.org/10.1007/s11263-021-01496-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a domain flow generation (DLOW) model to bridge two different domains by generating a continuous sequence of intermediate domains flowing from one domain to the other. The benefits of our DLOW model are twofold. First, it is able to transfer source images into a domain flow, which consists of images with smoothly changing distributions from the source to the target domain. The domain flow bridges the gap between source and target domains, thus easing the domain adaptation task. Second, when multiple target domains are provided for training, our DLOW model is also able to generate new styles of images that are unseen in the training data. The new images are shown to be able to mimic different artists to produce a natural blend of multiple art styles. Furthermore, for the semantic segmentation in the adverse weather condition, we take advantage of our DLOW model to generate images with gradually changing fog density, which can be readily used for boosting the segmentation performance when combined with a curriculum learning strategy. We demonstrate the effectiveness of our model on benchmark datasets for different applications, including cross-domain semantic segmentation, style generalization, and foggy scene understanding. Our implementation is available at https://github.com/ETHRuiGong/DLOW .},
  archive      = {J_IJCV},
  author       = {Gong, Rui and Li, Wen and Chen, Yuhua and Dai, Dengxin and Van Gool, Luc},
  doi          = {10.1007/s11263-021-01496-2},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2865-2888},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DLOW: Domain flow and applications},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SportsCap: Monocular 3D human motion capture and
fine-grained understanding in challenging sports videos. <em>IJCV</em>,
<em>129</em>(10), 2846–2864. (<a
href="https://doi.org/10.1007/s11263-021-01486-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markerless motion capture and understanding of professional non-daily human movements is an important yet unsolved task, which suffers from complex motion patterns and severe self-occlusion, especially for the monocular setting. In this paper, we propose SportsCap—the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. Our approach utilizes the semantic and temporally structured sub-motion prior in the embedding space for motion capture and understanding in a data-driven multi-task manner. To enable robust capture under complex motion patterns, we propose an effective motion embedding module to recover both the implicit motion embedding and explicit 3D motion details via a corresponding mapping function as well as a sub-motion classifier. Based on such hybrid motion information, we introduce a multi-stream spatial-temporal graph convolutional network to predict the fine-grained semantic action attributes, and adopt a semantic attribute mapping block to assemble various correlated action attributes into a high-level action label for the overall detailed understanding of the whole sequence, so as to enable various applications like action assessment or motion scoring. Comprehensive experiments on both public and our proposed datasets show that with a challenging monocular sports video input, our novel approach not only significantly improves the accuracy of 3D human motion capture, but also recovers accurate fine-grained semantic action attribute.},
  archive      = {J_IJCV},
  author       = {Chen, Xin and Pang, Anqi and Yang, Wei and Ma, Yuexin and Xu, Lan and Yu, Jingyi},
  doi          = {10.1007/s11263-021-01486-4},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2846-2864},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SportsCap: Monocular 3D human motion capture and fine-grained understanding in challenging sports videos},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep maximum a posterior estimator for video denoising.
<em>IJCV</em>, <em>129</em>(10), 2827–2845. (<a
href="https://doi.org/10.1007/s11263-021-01510-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike the maturity of image denoising research, video denoising has remained a challenging problem. A fundamental issue at the core of the video denoising (VD) problem is how to efficiently remove noise by exploiting temporal redundancy in video frames in a principled manner. Based on the maximum a posterior (MAP) estimation framework and recent advances in deep learning, we present a novel deep MAP-based video denoising method named MAP-VDNet with adaptive temporal fusion and deep image prior. The proposed MAP-based VD algorithm allows computationally efficient untangling of motion estimation (frame alignment) and image restoration (denoising). To address the misalignment issue, we also present a robust multi-frame fusion strategy for predicting spatially varying fusion weights by a neural network. To facilitate end-to-end optimization, we unfold the proposed iterative MAP-based VD algorithm into a deep convolutional network named MAP-VDNet. Extensive experimental results on three popular video datasets have shown that the proposed MAP-VDNet significantly outperforms current state-of-the-art VD techniques such as ViDeNN and FastDVDnet. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/MAP-VDNet.htm .},
  archive      = {J_IJCV},
  author       = {Sun, Lu and Dong, Weisheng and Li, Xin and Wu, Jinjian and Li, Leida and Shi, Guangming},
  doi          = {10.1007/s11263-021-01510-7},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2827-2845},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep maximum a posterior estimator for video denoising},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A decomposable winograd method for n–d convolution
acceleration in video analysis. <em>IJCV</em>, <em>129</em>(10),
2806–2826. (<a
href="https://doi.org/10.1007/s11263-021-01500-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Winograd’s minimal filtering algorithm has been widely used in 2-D Convolutional Neural Networks (CNNs) to reduce the number of multiplications for faster processing. However, it is only effective on convolutions with kernel size as $$3$$ and stride as 1, because it suffers from significantly increased FLOPs and numerical accuracy problems for kernel size larger than $$3$$ and fails on convolution with stride larger than 1. Worse, the extension to N–D convolution will intensify the numerical accuracy problem. These problems severely obstruct Winograd’s minimal filtering algorithm’s application to video analysis. In this paper, we propose a novel Decomposable Winograd Method (DWM) for the N–D convolution acceleration, which breaks through the limitation of original Winograd’s minimal filtering algorithm to more general convolutions. DWM decomposes kernels with large size or stride&gt;1 to several small kernels with stride as 1 for further applying Winograd algorithm, so that DWM can reduce the number of multiplications while keeping the numerical accuracy. It enables the fast exploration of larger kernel size, larger stride value, and higher dimensions in CNNs for high performance and accuracy and even the potential for new CNNs. Comparing against the original Winograd algorithm, the proposed DWM is able to support all kinds of N–D convolutions with a speedup of $$1.44\times $$ – $$3.38\times $$ , without affecting the numerical accuracy.},
  archive      = {J_IJCV},
  author       = {Huang, Di and Zhang, Rui and Zhang, Xishan and Wu, Fan and Wang, Xianzhuo and Jin, Pengwei and Liu, Shaoli and Li, Ling and Chen, Yunji},
  doi          = {10.1007/s11263-021-01500-9},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2806-2826},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A decomposable winograd method for N–D convolution acceleration in video analysis},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pluralistic free-form image completion. <em>IJCV</em>,
<em>129</em>(10), 2786–2805. (<a
href="https://doi.org/10.1007/s11263-021-01502-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image completion involves filling plausible contents to missing regions in images. Current image completion methods produce only one result for a given masked image, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion—the task of generating multiple and diverse plausible solutions for free-form image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label for this multi-output problem. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one ground truth to get prior distribution of missing patches and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by adversarial learning. We then introduce a new short+long term patch attention layer that exploits distant relations among decoder and encoder features, to improve appearance consistency between the original visible and the generated new regions. Experiments show that our method not only yields better results in various datasets than existing state-of-the-art methods, but also provides multiple and diverse outputs.},
  archive      = {J_IJCV},
  author       = {Zheng, Chuanxia and Cham, Tat-Jen and Cai, Jianfei},
  doi          = {10.1007/s11263-021-01502-7},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2786-2805},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pluralistic free-form image completion},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SDNet: A versatile squeeze-and-decomposition network for
real-time image fusion. <em>IJCV</em>, <em>129</em>(10), 2761–2785. (<a
href="https://doi.org/10.1007/s11263-021-01501-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a squeeze-and-decomposition network (SDNet) is proposed to realize multi-modal and digital photography image fusion in real time. Firstly, we generally transform multiple fusion problems into the extraction and reconstruction of gradient and intensity information, and design a universal form of loss function accordingly, which is composed of intensity term and gradient term. For the gradient term, we introduce an adaptive decision block to decide the optimization target of the gradient distribution according to the texture richness at the pixel scale, so as to guide the fused image to contain richer texture details. For the intensity term, we adjust the weight of each intensity loss term to change the proportion of intensity information from different images, so that it can be adapted to multiple image fusion tasks. Secondly, we introduce the idea of squeeze and decomposition into image fusion. Specifically, we consider not only the squeeze process from source images to the fused result, but also the decomposition process from the fused result to source images. Because the quality of decomposed images directly depends on the fused result, it can force the fused result to contain more scene details. Experimental results demonstrate the superiority of our method over the state-of-the-arts in terms of subjective visual effect and quantitative metrics in a variety of fusion tasks. Moreover, our method is much faster than the state-of-the-arts, which can deal with real-time fusion tasks.},
  archive      = {J_IJCV},
  author       = {Zhang, Hao and Ma, Jiayi},
  doi          = {10.1007/s11263-021-01501-8},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2761-2785},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SDNet: A versatile squeeze-and-decomposition network for real-time image fusion},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral shape recovery and analysis via data-driven
connections. <em>IJCV</em>, <em>129</em>(10), 2745–2760. (<a
href="https://doi.org/10.1007/s11263-021-01492-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel learning-based method to recover shapes from their Laplacian spectra, based on establishing and exploring connections in a learned latent space. The core of our approach consists in a cycle-consistent module that maps between a learned latent space and sequences of eigenvalues. This module provides an efficient and effective link between the shape geometry, encoded in a latent vector, and its Laplacian spectrum. Our proposed data-driven approach replaces the need for ad-hoc regularizers required by prior methods, while providing more accurate results at a fraction of the computational cost. Moreover, these latent space connections enable novel applications for both analyzing and controlling the spectral properties of deformable shapes, especially in the context of a shape collection. Our learning model and the associated analysis apply without modifications across different dimensions (2D and 3D shapes alike), representations (meshes, contours and point clouds), nature of the latent space (generated by an auto-encoder or a parametric model), as well as across different shape classes, and admits arbitrary resolution of the input spectrum without affecting complexity. The increased flexibility allows us to address notoriously difficult tasks in 3D vision and geometry processing within a unified framework, including shape generation from spectrum, latent space exploration and analysis, mesh super-resolution, shape exploration, style transfer, spectrum estimation for point clouds, segmentation transfer and non-rigid shape matching.},
  archive      = {J_IJCV},
  author       = {Marin, Riccardo and Rampini, Arianna and Castellani, Umberto and Rodolà, Emanuele and Ovsjanikov, Maks and Melzi, Simone},
  doi          = {10.1007/s11263-021-01492-6},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2745-2760},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spectral shape recovery and analysis via data-driven connections},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cascaded split-and-aggregate learning with feature
recombination for pedestrian attribute recognition. <em>IJCV</em>,
<em>129</em>(10), 2731–2744. (<a
href="https://doi.org/10.1007/s11263-021-01499-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label pedestrian attribute recognition in surveillance is inherently a challenging task due to poor imaging quality, large pose variations, and so on. In this paper, we improve its performance from the following two aspects: (1) We propose a cascaded Split-and-Aggregate Learning (SAL) to capture both the individuality and commonality for all attributes, with one at the feature map level and the other at the feature vector level. For the former, we split the features of each attribute by using a designed attribute-specific attention module (ASAM). For the later, the split features for each attribute are learned by using constrained losses. In both modules, the split features are aggregated by using several convolutional or fully connected layers. (2) We propose a Feature Recombination (FR) that conducts a random shuffle based on the split features over a batch of samples to synthesize more training samples, which spans the potential samples’ variability. To the end, we formulate a unified framework, named CAScaded Split-and-Aggregate Learning with Feature Recombination (CAS-SAL-FR), to learn the above modules jointly and concurrently. Experiments on five popular benchmarks, including RAP, PA-100K, PETA, Market-1501 and Duke attribute datasets, show the proposed CAS-SAL-FR achieves new state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Yang, Yang and Tan, Zichang and Tiwari, Prayag and Pandey, Hari Mohan and Wan, Jun and Lei, Zhen and Guo, Guodong and Li, Stan Z.},
  doi          = {10.1007/s11263-021-01499-z},
  journal      = {International Journal of Computer Vision},
  number       = {10},
  pages        = {2731-2744},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Cascaded split-and-aggregate learning with feature recombination for pedestrian attribute recognition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to: Long-short temporal–spatial clues excited
network for robust person re-identification. <em>IJCV</em>,
<em>129</em>(9), 2730. (<a
href="https://doi.org/10.1007/s11263-021-01497-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Li, Shuai and Song, Wenfeng and Fang, Zheng and Shi, Jiaying and Hao, Aimin and Zhao, Qinping and Qin, Hong},
  doi          = {10.1007/s11263-021-01497-1},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2730},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction to: Long-short Temporal–Spatial clues excited network for robust person re-identification},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning adaptive attribute-driven representation for
real-time RGB-t tracking. <em>IJCV</em>, <em>129</em>(9), 2714–2729. (<a
href="https://doi.org/10.1007/s11263-021-01495-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of a real-time and robust RGB-T tracker is an extremely challenging task because the tracked object may suffer from shared and specific challenges in RGB and thermal (T) modalities. In this work, we observe that the implicit attribute information can boost the model discriminability, and propose a novel attribute-driven representation network to improve the RGB-T tracking performance. First, according to appearance change in RGB-T tracking scenarios, we divide the major and special challenges into four typical attributes: extreme illumination, occlusion, motion blur, and thermal crossover. Second, we design an attribute-driven residual branch for each heterogeneous attribute to mine the attribute-specific property and therefore build a powerful residual representation for object modeling. Furthermore, we aggregate these representations in channel and pixel levels by using the proposed attribute ensemble network (AENet) to adaptively fit the attribute-agnostic tracking process. The AENet can effectively make aware of appearance change while suppressing the distractors. Finally, we conduct numerous experiments on three RGB-T tracking benchmarks to compare the proposed trackers with other state-of-the-art methods. Experimental results show that our tracker achieves very competitive results with a real-time tracking speed. Code will be available at https://github.com/zhang-pengyu/ADRNet.},
  archive      = {J_IJCV},
  author       = {Zhang, Pengyu and Wang, Dong and Lu, Huchuan and Yang, Xiaoyun},
  doi          = {10.1007/s11263-021-01495-3},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2714-2729},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning adaptive attribute-driven representation for real-time RGB-T tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape my face: Registering 3D face scans by
surface-to-surface translation. <em>IJCV</em>, <em>129</em>(9),
2680–2713. (<a
href="https://doi.org/10.1007/s11263-021-01494-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard registration algorithms need to be independently applied to each surface to register, following careful pre-processing and hand-tuning. Recently, learning-based approaches have emerged that reduce the registration of new scans to running inference with a previously-trained model. The potential benefits are multifold: inference is typically orders of magnitude faster than solving a new instance of a difficult optimization problem, deep learning models can be made robust to noise and corruption, and the trained model may be re-used for other tasks, e.g. through transfer learning. In this paper, we cast the registration task as a surface-to-surface translation problem, and design a model to reliably capture the latent geometric information directly from raw 3D face scans. We introduce Shape-My-Face (SMF), a powerful encoder-decoder architecture based on an improved point cloud encoder, a novel visual attention mechanism, graph convolutional decoders with skip connections, and a specialized mouth model that we smoothly integrate with the mesh convolutions. Compared to the previous state-of-the-art learning algorithms for non-rigid registration of face scans, SMF only requires the raw data to be rigidly aligned (with scaling) with a pre-defined face template. Additionally, our model provides topologically-sound meshes with minimal supervision, offers faster training time, has orders of magnitude fewer trainable parameters, is more robust to noise, and can generalize to previously unseen datasets. We extensively evaluate the quality of our registrations on diverse data. We demonstrate the robustness and generalizability of our model with in-the-wild face scans across different modalities, sensor types, and resolutions. Finally, we show that, by learning to register scans, SMF produces a hybrid linear and non-linear morphable model. Manipulation of the latent space of SMF allows for shape generation, and morphing applications such as expression transfer in-the-wild. We train SMF on a dataset of human faces comprising 9 large-scale databases on commodity hardware.},
  archive      = {J_IJCV},
  author       = {Bahri, Mehdi and O’ Sullivan, Eimear and Gong, Shunwang and Liu, Feng and Liu, Xiaoming and Bronstein, Michael M. and Zafeiriou, Stefanos},
  doi          = {10.1007/s11263-021-01494-4},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2680-2713},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Shape my face: Registering 3D face scans by surface-to-surface translation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to caricature via semantic shape transform.
<em>IJCV</em>, <em>129</em>(9), 2663–2679. (<a
href="https://doi.org/10.1007/s11263-021-01489-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caricature is an artistic drawing created to abstract or exaggerate facial features of a person. Rendering visually pleasing caricatures is a difficult task that requires professional skills, and thus it is of great interest to design a method to automatically generate such drawings. To deal with large shape changes, we propose an algorithm based on a semantic shape transform to produce diverse and plausible shape exaggerations. Specifically, we predict pixel-wise semantic correspondences and perform image warping on the input photo to achieve dense shape transformation. We show that the proposed framework is able to render visually pleasing shape exaggerations while maintaining their facial structures. In addition, our model allows users to manipulate the shape via the semantic map. We demonstrate the effectiveness of our approach on a large photograph-caricature benchmark dataset with comparisons to the state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Chu, Wenqing and Hung, Wei-Chih and Tsai, Yi-Hsuan and Chang, Yu-Ting and Li, Yijun and Cai, Deng and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-021-01489-1},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2663-2679},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to caricature via semantic shape transform},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards high performance human keypoint detection.
<em>IJCV</em>, <em>129</em>(9), 2639–2662. (<a
href="https://doi.org/10.1007/s11263-021-01482-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM’s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at https://github.com/chaimi2013/CCM .},
  archive      = {J_IJCV},
  author       = {Zhang, Jing and Chen, Zhe and Tao, Dacheng},
  doi          = {10.1007/s11263-021-01482-8},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2639-2662},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards high performance human keypoint detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-measure: A new way to evaluate foreground maps.
<em>IJCV</em>, <em>129</em>(9), 2622–2638. (<a
href="https://doi.org/10.1007/s11263-021-01490-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground map evaluation is crucial for gauging the progress of object segmentation algorithms, in particular in the field of salient object detection where the purpose is to accurately detect and segment the most salient object in a scene. Several measures (e.g., area-under-the-curve, F1-measure, average precision, etc.) have been used to evaluate the similarity between a foreground map and a ground-truth map. The existing measures are based on pixel-wise errors and often ignore the structural similarities. Behavioral vision studies, however, have shown that the human visual system is highly sensitive to structures in scenes. Here, we propose a novel, efficient (0.005 s per image), and easy to calculate measure known as S-measure (structural measure) to evaluate foreground maps. Our new measure simultaneously evaluates region-aware and object-aware structural similarity between a foreground map and a ground-truth map. We demonstrate superiority of our measure over existing ones using 4 meta-measures on 5 widely-used benchmark datasets. Furthermore, we conduct a behavioral judgment study over a new database. Data from 45 subjects shows that on average they preferred the saliency maps chosen by our measure over the saliency maps chosen by the state-of-the-art measures. Our experimental results offer new insights into foreground map evaluation where current measures fail to truly examine the strengths and weaknesses of models. Code: https://github.com/DengPingFan/S-measure .},
  archive      = {J_IJCV},
  author       = {Cheng, Ming-Ming and Fan, Deng-Ping},
  doi          = {10.1007/s11263-021-01490-8},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2622-2638},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Structure-measure: A new way to evaluate foreground maps},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantics-to-signal scalable image compression with learned
revertible representations. <em>IJCV</em>, <em>129</em>(9), 2605–2621.
(<a href="https://doi.org/10.1007/s11263-021-01491-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image/video compression and communication need to serve both human vision and machine vision. To address this need, we propose a scalable image compression solution. We assume that machine vision needs less information that is related to semantics, whereas human vision needs more information that is to reconstruct signal. We then propose semantics-to-signal scalable compression, where partial bitstream is decodeable for machine vision and the entire bitstream is decodeable for human vision. Our method is inspired by the scalable image coding standard, JPEG2000, and similarly adopts subband-wise representations. We first design a trainable and revertible transform based on the lifting structure, which converts an image into a pyramid of multiple subbands; the transform is trained to make the partial representations useful for multiple machine vision tasks. We then design an end-to-end optimized encoding/decoding network for compressing the multiple subbands, to jointly optimize compression ratio, semantic analysis accuracy, and signal reconstruction quality. We experiment with two datasets: CUB200-2011 and FGVC-Aircraft, taking coarse-to-fine image classification tasks as an example. Experimental results demonstrate that our proposed method achieves semantics-to-signal scalable compression, and outperforms JPEG2000 in compression efficiency. The proposed method sheds light on a generic approach for image/video coding for human and machines.},
  archive      = {J_IJCV},
  author       = {Liu, Kang and Liu, Dong and Li, Li and Yan, Ning and Li, Houqiang},
  doi          = {10.1007/s11263-021-01491-7},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2605-2621},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantics-to-signal scalable image compression with learned revertible representations},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking by deblatting. <em>IJCV</em>, <em>129</em>(9),
2583–2604. (<a
href="https://doi.org/10.1007/s11263-021-01480-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects moving at high speed along complex trajectories often appear in videos, especially videos of sports. Such objects travel a considerable distance during exposure time of a single frame, and therefore, their position in the frame is not well defined. They appear as semi-transparent streaks due to the motion blur and cannot be reliably tracked by general trackers. We propose a novel approach called Tracking by Deblatting based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. By postprocessing, non-causal Tracking by Deblatting estimates continuous, complete, and accurate object trajectories for the whole sequence. Tracked objects are precisely localized with higher temporal resolution than by conventional trackers. Energy minimization by dynamic programming is used to detect abrupt changes of motion, called bounces. High-order polynomials are then fitted to smooth trajectory segments between bounces. The output is a continuous trajectory function that assigns location for every real-valued time stamp from zero to the number of frames. The proposed algorithm was evaluated on a newly created dataset of videos from a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms the baselines both in recall and trajectory accuracy. Additionally, we show that from the trajectory function precise physical calculations are possible, such as radius, gravity, and sub-frame object velocity. Velocity estimation is compared to the high-speed camera measurements and radars. Results show high performance of the proposed method in terms of Trajectory-IoU, recall, and velocity estimation.},
  archive      = {J_IJCV},
  author       = {Rozumnyi, Denys and Kotera, Jan and Šroubek, Filip and Matas, Jiří},
  doi          = {10.1007/s11263-021-01480-w},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2583-2604},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Tracking by deblatting},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learned collaborative stereo refinement. <em>IJCV</em>,
<em>129</em>(9), 2565–2582. (<a
href="https://doi.org/10.1007/s11263-021-01485-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a learning-based method to denoise and refine disparity maps. The proposed variational network arises naturally from unrolling the iterates of a proximal gradient method applied to a variational energy defined in a joint disparity, color, and confidence image space. Our method allows to learn a robust collaborative regularizer leveraging the joint statistics of the color image, the confidence map and the disparity map. Due to the variational structure of our method, the individual steps can be easily visualized, thus enabling interpretability of the method. We can therefore provide interesting insights into how our method refines and denoises disparity maps. To this end, we can visualize and interpret the learned filters and activation functions and prove the increased reliability of the predicted pixel-wise confidence maps. Furthermore, the optimization based structure of our refinement module allows us to compute eigen disparity maps, which reveal structural properties of our refinement module. The efficiency of our method is demonstrated on the publicly available stereo benchmarks Middlebury 2014 and Kitti 2015.},
  archive      = {J_IJCV},
  author       = {Knöbelreiter, Patrick and Pock, Thomas},
  doi          = {10.1007/s11263-021-01485-5},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2565-2582},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learned collaborative stereo refinement},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised scale-consistent depth learning from video.
<em>IJCV</em>, <em>129</em>(9), 2548–2564. (<a
href="https://doi.org/10.1007/s11263-021-01484-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a monocular depth estimation method SC-Depth, which requires only unlabelled videos for training and enables the scale-consistent prediction at inference time. Our contributions include: (i) we propose a geometry consistency loss, which penalizes the inconsistency of predicted depths between adjacent views; (ii) we propose a self-discovered mask to automatically localize moving objects that violate the underlying static scene assumption and cause noisy signals during training; (iii) we demonstrate the efficacy of each component with a detailed ablation study and show high-quality depth estimation results in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of scale-consistent prediction, we show that our monocular-trained deep networks are readily integrated into ORB-SLAM2 system for more robust and accurate tracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in KITTI, and it generalizes well to the KAIST dataset without additional training. Finally, we provide several demos for qualitative evaluation. The source code is released on GitHub.},
  archive      = {J_IJCV},
  author       = {Bian, Jia-Wang and Zhan, Huangying and Wang, Naiyan and Li, Zhichao and Zhang, Le and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
  doi          = {10.1007/s11263-021-01484-6},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2548-2564},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised scale-consistent depth learning from video},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning regression and verification networks for robust
long-term tracking. <em>IJCV</em>, <em>129</em>(9), 2536–2547. (<a
href="https://doi.org/10.1007/s11263-021-01487-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking. To this end, a regression network is learned offline to detect a set of target candidates through target template matching. To cope with target appearance variations in long-term scenarios, a target-aware feature fusion mechanism is also developed, giving rise to more effective template matching. Meanwhile, a verification network is trained online to better capture target appearance and identify the target from potential candidates. During online update, contaminated training samples can be filtered out through a monitoring module, alleviating model degeneration caused by error accumulation. The regression and verification networks operate in a cascaded manner, which allows tracking to be performed in a coarse-to-fine manner and enforces the discriminative power. To further address the target reappearance issues in long-term tracking, a learning-based switching scheme is proposed, which learns to switch the tracking mode between local and global search based on the tracking results. Extensive evaluations on long-term tracking in the wild have been conducted. We achieve state-of-the-art performance on the OxUvA long-term tracking dataset. Our submission based on the proposed method has also won the 1st place of the long-term tracking challenge in VOT-2018 competition.},
  archive      = {J_IJCV},
  author       = {Zhang, Yunhua and Wang, Lijun and Wang, Dong and Qi, Jinqing and Lu, Huchuan},
  doi          = {10.1007/s11263-021-01487-3},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2536-2547},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning regression and verification networks for robust long-term tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level motion attention for human motion prediction.
<em>IJCV</em>, <em>129</em>(9), 2513–2535. (<a
href="https://doi.org/10.1007/s11263-021-01483-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction aims to forecast future human poses given a historical motion. Whether based on recurrent or feed-forward neural networks, existing learning based methods fail to model the observation that human motion tends to repeat itself, even for complex sports actions and cooking activities. Here, we introduce an attention based feed-forward network that explicitly leverages this observation. In particular, instead of modeling frame-wise attention via pose similarity, we propose to extract motion attention to capture the similarity between the current motion context and the historical motion sub-sequences. In this context, we study the use of different types of attention, computed at joint, body part, and full pose levels. Aggregating the relevant past motions and processing the result with a graph convolutional network allows us to effectively exploit motion patterns from the long-term history to predict the future poses. Our experiments on Human3.6M, AMASS and 3DPW validate the benefits of our approach for both periodical and non-periodical actions. Thanks to our attention model, it yields state-of-the-art results on all three datasets. Our code is available at https://github.com/wei-mao-2019/HisRepItself .},
  archive      = {J_IJCV},
  author       = {Mao, Wei and Liu, Miaomiao and Salzmann, Mathieu and Li, Hongdong},
  doi          = {10.1007/s11263-021-01483-7},
  journal      = {International Journal of Computer Vision},
  number       = {9},
  pages        = {2513-2535},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-level motion attention for human motion prediction},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep unsupervised 3D human body reconstruction from a sparse
set of landmarks. <em>IJCV</em>, <em>129</em>(8), 2499–2512. (<a
href="https://doi.org/10.1007/s11263-021-01488-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose the first deep unsupervised approach in human body reconstruction to estimate body surface from a sparse set of landmarks, so called DeepMurf. We apply a denoising autoencoder to estimate missing landmarks. Then we apply an attention model to estimate body joints from landmarks. Finally, a cascading network is applied to regress parameters of a statistical generative model that reconstructs body. Our set of proposed loss functions allows us to train the network in an unsupervised way. Results on four public datasets show that our approach accurately reconstructs the human body from real world mocap data.},
  archive      = {J_IJCV},
  author       = {Madadi, Meysam and Bertiche, Hugo and Escalera, Sergio},
  doi          = {10.1007/s11263-021-01488-2},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2499-2512},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep unsupervised 3D human body reconstruction from a sparse set of landmarks},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SODA: Weakly supervised temporal action localization based
on astute background response and self-distillation learning.
<em>IJCV</em>, <em>129</em>(8), 2474–2498. (<a
href="https://doi.org/10.1007/s11263-021-01473-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised temporal action localization is a practical yet challenging task. Although great efforts have been made in recent years, the existing methods still have limited capacity in dealing with the challenges of over-localization, joint-localization, and under-localization. Based on our investigation, the first two challenges arise from insufficient ability to suppress background response, while the third challenge is due to the lack of discovering action frames. To better address these challenges, we first propose the astute background response strategy. By enforcing the classification target of the background category to be zero, such a strategy can endow the conductive effect between video-level classification and frame-level classification, thus guiding the action category to suppress responses at background frames astutely and helping address the over-localization and joint-localization challenges. For alleviating the under-localization challenge, we introduce the self-distillation learning strategy. It simultaneously learns one master network and multiple auxiliary networks, where the auxiliary networks enhance the master network to discover complete action frames. Experimental results on three benchmarks demonstrate the favorable performance of the proposed method against previous counterparts, and its efficacy to tackle the existing three challenges.},
  archive      = {J_IJCV},
  author       = {Zhao, Tao and Han, Junwei and Yang, Le and Wang, Binglu and Zhang, Dingwen},
  doi          = {10.1007/s11263-021-01473-9},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2474-2498},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SODA: Weakly supervised temporal action localization based on astute background response and self-distillation learning},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ShadingNet: Image intrinsics by fine-grained shading
decomposition. <em>IJCV</em>, <em>129</em>(8), 2445–2473. (<a
href="https://doi.org/10.1007/s11263-021-01477-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, intrinsic image decomposition algorithms interpret shading as one unified component including all photometric effects. As shading transitions are generally smoother than reflectance (albedo) changes, these methods may fail in distinguishing strong photometric effects from reflectance variations. Therefore, in this paper, we propose to decompose the shading component into direct (illumination) and indirect shading (ambient light and shadows) subcomponents. The aim is to distinguish strong photometric effects from reflectance variations. An end-to-end deep convolutional neural network (ShadingNet) is proposed that operates in a fine-to-coarse manner with a specialized fusion and refinement unit exploiting the fine-grained shading model. It is designed to learn specific reflectance cues separated from specific photometric effects to analyze the disentanglement capability. A large-scale dataset of scene-level synthetic images of outdoor natural environments is provided with fine-grained intrinsic image ground-truths. Large scale experiments show that our approach using fine-grained shading decompositions outperforms state-of-the-art algorithms utilizing unified shading on NED, MPI Sintel, GTA V, IIW, MIT Intrinsic Images, 3DRMS and SRD datasets.},
  archive      = {J_IJCV},
  author       = {Baslamisli, Anil S. and Das, Partha and Le, Hoang-An and Karaoglu, Sezer and Gevers, Theo},
  doi          = {10.1007/s11263-021-01477-5},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2445-2473},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ShadingNet: Image intrinsics by fine-grained shading decomposition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A numerical framework for elastic surface matching,
comparison, and interpolation. <em>IJCV</em>, <em>129</em>(8),
2425–2444. (<a
href="https://doi.org/10.1007/s11263-021-01476-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface comparison and matching is a challenging problem in computer vision. While elastic Riemannian metrics provide meaningful shape distances and point correspondences via the geodesic boundary value problem, solving this problem numerically tends to be difficult. Square root normal fields considerably simplify the computation of certain distances between parametrized surfaces. Yet they leave open the issue of finding optimal reparametrizations, which induce corresponding distances between unparametrized surfaces. This issue has concentrated much effort in recent years and led to the development of several numerical frameworks. In this paper, we take an alternative approach which bypasses the direct estimation of reparametrizations: we relax the geodesic boundary constraint using an auxiliary parametrization-blind varifold fidelity metric. This reformulation has several notable benefits. By avoiding altogether the need for reparametrizations, it provides the flexibility to deal with simplicial meshes of arbitrary topologies and sampling patterns. Moreover, the problem lends itself to a coarse-to-fine multi-resolution implementation, which makes the algorithm scalable to large meshes. Furthermore, this approach extends readily to higher-order feature maps such as square root curvature fields and is also able to include surface textures in the matching problem. We demonstrate these advantages on several examples, synthetic and real.},
  archive      = {J_IJCV},
  author       = {Bauer, Martin and Charon, Nicolas and Harms, Philipp and Hsieh, Hsi-Wei},
  doi          = {10.1007/s11263-021-01476-6},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2425-2444},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A numerical framework for elastic surface matching, comparison, and interpolation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MADAN: Multi-source adversarial domain aggregation network
for domain adaptation. <em>IJCV</em>, <em>129</em>(8), 2399–2424. (<a
href="https://doi.org/10.1007/s11263-021-01479-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to learn a transferable model to bridge the domain shift between one labeled source domain and another sparsely labeled or unlabeled target domain. Since the labeled data may be collected from multiple sources, multi-source domain adaptation (MDA) has attracted increasing attention. Recent MDA methods do not consider the pixel-level alignment between sources and target or the misalignment across different sources. In this paper, we propose a novel MDA framework to address these challenges. Specifically, we design a novel Multi-source Adversarial Domain Aggregation Network (MADAN). First, an adapted domain is generated for each source with dynamic semantic consistency while aligning towards the target at the pixel-level cycle-consistently. Second, sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and the target domain while training the task network. For the segmentation adaptation, we further enforce category-level alignment and incorporate multi-scale image generation, which constitutes MADAN+. We conduct extensive MDA experiments on digit recognition, object classification, and simulation-to-real semantic segmentation tasks. The results demonstrate that the proposed MADAN and MADAN+ models outperform state-of-the-art approaches by a large margin.},
  archive      = {J_IJCV},
  author       = {Zhao, Sicheng and Li, Bo and Xu, Pengfei and Yue, Xiangyu and Ding, Guiguang and Keutzer, Kurt},
  doi          = {10.1007/s11263-021-01479-3},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2399-2424},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MADAN: Multi-source adversarial domain aggregation network for domain adaptation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OCNet: Object context for semantic segmentation.
<em>IJCV</em>, <em>129</em>(8), 2375–2398. (<a
href="https://doi.org/10.1007/s11263-021-01465-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the semantic segmentation task with a new context aggregation scheme named object context, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise. We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices. To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling (Zhao et al. 2017) and atrous spatial pyramid pooling (Chen et al. 2018). We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff.},
  archive      = {J_IJCV},
  author       = {Yuan, Yuhui and Huang, Lang and Guo, Jianyuan and Zhang, Chao and Chen, Xilin and Wang, Jingdong},
  doi          = {10.1007/s11263-021-01465-9},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2375-2398},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OCNet: Object context for semantic segmentation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency detection inspired by topological perception
theory. <em>IJCV</em>, <em>129</em>(8), 2352–2374. (<a
href="https://doi.org/10.1007/s11263-021-01478-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topological perception theory claims that visual perception of a scene begins from topological properties and then exploits local details. Inspired by this theory, we defined the topological descriptor and topological complexity, and we observed, based on statistics, that the saliencies of the regions with higher topological complexities are generally higher than those of regions with lower topological complexities. We then introduced the topological complexity as a saliency prior and proposed a novel unsupervised topo-prior-guided saliency detection system (TOPS). This system is framed as a topological saliency prior (topo-prior)-guided two-level local cue processing (i.e., pixel- and regional-level cues) with a multi-scale strategy, which includes three main modules: (1) a basic computational model of the topological perception theory for extracting topological features from images, (2) a topo-prior calculation method based on the topological features, and (3) a global–local saliency combination framework guided by the topo-prior. Extensive experiments on widely used salient object detection (SOD) datasets demonstrate that our system outperforms the unsupervised state-of-the-art algorithms. In addition, the topo-prior proposed in this work can be used to boost supervised methods including the deep-learning-based ones for fixation prediction and SOD tasks.},
  archive      = {J_IJCV},
  author       = {Peng, Peng and Yang, Kai-Fu and Luo, Fu-Ya and Li, Yong-Jie},
  doi          = {10.1007/s11263-021-01478-4},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2352-2374},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Saliency detection inspired by topological perception theory},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep CockTail networks. <em>IJCV</em>, <em>129</em>(8),
2328–2351. (<a
href="https://doi.org/10.1007/s11263-021-01463-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferable deep representations for visual domain adaptation (DA) provides a route to learn from labeled source images to recognize target images without the aid of target-domain supervision. Relevant researches increasingly arouse a great amount of interest due to its potential industrial prospect for non-laborious annotation and remarkable generalization. However, DA presumes source images are identically sampled from a single source while Multi-Source DA (MSDA) is ubiquitous in the real-world. In MSDA, the domain shifts exist not only between source and target domains but also among the sources; especially, the multi-source and target domains may disagree on their semantics (e.g., category shifts). This issue challenges the existing solutions for MSDAs. In this paper, we propose Deep CockTail Network (DCTN), a universal and flexibly-deployed framework to address the problems. DCTN uses a multi-way adversarial learning pipeline to minimize the domain discrepancy between the target and each of the multiple in order to learn domain-invariant features. The derived source-specific perplexity scores measure how similar each target feature appears as a feature from one of source domains. The multi-source category classifiers are integrated with the perplexity scores to categorize target images. We accordingly derive a theoretical analysis towards DCTN, including the interpretation why DCTN can be successful without precisely crafting the source-specific hyper-parameters, and target expected loss upper bounds in terms of domain and category shifts. In our experiments, DCTNs have been evaluated on four benchmarks, whose empirical studies involve vanilla and three challenging category-shift transfer problems in MSDA, i.e., source-shift, target-shift and source-target-shift scenarios. The results thoroughly reveal that DCTN significantly boosts classification accuracies in MSDA and performs extraordinarily to resist negative transfers across different MSDA scenarios.},
  archive      = {J_IJCV},
  author       = {Chen, Ziliang and Wei, Pengxu and Zhuang, Jingyu and Li, Guanbin and Lin, Liang},
  doi          = {10.1007/s11263-021-01463-x},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2328-2351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep CockTail networks},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modal pyramid translation for RGB-d scene recognition.
<em>IJCV</em>, <em>129</em>(8), 2309–2327. (<a
href="https://doi.org/10.1007/s11263-021-01475-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing RGB-D scene recognition approaches typically employ two separate and modality-specific networks to learn effective RGB and Depth representations respectively. This independent training scheme fails to capture the correlation of two modalities, and thus may be suboptimal for RGB-D scene recognition. To address this issue, this paper proposes a general and flexible framework to enhance RGB-D representation learning with a customized cross-modal pyramid translation branch, coined as TRecgNet. This framework unifies the tasks of cross-modal translation and modality-specific recognition with a shared feature encoder, and aims at leveraging the correspondence between two modalities to regularize the representation learning of each modality. Specifically, we present a cross-modal pyramid translation strategy to perform multi-scale image generation with a carefully designed layer-wise perceptual supervision. To improve the complementarity of cross-modal translation to modality specific scene recognition, we devise a feature selection module to adaptively enhance the discriminative information during the translation procedure. In addition, we train multiple auxiliary classifiers to further regularize the behavior of generated data to be consistent with its paired data on label prediction. Meanwhile, our translation branch enables us to generate cross-modal data for training data augmentation and further improve single modality scene recognition. Extensive experiments on benchmarks of SUN RGB-D and NYU Depth V2 demonstrate the superiority of the proposed method to the state-of-the-art RGB-D scene recognition methods. We also generalize the TRecgNet to the single modality scene recognition benchmark of MIT Indoor, and automatically synthesize a depth view to boost the final recognition accuracy.},
  archive      = {J_IJCV},
  author       = {Du, Dapeng and Wang, Limin and Li, Zhaoyang and Wu, Gangshan},
  doi          = {10.1007/s11263-021-01475-7},
  journal      = {International Journal of Computer Vision},
  number       = {8},
  pages        = {2309-2327},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Cross-modal pyramid translation for RGB-D scene recognition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating demographic bias in facial datasets with
style-based multi-attribute transfer. <em>IJCV</em>, <em>129</em>(7),
2288–2307. (<a
href="https://doi.org/10.1007/s11263-021-01448-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has catalysed progress in tasks such as face recognition and analysis, leading to a quick integration of technological solutions in multiple layers of our society. While such systems have proven to be accurate by standard evaluation metrics and benchmarks, a surge of work has recently exposed the demographic bias that such algorithms exhibit–highlighting that accuracy does not entail fairness. Clearly, deploying biased systems under real-world settings can have grave consequences for affected populations. Indeed, learning methods are prone to inheriting, or even amplifying the bias present in a training set, manifested by uneven representation across demographic groups. In facial datasets, this particularly relates to attributes such as skin tone, gender, and age. In this work, we address the problem of mitigating bias in facial datasets by data augmentation. We propose a multi-attribute framework that can successfully transfer complex, multi-scale facial patterns even if these belong to underrepresented groups in the training set. This is achieved by relaxing the rigid dependence on a single attribute label, and further introducing a tensor-based mixing structure that captures multiplicative interactions between attributes in a multilinear fashion. We evaluate our method with an extensive set of qualitative and quantitative experiments on several datasets, with rigorous comparisons to state-of-the-art methods. We find that the proposed framework can successfully mitigate dataset bias, as evinced by extensive evaluations on established diversity metrics, while significantly improving fairness metrics such as equality of opportunity.},
  archive      = {J_IJCV},
  author       = {Georgopoulos, Markos and Oldfield, James and Nicolaou, Mihalis A. and Panagakis, Yannis and Pantic, Maja},
  doi          = {10.1007/s11263-021-01448-w},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2288-2307},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mitigating demographic bias in facial datasets with style-based multi-attribute transfer},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthetic humans for action recognition from unseen
viewpoints. <em>IJCV</em>, <em>129</em>(7), 2264–2287. (<a
href="https://doi.org/10.1007/s11263-021-01467-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although synthetic training data has been shown to be beneficial for tasks such as human pose estimation, its use for RGB human action recognition is relatively unexplored. Our goal in this work is to answer the question whether synthetic humans can improve the performance of human action recognition, with a particular focus on generalization to unseen viewpoints. We make use of the recent advances in monocular 3D human body reconstruction from real action sequences to automatically render synthetic training videos for the action labels. We make the following contributions: (1) we investigate the extent of variations and augmentations that are beneficial to improving performance at new viewpoints. We consider changes in body shape and clothing for individuals, as well as more action relevant augmentations such as non-uniform frame sampling, and interpolating between the motion of individuals performing the same action; (2) We introduce a new data generation methodology, SURREACT, that allows training of spatio-temporal CNNs for action classification; (3) We substantially improve the state-of-the-art action recognition performance on the NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally, (4) we extend the augmentation approach to in-the-wild videos from a subset of the Kinetics dataset to investigate the case when only one-shot training data is available, and demonstrate improvements in this case as well.},
  archive      = {J_IJCV},
  author       = {Varol, Gül and Laptev, Ivan and Schmid, Cordelia and Zisserman, Andrew},
  doi          = {10.1007/s11263-021-01467-7},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2264-2287},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Synthetic humans for action recognition from unseen viewpoints},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised domain adaptation with background shift
mitigating for person re-identification. <em>IJCV</em>, <em>129</em>(7),
2244–2263. (<a
href="https://doi.org/10.1007/s11263-021-01474-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation has been a popular approach for cross-domain person re-identification (re-ID). There are two solutions based on this approach. One solution is to build a model for data transformation across two different domains. Thus, the data in source domain can be transferred to target domain where re-ID model can be trained by rich source domain data. The other solution is to use target domain data plus corresponding virtual labels to train a re-ID model. Constrains in both solutions are very clear. The first solution heavily relies on the quality of data transformation model. Moreover, the final re-ID model is trained by source domain data but lacks knowledge of the target domain. The second solution in fact mixes target domain data with virtual labels and source domain data with true annotation information. But such a simple mixture does not well consider the raw information gap between data of two domains. This gap can be largely contributed by the background differences between domains. In this paper, a Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to mitigate the gaps of data between two domains. In order to tackle the constraints in the first solution mentioned above, this paper proposes a Densely Associated 2-Stream (DA-2S) network with an update strategy to best learn discriminative ID features from generated data that consider both human body information and also certain useful ID-related cues in the environment. The built re-ID model is further updated using target domain data with corresponding virtual labels. Extensive evaluations on three large benchmark datasets show the effectiveness of the proposed method.},
  archive      = {J_IJCV},
  author       = {Huang, Yan and Wu, Qiang and Xu, Jingsong and Zhong, Yi and Zhang, Zhaoxiang},
  doi          = {10.1007/s11263-021-01474-8},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2244-2263},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised domain adaptation with background shift mitigating for person re-identification},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scale-aware domain adaptive faster r-CNN. <em>IJCV</em>,
<em>129</em>(7), 2223–2243. (<a
href="https://doi.org/10.1007/s11263-021-01447-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection typically assumes that training and test samples are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch may lead to a significant performance drop. In this work, we present Scale-aware Domain Adaptive Faster R-CNN, a model aiming at improving the cross-domain robustness of object detection. In particular, our model improves the traditional Faster R-CNN model by tackling the domain shift on two levels: (1) the image-level shift, such as image style, illumination, etc., and (2) the instance-level shift, such as object appearance, size, etc. The two domain adaptation modules are implemented by learning domain classifiers in an adversarial training manner. Moreover, we observe that the large variance in object scales often brings a crucial challenge to cross-domain object detection. Thus, we improve our model by explicitly incorporating the object scale into adversarial training. We evaluate our proposed model on multiple cross-domain scenarios, including object detection in adverse weather, learning from synthetic data, and cross-camera adaptation, where the proposed model outperforms baselines and competing methods by a significant margin. The promising results demonstrate the effectiveness of our proposed model for cross-domain object detection. The implementation of our model is available at https://github.com/yuhuayc/sa-da-faster .},
  archive      = {J_IJCV},
  author       = {Chen, Yuhua and Wang, Haoran and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  doi          = {10.1007/s11263-021-01447-x},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2223-2243},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Scale-aware domain adaptive faster R-CNN},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The isowarp: The template-based visual geometry of isometric
surfaces. <em>IJCV</em>, <em>129</em>(7), 2194–2222. (<a
href="https://doi.org/10.1007/s11263-021-01472-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Registration maps or warps form a key element in Shape-from-Template (SfT). They relate the template with the input image, which contains the projection of the deformed surface. Recently, it was shown that isometric SfT can be solved analytically if the warp and its first-order derivatives are known. In practice, the warp is recovered by interpolating a set of discrete template-to-image point correspondences. This process relies on smoothness priors but ignores the 3D geometry. This may produce errors in the warp and poor reconstructions. In contrast, we propose to create a 3D consistent warp, which technically is a very challenging task, as the 3D shape variables must be eliminated from the isometric SfT equations to find differential constraints for the warp only. Integrating these constraints in warp estimation yields the isowarp, a warp 3D consistent with isometric SfT. Experimental results show that incorporating the isowarp in the SfT pipeline allows the analytic solution to outperform non-convex 3D shape refinement methods and the recent DNN-based SfT methods. The isowarp can be properly initialized with convex methods and its hyperparameters can be automatically obtained with cross-validation. The isowarp is resistant to 3D ambiguities and less computationally expensive than existing 3D shape refinement methods. The isowarp is thus a theoretical and practical breakthrough in SfT.},
  archive      = {J_IJCV},
  author       = {Casillas-Perez, David and Pizarro, Daniel and Fuentes-Jimenez, David and Mazo, Manuel and Bartoli, Adrien},
  doi          = {10.1007/s11263-021-01472-w},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2194-2222},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {The isowarp: The template-based visual geometry of isometric surfaces},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention guided low-light image enhancement with a large
scale low-light simulation dataset. <em>IJCV</em>, <em>129</em>(7),
2175–2193. (<a
href="https://doi.org/10.1007/s11263-021-01466-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement is challenging in that it needs to consider not only brightness recovery but also complex issues like color distortion and noise, which usually hide in the dark. Simply adjusting the brightness of a low-light image will inevitably amplify those artifacts. To address this difficult problem, this paper proposes a novel end-to-end attention-guided method based on multi-branch convolutional neural network. To this end, we first construct a synthetic dataset with carefully designed low-light simulation strategies. The dataset is much larger and more diverse than existing ones. With the new dataset for training, our method learns two attention maps to guide the brightness enhancement and denoising tasks respectively. The first attention map distinguishes underexposed regions from well lit regions, and the second attention map distinguishes noises from real textures. With their guidance, the proposed multi-branch decomposition-and-fusion enhancement network works in an input adaptive way. Moreover, a reinforcement-net further enhances color and contrast of the output image. Extensive experiments on multiple datasets demonstrate that our method can produce high fidelity enhancement results for low-light images and outperforms the current state-of-the-art methods by a large margin both quantitatively and visually.},
  archive      = {J_IJCV},
  author       = {Lv, Feifan and Li, Yu and Lu, Feng},
  doi          = {10.1007/s11263-021-01466-8},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2175-2193},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Attention guided low-light image enhancement with a large scale low-light simulation dataset},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VPR-bench: An open-source visual place recognition
evaluation framework with quantifiable viewpoint and appearance change.
<em>IJCV</em>, <em>129</em>(7), 2136–2174. (<a
href="https://doi.org/10.1007/s11263-021-01469-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition (VPR) is the process of recognising a previously visited place using visual information, often under varying appearance conditions and viewpoint changes and with computational constraints. VPR is related to the concepts of localisation, loop closure, image retrieval and is a critical component of many autonomous navigation systems ranging from autonomous vehicles to drones and computer vision systems. While the concept of place recognition has been around for many years, VPR research has grown rapidly as a field over the past decade due to improving camera hardware and its potential for deep learning-based techniques, and has become a widely studied topic in both the computer vision and robotics communities. This growth however has led to fragmentation and a lack of standardisation in the field, especially concerning performance evaluation. Moreover, the notion of viewpoint and illumination invariance of VPR techniques has largely been assessed qualitatively and hence ambiguously in the past. In this paper, we address these gaps through a new comprehensive open-source framework for assessing the performance of VPR techniques, dubbed “VPR-Bench”. VPR-Bench (Open-sourced at: https://github.com/MubarizZaffar/VPR-Bench ) introduces two much-needed capabilities for VPR researchers: firstly, it contains a benchmark of 12 fully-integrated datasets and 10 VPR techniques, and secondly, it integrates a comprehensive variation-quantified dataset for quantifying viewpoint and illumination invariance. We apply and analyse popular evaluation metrics for VPR from both the computer vision and robotics communities, and discuss how these different metrics complement and/or replace each other, depending upon the underlying applications and system requirements. Our analysis reveals that no universal SOTA VPR technique exists, since: (a) state-of-the-art (SOTA) performance is achieved by 8 out of the 10 techniques on at least one dataset, (b) SOTA technique in one community does not necessarily yield SOTA performance in the other given the differences in datasets and metrics. Furthermore, we identify key open challenges since: (c) all 10 techniques suffer greatly in perceptually-aliased and less-structured environments, (d) all techniques suffer from viewpoint variance where lateral change has less effect than 3D change, and (e) directional illumination change has more adverse effects on matching confidence than uniform illumination change. We also present detailed meta-analyses regarding the roles of varying ground-truths, platforms, application requirements and technique parameters. Finally, VPR-Bench provides a unified implementation to deploy these VPR techniques, metrics and datasets, and is extensible through templates.},
  archive      = {J_IJCV},
  author       = {Zaffar, Mubariz and Garg, Sourav and Milford, Michael and Kooij, Julian and Flynn, David and McDonald-Maier, Klaus and Ehsan, Shoaib},
  doi          = {10.1007/s11263-021-01469-5},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2136-2174},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {VPR-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous 3D multi-channel sign language production via
progressive transformers and mixture density networks. <em>IJCV</em>,
<em>129</em>(7), 2113–2135. (<a
href="https://doi.org/10.1007/s11263-021-01457-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate. Sign language production (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation and full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works have produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-expressive production. In this work, we propose a novel Progressive Transformer architecture, the first SLP model to translate from spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer network architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking the production progress over time and predicting the end of sequence. We present extensive data augmentation techniques to reduce prediction drift, alongside an adversarial training regime and a mixture density network (MDN) formulation to produce realistic and expressive sign pose sequences. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further provide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions.},
  archive      = {J_IJCV},
  author       = {Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard},
  doi          = {10.1007/s11263-021-01457-9},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2113-2135},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continuous 3D multi-channel sign language production via progressive transformers and mixture density networks},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quo vadis, skeleton action recognition? <em>IJCV</em>,
<em>129</em>(7), 2097–2112. (<a
href="https://doi.org/10.1007/s11263-021-01470-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To study skeleton-action recognition in the wild, we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. We also introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. The results from benchmarking the top performers of NTU-120 on the newly introduced datasets reveal the challenges and domain gap induced by actions in the wild. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. Via the introduced datasets, our work enables new frontiers for human action recognition.},
  archive      = {J_IJCV},
  author       = {Gupta, Pranay and Thatipelli, Anirudh and Aggarwal, Aditya and Maheshwari, Shubh and Trivedi, Neel and Das, Sourav and Sarvadevabhatla, Ravi Kiran},
  doi          = {10.1007/s11263-021-01470-y},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2097-2112},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Quo vadis, skeleton action recognition?},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN-based RGB-d salient object detection: Learn, select, and
fuse. <em>IJCV</em>, <em>129</em>(7), 2076–2096. (<a
href="https://doi.org/10.1007/s11263-021-01452-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection, and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which we use the progressive predictions from the well-learned source modality to supervise learning feature hierarchies and inference in the new modality. To better select complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal cross-level interactions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in learning from a new modality, the advantages of the proposed multi-modal fusion pattern in selecting and fusing cross-modal complements, and the generalization of the proposed designs in different tasks.},
  archive      = {J_IJCV},
  author       = {Chen, Hao and Li, Youfu and Deng, Yongjian and Lin, Guosheng},
  doi          = {10.1007/s11263-021-01452-0},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2076-2096},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CNN-based RGB-D salient object detection: Learn, select, and fuse},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A shape-aware retargeting approach to transfer human motion
and appearance in monocular videos. <em>IJCV</em>, <em>129</em>(7),
2057–2075. (<a
href="https://doi.org/10.1007/s11263-021-01471-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring human motion and appearance between videos of human actors remains one of the key challenges in Computer Vision. Despite the advances from recent image-to-image translation approaches, there are several transferring contexts where most end-to-end learning-based retargeting methods still perform poorly. Transferring human appearance from one actor to another is only ensured when a strict setup has been complied, which is generally built considering their training regime’s specificities. In this work, we propose a shape-aware approach based on a hybrid image-based rendering technique that exhibits competitive visual retargeting quality compared to state-of-the-art neural rendering approaches. The formulation leverages the user body shape into the retargeting while considering physical constraints of the motion in 3D and the 2D image domain. We also present a new video retargeting benchmark dataset composed of different videos with annotated human motions to evaluate the task of synthesizing people’s videos, which can be used as a common base to improve tracking the progress in the field. The dataset and its evaluation protocols are designed to evaluate retargeting methods in more general and challenging conditions. Our method is validated in several experiments, comprising publicly available videos of actors with different shapes, motion types, and camera setups. The dataset and retargeting code are publicly available to the community at: https://www.verlab.dcc.ufmg.br/retargeting-motion .},
  archive      = {J_IJCV},
  author       = {Gomes, Thiago L. and Martins, Renato and Ferreira, João and Azevedo, Rafael and Torres, Guilherme and Nascimento, Erickson R.},
  doi          = {10.1007/s11263-021-01471-x},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2057-2075},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A shape-aware retargeting approach to transfer human motion and appearance in monocular videos},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmentation by continuous latent semantic analysis for
multi-structure model fitting. <em>IJCV</em>, <em>129</em>(7),
2034–2056. (<a
href="https://doi.org/10.1007/s11263-021-01468-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel continuous latent semantic analysis fitting method, to efficiently and effectively estimate the parameters of model instances in data, based on latent semantic analysis and continuous preference analysis. Specifically, we construct a new latent semantic space (LSS): where inliers of different model instances are mapped into several independent directions, while gross outliers are distributed close to the origin of LSS. After that, we analyze the data distribution to effectively remove gross outliers in LSS, and propose an improved clustering algorithm to segment the remaining data points. On the one hand, the proposed fitting method is able to achieve excellent fitting results; due to the effective continuous preference analysis in LSS. On the other hand, the proposed method can efficiently obtain final fitting results due to the dimensionality reduction in LSS. Experimental results on both synthetic data and real images demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both fitting accuracy and computational speed.},
  archive      = {J_IJCV},
  author       = {Xiao, Guobao and Wang, Hanzi and Ma, Jiayi and Suter, David},
  doi          = {10.1007/s11263-021-01468-6},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2034-2056},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Segmentation by continuous latent semantic analysis for multi-structure model fitting},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on “computer vision for all
seasons: Adverse weather and lighting conditions.” <em>IJCV</em>,
<em>129</em>(7), 2031–2033. (<a
href="https://doi.org/10.1007/s11263-021-01464-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Dai, Dengxin and Tan, Robby T. and Patel, Vishal and Matas, Jiri and Schiele, Bernt and Van Gool, Luc},
  doi          = {10.1007/s11263-021-01464-w},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2031-2033},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: special issue on “Computer vision for all seasons: adverse weather and lighting conditions”},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on performance evaluation in
computer vision. <em>IJCV</em>, <em>129</em>(7), 2029–2030. (<a
href="https://doi.org/10.1007/s11263-021-01455-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Scharstein, Daniel and Dai, Angela and Kondermann, Daniel and Sattler, Torsten and Schindler, Konrad},
  doi          = {10.1007/s11263-021-01455-x},
  journal      = {International Journal of Computer Vision},
  number       = {7},
  pages        = {2029-2030},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on performance evaluation in computer vision},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Renormalization for initialization of rolling shutter
visual-inertial odometry. <em>IJCV</em>, <em>129</em>(6), 2011–2027. (<a
href="https://doi.org/10.1007/s11263-021-01462-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we deal with the initialization problem of a visual-inertial odometry system with rolling shutter cameras. Initialization is a prerequisite for using inertial signals and fusing them with visual data. We propose a novel statistical solution to the initialization problem on visual and inertial data simultaneously, by casting it into the renormalization scheme of Kanatani. The renormalization is an optimization scheme which intends to reduce the inherent statistical bias of common linear systems. We derive and present the necessary steps and methodology specific to the initialization problem. Extensive evaluations on ground truth exhibit superior performance and a gain in accuracy of up to $$20\%$$ over the originally proposed Least Squares solution. The renormalization performs similarly to the optimal Maximum Likelihood estimate, despite arriving at the solution by different means. With this paper we are adding to the set of Computer Vision problems which can be cast into the renormalization scheme.},
  archive      = {J_IJCV},
  author       = {Micusik, Branislav and Evangelidis, Georgios},
  doi          = {10.1007/s11263-021-01462-y},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {2011-2027},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Renormalization for initialization of rolling shutter visual-inertial odometry},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Deep human-interaction and association by graph-based
learning for multiple object tracking in the wild. <em>IJCV</em>,
<em>129</em>(6), 1993–2010. (<a
href="https://doi.org/10.1007/s11263-021-01460-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Object Tracking (MOT) in the wild has a wide range of applications in surveillance retrieval and autonomous driving. Tracking-by-Detection has become a mainstream solution in MOT, which is composed of feature extraction and data association. Most of the existing methods focus on extracting targets’ individual features and optimizing the association by hand-crafted algorithms. In this paper, we specially consider the interrelation cue between targets and we propose Human-Interaction Model (HIM) to extract interaction features between the tracked target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded (dense) scene. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Both HIM and DAN are constructed by three kinds of deep networks, which include Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Graph Neural Network (GNN). The CNNs extract appearance features from bounding box images, the RNNs encoder motion features from historical positions of trajectory. And then the GNNs aim to extract interaction features and optimize graph structure to associate the objects in different frames. In addition, we present a novel end-to-end training strategy for Deep Association Network and Human-Interaction Model. Our experimental results demonstrate performance of our method reaches the state-of-the-art on MOT15, MOT16 and DukeMTMCT datasets.},
  archive      = {J_IJCV},
  author       = {Ma, Cong and Yang, Fan and Li, Yuan and Jia, Huizhu and Xie, Xiaodong and Gao, Wen},
  doi          = {10.1007/s11263-021-01460-0},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1993-2010},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep human-interaction and association by graph-based learning for multiple object tracking in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the capacity of an orderless box discretization
network for multi-orientation scene text detection. <em>IJCV</em>,
<em>129</em>(6), 1972–1992. (<a
href="https://doi.org/10.1007/s11263-021-01459-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-orientation scene text detection has recently gained significant research attention. Previous methods directly predict words or text lines, typically by using quadrilateral shapes. However, many of these methods neglect the significance of consistent labeling, which is important for maintaining a stable training process, especially when it comprises a large amount of data. Here we solve this problem by proposing a new method, Orderless Box Discretization (OBD), which first discretizes the quadrilateral box into several key edges containing all potential horizontal and vertical positions. To decode accurate vertex positions, a simple yet effective matching procedure is proposed for reconstructing the quadrilateral bounding boxes. Our method solves the ambiguity issue, which has a significant impact on the learning process. Extensive ablation studies are conducted to validate the effectiveness of our proposed method quantitatively. More importantly, based on OBD, we provide a detailed analysis of the impact of a collection of refinements, which may inspire others to build state-of-the-art text detectors. Combining both OBD and these useful refinements, we achieve state-of-the-art performance on various benchmarks, including ICDAR 2015 and MLT. Our method also won the first place in the text detection task at the recent ICDAR2019 Robust Reading Challenge for Reading Chinese Text on Signboards, further demonstrating its superior performance. The code is available at https://git.io/TextDet .},
  archive      = {J_IJCV},
  author       = {Liu, Yuliang and He, Tong and Chen, Hao and Wang, Xinyu and Luo, Canjie and Zhang, Shuaitao and Shen, Chunhua and Jin, Lianwen},
  doi          = {10.1007/s11263-021-01459-7},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1972-1992},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring the capacity of an orderless box discretization network for multi-orientation scene text detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object priors for classifying and localizing unseen actions.
<em>IJCV</em>, <em>129</em>(6), 1954–1971. (<a
href="https://doi.org/10.1007/s11263-021-01454-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work strives for the classification and localization of human actions in videos, without the need for any labeled video training examples. Where existing work relies on transferring global attribute or object information from seen to unseen action videos, we seek to classify and spatio-temporally localize unseen actions in videos from image-based object information only. We propose three spatial object priors, which encode local person and object detectors along with their spatial relations. On top we introduce three semantic object priors, which extend semantic matching through word embeddings with three simple functions that tackle semantic ambiguity, object discrimination, and object naming. A video embedding combines the spatial and semantic object priors. It enables us to introduce a new video retrieval task that retrieves action tubes in video collections based on user-specified objects, spatial relations, and object size. Experimental evaluation on five action datasets shows the importance of spatial and semantic object priors for unseen actions. We find that persons and objects have preferred spatial relations that benefit unseen action localization, while using multiple languages and simple object filtering directly improves semantic matching, leading to state-of-the-art results for both unseen action classification and localization.},
  archive      = {J_IJCV},
  author       = {Mettes, Pascal and Thong, William and Snoek, Cees G. M.},
  doi          = {10.1007/s11263-021-01454-y},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1954-1971},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Object priors for classifying and localizing unseen actions},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning adaptive classifiers synthesis for generalized
few-shot learning. <em>IJCV</em>, <em>129</em>(6), 1930–1953. (<a
href="https://doi.org/10.1007/s11263-020-01381-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition in the real-world requires handling long-tailed or even open-ended data. An ideal visual system needs to recognize the populated head visual concepts reliably and meanwhile efficiently learn about emerging new tail categories with a few training instances. Class-balanced many-shot learning and few-shot learning tackle one side of this problem, by either learning strong classifiers for head or learning to learn few-shot classifiers for the tail. In this paper, we investigate the problem of generalized few-shot learning (GFSL)—a model during the deployment is required to learn about tail categories with few shots and simultaneously classify the head classes. We propose the ClAssifier SynThesis LEarning (Castle), a learning framework that learns how to synthesize calibrated few-shot classifiers in addition to the multi-class classifiers of head classes with a shared neural dictionary, shedding light upon the inductive GFSL. Furthermore, we propose an adaptive version of Castle  (a Castle) that adapts the head classifiers conditioned on the incoming tail training examples, yielding a framework that allows effective backward knowledge transfer. As a consequence, a Castle can handle GFSL with classes from heterogeneous domains effectively. Castle and a Castle demonstrate superior performances than existing GFSL algorithms and strong baselines on MiniImageNet as well as TieredImageNet datasets. More interestingly, they outperform previous state-of-the-art methods when evaluated with standard few-shot learning criteria.},
  archive      = {J_IJCV},
  author       = {Ye, Han-Jia and Hu, Hexiang and Zhan, De-Chuan},
  doi          = {10.1007/s11263-020-01381-4},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1930-1953},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning adaptive classifiers synthesis for generalized few-shot learning},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polysemy deciphering network for robust human–object
interaction detection. <em>IJCV</em>, <em>129</em>(6), 1910–1929. (<a
href="https://doi.org/10.1007/s11263-021-01458-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–Object Interaction (HOI) detection is important to human-centric scene understanding tasks. Existing works tend to assume that the same verb has similar visual characteristics in different HOI categories, an approach that ignores the diverse semantic meanings of the verb. To address this issue, in this paper, we propose a novel Polysemy Deciphering Network (PD-Net) that decodes the visual polysemy of verbs for HOI detection in three distinct ways. First, we refine features for HOI detection to be polysemy-aware through the use of two novel modules: namely, Language Prior-guided Channel Attention (LPCA) and Language Prior-based Feature Augmentation (LPFA). LPCA highlights important elements in human and object appearance features for each HOI category to be identified; moreover, LPFA augments human pose and spatial features for HOI detection using language priors, enabling the verb classifiers to receive language hints that reduce intra-class variation for the same verb. Second, we introduce a novel Polysemy-Aware Modal Fusion module, which guides PD-Net to make decisions based on feature types deemed more important according to the language priors. Third, we propose to relieve the verb polysemy problem through sharing verb classifiers for semantically similar HOI categories. Furthermore, to expedite research on the verb polysemy problem, we build a new benchmark dataset named HOI-VerbPolysemy (HOI-VP), which includes common verbs (predicates) that have diverse semantic meanings in the real world. Finally, through deciphering the visual polysemy of verbs, our approach is demonstrated to outperform state-of-the-art methods by significant margins on the HICO-DET, V-COCO, and HOI-VP databases. Code and data in this paper are available at https://github.com/MuchHair/PD-Net .},
  archive      = {J_IJCV},
  author       = {Zhong, Xubin and Ding, Changxing and Qu, Xian and Tao, Dacheng},
  doi          = {10.1007/s11263-021-01458-8},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1910-1929},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Polysemy deciphering network for robust Human–Object interaction detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual structure constraint for transductive zero-shot
learning in the wild. <em>IJCV</em>, <em>129</em>(6), 1893–1909. (<a
href="https://doi.org/10.1007/s11263-021-01451-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To recognize objects of the unseen classes, most existing Zero-Shot Learning(ZSL) methods first learn a compatible projection function between the common semantic space and the visual space based on the data of source seen classes, then directly apply it to the target unseen classes. However, for data in the wild, distributions between the source and target domain might not match well, thus causing the well-known domain shift problem. Based on the observation that visual features of test instances can be separated into different clusters, we propose a new visual structure constraint on class centers for transductive ZSL, to improve the generality of the projection function (i.e.alleviate the above domain shift problem). Specifically, three different strategies (symmetric Chamfer-distance, Bipartite matching distance, and Wasserstein distance) are adopted to align the projected unseen semantic centers and visual cluster centers of test instances. We also propose two new training strategies to handle the data in the wild, where many unrelated images in the test dataset may exist. This realistic setting has never been considered in previous methods. Extensive experiments demonstrate that the proposed visual structure constraint brings substantial performance gain consistently and the new training strategies make it generalize well for data in the wild. The source code is available at https://github.com/raywzy/VSC .},
  archive      = {J_IJCV},
  author       = {Wan, Ziyu and Chen, Dongdong and Liao, Jing},
  doi          = {10.1007/s11263-021-01451-1},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1893-1909},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Visual structure constraint for transductive zero-shot learning in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guided attention in CNNs for occluded pedestrian detection
and re-identification. <em>IJCV</em>, <em>129</em>(6), 1875–1892. (<a
href="https://doi.org/10.1007/s11263-021-01461-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection and re-identification have progressed significantly in the last few years. However, occluded people are notoriously hard to detect and recognize, as their appearance varies substantially depending on a wide range of occlusion patterns. In this paper, we aim to propose a simple and compact method based on CNNs for occlusion handling. We start with interpreting CNN channel features of a pedestrian detector, and we find that different channels activate responses for different body parts respectively. These findings motivate us to employ an attention mechanism across channels to represent various occlusion patterns in one single model, as each occlusion pattern can be formulated as some specific combination of body parts. Therefore, an attention network with self or external guidances is proposed as an add-on to the baseline CNN method. Also, we propose an attention guided self-paced learning method to balance the optimization across different occlusion levels. Our proposed method shows significant improvements over the baseline methods for both pedestrian detection and re-identification tasks. For pedestrian detection, we achieve a considerable improvement of 8pp to the baseline FasterRCNN detector on the heavy occlusion subset of CityPersons and on Caltech we outperform the state-of-the-art method by 5pp. For pedestrian re-identification, our method surpasses the baseline and achieves state-of-the-art performance on multiple re-identification benchmarks.},
  archive      = {J_IJCV},
  author       = {Zhang, Shanshan and Chen, Di and Yang, Jian and Schiele, Bernt},
  doi          = {10.1007/s11263-021-01461-z},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1875-1892},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guided attention in CNNs for occluded pedestrian detection and re-identification},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vote-based 3D object detection with context modeling and
SOB-3DNMS. <em>IJCV</em>, <em>129</em>(6), 1857–1874. (<a
href="https://doi.org/10.1007/s11263-021-01456-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing 3D object detection methods recognize objects individually, without giving any consideration on contextual information between these objects. However, objects in indoor scenes are usually related to each other and the scene, forming the contextual information. Based on this observation, we propose a novel 3D object detection network, which is built on the state-of-the-art VoteNet but takes into consideration of the contextual information at multiple levels for detection and recognition of 3D objects. To encode relationships between elements at different levels, we introduce three contextual sub-modules, capturing contextual information at patch, object, and scene levels respectively, and build them into the voting and classification stages of VoteNet. In addition, at the post-processing stage, we also consider the spatial diversity of detected objects and propose an improved 3D NMS (non-maximum suppression) method, namely Survival-Of-the-Best 3DNMS (SOB-3DNMS), to reduce false detections. Experiments demonstrate that our method is an effective way to promote detection accuracy, and has achieved new state-of-the-art detection performance on challenging 3D object detection datasets, i.e., SUN RGBD and ScanNet, when only taking point cloud data as input.},
  archive      = {J_IJCV},
  author       = {Xie, Qian and Lai, Yu-Kun and Wu, Jing and Wang, Zhoutao and Zhang, Yiming and Xu, Kai and Wang, Jun},
  doi          = {10.1007/s11263-021-01456-w},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1857-1874},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Vote-based 3D object detection with context modeling and SOB-3DNMS},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development and validation of an unsupervised feature
learning system for leukocyte characterization and classification: A
multi-hospital study. <em>IJCV</em>, <em>129</em>(6), 1837–1856. (<a
href="https://doi.org/10.1007/s11263-021-01449-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characterization and classification of white blood cells (WBC) are critical for the diagnosis of anemia, leukemia, and many other hematologic diseases. We developed WBC-Profiler, an unsupervised feature learning system for quantitative analysis of leukocytes. We demonstrate, through independent validation, that WBC-Profiler enables automatic extraction of complex and robust signatures from microscopic images without human-intervention and, thereafter, effective construction of interpretable leukocyte profiles, which decouples large scale complex leukocyte characterization from limitations in both human-based feature engineering/optimization and the end-to-end solutions provided by many modern deep neural networks. Further evaluation in a real-world clinical setting confirms that, compared with 23 clinicians from 8 hospitals (class-average-sensitivity, 0.798; class-average-specificity, 0.963; cell-average-timecost: 3.158  s), WBC-Profiler performs with significantly improved accuracy and speed (class-average-sensitivity, 0.890; class-average-specificity, 0.980; cell-average-timecost: 0.375  s). Our findings suggest that WBC-Profiler has the potential clinical implications.},
  archive      = {J_IJCV},
  author       = {Yan, Hong and Mao, Xuanyu and Yang, Xu and Xia, Yongquan and Wang, Chengbin and Wang, Junjun and Xia, Rui and Xu, Xuejing and Wang, Zhiqiang and Li, Zhiyang and Zhao, Xie and Li, Yan and Liu, Guoye and He, Li and Wang, Zhongyu and Wang, Zhiqiong and Li, Zhiqiang and Cai, Weidong and Shen, Han and Chang, Hang},
  doi          = {10.1007/s11263-021-01449-9},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1837-1856},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Development and validation of an unsupervised feature learning system for leukocyte characterization and classification: A multi-hospital study},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning deep patch representation for probabilistic
graphical model-based face sketch synthesis. <em>IJCV</em>,
<em>129</em>(6), 1820–1836. (<a
href="https://doi.org/10.1007/s11263-021-01442-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face sketch synthesis has a wide range of applications in both digital entertainment and law enforcement. State-of-the-art examplar-based methods typically exploit a Probabilistic Graphical Model (PGM) to represent the joint probability distribution over all of the patches selected from a set of training data. However, these methods suffer from two main shortcomings: (1) most of these methods capture the evidence between patches in pixel-level, which lead to inaccurate parameter estimation under bad environment conditions such as light variations and clutter backgrounds; (2) the assumption that a photo patch and its corresponding sketch patch share similar geometric manifold structure is not rigorous. It has shown that deep convolutional neural network (CNN) has outstanding performance in learning to extract high-level feature representation. Therefore, we extract uniform deep patch representations of test photo patches and training sketch patches from a specially designed CNN model to replace pixel intensity, and directly match between them, which can help select better candidate patches from training data as well as improve parameter learning process. In this way, we investigate a novel face sketch synthesis method called DPGM that combines generative PGM and discriminative deep patch representation, which can jointly model the distribution over the parameters for deep patch representation and the distribution over the parameters for sketch patch reconstruction. Then, we apply an alternating iterative optimization strategy to simultaneously optimize two kinds of parameters. Therefore, both the representation capability of deep patch representation and the reconstruction ability of sketch patches can be boosted. Eventually, high quality reconstructed sketches which is robust against light variations and clutter backgrounds can be obtained. Extensive experiments on several benchmark datasets demonstrate that our method can achieve superior performance than other state-of-the-art methods, especially under the case of bad light conditions or clutter backgrounds.},
  archive      = {J_IJCV},
  author       = {Zhu, Mingrui and Li, Jie and Wang, Nannan and Gao, Xinbo},
  doi          = {10.1007/s11263-021-01442-2},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1820-1836},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning deep patch representation for probabilistic graphical model-based face sketch synthesis},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge distillation: A survey. <em>IJCV</em>,
<em>129</em>(6), 1789–1819. (<a
href="https://doi.org/10.1007/s11263-021-01453-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  archive      = {J_IJCV},
  author       = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
  doi          = {10.1007/s11263-021-01453-z},
  journal      = {International Journal of Computer Vision},
  number       = {6},
  pages        = {1789-1819},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Knowledge distillation: A survey},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Parallel single-pixel imaging: A general
method for direct–global separation and 3D shape reconstruction under
strong global illumination. <em>IJCV</em>, <em>129</em>(5), 1787. (<a
href="https://doi.org/10.1007/s11263-021-01441-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the original publication of the article, the name of the last author should be Yang Xu, instead of Xu Yang. “Xu” is the family name, and “Yang” is the given name.},
  archive      = {J_IJCV},
  author       = {Jiang, Hongzhi and Li, Yuxi and Zhao, Huijie and Li, Xudong and Xu, Yang},
  doi          = {10.1007/s11263-021-01441-3},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1787},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction to: parallel single-pixel imaging: a general method for Direct–Global separation and 3D shape reconstruction under strong global illumination},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exposing semantic segmentation failures via maximum
discrepancy competition. <em>IJCV</em>, <em>129</em>(5), 1768–1786. (<a
href="https://doi.org/10.1007/s11263-021-01450-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is an extensively studied task in computer vision, with numerous methods proposed every year. Thanks to the advent of deep learning in semantic segmentation, the performance on existing benchmarks is close to saturation. A natural question then arises: Does the superior performance on the closed (and frequently re-used) test sets transfer to the open visual world with unconstrained variations? In this paper, we take steps toward answering the question by exposing failures of existing semantic segmentation methods in the open visual world under the constraint of very limited human labeling effort. Inspired by previous research on model falsification, we start from an arbitrarily large image set, and automatically sample a small image set by maximizing the discrepancy (MAD) between two segmentation methods. The selected images have the greatest potential in falsifying either (or both) of the two methods. We also explicitly enforce several conditions to diversify the exposed failures, corresponding to different underlying root causes. A segmentation method, whose failures are more difficult to be exposed in the MAD competition, is considered better. We conduct a thorough MAD diagnosis of ten PASCAL VOC semantic segmentation algorithms. With detailed analysis of experimental results, we point out strengths and weaknesses of the competing algorithms, as well as potential research directions for further advancement in semantic segmentation. The codes are publicly available at https://github.com/QTJiebin/MAD_Segmentation .},
  archive      = {J_IJCV},
  author       = {Yan, Jiebin and Zhong, Yu and Fang, Yuming and Wang, Zhangyang and Ma, Kede},
  doi          = {10.1007/s11263-021-01450-2},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1768-1786},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exposing semantic segmentation failures via maximum discrepancy competition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). You only look yourself: Unsupervised and untrained single
image dehazing neural network. <em>IJCV</em>, <em>129</em>(5),
1754–1767. (<a
href="https://doi.org/10.1007/s11263-021-01431-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study two challenging and less-touched problems in single image dehazing, namely, how to make deep learning achieve image dehazing without training on the ground-truth clean image (unsupervised) and an image collection (untrained). An unsupervised model will avoid the intensive labor of collecting hazy-clean image pairs, and an untrained model is a “real” single image dehazing approach which could remove haze based on the observed hazy image only and no extra images are used. Motivated by the layer disentanglement, we propose a novel method, called you only look yourself (YOLY) which could be one of the first unsupervised and untrained neural networks for image dehazing. In brief, YOLY employs three joint subnetworks to separate the observed hazy image into several latent layers, i.e., scene radiance layer, transmission map layer, and atmospheric light layer. After that, three layers are further composed to the hazy image in a self-supervised manner. Thanks to the unsupervised and untrained characteristics of YOLY, our method bypasses the conventional training paradigm of deep models on hazy-clean pairs or a large scale dataset, thus avoids the labor-intensive data collection and the domain shift issue. Besides, our method also provides an effective learning-based haze transfer solution thanks to its layer disentanglement mechanism. Extensive experiments show the promising performance of our method in image dehazing compared with 14 methods on six databases. The code could be accessed at www.pengxi.me .},
  archive      = {J_IJCV},
  author       = {Li, Boyun and Gou, Yuanbiao and Gu, Shuhang and Liu, Jerry Zitao and Zhou, Joey Tianyi and Peng, Xi},
  doi          = {10.1007/s11263-021-01431-5},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1754-1767},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {You only look yourself: Unsupervised and untrained single image dehazing neural network},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating visual properties via robust HodgeRank.
<em>IJCV</em>, <em>129</em>(5), 1732–1753. (<a
href="https://doi.org/10.1007/s11263-021-01438-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, how to effectively evaluate visual properties has become a popular topic for fine-grained visual comprehension. In this paper we study the problem of how to estimate such visual properties from a ranking perspective with the help of the annotators from online crowdsourcing platforms. The main challenges of our task are two-fold. On one hand, the annotations often contain contaminated information, where a small fraction of label flips might ruin the global ranking of the whole dataset. On the other hand, considering the large data capacity, the annotations are often far from being complete. What is worse, there might even exist imbalanced annotations where a small subset of samples are frequently annotated. Facing such challenges, we propose a robust ranking framework based on the principle of Hodge decomposition of imbalanced and incomplete ranking data. According to the HodgeRank theory, we find that the major source of the contamination comes from the cyclic ranking component of the Hodge decomposition. This leads us to an outlier detection formulation as sparse approximations of the cyclic ranking projection. Taking a step further, it facilitates a novel outlier detection model as Huber’s LASSO in robust statistics. Moreover, simple yet scalable algorithms are developed based on Linearized Bregman Iteration to achieve an even less biased estimator. Statistical consistency of outlier detection is established in both cases under nearly the same conditions. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a promising tool for robust ranking with large scale crowdsourcing data arising from computer vision.},
  archive      = {J_IJCV},
  author       = {Xu, Qianqian and Xiong, Jiechao and Cao, Xiaochun and Huang, Qingming and Yao, Yuan},
  doi          = {10.1007/s11263-021-01438-y},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1732-1753},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Evaluating visual properties via robust HodgeRank},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation metrics for conditional image generation.
<em>IJCV</em>, <em>129</em>(5), 1712–1731. (<a
href="https://doi.org/10.1007/s11263-020-01424-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two new metrics for evaluating generative models in the class-conditional image generation setting. These metrics are obtained by generalizing the two most popular unconditional metrics: the Inception Score (IS) and the Fréchet Inception Distance (FID). A theoretical analysis shows the motivation behind each proposed metric and links the novel metrics to their unconditional counterparts. The link takes the form of a product in the case of IS or an upper bound in the FID case. We provide an extensive empirical evaluation, comparing the metrics to their unconditional variants and to other metrics, and utilize them to analyze existing generative models, thus providing additional insights about their performance, from unlearned classes to mode collapse.},
  archive      = {J_IJCV},
  author       = {Benny, Yaniv and Galanti, Tomer and Benaim, Sagie and Wolf, Lior},
  doi          = {10.1007/s11263-020-01424-w},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1712-1731},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Evaluation metrics for conditional image generation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Successive graph convolutional network for image de-raining.
<em>IJCV</em>, <em>129</em>(5), 1691–1711. (<a
href="https://doi.org/10.1007/s11263-020-01428-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have shown their advantages in the single image de-raining task. However, most existing CNNs-based methods utilize only local spatial information without considering long-range contextual information. In this paper, we propose a graph convolutional networks (GCNs)-based model to solve the above problem. We specifically design two graphs to extract representations from new dimensions. The first graph models the global spatial relationship between pixels in the feature, while the second graph models the interrelationship across the channels. By integrating conventional CNNs and our GCNs into a single framework, the proposed method is able to explore comprehensive feature representations from three aspects, i.e., local spatial patterns, global spatial coherence and channel correlation. To better exploit the explored rich feature representations, we further introduce a simple yet effective recurrent operations to perform the de-raining process in a successive manner. Benefiting from the rich information exploration and exploitation, our method achieves state-of-the-art results on both synthetic and real-world data sets.},
  archive      = {J_IJCV},
  author       = {Fu, Xueyang and Qi, Qi and Zha, Zheng-Jun and Ding, Xinghao and Wu, Feng and Paisley, John},
  doi          = {10.1007/s11263-020-01428-6},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1691-1711},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Successive graph convolutional network for image de-raining},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mimetics: Towards understanding human actions out of
context. <em>IJCV</em>, <em>129</em>(5), 1675–1690. (<a
href="https://doi.org/10.1007/s11263-021-01446-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods for video action recognition have reached outstanding performances on existing benchmarks. However, they tend to leverage context such as scenes or objects instead of focusing on understanding the human action itself. For instance, a tennis field leads to the prediction playing tennis irrespectively of the actions performed in the video. In contrast, humans have a more complete understanding of actions and can recognize them without context. The best example of out-of-context actions are mimes, that people can typically recognize despite missing relevant objects and scenes. In this paper, we propose to benchmark action recognition methods in such absence of context and introduce a novel dataset, Mimetics, consisting of mimed actions for a subset of 50 classes from the Kinetics benchmark. Our experiments show that (a) state-of-the-art 3D convolutional neural networks obtain disappointing results on such videos, highlighting the lack of true understanding of the human actions and (b) models leveraging body language via human pose are less prone to context biases. In particular, we show that applying a shallow neural network with a single temporal convolution over body pose features transferred to the action recognition problem performs surprisingly well compared to 3D action recognition methods.},
  archive      = {J_IJCV},
  author       = {Weinzaepfel, Philippe and Rogez, Grégory},
  doi          = {10.1007/s11263-021-01446-y},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1675-1690},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mimetics: Towards understanding human actions out of context},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-enhanced representation learning for single image
deraining. <em>IJCV</em>, <em>129</em>(5), 1650–1674. (<a
href="https://doi.org/10.1007/s11263-020-01425-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perception of content and structure in images with rainstreaks or raindrops is challenging, and it often calls for robust deraining algorithms to remove the diversified rainy effects. Much progress has been made on the design of advanced encoder–decoder single image deraining networks. However, most of the existing networks are built in a blind manner and often produce over/under-deraining artefacts. In this paper, we point out, for the first time, that the unsatisfactory results are caused by the highly imbalanced distribution between rainy effects and varied background scenes. Ignoring this phenomenon results in the representation learned by the encoder being biased towards rainy regions, while paying less attention to the valuable contextual regions. To resolve this, a context-enhanced representation learning and deraining network is proposed with a novel two-branch encoder design. Specifically, one branch takes the rainy image directly as input for learning a mixed representation depicting the variation of both rainy regions and contextual regions, and another branch is guided by a carefully learned soft attention mask to learn an embedding only depicting the contextual regions. By combining the embeddings from these two branches with a carefully designed co-occurrence modelling module, and then improving the semantic property of the co-occurrence features via a bi-directional attention layer, the underlying imbalanced learning problem is resolved. Extensive experiments are carried out for removing rainstreaks and raindrops from both synthetic and real rainy images, and the proposed model is demonstrated to produce significantly better results than state-of-the-art models. In addition, comprehensive ablation studies are also performed to analyze the contributions of different designs. Code and pre-trained models will be publicly available at https://github.com/RobinCSIRO/CERLD-Net.git .},
  archive      = {J_IJCV},
  author       = {Wang, Guoqing and Sun, Changming and Sowmya, Arcot},
  doi          = {10.1007/s11263-020-01425-9},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1650-1674},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Context-enhanced representation learning for single image deraining},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An exploration of embodied visual exploration.
<em>IJCV</em>, <em>129</em>(5), 1616–1649. (<a
href="https://doi.org/10.1007/s11263-021-01437-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied computer vision considers perception for robots in novel, unstructured environments. Of particular importance is the embodied visual exploration problem: how might a robot equipped with a camera scope out a new environment? Despite the progress thus far, many basic questions pertinent to this problem remain unanswered: (i) What does it mean for an agent to explore its environment well? (ii) Which methods work well, and under which assumptions and environmental settings? (iii) Where do current approaches fall short, and where might future work seek to improve? Seeking answers to these questions, we first present a taxonomy for existing visual exploration algorithms and create a standard framework for benchmarking them. We then perform a thorough empirical study of the four state-of-the-art paradigms using the proposed framework with two photorealistic simulated 3D environments, a state-of-the-art exploration architecture, and diverse evaluation metrics. Our experimental results offer insights and suggest new performance metrics and baselines for future work in visual exploration. Code, models and data are publicly available.},
  archive      = {J_IJCV},
  author       = {Ramakrishnan, Santhosh K. and Jayaraman, Dinesh and Grauman, Kristen},
  doi          = {10.1007/s11263-021-01437-z},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1616-1649},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An exploration of embodied visual exploration},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced 3D human pose estimation from videos by using
attention-based neural network with dilated convolutions. <em>IJCV</em>,
<em>129</em>(5), 1596–1615. (<a
href="https://doi.org/10.1007/s11263-021-01436-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism provides a sequential prediction framework for learning spatial models with enhanced implicit temporal consistency. In this work, we show a systematic design (from 2D to 3D) for how conventional networks and other forms of constraints can be incorporated into the attention framework for learning long-range dependencies for the task of pose estimation. The contribution of this paper is to provide a systematic approach for designing and training of attention-based models for the end-to-end pose estimation, with the flexibility and scalability of arbitrary video sequences as input. We achieve this by adapting temporal receptive field via a multi-scale structure of dilated convolutions. Besides, the proposed architecture can be easily adapted to a causal model enabling real-time performance. Any off-the-shelf 2D pose estimation systems, e.g. Our method achieves the state-of-the-art performance and outperforms existing methods by reducing the mean per joint position error to 33.4mm on Human 3.6M dataset. Our code is available at https://github.com/lrxjason/Attention3DHumanPose},
  archive      = {J_IJCV},
  author       = {Liu, Ruixu and Shen, Ju and Wang, He and Chen, Chen and Cheung, Sen-ching and Asari, Vijayan K.},
  doi          = {10.1007/s11263-021-01436-0},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1596-1615},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Enhanced 3D human pose estimation from videos by using attention-based neural network with dilated convolutions},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intra-camera supervised person re-identification.
<em>IJCV</em>, <em>129</em>(5), 1580–1595. (<a
href="https://doi.org/10.1007/s11263-021-01440-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person re-identification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness superiority of our method over the alternative approaches on three large person re-id datasets. For example, MATE yields 88.7\% rank-1 score on Market-1501 in the proposed ICS person re-id setting, significantly outperforming unsupervised learning models and closely approaching conventional fully supervised learning competitors.},
  archive      = {J_IJCV},
  author       = {Zhu, Xiangping and Zhu, Xiatian and Li, Minxian and Morerio, Pietro and Murino, Vittorio and Gong, Shaogang},
  doi          = {10.1007/s11263-021-01440-4},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1580-1595},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Intra-camera supervised person re-identification},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EfficientPS: Efficient panoptic segmentation. <em>IJCV</em>,
<em>129</em>(5), 1551–1579. (<a
href="https://doi.org/10.1007/s11263-021-01445-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.},
  archive      = {J_IJCV},
  author       = {Mohan, Rohit and Valada, Abhinav},
  doi          = {10.1007/s11263-021-01445-z},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1551-1579},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {EfficientPS: Efficient panoptic segmentation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual interestingness prediction: A benchmark framework and
literature review. <em>IJCV</em>, <em>129</em>(5), 1526–1550. (<a
href="https://doi.org/10.1007/s11263-021-01443-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we report on the creation of a publicly available, common evaluation framework for image and video visual interestingness prediction. We propose a robust data set, the Interestingness10k, with 9831 images and more than 4 h of video, interestigness scores determined based on more than 1M pair-wise annotations of 800 trusted annotators, some pre-computed multi-modal descriptors, and 192 system output results as baselines. The data were validated extensively during the 2016–2017 MediaEval benchmark campaigns. We provide an in-depth analysis of the crucial components of visual interestingness prediction algorithms by reviewing the capabilities and the evolution of the MediaEval benchmark systems, as well as of prominent systems from the literature. We discuss overall trends, influence of the employed features and techniques, generalization capabilities and the reliability of results. We also discuss the possibility of going beyond state-of-the-art performance via an automatic, ad-hoc system fusion, and propose a deep MLP-based architecture that outperforms the current state-of-the-art systems by a large margin. Finally, we provide the most important lessons learned and insights gained.},
  archive      = {J_IJCV},
  author       = {Constantin, Mihai Gabriel and Ştefan, Liviu-Daniel and Ionescu, Bogdan and Duong, Ngoc Q. K. and Demarty, Claire-Héléne and Sjöberg, Mats},
  doi          = {10.1007/s11263-021-01443-1},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1526-1550},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Visual interestingness prediction: A benchmark framework and literature review},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time semantic segmentation via auto depth, downsampling
joint decision and feature aggregation. <em>IJCV</em>, <em>129</em>(5),
1506–1525. (<a
href="https://doi.org/10.1007/s11263-021-01433-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To satisfy the stringent requirements for computational resources in the field of real-time semantic segmentation, most approaches focus on the hand-crafted design of light-weight segmentation networks. To enjoy the ability of model auto-design, Neural Architecture Search (NAS) has been introduced to search for the optimal building blocks of networks automatically. However, the network depth, downsampling strategy, and feature aggregation method are still set in advance and nonadjustable during searching. Moreover, these key properties are highly correlated and essential for a remarkable real-time segmentation model. In this paper, we propose a joint search framework, called AutoRTNet, to automate all the aforementioned key properties in semantic segmentation. Specifically, we propose hyper-cells to jointly decide the network depth and the downsampling strategy via a novel cell-level pruning process. Furthermore, we propose an aggregation cell to achieve automatic multi-scale feature aggregation. Extensive experimental results on Cityscapes and CamVid datasets demonstrate that the proposed AutoRTNet achieves the new state-of-the-art trade-off between accuracy and speed. Notably, our AutoRTNet achieves 73.9\% mIoU on Cityscapes and 110.0 FPS on an NVIDIA TitanXP GPU card with input images at a resolution of $$768 \times 1536$$ .},
  archive      = {J_IJCV},
  author       = {Sun, Peng and Wu, Jiaxiang and Li, Songyuan and Lin, Peiwen and Huang, Junzhou and Li, Xi},
  doi          = {10.1007/s11263-021-01433-3},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1506-1525},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Real-time semantic segmentation via auto depth, downsampling joint decision and feature aggregation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial–temporal relation reasoning for action prediction in
videos. <em>IJCV</em>, <em>129</em>(5), 1484–1505. (<a
href="https://doi.org/10.1007/s11263-020-01409-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action prediction in videos refers to inferring the action category label by an early observation of a video. Existing studies mainly focus on exploiting multiple visual cues to enhance the discriminative power of feature representation while neglecting important structure information in videos including interactions and correlations between different object entities. In this paper, we focus on reasoning about the spatial–temporal relations between persons and contextual objects to interpret the observed video part for predicting action categories. With this in mind, we propose a novel spatial–temporal relation reasoning approach that extracts the spatial relations between persons and objects in still frames and explores how these spatial relations change over time. Specifically, for spatial relation reasoning, we propose an improved gated graph neural network to perform spatial relation reasoning between the visual objects in video frames. For temporal relation reasoning, we propose a long short-term graph network to model both the short-term and long-term varying dynamics of the spatial relations with multi-scale receptive fields. By this means, our approach can accurately recognize the video content in terms of fine-grained object relations in both spatial and temporal domains to make prediction decisions. Moreover, in order to learn the latent correlations between spatial–temporal object relations and action categories in videos, a visual semantic relation loss is proposed to model the triple constraints between objects in semantic domain via VTransE. Extensive experiments on five public video datasets (i.e., 20BN-something-something, CAD120, UCF101, BIT-Interaction and HMDB51) demonstrate the effectiveness of the proposed spatial–temporal relation reasoning on action prediction.},
  archive      = {J_IJCV},
  author       = {Wu, Xinxiao and Wang, Ruiqi and Hou, Jingyi and Lin, Hanxi and Luo, Jiebo},
  doi          = {10.1007/s11263-020-01409-9},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1484-1505},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatial–Temporal relation reasoning for action prediction in videos},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LAMP-HQ: A large-scale multi-pose high-quality database and
benchmark for NIR-VIS face recognition. <em>IJCV</em>, <em>129</em>(5),
1467–1483. (<a
href="https://doi.org/10.1007/s11263-021-01432-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-infrared-visible (NIR-VIS) heterogeneous face recognition matches NIR to corresponding VIS face images. However, due to the sensing gap, NIR images often lose some identity information so that the NIR-VIS recognition issue is more difficult than conventional VIS face recognition. Recently, NIR-VIS heterogeneous face recognition has attracted considerable attention in the computer vision community because of its convenience and adaptability in practical applications. Various deep learning-based methods have been proposed and substantially increased the recognition performance, but the lack of NIR-VIS training samples leads to the difficulty of the model training process. In this paper, we propose a new $$\mathbf{L} {} \mathbf{a} $$ rge-Scale $$\mathbf{M} $$ ulti- $$\mathbf{P} $$ ose $$\mathbf{H} $$ igh- $$\mathbf{Q} $$ uality NIR-VIS database ‘ $$\mathbf{LAMP}-HQ $$ ’ containing 56,788 NIR and 16,828 VIS images of 573 subjects with large diversities in pose, illumination, attribute, scene and accessory. We furnish a benchmark along with the protocol for NIR-VIS face recognition via generation on LAMP-HQ, including Pixel2-Pixel, CycleGAN, ADFL, PCFH, and PACH. Furthermore, we propose a novel exemplar-based variational spectral attention network to produce high-fidelity VIS images from NIR data. A spectral conditional attention module is introduced to reduce the domain gap between NIR and VIS data and then improve the performance of NIR-VIS heterogeneous face recognition on various databases including the LAMP-HQ.},
  archive      = {J_IJCV},
  author       = {Yu, Aijing and Wu, Haoxue and Huang, Huaibo and Lei, Zhen and He, Ran},
  doi          = {10.1007/s11263-021-01432-4},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1467-1483},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LAMP-HQ: A large-scale multi-pose high-quality database and benchmark for NIR-VIS face recognition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic hierarchy emerges in deep generative
representations for scene synthesis. <em>IJCV</em>, <em>129</em>(5),
1451–1466. (<a
href="https://doi.org/10.1007/s11263-020-01429-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success of Generative Adversarial Networks (GANs) in synthesizing images, there lacks enough understanding of how photo-realistic images are generated from the layer-wise stochastic latent codes introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges in the deep generative representations from the state-of-the-art GANs like StyleGAN and BigGAN, trained for scene synthesis. By probing the per-layer representation with a broad set of semantics at different abstraction levels, we manage to quantify the causality between the layer-wise activations and the semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors that can be further used to steer the generation process, such as changing the lighting condition and varying the viewpoint of the scene. Extensive qualitative and quantitative results suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize various concepts in a hierarchical manner: the early layers tend to determine the spatial layout, the middle layers control the categorical objects, and the later layers render the scene attributes as well as the color scheme. Identifying such a set of steerable variation factors facilitates high-fidelity scene editing based on well-learned GAN models without any retraining (code and demo video are available at https://genforce.github.io/higan ).},
  archive      = {J_IJCV},
  author       = {Yang, Ceyuan and Shen, Yujun and Zhou, Bolei},
  doi          = {10.1007/s11263-020-01429-5},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1451-1466},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic hierarchy emerges in deep generative representations for scene synthesis},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACTNET: End-to-end learning of feature activations and
multi-stream aggregation for effective instance image retrieval.
<em>IJCV</em>, <em>129</em>(5), 1432–1450. (<a
href="https://doi.org/10.1007/s11263-021-01444-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel CNN architecture called ACTNET for robust instance image retrieval from large-scale datasets. Our key innovation is a learnable activation layer designed to improve the signal-to-noise ratio of deep convolutional feature maps. Further, we introduce a controlled multi-stream aggregation, where complementary deep features from different convolutional layers are optimally transformed and balanced using our novel activation layers, before aggregation into a global descriptor. Importantly, the learnable parameters of our activation blocks are explicitly trained, together with the CNN parameters, in an end-to-end manner minimising triplet loss. This means that our network jointly learns the CNN filters and their optimal activation and aggregation for retrieval tasks. To our knowledge, this is the first time parametric functions have been used to control and learn optimal multi-stream aggregation. We conduct an in-depth experimental study on three non-linear activation functions: Sine-Hyperbolic, Exponential and modified Weibull, showing that while all bring significant gains the Weibull function performs best thanks to its ability to equalise strong activations. The results clearly demonstrate that our ACTNET architecture significantly enhances the discriminative power of deep features, improving significantly over the state-of-the-art retrieval results on all datasets.},
  archive      = {J_IJCV},
  author       = {Husain, Syed Sameed and Ong, Eng-Jon and Bober, Miroslaw},
  doi          = {10.1007/s11263-021-01444-0},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1432-1450},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ACTNET: End-to-end learning of feature activations and multi-stream aggregation for effective instance image retrieval},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manhattan room layout reconstruction from a single <span
class="math display">360<sup>∘</sup></span> image: A comparative study
of state-of-the-art methods. <em>IJCV</em>, <em>129</em>(5), 1410–1431.
(<a href="https://doi.org/10.1007/s11263-020-01426-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent approaches for predicting layouts from 360 $$^{\circ }$$ panoramas produce excellent results. These approaches build on a common framework consisting of three steps: a pre-processing step based on edge-based alignment, prediction of layout elements, and a post-processing step by fitting a 3D layout to the layout elements. Until now, it has been difficult to compare the methods due to multiple different design decisions, such as the encoding network (e.g., SegNet or ResNet), type of elements predicted (e.g., corners, wall/floor boundaries, or semantic segmentation), or method of fitting the 3D layout. To address this challenge, we summarize and describe the common framework, the variants, and the impact of the design decisions. For a complete evaluation, we also propose extended annotations for the Matterport3D dataset (Chang et al.: Matterport3d: learning from rgb-d data in indoor environments. arXiv:1709.06158 , 2017), and introduce two depth-based evaluation metrics.},
  archive      = {J_IJCV},
  author       = {Zou, Chuhang and Su, Jheng-Wei and Peng, Chi-Han and Colburn, Alex and Shan, Qi and Wonka, Peter and Chu, Hung-Kuo and Hoiem, Derek},
  doi          = {10.1007/s11263-020-01426-8},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1410-1431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Manhattan room layout reconstruction from a single $$360^{\circ }$$ image: A comparative study of state-of-the-art methods},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Letter-level online writer identification. <em>IJCV</em>,
<em>129</em>(5), 1394–1409. (<a
href="https://doi.org/10.1007/s11263-020-01414-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writer identification (writer-id), an important field in biometrics, aims to identify a writer by their handwriting. Identification in existing writer-id studies requires a complete document or text, limiting the scalability and flexibility of writer-id in realistic applications. To make the application of writer-id more practical (e.g., on mobile devices), we focus on a novel problem, letter-level online writer-id, which requires only a few trajectories of written letters as identification cues. Unlike text- $$\backslash $$ document-based writer-id which has rich context for identification, there are much fewer clues to recognize an author from only a few single letters. A main challenge is that a person often writes a letter in different styles from time to time. We refer to this problem as the variance of online writing styles (Var-O-Styles). We address the Var-O-Styles in a capture-normalize-aggregate fashion: Firstly, we extract different features of a letter trajectory by a carefully designed multi-branch encoder, in an attempt to capture different online writing styles. Then we convert all these style features to a reference style feature domain by a novel normalization layer. Finally, we aggregate the normalized features by a hierarchical attention pooling (HAP), which fuses all the input letters with multiple writing styles into a compact feature vector. In addition, we also contribute a large-scale LEtter-level online wRiter IDentification dataset (LERID) for evaluation. Extensive comparative experiments demonstrate the effectiveness of the proposed framework.},
  archive      = {J_IJCV},
  author       = {Chen, Zelin and Yu, Hong-Xing and Wu, Ancong and Zheng, Wei-Shi},
  doi          = {10.1007/s11263-020-01414-y},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1394-1409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Letter-level online writer identification},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards balanced learning for instance recognition.
<em>IJCV</em>, <em>129</em>(5), 1376–1393. (<a
href="https://doi.org/10.1007/s11263-021-01434-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance recognition is rapidly advanced along with the developments of deep convolutional neural networks. Compared to the model architectures the training process, which is also crucial to the success of detectors, has received relatively less attention. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels—sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple yet effective framework towards balanced learning for instance recognition. It integrates IoU-balanced sampling, balanced feature pyramid, and objective re-weighting, respectively for reducing the imbalance at sample, feature, and objective level. Extensive experiments conducted on MS COCO, LVIS and Pascal VOC datasets prove the effectiveness of the overall balanced design.},
  archive      = {J_IJCV},
  author       = {Pang, Jiangmiao and Chen, Kai and Li, Qi and Xu, Zhihai and Feng, Huajun and Shi, Jianping and Ouyang, Wanli and Lin, Dahua},
  doi          = {10.1007/s11263-021-01434-2},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1376-1393},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards balanced learning for instance recognition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive channel selection for robust visual object tracking
with discriminative correlation filters. <em>IJCV</em>, <em>129</em>(5),
1359–1375. (<a
href="https://doi.org/10.1007/s11263-021-01435-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative Correlation Filters (DCF) have been shown to achieve impressive performance in visual object tracking. However, existing DCF-based trackers rely heavily on learning regularised appearance models from invariant image feature representations. To further improve the performance of DCF in accuracy and provide a parsimonious model from the attribute perspective, we propose to gauge the relevance of multi-channel features for the purpose of channel selection. This is achieved by assessing the information conveyed by the features of each channel as a group, using an adaptive group elastic net inducing independent sparsity and temporal smoothness on the DCF solution. The robustness and stability of the learned appearance model are significantly enhanced by the proposed method as the process of channel selection performs implicit spatial regularisation. We use the augmented Lagrangian method to optimise the discriminative filters efficiently. The experimental results obtained on a number of well-known benchmarking datasets demonstrate the effectiveness and stability of the proposed method. A superior performance over the state-of-the-art trackers is achieved using less than $$10\%$$ deep feature channels.},
  archive      = {J_IJCV},
  author       = {Xu, Tianyang and Feng, Zhenhua and Wu, Xiao-Jun and Kittler, Josef},
  doi          = {10.1007/s11263-021-01435-1},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1359-1375},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive channel selection for robust visual object tracking with discriminative correlation filters},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deformable image registration based on functions of bounded
generalized deformation. <em>IJCV</em>, <em>129</em>(5), 1341–1358. (<a
href="https://doi.org/10.1007/s11263-021-01439-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functions of bounded deformation (BD) are widely used in the theory of elastoplasticity to describe the possibly discontinuous displacement fields inside elastoplastic bodies. BD functions have been proved suitable for deformable image registration, the goal of which is to find the displacement field between a moving image and a fixed image. Recently BD functions have been generalized to symmetric tensor fields of bounded generalized variation. In this paper, we focus on the first-order symmetric tensor fields, i.e., vector-valued functions, of bounded generalized variation. We specify these functions as functions of bounded generalized deformation (BGD) since BGD functions are natural generalizations of BD functions. We propose a BGD model for deformable image registration problems by regarding concerned displacement fields as BGD functions. BGD model employs not only the first-order but also higher-order coupling information of components of the displacement field. It turns out that BGD model allows for jump discontinuities of displacements while, in contrast to BD model, at the same time is able to employ higher-order derivatives of displacements in smooth regions. As a result, BGD model tends to capture possible discontinuities of displacements appeared around edges of the target objects while keep the smoothness of displacements inside the target objects as well. This characteristic enables BGD model to obtain better registration results than BD model and other variational models. To our knowledge, it is the first time in literature to use BGD functions for image registration. A first-order adaptive primal–dual algorithm is adopted to solve the proposed BGD model. Numerical experiments on 2D and 3D images show both effectiveness and advantages of BGD model.},
  archive      = {J_IJCV},
  author       = {Nie, Ziwei and Li, Chen and Liu, Hairong and Yang, Xiaoping},
  doi          = {10.1007/s11263-021-01439-x},
  journal      = {International Journal of Computer Vision},
  number       = {5},
  pages        = {1341-1358},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deformable image registration based on functions of bounded generalized deformation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepFlux for skeleton detection in the wild. <em>IJCV</em>,
<em>129</em>(4), 1323–1339. (<a
href="https://doi.org/10.1007/s11263-021-01430-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The medial axis, or skeleton, is a fundamental object representation that has been extensively used in shape recognition. Yet, its extension to natural images has been challenging due to the large appearance and scale variations of objects and complex background clutter that appear in this setting. In contrast to recent methods that address skeleton extraction as a binary pixel classification problem, in this article we present an alternative formulation for skeleton detection. We follow the spirit of flux-based algorithms for medial axis recovery by training a convolutional neural network to predict a two-dimensional vector field encoding the flux representation. The skeleton is then recovered from the flux representation, which captures the position of skeletal pixels relative to semantically meaningful entities (e.g., image points in spatial context, and hence the implied object boundaries), resulting in precise skeleton detection. Moreover, since the flux representation is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method, termed DeepFlux, on six benchmark datasets, consistently achieving superior performance over state-of-the-art methods. Finally, we demonstrate an application of DeepFlux, augmented with a skeleton scale estimation module, to detect objects in aerial images. This combination yields results that are competitive with models trained specifically for object detection, showcasing the versatility and effectiveness of mid-level representations in high-level tasks. An implementation of our method is available at https://github.com/YukangWang/DeepFlux .},
  archive      = {J_IJCV},
  author       = {Xu, Yongchao and Wang, Yukang and Tsogkas, Stavros and Wan, Jianqiang and Bai, Xiang and Dickinson, Sven and Siddiqi, Kaleem},
  doi          = {10.1007/s11263-021-01430-6},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1323-1339},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DeepFlux for skeleton detection in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive benchmark analysis of single image
deraining: Current challenges and future perspectives. <em>IJCV</em>,
<em>129</em>(4), 1301–1322. (<a
href="https://doi.org/10.1007/s11263-020-01416-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of image deraining is a highly desirable component of intelligent decision-making in autonomous driving and outdoor surveillance systems. Image deraining aims to restore the clean scene from the degraded image captured in a rainy day. Although numerous single image deraining algorithms have been recently proposed, these algorithms are mainly evaluated using certain type of synthetic images, assuming a specific rain model, plus a few real images. It remains unclear how these algorithms would perform on rainy images acquired “in the wild” and how we could gauge the progress in the field. This paper aims to bridge this gap. We present a comprehensive study and evaluation of existing single image deraining algorithms, using a new large-scale benchmark consisting of both synthetic and real-world rainy images of various rain types. This dataset highlights diverse rain models (rain streak, rain drop, rain and mist), as well as a rich variety of evaluation criteria (full- and no-reference objective, subjective, and task-specific). We further provide a comprehensive suite of criteria for deraining algorithm evaluation, including full- and no-reference metrics, subjective evaluation, and the novel task-driven evaluation. The proposed benchmark is accompanied with extensive experimental results that facilitate the assessment of the state-of-the-arts on a quantitative basis. Our evaluation and analysis indicate the gap between the achievable performance on synthetic rainy images and the practical demand on real-world images. We show that, despite many advances, image deraining is still a largely open problem. The paper is concluded by summarizing our general observations, identifying open research challenges and pointing out future directions. Our code and dataset is publicly available at http://uee.me/ddQsw .},
  archive      = {J_IJCV},
  author       = {Li, Siyuan and Ren, Wenqi and Wang, Feng and Araujo, Iago Breno and Tokuda, Eric K. and Junior, Roberto Hirata and Cesar-Jr., Roberto M. and Wang, Zhangyang and Cao, Xiaochun},
  doi          = {10.1007/s11263-020-01416-w},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1301-1322},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive benchmark analysis of single image deraining: Current challenges and future perspectives},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selective wavelet attention learning for single image
deraining. <em>IJCV</em>, <em>129</em>(4), 1282–1300. (<a
href="https://doi.org/10.1007/s11263-020-01421-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image deraining refers to the process of restoring the clean background scene from a rainy image. Current approaches have resorted to deep learning techniques to remove rain from a single image by leveraging some prior information. However, due to the various appearances of rain streaks and accumulation, it is difficult to separate rain and background information in the embedding space, which results in inaccurate deraining. To address this issue, this paper proposes a selective wavelet attention learning method by learning a series of wavelet attention maps to guide the separation of rain and background information in both spatial and frequency domains. The key aspect of our method is utilizing wavelet transform to learn the content and structure of rainy features because the high-frequency features are more sensitive to rain degradations, whereas the low-frequency features preserve more of the background content. To begin with, we develop a selective wavelet attention encoder–decoder network to learn wavelet attention maps guiding the separation of rainy and background features at multiple scales. Meanwhile, we introduce wavelet pooling and unpooling to the encoder–decoder network, which shows superiority in learning increasingly abstract representations while preserving the background details. In addition, we propose latent alignment learning to supervise the background features as well as augment the training data to further improve the accuracy of deraining. Finally, we employ a hierarchical discriminator network based on selective wavelet attention to adversarially improve the visual fidelity of the generated results both globally and locally. Extensive experiments on synthetic and real datasets demonstrate that the proposed approach achieves more appealing results both quantitatively and qualitatively than the recent state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Huang, Huaibo and Yu, Aijing and Chai, Zhenhua and He, Ran and Tan, Tieniu},
  doi          = {10.1007/s11263-020-01421-z},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1282-1300},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Selective wavelet attention learning for single image deraining},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of full-reference image quality models for
optimization of image processing systems. <em>IJCV</em>,
<em>129</em>(4), 1258–1281. (<a
href="https://doi.org/10.1007/s11263-020-01419-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of objective image quality assessment (IQA) models has been evaluated primarily by comparing model predictions to human quality judgments. Perceptual datasets gathered for this purpose have provided useful benchmarks for improving IQA methods, but their heavy use creates a risk of overfitting. Here, we perform a large-scale comparison of IQA models in terms of their use as objectives for the optimization of image processing algorithms. Specifically, we use eleven full-reference IQA models to train deep neural networks for four low-level vision tasks: denoising, deblurring, super-resolution, and compression. Subjective testing on the optimized images allows us to rank the competing models in terms of their perceptual performance, elucidate their relative advantages and disadvantages in these tasks, and propose a set of desirable properties for incorporation into future IQA models.},
  archive      = {J_IJCV},
  author       = {Ding, Keyan and Ma, Kede and Wang, Shiqi and Simoncelli, Eero P.},
  doi          = {10.1007/s11263-020-01419-7},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1258-1281},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Comparison of full-reference image quality models for optimization of image processing systems},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified quality assessment of in-the-wild videos with mixed
datasets training. <em>IJCV</em>, <em>129</em>(4), 1238–1257. (<a
href="https://doi.org/10.1007/s11263-020-01408-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video quality assessment (VQA) is an important problem in computer vision. The videos in computer vision applications are usually captured in the wild. We focus on automatically assessing the quality of in-the-wild videos, which is a challenging problem due to the absence of reference videos, the complexity of distortions, and the diversity of video contents. Moreover, the video contents and distortions among existing datasets are quite different, which leads to poor performance of data-driven methods in the cross-dataset evaluation setting. To improve the performance of quality assessment models, we borrow intuitions from human perception, specifically, content dependency and temporal-memory effects of human visual system. To face the cross-dataset evaluation challenge, we explore a mixed datasets training strategy for training a single VQA model with multiple datasets. The proposed unified framework explicitly includes three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment, to jointly predict relative quality, perceptual quality, and subjective quality. Experiments are conducted on four publicly available datasets for VQA in the wild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental results verify the effectiveness of the mixed datasets training strategy and prove the superior performance of the unified model in comparison with the state-of-the-art models. For reproducible research, we make the PyTorch implementation of our method available at https://github.com/lidq92/MDTVSFA .},
  archive      = {J_IJCV},
  author       = {Li, Dingquan and Jiang, Tingting and Jiang, Ming},
  doi          = {10.1007/s11263-020-01408-w},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1238-1257},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unified quality assessment of in-the-wild videos with mixed datasets training},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complete singularity analysis for the perspective-four-point
problem. <em>IJCV</em>, <em>129</em>(4), 1217–1237. (<a
href="https://doi.org/10.1007/s11263-020-01420-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with pose estimation and visual servoing from four points. We determine the configurations for which the corresponding Jacobian matrix becomes singular, leading to inaccurate and unstable results. Using an adequate representation and algebraic geometry, it is shown that, for any orientation between the camera and the object, there are always two to six singular locations of the camera in the generic case where the points are not coplanar, corresponding to the intersection of four cylinders. The particular case where the four points are coplanar is also characterized. Furthermore, some realistic example configurations are considered to substantiate the theory and to demonstrate failure cases in pose estimation and image-based visual servoing when the camera approaches a singularity.},
  archive      = {J_IJCV},
  author       = {Pascual-Escudero, Beatriz and Nayak, Abhilash and Briot, Sébastien and Kermorgant, Olivier and Martinet, Philippe and El Din, Mohab Safey and Chaumette, François},
  doi          = {10.1007/s11263-020-01420-0},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1217-1237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Complete singularity analysis for the perspective-four-point problem},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental rotation averaging. <em>IJCV</em>,
<em>129</em>(4), 1202–1216. (<a
href="https://doi.org/10.1007/s11263-020-01427-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a simple yet effective rotation averaging pipeline, termed Incremental Rotation Averaging (IRA), which is inspired by the well-developed incremental Structure from Motion (SfM) techniques. Unlike the traditional rotation averaging methods which estimate all the absolute rotations simultaneously and focus on designing either robust loss function or outlier filtering strategy, here the absolute rotations are estimated in an incremental way. Similar to the incremental SfM, our IRA is robust to relative rotation outliers and could achieve accurate rotation averaging results. In addition, we propose several key techniques, such as initial triplet and Next-Best-View selection, Weighted Local/Global Optimization, and Re-Rotation Averaging, to push the rotation averaging results one step further. Ablation studies and comparison experiments on the 1DSfM, Campus, and San Francisco datasets demonstrate the effectiveness of our IRA and its advantages over the state-of-the-art rotation averaging methods in accuracy and robustness.},
  archive      = {J_IJCV},
  author       = {Gao, Xiang and Zhu, Lingjie and Xie, Zexiao and Liu, Hongmin and Shen, Shuhan},
  doi          = {10.1007/s11263-020-01427-7},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1202-1216},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Incremental rotation averaging},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Label-free robustness estimation of object detection CNNs
for autonomous driving applications. <em>IJCV</em>, <em>129</em>(4),
1185–1201. (<a
href="https://doi.org/10.1007/s11263-020-01423-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Convolutional Neural Networks (CNNs) has led to its increased application in several domains. One noteworthy application is the perception system for autonomous driving that rely on the predictions from CNNs. On one hand, predicting the learned objects with maximum accuracy is of importance. On the other hand, it is still a challenge to evaluate the reliability of CNN-based perception systems without ground truth information. Such evaluations are of significance for autonomous driving applications. One way to estimate reliability is by evaluating robustness of the detections in the presence of artificial perturbations. However, several existing works on perturbation-based robustness quantification rely on the ground truth labels. Acquiring the ground truth labels is a tedious, expensive and error-prone process. In this work we propose a novel label-free robustness metric for quantifying the robustness of CNN object detectors. We quantify the robustness of the detections to a specific type of input perturbation based on the prediction confidences. In short, we check the sensitivity of the predicted confidences under increased levels of artificial perturbation. Thereby, we avoid the need for ground truth annotations. We perform extensive evaluations on our traffic light detector from autonomous driving applications and on public object detection networks and datasets. The evaluations show that our label-free metric is comparable to the ground truth aided robustness scoring.},
  archive      = {J_IJCV},
  author       = {Shekar, Arvind Kumar and Gou, Liang and Ren, Liu and Wendt, Axel},
  doi          = {10.1007/s11263-020-01423-x},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1185-1201},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Label-free robustness estimation of object detection CNNs for autonomous driving applications},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking low-light image enhancement and beyond.
<em>IJCV</em>, <em>129</em>(4), 1153–1184. (<a
href="https://doi.org/10.1007/s11263-020-01418-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a systematic review and evaluation of existing single-image low-light enhancement algorithms. Besides the commonly used low-level vision oriented evaluations, we additionally consider measuring machine vision performance in the low-light condition via face detection task to explore the potential of joint optimization of high-level and low-level vision enhancement. To this end, we first propose a large-scale low-light image dataset serving both low/high-level vision with diversified scenes and contents as well as complex degradation in real scenarios, called Vision Enhancement in the LOw-Light condition (VE-LOL). Beyond paired low/normal-light images without annotations, we additionally include the analysis resource related to human, i.e. face images in the low-light condition with annotated face bounding boxes. Then, efforts are made on benchmarking from the perspective of both human and machine visions. A rich variety of criteria is used for the low-level vision evaluation, including full-reference, no-reference, and semantic similarity metrics. We also measure the effects of the low-light enhancement on face detection in the low-light condition. State-of-the-art face detection methods are used in the evaluation. Furthermore, with the rich material of VE-LOL, we explore the novel problem of joint low-light enhancement and face detection. We develop an enhanced face detector to apply low-light enhancement and face detection jointly. The features extracted by the enhancement module are fed to the successive layer with the same resolution of the detection module. Thus, these features are intertwined together to unitedly learn useful information across two phases, i.e. enhancement and detection. Experiments on VE-LOL provide a comparison of state-of-the-art low-light enhancement algorithms, point out their limitations, and suggest promising future directions. Our dataset has supported the Track “Face Detection in Low Light Conditions” of CVPR UG2+ Challenge (2019–2020) ( http://cvpr2020.ug2challenge.org/ ).},
  archive      = {J_IJCV},
  author       = {Liu, Jiaying and Xu, Dejia and Yang, Wenhan and Fan, Minhao and Huang, Haofeng},
  doi          = {10.1007/s11263-020-01418-8},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1153-1184},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Benchmarking low-light image enhancement and beyond},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Excitation dropout: Encouraging plasticity in deep neural
networks. <em>IJCV</em>, <em>129</em>(4), 1139–1152. (<a
href="https://doi.org/10.1007/s11263-020-01422-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a guided dropout regularizer for deep networks based on the evidence of a network prediction defined as the firing of neurons in specific paths. In this work, we utilize the evidence at each neuron to determine the probability of dropout, rather than dropping out neurons uniformly at random as in standard dropout. In essence, we dropout with higher probability those neurons which contribute more to decision making at training time. This approach penalizes high saliency neurons that are most relevant for model prediction, i.e. those having stronger evidence. By dropping such high-saliency neurons, the network is forced to learn alternative paths in order to maintain loss minimization, resulting in a plasticity-like behavior, a characteristic of human brains too. We demonstrate better generalization ability, an increased utilization of network neurons, and a higher resilience to network compression using several metrics over four image/video recognition benchmarks.},
  archive      = {J_IJCV},
  author       = {Zunino, Andrea and Bargal, Sarah Adel and Morerio, Pietro and Zhang, Jianming and Sclaroff, Stan and Murino, Vittorio},
  doi          = {10.1007/s11263-020-01422-y},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1139-1152},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Excitation dropout: Encouraging plasticity in deep neural networks},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A shape transformation-based dataset augmentation framework
for pedestrian detection. <em>IJCV</em>, <em>129</em>(4), 1121–1138. (<a
href="https://doi.org/10.1007/s11263-020-01412-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based computer vision is usually data-hungry. Many researchers attempt to augment datasets with synthesized data to improve model robustness. However, the augmentation of popular pedestrian datasets, such as Caltech and Citypersons, can be extremely challenging because real pedestrians are commonly in low quality. Due to the factors like occlusions, blurs, and low-resolution, it is significantly difficult for existing augmentation approaches, which generally synthesize data using 3D engines or generative adversarial networks (GANs), to generate realistic-looking pedestrians. Alternatively, to access much more natural-looking pedestrians, we propose to augment pedestrian detection datasets by transforming real pedestrians from the same dataset into different shapes. Accordingly, we propose the Shape Transformation-based Dataset Augmentation (STDA) framework. The proposed framework is composed of two subsequent modules, i.e.  the shape-guided deformation and the environment adaptation. In the first module, we introduce a shape-guided warping field to help deform the shape of a real pedestrian into a different shape. Then, in the second stage, we propose an environment-aware blending map to better adapt the deformed pedestrians into surrounding environments, obtaining more realistic-looking pedestrians and more beneficial augmentation results for pedestrian detection. Extensive empirical studies on different pedestrian detection benchmarks show that the proposed STDA framework consistently produces much better augmentation results than other pedestrian synthesis approaches using low-quality pedestrians. By augmenting the original datasets, our proposed framework also improves the baseline pedestrian detector by up to 38\% on the evaluated benchmarks, achieving state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Chen, Zhe and Ouyang, Wanli and Liu, Tongliang and Tao, Dacheng},
  doi          = {10.1007/s11263-020-01412-0},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1121-1138},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A shape transformation-based dataset augmentation framework for pedestrian detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rectifying pseudo label learning via uncertainty estimation
for domain adaptive semantic segmentation. <em>IJCV</em>,
<em>129</em>(4), 1106–1120. (<a
href="https://doi.org/10.1007/s11263-020-01395-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 $$\rightarrow $$ Cityscapes and SYNTHIA $$\rightarrow $$ Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes $$\rightarrow $$ Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.},
  archive      = {J_IJCV},
  author       = {Zheng, Zhedong and Yang, Yi},
  doi          = {10.1007/s11263-020-01395-y},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1106-1120},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoDet: Pyramid network architecture search for object
detection. <em>IJCV</em>, <em>129</em>(4), 1087–1105. (<a
href="https://doi.org/10.1007/s11263-020-01415-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature pyramids have delivered significant improvement in object detection. However, building effective feature pyramids heavily relies on expert knowledge, and also requires strenuous efforts to balance effectiveness and efficiency. Automatic search methods, such as NAS-FPN, automates the design of feature pyramids, but the low search efficiency makes it difficult to apply in a large search space. In this paper, we propose a novel search framework for a feature pyramid network, called AutoDet, which enables to automatic discovery of informative connections between multi-scale features and configure detection architectures with both high efficiency and state-of-the-art performance. In AutoDet, a new search space is specifically designed for feature pyramids in object detectors, which is more general than NAS-FPN. Furthermore, the architecture search process is formulated as a combinatorial optimization problem and solved by a Simulated Annealing-based Network Architecture Search method (SA-NAS). Compared with existing NAS methods, AutoDet ensures a dramatic reduction in search times. For example, our SA-NAS can be up to 30x faster than reinforcement learning-based approaches. Furthermore, AutoDet is compatible with both one-stage and two-stage structures with all kinds of backbone networks. We demonstrate the effectiveness of AutoDet with outperforming single-model results on the COCO dataset. Without pre-training on OpenImages, AutoDet with the ResNet-101 backbone achieves an AP of 39.7 and 47.3 for one-stage and two-stage architectures, respectively, which surpass current state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Li, Zhihang and Xi, Teng and Zhang, Gang and Liu, Jingtuo and He, Ran},
  doi          = {10.1007/s11263-020-01415-x},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1087-1105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AutoDet: Pyramid network architecture search for object detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Parallel single-pixel imaging: A general method for
direct–global separation and 3D shape reconstruction under strong global
illumination. <em>IJCV</em>, <em>129</em>(4), 1060–1086. (<a
href="https://doi.org/10.1007/s11263-020-01413-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present parallel single-pixel imaging (PSI), a photography technique that captures light transport coefficients and enables the separation of direct and global illumination, to achieve 3D shape reconstruction under strong global illumination. PSI is achieved by extending single-pixel imaging (SI) to modern digital cameras. Each pixel on an imaging sensor is considered an independent unit that can obtain an image using the SI technique. The obtained images characterize the light transport behavior between pixels on the projector and the camera. However, the required number of SI illumination patterns generally becomes unacceptably large in practical situations. We introduce local region extension (LRE) method to accelerate the data acquisition of PSI. LRE perceives that the visible region of each camera pixel accounts for a local region. Thus, the number of detected unknowns is determined by local region area, which is extremely beneficial in terms of data acquisition efficiency. PSI possesses several properties and advantages. For instance, PSI captures the complete light transport coefficients between the projector–camera pair, without making specific assumptions on measured objects and without requiring special hardware and restrictions on the arrangement of the projector–camera pair. The perfect reconstruction property of LRE can be proven mathematically. The acquisition and reconstruction stages are straightforward and easy to implement in the existing projector–camera systems. These properties and advantages make PSI a general and sound theoretical model to decompose direct and global illuminations and perform 3D shape reconstruction under global illumination.},
  archive      = {J_IJCV},
  author       = {Jiang, Hongzhi and Li, Yuxi and Zhao, Huijie and Li, Xudong and Xu, Yang},
  doi          = {10.1007/s11263-020-01413-z},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1060-1086},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Parallel single-pixel imaging: A general method for Direct–Global separation and 3D shape reconstruction under strong global illumination},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The MVTec anomaly detection dataset: A comprehensive
real-world dataset for unsupervised anomaly detection. <em>IJCV</em>,
<em>129</em>(4), 1038–1059. (<a
href="https://doi.org/10.1007/s11263-020-01400-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the field of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec anomaly detection dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth annotations for all anomalies. We conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pretrained convolutional neural networks, as well as classical computer vision methods. We highlight the advantages and disadvantages of multiple performance metrics as well as threshold estimation techniques. This benchmark indicates that methods that leverage descriptors of pretrained networks outperform all other approaches and deep-learning-based generative models show considerable room for improvement.},
  archive      = {J_IJCV},
  author       = {Bergmann, Paul and Batzner, Kilian and Fauser, Michael and Sattlegger, David and Steger, Carsten},
  doi          = {10.1007/s11263-020-01400-4},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1038-1059},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {The MVTec anomaly detection dataset: A comprehensive real-world dataset for unsupervised anomaly detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond brightening low-light images. <em>IJCV</em>,
<em>129</em>(4), 1013–1037. (<a
href="https://doi.org/10.1007/s11263-020-01407-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradation, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify pollution. Thus, low-light image enhancement should not only brighten dark regions, but also remove hidden artifacts. To achieve the goal, this work builds a simple yet effective network, which, inspired by Retinex theory, decomposes images into two components. Following a divide-and-conquer principle, one component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting for better regularization/learning. It is worth noticing that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over the state-of-the-art alternatives, especially in terms of the robustness against severe visual defects and the flexibility in adjusting light levels. Our code is made publicly available at https://github.com/zhangyhuaee/KinD_plus .},
  archive      = {J_IJCV},
  author       = {Zhang, Yonghua and Guo, Xiaojie and Ma, Jiayi and Liu, Wei and Zhang, Jiawan},
  doi          = {10.1007/s11263-020-01407-x},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {1013-1037},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Beyond brightening low-light images},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rectified binary convolutional networks with generative
adversarial learning. <em>IJCV</em>, <em>129</em>(4), 998–1012. (<a
href="https://doi.org/10.1007/s11263-020-01417-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binarized convolutional neural networks (BNNs) are widely used to improve the memory and computational efficiency of deep convolutional neural networks for to be employed on embedded devices. However, existing BNNs fail to explore their corresponding full-precision models’ potential, resulting in a significant performance gap. This paper introduces a Rectified Binary Convolutional Network (RBCN) by combining full precision kernels and feature maps to rectify the binarization process in a generative adversarial network (GAN) framework. We further prune our RBCNs using the GAN framework to increase the model efficiency and promote flexibly in practical applications. Extensive experiments validate the superior performance of the proposed RBCN over state-of-the-art BNNs on tasks such as object classification, object tracking, face recognition, and person re-identification.},
  archive      = {J_IJCV},
  author       = {Liu, Chunlei and Ding, Wenrui and Hu, Yuan and Zhang, Baochang and Liu, Jianzhuang and Guo, Guodong and Doermann, David},
  doi          = {10.1007/s11263-020-01417-9},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {998-1012},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rectified binary convolutional networks with generative adversarial learning},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of inpainting and augmentation for censored image
queries. <em>IJCV</em>, <em>129</em>(4), 977–997. (<a
href="https://doi.org/10.1007/s11263-020-01403-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images can be censored by masking the region(s) of interest with a solid color or pattern. When a censored image is used for classification or matching, the mask itself may impact the results. Recent work in image inpainting and data augmentation provide two different approaches for dealing with censored images. In this paper, we perform an extensive evaluation of these methods to understand if the impact of censoring can be mitigated for image classification and retrieval. Results indicate that modern learning-based inpainting approaches outperform augmentation strategies and that metrics typically used to evaluate inpainting performance (e.g., reconstruction accuracy) do not necessarily correspond to improved classification or retrieval, especially in the case of person-shaped masked regions.},
  archive      = {J_IJCV},
  author       = {Black, Samuel and Keshavarz, Somayeh and Souvenir, Richard},
  doi          = {10.1007/s11263-020-01403-1},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {977-997},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Evaluation of inpainting and augmentation for censored image queries},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Separating content from style using adversarial learning for
recognizing text in the wild. <em>IJCV</em>, <em>129</em>(4), 960–976.
(<a href="https://doi.org/10.1007/s11263-020-01411-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition is an important task in computer vision. Despite tremendous progress achieved in the past few years, issues such as varying font styles, arbitrary shapes and complex backgrounds etc. have made the problem very challenging. In this work, we propose to improve text recognition from a new perspective by separating the text content from complex backgrounds, thus making the recognition considerably easier and significantly improving recognition accuracy. To this end, we exploit the generative adversarial networks (GANs) for removing backgrounds while retaining the text content . As vanilla GANs are not sufficiently robust to generate sequence-like characters in natural images, we propose an adversarial learning framework for the generation and recognition of multiple characters in an image. The proposed framework consists of an attention-based recognizer and a generative adversarial architecture. Furthermore, to tackle the issue of lacking paired training samples, we design an interactive joint training scheme, which shares attention masks from the recognizer to the discriminator, and enables the discriminator to extract the features of each character for further adversarial training. Benefiting from the character-level adversarial training, our framework requires only unpaired simple data for style supervision. Each target style sample containing only one randomly chosen character can be simply synthesized online during the training. This is significant as the training does not require costly paired samples or character-level annotations. Thus, only the input images and corresponding text labels are needed. In addition to the style normalization of the backgrounds, we refine character patterns to ease the recognition task. A feedback mechanism is proposed to bridge the gap between the discriminator and the recognizer. Therefore, the discriminator can guide the generator according to the confusion of the recognizer, so that the generated patterns are clearer for recognition. Experiments on various benchmarks, including both regular and irregular text, demonstrate that our method significantly reduces the difficulty of recognition. Our framework can be integrated into recent recognition methods to achieve new state-of-the-art recognition accuracy.},
  archive      = {J_IJCV},
  author       = {Luo, Canjie and Lin, Qingxiang and Liu, Yuliang and Jin, Lianwen and Shen, Chunhua},
  doi          = {10.1007/s11263-020-01411-1},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {960-976},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Separating content from style using adversarial learning for recognizing text in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adding knowledge to unsupervised algorithms for the
recognition of intent. <em>IJCV</em>, <em>129</em>(4), 942–959. (<a
href="https://doi.org/10.1007/s11263-020-01404-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision algorithms performance are near or superior to humans in the visual problems including object recognition (especially those of fine-grained categories), segmentation, and 3D object reconstruction from 2D views. Humans are, however, capable of higher-level image analyses. A clear example, involving theory of mind, is our ability to determine whether a perceived behavior or action was performed intentionally or not. In this paper, we derive an algorithm that can infer whether the behavior of an agent in a scene is intentional or unintentional based on its 3D kinematics, using the knowledge of self-propelled motion, Newtonian motion and their relationship. We show how the addition of this basic knowledge leads to a simple, unsupervised algorithm. To test the derived algorithm, we constructed three dedicated datasets from abstract geometric animation to realistic videos of agents performing intentional and non-intentional actions. Experiments on these datasets show that our algorithm can recognize whether an action is intentional or not, even without training data. The performance is comparable to various supervised baselines quantitatively, with sensible intentionality segmentation qualitatively.},
  archive      = {J_IJCV},
  author       = {Synakowski, Stuart and Feng, Qianli and Martinez, Aleix},
  doi          = {10.1007/s11263-020-01404-0},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {942-959},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adding knowledge to unsupervised algorithms for the recognition of intent},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical visual-textual knowledge distillation for
life-long correlation learning. <em>IJCV</em>, <em>129</em>(4), 921–941.
(<a href="https://doi.org/10.1007/s11263-020-01392-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation learning among different types of multimedia data, such as visual and textual content, faces huge challenges from two important perspectives, namely, cross modal and cross domain. Cross modal means the heterogeneous properties of different types of multimedia data, where the data from different modalities have inconsistent distributions and representations. This situation leads to the first challenge: cross-modal similarity measurement. Cross domain means the multisource property of multimedia data from various domains, in which data from new domains arrive continually, leading to the second challenge: model storage and retraining. Therefore, correlation learning requires a cross-modal continual learning approach, in which only the data from the new domains are used for training, but the previously learned correlation capabilities are preserved. To address the above issues, we introduce the idea of life-long learning into visual-textual cross-modal correlation modeling and propose a visual-textual life-long knowledge distillation (VLKD) approach. In this study, we construct a hierarchical recurrent network that can leverage knowledge from both semantic and attention levels through adaptive network expansion to support cross-modal retrieval in life-long scenarios across various domains. The results of extensive experiments performed on multiple cross-modal datasets with different domains verify the effectiveness of the proposed VLKD approach for life-long cross-modal retrieval.},
  archive      = {J_IJCV},
  author       = {Peng, Yuxin and Qi, Jinwei and Ye, Zhaoda and Zhuo, Yunkan},
  doi          = {10.1007/s11263-020-01392-1},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {921-941},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical visual-textual knowledge distillation for life-long correlation learning},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to reconstruct HDR images from events, with
applications to depth and flow prediction. <em>IJCV</em>,
<em>129</em>(4), 900–920. (<a
href="https://doi.org/10.1007/s11263-020-01410-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras have numerous advantages over traditional cameras, such as low latency, high temporal resolution, and high dynamic range (HDR). We initially investigate the potential of creating intensity images/videos from an adjustable portion of the event data stream via event-based conditional generative adversarial networks (cGANs). Using the proposed framework, we further show the versatility of our method in directly handling similar supervised tasks, such as optical flow and depth prediction. Stacks of space-time coordinates of events are used as the inputs while the proposed framework is trained to predict either the intensity images, optical flows, or depth outputs according to the target task. We further demonstrate the unique capability of our approach in generating HDR images even under extreme illumination conditions, creating non-blurred images under rapid motion, and generating very high frame rate videos up to the temporal resolution of event cameras. The proposed framework is evaluated using a publicly available real-world dataset and a synthetic dataset we prepared by utilizing an event camera simulator.},
  archive      = {J_IJCV},
  author       = {Mostafavi, Mohammad and Wang, Lin and Yoon, Kuk-Jin},
  doi          = {10.1007/s11263-020-01410-2},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {900-920},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to reconstruct HDR images from events, with applications to depth and flow prediction},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A benchmark and evaluation of non-rigid structure from
motion. <em>IJCV</em>, <em>129</em>(4), 882–899. (<a
href="https://doi.org/10.1007/s11263-020-01406-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid structure from motion (nrs f m), is a long standing and central problem in computer vision and its solution is necessary for obtaining 3D information from multiple images when the scene is dynamic. A main issue regarding the further development of this important computer vision topic, is the lack of high quality data sets. We here address this issue by presenting a data set created for this purpose, which is made publicly available, and considerably larger than the previous state of the art. To validate the applicability of this data set, and provide an investigation into the state of the art of nrs f m, including potential directions forward, we here present a benchmark and a scrupulous evaluation using this data set. This benchmark evaluates 18 different methods with available code that reasonably spans the state of the art in sparse nrs f m. This new public data set and evaluation protocol will provide benchmark tools for further development in this challenging field.},
  archive      = {J_IJCV},
  author       = {Jensen, Sebastian Hoppe Nesgaard and Doest, Mads Emil Brix and Aanæs, Henrik and Del Bue, Alessio},
  doi          = {10.1007/s11263-020-01406-y},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {882-899},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A benchmark and evaluation of non-rigid structure from motion},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MOTChallenge: A benchmark for single-camera multiple target
tracking. <em>IJCV</em>, <em>129</em>(4), 845–881. (<a
href="https://doi.org/10.1007/s11263-020-01393-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standardized benchmarks have been crucial in pushing the performance of computer vision algorithms, especially since the advent of deep learning. Although leaderboards should not be over-claimed, they often provide the most objective measure of performance and are therefore important guides for research. We present MOTChallenge, a benchmark for single-camera Multiple Object Tracking (MOT) launched in late 2014, to collect existing and new data and create a framework for the standardized evaluation of multiple object tracking methods. The benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community, with applications ranging from robot navigation to self-driving cars. This paper collects the first three releases of the benchmark: (i) MOT15, along with numerous state-of-the-art results that were submitted in the last years, (ii) MOT16, which contains new challenging videos, and (iii) MOT17, that extends MOT16 sequences with more precise labels and evaluates tracking performance on three different object detectors. The second and third release not only offers a significant increase in the number of labeled boxes, but also provide labels for multiple object classes beside pedestrians, as well as the level of visibility for every single object of interest. We finally provide a categorization of state-of-the-art trackers and a broad error analysis. This will help newcomers understand the related work and research trends in the MOT community, and hopefully shed some light into potential future research directions.},
  archive      = {J_IJCV},
  author       = {Dendorfer, Patrick and Os̆ep, Aljos̆a and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan and Leal-Taixé, Laura},
  doi          = {10.1007/s11263-020-01393-0},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {845-881},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MOTChallenge: A benchmark for single-camera multiple target tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reference pose generation for long-term visual localization
via learned features and view synthesis. <em>IJCV</em>, <em>129</em>(4),
821–844. (<a href="https://doi.org/10.1007/s11263-020-01399-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Localization is one of the key enabling technologies for autonomous driving and augmented reality. High quality datasets with accurate 6 Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and improving existing methods. Traditionally, reference poses have been obtained via Structure-from-Motion (SfM). However, SfM itself relies on local features which are prone to fail when images were taken under different conditions, e.g., day/night changes. At the same time, manually annotating feature correspondences is not scalable and potentially inaccurate. In this work, we propose a semi-automated approach to generate reference poses based on feature matching between renderings of a 3D model and real images via learned features. Given an initial pose estimate, our approach iteratively refines the pose based on feature matches against a rendering of the model from the current pose estimate. We significantly improve the nighttime reference poses of the popular Aachen Day–Night dataset, showing that state-of-the-art visual localization methods perform better (up to 47\%) than predicted by the original reference poses. We extend the dataset with new nighttime test images, provide uncertainty estimates for our new reference poses, and introduce a new evaluation criterion. We will make our reference poses and our framework publicly available upon publication.},
  archive      = {J_IJCV},
  author       = {Zhang, Zichao and Sattler, Torsten and Scaramuzza, Davide},
  doi          = {10.1007/s11263-020-01399-8},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {821-844},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Reference pose generation for long-term visual localization via learned features and view synthesis},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning extremal representations with deep archetypal
analysis. <em>IJCV</em>, <em>129</em>(4), 805–820. (<a
href="https://doi.org/10.1007/s11263-020-01390-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Archetypes represent extreme manifestations of a population with respect to specific characteristic traits or features. In linear feature space, archetypes approximate the data convex hull allowing all data points to be expressed as convex mixtures of archetypes. As mixing of archetypes is performed directly on the input data, linear Archetypal Analysis requires additivity of the input, which is a strong assumption unlikely to hold e.g. in case of image data. To address this problem, we propose learning an appropriate latent feature space while simultaneously identifying suitable archetypes. We thus introduce a generative formulation of the linear archetype model, parameterized by neural networks. By introducing the distance-dependent archetype loss, the linear archetype model can be integrated into the latent space of a deep variational information bottleneck and an optimal representation, together with the archetypes, can be learned end-to-end. Moreover, the information bottleneck framework allows for a natural incorporation of arbitrarily complex side information during training. As a consequence, learned archetypes become easily interpretable as they derive their meaning directly from the included side information. Applicability of the proposed method is demonstrated by exploring archetypes of female facial expressions while using multi-rater based emotion scores of these expressions as side information. A second application illustrates the exploration of the chemical space of small organic molecules. By using different kinds of side information we demonstrate how identified archetypes, along with their interpretation, largely depend on the side information provided.},
  archive      = {J_IJCV},
  author       = {Keller, Sebastian Mathias and Samarin, Maxim and Arend Torres, Fabricio and Wieser, Mario and Roth, Volker},
  doi          = {10.1007/s11263-020-01390-3},
  journal      = {International Journal of Computer Vision},
  number       = {4},
  pages        = {805-820},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning extremal representations with deep archetypal analysis},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to: Rooted spanning superpixels. <em>IJCV</em>,
<em>129</em>(3), 803. (<a
href="https://doi.org/10.1007/s11263-020-01391-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The author regrets the omission of the following additional references to the International Journal of Computer Vision article, “Rooted Spanning Superpixels”.},
  archive      = {J_IJCV},
  author       = {Chai, Dengfeng},
  doi          = {10.1007/s11263-020-01391-2},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {803},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction to: Rooted spanning superpixels},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep nets: What have they ever done for vision?
<em>IJCV</em>, <em>129</em>(3), 781–802. (<a
href="https://doi.org/10.1007/s11263-020-01405-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where “big data is not enough” and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community.},
  archive      = {J_IJCV},
  author       = {Yuille, Alan L. and Liu, Chenxi},
  doi          = {10.1007/s11263-020-01405-z},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {781-802},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep nets: What have they ever done for vision?},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CDTD: A large-scale cross-domain benchmark for
instance-level image-to-image translation and domain adaptive object
detection. <em>IJCV</em>, <em>129</em>(3), 761–780. (<a
href="https://doi.org/10.1007/s11263-020-01394-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain visual problems, such as image-to-image translation and domain adaptive object detection, have attracted increasing attentions in the last few years, and also become new rising and challenging directions for the computer vision community. Recently, despite enormous efforts of the field in data collection, there are still few datasets covering the instance-level image-to-image translation and domain adaptive object detection tasks simultaneously. In this work, we introduce a large-scale cross-domain benchmark CDTD (contains 155,529 high-resolution natural images across four different modalities with object bounding box annotations. A summary of the entire dataset is provided in the following sections. Dataset is available at: http://zhiqiangshen.com/projects/INIT/index.html .) for the new instance-level translation and object detection tasks. We provide comprehensive baseline results of the benchmark on both of these two tasks. Moreover, we proposed a novel instance-level image-to-image translation approach called INIT and a gradient detach method for the domain adaptive object detection to harvest and exert dataset’s function of the instance level annotations across different domains.},
  archive      = {J_IJCV},
  author       = {Shen, Zhiqiang and Huang, Mingyang and Shi, Jianping and Liu, Zechun and Maheshwari, Harsh and Zheng, Yutong and Xue, Xiangyang and Savvides, Marios and Huang, Thomas S.},
  doi          = {10.1007/s11263-020-01394-z},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {761-780},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CDTD: A large-scale cross-domain benchmark for instance-level image-to-image translation and domain adaptive object detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compositional convolutional neural networks: A robust and
interpretable model for object recognition under occlusion.
<em>IJCV</em>, <em>129</em>(3), 736–760. (<a
href="https://doi.org/10.1007/s11263-020-01401-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision systems in real-world applications need to be robust to partial occlusion while also being explainable. In this work, we show that black-box deep convolutional neural networks (DCNNs) have only limited robustness to partial occlusion. We overcome these limitations by unifying DCNNs with part-based models into Compositional Convolutional Neural Networks (CompositionalNets)—an interpretable deep architecture with innate robustness to partial occlusion. Specifically, we propose to replace the fully connected classification head of DCNNs with a differentiable compositional model that can be trained end-to-end. The structure of the compositional model enables CompositionalNets to decompose images into objects and context, as well as to further decompose object representations in terms of individual parts and the objects’ pose. The generative nature of our compositional model enables it to localize occluders and to recognize objects based on their non-occluded parts. We conduct extensive experiments in terms of image classification and object detection on images of artificially occluded objects from the PASCAL3D+ and ImageNet dataset, and real images of partially occluded vehicles from the MS-COCO dataset. Our experiments show that CompositionalNets made from several popular DCNN backbones (VGG-16, ResNet50, ResNext) improve by a large margin over their non-compositional counterparts at classifying and detecting partially occluded objects. Furthermore, they can localize occluders accurately despite being trained with class-level supervision only. Finally, we demonstrate that CompositionalNets provide human interpretable predictions as their individual components can be understood as detecting parts and estimating an objects’ viewpoint.},
  archive      = {J_IJCV},
  author       = {Kortylewski, Adam and Liu, Qing and Wang, Angtian and Sun, Yihong and Yuille, Alan},
  doi          = {10.1007/s11263-020-01401-3},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {736-760},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Compositional convolutional neural networks: A robust and interpretable model for object recognition under occlusion},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewpoint and scale consistency reinforcement for UAV
vehicle re-identification. <em>IJCV</em>, <em>129</em>(3), 719–735. (<a
href="https://doi.org/10.1007/s11263-020-01402-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies vehicle ReID in aerial videos taken by Unmanned Aerial Vehicles (UAVs). Compared with existing vehicle ReID tasks performed with fixed surveillance cameras, UAV vehicle ReID is still under-explored and could be more challenging, e.g., aerial videos have dynamic and complex backgrounds, different vehicles show similar appearance, and the same vehicle commonly show distinct viewpoints and scales. To facilitate the research on UAV vehicle ReID, this paper contributes a novel dataset called UAV-VeID. UAV-VeID contains 41,917 images of 4601 vehicles captured by UAVs, where each vehicle has multiple images taken from different viewpoints. UAV-VeID also includes a large-scale distractor set to encourage the research on efficient ReID schemes. Compared with existing vehicle ReID datasets, UAV-VeID exhibits substantial variances in viewpoints and scales of vehicles, thus requires more robust features. To alleviate the negative effects of those variances, this paper also proposes a viewpoint adversarial training strategy and a multi-scale consensus loss to promote the robustness and discriminative power of learned deep features. Extensive experiments on UAV-VeID show our approach outperforms recent vehicle ReID algorithms. Moreover, our method also achieves competitive performance compared with recent works on existing vehicle ReID datasets including VehicleID, VeRi-776 and VERI-Wild.},
  archive      = {J_IJCV},
  author       = {Teng, Shangzhi and Zhang, Shiliang and Huang, Qingming and Sebe, Nicu},
  doi          = {10.1007/s11263-020-01402-2},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {719-735},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Viewpoint and scale consistency reinforcement for UAV vehicle re-identification},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). AdaFuse: Adaptive multiview fusion for accurate human pose
estimation in the wild. <em>IJCV</em>, <em>129</em>(3), 703–718. (<a
href="https://doi.org/10.1007/s11263-020-01398-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is probably the biggest challenge for human pose estimation in the wild. Typical solutions often rely on intrusive sensors such as IMUs to detect occluded joints. To make the task truly unconstrained, we present AdaFuse, an adaptive multiview fusion method, which can enhance the features in occluded views by leveraging those in visible views. The core of AdaFuse is to determine the point-point correspondence between two views which we solve effectively by exploring the sparsity of the heatmap representation. We also learn an adaptive fusion weight for each camera view to reflect its feature quality in order to reduce the chance that good features are undesirably corrupted by “bad” views. The fusion model is trained end-to-end with the pose estimation network, and can be directly applied to new camera configurations without additional adaptation. We extensively evaluate the approach on three public datasets including Human3.6M, Total Capture and CMU Panoptic. It outperforms the state-of-the-arts on all of them. We also create a large scale synthetic dataset Occlusion-Person, which allows us to perform numerical evaluation on the occluded joints, as it provides occlusion labels for every joint in the images. The dataset and code are released at https://github.com/zhezh/adafuse-3d-human-pose .},
  archive      = {J_IJCV},
  author       = {Zhang, Zhe and Wang, Chunyu and Qiu, Weichao and Qin, Wenhu and Zeng, Wenjun},
  doi          = {10.1007/s11263-020-01398-9},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {703-718},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AdaFuse: Adaptive multiview fusion for accurate human pose estimation in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised group mask network for object detection.
<em>IJCV</em>, <em>129</em>(3), 681–702. (<a
href="https://doi.org/10.1007/s11263-020-01397-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning object detectors from weak image annotations is an important yet challenging problem. Many weakly supervised approaches formulate the task as a multiple instance learning problem, where each image is represented as a bag of instances. For predicting the score for each object that occurs in an image, existing MIL based approaches tend to select the instance that responds more strongly to a specific class, which, however, overlooks the contextual information. Besides, objects often exhibit dramatic variations such as scaling and transformations, which makes them hard to detect. In this paper, we propose the weakly supervised group mask network (WSGMN), which mainly has two distinctive properties: (i) it exploits the relations among regions to generate community instances, which contain context information and are robust to object variations. (ii) It generates a mask for each label group, and utilizes these masks to dynamically select the feature information of the most useful community instances for recognizing specific objects. Extensive experiments on several benchmark datasets demonstrate the effectiveness of WSGMN on the tasks of weakly supervised object detection.},
  archive      = {J_IJCV},
  author       = {Song, Lingyun and Liu, Jun and Sun, Mingxuan and Shang, Xuequn},
  doi          = {10.1007/s11263-020-01397-w},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {681-702},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Weakly supervised group mask network for object detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Entrack: Probabilistic spherical regression with entropy
regularization for fiber tractography. <em>IJCV</em>, <em>129</em>(3),
656–680. (<a href="https://doi.org/10.1007/s11263-020-01384-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White matter tractography, based on diffusion-weighted magnetic resonance images, is currently the only available in vivo method to gather information on the structural brain connectivity. The low resolution of diffusion MRI data suggests to employ probabilistic methods for streamline reconstruction, i.e., for fiber crossings. We propose a general probabilistic model for spherical regression based on the Fisher-von-Mises distribution, which efficiently estimates maximum entropy posteriors of local streamline directions with machine learning methods. The optimal precision of posteriors for streamlines is determined by an information-theoretic technique, the expected log-posterior agreement concept. It relies on the requirement that the posterior distributions of streamlines, inferred on retest measurements of the same subject, should yield stable results within the precision determined by the noise level of the data source.},
  archive      = {J_IJCV},
  author       = {Wegmayr, Viktor and Buhmann, Joachim M.},
  doi          = {10.1007/s11263-020-01384-1},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {656-680},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Entrack: Probabilistic spherical regression with entropy regularization for fiber tractography},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive DARTS: Bridging the optimization gap for NAS in
the wild. <em>IJCV</em>, <em>129</em>(3), 638–655. (<a
href="https://doi.org/10.1007/s11263-020-01396-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of neural architecture search (NAS), researchers found powerful network architectures for a wide range of vision tasks. Like the manually designed counterparts, we desire the automatically searched architectures to have the ability of being freely transferred to different scenarios. This paper formally puts forward this problem, referred to as NAS in the wild, which explores the possibility of finding the optimal architecture in a proxy dataset and then deploying it to mostly unseen scenarios. We instantiate this setting using a currently popular algorithm named differentiable architecture search (DARTS), which often suffers unsatisfying performance while being transferred across different tasks. We argue that the accuracy drop originates from the formulation that uses a super-network for search but a sub-network for re-training. The different properties of these stages have resulted in a significant optimization gap, and consequently, the architectural parameters “over-fit” the super-network. To alleviate the gap, we present a progressive method that gradually increases the network depth during the search stage, which leads to the Progressive DARTS (P-DARTS) algorithm. With a reduced search cost (7 hours on a single GPU), P-DARTS achieves improved performance on both the proxy dataset (CIFAR10) and a few target problems (ImageNet classification, COCO detection and three ReID benchmarks). Our code is available at https://github.com/chenxin061/pdarts.},
  archive      = {J_IJCV},
  author       = {Chen, Xin and Xie, Lingxi and Wu, Jun and Tian, Qi},
  doi          = {10.1007/s11263-020-01396-x},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {638-655},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Progressive DARTS: Bridging the optimization gap for NAS in the wild},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual dual scale scene text spotting by fusing bottom-up
and top-down processing. <em>IJCV</em>, <em>129</em>(3), 619–637. (<a
href="https://doi.org/10.1007/s11263-020-01388-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for arbitrary shaped text spotting can be divided into two categories: bottom-up methods detect and recognize local areas of text, and then group them into text lines or words; top-down methods detect text regions of interest, then apply polygon fitting and text recognition to the detected regions. In this paper, we analyze the advantages and disadvantages of these two methods, and propose a novel text spotter by fusing bottom-up and top-down processing. To detect text of arbitrary shapes, we employ a bottom-up detector to describe text with a series of rotated squares, and design a top-down detector to represent the region of interest with a minimum enclosing rotated rectangle. Then the text boundary is determined by fusing the outputs of two detectors. To connect arbitrary shaped text detection and recognition, we propose a differentiable operator named RoISlide, which can extract features for arbitrary text regions from whole image feature maps. Based on the extracted features through RoISlide, a CNN and CTC based text recognizer is introduced to make the framework free from character-level annotations. To improve the robustness against scale variance, we further propose a residual dual scale spotting mechanism, where two spotters work on different feature levels, and the high-level spotter is based on residuals of the low-level spotter. Our method has achieved state-of-the-art performance on four English datasets and one Chinese dataset, including both arbitrary shaped and oriented texts. We also provide abundant ablation experiments to analyze how the key components affect the performance.},
  archive      = {J_IJCV},
  author       = {Feng, Wei and Yin, Fei and Zhang, Xu-Yao and He, Wenhao and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-020-01388-x},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {619-637},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Residual dual scale scene text spotting by fusing bottom-up and top-down processing},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive multi-granularity analysis for video prediction.
<em>IJCV</em>, <em>129</em>(3), 601–618. (<a
href="https://doi.org/10.1007/s11263-020-01389-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction is challenging as real-world motion dynamics are usually multi-modally distributed. Existing stochastic methods commonly formulate random noise input with simple prior distribution, which is insufficient to model highly complex motion dynamics. This work proposes a progressive multiple granularity analysis framework to tackle the above difficulty. Firstly, to achieve coarse alignment, the input sequence is matched to prototype motion dynamics in the training set, based on self-supervised auto-encoder learning via motion/appearance disentanglement. Secondly, motion dynamics is transferred from the matched prototype sequence to input sequence via adaptively learned kernel, and the predicted frames are further refined through a motion-aware prediction model. Extensive qualitative and quantitative experiments on three widely used video prediction datasets demonstrate that: (1) the proposed framework essentially decomposes the hard task into a series of more approachable sub-tasks where a better solution is easier to be sought and (2) our proposed method performs favorably against state-of-the-art prediction methods.},
  archive      = {J_IJCV},
  author       = {Xu, Jingwei and Ni, Bingbing and Yang, Xiaokang},
  doi          = {10.1007/s11263-020-01389-w},
  journal      = {International Journal of Computer Vision},
  number       = {3},
  pages        = {601-618},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Progressive multi-granularity analysis for video prediction},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deformable kernel networks for joint image filtering.
<em>IJCV</em>, <em>129</em>(2), 579–600. (<a
href="https://doi.org/10.1007/s11263-020-01386-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint image filters are used to transfer structural details from a guidance picture used as a prior to a target image, in tasks such as enhancing spatial resolution and suppressing noise. Previous methods based on convolutional neural networks (CNNs) combine nonlinear activations of spatially-invariant kernels to estimate structural details and regress the filtering result. In this paper, we instead learn explicitly sparse and spatially-variant kernels. We propose a CNN architecture and its efficient implementation, called the deformable kernel network (DKN), that outputs sets of neighbors and the corresponding weights adaptively for each pixel. The filtering result is then computed as a weighted average. We also propose a fast version of DKN that runs about seventeen times faster for an image of size $$640 \times 480$$ . We demonstrate the effectiveness and flexibility of our models on the tasks of depth map upsampling, saliency map upsampling, cross-modality image restoration, texture removal, and semantic segmentation. In particular, we show that the weighted averaging process with sparsely sampled $$3 \times 3$$ kernels outperforms the state of the art by a significant margin in all cases.},
  archive      = {J_IJCV},
  author       = {Kim, Beomjun and Ponce, Jean and Ham, Bumsub},
  doi          = {10.1007/s11263-020-01386-z},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {579-600},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deformable kernel networks for joint image filtering},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HOTA: A higher order metric for evaluating multi-object
tracking. <em>IJCV</em>, <em>129</em>(2), 548–578. (<a
href="https://doi.org/10.1007/s11263-020-01375-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, higher order tracking accuracy (HOTA), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.},
  archive      = {J_IJCV},
  author       = {Luiten, Jonathon and Os̆ep, Aljos̆a and Dendorfer, Patrick and Torr, Philip and Geiger, Andreas and Leal-Taixé, Laura and Leibe, Bastian},
  doi          = {10.1007/s11263-020-01375-2},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {548-578},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HOTA: A higher order metric for evaluating multi-object tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image matching across wide baselines: From paper to
practice. <em>IJCV</em>, <em>129</em>(2), 517–547. (<a
href="https://doi.org/10.1007/s11263-020-01385-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a comprehensive benchmark for local features and robust estimation algorithms, focusing on the downstream task—the accuracy of the reconstructed camera pose—as our primary metric. Our pipeline’s modular structure allows easy integration, configuration, and combination of different methods and heuristics. This is demonstrated by embedding dozens of popular algorithms and evaluating them, from seminal works to the cutting edge of machine learning research. We show that with proper settings, classical solutions may still outperform the perceived state of the art. Besides establishing the actual state of the art, the conducted experiments reveal unexpected properties of structure from motion pipelines that can help improve their performance, for both algorithmic and learned methods. Data and code are online ( https://github.com/ubc-vision/image-matching-benchmark ), providing an easy-to-use and flexible framework for the benchmarking of local features and robust estimation methods, both alongside and against top-performing methods. This work provides a basis for the Image Matching Challenge ( https://image-matching-challenge.github.io ).},
  archive      = {J_IJCV},
  author       = {Jin, Yuhe and Mishkin, Dmytro and Mishchuk, Anastasiia and Matas, Jiri and Fua, Pascal and Yi, Kwang Moo and Trulls, Eduard},
  doi          = {10.1007/s11263-020-01385-0},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {517-547},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image matching across wide baselines: From paper to practice},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binarized neural architecture search for efficient object
recognition. <em>IJCV</em>, <em>129</em>(2), 501–516. (<a
href="https://doi.org/10.1007/s11263-020-01379-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional neural architecture search (NAS) has a significant impact in computer vision by automatically designing network architectures for various tasks. In this paper, binarized neural architecture search (BNAS), with a search space of binarized convolutions, is introduced to produce extremely compressed models to reduce huge computational cost on embedded devices for edge computing. The BNAS calculation is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space, and the performance loss when handling the wild data in various computing applications. To address these issues, we introduce operation space reduction and channel sampling into BNAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy that is robust to wild data, which is further used to abandon less potential operations. Furthermore, we introduce the upper confidence bound to solve 1-bit BNAS. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a comparable performance to NAS on both CIFAR and ImageNet databases. An accuracy of 96.53\% vs. 97.22\% is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a 40\% faster search than the state-of-the-art PC-DARTS. On the wild face recognition task, our binarized models achieve a performance similar to their corresponding full-precision models.},
  archive      = {J_IJCV},
  author       = {Chen, Hanlin and Zhuo, Li’an and Zhang, Baochang and Zheng, Xiawu and Liu, Jianzhuang and Ji, Rongrong and Doermann, David and Guo, Guodong},
  doi          = {10.1007/s11263-020-01379-y},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {501-516},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Binarized neural architecture search for efficient object recognition},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained instance-level sketch-based image retrieval.
<em>IJCV</em>, <em>129</em>(2), 484–500. (<a
href="https://doi.org/10.1007/s11263-020-01382-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of fine-grained sketch-based image retrieval (FG-SBIR) is defined and investigated in this paper. In FG-SBIR, free-hand human sketch images are used as queries to retrieve photo images containing the same object instances. It is thus a cross-domain (sketch to photo) instance-level retrieval task. It is an extremely challenging problem because (i) visual comparisons and matching need to be executed under large domain gap, i.e., from black and white line drawing sketches to colour photos; (ii) it requires to capture the fine-grained (dis)similarities of sketches and photo images while free-hand sketches drawn by different people present different levels of deformation and expressive interpretation; and (iii) annotated cross-domain fine-grained SBIR datasets are scarce, challenging many state-of-the-art machine learning techniques, particularly those based on deep learning. In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based object instance retrieval application. Specifically, a new large-scale FG-SBIR database is introduced which is carefully designed to reflect the real-world application scenarios. A deep cross-domain matching model is then formulated to solve the intrinsic drawing style variability, large domain gap issues, and capture instance-level discriminative features. It distinguishes itself by a carefully designed attention module. Extensive experiments on the new dataset demonstrate the effectiveness of the proposed model and validate the need for a rigorous definition of the FG-SBIR problem and collecting suitable datasets.},
  archive      = {J_IJCV},
  author       = {Yu, Qian and Song, Jifei and Song, Yi-Zhe and Xiang, Tao and Hospedales, Timothy M.},
  doi          = {10.1007/s11263-020-01382-3},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {484-500},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fine-grained instance-level sketch-based image retrieval},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking the robustness of semantic segmentation models
with respect to common corruptions. <em>IJCV</em>, <em>129</em>(2),
462–483. (<a href="https://doi.org/10.1007/s11263-020-01383-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing a semantic segmentation model for a real-world application, such as autonomous driving, it is crucial to understand the robustness of the network with respect to a wide range of image corruptions. While there are recent robustness studies for full-image classification, we are the first to present an exhaustive study for semantic segmentation, based on many established neural network architectures. We utilize almost 400,000 images generated from the Cityscapes dataset, PASCAL VOC 2012, and ADE20K. Based on the benchmark study, we gain several new insights. Firstly, many networks perform well with respect to real-world image corruptions, such as a realistic PSF blur. Secondly, some architecture properties significantly affect robustness, such as a Dense Prediction Cell, designed to maximize performance on clean data only. Thirdly, the generalization capability of semantic segmentation models depends strongly on the type of image corruption. Models generalize well for image noise and image blur, however, not with respect to digitally corrupted data or weather corruptions.},
  archive      = {J_IJCV},
  author       = {Kamann, Christoph and Rother, Carsten},
  doi          = {10.1007/s11263-020-01383-2},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {462-483},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Benchmarking the robustness of semantic segmentation models with respect to common corruptions},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LaSOT: A high-quality large-scale single object tracking
benchmark. <em>IJCV</em>, <em>129</em>(2), 439–461. (<a
href="https://doi.org/10.1007/s11263-020-01387-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite great recent advances in visual tracking, its further development, including both algorithm design and evaluation, is limited due to lack of dedicated large-scale benchmarks. To address this problem, we present LaSOT, a high-quality Large-scale Single Object Tracking benchmark. LaSOT contains a diverse selection of 85 object classes, and offers 1550 totaling more than 3.87 million frames. Each video frame is carefully and manually annotated with a bounding box. This makes LaSOT, to our knowledge, the largest densely annotated tracking benchmark. Our goal in releasing LaSOT is to provide a dedicated high quality platform for both training and evaluation of trackers. The average video length of LaSOT is around 2500 frames, where each video contains various challenge factors that exist in real world video footage,such as the targets disappearing and re-appearing. These longer video lengths allow for the assessment of long-term trackers. To take advantage of the close connection between visual appearance and natural language, we provide language specification for each video in LaSOT. We believe such additions will allow for future research to use linguistic features to improve tracking. Two protocols, full-overlap and one-shot, are designated for flexible assessment of trackers. We extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis, and results reveal that there still exists significant room for improvement. The complete benchmark, tracking results as well as analysis are available at http://vision.cs.stonybrook.edu/~lasot/ .},
  archive      = {J_IJCV},
  author       = {Fan, Heng and Bai, Hexin and Lin, Liting and Yang, Fan and Chu, Peng and Deng, Ge and Yu, Sijia and Harshit and Huang, Mingzhen and Liu, Juehuan and Xu, Yong and Liao, Chunyuan and Yuan, Lin and Ling, Haibin},
  doi          = {10.1007/s11263-020-01387-y},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {439-461},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LaSOT: A high-quality large-scale single object tracking benchmark},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep hashing with hash-consistent large margin proxy
embeddings. <em>IJCV</em>, <em>129</em>(2), 419–438. (<a
href="https://doi.org/10.1007/s11263-020-01362-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hash codes are produced by binarizing the embeddings of convolutional neural networks (CNN) trained for either classification or retrieval. While proxy embeddings achieve good performance on both tasks, they are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and a procedure to design proxy sets that are nearly optimal for both classification and hashing is introduced. The resulting hash-consistent large margin (HCLM) proxies are shown to encourage saturation of hashing units, thus guaranteeing a small binarization error, while producing highly discriminative hash-codes. A semantic extension (sHCLM), aimed to improve hashing performance in a transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant improvements over state-of-the-art hashing procedures on several small and large datasets, both within and beyond the set of training classes.},
  archive      = {J_IJCV},
  author       = {Morgado, Pedro and Li, Yunsheng and Costa Pereira, Jose and Saberian, Mohammad and Vasconcelos, Nuno},
  doi          = {10.1007/s11263-020-01362-7},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {419-438},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep hashing with hash-consistent large margin proxy embeddings},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised deep representation learning for real-time
tracking. <em>IJCV</em>, <em>129</em>(2), 400–418. (<a
href="https://doi.org/10.1007/s11263-020-01357-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of visual tracking has continuously been brought by deep learning models. Typically, supervised learning is employed to train these models with expensive labeled data. In order to reduce the workload of manual annotation and learn to track arbitrary objects, we propose an unsupervised learning method for visual tracking. The motivation of our unsupervised learning is that a robust tracker should be effective in bidirectional tracking. Specifically, the tracker is able to forward localize a target object in successive frames and backtrace to its initial position in the first frame. Based on such a motivation, in the training process, we measure the consistency between forward and backward trajectories to learn a robust tracker from scratch merely using unlabeled videos. We build our framework on a Siamese correlation filter network, and propose a multi-frame validation scheme and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy of classic fully supervised trackers while achieving a real-time speed. Furthermore, our unsupervised framework exhibits a potential in leveraging more unlabeled or weakly labeled data to further improve the tracking accuracy.},
  archive      = {J_IJCV},
  author       = {Wang, Ning and Zhou, Wengang and Song, Yibing and Ma, Chao and Liu, Wei and Li, Houqiang},
  doi          = {10.1007/s11263-020-01357-4},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {400-418},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised deep representation learning for real-time tracking},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face image reflection removal. <em>IJCV</em>,
<em>129</em>(2), 385–399. (<a
href="https://doi.org/10.1007/s11263-020-01372-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face images captured through glass are usually contaminated by reflections. The low-transmitted reflections make the reflection removal more challenging than for general scenes because important facial features would be completely occluded. In this paper, we propose and solve the face image reflection removal problem. We recover the important facial structures by incorporating inpainting ideas into a guided reflection removal framework, which takes two images as the input and considers various face-specific priors. We use a newly collected face reflection image dataset to train our model and compare with state-of-the-art methods. The proposed method shows advantages in estimating reflection-free face images for improving face recognition.},
  archive      = {J_IJCV},
  author       = {Wan, Renjie and Shi, Boxin and Li, Haoliang and Duan, Ling-Yu and Kot, Alex C.},
  doi          = {10.1007/s11263-020-01372-5},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {385-399},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Face image reflection removal},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive analysis of weakly-supervised semantic
segmentation in different image domains. <em>IJCV</em>, <em>129</em>(2),
361–384. (<a href="https://doi.org/10.1007/s11263-020-01373-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub: https://github.com/lyndonchan/wsss-analysis .},
  archive      = {J_IJCV},
  author       = {Chan, Lyndon and Hosseini, Mahdi S. and Plataniotis, Konstantinos N.},
  doi          = {10.1007/s11263-020-01373-4},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {361-384},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive analysis of weakly-supervised semantic segmentation in different image domains},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rain rendering for evaluating and improving robustness to
bad weather. <em>IJCV</em>, <em>129</em>(2), 341–360. (<a
href="https://doi.org/10.1007/s11263-020-01366-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73\% more realistic than the state-of-the-art. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15\% for object detection, 60\% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21\% on object detection, 37\% on semantic segmentation, and 8\% on depth estimation.},
  archive      = {J_IJCV},
  author       = {Tremblay, Maxime and Halder, Shirsendu Sukanta and de Charette, Raoul and Lalonde, Jean-François},
  doi          = {10.1007/s11263-020-01366-3},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {341-360},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rain rendering for evaluating and improving robustness to bad weather},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JÂA-net: Joint facial action unit detection and face
alignment via adaptive attention. <em>IJCV</em>, <em>129</em>(2),
321–340. (<a href="https://doi.org/10.1007/s11263-020-01378-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action unit (AU) detection and face alignment are two highly correlated tasks, since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. However, most existing AU detection works handle the two tasks independently by treating face alignment as a preprocessing, and often use landmarks to predefine a fixed region or attention for each AU. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared feature is learned firstly, and high-level feature of face alignment is fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment feature and global feature for AU detection. Extensive experiments demonstrate that our framework (i) significantly outperforms the state-of-the-art AU detection methods on the challenging BP4D, DISFA, GFT and BP4D+ benchmarks, (ii) can adaptively capture the irregular region of each AU, (iii) achieves competitive performance for face alignment, and (iv) also works well under partial occlusions and non-frontal poses. The code for our method is available at https://github.com/ZhiwenShao/PyTorch-JAANet .},
  archive      = {J_IJCV},
  author       = {Shao, Zhiwen and Liu, Zhilei and Cai, Jianfei and Ma, Lizhuang},
  doi          = {10.1007/s11263-020-01378-z},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {321-340},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {JÂA-net: Joint facial action unit detection and face alignment via adaptive attention},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Beyond covariance: SICE and kernel based visual feature
representation. <em>IJCV</em>, <em>129</em>(2), 300–320. (<a
href="https://doi.org/10.1007/s11263-020-01376-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past several years have witnessed increasing research interest on covariance-based feature representation. Originally proposed as a region descriptor, it has now been used as a general representation in various recognition tasks, demonstrating promising performance. However, covariance matrix has some inherent shortcomings such as singularity in the case of small sample, limited capability in modeling complicated feature relationship, and a single, fixed form of representation. To achieve better recognition performance, this paper argues that more capable and flexible symmetric positive definite (SPD)-matrix-based representation shall be explored, and this is attempted in this work by exploiting prior knowledge of data and nonlinear representation. Specifically, to better deal with the issues of small number of feature vectors and high feature dimensionality, we propose to exploit the structure sparsity of visual features and exemplify sparse inverse covariance estimate as a new feature representation. Furthermore, to effectively model complicated feature relationship, we propose to directly compute kernel matrix over feature dimensions, leading to a robust, flexible and open framework of SPD-matrix-based representation. Through theoretical analysis and experimental study, the proposed two representations well demonstrate their advantages over the covariance counterpart in skeletal human action recognition, image set classification and object classification tasks.},
  archive      = {J_IJCV},
  author       = {Zhang, Jianjia and Wang, Lei and Zhou, Luping and Li, Wanqing},
  doi          = {10.1007/s11263-020-01376-1},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {300-320},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Beyond covariance: SICE and kernel based visual feature representation},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Volume sweeping: Learning photoconsistency for multi-view
shape reconstruction. <em>IJCV</em>, <em>129</em>(2), 284–299. (<a
href="https://doi.org/10.1007/s11263-020-01377-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a full study and methodology for multi-view stereo reconstruction with performance capture data. Multi-view 3D reconstruction has largely been studied with general, high resolution and high texture content inputs, where classic low-level feature extraction and matching are generally successful. However in performance capture scenarios, texture content is limited by wider angle shots resulting in smaller subject projection areas, and intrinsically low image content of casual clothing. We present a dedicated pipeline, based on a per-camera depth map sweeping strategy, analyzing in particular how recent deep network advances allow to replace classic multi-view photoconsistency functions with one that is learned. We show that learning based on a volumetric receptive field around a 3D depth candidate improves over using per-view 2D windows, giving the photoconsistency inference more visibility over local 3D correlations in viewpoint color aggregation. Despite being trained on a standard dataset of scanned static objects, the proposed method is shown to generalize and significantly outperform existing approaches on performance capture data, while achieving competitive results on recent benchmarks.},
  archive      = {J_IJCV},
  author       = {Leroy, Vincent and Franco, Jean-Sébastien and Boyer, Edmond},
  doi          = {10.1007/s11263-020-01377-0},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {284-299},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Volume sweeping: Learning photoconsistency for multi-view shape reconstruction},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised domain adaptation in the wild via disentangling
representation learning. <em>IJCV</em>, <em>129</em>(2), 267–283. (<a
href="https://doi.org/10.1007/s11263-020-01364-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recently proposed unsupervised domain adaptation algorithms attempt to learn domain invariant features by confusing a domain classifier through adversarial training. In this paper, we argue that this may not be an optimal solution in the real-world setting (a.k.a. in the wild) as the difference in terms of label information between domains has been largely ignored. As labeled instances are not available in the target domain in unsupervised domain adaptation tasks, it is difficult to explicitly capture the label difference between domains. To address this issue, we propose to learn a disentangled latent representation based on implicit autoencoders. In particular, a latent representation is disentangled into a global code and a local code. The global code is capturing category information via an encoder with a prior, and the local code is transferable across domains, which captures the “style” related information via an implicit decoder. Experimental results on digit recognition, object recognition and semantic segmentation demonstrate the effectiveness of our proposed method.},
  archive      = {J_IJCV},
  author       = {Li, Haoliang and Wan, Renjie and Wang, Shiqi and Kot, Alex C.},
  doi          = {10.1007/s11263-020-01364-5},
  journal      = {International Journal of Computer Vision},
  number       = {2},
  pages        = {267-283},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised domain adaptation in the wild via disentangling representation learning},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Talk2Nav: Long-range vision-and-language navigation with
dual attention and spatial memory. <em>IJCV</em>, <em>129</em>(1),
246–266. (<a href="https://doi.org/10.1007/s11263-020-01374-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with 10, 714 routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions—one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our project page .},
  archive      = {J_IJCV},
  author       = {Vasudevan, Arun Balajee and Dai, Dengxin and Van Gool, Luc},
  doi          = {10.1007/s11263-020-01374-3},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {246-266},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Talk2Nav: Long-range vision-and-language navigation with dual attention and spatial memory},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel-wise crowd understanding via synthetic data.
<em>IJCV</em>, <em>129</em>(1), 225–245. (<a
href="https://doi.org/10.1007/s11263-020-01365-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd analysis via computer vision techniques is an important topic in the field of video surveillance, which has wide-spread applications including crowd monitoring, public safety, space design and so on. Pixel-wise crowd understanding is the most fundamental task in crowd analysis because of its finer results for video sequences or still images than other analysis tasks. Unfortunately, pixel-level understanding needs a large amount of labeled training data. Annotating them is an expensive work, which causes that current crowd datasets are small. As a result, most algorithms suffer from over-fitting to varying degrees. In this paper, take crowd counting and segmentation as examples from the pixel-wise crowd understanding, we attempt to remedy these problems from two aspects, namely data and methodology. Firstly, we develop a free data collector and labeler to generate synthetic and labeled crowd scenes in a computer game, Grand Theft Auto V. Then we use it to construct a large-scale, diverse synthetic crowd dataset, which is named as “GCC Dataset”. Secondly, we propose two simple methods to improve the performance of crowd understanding via exploiting the synthetic data. To be specific, (1) supervised crowd understanding: pre-train a crowd analysis model on the synthetic data, then fine-tune it using the real data and labels, which makes the model perform better on the real world; (2) crowd understanding via domain adaptation: translate the synthetic data to photo-realistic images, then train the model on translated data and labels. As a result, the trained model works well in real crowd scenes.Extensive experiments verify that the supervision algorithm outperforms the state-of-the-art performance on four real datasets: UCF_CC_50, UCF-QNRF, and Shanghai Tech Part A/B Dataset. The above results show the effectiveness, values of synthetic GCC for the pixel-wise crowd understanding. The tools of collecting/labeling data, the proposed synthetic dataset and the source code for counting models are available at https://gjy3035.github.io/GCC-CL/ .},
  archive      = {J_IJCV},
  author       = {Wang, Qi and Gao, Junyu and Lin, Wei and Yuan, Yuan},
  doi          = {10.1007/s11263-020-01365-4},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {225-245},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pixel-wise crowd understanding via synthetic data},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepVS2.0: A saliency-structured deep learning method for
predicting dynamic visual attention. <em>IJCV</em>, <em>129</em>(1),
203–224. (<a href="https://doi.org/10.1007/s11263-020-01371-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have exhibited great success in image saliency prediction. However, few works apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method, called DeepVS2.0. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of LEDOV, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) in DeepVS2.0 to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that human attention has a temporal correlation with a smooth saliency transition across video frames. Therefore, a saliency-structured convolutional long short-term memory network (SS-ConvLSTM) is developed in DeepVS2.0 to predict inter-frame saliency, using the extracted features of OM-CNN as the input. Moreover, the center-bias dropout and sparsity-weighted loss are embedded in SS-ConvLSTM, to consider the center-bias and sparsity of human attention maps. Finally, the experimental results show that our DeepVS2.0 method advances the state-of-the-art video saliency prediction.},
  archive      = {J_IJCV},
  author       = {Jiang, Lai and Xu, Mai and Wang, Zulin and Sigal, Leonid},
  doi          = {10.1007/s11263-020-01371-6},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {203-224},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DeepVS2.0: A saliency-structured deep learning method for predicting dynamic visual attention},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving image description with auxiliary modality for
visual localization in challenging conditions. <em>IJCV</em>,
<em>129</em>(1), 185–202. (<a
href="https://doi.org/10.1007/s11263-020-01363-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.},
  archive      = {J_IJCV},
  author       = {Piasco, Nathan and Sidibé, Désiré and Gouet-Brunet, Valérie and Demonceaux, Cédric},
  doi          = {10.1007/s11263-020-01363-6},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {185-202},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Improving image description with auxiliary modality for visual localization in challenging conditions},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene text detection and recognition: The deep learning era.
<em>IJCV</em>, <em>129</em>(1), 161–184. (<a
href="https://doi.org/10.1007/s11263-020-01369-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise and development of deep learning, computer vision has been tremendously transformed and reshaped. As an important research area in computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution, consequentially entering the era of deep learning. In recent years, the community has witnessed substantial advancements in mindset, methodology and performance. This survey is aimed at summarizing and analyzing the major changes and significant progresses of scene text detection and recognition in the deep learning era. Through this article, we devote to: (1) introduce new insights and ideas; (2) highlight recent techniques and benchmarks; (3) look ahead into future trends. Specifically, we will emphasize the dramatic differences brought by deep learning and remaining grand challenges. We expect that this review paper would serve as a reference book for researchers in this field. Related resources are also collected in our Github repository ( https://github.com/Jyouhou/SceneTextPapers ).},
  archive      = {J_IJCV},
  author       = {Long, Shangbang and He, Xin and Yao, Cong},
  doi          = {10.1007/s11263-020-01369-0},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {161-184},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Scene text detection and recognition: The deep learning era},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursive context routing for object detection.
<em>IJCV</em>, <em>129</em>(1), 142–160. (<a
href="https://doi.org/10.1007/s11263-020-01370-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have confirmed that modeling contexts is important for object detection. However, current context modeling approaches still have limited expressive capacity and dynamics to encode contextual relationships and model contexts, deteriorating their effectiveness. In this paper, we instead seek to recast the current context modeling framework and perform more dynamic context modeling for object detection. In particular, we devise a novel Recursive Context Routing (ReCoR) mechanism to encode contextual relationships and model contexts more effectively. The ReCoR progressively models more contexts through a recursive structure, providing a more feasible and more comprehensive method to utilize complicated contexts and contextual relationships. For each recursive stage, we further decompose the modeling of contexts and contextual relationships into a spatial modeling process and a channel-wise modeling process, avoiding the need for exhaustive modeling of all the potential pair-wise contextual relationships with more dynamics in a single pass. The spatial modeling process focuses on spatial contexts and gradually involves more spatial contexts according to the recursive architecture. In the channel-wise modeling process, we introduce a context routing algorithm to improve the efficacy of modeling channel-wise contextual relationships dynamically. We perform a comprehensive evaluation of the proposed ReCoR on the popular MS COCO dataset and PASCAL VOC dataset. The effectiveness of the ReCoR can be validated on both datasets according to the consistent performance gains of applying our method on different baseline object detectors. For example, on MS COCO dataset, our approach can respectively deliver around 10\% relative improvements for a Mask RCNN detector on the bounding box task, and 7\% relative improvements on the instance segmentation task, surpassing existing context modeling approaches with a great margin. State-of-the-art detection performance can also be accessed by applying the ReCoR on the Cascade Mask RCNN detector, illustrating the great benefits of our method for improving context modeling and object detection.},
  archive      = {J_IJCV},
  author       = {Chen, Zhe and Zhang, Jing and Tao, Dacheng},
  doi          = {10.1007/s11263-020-01370-7},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {142-160},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Recursive context routing for object detection},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporally coherent general dynamic scene reconstruction.
<em>IJCV</em>, <em>129</em>(1), 123–141. (<a
href="https://doi.org/10.1007/s11263-020-01367-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: an automatic method for initial coarse reconstruction to initialize joint estimation; sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to various applications such as free-view rendering and virtual reality.},
  archive      = {J_IJCV},
  author       = {Mustafa, Armin and Volino, Marco and Kim, Hansung and Guillemaut, Jean-Yves and Hilton, Adrian},
  doi          = {10.1007/s11263-020-01367-2},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {123-141},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Temporally coherent general dynamic scene reconstruction},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving rolling shutter 3D vision problems using analogies
with non-rigidity. <em>IJCV</em>, <em>129</em>(1), 100–122. (<a
href="https://doi.org/10.1007/s11263-020-01368-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an original approach to absolute pose and structure-from-motion (SfM) which handles rolling shutter (RS) effects. Unlike most existing methods which either augment global shutter projection with velocity parameters or impose continuous time and motion through pose interpolation, we use local differential constraints. These are established by drawing analogies with non-rigid 3D vision techniques, namely shape-from-template and non-rigid SfM (NRSfM). The proposed idea is to interpret the images of a rigid surface acquired by a moving RS camera as those of a virtually deformed surface taken by a GS camera. These virtually deformed surfaces are first recovered by relaxing the RS constraint using SfT or NRSfM. Then we upgrade the virtually deformed surface to the actual rigid structure and compute the camera pose and ego-motion by reintroducing the RS constraint. This uses a new 3D-3D registration procedure that minimizes a cost function based on the Euclidean 3D point distance. This is more stable and physically meaningful than the reprojection error or the algebraic distance used in previous work. Experimental results obtained with synthetic and real data show that the proposed methods outperform existing ones in terms of accuracy and stability, even in the known critical configurations.},
  archive      = {J_IJCV},
  author       = {Lao, Yizhen and Ait-Aider, Omar and Bartoli, Adrien},
  doi          = {10.1007/s11263-020-01368-1},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {100-122},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Solving rolling shutter 3D vision problems using analogies with non-rigidity},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A camera model for line-scan cameras with telecentric
lenses. <em>IJCV</em>, <em>129</em>(1), 80–99. (<a
href="https://doi.org/10.1007/s11263-020-01358-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a camera model for line-scan cameras with telecentric lenses. The camera model assumes a linear relative motion with constant velocity between the camera and the object. It allows to model lens distortions, while supporting arbitrary positions of the line sensor with respect to the optical axis. We comprehensively examine the degeneracies of the camera model and propose methods to handle them. Furthermore, we examine the relation of the proposed camera model to affine cameras. In addition, we propose an algorithm to calibrate telecentric line-scan cameras using a planar calibration object. We perform an extensive evaluation of the proposed camera model that establishes the validity and accuracy of the proposed model. We also show that even for lenses with very small lens distortions, the distortions are statistically highly significant. Therefore, they cannot be omitted in real-world applications.},
  archive      = {J_IJCV},
  author       = {Steger, Carsten and Ulrich, Markus},
  doi          = {10.1007/s11263-020-01358-3},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {80-99},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A camera model for line-scan cameras with telecentric lenses},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image matching from handcrafted to deep features: A survey.
<em>IJCV</em>, <em>129</em>(1), 23–79. (<a
href="https://doi.org/10.1007/s11263-020-01359-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental and critical task in various visual applications, image matching can identify then correspond the same or similar structure/content from two or more images. Over the past decades, growing amount and diversity of methods have been proposed for image matching, particularly with the development of deep learning techniques over the recent years. However, it may leave several open questions about which method would be a suitable choice for specific applications with respect to different scenarios and task requirements and how to design better image matching methods with superior performance in accuracy, robustness and efficiency. This encourages us to conduct a comprehensive and systematic review and analysis for those classical and latest techniques. Following the feature-based image matching pipeline, we first introduce feature detection, description, and matching techniques from handcrafted methods to trainable ones and provide an analysis of the development of these methods in theory and practice. Secondly, we briefly introduce several typical image matching-based applications for a comprehensive understanding of the significance of image matching. In addition, we also provide a comprehensive and objective comparison of these classical and latest techniques through extensive experiments on representative datasets. Finally, we conclude with the current status of image matching technologies and deliver insightful discussions and prospects for future works. This survey can serve as a reference for (but not limited to) researchers and engineers in image matching and related fields.},
  archive      = {J_IJCV},
  author       = {Ma, Jiayi and Jiang, Xingyu and Fan, Aoxiang and Jiang, Junjun and Yan, Junchi},
  doi          = {10.1007/s11263-020-01359-2},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {23-79},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image matching from handcrafted to deep features: A survey},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). View transfer on human skeleton pose: Automatically
disentangle the view-variant and view-invariant information for pose
representation learning. <em>IJCV</em>, <em>129</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s11263-020-01354-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a good pose representation is significant for many applications, such as human pose estimation and action recognition. However, the representations learned by most approaches are not intrinsic and their transferability in different datasets and different tasks is limited. In this paper, we introduce a method to learn a versatile representation, which is capable of recovering unseen corrupted skeletons, being applied to the human action recognition, and transferring pose from one view to another view without knowing the relationships of cameras. To this end, a sequential bidirectional recursive network (SeBiReNet) is proposed for modeling kinematic dependency between skeleton joints. Utilizing the SeBiReNet as the core module, a denoising autoencoder is designed to learn intrinsic pose features through the task of recovering corrupted skeletons. Instead of only extracting the view-invariant feature as many other methods, we disentangle the view-invariant feature from the view-variant feature in the latent space and use them together as a representation of the human pose. For a better feature disentanglement, an adversarial augmentation strategy is proposed and applied to the denoising autoencoder. Disentanglement of view-variant and view-invariant features enables us to realize view transfer on 3D poses. Extensive experiments on different datasets and different tasks verify the effectiveness and versatility of the learned representation.},
  archive      = {J_IJCV},
  author       = {Nie, Qiang and Liu, Yunhui},
  doi          = {10.1007/s11263-020-01354-7},
  journal      = {International Journal of Computer Vision},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {View transfer on human skeleton pose: Automatically disentangle the view-variant and view-invariant information for pose representation learning},
  volume       = {129},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
