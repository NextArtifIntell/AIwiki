<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>APIN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="apin---476">APIN - 476</h2>
<ul>
<li><details>
<summary>
(2021). Coordinate-based anchor-free module for object detection.
<em>APIN</em>, <em>51</em>(12), 9066–9080. (<a
href="https://doi.org/10.1007/s10489-021-02373-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of some recent state-of-the-art detectors, small target detection, scale variation, and label ambiguities remain challenges. To tackle these issues, we present a coordinate-based anchor-free (CBAF) module for object detection. It can be used as a branch of a single-shot detector (e.g., RetinaNet or SSD) or predict the output probabilities and coordinates directly. The main idea of the CBAF module is to predict the category and the adjustments to the box of the object by part feature and its contextual part features, which are based on feature maps divided by spatial coordinates. This is inspired by the fact that human beings can infer an entire object by observing the part of the surrounding environment. The CBAF module will encode and decode boxes in the anchor-free manner per feature map with different resolutions during training and testing. During training, we first use the proposed spatial coordinate partition layer to divide feature maps into several parts of size n × n and then propose a contextual building layer to fuse the part and its contextual parts together. We will demonstrate the CBAF module through a concrete implementation. The CBAF module improves AP scores of object detection with nearly no additional computation when working in conjunction with the anchor-based RetinaNet. Furthermore, experimental results on the MS-COCO dataset show that the mAP of the CBAF module has increased by 1.1%, compared with RetinaNet. When the CBAF module works in conjunction with the anchor-based RetinaNet, the mAP increased by 2.2%.},
  archive      = {J_APIN},
  author       = {Tang, Zhiyong and Yang, Jianbing and Pei, Zhongcai and Song, Xiao},
  doi          = {10.1007/s10489-021-02373-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {9066-9080},
  shortjournal = {Appl. Intell.},
  title        = {Coordinate-based anchor-free module for object detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust synchronization of uncertain delayed neural networks
with packet dropout using sampled-data control. <em>APIN</em>,
<em>51</em>(12), 9054–9065. (<a
href="https://doi.org/10.1007/s10489-021-02388-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research for synchronization of neural networks have obtained some good results, but there are still some shortcomings in the research of uncertain delayed neural networks (DNN) with packet dropout. In this paper, we investigate the robust synchronization problem for uncertain neural networks with time-delay and packet dropout under sampled-data control (SDC). A neoteric time-delay Lyapunov-Krasovskii functional (TDLKF) with discrete and distributed delays is constructed by introducing some new constraints. With the help of the constructed TDLKF and the improved integral inequality, some stability criteria are derived to guarantee the error system synchronize exponentially when package dropout occurs randomly. The corresponding sampled-data controller can be acquired by solving a host of linear matrix inequalities (LMIs). Some numerical examples are used to illustrate the validity of the proposed method. The results show that the proposed method is an effective control strategy to solve the synchronization control problem of uncertain delayed neural networks with packet dropout.},
  archive      = {J_APIN},
  author       = {Zhang, Ganlei and Zhang, Jiayong and Li, Wei and Ge, Chao and Liu, Yajuan},
  doi          = {10.1007/s10489-021-02388-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {9054-9065},
  shortjournal = {Appl. Intell.},
  title        = {Robust synchronization of uncertain delayed neural networks with packet dropout using sampled-data control},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 2-SPIFF: A 2-stage packer identification method based on
function call graph and file attributes. <em>APIN</em>, <em>51</em>(12),
9038–9053. (<a
href="https://doi.org/10.1007/s10489-021-02347-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most malware employs packing technology to escape detection; thus, packer identification has become increasingly important in malware detection. To improve the accuracy of packer identification, this article analyses the differences in the function call graph (FCG) and file attributes between the non-packed executable files and the executable files packed by different packers, and further proposes a 2-stage packer i dentification method based on FCG and file attributes (2-SPIFF). In 2-SPIFF, the detection model of stage I distinguishes non-packed executable files from packed executable files based on the graph features extracted from the FCG, while the identification model of stage II identifies the packer used for packing the original executable file by using the concatenated features extracted from the FCG and file attributes. The experimental results show that 2-SPIFF can achieve an accuracy of 99.80% for packer detection and an accuracy of 98.49% for packer identification.},
  archive      = {J_APIN},
  author       = {Liu, Hao and Guo, Chun and Cui, Yunhe and Shen, Guowei and Ping, Yuan},
  doi          = {10.1007/s10489-021-02347-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {9038-9053},
  shortjournal = {Appl. Intell.},
  title        = {2-SPIFF: A 2-stage packer identification method based on function call graph and file attributes},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SeqVAE: Sequence variational autoencoder with policy
gradient. <em>APIN</em>, <em>51</em>(12), 9030–9037. (<a
href="https://doi.org/10.1007/s10489-021-02374-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we propose a variant of Variational Autoencoder (VAE) for sequence generation task, called SeqVAE, which is a combination of recurrent VAE and policy gradient in reinforcement learning. The goal of SeqVAE is to reduce the deviation of the optimization goal of VAE, which we achieved by adding the policy-gradient loss to SeqVAE. In the paper, we give two ways to calculate the policy-gradient loss, one is from SeqGAN and the other is proposed by us. In the experiments on them, our proposed method is better than all baselines, and experiments show that SeqVAE can alleviate the “post-collapse” problem. Essentially, SeqVAE can be regarded as a combination of VAE and Generative Adversarial Net (GAN) and has better learning ability than the plain VAE because of the increased adversarial process. Finally, an application of our SeqVAE to music melody generation is available online12.},
  archive      = {J_APIN},
  author       = {Gao, Ting and Cui, Yidong and Ding, Fanyu},
  doi          = {10.1007/s10489-021-02374-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {9030-9037},
  shortjournal = {Appl. Intell.},
  title        = {SeqVAE: Sequence variational autoencoder with policy gradient},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Novel best path selection approach based on hybrid improved
a* algorithm and reinforcement learning. <em>APIN</em>, <em>51</em>(12),
9015–9029. (<a
href="https://doi.org/10.1007/s10489-021-02303-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning of intelligent driving vehicles in emergencies is a hot research issue, this paper proposes a new method of the best path selection for the intelligent driving vehicles to solve this problem. Based on the prior knowledge applied reinforcement learning strategy and the searching- optimized A* algorithm, we designed a hybrid algorithm to help intelligent driving vehicles selecting the best path in the traffic network in emergencies including limited height, width, weight, accident, and traffic jam. Through simulation experiments and scene experiments, it is proved that the proposed algorithm has good stability, high efficiency, and practicability.},
  archive      = {J_APIN},
  author       = {Liu, Xiaohuan and Zhang, Degan and Zhang, Ting and Cui, Yuya and Chen, Lu and Liu, Si},
  doi          = {10.1007/s10489-021-02303-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {9015-9029},
  shortjournal = {Appl. Intell.},
  title        = {Novel best path selection approach based on hybrid improved a* algorithm and reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast gaussian kernel support vector machine recursive
feature elimination algorithm. <em>APIN</em>, <em>51</em>(12),
9001–9014. (<a
href="https://doi.org/10.1007/s10489-021-02298-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian kernel support vector machine recursive feature elimination (GKSVM-RFE) is a method for feature ranking in a nonlinear way. However, GKSVM-RFE suffers from the issue of high computational complexity, which hinders its applications. This paper investigates the issue of computational complexity in GKSVM-RFE, and proposes two fast versions for GKSVM-RFE, called fast GKSVM-RFE (FGKSVM-RFE), to speed up the procedure of recursive feature elimination in GKSVM-RFE. For this purpose, we design two kinds of ranking scores based on the first-order and second-order approximate schemes by introducing approximate Gaussian kernels. In iterations, FGKSVM-RFE fast calculates approximate ranking scores according to approximate schemes and ranks features based on approximate ranking scores. Experimental results reveal that our proposed methods can faster perform feature ranking than GKSVM-RFE and have compared performance to GKSVM-RFE.},
  archive      = {J_APIN},
  author       = {Zhang, Li and Zheng, Xiaohan and Pang, Qingqing and Zhou, Weida},
  doi          = {10.1007/s10489-021-02298-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {9001-9014},
  shortjournal = {Appl. Intell.},
  title        = {Fast gaussian kernel support vector machine recursive feature elimination algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bi-stage feature selection approach for COVID-19
prediction using chest CT images. <em>APIN</em>, <em>51</em>(12),
8985–9000. (<a
href="https://doi.org/10.1007/s10489-021-02292-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid spread of coronavirus disease has become an example of the worst disruptive disasters of the century around the globe. To fight against the spread of this virus, clinical image analysis of chest CT (computed tomography) images can play an important role for an accurate diagnostic. In the present work, a bi-modular hybrid model is proposed to detect COVID-19 from the chest CT images. In the first module, we have used a Convolutional Neural Network (CNN) architecture to extract features from the chest CT images. In the second module, we have used a bi-stage feature selection (FS) approach to find out the most relevant features for the prediction of COVID and non-COVID cases from the chest CT images. At the first stage of FS, we have applied a guided FS methodology by employing two filter methods: Mutual Information (MI) and Relief-F, for the initial screening of the features obtained from the CNN model. In the second stage, Dragonfly algorithm (DA) has been used for the further selection of most relevant features. The final feature set has been used for the classification of the COVID-19 and non-COVID chest CT images using the Support Vector Machine (SVM) classifier. The proposed model has been tested on two open-access datasets: SARS-CoV-2 CT images and COVID-CT datasets and the model shows substantial prediction rates of 98.39% and 90.0% on the said datasets respectively. The proposed model has been compared with a few past works for the prediction of COVID-19 cases. The supporting codes are uploaded in the Github link: https://github.com/Soumyajit-Saha/A-Bi-Stage-Feature-Selection-on-Covid-19-Dataset},
  archive      = {J_APIN},
  author       = {Sen, Shibaprasad and Saha, Soumyajit and Chatterjee, Somnath and Mirjalili, Seyedali and Sarkar, Ram},
  doi          = {10.1007/s10489-021-02292-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8985-9000},
  shortjournal = {Appl. Intell.},
  title        = {A bi-stage feature selection approach for COVID-19 prediction using chest CT images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection for semi-supervised multi-target
regression using genetic algorithm. <em>APIN</em>, <em>51</em>(12),
8961–8984. (<a
href="https://doi.org/10.1007/s10489-021-02291-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target regression (MTR) is an exciting area of machine learning where the challenge is to predict the values of more than one target variables which can take on continuous values. These variables may or may not be correlated. Such problems commonly occur in real life scenarios, and therefore, interest and research in this area has increased in recent times. Some examples of applications include analyzing brain-activity data gathered using multimedia sensors, stock information from continuous web data, data related to characteristics of the vegetation at a certain site, etc. For a real-world multi-target learning system, the problem can be further complicated when new issues emerge with very little data available. In such cases, a semi-supervised approach can be adopted. This paper proposes a Genetic Algorithm (GA) based semi-supervised technique on multi-target regression problems to predict new targets, using very small number of labelled examples by incorporating GA with MTR-SAFER. Experiments are carried out on real world MTR data sets. The proposed method isexplored with different variations and also compared with the state of the art MTR methods. Results have indicated a significantly better performance with the further benefit of having a reduced feature set.},
  archive      = {J_APIN},
  author       = {Syed, Farrukh Hasan and Tahir, Muhammad Atif and Rafi, Muhammad and Shahab, Mir Danish},
  doi          = {10.1007/s10489-021-02291-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8961-8984},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection for semi-supervised multi-target regression using genetic algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Software fault prediction based on the dynamic selection of
learning technique: Findings from the eclipse project study.
<em>APIN</em>, <em>51</em>(12), 8945–8960. (<a
href="https://doi.org/10.1007/s10489-021-02346-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective software fault prediction (SFP) model could help developers in the quick and prompt detection of faults and thus help enhance the overall reliability and quality of the software project. Variations in the prediction performance of learning techniques for different software systems make it difficult to select a suitable learning technique for fault prediction modeling. The evaluation of previously presented SFP approaches has shown that single machine learning-based models failed to provide the best accuracy in any context, highlighting the need to use multiple techniques to build the SFP model. To solve this problem, we present and discuss a software fault prediction approach based on selecting the most appropriate learning techniques from a set of competitive and accurate learning techniques for building a fault prediction model. In work, we apply the discussed SFP approach for the five Eclipse project datasets and nine Object-oriented (OO) project datasets and report the findings of the experimental study. We have used different performance measures, i.e., AUC, accuracy, sensitivity, and specificity, to assess the discussed approach’s performance. Further, we have performed a cost-benefit analysis to evaluate the economic viability of the approach. Results showed that the presented approach predicted the software’s faults effectively for the used accuracy, AUC, sensitivity, and specificity measures with the highest achieved values of 0.816, 0.835, 0.98, and 0.903 for AUC, accuracy, sensitivity, and specificity, respectively. The cost-benefit analysis of the approach showed that it could help reduce the overall software testing cost.},
  archive      = {J_APIN},
  author       = {Rathore, Santosh S. and Kumar, Sandeep},
  doi          = {10.1007/s10489-021-02346-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8945-8960},
  shortjournal = {Appl. Intell.},
  title        = {Software fault prediction based on the dynamic selection of learning technique: Findings from the eclipse project study},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-domain coordinated sentence similarity scheme for
question-answering robots regarding unpredictable outliers and
non-orthogonal categories. <em>APIN</em>, <em>51</em>(12), 8928–8944.
(<a href="https://doi.org/10.1007/s10489-021-02269-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial and challenging for the question-answering robot (Qabot) to match the customer-input questions with the priori identification questions due to highly diversified expressions, especially in the case of Chinese. This article proposes a coordinated scheme to analyze the similarity between sentences in two independent domains instead of a single deep learning model. In the structure domain, the BLEU and data preprocessing are applied for binary analysis to discriminate the unpredictable outliers (illegal questions) to existing library. In the semantics domain, the MC-BERT model, which integrates the BERT encoder and the Multi-kernel convolutional top classifier, is developed to handle the non-orthogonality of class identification questions. The two-domain analyses are in parallel and the two similarity scores are coordinated for the final response. The linguistic features of Chinese are also taken into account. A realistic case of Qabot on energy trading service and finance is numerically studied. Computational results validate the effectiveness and accuracy of the proposed algorithm: Top-1 and Top-3 accuracies are 90.5% and 95.5%, respectively, which are significantly superior to the latest published results.},
  archive      = {J_APIN},
  author       = {Li, Boyang and Xu, Weisheng and Xu, Zhiyu and Li, Jiaxin and Peng, Peng},
  doi          = {10.1007/s10489-021-02269-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8928-8944},
  shortjournal = {Appl. Intell.},
  title        = {A two-domain coordinated sentence similarity scheme for question-answering robots regarding unpredictable outliers and non-orthogonal categories},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo-label growth dictionary pair learning for crowd
counting. <em>APIN</em>, <em>51</em>(12), 8913–8927. (<a
href="https://doi.org/10.1007/s10489-021-02274-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting has received increasing attention in the field of video surveillance and urban security system. However, many previous models are prone to poor generalization capability to unknown samples when limited labeled samples are available. To improve or mitigate the above weakness, we develop a novel Pseudo-label Growth Dictionary Pair Learning (PG-DPL) method for crowd counting. To be exact, we treat crowd counting as a task of classification and leverage dictionary learning-based (DL) strategy to target the task. Considering that being short of diverse training samples and imbalanced distribution across different classes in crowd scene inevitably result in large prediction deviation caused by the DL model, we propose to apply pseudo-label growth (PG) and adaptive dictionary size (ADS) to improve the accuracy of crowd counting with limited labeled samples. In the proposed method, PG optimizes the initial prediction via reconstructing the discriminant term to improve the robustness of learned dictionary, while ADS explores the imbalanced distribution among different classes to adapt to the size of class-specific dictionary. Extensive validation experiments on five benchmark databases indicate that the proposed PG-DPL can achieve compelling performance compared to other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Liu, Wei and Wang, Huake and Luo, Hao and Zhang, Kaibing and Lu, Jian and Xiong, Zenggang},
  doi          = {10.1007/s10489-021-02274-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8913-8927},
  shortjournal = {Appl. Intell.},
  title        = {Pseudo-label growth dictionary pair learning for crowd counting},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-information embedding based entity alignment.
<em>APIN</em>, <em>51</em>(12), 8896–8912. (<a
href="https://doi.org/10.1007/s10489-021-02400-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment refers to discovering two entities in different knowledge bases that represent the same thing in reality. Existing methods generally only adopt TransE or TransE-like knowledge graph representation learning models, which usually assume that there are enough training triples for each entity, and entities appearing in few triples are easily misaligned. In this paper, we propose a multi-information embedding based entity alignment method (MEEA), which utilizes embedding methods based on multi-information, including triple embedding and neighbor information embedding, to obtain the vector representations of each entity, which are then used for aligning entities. In addition, we propose a weighted neighbor information encoding method to make the neighbor information based vector representation suitable for entity alignment, which measures the effects of different neighbors on entity alignment from three aspects (i.e., the mapping cardinality of the neighbor, the relation association of the neighbor, and the attention mechanism) and gives corresponding weights. Experiments are conducted on two cross-lingual knowledge bases, and the experimental results show that MEEA is able to yield a better performance compared to the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Chen, Ling and Tian, Xiaoxue and Tang, Xing and Cui, Jun},
  doi          = {10.1007/s10489-021-02400-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8896-8912},
  shortjournal = {Appl. Intell.},
  title        = {Multi-information embedding based entity alignment},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic coronary artery segmentation algorithm based on
deep learning and digital image processing. <em>APIN</em>,
<em>51</em>(12), 8881–8895. (<a
href="https://doi.org/10.1007/s10489-021-02197-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic segmentation of coronary artery in coronary computed tomography angiography (CCTA) image is of great significance for clinicians to evaluate patients with coronary heart disease. When a 3D image is limited by the amount of available GPU memory, reducing the resolution of 3D image will easily lead to the loss of image detail information. Taking patches of image as input cannot make full use of image context information. Image segmentation based on deep learning is difficult to recover perfect smooth edges. The use of smooth loss function may filter out some small lesions on the coronary artery. In this paper, we present a novel CCTA image segmentation framework that combines deep learning and digital image processing algorithms to address these challenging problems. We first use V-Net to process the CCTA image with lower resolution, and get the basic feature map (rough segmentation result) with the same resolution as the original CCTA image. Then, the original CCTA image is concatenated to the basic feature map and input it into the patch-based cascaded V-shaped module to obtain a accurate coronary artery segmentation image. Finally, the center points of coronary segmentation image and the basic gradient image of the original coronary image are obtained by morphological operation. The center points of coronary artery segmentation image are used as seed points, region growing is performed on the binary basic gradient image until the white contour boundary is searched, so as to obtain a coronary segmentation result with full segmentation and smooth edges. The proposed method is analyzed quantitatively and qualitatively, and the results show that the method is better than the mainstream baseline. The ablation experiment also proved the effectiveness of each module.},
  archive      = {J_APIN},
  author       = {Tian, Fangzheng and Gao, Yongbin and Fang, Zhijun and Gu, Jia},
  doi          = {10.1007/s10489-021-02197-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8881-8895},
  shortjournal = {Appl. Intell.},
  title        = {Automatic coronary artery segmentation algorithm based on deep learning and digital image processing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Writer identification using redundant writing patterns and
dual-factor analysis of variance. <em>APIN</em>, <em>51</em>(12),
8865–8880. (<a
href="https://doi.org/10.1007/s10489-021-02307-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writer identification (WI) is a typical pattern recognition problem with the goal of recognizing the writer of a text from images of his or her handwriting. For handwriting-based applications, a new approach is required that can confirm the writer based on a very small amount of available text. However, due to the limited number of characters and the diverse contents of the available samples, it is still challenging to achieve a high recognition rate on large-scale databases. To solve this problem, this paper proposes an efficient method for offline text-independent WI using redundant writing patterns and dual-factor analysis of variance (DF-ANOVA). In this method, a search for writing patterns in a limited number of handwritten texts is first conducted to generate meaningful codebooks. Then, these writing patterns are described by means of the simplified Wigner distribution function (WDF) and improved directional index histogram (DIH) descriptors to extract both structural and texture features, and the two corresponding feature distances are integrated by means of a fuzzy integral rule to make the final decision. The Fisher discriminant ratio (FDR) is used for feature selection, and DF-ANOVA is used to eliminate the influence of factors associated with the labels of samples on the feature distance. The results of evaluations on the IAM (96.92%), Firemaker (96.4%), and Uyghur2016 (100%) databases illustrate that this system shows strong robustness to an increasing number of writers and a decreasing number of words in the sample texts.},
  archive      = {J_APIN},
  author       = {Litifu, Ayixiamu and Yan, Yuchen and Xiao, Jinsheng and Jiang, Hao},
  doi          = {10.1007/s10489-021-02307-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8865-8880},
  shortjournal = {Appl. Intell.},
  title        = {Writer identification using redundant writing patterns and dual-factor analysis of variance},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new dictionary-based positive and unlabeled learning
method. <em>APIN</em>, <em>51</em>(12), 8850–8864. (<a
href="https://doi.org/10.1007/s10489-021-02344-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and unlabeled learning (PU learning) is designed to solve the problem that we only utilize the labeled positive examples and the unlabeled examples to train a classifier. A variety of methods have been proposed to solve this problem by incorporating unlabeled examples into learning. However, many methods treat the original features as input in the training stage and then build the classifier. In this paper, by use of two-step strategy, a novel method with dictionary learning is proposed for PU learning, which is briefly called PUDL. The proposed method is done in two steps. Firstly, we extract reliable negative examples from unlabeled examples to form the negative class. We then utilize dictionary learning to construct the feature representation which can learn new features for the original input. Secondly, we propose RankSVM-based (Ranking Support Vector Machine) model to incorporate the positive class, extracted negative class and similarity weights into learning so as to improve performance of the classifier. The Lagrange multiplier method is applied to convert the original model into its dual form. In addition, we put forward an interactive optimization framework to optimize the proposed objective model and obtain the classifier. Finally, we verify the performance of PUDL via experiment and the results demostrate that PUDL performs better than state-of-the-art PU learning methods.},
  archive      = {J_APIN},
  author       = {Liu, Bo and Liu, Zhijing and Xiao, Yanshan},
  doi          = {10.1007/s10489-021-02344-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8850-8864},
  shortjournal = {Appl. Intell.},
  title        = {A new dictionary-based positive and unlabeled learning method},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple teacher behavior recognition method for massive
teaching videos based on teacher set. <em>APIN</em>, <em>51</em>(12),
8828–8849. (<a
href="https://doi.org/10.1007/s10489-021-02329-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of teacher behavior of massive teaching videos has become a surge of research interest recently. Traditional methods rely on accurate manual analysis, which is extremely complex and time-consuming for analyzing massive teaching videos. However, existing works on action recognition are difficultly transplanted to the teacher behavior recognition, because it is difficult to extract teacher’s behavior from complex teaching scenario, and teacher’s behaviors are given professional educational semantics. These methods are not adequate for the need of the teacher behavior recognition. Thus, a novel and simple recognition method of teacher behavior in the actual teaching scene for massive teaching videos is proposed, which can provide technical assistance for analyzing teacher behavior and fill the blank of automatic recognition of teacher behavior in actual teaching scene. Firstly, we discover the educational pattern which it be named “teacher set”, that is, the spatial region of the video of the whole class where teachers should exist. Based on this, the algorithm of teacher set identification and extraction (Teacher-set IE algorithm) is studied to identify the teacher in the teaching video, and reduce the interference factors of classroom background. Then, an improved behavior recognition network based on 3D bilinear pooling (3D BP-TBR) is presented to enhance fusion representation of three-dimensional features thus identifying the categories of teacher behavior, and experiments show that 3D BP-TBR can achieve better performance on public and self-built dataset (TAD-08). Hence, our whole approach can increase recognition accuracy of teacher behavior in the actual teaching scene to utilize the deep integration of educational characteristics and action recognition technology.},
  archive      = {J_APIN},
  author       = {Gang, Zhao and Wenjuan, Zhu and Biling, Hu and Jie, Chu and Hui, He and Qing, Xia},
  doi          = {10.1007/s10489-021-02329-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8828-8849},
  shortjournal = {Appl. Intell.},
  title        = {A simple teacher behavior recognition method for massive teaching videos based on teacher set},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive channel and multiscale spatial context network for
breast mass segmentation in full-field mammograms. <em>APIN</em>,
<em>51</em>(12), 8810–8827. (<a
href="https://doi.org/10.1007/s10489-021-02297-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is currently the second most fatal cancer in women, but timely diagnosis and treatment can reduce its mortality. Breast masses are the most obvious means of cancer identification, and thus, accurate segmentation of masses is critical. In contrast to mass-centered patch segmentation, accurate segmentation of breast masses in full-field mammograms is always a challenging topic because of the extremely low signal-to-noise ratio and the uncertainty with respect to the shape, size, and location of the mass. In this study, we propose a novel adaptive channel and multiscale spatial context network for breast mass segmentation in full-field mammograms. A standard encoder-decoder structure is employed, and an elaborate adaptive channel and multiscale spatial context module (ACMSC module) is embedded in a multilevel manner in our network for accurate mass segmentation. The proposed ACMSC module utilizes the self-attention mechanism to adaptively capture discriminative contextual information among channel and spatial dimensions.The multilevel embedding of the ACMSC module enables the network to learn distinguishing features on multiple scales of feature maps. Our proposed model is evaluated on two public datasets, CBIS-DDSM and INbreast. The experimental results show that by adaptively capturing the context of the channel and spatial dimensions, our model can effectively remove false positives, predict difficult samples and achieve state-of-the-art results, with Dice coefficients of 82.81% for CBIS-DDSM and 84.11% for INbreast, respectively. We hope that our work will contribute to the CAD system for breast cancer diagnosis and ultimately improve clinical diagnosis.},
  archive      = {J_APIN},
  author       = {Zhao, Wenwei and Lou, Meng and Qi, Yunliang and Wang, Yiming and Xu, Chunbo and Deng, Xiangyu and Ma, Yide},
  doi          = {10.1007/s10489-021-02297-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8810-8827},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive channel and multiscale spatial context network for breast mass segmentation in full-field mammograms},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional neural networks and temporal CNNs for COVID-19
forecasting in france. <em>APIN</em>, <em>51</em>(12), 8784–8809. (<a
href="https://doi.org/10.1007/s10489-021-02359-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focus on multiple CNN-based (Convolutional Neural Network) models for COVID-19 forecast developed by our research team during the first French lockdown. In an effort to understand and predict both the epidemic evolution and the impacts of this disease, we conceived models for multiple indicators: daily or cumulative confirmed cases, hospitalizations, hospitalizations with artificial ventilation, recoveries, and deaths. In spite of the limited data available when the lockdown was declared, we achieved good short-term performances at the national level with a classical CNN for hospitalizations, leading to its integration into a hospitalizations surveillance tool after the lockdown ended. Also, A Temporal Convolutional Network with quantile regression successfully predicted multiple COVID-19 indicators at the national level by using data available at different scales (worldwide, national, regional). The accuracy of the regional predictions was improved by using a hierarchical pre-training scheme, and an efficient parallel implementation allows for quick training of multiple regional models. The resulting set of models represent a powerful tool for short-term COVID-19 forecasting at different geographical scales, complementing the toolboxes used by health organizations in France.},
  archive      = {J_APIN},
  author       = {Mohimont, Lucas and Chemchem, Amine and Alin, François and Krajecki, Michaël and Steffenel, Luiz Angelo},
  doi          = {10.1007/s10489-021-02359-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8784-8809},
  shortjournal = {Appl. Intell.},
  title        = {Convolutional neural networks and temporal CNNs for COVID-19 forecasting in france},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal graph inference network for scene graph
generation. <em>APIN</em>, <em>51</em>(12), 8768–8783. (<a
href="https://doi.org/10.1007/s10489-021-02304-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A scene graph can describe images concisely and structurally. However, existing methods of scene graph generation have low capabilities of inferring certain relationships, because of the lack of semantic information and their heavy dependence on the statistical distribution of the training set. To alleviate the above problems, a Multimodal Graph Inference Network (MGIN), which includes two modules; Multimodal Information Extraction (MIE) and Target with Multimodal Feature Inference (TMFI), is proposed in this study. MGIN can increase the inference capability of triplets, especially for uncommon samples. In the proposed MIE module, the prior statistical knowledge of the training set is incorporated into the network in a reprocess to relieve the problem of overfitting to the training set. Visual and semantic features are extracted in the MIE module and fused as unified multimodal features in the TMFI module. These features are efficient for the inference module to increase the prediction capability of MGIN, especially for some uncommon samples. The proposed method achieves 27.0% average mean recall and 55.9% average recall, with improvements of 0.48% and 0.50%, respectively, compared with state-of-the-art methods. It also increases the average recall of 20 relationships with the lowest probability by 4.91%.},
  archive      = {J_APIN},
  author       = {Duan, Jingwen and Min, Weidong and Lin, Deyu and Xu, Jianfeng and Xiong, Xin},
  doi          = {10.1007/s10489-021-02304-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8768-8783},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal graph inference network for scene graph generation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards efficient local search for the minimum total
dominating set problem. <em>APIN</em>, <em>51</em>(12), 8753–8767. (<a
href="https://doi.org/10.1007/s10489-021-02305-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an undirected graph G(V,E), the minimum total dominating set (MTDS) problem consists of finding a subset $D \subseteq V$ with the minimum vertices such that every vertex v ∈ V is adjacent to at least one vertex in D. That is, even for the vertices in D there should at least one neighbor in D. In this paper, we develop an efficient local search framework called LS_DTR to solve MTDS, which is with dynamic scoring function, tabu combined with configuration check, and balanced random walk. Firstly, a dynamic scoring function is presented to guide the search towards the promising solution space. Subsequently, the TaCC2 strategy combining tabu with two-level configuration checking is implemented to avoid visiting solutions repeatedly. Further, the balanced random walk strategy is applied to introduce the diversity into the search. Based on the three components, an efficient vertex selecting strategy is proposed. Finally, the vertex selecting strategy is applied to select the vertex to perform the remove or add operator during the local search. We use the commercial exact solver as the baseline and compare with the-state-of-art algorithm. Meanwhile, in order to verify the effectiveness of LS_DTR, we not only test on the DIMACS instances, but also extend the benchmark to the random general graphs and unit disk graphs. The results show that our algorithm LS_DTR outperforms the other algorithms on most instances.},
  archive      = {J_APIN},
  author       = {Hu, Shuli and Liu, Huan and Wang, Yupan and Li, Ruizhi and Yin, Minghao and Yang, Nan},
  doi          = {10.1007/s10489-021-02305-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8753-8767},
  shortjournal = {Appl. Intell.},
  title        = {Towards efficient local search for the minimum total dominating set problem},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A SHADE-based multimodal multi-objective evolutionary
algorithm with fitness sharing. <em>APIN</em>, <em>51</em>(12),
8720–8752. (<a
href="https://doi.org/10.1007/s10489-021-02299-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multimodal multi-objective optimization problems (MMOPs), at least two equivalent Pareto optimal solutions in decision space with an identical objective value are desired. The challenge for solving MMOPs is locating equivalent Pareto optimal solutions in decision space, and maintaining a fine balance between diversity and convergence of Pareto optimal solutions in both decision space and objective space, simultaneously. To address this issue, a success-history based parameter adaptation for multimodal multi-objective differential evolution algorithm using fitness sharing (MMOSHADE) is proposed in this paper. A success-history based parameter adaptation for differential evolution (SHADE) is integrated into MMOSHADE to find elite individuals and locate Pareto optimal solutions in decision space. Subsequently, a modified selection operation in differential evolution (DE) is introduced into MMOSHADE to explore outstanding convergence solutions. Furthermore, a double fitness sharing method is available for maintaining the diversity of Pareto optimal solutions in both decision space and objective space, simultaneously. The proposed MMOSHADE is performed on three categories of problems to test the performance of MMOSHADE. The comparison between MMOSHADE and six competing algorithms demonstrates the superiority of the proposed MMOSHADE in solving MMOPs and large-scale polygon-based MMOPs. MMOSHADE is also capable of finding the entire Pareto front in most cases when it is used to address multi-objective optimization problems. Additionally, the effectiveness of several strategies is validated by the designed experiments, and the parameters involved in MMOSHADE are discussed.},
  archive      = {J_APIN},
  author       = {Li, Guoqing and Wang, Wanliang and Chen, Haoli and You, Wenbo and Wang, Yule and Jin, Yawen and Zhang, Weiwei},
  doi          = {10.1007/s10489-021-02299-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8720-8752},
  shortjournal = {Appl. Intell.},
  title        = {A SHADE-based multimodal multi-objective evolutionary algorithm with fitness sharing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ELECTRE-II method for group decision-making in pythagorean
fuzzy environment. <em>APIN</em>, <em>51</em>(12), 8701–8719. (<a
href="https://doi.org/10.1007/s10489-021-02200-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a group decision support scheme that extends the widely accepted ELECTRE-II model to the Pythagorean fuzzy (PF) context. ELECTRE-II takes into consideration subjective human opinions, and establishes two types of embedded outranking relations which yield a complete preference ordering of actions. Its variants under different models have produced a considerable number of applications. Keeping in view the stronger capability of PF model to handle uncertain information, a group decision support algorithm, named as PF-ELECTRE-II is presented for opinions in the adaptable PF structure. A first phase aggregates the PF opinions of the experts on each alternative and criteria with the aid of suitable PF aggregation operators. Then, the algorithm introduces three types of PF outranking sets (concordance, indifferent and discordance sets), and establishes strong and weak outranking relations. The later are graphically represented by strong and weak outranking graphs, which are finally explored by a systematic iterative procedure that leads to the provision of a preferred system. We display these steps conducive to the PF-ELECTRE-II algorithm by a flow chart to facilitate its application. Finally, the decision results provided by this methodology in a supplier selection context are compared with those that available techniques put forward. A detailed comparative analysis is performed to establish the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Akram, Muhammad and Ilyas, Farwa and Garg, Harish},
  doi          = {10.1007/s10489-021-02200-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8701-8719},
  shortjournal = {Appl. Intell.},
  title        = {ELECTRE-II method for group decision-making in pythagorean fuzzy environment},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic reproductive ant colony algorithm based on piecewise
clustering. <em>APIN</em>, <em>51</em>(12), 8680–8700. (<a
href="https://doi.org/10.1007/s10489-021-02312-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the lack of convergence speed and diversity of Ant Colony Optimization (ACO), a dynamic reproductive ant colony algorithm based on piecewise clustering (RCACS) is proposed to optimize the problems. First, the data is segmented by the clustering algorithm, and the exit and entrance of each cluster are obtained by the nearest neighbor strategy. The algorithm traversed all points of the cluster to find an unclosed shortest path according to the exit and entrance. Many fragment paths eventually merge into a full TSP. This strategy can accelerate the convergence speed of the algorithm and help it get higher accuracy. Second, when the algorithm stagnates, the dynamic regeneration mechanism based on feature transfer will transfer the excellent features of the mother-ants to the child-ants, so that it can further explore the neighborhood of the current optimal solution and help the algorithm jump out of the local optimum. From the results of simulation experiments and the rank-sum test, it can be found that the improved algorithm can effectively improve the diversity and accuracy of the algorithm, especially when solving large-scale problems.},
  archive      = {J_APIN},
  author       = {Yu, Jin and You, Xiaoming and Liu, Sheng},
  doi          = {10.1007/s10489-021-02312-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8680-8700},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic reproductive ant colony algorithm based on piecewise clustering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved binary pigeon-inspired optimization and its
application for feature selection. <em>APIN</em>, <em>51</em>(12),
8661–8679. (<a
href="https://doi.org/10.1007/s10489-021-02302-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Pigeon-Inspired Optimization (PIO) algorithm is an intelligent algorithm inspired by the behavior of pigeons returned to the nest. The binary pigeon-inspired optimization (BPIO) algorithm is a binary version of the PIO algorithm, it can be used to optimize binary application problems. The transfer function plays a very important part in the BPIO algorithm. To improve the solution quality of the BPIO algorithm, this paper proposes four new transfer function, an improved speed update scheme, and a second-stage position update method. The original BPIO algorithm is easier to fall into the local optimal, so a new speed update equation is proposed. In the simulation experiment, the improved BPIO is compared with binary particle swarm optimization (BPSO) and binary grey wolf optimizer (BGWO). In addition, the benchmark test function, statistical analysis, Friedman’s test and Wilcoxon rank-sum test are used to prove that the improved algorithm is quite effective, and it also verifies how to set the speed of dynamic movement. Finally, feature selection was successfully implemented in the UCI data set, and higher classification results were obtained with fewer feature numbers.},
  archive      = {J_APIN},
  author       = {Pan, Jeng-Shyang and Tian, Ai-Qing and Chu, Shu-Chuan and Li, Jun-Bao},
  doi          = {10.1007/s10489-021-02302-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8661-8679},
  shortjournal = {Appl. Intell.},
  title        = {Improved binary pigeon-inspired optimization and its application for feature selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer learning of bayesian network for measuring QoS of
virtual machines. <em>APIN</em>, <em>51</em>(12), 8641–8660. (<a
href="https://doi.org/10.1007/s10489-021-02362-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Quality of Service (QoS) of virtual machines (VMs) are ensured through the Service Level Agreements (SLAs) signed between the consumers and the cloud providers. A main way to avoid the SLAs violation is to analyze the relationships among the multiple VM-related features and then measure the QoS of VMs accurately. Therefore, we first propose to construct a QoS Bayesian Network (QBN), so as to quantify the uncertain dependencies among the VM-related features and then measure the QoS of VMs effectively. Moreover, we show that the dynamical changes of hardware\software setting or the different types of loads will affect the measurement decisions of QBN. Thus, we further resort to the instance-based transfer learning and then propose a novel QBN updating method (QBNtransfer). QBNtransfer re-weights the constantly updated data instances, and then combine the Maximum Likelihood Estimation and the hill-climbing methods to revise the parameters and structures of QBN accordingly. The experiments conducted on the Alibaba published datasets and the benchmark running results on our simulated platform have shown that the QBN can measure the QoS of VMs accurately and QBNtransfer can update the QBN effectively.},
  archive      = {J_APIN},
  author       = {Hao, Jia and Yue, Kun and Zhang, Binbin and Duan, Liang and Fu, Xiaodong},
  doi          = {10.1007/s10489-021-02362-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8641-8660},
  shortjournal = {Appl. Intell.},
  title        = {Transfer learning of bayesian network for measuring QoS of virtual machines},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining colossal patterns with length constraints.
<em>APIN</em>, <em>51</em>(12), 8629–8640. (<a
href="https://doi.org/10.1007/s10489-021-02357-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining of colossal patterns is used to mine patterns in databases with many attributes and values, but the number of instances in each database is small. Although many efficient approaches for extracting colossal patterns have been proposed, they cannot be applied to colossal pattern mining with constraints. In this paper, we solve the challenge of extracting colossal patterns with length constraints. Firstly, we describe the problems of min-length constraint and max-length constraint and combine them with length constraints. After that, we evolve a proposal for efficiently truncating candidates in the mining process and another one for fast checking of candidates. Based on these properties, we offer the mining algorithm of Length Constraints for Colossal Pattern (LCCP) to extract colossal patterns with length constraints. Experiments are also conducted to show the effectiveness of the proposed LCCP algorithm with a comparison to some other ones.},
  archive      = {J_APIN},
  author       = {Le, Tuong and Nguyen, Thanh-Long and Huynh, Bao and Nguyen, Hung and Hong, Tzung-Pei and Snasel, Vaclav},
  doi          = {10.1007/s10489-021-02357-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8629-8640},
  shortjournal = {Appl. Intell.},
  title        = {Mining colossal patterns with length constraints},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximation of CIEDE2000 color closeness function using
neuro-fuzzy networks. <em>APIN</em>, <em>51</em>(12), 8613–8628. (<a
href="https://doi.org/10.1007/s10489-021-02326-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of efficient algorithms that perform qualitative operations in the processing and segmentation of graphical images requires good knowledge on both technical and biological aspects of the vision. Due to the subjective nature of the human vision, the evaluation of color identity and closeness, that has been widely used in a variety of image processing problems, is still subjected to research and improvement. This paper analyzes the problem of color tolerance, overviewing past and existing solutions, and suggests a simple Adaptive Neuro-Fuzzy Inference System (ANFIS)-based model as an alternative to the formula-based closeness evaluation. The aim of this research was to develop an alternative to the unified closeness formula, which could be built on expert knowledge or be adapted to fit a particular target. Methods of sample selection, training approaches, and configuration of the ANFIS network were used and a comparative analysis between the ANFIS models and the neural networks was provided. Additionally, the efficiency of regular back-propagation, hybrid, and stochastic learning methods were evaluated along with a critical analysis on the widely used hybrid training method. Experiments were designed and ran on a Java-based platform-independent system, developed as part of our research, to provide flexibility in the modeling and integration for the ANFIS based tasks.},
  archive      = {J_APIN},
  author       = {Hasanov, Jamaladdin and Garibov, Samir and Javadov, Aydin},
  doi          = {10.1007/s10489-021-02326-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8613-8628},
  shortjournal = {Appl. Intell.},
  title        = {Approximation of CIEDE2000 color closeness function using neuro-fuzzy networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring the outcome of movement-based three-way decision
using proportional utility functions. <em>APIN</em>, <em>51</em>(12),
8598–8612. (<a
href="https://doi.org/10.1007/s10489-021-02325-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trisecting-acting-outcome (TAO) model of three-way decisions includes trisecting a universal set into three separate and closely connected regions, devising, and applying efficient strategies on the three regions, furthermore evaluating the outcome. This paper introduces the proportional utility function (PUF), representing the ratio between an object’s initial and final quantity, to measure the outcomes from two different perspectives for movement-based three-way decision. The first perspective, is that if each object produces the same benefits or costs when they have the same movement (call region-independent), we sum up the three regions’ utility as the overall outcome. The second scenario, is that each object generates a different cost or benefit, even if they have the same movements (call region-dependent). Here, finding an optimal investment plan is an essential matter based on their equivalent classification according to their specific characteristics. For a single equivalence class, we design a strategy to reach the goal under a specific investment, although this investment may be conservative. For all equivalence classes, we give one-time optimal investment plans using PUF based on invested or reserved resources in the movement-based three-way decision. Based on the program, we adopt a series of strategies in action under these investment budgets. The experimental results show that our strategy choices have practical significance and are in line with expected results.},
  archive      = {J_APIN},
  author       = {Jiang, Chunmao and Guo, Doudou and Xu, Ruiyang},
  doi          = {10.1007/s10489-021-02325-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8598-8612},
  shortjournal = {Appl. Intell.},
  title        = {Measuring the outcome of movement-based three-way decision using proportional utility functions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 prediction using AI analytics for south korea.
<em>APIN</em>, <em>51</em>(12), 8579–8597. (<a
href="https://doi.org/10.1007/s10489-021-02352-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severe spread of the COVID-19 pandemic has created a situation of public health emergency and global awareness. In our research, we analyzed the demographical factors affecting the global pandemic spread along with the features that lead to death due to the infection. Modeling results stipulate that the mortality rate increase as the age increase and it is found that most of the death cases belong to the age group 60–80. Cluster-based analysis of age groups is also conducted to analyze the maximum targeted age-groups. An association between positive COVID-19 cases and deceased cases are also presented, with the impact on male and female death cases due to corona. Additionally, we have also presented an artificial intelligence-based statistical approach to predict the survival chances of corona infected people in South Korea with the analysis of the impact on the exploratory factors, including age-groups, gender, temporal evolution, etc. To analyze the coronavirus cases, we applied machine learning with hyperparameters tuning and deep learning models with an autoencoder-based approach for estimating the influence of the disparate features on the spread of the disease and predict the survival possibilities of the quarantined patients in isolation. The model calibrated in the study is based on positive corona infection cases and presents the analysis over different aspects that proven to be impactful to analyze the temporal trends in the current situation along with the exploration of deceased cases due to coronavirus. Analysis delineates key points in the outbreak spreading, indicating that the models driven by machine intelligence and deep learning can be effective in providing a quantitative view of the epidemical outbreak.},
  archive      = {J_APIN},
  author       = {Sinha, Adwitiya and Rathi, Megha},
  doi          = {10.1007/s10489-021-02352-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8579-8597},
  shortjournal = {Appl. Intell.},
  title        = {COVID-19 prediction using AI analytics for south korea},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level spatial and semantic enhancement network for
expression recognition. <em>APIN</em>, <em>51</em>(12), 8565–8578. (<a
href="https://doi.org/10.1007/s10489-021-02254-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) on real world databases is an active and challenging research topic. Existing CNN-based facial expression classifiers usually have good performance on common expressions, including happy and surprise, but have lower accuracy on difficult expressions, such as disgust and fear. Two main factors are responsible for this problem. Firstly, intra-class variation makes classification of difficult expressions more complex than other expressions. Secondly, severe data imbalance of difficult expressions in most FER datasets leads to overfitting during training. In this work, a new network architecture is proposed to address the intra-class variation problem. The proposed model consists of a spatial enhancement module and a semantic aggregation module to enhance fine-level expression features and high-level semantic features. To alleviate the data imbalance problem, an iterative learning method is introduced to collect difficult expression samples. New samples with inconsistent labels are classified by using a fuzzy clustering algorithm. The proposed FER framework has been evaluated on three real world expression datasets. Experimental results demonstrate that the proposed method significantly improved the recognition accuracy of difficult expressions and achieved top performance compared with state-of-the-art works.},
  archive      = {J_APIN},
  author       = {Ma, Yingdong and Wang, Xia and Wei, Lihua},
  doi          = {10.1007/s10489-021-02254-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {8565-8578},
  shortjournal = {Appl. Intell.},
  title        = {Multi-level spatial and semantic enhancement network for expression recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint latent low-rank and non-negative induced sparse
representation for face recognition. <em>APIN</em>, <em>51</em>(11),
8349–8364. (<a
href="https://doi.org/10.1007/s10489-021-02338-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation-based methods have achieved exciting results in recent applications of face recognition. However, it is still challenging for the face recognition task due to noise and outliers in the data. Many existing methods avoid these problems by constructing an auxiliary dictionary from the extended data but fail to achieve good performances because they use the main dictionary only for classification. In this paper, to avoid the need to manually construct an auxiliary dictionary and the effects of noise, we propose a Joint Latent Low-Rank and Non-Negative Induced Sparse Representation (JLSRC) for face recognition. Specifically, JLSRC adaptively learns two clean low-rank reconstructed dictionaries jointly via an extended latent low-rank representation to reveal the potential relationships in the data and then embeds a non-negative constraint and an Elastic Net regularization in the coefficient vectors of the dictionaries to enhance the performance on classification. In this way, the learned low-rank dictionaries can be mutually boosted to extract discriminative features and handle the noise, and the obtained coefficient vectors are simultaneously both sparse and discriminative. Moreover, the proposed method seamlessly and elegantly integrates low-rank learning and sparse representation-based classification. Extensive experiments on three challenging face databases demonstrate the effectiveness and robustness of JLSRC in comparison with the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wu, Mingna and Wang, Shu and Li, Zhigang and Zhang, Long and Wang, Ling and Ren, Zhenwen},
  doi          = {10.1007/s10489-021-02338-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8349-8364},
  shortjournal = {Appl. Intell.},
  title        = {Joint latent low-rank and non-negative induced sparse representation for face recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A conditional classification recurrent RBM for improved
series mid-term forecasting. <em>APIN</em>, <em>51</em>(11), 8334–8348.
(<a href="https://doi.org/10.1007/s10489-021-02315-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of human-robot and robot-robot interactions, the better cooperation can be achieved by predicting the other party’s subsequent actions based on the current action of the other party. The time duration for adjustment is not sufficient provided by short term forecasting models to robots. A longer duration can by achieved by mid-term forecasting. But the mid-term forecasting models introduce the previous errors into the follow-up forecasting and amplified gradually, eventually invalidating the forecasting. A new mid-term forecasting with error suppression based on restricted Boltzmann machine(RBM) is proposed in this paper. The proposed model can suppress the error amplification by replacing the previous inputs with their features, which are retrieved by a deep belief network(DBN). Furthermore, a new mechanism is proposed to decide whether the forecasting result is accepted or not. The model is evaluated with several datasets. The reported experiments demonstrate the superior performance of the proposed model compared to the state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Xia, Lei and Lv, Jiancheng and Xie, Chunzhi and Yin, Jing},
  doi          = {10.1007/s10489-021-02315-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8334-8348},
  shortjournal = {Appl. Intell.},
  title        = {A conditional classification recurrent RBM for improved series mid-term forecasting},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling polypharmacy effects with heterogeneous signed
graph convolutional networks. <em>APIN</em>, <em>51</em>(11), 8316–8333.
(<a href="https://doi.org/10.1007/s10489-021-02296-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pharmaceutical drug combinations can effectively treat various medical conditions. However, some combinations can cause serious adverse drug reactions (ADR). Therefore, predicting ADRs is an essential and challenging task. Some existing studies rely on single-modal information, such as drug-drug interaction or drug-drug similarity, to predict ADRs. However, those approaches ignore relationships among multi-source information. Other studies predict ADRs using integrated multi-modal drug information; however, such studies generally describe these relations as heterogeneous unsigned networks rather than signed ones. In fact, multi-modal relations of drugs can be classified as positive or negative. If these two types of relations are depicted simultaneously, semantic correlation of drugs in the real world can be predicted effectively. Therefore, in this study, we propose an innovative heterogeneous signed network model called SC-DDIS, to learn drug representations. SC-DDIS integrates multi-modal features, such as drug-drug interactions, drug-protein interactions, drug-chemical interactions, and other heterogeneous information, into drug embedding. Drug embedding means using feature vectors to express drugs. Then, the SC-DDIS model is also used for ADR prediction tasks. First, we fuse heterogeneous drug relations, positive/negative, to obtain a drug-drug interaction signed network (DDISN). Then, inspired by social network, we extend structural balance theory and apply it to DDISN. Using extended structural balance theory, we constrain sign propagation in DDISN. We learn final embedding of drugs by training a graph spectral convolutional neural network. Finally, we train a decoding matrix to decode the drug embedding to predict ADRs. Experimental results demonstrate effectiveness of the proposed model compared to several conventional multi-relational prediction approaches and the state-of-the-art deep learning-based Decagon model.},
  archive      = {J_APIN},
  author       = {Liu, Taoran and Cui, Jiancong and Zhuang, Hui and Wang, Hong},
  doi          = {10.1007/s10489-021-02296-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8316-8333},
  shortjournal = {Appl. Intell.},
  title        = {Modeling polypharmacy effects with heterogeneous signed graph convolutional networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting earthquakes: A novel deep learning-based approach
for effective disaster response. <em>APIN</em>, <em>51</em>(11),
8305–8315. (<a
href="https://doi.org/10.1007/s10489-021-02285-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present study, we present an intelligent earthquake signal detector that provides added assistance to automate traditional disaster responses. To effectively respond in a crisis scenario, additional sensors and automation are always necessary. Deep learning has achieved success in various low signal-to-noise ratio tasks, which motivated us to propose a novel 3-dimensional (3D) CNN-RNN-based earthquake detector from a demonstration paradigm to real-time implementation. Data taken from the ST anford EA rthquake D ataset (STEAD) are used to train the network. After preprocessing the raw earthquake signals, features such as log-mel spectrograms are extracted. Once the model has learned spatial and temporal information from low-frequency earthquake waves, it can be employed in real time to distinguish small and large earthquakes from seismic noise with an accuracy, sensitivity, and specificity of 99.057%, 98.488%, and 99.621%, respectively. We also observe that the choice of filters in log-mel spectrogram impacts the results much more than the model complexity. Furthermore, we implement and test the model on data collected continuously over two months by a personal seismometer in the laboratory. The inference speed for a single prediction is 2.27 seconds, and the system delivers a stable detection of all 63 major earthquakes from November 2019 to December 2019 reported by the Japan Meteorological Agency.},
  archive      = {J_APIN},
  author       = {Shakeel, Muhammad and Itoyama, Katsutoshi and Nishida, Kenji and Nakadai, Kazuhiro},
  doi          = {10.1007/s10489-021-02285-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8305-8315},
  shortjournal = {Appl. Intell.},
  title        = {Detecting earthquakes: A novel deep learning-based approach for effective disaster response},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using distributed ledger technology to democratize neural
network training. <em>APIN</em>, <em>51</em>(11), 8288–8304. (<a
href="https://doi.org/10.1007/s10489-021-02340-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence has regained research interest, primarily because of big data. Internet expansion, social networks and online sensors led to the generation of an enormous amount of information daily. This unprecedented data availability boosted Machine Learning. A research area that has greatly benefited from this fact is Deep Neural Networks. Nowadays many use cases require huge models with millions of parameters and big data are proven to be essential to their proper training. The scientific community has proposed several methods to generate more accurate models. Usually, these methods need high performance infrastructure, which limits their applicability to large organizations and institutions that have the required funds. Another source of concern is privacy; anyone using the leased processing power of a remote data center, must trust another entity with their data. Unfortunately, in many cases sensitive data were leaked, either for financial exploitation or due to security issues. However, there is a lack of research studies when it comes to open communities of individuals with commodity hardware, who wish to join forces in a way that is non-binding and without the need for a central authority. Our work on LEARNAE attempts to fill this gap, by creating a way of providing training in Artificial Neural Networks, featuring decentralization, data ownership and fault tolerance. This article adds some important pieces to the puzzle: It studies the resilience of LEARNAE when dealing with network disruptions and proposes a novel way of embedding low-energy sensors that reside in the Internet of Things domain, retaining at the same time the established distributed philosophy.},
  archive      = {J_APIN},
  author       = {Nikolaidis, Spyridon and Refanidis, Ioannis},
  doi          = {10.1007/s10489-021-02340-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8288-8304},
  shortjournal = {Appl. Intell.},
  title        = {Using distributed ledger technology to democratize neural network training},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction of software vulnerability based deep symbiotic
genetic algorithms: Phenotyping of dominant-features. <em>APIN</em>,
<em>51</em>(11), 8271–8287. (<a
href="https://doi.org/10.1007/s10489-021-02324-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of software vulnerabilities is considered a vital problem in the software security area for a long time. Nowadays, it is challenging to manage software security due to its increased complexity and diversity. So, vulnerability detection applications play a significant part in software development and maintenance. The ability of the forecasting techniques in vulnerability detection is still weak. Thus, one of the efficient defining features methods that have been used to determine the software vulnerabilities is the metaheuristic optimization methods. This paper proposes a novel software vulnerability prediction model based on using a deep learning method and SYMbiotic Genetic algorithm. We are first to apply Diploid Genetic algorithms with deep learning networks on software vulnerability prediction to the best of our knowledge. In this proposed method, a deep SYMbiotic-based genetic algorithm model (DNN-SYMbiotic GAs) is used by learning the phenotyping of dominant-features for software vulnerability prediction problems. The proposed method aimed at increasing the detection abilities of vulnerability patterns with vulnerable components in the software. Comprehensive experiments are conducted on several benchmark datasets; these datasets are taken from Drupal, Moodle, and PHPMyAdmin projects. The obtained results revealed that the proposed method (DNN-SYMbiotic GAs) enhanced vulnerability prediction, which reflects improving software quality prediction.},
  archive      = {J_APIN},
  author       = {Şahin, Canan Batur and Dinler, Özlem Batur and Abualigah, Laith},
  doi          = {10.1007/s10489-021-02324-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8271-8287},
  shortjournal = {Appl. Intell.},
  title        = {Prediction of software vulnerability based deep symbiotic genetic algorithms: Phenotyping of dominant-features},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of vertical eye movements in parkinson’s disease
and its potential for diagnosis. <em>APIN</em>, <em>51</em>(11),
8260–8270. (<a
href="https://doi.org/10.1007/s10489-021-02364-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that eye movements are highly affected by Parkinson’s disease. The majority of studies related to effects of Parkinson’s disease on eye movements have been performed for rapid eye movements during sleep. However, in the current study, eye movements during resting-state (eyes-open and eyes-closed conditions) were studied to evaluate Parkinson’s disease. To measure eye movements, vertical electrooculography (VEOG) was used. Using extracted features in time-, frequency-, and time-frequency domains of VEOG time-series, a classification analysis between healthy subjects and Parkinson’s disease patients in OFF- and ON-medication states was performed. The most-informative features for an error-correcting output codes support vector machine classifier were selected according to the multiple-comparison corrected p-values. VEOG data obtained 69.10 % and 87.27 % discrimination accuracy for OFF- and ON-medication states, respectively. Interestingly, higher discrimination was obtained for the lower frequency contents of VEOG time-series (0.1–1.25 Hz). The most discriminative features were related to the variation of amplitude and frequency content for OFF-medication, while in ON-medication the features according to the variation of VEOG amplitudes were removed from the most discriminative features list. Based on the obtained results, it was concluded that vertical eye movements of Parkinson’s disease patients had lower amplitude variation compared with healthy subjects in OFF-medication, while levodopa prescription increased such variation in vertical eye movements during the eyes-closed condition and decreased the variation during the eyes-open condition. Levodopa prescription possibly affects the amplitude variation of VEOG time-series, while it had no effect on the movement rates (frequency contents) of vertical eye movements.},
  archive      = {J_APIN},
  author       = {Farashi, Sajjad},
  doi          = {10.1007/s10489-021-02364-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8260-8270},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of vertical eye movements in parkinson’s disease and its potential for diagnosis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multichannel environmental sound segmentation.
<em>APIN</em>, <em>51</em>(11), 8245–8259. (<a
href="https://doi.org/10.1007/s10489-021-02314-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multichannel environmental sound segmentation method. Environmental sound segmentation is an integrated method to achieve sound source localization, sound source separation and classification, simultaneously. When multiple microphones are available, spatial features can be used to improve the localization and separation accuracy of sounds from different directions; however, conventional methods have three drawbacks: (a) Sound source localization and sound source separation methods using spatial features and classification using spectral features trained in the same neural network, may overfit to the relationship between the direction of arrival and the class of a sound, thereby reducing their reliability to deal with novel events. (b) Although permutation invariant training used in autonomous speech recognition could be extended, it is impractical for environmental sounds that include an unlimited number of sound sources. (c) Various features, such as complex values of short time Fourier transform and interchannel phase differences have been used as spatial features, but no study has compared them. This paper proposes a multichannel environmental sound segmentation method comprising two discrete blocks, a sound source localization and separation block and a sound source separation and classification block. By separating the blocks, overfitting to the relationship between the direction of arrival and the class is avoided. Simulation experiments using created datasets including 75-class environmental sounds showed the root mean squared error of the proposed method was lower than that of conventional methods.},
  archive      = {J_APIN},
  author       = {Sudo, Yui and Itoyama, Katsutoshi and Nishida, Kenji and Nakadai, Kazuhiro},
  doi          = {10.1007/s10489-021-02314-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8245-8259},
  shortjournal = {Appl. Intell.},
  title        = {Multichannel environmental sound segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection accelerated convolutional neural networks
for visual tracking. <em>APIN</em>, <em>51</em>(11), 8230–8244. (<a
href="https://doi.org/10.1007/s10489-021-02234-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing tracking methods based on convolutional neural network (CNN) models are too slow for use in real-time applications despite their excellent tracking accuracy in comparison with traditional methods. Meanwhile, CNN tracking solutions are memory intensive and require considerable computational resources. In this paper, we propose a time-efficient and accurate tracking scheme, a feature selection accelerated CNN (FSNet) tracking solution based on MDNet (Multi-Domain Network). The large number of convolutional operations is a major contributor to the high computational cost of MDNet. To reduce the computational complexity, we incorporated an efficient mutual information-based feature selection over the convolutional layer that reduces the feature redundancy in feature maps. Considering that tracking is a typical binary classification problem, redundant feature maps can simply be pruned, which results in an insignificant influence on the tracking performance. To further accelerate the CNN tracking solution, a RoIAlign layer is added that can apply convolution to the entire image instead of just to each RoI (Region of Interest). The bilinear interpolation of RoIAlign could well reduce misalignment errors of the tracked target. In addition, a new fine-tuning strategy is used in the fully-connected layers to accelerate the online updating process. By combining the above strategies, the accelerated CNN achieves a speedup to 60 FPS (Frame Per Second) on the GPU compared with the original MDNet, which functioned at 1 FPS with a very low impact on tracking accuracy. We evaluated the proposed solution on four benchmarks: OTB50, OTB100 ,VOT2016 and UAV123. The extensive comparison results verify the superior performance of FSNet.},
  archive      = {J_APIN},
  author       = {Cui, Zhiyan and Lu, Na},
  doi          = {10.1007/s10489-021-02234-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8230-8244},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection accelerated convolutional neural networks for visual tracking},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel two-stage constraints handling framework for
real-world multi-constrained multi-objective optimization problem based
on evolutionary algorithm. <em>APIN</em>, <em>51</em>(11), 8212–8229.
(<a href="https://doi.org/10.1007/s10489-020-02174-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-constrained multi-objective optimization is a challenging topic, which is very common in dealing with real-world problems. This paper proposes a novel two-stage ρg / μg framework based on multi-objective evolutionary algorithm (MOEA) to solve the multi-constrained multi-objective optimization problems (MCMOPs), which dynamically balances the diversity and convergence of solutions. During the multi-constraints handling process, ρg / μg -MOEA makes the reduction of violated constraints as its primary goal, and converges to feasible regions by a proposed ρg -criterion based constraints relaxation method. Moreover, in the late stage of evolution, by introducing the improved dynamic stochastic ranking (DSR) strategy, the “potential” infeasible individuals are utilized to find more feasible regions, which would guarantee a good distribution of the obtained Pareto frontiers. Thereafter, the proposed framework combined with non-dominated sorting genetic algorithm II (NSGAII) is applied to ten benchmark functions and a series of real-world MCMOPs, and the performances are compared with those obtained by some state-of-the-art constraints handling methods. Experimental results indicate that the proposed ρg / μg framework outperforms the current efficient methods in dealing with test CMOPs, and can achieve satisfactory results when solving real-world MCMOPs.},
  archive      = {J_APIN},
  author       = {Li, Xin and An, Qing and Zhang, Jun and Xu, Fan and Tang, Ruoli and Dong, Zhengcheng and Zhang, Xiaodi and Lai, Jingang and Mao, Xiaobing},
  doi          = {10.1007/s10489-020-02174-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8212-8229},
  shortjournal = {Appl. Intell.},
  title        = {A novel two-stage constraints handling framework for real-world multi-constrained multi-objective optimization problem based on evolutionary algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximizing a deep submodular function optimization with a
weighted MAX-SAT problem for trajectory clustering and motion
segmentation. <em>APIN</em>, <em>51</em>(11), 8192–8211. (<a
href="https://doi.org/10.1007/s10489-021-02276-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision models are commonly defined for maximum constrained submodular functions lies at the core of low-level and high-level models. In such that, the pixels that are to be grouped or segmenting moving object remains a challenging task. This paper proposes a joint framework for maximizing submodular energy subject to a matroid constraint using Deep Submodular Function (DSF) optimization approximately to solve the weighted MAX-SAT (Maximum Satisfiability) problem and a new trajectory clustering method called Simple Slice Linear clustering (SSLIC) and motion cue method for trajectory clustering and motion segmentation. In this objective function, the illustrative trajectories of a small number are selected automatically by deep submodular maximization. Although, the exploitation of monotone and submodular properties are further maximized and the complexity is reduced by a continuous greedy algorithm. The bound guarantees a fully sliced curve of (1- S/e) to (1–1/e) with less running time. Lastly, the motion is segmented by the motion cue method to accurately differentiate the set of frames for different scenes. Experiments on the Hopkins 155, Berkley Motion Segmentation (BMS) and FBMS-59 datasets display the trajectory clustering and motion segmentation result over its superior performance with respect to 14 quality evaluation metrics. Hence the simulation result shows that the proposed joint framework attains better performance than existing methods on trajectory clustering and motion segmentation task.},
  archive      = {J_APIN},
  author       = {Chandriah, Kiran Kumar and Naraganahalli, Raghavendra V},
  doi          = {10.1007/s10489-021-02276-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8192-8211},
  shortjournal = {Appl. Intell.},
  title        = {Maximizing a deep submodular function optimization with a weighted MAX-SAT problem for trajectory clustering and motion segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor projection mechanism and algorithm implementation.
<em>APIN</em>, <em>51</em>(11), 8176–8191. (<a
href="https://doi.org/10.1007/s10489-021-02332-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyses the mechanism of tensor projection transformation in depth and introduces a high-efficiency original algorithm developed in a quantum computing language for forward and backward projection between multidimensional tensors and one-dimensional vectors. Additionally, the author compares this algorithm with similar methods from both the Python scientific computing package and other relative development kits in method calls and source code to demonstrate the innovation of the tensor projection algorithm. On this basis, the classical convolution operation program commonly used in machine learning has been parallelized and improved, the analysis algorithm of the Beidou communication satellite view area has been parallelized and improved, and the actual operating efficiency has been greatly improved. After verification, the tensor projection transformation successfully solves the problem of location index mapping of the entity units among different dimensions, can provide a means for optimizing the traditional model of traversal algorithm, and can have significant reference value in eigenspace transformation against a tensor field.},
  archive      = {J_APIN},
  author       = {Wang, Fei and Wang, Zhanchang and Li, Xiang and Chen, Yu},
  doi          = {10.1007/s10489-021-02332-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8176-8191},
  shortjournal = {Appl. Intell.},
  title        = {Tensor projection mechanism and algorithm implementation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A heuristic and reliable track-to-track data association
approach for multi-cell track reconstruction. <em>APIN</em>,
<em>51</em>(11), 8162–8175. (<a
href="https://doi.org/10.1007/s10489-021-02209-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To encompass irregular cell movement and mitotic events, we present a novel cell track-to-track association approach that rebuilds lineage trees through the pheromone field of a proposed ant colony optimization. With the constraint of maximum inter-frame displacement, the algorithm can link potential tracks by minimizing the proposed cost function considering both cell motion and morphology that mainly occurs on the fragmented intervals. Two different decisions are defined for ant colonies to predict mitotic and non-mitotic events used to construct relevant trail pheromone fields. A novel subsequent processing technique including threshold processing, trail merging and identity fusion is ultimately proposed, that makes full use of the spatial information of the pheromone field to realize track-to-track association. The proposed method has proven to be feasible and reliable on several challenging datasets.},
  archive      = {J_APIN},
  author       = {Wu, Di and Xu, Benlian and Lu, Mingli},
  doi          = {10.1007/s10489-021-02209-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8162-8175},
  shortjournal = {Appl. Intell.},
  title        = {A heuristic and reliable track-to-track data association approach for multi-cell track reconstruction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-optimized coupled discriminant projections for
cross-view gait recognition. <em>APIN</em>, <em>51</em>(11), 8149–8161.
(<a href="https://doi.org/10.1007/s10489-021-02322-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-Embedding is a widely used learning technique in pattern recognition. However, it is difficult to construct an inter-view graph for cross-view gait samples. To remedy it, in this paper, we propose a novel cross-view gait recognition algorithm named Graph-optimized Coupled Discriminant Projections (GoCDP), which seeks coupled projections based on adaptive graph learning. Regarding the embedding graphs as variables rather than predefined constants, we integrate inter-view graph construction with projection optimization process into a unified framework. By an alternate iteration algorithm, we can ultimately obtain the optimal coupled projections. Moreover, we extend GoCDP to multi-view case called Graph-optimized Multiview Discriminant Projections (GoMDP) for multi-view subspace learning. Experimental results on two benchmark gait datasets, CASIA-B and OU-ISIR, demonstrate the effectiveness of the proposed methods.},
  archive      = {J_APIN},
  author       = {Xu, Wanjiang},
  doi          = {10.1007/s10489-021-02322-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8149-8161},
  shortjournal = {Appl. Intell.},
  title        = {Graph-optimized coupled discriminant projections for cross-view gait recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ConvNet frameworks for multi-modal fake news detection.
<em>APIN</em>, <em>51</em>(11), 8132–8148. (<a
href="https://doi.org/10.1007/s10489-021-02345-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An upsurge of false information revolves around the internet. Social media and websites are flooded with unverified news posts. These posts are comprised of text, images, audio, and videos. There is a requirement for a system that detects fake content in multiple data modalities. We have seen a considerable amount of research on classification techniques for textual fake news detection, while frameworks dedicated to visual fake news detection are very few. We explored the state-of-the-art methods using deep networks such as CNNs and RNNs for multi-modal online information credibility analysis. They show rapid improvement in classification tasks without requiring pre-processing. To aid the ongoing research over fake news detection using CNN models, we build textual and visual modules to analyze their performances over multi-modal datasets. We exploit latent features present inside text and images using layers of convolutions. We see how well these convolutional neural networks perform classification when provided with only latent features and analyze what type of images are needed to be fed to perform efficient fake news detection. We propose a multi-modal Coupled ConvNet architecture that fuses both the data modules and efficiently classifies online news depending on its textual and visual content. We thence offer a comparative analysis of the results of all the models utilized over three datasets. The proposed architecture outperforms various state-of-the-art methods for fake news detection with considerably high accuracies.},
  archive      = {J_APIN},
  author       = {Raj, Chahat and Meel, Priyanka},
  doi          = {10.1007/s10489-021-02345-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8132-8148},
  shortjournal = {Appl. Intell.},
  title        = {ConvNet frameworks for multi-modal fake news detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Portfolio management system in equity market neutral using
reinforcement learning. <em>APIN</em>, <em>51</em>(11), 8119–8131. (<a
href="https://doi.org/10.1007/s10489-021-02262-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio management involves position sizing and resource allocation. Traditional and generic portfolio strategies require forecasting of future stock prices as model inputs, which is not a trivial task since those values are difficult to obtain in the real-world applications. To overcome the above limitations and provide a better solution for portfolio management, we developed a Portfolio Management System (PMS) using reinforcement learning with two neural networks (CNN and RNN). A novel reward function involving Sharpe ratios is also proposed to evaluate the performance of the developed systems. Experimental results indicate that the PMS with the Sharpe ratio reward function exhibits outstanding performance, increasing return by 39.0% and decreasing drawdown by 13.7% on average compared to the reward function of trading return. In addition, the proposed PMS_CNN model is more suitable for the construction of a reinforcement learning portfolio, but has 1.98 times more drawdown risk than the PMS_RNN. Among the conducted datasets, the PMS outperforms the benchmark strategies in TW50 and traditional stocks, but is inferior to a benchmark strategy in the financial dataset. The PMS is profitable, effective, and offers lower investment risk among almost all datasets. The novel reward function involving the Sharpe ratio enhances performance, and well supports resource-allocation for empirical stock trading.},
  archive      = {J_APIN},
  author       = {Wu, Mu-En and Syu, Jia-Hao and Lin, Jerry Chun-Wei and Ho, Jan-Ming},
  doi          = {10.1007/s10489-021-02262-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8119-8131},
  shortjournal = {Appl. Intell.},
  title        = {Portfolio management system in equity market neutral using reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using extended siamese networks to provide decision support
in aquaculture operations. <em>APIN</em>, <em>51</em>(11), 8107–8118.
(<a href="https://doi.org/10.1007/s10489-021-02251-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aquaculture as an industry is quickly expanding. As a result, new aquaculture sites are being established at more exposed locations previously deemed unfit because they are more difficult and resource demanding to safely operate than are traditional sites. To help the industry deal with these challenges, we have developed a decision support system to support decision makers in establishing better plans and make decisions that facilitate operating these sites in an optimal manner. We propose a case-based reasoning system called aquaculture case-based reasoning (AQCBR), which is able to predict the success of an aquaculture operation at a specific site, based on previously applied and recorded cases. In particular, AQCBR is trained to learn a similarity function between recorded operational situations/cases and use the most similar case to provide explanation-by-example information for its predictions. The novelty of AQCBR is that it uses extended Siamese neural networks to learn the similarity between cases. Our extensive experimental evaluation shows that extended Siamese neural networks outperform state-of-the-art methods for similarity learning in this task, demonstrating the effectiveness and the feasibility of our approach.},
  archive      = {J_APIN},
  author       = {Mathisen, Bjørn Magnus and Bach, Kerstin and Aamodt, Agnar},
  doi          = {10.1007/s10489-021-02251-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8107-8118},
  shortjournal = {Appl. Intell.},
  title        = {Using extended siamese networks to provide decision support in aquaculture operations},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Where do university graduates live? – a
computer vision approach using satellite images. <em>APIN</em>,
<em>51</em>(11), 8106. (<a
href="https://doi.org/10.1007/s10489-021-02433-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Correction to this paper has been published: https://doi.org/10.1007/s10489-021-02433-z},
  archive      = {J_APIN},
  author       = {Koch, David and Despotovic, Miroslav and Thaler, Simon and Zeppelzauer, Matthias},
  doi          = {10.1007/s10489-021-02433-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8106},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Where do university graduates live? – a computer vision approach using satellite images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Where do university graduates live? – a computer vision
approach using satellite images. <em>APIN</em>, <em>51</em>(11),
8088–8105. (<a
href="https://doi.org/10.1007/s10489-021-02268-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we examine to what extent the settlement of university graduates can be derived from satellite images. We apply a convolutional neural network (CNN) to grid images of a city and predict five density classes of university graduates at a micro level (250 m × 250 m grid size). The CNN reaches an accuracy rate of 40.5% (random approach: 20%). Furthermore, the accuracy increases to 78.3% when considering a one-class deviation compared to the true class. We also examine the predictability of inhabited and uninhabited grid cells, where we achieve an accuracy of 95.3% using the same CNN. From this, we conclude that there is information that correlates with graduate density that can be derived by analysing only satellite images. The findings show the high potential of computer vision for urban and regional economics. Particularly in data-poor regions, the approach utilised facilitates comparative analytics and provides a possible solution for the modifiable aerial unit (MAU) problem. The MAU problem is a statistical bias that can influence the results of a spatial data analysis of point-estimate data that is aggregated in districts of different shapes and sizes, distorting the results.},
  archive      = {J_APIN},
  author       = {Koch, David and Despotovic, Miroslav and Thaler, Simon and Zeppelzauer, Matthias},
  doi          = {10.1007/s10489-021-02268-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8088-8105},
  shortjournal = {Appl. Intell.},
  title        = {Where do university graduates live? – a computer vision approach using satellite images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overlapping community detection using core label propagation
algorithm and belonging functions. <em>APIN</em>, <em>51</em>(11),
8067–8087. (<a
href="https://doi.org/10.1007/s10489-021-02250-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The community detection in complex networks has become a major field of research. Disjoint community detection deals often with getting a partition of nodes where every node belongs to only one community. However, in social networks, individuals may belong to more than one community such as in co-purchasing field, a co-authorship of scientist papers or anthropological networks. We propose in this paper a method to find overlapping communities from pre-computed disjoint communities obtained by using the core detection label propagation. The algorithm selects candidates nodes for overlapping and uses belonging functions to decide the assignment or not of a candidate node to each of its neighbours communities. we propose and experiment in this paper several belonging functions, all based on the topology of the communities. These belonging functions are either based on global measures which are the density and the clustering coefficient or on average node measures which are the betweenness and the closeness centralities. We expose then a new similarity measure between two covers regarding the overlapping nodes. The goal is to assess the similarity between two covers that overlap several communities. We finally propose a comparative analysis with the literature algorithms.},
  archive      = {J_APIN},
  author       = {Attal, Jean-Philippe and Malek, Maria and Zolghadri, Marc},
  doi          = {10.1007/s10489-021-02250-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8067-8087},
  shortjournal = {Appl. Intell.},
  title        = {Overlapping community detection using core label propagation algorithm and belonging functions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural cryptography using optimal structure of neural
networks. <em>APIN</em>, <em>51</em>(11), 8057–8066. (<a
href="https://doi.org/10.1007/s10489-021-02334-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The asymmetric cryptography method is typically used to transfer the key via an insecure channel while creating a key between two parties. However, since the methods using this strategy, like RSA, are now breached, new strategies must be sought to generate a key that can provide security. To solve this issue, a new group of cryptography was created known as neural cryptography. The main objective of this neural cryptography is to create a secret key using an unsafe medium. This paper suggests an overview of the optimal configuration of the neural network that enables the generation and establishment of a secret key between the two approved entities. Synchronization of two neural networks with three hidden layers is proposed for the development of the public key exchange protocol. Over 15 million simulations were carried out to measure the synchronization time, the steps taken as well as the number of times the assaulting neural network can replicate the behavior of the two authorized networks. The proposed technique has been passed through different parametric tests. Simulations of the process show effectiveness in terms of cited results in the paper.},
  archive      = {J_APIN},
  author       = {Sarkar, Arindam},
  doi          = {10.1007/s10489-021-02334-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8057-8066},
  shortjournal = {Appl. Intell.},
  title        = {Neural cryptography using optimal structure of neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards data-free gating of heterogeneous pre-trained neural
networks. <em>APIN</em>, <em>51</em>(11), 8045–8056. (<a
href="https://doi.org/10.1007/s10489-021-02301-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination and aggregation of knowledge from multiple neural networks can be commonly seen in the form of mixtures of experts. However, such combinations are usually done using networks trained on the same tasks, with little mention of the combination of heterogeneous pre-trained networks, especially in the data-free regime. The problem of combining pre-trained models in the absence of relevant datasets is likely to become increasingly important, as machine learning continues to dominate the AI landscape, and the number of useful but specialized models explodes. This paper proposes multiple data-free methods for the combination of heterogeneous neural networks, ranging from the utilization of simple output logit statistics, to training specialized gating networks. The gating networks decide whether specific inputs belong to specific networks based on the nature of the expert activations generated. The experiments revealed that the gating networks, including the universal gating approach, constituted the most accurate approach, and therefore represent a pragmatic step towards applications with heterogeneous mixtures of experts in a data-free regime. The code for this project is hosted on github at https://github.com/cwkang1998/network-merging .},
  archive      = {J_APIN},
  author       = {Kang, Chen Wen and Hong, Chua Meng and Maul, Tomas},
  doi          = {10.1007/s10489-021-02301-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8045-8056},
  shortjournal = {Appl. Intell.},
  title        = {Towards data-free gating of heterogeneous pre-trained neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended approach by using best–worst method on the basis of
importance–necessity concept and its application. <em>APIN</em>,
<em>51</em>(11), 8030–8044. (<a
href="https://doi.org/10.1007/s10489-021-02316-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, several concepts such as fuzzy sets, Z-numbers, and D-numbers have been proposed to handle real-world decision-making problems. Despite the desirable properties of these types of numbers, they do not consider the concept of Necessity. On the other hand, recently, the Best-Worst Method (BWM) has been introduced as a technique based on a systematic pairwise comparison of decision criteria. The advantage of this method is that it reduces the level of inconsistency or ambiguity in the results. Since ambiguity is associated with information, it is important to consider it in the decision-making process to boost the accuracy of findings. The main aim of this study is to reduce the ambiguity in attributing weights to the criteria by incorporating the BWM method and the Importance-Necessity concept (G-number), and to present a novel method, namely The GBWM method. By decreasing levels of ambiguity in the final results through the addition of the Necessity and Importance concepts, this method can be applied to an extensive range of practical and complex decision-making problems. To express the feasibility and usefulness of the proposed method in the real-world, two case studies have been investigated. Finally, the sensitivity analysis and a conceptual comparison with other methods have been conducted to confirm the strength and stability of this method.},
  archive      = {J_APIN},
  author       = {Jafarzadeh Ghoushchi, Saeid and Dorosti, Shadi and Khazaeili, Mohammad and Mardani, Abbas},
  doi          = {10.1007/s10489-021-02316-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8030-8044},
  shortjournal = {Appl. Intell.},
  title        = {Extended approach by using best–worst method on the basis of importance–necessity concept and its application},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An online multiple object tracker based on structure keeper
net. <em>APIN</em>, <em>51</em>(11), 8010–8029. (<a
href="https://doi.org/10.1007/s10489-021-02294-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel online multiple object tracker taking structure information into account. State-of-the-art multi-object tracking (MOT) approaches commonly focus on discriminative appearance features, while neglect in different levels structure information and the core of data association. Addressing this, we design a new tracker fully exploiting structure information and encoding such information into the cost function of the graph matching model. Firstly, a new measurement is proposed to compare the structure similarity of two graphs whose nodes are equal. With this measurement, we define a complete matching which performs association in high efficiency. Secondly, for incomplete matching scenarios, a structure keeper net (SKnet) is designed to adaptively establish the graph for matching. Finally, we conduct extensive experiments on benchmarks including MOT2015 and MOT17. The results demonstrate the competitiveness and practicability of our tracker.},
  archive      = {J_APIN},
  author       = {Wang, Nan and Zou, Qi and Ma, Qiulin and Huang, Yaping and Lou, Haitao and Wu, Xiaoyu and Liu, Huiyong},
  doi          = {10.1007/s10489-021-02294-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {8010-8029},
  shortjournal = {Appl. Intell.},
  title        = {An online multiple object tracker based on structure keeper net},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A learning search algorithm with propagational reinforcement
learning. <em>APIN</em>, <em>51</em>(11), 7990–8009. (<a
href="https://doi.org/10.1007/s10489-020-02117-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When reinforcement learning with a deep neural network is applied to heuristic search, the search becomes a learning search. In a learning search system, there are two key components: (1) a deep neural network with sufficient expression ability as a heuristic function approximator that estimates the distance from any state to a goal; (2) a strategy to guide the interaction of an agent with its environment to obtain more efficient simulated experience to update the Q-value or V-value function of reinforcement learning. To date, neither component has been sufficiently discussed. This study theoretically discusses the size of a deep neural network for approximating a product function of p piecewise multivariate linear functions. The existence of such a deep neural network with O(n + p) layers and O(dn + dnp + dp) neurons has been proven, where d is the number of variables of the multivariate function being approximated, 𝜖 is the approximation error, and n = O(p + log2(pd/𝜖)). For the second component, this study proposes a general propagational reinforcement-learning-based learning search method that improves the estimate h(.) according to the newly observed distance information about the goals, propagates the improvement bidirectionally in the search tree, and consequently obtains a sequence of more accurate V-values for a sequence of states. Experiments on the maze problems show that our method increases the convergence rate of reinforcement learning by a factor of 2.06 and reduces the number of learning episodes to 1/4 that of other nonpropagating methods.},
  archive      = {J_APIN},
  author       = {Zhang, Wei},
  doi          = {10.1007/s10489-020-02117-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7990-8009},
  shortjournal = {Appl. Intell.},
  title        = {A learning search algorithm with propagational reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering relational and numerical expressions from plan
traces for learning action models. <em>APIN</em>, <em>51</em>(11),
7973–7989. (<a
href="https://doi.org/10.1007/s10489-021-02232-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a domain learning process build on a machine learning-based process that, starting from plan traces with (partially known) intermediate states, returns a planning domain with numeric predicates, and expressive logical/arithmetic relations between domain predicates written in the planning domain definition language (PDDL). The novelty of our approach is that it can discover relations with little information about the ontology of the target domain to be learned. This is achieved by applying a selection of preprocessing, regression, and classification techniques to infer information from the input plan traces. These techniques are used to prepare the planning data, discover relational/numeric expressions, or extract the preconditions and effects of the domain’s actions. Our solution was evaluated using several metrics from the literature, taking as experimental data plan traces obtained from several domains from the International Planning Competition. The experiments demonstrate that our proposal—even with high levels of incompleteness—correctly learns a wide variety of domains discovering relational/arithmetic expressions, showing F-Score values above 0.85 and obtaining valid domains in most of the experiments.},
  archive      = {J_APIN},
  author       = {Segura-Muros, José Á. and Pérez, Raúl and Fernández-Olivares, Juan},
  doi          = {10.1007/s10489-021-02232-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7973-7989},
  shortjournal = {Appl. Intell.},
  title        = {Discovering relational and numerical expressions from plan traces for learning action models},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new parameter reduction algorithm for soft sets based on
chi-square test. <em>APIN</em>, <em>51</em>(11), 7960–7972. (<a
href="https://doi.org/10.1007/s10489-021-02265-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundancy is an extremely significant challenge in data integration and decision making based on the model of soft set which is able to process data under uncertainty. There are four available methods which are designed to reduce the redundant parameters of the soft set. But there is a very low success rate on a large number of data sets obtained from practices by these methods. In order to overcome the inherent weakness, we propose a parameter reduction method based on chi square distribution for the model of soft set. Experimental results on two real-life application cases and thirty randomly generated data sets demonstrate that our algorithm largely improves the success rate of parameter reduction, redundant degree of parameter, and has higher practicability in comparison with the four existing normal parameter reduction algorithms.},
  archive      = {J_APIN},
  author       = {Qin, Hongwu and Fei, Qinghua and Ma, Xiuqin and Chen, Wanghu},
  doi          = {10.1007/s10489-021-02265-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7960-7972},
  shortjournal = {Appl. Intell.},
  title        = {A new parameter reduction algorithm for soft sets based on chi-square test},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid user-based collaborative filtering algorithm with
topic model. <em>APIN</em>, <em>51</em>(11), 7946–7959. (<a
href="https://doi.org/10.1007/s10489-021-02207-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently available Collaborative Filtering(CF) algorithms often utilize user behavior data to generate recommendations. The similarity calculation between users is mostly based on the scores, without considering the explicit attributes of the users with profiles, as these are difficult to generate, or their evolution of preferences over time. This paper proposes a collaborative filtering algorithm named T-LDA (Time-decay Dirichlet Allocation), which is based on the topic model. In this method, we generate a hybrid score for similarity calculation with topic model. However, most topic models ignore the attribute of time order. In order to further improve the prediction accuracy, a time-decay function is introduced in topic model. The experimental results show that this algorithm has better performance than currently available algorithms on the MovieLens dataset, Netflix dataset and la.fm dataset.},
  archive      = {J_APIN},
  author       = {Na, Liu and Ming-xia, Li and Hai-yang, Qiu and Hao-long, Su},
  doi          = {10.1007/s10489-021-02207-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7946-7959},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid user-based collaborative filtering algorithm with topic model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recognizing diseases with multivariate physiological signals
by a DeepCNN-LSTM network. <em>APIN</em>, <em>51</em>(11), 7933–7945.
(<a href="https://doi.org/10.1007/s10489-021-02309-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of multivariate time series to identify diseases plays an important role in the medical field, as it can help medical staff to improve diagnose accuracy and reduce medical costs. Current research shows that deep Convolutional Neural Networks (CNN) can automatically capture features from raw data and Long Short-Term Memory (LSTM) networks can manage and learn temporal dependence between time series data such as physiological signals. In this work, we propose a deep learning framework called DeepCNN-LSTM by combining the CNN and LSTM to leverage their respective advantages for disease recognition, allowing itself to characterize complex temporal varieties with multiple autoencoded features. In particular, we use stationary wavelet transform together with median filter to preprocess low-frequency signal data, and introduce sliding window to segment physiological time series before model training for performance improvement on the training speed as well as the accuracy for recognizing diseases. In addition, we validate our model on a hybrid benchmark dataset collecting from MIMIC and Fantasia databases and set up four kinds of comparative experiments. Empirical evaluations on the benchmark dataset demonstrate that the proposed model outperforms other competitive approaches.},
  archive      = {J_APIN},
  author       = {Liao, Jun and Liu, Dandan and Su, Guoxin and Liu, Li},
  doi          = {10.1007/s10489-021-02309-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7933-7945},
  shortjournal = {Appl. Intell.},
  title        = {Recognizing diseases with multivariate physiological signals by a DeepCNN-LSTM network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A community detection algorithm based on quasi-laplacian
centrality peaks clustering. <em>APIN</em>, <em>51</em>(11), 7917–7932.
(<a href="https://doi.org/10.1007/s10489-021-02278-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for key nodes in social networks and clustering communities are indispensable components in community detection methods. With the wide application demand of detecting community networks, more and more algorithms have been proposed. Laplacian centrality peaks clustering (LPC) is an efficient and simple algorithm which is proposed on the basis of density peaks clustering (DPC) to identify clusters without parameters and prior knowledge. Before LPC is widely applied in community detection algorithms, some shortcomings should be addressed. Firstly, LPC fails to search for key nodes in networks accurately because of the similarity calculation method. Secondly, it takes too much time for LPC to calculate the Laplacian centrality of each point. To address these issues, a community detection algorithm based on Quasi-Laplacian centrality peaks clustering (CD-QLPC) is proposed after studying the advantages of Quasi-Laplacian centrality which can replace density or Laplacian centrality to characterize the importance of nodes in networks. Quasi-Laplacian centrality is obtained by the degree of each node directly, which needs less time than Laplacian centrality. In addition, a trust-based function is utilized to obtain the similarity accurately. Moreover, a new modularity-based merging strategy is adopted to identify the optimal number of communities adaptively. Experimental results show that CD-QLPC outperforms many state-of-the-art methods on both real-world networks and synthetic networks.},
  archive      = {J_APIN},
  author       = {Shi, Tianhao and Ding, Shifei and Xu, Xiao and Ding, Ling},
  doi          = {10.1007/s10489-021-02278-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7917-7932},
  shortjournal = {Appl. Intell.},
  title        = {A community detection algorithm based on quasi-laplacian centrality peaks clustering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subtler mixed attention network on fine-grained image
classification. <em>APIN</em>, <em>51</em>(11), 7903–7916. (<a
href="https://doi.org/10.1007/s10489-021-02280-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key of fine-grained image categorization is to locate discriminative regions and feature extraction from these regions correspond to subtle visual traits. Some of the current methods use the attention mechanism to identify the discriminative region, but ignore that there is still a large amount of non-foreground noise information in these regions. In this work, we propose a Subtler Mixed Attention Network (SMA-Net), which contains two modules: 1) Discriminative region location module uses the channel attention mechanism to construct a feature pyramid network to locate the discriminative regions. And use the positive effect of classification to screen a group of the most discriminative regions and learn through rank to learn. 2) Mixed attention module (MAM) of feature extraction that can focus on subtler and differentiated regions. We divide the feature map into intervals according to regions, and learn attention features according to regional orientation. Then the attention maps are multiplied to the input feature map for adaptive features reinforce. At the same time, MAM is a lightweight module that can be easily integrated into advanced networks without increasing too much calculation. We validated our SMA-Net through substantial experiments on Caltech-UCSD Birds (CUB-200-2011), Stanford Cars, CIFAR-10, Fish4Knowledge and Flower17. In particular, the accuracy on two widely used fine-grained datasets, CUB-2011 and Stanford Cars, reached 87.71% and 94.37%, respectively.},
  archive      = {J_APIN},
  author       = {Liu, Chao and Huang, Lei and Wei, Zhiqiang and Zhang, Wenfeng},
  doi          = {10.1007/s10489-021-02280-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7903-7916},
  shortjournal = {Appl. Intell.},
  title        = {Subtler mixed attention network on fine-grained image classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A modification of i-SOS: Performance analysis to large scale
functions. <em>APIN</em>, <em>51</em>(11), 7881–7902. (<a
href="https://doi.org/10.1007/s10489-020-01974-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SOS is a global optimization algorithm, based on nature, and is utilized to execute the various complex hard optimization problems. Be that as it may, some basic highlights of SOS, for example, pitfall among neighborhood optima and weaker convergence zone should be upgraded to discover better answers for progressively intricate, nonlinear, many optimum solution type problems. To diminish these deficiencies, as of late, numerous analysts increase the exhibition of the SOS by designing up a few changed form of the SOS. This paper suggests an improved form of the SOS to build up an increasingly steady balance between discovery and activity cores. This technique uses three unique procedures called adjusted benefit factor, altered parasitism stage, and random weighted number-based search. The technique is referred to as mISOS and tested in a popular series of twenty classic benchmarks. The dimension of these problems is considered to be hundred to monitor the impact of the suggested technique on the versatility of the test problems. Also, some real-life optimization problems are solved with the help of the proposed mISOS. The results investigated based on three different way and theses are statistical measures, convergence, and statistical analyses. The comparison of results of the mISOS with the standard SOS, SOS variants, and certain other cutting-edge algorithms shows its improved search performance.},
  archive      = {J_APIN},
  author       = {Nama, Sukanta},
  doi          = {10.1007/s10489-020-01974-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7881-7902},
  shortjournal = {Appl. Intell.},
  title        = {A modification of I-SOS: Performance analysis to large scale functions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep collaborative filtering with social promoter
score-based user-item interaction: A new perspective in recommendation.
<em>APIN</em>, <em>51</em>(11), 7855–7880. (<a
href="https://doi.org/10.1007/s10489-020-02162-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing recommender systems understand the preference level of users based on user-item interaction ratings. Rating-based recommendation systems mostly ignore negative users/reviewers (who give poor ratings). There are two types of negative users. Some negative users give negative or poor ratings randomly, and some negative users give ratings according to the quality of items. Some negative users, who give ratings according to the quality of items, are known as reliable negative users, and they are crucial for a better recommendation. Similar characteristics are also applicable to positive users. From a poor reflection of a user to a specific item, the existing recommender systems presume that this item is not in the user’s preferred category. That may not always be correct. We should investigate whether the item is not in the user’s preferred category, whether the user is dissatisfied with the quality of a favorite item or whether the user gives ratings randomly/casually. To overcome this problem, we propose a Social Promoter Score (SPS)-based recommendation. We construct two user-item interaction matrices with users’ explicit SPS value and users’ view activities as implicit feedback. With these matrices as inputs, our attention layer-based deep neural model deepCF_SPS learns a common low-dimensional space to present the features of users and items and understands the way users rate items. Extensive experiments on online review datasets present that our method can be remarkably futuristic compared to some popular baselines. The empirical evidence from the experimental results shows that our model is the best in terms of scalability and runtime over the baselines.},
  archive      = {J_APIN},
  author       = {Mandal, Supriyo and Maiti, Abyayananda},
  doi          = {10.1007/s10489-020-02162-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7855-7880},
  shortjournal = {Appl. Intell.},
  title        = {Deep collaborative filtering with social promoter score-based user-item interaction: A new perspective in recommendation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint sparse neural network compression via
multi-application multi-objective optimization. <em>APIN</em>,
<em>51</em>(11), 7837–7854. (<a
href="https://doi.org/10.1007/s10489-021-02243-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the pass decade, deep neural network (DNN) has been widely applied in various applications. To alleviate the storage and computation requirement of the complicated DNNs, network compression methods are developed. The sparse structure learning methods based on multi-objective optimization have been proven to be valid to balance the sparsity of the network model and network performance. However, when multiple applications are deployed on one single platform simultaneously, these methods become inefficient because each network model for each application needs to be trained and optimized individually. In this article, a multi-objective, multi-application sparse learning model is proposed to optimize multiple targets from a set of applications together. The joint network structure is first proposed. After a pre-training of the network model, a joint multi-objective evolutionary algorithm is derived to solve the optimization problems. Note that an improved initialization method for parent model generation is also developed. Finally, based on the joint loss between the objectives, fine tuning is used to compute the final models with good performance. The proposed method is evaluated under different datasets with a comparison to the state-of-the-art approaches, and experimental results demonstrate that the multi-application optimization model can give much better performance than the single-application optimization ones, especially in the case that different datasets are involved simultaneously.},
  archive      = {J_APIN},
  author       = {Chen, Jinzhuo and Xu, Yongnan and Sun, Weize and Huang, Lei},
  doi          = {10.1007/s10489-021-02243-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7837-7854},
  shortjournal = {Appl. Intell.},
  title        = {Joint sparse neural network compression via multi-application multi-objective optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Noise-adaptive synthetic oversampling technique.
<em>APIN</em>, <em>51</em>(11), 7827–7836. (<a
href="https://doi.org/10.1007/s10489-021-02341-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of supervised learning, the problem of class imbalance is one of the most difficult problems, and has attracted a great deal of research attention in recent years. In an imbalanced dataset, minority classes are those that contain very small numbers of data samples, while the remaining classes have a very large number of data samples. This type of imbalance reduces the predictive performance of machine learning models. There are currently three approaches for dealing with the class imbalance problem: algorithm-level, data-level, and ensemble-based approaches. Of these, data-level approaches are the most widely used, and consist of three sub-categories: under-sampling, oversampling, and hybrid techniques. Oversampling techniques generate synthetic samples for the minority class to balance an imbalanced dataset. However, existing oversampling approaches do not have a strategy for handling noise samples in imbalanced and noisy datasets, which leads to a reduction in the predictive performance of machine learning models. This study therefore proposes a noise-adaptive synthetic oversampling technique (NASOTECH) to deal with the class imbalance problem in imbalanced and noisy datasets. The noise-adaptive synthetic oversampling (NASO) strategy is first introduced, which is used to identify the number of samples generated for each sample in the minority class, based on the concept of the noise ratio. Next, the NASOTECH algorithm is proposed, based on the NASO strategy, to handle the class imbalance problem in imbalanced and noisy datasets. Finally, empirical experiments are conducted on several synthetic and real datasets to verify the effectiveness of the proposed approach. The experimental results confirm that NASOTECH outperforms three state-of-the-art oversampling techniques in terms of accuracy and geometric mean (G-mean) on imbalanced and noisy datasets.},
  archive      = {J_APIN},
  author       = {Vo, Minh Thanh and Nguyen, Trang and Vo, H. Anh and Le, Tuong},
  doi          = {10.1007/s10489-021-02341-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7827-7836},
  shortjournal = {Appl. Intell.},
  title        = {Noise-adaptive synthetic oversampling technique},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust graph convolutional networks with directional graph
adversarial training. <em>APIN</em>, <em>51</em>(11), 7812–7826. (<a
href="https://doi.org/10.1007/s10489-021-02272-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs), an emerging type of neural network model on graphs, have presented state-of-the-art performance on the node classification task. However, recent studies show that neural networks are vulnerable to the small but deliberate perturbations on input features. And GCNs could be more sensitive to the perturbations since the perturbations from neighbor nodes exacerbate the impact on a target node through the convolution. Adversarial training (AT) is a regularization technique that has been shown capable of improving the robustness of the model against perturbations on image classification. However, directly adopting AT on GCNs is less effective since AT regards examples as independent of each other and does not consider the impact from connected examples. In this work, we explore AT on graph and propose a graph-specific AT method, Directional Graph Adversarial Training (DGAT), which incorporates the graph structure into the adversarial process and automatically identifies the impact of perturbations from neighbor nodes. Concretely, we consider the impact from the connected nodes to define the neighbor perturbation which restricts the perturbation direction on node features towards their neighbor nodes, and additionally introduce an adversarial regularizer to defend the worst-case perturbations. In this way, DGAT can resist the impact of worst-case adversarial perturbations and reduce the impact of perturbations from neighbor nodes. Extensive experiments demonstrate that DGAT can effectively improve the robustness and generalization performance of GCNs. Specially, GCNs with DGAT can provide better performance when there are rare few labels available for training.},
  archive      = {J_APIN},
  author       = {Hu, Weibo and Chen, Chuan and Chang, Yaomin and Zheng, Zibin and Du, Yunfei},
  doi          = {10.1007/s10489-021-02272-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7812-7826},
  shortjournal = {Appl. Intell.},
  title        = {Robust graph convolutional networks with directional graph adversarial training},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A novel feature selection approach with pareto optimality
for multi-label data. <em>APIN</em>, <em>51</em>(11), 7794–7811. (<a
href="https://doi.org/10.1007/s10489-021-02228-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning has widely applied in machine learning and data mining. The purpose of feature selection is to select an approximately optimal feature subset to characterize the original feature space. Similar to single-label data, feature selection is an import preprocessing step to enhance the performance of multi-label classification model. In this paper, we propose a multi-label feature selection approach with Pareto optimality for continuous data, called MLFSPO. It maps multi-label features to high-dimensional space to evaluate the correlation between features and labels by utilizing the Hilbert-Schmidt Independence Criterion (HSIC). Then, the feature subset obtains by combining the Pareto optimization with feature ordering criteria and label weighting. Eventually, extensive experimental results on publicly available data sets show the effectiveness of the proposed algorithm in multi-label tasks.},
  archive      = {J_APIN},
  author       = {Li, Guohe and Li, Yong and Zheng, Yifeng and Li, Ying and Hong, Yunfeng and Zhou, Xiaoming},
  doi          = {10.1007/s10489-021-02228-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7794-7811},
  shortjournal = {Appl. Intell.},
  title        = {A novel feature selection approach with pareto optimality for multi-label data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene graph generation by multi-level semantic tasks.
<em>APIN</em>, <em>51</em>(11), 7781–7793. (<a
href="https://doi.org/10.1007/s10489-020-02115-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding scene image includes detecting and recognizing objects, estimating the interaction relationships of the detected objects, and describing image regions with sentences. However, since the complexity and variety of scene image, existing methods take object detection or vision relationship estimate as the research targets in scene understanding, and the obtained results are not satisfactory. In this work, we propose a Multi-level Semantic Tasks Generation Network (MSTG) to leverage mutual connections across object detection, visual relationship detection and image captioning, to solve jointly and improve the accuracy of the three vision tasks and achieve the more comprehensive and accurate understanding of scene image. The model uses a message pass graph to mutual connections and iterative updates across the different semantic features to improve the accuracy of scene graph generation, and introduces a fused attention mechanism to improve the accuracy of image captioning while using the mutual connections and refines of different semantic features to improve the accuracy of object detection and scene graph generation. Experiments on Visual Genome and COCO datasets indicate that the proposed method can jointly learn the three vision tasks to improve the accuracy of those visual tasks generation.},
  archive      = {J_APIN},
  author       = {Tian, Peng and Mo, Hongwei and Jiang, Laihao},
  doi          = {10.1007/s10489-020-02115-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7781-7793},
  shortjournal = {Appl. Intell.},
  title        = {Scene graph generation by multi-level semantic tasks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A band selection approach based on wavelet support vector
machine ensemble model and membrane whale optimization algorithm for
hyperspectral image. <em>APIN</em>, <em>51</em>(11), 7766–7780. (<a
href="https://doi.org/10.1007/s10489-021-02270-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral Image (HSI) has become one of the important remote sensing sources for object interpretation by its abundant band information. Among them, band selection is considered as the main theme in HSI classification to reduce the data dimension, and it is a combinatorial optimization problem and difficult to be completely solved by previous techniques. Whale Optimization Algorithm (WOA) is a newly proposed swarm intelligence algorithm that imitates the predatory strategy of humpback whales, and membrane computing is able to decompose the band information into a series of elementary membranes that decreases the coding length. In addition, Support Vector Machine (SVM) combined with wavelet kernel is adapted to HSI datasets with high dimension and small samples, ensemble learning is an effective tool that synthesizes multiple sub-classifiers to solve the same problem and obtains accurate category label for each sample. In the paper, a band selection approach based on wavelet SVM (WSVM) ensemble model and membrane WOA (MWOA) is proposed, experimental results indicate that the proposed HSI classification technique is superior to other corresponding and newly proposed methods, achieves the optimal band subset with a fast convergence speed, and the overall classification accuracy has reached 93% for HSIs.},
  archive      = {J_APIN},
  author       = {Wang, Mingwei and Yan, Ziqi and Luo, Jianwei and Ye, Zhiwei and He, Peipei},
  doi          = {10.1007/s10489-021-02270-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7766-7780},
  shortjournal = {Appl. Intell.},
  title        = {A band selection approach based on wavelet support vector machine ensemble model and membrane whale optimization algorithm for hyperspectral image},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Identification of top-k influential nodes based on discrete
crow search algorithm optimization for influence maximization.
<em>APIN</em>, <em>51</em>(11), 7749–7765. (<a
href="https://doi.org/10.1007/s10489-021-02283-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization refers to selecting a small number of influential nodes in a given network to maximize the influence affected by the subset. In social network analysis and viral marketing, influence maximization is greatly significant. The greedy-based algorithm is time-consuming in estimating the expected influence diffusion of a given node set, which is unsuitable for large-scale network. The traditional heuristics often have the problem of low accuracy. In this study, in order to solve the influence maximization problem more effectively, a meta-heuristic discrete crow search algorithm (DCSA) using the intelligence of crow population is proposed. In DCSA, a new coding mechanism and discrete evolution rules are constructed. The degree-based initialization method and the random walk strategy are adopted to enhance the search ability. Moreover, according to the network topology, influential nodes Candidates are generated to avoid blindness in the process of crow search. Extensive experiments are conducted on six real-world social networks under independent cascade (IC) model, the results show that DCSA outperforms other state-of-the-art algorithms and obtains comparable influence diffusion results to CELF but with lower time complexity.},
  archive      = {J_APIN},
  author       = {Li, Huan and Zhang, Ruisheng and Zhao, Zhili and Liu, Xin and Yuan, Yongna},
  doi          = {10.1007/s10489-021-02283-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7749-7765},
  shortjournal = {Appl. Intell.},
  title        = {Identification of top-k influential nodes based on discrete crow search algorithm optimization for influence maximization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new complete color normalization method for h&amp;e
stained histopatholgical images. <em>APIN</em>, <em>51</em>(11),
7735–7748. (<a
href="https://doi.org/10.1007/s10489-021-02231-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of digital histopathology is growing rapidly in the development of computer aided disease diagnosis systems. However, the color variations due to manual cell sectioning and stain concentration make the process challenging in various digital pathological image analysis such as histopathological image segmentation and classification. Hence, the normalization of these variations are needed to obtain the promising results. The proposed research intends to introduce a reliable and robust new complete color normalization method, addressing the problems of color and stain variability. The new complete color normalization involves three phases, namely enhanced fuzzy illuminant normalization, fuzzy-based stain normalization, and modified spectral normalization. The extensive simulations are performed and validated on histopathological images. The presented algorithm outperforms the existing conventional normalization methods by overcoming the certain limitations and challenges. As per the experimental quality metrics and comparative analysis, the proposed algorithm performs efficiently and provides promising results.},
  archive      = {J_APIN},
  author       = {Vijh, Surbhi and Saraswat, Mukesh and Kumar, Sumit},
  doi          = {10.1007/s10489-021-02231-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7735-7748},
  shortjournal = {Appl. Intell.},
  title        = {A new complete color normalization method for H&amp;E stained histopatholgical images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed dictionary learning for industrial process
monitoring with big data. <em>APIN</em>, <em>51</em>(11), 7718–7734. (<a
href="https://doi.org/10.1007/s10489-020-02128-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of sensor and communication technology, industrial systems have accumulated a large amount of data. This data has provided new perspectives and methods for industrial system analysis, monitoring and control, which is proven to be of great significance. However, with the collection and storage of industrial data in a 7 × 24 manner, the computing and information processing capabilities of edge controllers and computers at industrial sites face new challenges. Therefore, this paper proposes a distributed dictionary learning algorithm based on the MapReduce framework. The dictionary learning method can efficiently extract useful information from high-dimensional data for process monitoring. In addition, deploying the algorithm under the MapReduce framework can achieve the purpose of parallel distributed computing, which would solve the issue that the ability of calculation and information processing is limited at industrial sites. Based on extensive numerical experiments, the proposed method can improve the effectiveness and robustness of process monitoring for industrial processes.},
  archive      = {J_APIN},
  author       = {Huang, Keke and Wei, Ke and Li, Yonggang and Yang, Chunhua},
  doi          = {10.1007/s10489-020-02128-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7718-7734},
  shortjournal = {Appl. Intell.},
  title        = {Distributed dictionary learning for industrial process monitoring with big data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic-enhanced sequential modeling for personality trait
recognition from texts. <em>APIN</em>, <em>51</em>(11), 7705–7717. (<a
href="https://doi.org/10.1007/s10489-021-02277-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic recognition of personality traits from texts has attracted significant attention. Existing studies typically combine linguistic feature engineering with traditional models, use five various neural networks to predict personality traits with multiple labels, and fail to achieve the best performance on each label. To this end, in this paper, we propose a novel semantic-enhanced personality recognition neural network (SEPRNN) model, which has a goal of avoiding dependence on feature engineering, allowing the same model to adapt to detecting five various personality traits with no modification to the model itself, and employing deep learning based methods and atomic features of texts to build vectorial word-level representation for personality trait recognition. Specifically, to precisely recognize multi-labeled personality traits, we first propose a word-level semantic representation for texts based on context learning. Then, a fully connected layer is used to obtain higher-level semantics of texts. Finally, the experimental results demonstrate that the proposed approach achieves significant performance improvement for multi-labeled personality traits compared with several baselines.},
  archive      = {J_APIN},
  author       = {Xue, Xia and Feng, Jun and Sun, Xia},
  doi          = {10.1007/s10489-021-02277-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7705-7717},
  shortjournal = {Appl. Intell.},
  title        = {Semantic-enhanced sequential modeling for personality trait recognition from texts},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flexible data representation with feature convolution for
semi-supervised learning. <em>APIN</em>, <em>51</em>(11), 7690–7704. (<a
href="https://doi.org/10.1007/s10489-021-02210-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data representation plays a crucial role in semi-supervised learning. This paper proposes a framework for semi-supervised data representation. It introduces a flexible nonlinear embedding model that integrates graph-based data convolutions. The proposed approach exploits structured data in order to estimate a nonlinear data representation as well as a linear transformation, enabling an inductive semi-supervised model. The introduced approach exploits data graphs at two different levels. First, it integrates manifold regularization that is encoded by the graph itself. Second, it optimizes a flexible linear transformation that maps the convolved data samples to their nonlinear representations. These convolved data are generated by the joint use of the graph and data. The proposed semi-supervised model overcomes some challenges related to some samples distributions in the original spaces. The proposed Graph Convolution based Semi-supervised Embedding (GCSE) provides flexible models which can improve both the data representation and the final performance of the learning model. Experiments are run on six image datasets for comparing the proposed approach with several state-of-art semi-supervised methods. These results show the effectiveness of the proposed framework.},
  archive      = {J_APIN},
  author       = {Dornaika, F.},
  doi          = {10.1007/s10489-021-02210-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7690-7704},
  shortjournal = {Appl. Intell.},
  title        = {Flexible data representation with feature convolution for semi-supervised learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image generation and constrained two-stage feature fusion
for person re-identification. <em>APIN</em>, <em>51</em>(11), 7679–7689.
(<a href="https://doi.org/10.1007/s10489-021-02271-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network is widely used in person re-identification to expand data by generating auxiliary data. However, researchers all believe that using too much generated data in the training phase will reduce the accuracy of re-identification models. In this study, an improved generator and a constrained two-stage fusion network are proposed. A novel gesture discriminator embedded into the generator is used to calculate the completeness of skeleton pose images. The improved generator can make generated images more realistic, which would be conducive to feature extraction. The role of the constrained two-stage fusion network is to extract and utilize the real information of the generated images for person re-identification. Unlike previous studies, the fusion of shallow features is considered in this work. In detail, the proposed network has two branches based on the structure of ResNet50. One branch is for the fusion of images that are generated by the generated adversarial network, the other is applied to fuse the result of the first fusion and the original image. Experimental results show that our method outperforms most existing similar methods on Market-1501 and DukeMTMC-reID.},
  archive      = {J_APIN},
  author       = {Zhang, Tao and Sun, Xing and Li, Xuan and Yi, Zhengming},
  doi          = {10.1007/s10489-021-02271-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7679-7689},
  shortjournal = {Appl. Intell.},
  title        = {Image generation and constrained two-stage feature fusion for person re-identification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Adversarial training with wasserstein distance for learning
cross-lingual word embeddings. <em>APIN</em>, <em>51</em>(11),
7666–7678. (<a
href="https://doi.org/10.1007/s10489-020-02136-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have managed to learn cross-lingual word embeddings in a completely unsupervised manner through generative adversarial networks (GANs). These GANs-based methods enable the alignment of two monolingual embedding spaces approximately, but the performance on the embeddings of low-frequency words (LFEs) is still unsatisfactory. The existing solution is to set up the low sampling rates for the embeddings of LFEs based on word-frequency information. However, such a solution has two shortcomings. First, this solution relies on the word-frequency information that is not always available in real scenarios. Second, the uneven sampling may cause the models to overlook the distribution information of LFEs, thereby negatively affecting their performance. In this study, we propose a novel unsupervised GANs-based method that effectively improves the quality of LFEs, circumventing the above two issues. Our method is based on the observation that LFEs tend to be densely clustered in the embedding space. In these dense embedding points, obtaining fine-grained alignment through adversarial training is difficult. We use this idea to introduce a noise function that can disperse the dense embedding points to a certain extent. In addition, we train a Wasserstein critic network to encourage the noise-adding embeddings and the original embeddings to have similar semantics. We test our approach on two common evaluation tasks, namely, bilingual lexicon induction and cross-lingual word similarity. Experimental results show that the proposed model has stronger or competitive performance compared with the supervised and unsupervised baselines.},
  archive      = {J_APIN},
  author       = {Li, Yuling and Zhang, Yuhong and Yu, Kui and Hu, Xuegang},
  doi          = {10.1007/s10489-020-02136-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7666-7678},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial training with wasserstein distance for learning cross-lingual word embeddings},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IM-ELPR: Influence maximization in social networks using
label propagation based community structure. <em>APIN</em>,
<em>51</em>(11), 7647–7665. (<a
href="https://doi.org/10.1007/s10489-021-02266-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of social networks has grown manifolds in recent years because of various activities like fast propagation of ideas, publicity, and news. Influence maximization (IM) is one of the most highly studied problems in the field of social network analysis due to its business values. Influence maximization aims to identify influential nodes that can spread the information to the maximum number of nodes in the network through diffusion cascade. Traditional methods for IM include centrality based and greedy based measures. However, each method has some limitations. Recently, some methods of IM are introduced, which consider the presence of community structure in networks. Community structure has a significant impact on information diffusion, because of dense connections between the nodes in the community. In this paper, we propose a novel influence maximization algorithm using node seeding, label propagation, and community detection. We first use extended h-index centrality to detect the seed nodes and then use the label propagation technique to detect communities. Further, we merge smaller and related communities to a larger community with the help of a relationship matrix. Finally, top-k influential nodes from these communities are identified. These ideas lead to our proposed algorithm: Influence Maximization using Extended h-index, and Label Propagation with Relationship matrix (IM-ELPR). We adopt Independent Cascade (IC) information diffusion model to spread the information originating from chosen influential nodes. The proposed algorithm intends to identify influential nodes from different communities globally and does not depend on the specific community’s antecedent structural information. Experimental results performed on several real-life data sets reveal that the proposed algorithm performs better than many other existing popular algorithms.},
  archive      = {J_APIN},
  author       = {Kumar, Sanjay and Singhla, Lakshay and Jindal, Kshitij and Grover, Khyati and Panda, B. S.},
  doi          = {10.1007/s10489-021-02266-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7647-7665},
  shortjournal = {Appl. Intell.},
  title        = {IM-ELPR: Influence maximization in social networks using label propagation based community structure},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved differential evolution based on multi-armed bandit
for multimodal optimization problems. <em>APIN</em>, <em>51</em>(11),
7625–7646. (<a
href="https://doi.org/10.1007/s10489-021-02261-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of multimodal optimization problems (MMOPs) is to find and deal with multiple optimal solutions using an objective function. MMOPs perform the exploration and exploitation simultaneously in the search space. The novelty of this paper includes the following improvements in differential evolution to be able to solve MMOPs. Clusters are formed from the whole population by applying a niching technique which uses the softmax strategy to assign a cutting probability to the species. Then iterative mutation strategy is followed to generate the unbiased mutant vector. Further, Multi-Armed Bandit (MAB) strategy is used to ensure that new individuals are generated in promising areas. The experimentation of the proposed algorithm has been performed on 20 benchmark functions from IEEE Congress on Evolutionary Computation 2013 (CEC2013). The results depict that the proposed algorithm can be compared with 15 state-of-the-art multimodal optimization algorithms in terms of locating accurate optimal solutions.},
  archive      = {J_APIN},
  author       = {Agrawal, Suchitra and Tiwari, Aruna and Naik, Prathamesh and Srivastava, Arjun},
  doi          = {10.1007/s10489-021-02261-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7625-7646},
  shortjournal = {Appl. Intell.},
  title        = {Improved differential evolution based on multi-armed bandit for multimodal optimization problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved evidence fusion algorithm in multi-sensor
systems. <em>APIN</em>, <em>51</em>(11), 7614–7624. (<a
href="https://doi.org/10.1007/s10489-021-02279-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-sensor information fusion plays an important role in practical application. Although D-S evidence theory can handle this information fusion task regardless of prior knowledge, counter-intuitive conclusions may arise when dealing with highly conflicting evidence. To address this weakness, an improved algorithm of evidence theory is proposed. First, a new distribution distance measurement method is first proposed to measure the conflict between the evidences, and the credibility degree of the evidences can be obtained. Next, a modified information volume calculation method is also introduced to measure the effect of the evidence itself, and the information volume of the evidences can be generated. Afterwards, the credibility degree of each evidence can be modified based on the information volume to obtain the weight of each evidence. Ultimately, the weights of the evidences will be used to adjust the body of evidence before fusion. A numerical example for engine fault diagnosis exhibits the availability and effectiveness of the proposed method, where the BPA of the true fault is 89.680%. Furthermore, an application for target recognition is given to show the validity of the proposed algorithm, where the BPA of the true target is 98.948%. The experimental results show that the proposed algorithm has the best performance than other methods.},
  archive      = {J_APIN},
  author       = {Zhao, Kaiyi and Sun, Rutai and Li, Li and Hou, Manman and Yuan, Gang and Sun, Ruizhi},
  doi          = {10.1007/s10489-021-02279-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7614-7624},
  shortjournal = {Appl. Intell.},
  title        = {An improved evidence fusion algorithm in multi-sensor systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A user-knowledge dynamic pattern matching process and
optimization strategy based on the expert knowledge recommendation
system. <em>APIN</em>, <em>51</em>(11), 7601–7613. (<a
href="https://doi.org/10.1007/s10489-021-02289-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When automated pattern matching tools are used to execute user-knowledge pattern matching (UKPM) in the expert knowledge recommendation system (EKRS), user-knowledge matching is uncertain and the matching efficiency is low. To solve the above problems, the dynamic UKPM mathematical model is established and the “Entropy-Beta” method of crowdsourcing task assignment is designed to solve the model in the study. Firstly, the concept of Entropy is combined with crowdsourcing. The uncertainty of user-knowledge matching results is measured and the magnitude of the uncertainty is calculated. Secondly, based on the Beta distribution function, the accuracy of matching results is measured. The optimal matching results are selected and the matching results were sent to EKRS according to the matching probability. Thirdly, the knowledge recommendation process of UKPM is dynamically adjusted according to the matching probability. Finally, the comparison results of several algorithms showed that the Entropy-Beta algorithm could largely improve the accuracy, efficiency, dynamic regulation, and other performances of EKRS.},
  archive      = {J_APIN},
  author       = {Gao, Li and Gan, Yi and Yao, Zhen and Zhang, Xianglei},
  doi          = {10.1007/s10489-021-02289-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {7601-7613},
  shortjournal = {Appl. Intell.},
  title        = {A user-knowledge dynamic pattern matching process and optimization strategy based on the expert knowledge recommendation system},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Genetic programming hyperheuristic parameter configuration
using fitness landscape analysis. <em>APIN</em>, <em>51</em>(10),
7402–7426. (<a
href="https://doi.org/10.1007/s10489-021-02227-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitness landscape analysis is a tool that can help us gain insight into a problem, determine how hard it is to solve a problem using a given algorithm, choose an algorithm for solving a given problem, or choose good algorithm parameters for solving the problem. In this paper, fitness landscape analysis of hyperheuristics is used for clustering instances of three scheduling problems. After that, good parameters for tree-based genetic programming that can solve a given scheduling problem are calculated automatically for every cluster. Additionally, we introduce tree editing operators which help in the calculation of fitness landscape features in tree based genetic programming. A heuristic is proposed based on introduced operators, and it calculates the distance between any two trees. The results show that the proposed approach can obtain parameters that offer better performance compared to manual parameter selection.},
  archive      = {J_APIN},
  author       = {Čorić, Rebeka and Ðumić, Mateja and Jakobović, Domagoj},
  doi          = {10.1007/s10489-021-02227-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7402-7426},
  shortjournal = {Appl. Intell.},
  title        = {Genetic programming hyperheuristic parameter configuration using fitness landscape analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MMD-encouraging convolutional autoencoder: A novel
classification algorithm for imbalanced data. <em>APIN</em>,
<em>51</em>(10), 7384–7401. (<a
href="https://doi.org/10.1007/s10489-021-02235-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification problem is widely existed in commercial activities and social production. It refers to the scenarios with considerable gap of sample amount among classes, thus significantly deteriorating the performance of the traditional classification algorithms. The previous dealing methods often focus on resampling and algorithm adjustment, but ignore enhancing the ability of feature learning. In this study, we have proposed a novel algorithm for imbalanced data classification: Maximum Mean Discrepancy-Encouraging Convolutional Autoencoder (MMD-CAE), from the perspective of feature learning. The algorithm adopts a two-phase target training process. The cross entropy loss is employed to calculate reconstruction loss of data, and the Maximum Mean Discrepancy (MMD) with intra-variance constraint is used to stimulate the feature discrepancy in bottleneck layer. By encouraging maximization of MMD between two-class samples, and mapping the original space to a higher dimension space via kernel skills, the features can be learned to form a more effective feature space. The proposed algorithm is tested on ten groups of samples with different imbalance ratios. The performance metrics of recall rate, F1 score, G-means and AUC verify that the proposed algorithm surpasses the existing state-of-the-art methods in this field, also with stronger generalization ability. This study could shed new lights on the related studies in terms of constituting more effective feature space via the proposed MMD with intra-variance constraint method, and the holistic MMD-CAE algorithm for imbalanced data classification.},
  archive      = {J_APIN},
  author       = {Li, Bin and Gong, Xiaofeng and Wang, Chen and Wu, Ruijuan and Bian, Tong and Li, Yanming and Wang, Zhiyuan and Luo, Ruisen},
  doi          = {10.1007/s10489-021-02235-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7384-7401},
  shortjournal = {Appl. Intell.},
  title        = {MMD-encouraging convolutional autoencoder: A novel classification algorithm for imbalanced data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel social behavior-based algorithm for identification
of influential users in social network. <em>APIN</em>, <em>51</em>(10),
7365–7383. (<a
href="https://doi.org/10.1007/s10489-021-02203-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization in social networks refers to the process of finding influential users who make the most of information or product adoption. The social networks is prone to grow exponentially, which makes it difficult to analyze. Critically, most of approaches in the literature focus only on modeling structural properties, ignoring the social behavior in the relations between users. For this, we tend to parallelize the influence maximization task based on social behavior. In this paper, we introduce a new parallel algorithm, named PSAIIM, for identification of influential users in social network. In PSAIIM, we uses two semantic metrics: the user’s interests and the dynamically-weighted social actions as user interactive behaviors. In order to overcome the size of actual real-world social networks and to minimize the execution time, we used the community structure to apply perfect parallelism to the CPU architecture of the machines to compute an optimal set of influential nodes. Experimental results on real-world networks reveal effectiveness of the proposed method as compared to the existing state-of-the-art influence maximization algorithms, especially in the speed of calculation.},
  archive      = {J_APIN},
  author       = {Mnasri, Wassim and Azaouzi, Mehdi and Romdhane, Lotfi Ben},
  doi          = {10.1007/s10489-021-02203-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7365-7383},
  shortjournal = {Appl. Intell.},
  title        = {Parallel social behavior-based algorithm for identification of influential users in social network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel hybrid particle swarm optimization for multi-UAV
cooperate path planning. <em>APIN</em>, <em>51</em>(10), 7350–7364. (<a
href="https://doi.org/10.1007/s10489-020-02082-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The path planning of unmanned aerial vehicle (UAV) in three-dimensional (3D) environment is an important part of the entire UAV’s autonomous control system. In the constrained mission environment, planning optimal paths for multiple UAVs is a challenging problem. To solve this problem, the time stamp segmentation (TSS) model is adopted to simplify the handling of coordination cost of UAVs, and then a novel hybrid algorithm called HIPSO-MSOS is proposed by combining improved particle swarm optimization (IPSO) and modified symbiotic organisms search (MSOS). The exploration and exploitation abilities are combined efficiently, which brings good performance to the proposed algorithm. The cubic B-spline curve is used to smooth the generated path so that the planned path is flyable for UAV. To assess performance, the simulation is carried out in the virtual three-dimensional complex terrain environment. The experimental results show that the HIPSO-MSOS algorithm can successfully generate feasible and effective paths for each UAV, and its performance is superior to the other five algorithms, namely PSO, Firefly, DE, MSOS and HSGWO-MSOS algorithms in terms of accuracy, convergence speed, stability and robustness. Moreover, HIPSO-MSOS performs better than other tested methods in multi-objective optimization problems. Thus, the HIPSO-MSOS algorithm is a feasible and reliable alternative for some difficult and practical problems.},
  archive      = {J_APIN},
  author       = {He, Wenjian and Qi, Xiaogang and Liu, Lifang},
  doi          = {10.1007/s10489-020-02082-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7350-7364},
  shortjournal = {Appl. Intell.},
  title        = {A novel hybrid particle swarm optimization for multi-UAV cooperate path planning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable semantic textual similarity of sentences using
alignment of chunks with classification and regression. <em>APIN</em>,
<em>51</em>(10), 7322–7349. (<a
href="https://doi.org/10.1007/s10489-020-02144-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed work is focused on establishing an interpretable Semantic Textual Similarity (iSTS) method for a pair of sentences, which can clarify why two sentences are completely or partially similar or have some variations. This proposed interpretable approach is a pipeline of five modules that begins with the pre-processing and chunking of text. Further chunks of two sentences are aligned using a one–to–multi (1:M) chunk aligner. Thereafter, support vector, Gaussian Naive Bayes and k–Nearest Neighbours classifiers are then used to create a multiclass classification algorithm, and different class labels are used to define an alignment type. At last, a multivariate regression algorithm is developed to find the semantic equivalence of an alignment with a score (that ranges from 0 to 5). The efficiency of the proposed method is verified on three different datasets and also compared to other state–of–the–art interpretable STS (iSTS) methods. The evaluated results show that the proposed method performs better than other iSTS methods. Most importantly, the modules of the proposed iSTS method are used to develop a Textual Entailment (TE) method. It is found that, when we combined chunk level, alignment, and sentence level features the entailment results significantly improves.},
  archive      = {J_APIN},
  author       = {Majumder, Goutam and Pakray, Partha and Das, Ranjita and Pinto, David},
  doi          = {10.1007/s10489-020-02144-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7322-7349},
  shortjournal = {Appl. Intell.},
  title        = {Interpretable semantic textual similarity of sentences using alignment of chunks with classification and regression},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A privacy-conserving framework based intrusion detection
method for detecting and recognizing malicious behaviours in
cyber-physical power networks. <em>APIN</em>, <em>51</em>(10),
7306–7321. (<a
href="https://doi.org/10.1007/s10489-021-02222-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary Smart Power Systems (SPNs) depend on Cyber-Physical Systems (CPSs) to connect physical devices and control tools. Developing a robust privacy-conserving intrusion detection method involves network and physical data regarding the setups, such as Supervisory Control and Data Acquisition (SCADA), for defending real data and recognizing cyber-attacks. A key issue in the implementation of SPNs is the security against cyber-attacks, targeting to interrupt SCADA operations and violate data privacy over the usage of penetration and data poisoning attacks. In this paper, a privacy-conserving framework, so-called PC-IDS, is proposed for realizing the privacy and safety features of SPNs through hybrid machine learning approach. The framework includes two key components. Primarily, a data pre-processing component is proposed for cleaning and transforming actual data into a different layout that accomplishes the aim of privacy conservation. Then, an intrusion detection component is proposed using a particle swarm optimization-based probabilistic neural network for the identification and recognition of malicious events. The performance of PC-IDS framework is evaluated by means of two commonly available datasets, i.e. the Power System and UNSW-NB15 datasets. The experimental outcomes highlight that the framework can proficiently protect data of SPNs and determine anomalous behaviours compared to numerous recent compelling state-of-the-art methods with respect to false positive rate (FPR), detection rate (DR) and computational processing time (CPT) by achieving 96.03% of DR, 0.18% FPR for Power System dataset and 95.91% of DR, 0.14% FPR for UNSW-NB15 dataset.},
  archive      = {J_APIN},
  author       = {Khan, Izhar Ahmed and Pi, Dechang and Khan, Nasrullah and Khan, Zaheer Ullah and Hussain, Yasir and Nawaz, Asif and Ali, Farman},
  doi          = {10.1007/s10489-021-02222-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7306-7321},
  shortjournal = {Appl. Intell.},
  title        = {A privacy-conserving framework based intrusion detection method for detecting and recognizing malicious behaviours in cyber-physical power networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed attention dense network for sketch classification.
<em>APIN</em>, <em>51</em>(10), 7298–7305. (<a
href="https://doi.org/10.1007/s10489-021-02211-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art convolutional neural networks (CNNs) on sketch classification cannot balance the expression ability of final feature vectors and the problems of gradient vanishing and network degradation. In order to improve the classification accuracy, we design a mixed attention dense network for sketch classification. According to the sparse characteristics of the sketch, this network uses overlapping pooling of a large size. In addition, dense blocks are added on the top of the middle convolutional layers to achieve feature reuse. Specifically, in order to extract more representative local, detail information, mixed attention is applied in the dense blocks. Finally, the center loss is combined with the softmax cross entropy loss to improve the classification accuracy. Through experiments, we compare our model with several state-of-the-art methods on the TU-Berlin dataset, and the experimental results demonstrate the effectiveness of our model.},
  archive      = {J_APIN},
  author       = {Zhu, Ming and Chen, Chun and Wang, Nian and Tang, Jun and Zhao, Chen},
  doi          = {10.1007/s10489-021-02211-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7298-7305},
  shortjournal = {Appl. Intell.},
  title        = {Mixed attention dense network for sketch classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random area-perimeter method for generation of unimodal and
multimodal cancelable biometric templates. <em>APIN</em>,
<em>51</em>(10), 7281–7297. (<a
href="https://doi.org/10.1007/s10489-021-02201-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, biometric based authentication systems have seen a tremendous growth in various applications. However, if databases in multiple applications are created using the same biometric characteristic and algorithm, then any compromise of the stored template in one biometric system may jeopardise the security of the other biometric systems as well. More importantly, such a compromise may also lead to a permanent loss of the biometric characteristic. Therefore, the cancelability or revocability of biometrics has become quite an essential requirement. In this paper, a novel scheme, the Random Area &amp; Perimeter Method (RAPM)) is presented in which a biometric characteristic of an individual is transformed into random values which are stored as cancelable biometric templates. The proposed scheme computes area and perimeter of the Bezier curve which are obtained through interpolation of feature points of original biometrics and a random point chosen by the user. The area and perimeter thus computed exhibit pseudo-random properties. This technique outperforms many existing techniques. Moreover, a dimensionality reduction to the tune of more than 95% has been obtained without compromising the matching performance. The average values obtained for EER, DI and RI are 0.0045, 6.28 and 99.64 respectively. These values are better than those obtained from the available state of the art approaches. The proposed scheme has also been analysed for various privacy issues like non-invertibility, revocability, and unlinkability.},
  archive      = {J_APIN},
  author       = {Asthana, Rajesh and Walia, Gurjit Singh and Gupta, Anjana},
  doi          = {10.1007/s10489-021-02201-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7281-7297},
  shortjournal = {Appl. Intell.},
  title        = {Random area-perimeter method for generation of unimodal and multimodal cancelable biometric templates},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale depth information fusion network for image
dehazing. <em>APIN</em>, <em>51</em>(10), 7262–7280. (<a
href="https://doi.org/10.1007/s10489-021-02236-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the atmospheric physical model, we can use accurate transmittance and atmospheric light information to convert a hazy image into a clean one. The scene-depth information is very important for image dehazing due to the transmittance directly corresponds to the scene depth. In this paper, we propose a multi-scale depth information fusion network based on the U-Net architecture. The model uses hazy images as inputs and extracts the depth information from these images; then, it encodes and decodes this information. In this process, hazy image features of different scales are skip-connected to the corresponding positions. Finally, the model outputs a clean image. The proposed method does not rely on atmospheric physical models, and it directly outputs clean images in an end-to-end manner. Through numerous experiments, we prove that the multi-scale deep information fusion network can effectively remove haze from images; it outperforms other methods in the synthetic dataset experiments and also performs well in the real-scene test set.},
  archive      = {J_APIN},
  author       = {Fan, Guodong and Hua, Zhen and Li, Jinjiang},
  doi          = {10.1007/s10489-021-02236-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7262-7280},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale depth information fusion network for image dehazing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new twin SVM method with dictionary learning.
<em>APIN</em>, <em>51</em>(10), 7245–7261. (<a
href="https://doi.org/10.1007/s10489-021-02273-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, dictionary learning has been widely studied, and lots of dictionary learning methods have been developed to solve the problem of classification. In this paper, we propose a new twin SVMs method with dictionary learning (TSVMDL) for classification. In the proposed method, we first incorporate the dictionary learning into twin SVMs to construct a unify model for prediction, in which we embed an analysis dictionary into learning that can obtain the coding coefficients and improve the representation ability of the dictionary. We further utilize the Lagrangian multiplier method to optimize the proposed TSVMDL objective model. We then obtain two nonparallel hyperplanes by solving two smaller sized quadratic programming problems (QPPs). Finally, extensive experiments have been conducted to evaluate the performance of the proposed TSVMDL method. The results have shown that our proposed method can obtain a better performance compared with state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Che, Zhiyong and Liu, Bo and Xiao, Yanshan and Cai, Hao},
  doi          = {10.1007/s10489-021-02273-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7245-7261},
  shortjournal = {Appl. Intell.},
  title        = {A new twin SVM method with dictionary learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic feature selection algorithm based on q-learning
mechanism. <em>APIN</em>, <em>51</em>(10), 7233–7244. (<a
href="https://doi.org/10.1007/s10489-021-02257-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a technique to improve the classification accuracy of classifiers and a convenient data visualization method. As an incremental, task oriented, and model-free learning algorithm, Q-learning is suitable for feature selection, this study proposes a dynamic feature selection algorithm, which combines feature selection and Q-learning into a framework. First, the Q-learning is used to construct the discriminant functions for each class of the data. Next, the feature ranking is achieved according to the all discrimination functions vectors for each class of the data comprehensively, and the feature ranking is doing during the process of updating discriminant function vectors. Finally, experiments are designed to compare the performance of the proposed algorithm with four feature selection algorithms, the experimental results on the benchmark data set verify the effectiveness of the proposed algorithm, the classification performance of the proposed algorithm is better than the other feature selection algorithms, meanwhile the proposed algorithm also has good performance in removing the redundant features, and the experiments of the effect of learning rates on the our algorithm demonstrate that the selection of parameters in our algorithm is very simple.},
  archive      = {J_APIN},
  author       = {Xu, Ruohao and Li, Mengmeng and Yang, Zhongliang and Yang, Lifang and Qiao, Kangjia and Shang, Zhigang},
  doi          = {10.1007/s10489-021-02257-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7233-7244},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic feature selection algorithm based on Q-learning mechanism},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual-branch model for diagnosis of parkinson’s disease
based on the independent and joint features of the left and right gait.
<em>APIN</em>, <em>51</em>(10), 7221–7232. (<a
href="https://doi.org/10.1007/s10489-020-02182-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical diagnosis of Parkingson’s disease (PD) requires the physician to assess the patient’s gait and other symptoms. A dual-branch model is proposed in this paper as an objective diagnostic tool to diagnose PD automatically. In this research, the joint features and independent features of left and right gait are fused innovatively. Convolutional neural network (CNN) and long short-term memory network (LSTM) are used to extract the spatial and temporal characteristics of sensors respectively. After the independent features extracted from the branches are collapsed, LSTM is used to incorporate the joint features between the left and right gait. Compared with other methods, the proposed model can learn the correlation between the two feet and extract higher discriminative features to effectively improve the accuracy of Parkinson detection. The model shows the state-of-the-art performance for the public dataset, with the accuracy, sensitivity, and specificity being 99.22%, 100%, and 98.04%, respectively. A simple, fast, and objective method proposed in this paper was believed to improve diagnostic performance.},
  archive      = {J_APIN},
  author       = {Liu, Xu and Li, Wang and Liu, Zheng and Du, Feixiang and Zou, Qiang},
  doi          = {10.1007/s10489-020-02182-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7221-7232},
  shortjournal = {Appl. Intell.},
  title        = {A dual-branch model for diagnosis of parkinson’s disease based on the independent and joint features of the left and right gait},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining sequential rules with itemset constraints.
<em>APIN</em>, <em>51</em>(10), 7208–7220. (<a
href="https://doi.org/10.1007/s10489-020-02153-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining sequential rules from a sequence database usually returns a set of rules with great cardinality. However, in real world applications, the end-users are often interested in a subset of sequential rules. Particularly, they may consider only rules that contain a specific set of items. The naïve strategy is to apply such itemset constraints into the post-processing step. However, such approaches require much effort and time. This paper proposes the effective methods for integrating itemset constraints into the actual mining process. We proposed two algorithms, namely MSRIC-R and MSRIC-P, to solve this problem in which MSRIC-R pushed the constraints into the rule generating phase, and MSRIC-P pushes into the pattern mining phase. Experiments show that the proposed algorithms outperform the post-processing approach.},
  archive      = {J_APIN},
  author       = {Van, Trang and Le, Bac},
  doi          = {10.1007/s10489-020-02153-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7208-7220},
  shortjournal = {Appl. Intell.},
  title        = {Mining sequential rules with itemset constraints},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Celiac trunk segmentation incorporating with additional
contour constraint. <em>APIN</em>, <em>51</em>(10), 7196–7207. (<a
href="https://doi.org/10.1007/s10489-021-02221-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the vascular direction of celiac trunk plays an important role in the resection of gastric cancer as it can help doctors to plan the specific gastrectomy. The traditional manual estimation is the current clinical standard but time-consuming and subjective. Still, it is a challenging task for current computer vision methods to fully use the volume data in computer-aided clinical diagnosis because the segmented contour of object is of low quality and the small target can’t be completely segmented in the existing methods of convolutional neural network. In this paper, we proposed a novel contour constraint multi-branches network which used an additional contour segmentation branch as constraint for celiac trunk segmentation. The experiments show that our method outperforms other compared methods both quantitively and qualitatively, which provides effective framework for automated, accurate, and reliable vascular direction of celiac trunk estimation to help doctors in clinical diagnosis.},
  archive      = {J_APIN},
  author       = {Tang, Xianhua and Huang, Bo and Cai, Qingping and Wei, Ziran and Gao, Yongbin and Wang, Yinglin and Tong, Huilin and Liang, Pan and Zhong, Cengsi},
  doi          = {10.1007/s10489-021-02221-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7196-7207},
  shortjournal = {Appl. Intell.},
  title        = {Celiac trunk segmentation incorporating with additional contour constraint},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two robust long short-term memory frameworks for trading
stocks. <em>APIN</em>, <em>51</em>(10), 7177–7195. (<a
href="https://doi.org/10.1007/s10489-021-02249-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to find a superior strategy for the daily trading on a portfolio of stocks for which traditional trading strategies perform poorly due to the low frequency of new information. The experimental work is divided into a set of traditional trading strategies and a set of long short-term memory networks. The networks incorporate general and specific trading patterns, where the former takes into account the universal decision factors for trading across many stocks, while the latter takes into account stock-specific decision factors. Our research shows that both long short-term memory networks, regardless of whether they are based on universal or stock-specific decision factors, significantly outperform traditional trading strategies. Interestingly, however, on average neither has the edge compared to the other, thus remaining ambivalent as to whether universality or specificality is to be preferred when it comes to designing long short-term memory networks for optimal trading.},
  archive      = {J_APIN},
  author       = {Fister, Dušan and Perc, Matjaž and Jagrič, Timotej},
  doi          = {10.1007/s10489-021-02249-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7177-7195},
  shortjournal = {Appl. Intell.},
  title        = {Two robust long short-term memory frameworks for trading stocks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond AP: A new evaluation index for multiclass
classification task accuracy. <em>APIN</em>, <em>51</em>(10), 7166–7176.
(<a href="https://doi.org/10.1007/s10489-021-02223-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Average precision (AP) and many other related evaluation indices have been employed ubiquitously in classification tasks for a long time. However, they have defects and can hardly provide both overall evaluations and individual evaluations. In practice, we have to strike a balance between whole and individual performances to satisfy diverse demands. To this end, we propose a new index for multiclass classification tasks, named ${R^{\prime }}$ , which is an unbiased estimator of AP. Specifically, we improve the R index by taking the numerical differences between the real labels and predicted labels of each class into consideration. We evaluate its effectiveness and robustness on the MNIST and CIFAR-10 datasets. Experimental results show that it is positively correlated with some related indices. More importantly, we can obtain both overall and individual evaluations, which can be beneficial for improving training processes and model selection. Furthermore, as an evaluation architecture, the index can be promoted to evaluate any classification task, thereby implying broad application prospects.},
  archive      = {J_APIN},
  author       = {Zhang, Kaifang and Su, Huayou and Dou, Yong},
  doi          = {10.1007/s10489-021-02223-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7166-7176},
  shortjournal = {Appl. Intell.},
  title        = {Beyond AP: A new evaluation index for multiclass classification task accuracy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpreting the black box of supervised learning models:
Visualizing the impacts of features on prediction. <em>APIN</em>,
<em>51</em>(10), 7151–7165. (<a
href="https://doi.org/10.1007/s10489-021-02255-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models have been widely used in various domains. However, the internal mechanisms of popular models, such as neural networks and support vector machines, are difficult for humans to understand; such models are often called “black boxes”. In this study, a general method is proposed to gain insight into the black boxes of supervised learning models by visualizing the impacts of input features on their prediction results. Compared with the existing methods, which may overlook the overall understanding of prediction models by analyzing the feature impacts for each individual observation or ignore the impact differences by providing a single impact pattern for all observations, the proposed method distinguishes some typical impact patterns that correspond to different groups of observations. The method maps the detected impact patterns into feature space using tree rules that help locate the impact patterns in the feature space. More importantly, the feature relationships embedded in the prediction models can be revealed through this tree rule-based feature relationship network. We apply the proposed method to various simulated and real data, and the results demonstrate how it can help us understand how features affect model prediction results and the relationships among features.},
  archive      = {J_APIN},
  author       = {Zhang, Xiaohang and Wang, Yuan and Li, Zhengren},
  doi          = {10.1007/s10489-021-02255-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7151-7165},
  shortjournal = {Appl. Intell.},
  title        = {Interpreting the black box of supervised learning models: Visualizing the impacts of features on prediction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cornerstone network with feature extractor: A metric-based
few-shot model for chinese natural sign language. <em>APIN</em>,
<em>51</em>(10), 7139–7150. (<a
href="https://doi.org/10.1007/s10489-020-02170-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StandardChinese natural sign language (CNSL) contains over 8,000 words. We consider dividing the task of CNSL recognition into multiple subtasks. Few-shot learning on subtasks can achieve minimal acquisition cost and short-term training. However, the existing few-shot learning methods do not take into account the impact of ill-conditioned support samples, so we propose a new metric-based model, Cornerstone Network (CN), to complete the subtasks. CN is mainly composed of feature extractor (optional), embedding network and cornerstone generator. The cornerstone generator is designed as a semi-supervised clusterer. Compared with other metric-based few-shot models, CN without feature extractor improves 5-shot accuracy on Omniglot and miniImageNet. In order to verify the feasibility of our model on the task of CNSL recognition, we expanded the Chinese Natural Sign Language database, from CNSL-80 to CNSL-139, which integrates surface electromyography and inertial signals. The 5-shot accuracy on CNSL-139 increases from 65.25% to 68.83% comparing with the state-of-art model. After connecting with the 1-D convolution feature extractor using Siamese Network’s idea for secondary training, the accuracy increases by 10.38%. During the online test, the feature vector norms are used for selective matching. Although the accuracy drops, it is still at least 5% higher than that without feature extractor. Experimental results confirm the effectiveness of our model on 2-D images and 1-D time-series signals and the improvement of real-time recognition by SM.},
  archive      = {J_APIN},
  author       = {Wang, Fei and Li, Chen and Zeng, Zhen and Xu, Ke and Cheng, Sirui and Liu, Yanjun and Sun, Shizhuo},
  doi          = {10.1007/s10489-020-02170-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7139-7150},
  shortjournal = {Appl. Intell.},
  title        = {Cornerstone network with feature extractor: A metric-based few-shot model for chinese natural sign language},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group decision support model based on sequential additive
complementary pairwise comparisons. <em>APIN</em>, <em>51</em>(10),
7122–7138. (<a
href="https://doi.org/10.1007/s10489-021-02248-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In group decision support systems, it is important on how to process and manage individual decision information. In the paper, a sequential model is proposed to manage individual judgements with additively reciprocal property over paired alternatives. The process of realizing additive complementary pairwise comparisons (ACPCs) is captured. A real-time feedback mechanism is constructed to address the irrational behavior of individuals. An optimization model is established and solved by using the particle swarm optimization (PSO) algorithm, such that the consistency of individual judgements can be improved fast yet effectively. For the aggregation of individual decision information in group decision making (GDM), the weighted averaging operator is used. It is found that when all individual judgements are acceptably additively consistent, the collective matrix is with acceptable additive consistency. Under the control of individual consistency degrees, the approach of reaching consensus in GDM is further proposed. By comparing with some existing models, the observations reveal that the sequential model of originating additive complementary pairwise comparisons possesses the ability to rationally manage individual decision information.},
  archive      = {J_APIN},
  author       = {Liu, Fang and Zhang, Jia-Wei and Luo, Zhang-Hua},
  doi          = {10.1007/s10489-021-02248-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7122-7138},
  shortjournal = {Appl. Intell.},
  title        = {Group decision support model based on sequential additive complementary pairwise comparisons},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Emotion-cause span extraction: A new task to emotion cause
identification in texts. <em>APIN</em>, <em>51</em>(10), 7109–7121. (<a
href="https://doi.org/10.1007/s10489-021-02188-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional cause identification (ECI) is an important task for emotion analysis, aiming to identify the causes behind a certain emotion expressed in the text. Most of the previous studies are restricted to the clause-level binary classification, which have ignored an important fact that not all words in the clause are useful cause information for people to express emotions. In this work, we propose a new task: emotion-cause span extraction (ECSE), which is capable of obtaining more accurate and effective emotion causes. Inspired by recent advances in using joint learning approaches to ECI, we propose a novel joint learning framework for emotion-cause span extraction and span-based emotion classification so as to better address the ECSE task. Taken as the default backbone network, the Bidirectional Encoder Representations from Transformers (BERT) is used to encode multiple words and serve contextualized token representations. Furthermore, we also propose a multi-attention mechanism with emotional context awareness and a relative position learning mechanism on word-level, which is able to further capture the mutual interactions between the emotion clauses and candidate spans. According to the experimental results on a benchmark emotion cause corpus, it proves the reliability of the ECSE task and the effectiveness of our approach. In addition, through an in-depth analysis of traditional ECI task by converting ECSE into the clause-level binary classification task, we achieve the best performance among the systems in comparison, which further demonstrates the feasibility of the new ECSE task.},
  archive      = {J_APIN},
  author       = {Li, Min and Zhao, Hui and Su, Hao and Qian, YuRong and Li, Ping},
  doi          = {10.1007/s10489-021-02188-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7109-7121},
  shortjournal = {Appl. Intell.},
  title        = {Emotion-cause span extraction: A new task to emotion cause identification in texts},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised deep learning approach for network intrusion
detection combining convolutional autoencoder and one-class SVM.
<em>APIN</em>, <em>51</em>(10), 7094–7108. (<a
href="https://doi.org/10.1007/s10489-021-02205-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement in network technologies, the need for cybersecurity has gained increasing momentum in recent years. As a primary defense mechanism, an intrusion detection system (IDS) is expected to adapt and secure the computing infrastructures from the ever-changing sophisticated threat landscape. Many deep learning approaches have recently been proposed; however, these techniques face significant challenges in identifying all types of attacks, especially rare attacks due to network traffic imbalances and the lack of a sufficient number of abnormal traffic samples for model training. To overcome these shortcomings and improve detection performance, this paper presents an unsupervised deep learning approach for intrusion detection. Unlike the existing IDS model that extracts features and trains a classifier in two separate stages, a single-stage IDS approach that integrates a one-dimensional convolutional autoencoder (1D CAE) and a one-class support vector machine (OCSVM) as a classifier into a joint optimization framework is introduced in this paper for the first time. Using only the normal traffic samples, the approach simultaneously optimizes the 1D CAE for compact feature representation and the OCSVM for classification by defining a unified objective function combining reconstruction error with classification error. Thus, the generated compact feature representation has not only reconstruction ability but also discriminative ability for classification. An in-depth ablation analysis validates the design decisions and provides further insight of the proposed approach. An extensive set of experiments on two benchmark intrusion datasets, NSL-KDD and UNSW-NB15, demonstrates the generalization ability of the proposed model for unseen attacks and confirms it as a competitive approach over the recent state-of-the-art intrusion detection baselines. Overall, the obtained results emphasize that the proposed approach has potential to serve as a baseline for building an effective IDS.},
  archive      = {J_APIN},
  author       = {Binbusayyis, Adel and Vaiyapuri, Thavavel},
  doi          = {10.1007/s10489-021-02205-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7094-7108},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised deep learning approach for network intrusion detection combining convolutional autoencoder and one-class SVM},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Least squares large margin distribution machine for
regression. <em>APIN</em>, <em>51</em>(10), 7058–7093. (<a
href="https://doi.org/10.1007/s10489-020-02166-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Better prediction ability is the main objective of any regression-based model. Large margin Distribution Machine for Regression (LDMR) is an efficient approach where it tries to reduce both loss functions, i.e. ε-insensitive and quadratic loss to diminish the effects of outliers. However, still, it has a significant drawback, i.e. high computational complexity. To achieve the improved generalization of the regression model with less computational cost, we propose an enhanced form of LDMR named as Least Squares Large margin Distribution Machine-based Regression (LS-LDMR) by transforming the inequality conditions alleviate to equality conditions. The elucidation is attained by handling a system of linear equations where we need to measure the inverse of the matrix only. Hence, there is no need to solve the large size of the quadratic programming problem, unlike in the case of other regression-based algorithms as SVR, Twin SVR, and LDMR. The numerical experiment has been performed on the benchmark real-life datasets along with synthetically generated datasets by using the linear and Gaussian kernel. All the experiments of presented LS-LDMR are analyzed with standard SVR, Twin SVR, primal least squares Twin SVR (PLSTSVR), ε-Huber SVR (ε-HSVR), ε-support vector quantile regression (ε-SVQR), minimum deviation regression (MDR), and LDMR, which shows the effectiveness and usability of LS-LDMR. This approach is also statistically validated and verified in terms of various metrics.},
  archive      = {J_APIN},
  author       = {Gupta, Umesh and Gupta, Deepak},
  doi          = {10.1007/s10489-020-02166-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7058-7093},
  shortjournal = {Appl. Intell.},
  title        = {Least squares large margin distribution machine for regression},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient attention module for 3d convolutional neural
networks in action recognition. <em>APIN</em>, <em>51</em>(10),
7043–7057. (<a
href="https://doi.org/10.1007/s10489-021-02195-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to illumination changes, varying postures, and occlusion, accurately recognizing actions in videos is still a challenging task. A three-dimensional convolutional neural network (3D CNN), which can simultaneously extract spatio-temporal features from sequences, is one of the mainstream models for action recognition. However, most of the existing 3D CNN models ignore the importance of individual frames and spatial regions when recognizing actions. To address this problem, we propose an efficient attention module (EAM) that contains two sub-modules, that is, a spatial efficient attention module (EAM-S) and a temporal efficient attention module (EAM-T). Specifically, without dimensionality reduction, EAM-S concentrates on mining category-based correlation by local cross-channel interaction and assigns high weights to important image regions, while EAM-T estimates the importance score of different frames by cross-frame interaction between each frame and its neighbors. The proposed EAM module is lightweight yet effective, and it can be easily embedded into 3D CNN-based action recognition models. Extensive experiments on the challenging HMDB-51 and UCF-101 datasets showed that our proposed module achieves state-of-the-art performance and can significantly improve the recognition accuracy of 3D CNN-based action recognition methods.},
  archive      = {J_APIN},
  author       = {Jiang, Guanghao and Jiang, Xiaoyan and Fang, Zhijun and Chen, Shanshan},
  doi          = {10.1007/s10489-021-02195-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7043-7057},
  shortjournal = {Appl. Intell.},
  title        = {An efficient attention module for 3d convolutional neural networks in action recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Covariance matrix forecasting using support vector
regression. <em>APIN</em>, <em>51</em>(10), 7029–7042. (<a
href="https://doi.org/10.1007/s10489-021-02217-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector regression is a promising method for time-series prediction, as it has good generalisability and an overall stable behaviour. Recent studies have shown that it can describe the dynamic characteristics of financial processes and make more accurate forecasts than other machine learning techniques. The first main contribution of this paper is to propose a methodology for dynamic modelling and forecasting covariance matrices based on support vector regression using the Cholesky decomposition. The procedure is applied to range-based covariance matrices of returns, which are estimated on the basis of low and high prices. Such prices are most often available with closing prices for many financial series and contain more information about volatility and relationships between returns. The methodology guarantees the positive definiteness of the forecasted covariance matrices and is flexible, as it can be applied to different dependence patterns. The second contribution of the paper is to show with an example of the exchange rates from the forex market that the covariance matrix forecasts calculated using the proposed approach are more accurate than the forecasts from the benchmark dynamic conditional correlation model. The advantage of the suggested procedure is higher during turbulent periods, i.e., when forecasting is the most difficult and accurate forecasts matter most.},
  archive      = {J_APIN},
  author       = {Fiszeder, Piotr and Orzeszko, Witold},
  doi          = {10.1007/s10489-021-02217-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7029-7042},
  shortjournal = {Appl. Intell.},
  title        = {Covariance matrix forecasting using support vector regression},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HFPQ: Deep neural network compression by hardware-friendly
pruning-quantization. <em>APIN</em>, <em>51</em>(10), 7016–7028. (<a
href="https://doi.org/10.1007/s10489-020-01968-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a hardware-friendly compression method for deep neural networks. This method effectively combines layered channel pruning with quantization by a power exponential of 2. While keeping a small decrease in the accuracy of the network model, the computational resources for neural networks to be deployed on the hardware are greatly reduced. These computing resources for hardware resolution include memory, multiple accumulation cells (MACs), and many logic gates for neural networks. Layered channel pruning groups the different layers by decreasing the model accuracy of the pruned network. After pruning each layer in a specific order, the network is retrained. The pruning method in this paper sets a parameter, that can be adjusted to meet different pruning rates in practical applications. The quantization method converts high-precision weights to low-precision weights. The latter are all composed of 0 and powers of 2. In the same way, another parameter is set to control the quantized bit width, which can also be adjusted to meet different quantization precisions. The hardware-friendly pruning quantization (HFPQ) method proposed in this paper trains the network after pruning and then quantizes the weights. The experimental results show that the HFPQ method compresses VGGNet, ResNet and GoogLeNet by 30+ times while reducing the number of FLOPs by more than 85%.},
  archive      = {J_APIN},
  author       = {Fan, YingBo and Pang, Wei and Lu, ShengLi},
  doi          = {10.1007/s10489-020-01968-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {7016-7028},
  shortjournal = {Appl. Intell.},
  title        = {HFPQ: Deep neural network compression by hardware-friendly pruning-quantization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Missing multi-label learning with non-equilibrium based on
two-level autoencoder. <em>APIN</em>, <em>51</em>(10), 6997–7015. (<a
href="https://doi.org/10.1007/s10489-020-02140-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a multi-label learning framework, each instance may belong to multiple labels simultaneously. The classification accuracy can be improved significantly by exploiting various correlations, such as label correlations, feature correlations, or the correlations between features and labels. There are few studies on how to combine the feature and label correlations, and they deal more with complete data sets. However, missing labels or other phenomena often occur because of the cost or technical limitations in the data acquisition process. A few label completion algorithms currently suitable for missing multi-label learning, ignore the noise interference of the feature space. At the same time, the threshold of the discriminant function often affects the classification results, especially those of the labels near the threshold. All these factors pose considerable difficulties in dealing with missing labels using label correlations. Therefore, we propose a missing multi-label learning algorithm with non-equilibrium based on a two-level autoencoder. First, label density is introduced to enlarge the classification margin of the label space. Then, a new supplementary label matrix is augmented from the missing label matrix with the non-equilibrium label completion method. Finally, considering feature space noise, a two-level kernel extreme learning machine autoencoder is constructed to implement the information feature and label correlation. The effectiveness of the proposed algorithm is verified by many experiments on both missing and complete label data sets. A statistical analysis of hypothesis validates our approach.},
  archive      = {J_APIN},
  author       = {Cheng, Yusheng and Song, Fan and Qian, Kun},
  doi          = {10.1007/s10489-020-02140-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6997-7015},
  shortjournal = {Appl. Intell.},
  title        = {Missing multi-label learning with non-equilibrium based on two-level autoencoder},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional neural networks with hybrid weights for 3D
point cloud classification. <em>APIN</em>, <em>51</em>(10), 6983–6996.
(<a href="https://doi.org/10.1007/s10489-021-02240-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of 3D point clouds is a regular task, but remains a highly challenging problem because 3D point clouds usually contain a large amount of information on irregular shapes. Several recent studies have shown the excellent performance of deep learning in 3D point cloud classification. Convolutional neural network (CNN)-based 3D point cloud classification methods are also increasingly used owing to their efficient and convenient feature extraction capability. However, most of these methods do not take much prior information and local structural information into consideration, often resulting in their inability to extract sufficient information to improve the classification accuracy. In this study, we present a novel convolution operation named HyConv, which includes two key components. First, inspired by 2D convolution, we design a feature transformation module to capture more local structural information. Second, to extract the prior information, a hybrid weight module is introduced to estimate two types of weights on the basis of the distribution information of the spatial and feature domains. Additionally, we propose an adaptive method to learn hybrid weights to obtain hybrid distribution information. Finally, based on the proposed convolutional operator HyConv, we build a deep neural network Hybrid-CNN and conduct experiments on two commonly used datasets. The results show that our hybrid network outperforms most existing methods on ModelNet40. Furthermore, state-of-the-art performance is achieved with ScanObjectNN, which is a great improvement compared with existing methods.},
  archive      = {J_APIN},
  author       = {Hu, Meng and Ye, Hailiang and Cao, Feilong},
  doi          = {10.1007/s10489-021-02240-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6983-6996},
  shortjournal = {Appl. Intell.},
  title        = {Convolutional neural networks with hybrid weights for 3D point cloud classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A priority based genetic algorithm for limited view
tomography. <em>APIN</em>, <em>51</em>(10), 6968–6982. (<a
href="https://doi.org/10.1007/s10489-021-02192-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography is a noninvasive method to know the internal structure of the objects. It has a wide range of applications, i.e. engineering and medical application. This article presents an adaptive supper parents based GA for limited view tomography. Here, we propose two novel algorithms, namely, single supper based GA and multi supper parents based GA. These algorithms are suitable for engineering as well as the medical application for limited view data or sparse data. This article proposes two novel parental selection strategy. This strategy includes all the advantages of deterministic selection methods and stochastic selection methods. These novel selection methods reduce the loss of diversity using the distribution of the selection opportunity of the entire population members. The proposed algorithm uses adaptive crossover and mutation function that function increase the convergence rate, and the method ensure the consistent performance of the algorithm. Experimental results reveal that the proposed algorithm produces satisfactory results with low computation overhead. The proposed algorithm outperforms with other states of the art algorithm for limited view data or sparse data.},
  archive      = {J_APIN},
  author       = {Mishra, Raghavendra and Bajpai, Manish Kumar},
  doi          = {10.1007/s10489-021-02192-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6968-6982},
  shortjournal = {Appl. Intell.},
  title        = {A priority based genetic algorithm for limited view tomography},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive collaborative optimization of traffic network
signal timing based on immune-fireworks algorithm and hierarchical
strategy. <em>APIN</em>, <em>51</em>(10), 6951–6967. (<a
href="https://doi.org/10.1007/s10489-021-02256-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary urban areas, the construction speed of urban roads lags far behind the growth rate of the number of vehicles. The traffic delay caused by excessive vehicles is a major challenge for modern transportation systems. In this paper, we design an adaptive coordination traffic signal control system based on a three-level framework. In the framework, we propose a hierarchical strategy, which is helpful in avoiding possible offset conflicts and configuring the offsets reasonably. In addition, we establish a multi-intersection traffic signal control model with the goal of minimizing traffic delays. To solve the proposed model, we propose a new algorithm, called the Immune-Fireworks algorithm (IM-FWA), on the basis of the artificial immune and fireworks algorithms. Inspired by the antibody maintenance mechanism, diversity mechanism and communication mechanism of the artificial immune algorithm, IM-FWA can effectively overcome the shortcomings of the fireworks algorithm, such as its limited search range and lack of interaction among fireworks. The experiments show that the proposed model and algorithm have good practicability and that our control system can obtain a better signal timing schedule to effectively reduce traffic delays.},
  archive      = {J_APIN},
  author       = {Qiao, Zhimin and Ke, Liangjun and Zhang, Gewei and Wang, Xiaoqiang},
  doi          = {10.1007/s10489-021-02256-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6951-6967},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive collaborative optimization of traffic network signal timing based on immune-fireworks algorithm and hierarchical strategy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wise-local response convolutional neural network based on
naïve bayes theorem for rotating machinery fault classification.
<em>APIN</em>, <em>51</em>(10), 6932–6950. (<a
href="https://doi.org/10.1007/s10489-021-02252-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault identification is a vital task to ensure the integrity and reliability of rotating machinery. The vibration signals produced by the defective system components typically bear a significant amount of noise, including non-linear and non-stationary characteristics induced by the intricate operational environment. Advanced signal processing technologies still have difficulty in detecting faults in mechanical systems. This paper presents a wise local response convolution neural network-based Naïve Bayes algorithm (WCNN-NB) to identify multiple faults in rotating machines. In WCNN-NB, the WCNN structure is first used to characterize the features of the gray-scale images transformed from the original vibration signals. The nonlinearity parameter value of WCNN is explored in order to enhance learning efficiency. The NB algorithm is then used as a robust steady-state method to identify the learned features. Experimental data regarding helical gears and bearing test rigs are used to validate the feasibility of the WCNN-NB model. The superiority of the classification results is verified by comparing the current model with the WCNN-support vector machine, WCNN-random forest, standard CNN, the support vector machine (SVM) and the neural backpropagation network (BP) models. The results demonstrate that the classification accuracy is 99.68%, 92.5% and 97.5% for three data sets with tolerable misclassification rates under all operational conditions.},
  archive      = {J_APIN},
  author       = {Aljemely, Anas H. and Xuan, Jianping and Xu, Long and Jawad, Farqad K. J. and Al-Azzawi, Osama},
  doi          = {10.1007/s10489-021-02252-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6932-6950},
  shortjournal = {Appl. Intell.},
  title        = {Wise-local response convolutional neural network based on naïve bayes theorem for rotating machinery fault classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-aware hybrid expertise retrieval system in community
question answering services. <em>APIN</em>, <em>51</em>(10), 6914–6931.
(<a href="https://doi.org/10.1007/s10489-020-02177-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a time-aware hybrid expertise retrieval (TaHER) system for community question answering (CQA) services. It comprises of a text-based part and a network-based part. The text-based part makes use of the textual and the temporal information associated with questions and answers. Moreover, it assesses the recent interests and the activities of answerers. For a given question, it determines the knowledge of each answerer and identify active answerers with adequate knowledge. The network-based part is composed of several period-dependent networks. It uses the relationships among the answerers along with temporal information. Next, it applies a link analysis technique on the networks to determine the time-aware authority of each answerer in the community. We, nonetheless, propose a fusion strategy for combining the offshoots of these two parts. Using 5 performance measures, TaHER system is compared with 20 state-of-the-art algorithms on 4 real-world datasets. According to our experiments, in 93.75% (375 out of 400) cases, the proposed approach outperforms the comparing approaches. We also experimentally validate the importance of each assumption used by us.},
  archive      = {J_APIN},
  author       = {Kundu, Dipankar and Pal, Rajat Kumar and Mandal, Deba Prasad},
  doi          = {10.1007/s10489-020-02177-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6914-6931},
  shortjournal = {Appl. Intell.},
  title        = {Time-aware hybrid expertise retrieval system in community question answering services},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-city traffic flow forecasting via multi-task learning.
<em>APIN</em>, <em>51</em>(10), 6895–6913. (<a
href="https://doi.org/10.1007/s10489-020-02074-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow forecasting or prediction plays an important role in the traffic control and management of a city. Existing works mostly train a model using the traffic flow data of a city and then test the trained model using the data of the same city. It may not be truly intelligent as there are many cities around us and there should be some shared knowledge among different cities. The data of a city and its knowledge can be used to help improve the traffic flow forecasting of other cities. To address this motivation, we study building a universal deep learning model for multi-city traffic flow forecasting. In this paper, we exploit spatial-temporal correlations among different cities with multi-task learning to approach the traffic flow forecasting tasks of multiple cities. As a result, we propose a M ulti-city T raffic flow forecasting N etwork (MTN) via multi-task learning to extract the spatial dependency and temporal regularity among multiple cities later used to improve the performance of each individual city traffic flow forecasting collaboratively. In brief, the proposed model is a quartet of methods: (1) It integrates three temporal intervals and formulates a multi-interval component for each city to extract temporal features of each city; (2) A spatial-temporal attention layer with 3D Convolutional kernels is plugged into the neural networks to learn spatial-temporal relationship; (3) As traffic peak distributions of different cities are often similar, it proposes to use a peak zoom network to learn the peak effect of multiple cities and enhance the prediction performance on important time steps in different cities; (4) It uses a fusion layer to merge the outputs from distinct temporal intervals for the final forecasting results. Experimental results using real-world datasets from DIDI show the superior performance of the proposed model.},
  archive      = {J_APIN},
  author       = {Zhang, Yiling and Yang, Yan and Zhou, Wei and Wang, Hao and Ouyang, Xiaocao},
  doi          = {10.1007/s10489-020-02074-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6895-6913},
  shortjournal = {Appl. Intell.},
  title        = {Multi-city traffic flow forecasting via multi-task learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-local duplicate pooling network for salient object
detection. <em>APIN</em>, <em>51</em>(10), 6881–6894. (<a
href="https://doi.org/10.1007/s10489-020-02147-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing salient object detection methods are dedicated to fusing features from different levels of a pre-trained convolutional neural network (CNN). However, these methods can easily lead to internal discontinuities within the salient objects because of unreasonable feature fusion strategies and short-range dependencies resulting from common convolution and pooling operations. In this paper, we propose a novel non-local duplicate pooling (NLDP) network to overcome these internal discontinuities. NLDP begins by removing the first few convolutional layers of a classic CNN, which have small receptive fields and require large amounts of calculation. A novel duplicate pooling module (DPM) is then used to generate richer and more detailed saliency maps. This is achieved by constructing a double-pathway that can integrating partial feature maps. Within the DPM, a non-local module (NLM) is used to obtain long-range dependencies. This enhances the internal continuities between the saliency maps. Comprehensive experiments conducted on six benchmark datasets have confirmed the increased effectiveness and detection speed of our method in relation to other salient object detection methods.},
  archive      = {J_APIN},
  author       = {Jiao, Jun and Xue, Hui and Ding, Jundi},
  doi          = {10.1007/s10489-020-02147-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6881-6894},
  shortjournal = {Appl. Intell.},
  title        = {Non-local duplicate pooling network for salient object detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A method of credit evaluation modeling based on block-wise
missing data. <em>APIN</em>, <em>51</em>(10), 6859–6880. (<a
href="https://doi.org/10.1007/s10489-021-02225-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common problem in credit evaluation practice and can obstruct the development and application of an evaluation model. Block-wise missing data is a particularly troublesome issue. Based on multi-task feature selection approach, this paper proposes a method called MMPFS to build a model for credit evaluation that primarily includes two steps: (1) dividing the dataset into several nonoverlapping subsets based on missing patterns, and (2) integrating the multi-task feature selection approach using logistic regression to perform joint feature learning on all subsets. The proposed method has the following advantages: (1) missing data do not need to be managed in advance, (2) available data can be fully used for model learning, (3) information loss or bias caused by general missing data processing methods can be avoided, and (4) overfitting risk caused by redundant features can be reduced. The implementation framework and algorithm principle of the proposed method are described, and three credit datasets from UCI are investigated to compare the proposed method with other commonly used missing data treatments. The results show that MMPFS can produce a better credit evaluation model than data preprocessing methods, such as sample deletion and data imputation.},
  archive      = {J_APIN},
  author       = {Lan, Qiujun and Jiang, Shan},
  doi          = {10.1007/s10489-021-02225-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6859-6880},
  shortjournal = {Appl. Intell.},
  title        = {A method of credit evaluation modeling based on block-wise missing data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive boosting algorithm based on weighted feature
selection and category classification confidence. <em>APIN</em>,
<em>51</em>(10), 6837–6858. (<a
href="https://doi.org/10.1007/s10489-020-02184-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive boosting (Adaboost) is a typical ensemble learning algorithm, which has been studied and widely used in classification tasks. Traditional Adaboost algorithms ignore the sample weights while selecting the most useful features, and most of them ignore the fact that the performances of weak classifiers on each category are always different. On this basis, a weighted feature selection and category classification confidence based Adaboost algorithm is proposed in this paper. The first contribution, is that we propose a weighted feature selection to select the most useful features, which can both distinguish the majority of all samples and the previous misclassified samples. The second contribution, is that we improve the traditional error rate calculation method and propose a category based error rate calculation method to combine the classification abilities of Adaboost on different categories. A detailed performances comparison of various Adaboost algorithms are carried out on eight typical datasets. The experimental results show that the proposed algorithm obtains significant improvement on classification accuracy compared to typical Adaboost algorithms when different datasets especially the unbalanced datasets are used.},
  archive      = {J_APIN},
  author       = {Wang, Youwei and Feng, Lizhou},
  doi          = {10.1007/s10489-020-02184-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6837-6858},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive boosting algorithm based on weighted feature selection and category classification confidence},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving and verifiable multi-instance iris remote
authentication using public auditor. <em>APIN</em>, <em>51</em>(10),
6823–6836. (<a
href="https://doi.org/10.1007/s10489-021-02187-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic Encryption (HE) is the most widely explored research area to construct privacy-preserving biometric authentication systems due to its advantages over cancelable biometrics and biometric cryptosystem. However, most of the existing privacy-preserving biometric authentication systems using HE assume that the server performs computations honestly. In a malicious server setting, the server may return an arbitrary result to save the computational resources results in false accept/reject. To address this, we propose a privacy-preserving and verifiable multi-instance iris authentication using public auditor (PviaPA). Paillier HE provides confidentiality for the iris templates in PviaPA. A public auditor ensures the correctness of comparator result in PviaPA. Extensive experimental results on benchmark iris databases demonstrate that PviaPA provides privacy to the iris templates with no loss in the accuracy as well as trust on the comparator result.},
  archive      = {J_APIN},
  author       = {Morampudi, Mahesh Kumar and Prasad, Munaga V. N. K. and Raju, U. S. N.},
  doi          = {10.1007/s10489-021-02187-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6823-6836},
  shortjournal = {Appl. Intell.},
  title        = {Privacy-preserving and verifiable multi-instance iris remote authentication using public auditor},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative rating prediction for neighborhood-based
collaborative filtering. <em>APIN</em>, <em>51</em>(10), 6810–6822. (<a
href="https://doi.org/10.1007/s10489-021-02237-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the issue of rating prediction for neighborhood-based collaborative filtering in recommendation systems. A novel rating prediction algorithm, called iterative rating prediction (IRP), is proposed for neighborhood-based collaborative filtering. The main idea behind IRP is neighborhood propagation. To predict ratings of items for target users, IRP relies on not only the rating information of direct neighbors but also that of indirect neighbors with different propagation depth. To implement the idea, IRP iteratively updates the ratings of items for users. The efficiency of the proposed method is examined through extensive experiments. Experimental results demonstrate the superior performance of our method, especially on small-scaled and sparse datasets.},
  archive      = {J_APIN},
  author       = {Zhang, Li and Li, Zepeng and Sun, Xiaohan},
  doi          = {10.1007/s10489-021-02237-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6810-6822},
  shortjournal = {Appl. Intell.},
  title        = {Iterative rating prediction for neighborhood-based collaborative filtering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FHUQI-miner: Fast high utility quantitative itemset mining.
<em>APIN</em>, <em>51</em>(10), 6785–6809. (<a
href="https://doi.org/10.1007/s10489-021-02204-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility itemset mining is a popular pattern mining task, which aims at revealing all sets of items that yield a high profit in a transaction database. Although this task is useful to understand customer behavior, an important limitation is that high utility itemsets do not provide information about the purchase quantities of items. Recently, some algorithms were designed to address this issue by finding quantitative high utility itemsets but they can have very long execution times due to the larger search space. This paper addresses this issue by proposing a novel efficient algorithm for high utility quantitative itemset mining, called FHUQI-Miner (Fast High Utility Quantitative Itemset Miner). It performs a depth-first search and adopts two novel search space reduction strategies, named Exact Q-items Co-occurrence Pruning Strategy (EQCPS) and Range Q-items Co-occurrence Pruning Strategy (RQCPS). Experimental results show that the proposed algorithm is much faster than the state-of-art HUQI-Miner algorithm on sparse datasets.},
  archive      = {J_APIN},
  author       = {Nouioua, Mourad and Fournier-Viger, Philippe and Wu, Cheng-Wei and Lin, Jerry Chun-Wei and Gan, Wensheng},
  doi          = {10.1007/s10489-021-02204-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6785-6809},
  shortjournal = {Appl. Intell.},
  title        = {FHUQI-miner: Fast high utility quantitative itemset mining},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diversity and consistency embedding learning for multi-view
subspace clustering. <em>APIN</em>, <em>51</em>(10), 6771–6784. (<a
href="https://doi.org/10.1007/s10489-020-02126-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of multi-view data, many multi-view clustering methods have been developed due to the effectiveness of exploiting the complementary information of multi-view data. However, most existing multi-view clustering methods have the following two drawbacks: (1) they usually explore the relationships between samples in the original space, where the high-dimensional features contain noise and outliers; (2) they only pay attention to exploring the consistency or enhancing the diversity of different views, such that the multi-view information cannot be completely utilized. In this paper, we propose a novel multi-view subspace clustering method, namely Diversity and Consistency Embedding Learning (DCEL), which learns a better affinity matrix in a learned latent embedding space while simultaneously considering diversity and consistency of multi-view data. Specifically, by leveraging a projection method, the multi-view data in the latent embedding space can be learned. Then, with the self-expression property, we seek a shared consistent representation among all views and a set of diverse representations of each view to better learn an affinity matrix in the latent embedding space. Furthermore, we develop an optimization scheme based on the alternating direction method of multipliers (ADMM) to solve the proposed method. Experimental evaluations on five benchmark datasets show the superiority of our method, compared with two single-view clustering methods and some state-of-the-art multi-view clustering methods.},
  archive      = {J_APIN},
  author       = {Mi, Yong and Ren, Zhenwen and Mukherjee, Mithun and Huang, Yuqing and Sun, Quansen and Chen, Liwan},
  doi          = {10.1007/s10489-020-02126-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6771-6784},
  shortjournal = {Appl. Intell.},
  title        = {Diversity and consistency embedding learning for multi-view subspace clustering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection and hyper parameters optimization for
short-term wind power forecast. <em>APIN</em>, <em>51</em>(10),
6752–6770. (<a
href="https://doi.org/10.1007/s10489-021-02191-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate wind power forecasting plays an increasingly significant role in power grid normal operation with large-scale wind energy. The precise and stable forecasting of wind power with short computational time is still a challenge owing to various uncertainty factors. This study proposes a hybrid model based on a data prepossessing strategy, a modified Bayesian optimization algorithm, and the gradient boosted regression trees approach. More specifically, the powerful information mining ability of maximum information coefficient is used to select the important input features, and the modified Bayesian optimization algorithm is introduced to optimize the hyperparameters of the gradient boosted regression trees to acquire more satisfactory forecasting precision and computation cost. Datasets from a Chinese wind farm are used in case studies to analyze the prediction accuracy, stability, and computation efficiency of the proposed model. The point forecasting and multi-step forecasting results reveal that the performance of the hybrid forecasting model positively exceeds all the contrasted models. The developed model is extremely useful for enhancing prediction precision and is a reasonable and valid tool for online prediction with increasing data.},
  archive      = {J_APIN},
  author       = {Huang, Hui and Jia, Rong and Shi, Xiaoyu and Liang, Jun and Dang, Jian},
  doi          = {10.1007/s10489-021-02191-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6752-6770},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection and hyper parameters optimization for short-term wind power forecast},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-target regression via self-parameterized lasso and
refactored target space. <em>APIN</em>, <em>51</em>(10), 6743–6751. (<a
href="https://doi.org/10.1007/s10489-021-02238-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target regression (MTR) aims at simultaneously predicting multiple continuous target variables based on the same set of input variables. It has been used to solve some challenging problems. A self-parameterized Lasso for MTR is proposed in this paper, which is applied to refactored target space via linear combinations of existing targets. Our approach can simultaneously model intrinsic inter-target correlations and input-target correlations, which makes full use of the information contained in the data. Meanwhile, this information helps automatically generate the parameters needed in the model. Compared with the common method, which requires a manual setting of parameters and is expensive to optimize these parameters, our method can save a lot of time while no reduction in performance. Besides, our method can be used not only for MTR tasks but also for multi-classification tasks. The experimental results show that our method performs well in different tasks and has a wide range of applications.},
  archive      = {J_APIN},
  author       = {Xiao, Xinshuang and Xu, Yitian},
  doi          = {10.1007/s10489-021-02238-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6743-6751},
  shortjournal = {Appl. Intell.},
  title        = {Multi-target regression via self-parameterized lasso and refactored target space},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A graph-based QoS prediction approach for web service
recommendation. <em>APIN</em>, <em>51</em>(10), 6728–6742. (<a
href="https://doi.org/10.1007/s10489-020-02120-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the Internet, the recommendation based on Quality of Service(QoS) is proven to be an efficient way to deal with the ever-increasing web services in both industry and academia. However, it is hard to make an accurate recommendation using sparse QoS data, which makes QoS prediction a growing concern in the context of web service recommendation. In this research, a novel Graph-based Matrix Factorization approach(GMF) is proposed for QoS prediction. First, a concept of integrated-graph is put forward to consolidate multi-source information from user–aware context and service-aware context, and to deep mine potential relationships based on QoS matrix. Furthermore, the integrated-graph is divided into several sub-graphs by cutting insignificant edges to reduce noises and strengthen interactions between users and services. Based on the local information of each sub-graph and the global information of integrated-graph, a Gaussian Mixture Model(GMM) of QoS value is built as a fusion method to combine local and global information adaptively and to complete final QoS prediction. The extensive experimental analysis on a publicly available dataset indicate that our graph-based method is both accurate and practical.},
  archive      = {J_APIN},
  author       = {Chang, Zhenhua and Ding, Ding and Xia, Youhao},
  doi          = {10.1007/s10489-020-02120-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6728-6742},
  shortjournal = {Appl. Intell.},
  title        = {A graph-based QoS prediction approach for web service recommendation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid multi-criteria decision making approach for
assessing health-care waste management technologies based on soft
likelihood function and d-numbers. <em>APIN</em>, <em>51</em>(10),
6708–6727. (<a
href="https://doi.org/10.1007/s10489-020-02148-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health-care waste (HCW) management is an important issue, especially in developing countries. How to choose the best management technology is a challenging and open subject in this issue. Limited work has been done, but there is still a lack of a technical approach that not only takes into account multi-granular linguistic terminology, but also considers the attitude characters of decision makers (DMs). To adress the above problem, in this paper a hybrid multi-criteria decision making scheme is proposed based on soft likelihood function and D-numbers. First, the D-numbers is used to characterize complex multi-grained decision information. Secondly, a novel soft likelihood function based on power ordered weighted averaging operator (POWA) is designed to effectively take into account the DMs’ preferences, which is then integrated into the proposed HCW management approach. Eventually, the effectiveness and superiority of the proposed approach is demonstrated through an application example. In particular, an intuitive advantage has been confirmed that the proposed method can adjust the gap between adjacent alternatives through decision preference to distinguish differences. This is expected to provide a reliable fault-tolerant interval for decision-making in HCW management, and further improve the reliability of the algorithm. In addition, the analysis and evaluation also confirm the reliability and practicality of the proposed technology.},
  archive      = {J_APIN},
  author       = {Mi, Xiangjun and Tian, Ye and Kang, Bingyi},
  doi          = {10.1007/s10489-020-02148-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6708-6727},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid multi-criteria decision making approach for assessing health-care waste management technologies based on soft likelihood function and D-numbers},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PIEED: Position information enhanced encoder-decoder
framework for scene text recognition. <em>APIN</em>, <em>51</em>(10),
6698–6707. (<a
href="https://doi.org/10.1007/s10489-021-02219-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition (STR) technology has a rapid development with the rise of deep learning. Recently, the encoder-decoder framework based on attention mechanism is widely used in STR for better recognition. However, the commonly used Long Short Term Memory (LSTM) network in the framework tends to ignore certain position or visual information. To address this problem, we propose a Position Information Enhanced Encoder-Decoder (PIEED) framework for scene text recognition, in which an addition position information enhancement (PIE) module is proposed to compensate the shortage of the LSTM network. Our module tends to retain more position information in the feature sequence, as well as the context information extracted by the LSTM network, which is helpful to improve the recognition accuracy of the text without context. Besides that, our fusion decoder can make full use of the output of the proposed module and the LSTM network, so as to independently learn and preserve useful features, which is helpful to improve the recognition accuracy while not increase the number of arguments. Our overall framework can be trained end-to-end only using images and ground truth. Extensive experiments on several benchmark datasets demonstrate that our proposed framework surpass state-of-the-art ones on both regular and irregular text recognition.},
  archive      = {J_APIN},
  author       = {Ma, Xitao and He, Kai and Zhang, Dazhuang and Li, Dashuang},
  doi          = {10.1007/s10489-021-02219-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6698-6707},
  shortjournal = {Appl. Intell.},
  title        = {PIEED: Position information enhanced encoder-decoder framework for scene text recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple noncooperative games with intuitionistic fuzzy
information and application in ecological management. <em>APIN</em>,
<em>51</em>(10), 6685–6697. (<a
href="https://doi.org/10.1007/s10489-021-02215-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The matrix game and bi-matrix game are typical non-cooperative games. Nowadays, interactive game decisions on social management are mainly concerned, especially when players are faced with uncertain payoffs. The aim of this paper is to develop simple and effective parameterized linear programming methods for solving two types of matrix games with payoffs expressed by intuitionistic fuzzy (IF) information. In these methods, the expected values of players are regarded as monotonic functions with their risk preferences. Hereby we construct two models of auxiliary bilinear programming, and the corresponding optimal strategies can be easily obtained by explicit computation. The models and methods proposed in this paper are applied in two ecological management problems, and the validity and applicability are verified.},
  archive      = {J_APIN},
  author       = {Yang, Jie and Xu, Zeshui and Dai, Yongwu},
  doi          = {10.1007/s10489-021-02215-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6685-6697},
  shortjournal = {Appl. Intell.},
  title        = {Simple noncooperative games with intuitionistic fuzzy information and application in ecological management},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse portfolio selection with uncertain probability
distribution. <em>APIN</em>, <em>51</em>(10), 6665–6684. (<a
href="https://doi.org/10.1007/s10489-020-02161-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designed as remedies for uncertain parameters and tiny optimal weights in the portfolio selection problem, we consider a class of distributionally robust portfolio optimization models with cardinality constraints. For considering the statistical significance and tractability, we construct two kinds of ambiguity sets based on L1-norm and moment information, respectively. The nominal distribution, as the core of the first ambiguity set, is determined by non-parametric estimation method. To reduce the disturbing error of the second ambiguity set, we apply a shrinkage estimation method to determine the moment information based on historical data. By introducing a binary variable, the proposed sparse portfolio optimization model can be converted equivalently to a tractable mixed-integer 0-1 programming problem, which can be dealt with efficiently by a modified primal-dual Benders’ decomposition method. Through the actual market data, we test the proposed models and show their validity. Furthermore, performances measured by net portfolio return, Sharpe ratio, and cumulative return are superior to the classical portfolio selection models in the back-testing of out-of-sample data.},
  archive      = {J_APIN},
  author       = {Huang, Ripeng and Qu, Shaojian and Yang, Xiaoguang and Xu, Fengmin and Xu, Zeshui and Zhou, Wei},
  doi          = {10.1007/s10489-020-02161-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6665-6684},
  shortjournal = {Appl. Intell.},
  title        = {Sparse portfolio selection with uncertain probability distribution},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bearing fault diagnosis based on combined multi-scale
weighted entropy morphological filtering and bi-LSTM. <em>APIN</em>,
<em>51</em>(10), 6647–6664. (<a
href="https://doi.org/10.1007/s10489-021-02229-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of industry and technology, mechanical systems’ safety has strong relations with the diagnosis of bearing faults. Accurate fault diagnosis is essential for the safe and stable operation of rotating machinery. Most former research depends too much on the fault signal specificity and learning model’s choices. To overcome the disadvantages of lacking intrinsic mode function (IMF) modal aliasing, low degree of discrimination between data of different fault types, high computational complexity. This paper proposes a method that combines multi-scale weighted entropy morphological filtering (MWEMF) signal processing and bidirectional long-short term memory neural networks (Bi-LSTM). The developed rolling bearing fault diagnosis strategy is then implemented to different databases and potential models to demonstrate the greatly improved system’s ability to reconstruct the time-to-frequency domain characteristics of fault signature signals and reduce learning cost. After verification, the classification accuracy of the proposed model reaches 99%.},
  archive      = {J_APIN},
  author       = {Zou, Fengqian and Zhang, Haifeng and Sang, Shengtian and Li, Xiaoming and He, Wanying and Liu, Xiaowei},
  doi          = {10.1007/s10489-021-02229-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6647-6664},
  shortjournal = {Appl. Intell.},
  title        = {Bearing fault diagnosis based on combined multi-scale weighted entropy morphological filtering and bi-LSTM},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-attention fusion based deep neural network for chinese
medical answer selection. <em>APIN</em>, <em>51</em>(10), 6633–6646. (<a
href="https://doi.org/10.1007/s10489-021-02212-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese selection is one of the most important subtasks in Chinese medical question-answer system. To obtain the representations of question and answer, an attractive method is to use the attentive pooling based deep neural network. However, this method suffers from the over-pooling problem. It generates attentive information by only using the related medical keywords, and neglects the local semantic information of sentences. In this paper, a novel co-attention fusion based deep neural network method is proposed. Our method solves the over-pooling problem by fusing local semantic information with attentive information. Because of the usage of the fusion mechanism, the proposed method tends to obtain more useful information for pooling and produce better representations for question and answer. For comparison, we create a new Chinese medical answer selection dataset in the epilepsy theme (i.e., cEpilepsyQA), and our method performs much better than the state-of-the-art methods. Also, the proposed method gets competitive results on the public Chinese medical answer selection datasets: cMedQA v1.0 and v2.0.},
  archive      = {J_APIN},
  author       = {Chen, Xichen and Yang, Zuyuan and Liang, Naiyao and Li, Zhenni and Sun, Weijun},
  doi          = {10.1007/s10489-021-02212-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6633-6646},
  shortjournal = {Appl. Intell.},
  title        = {Co-attention fusion based deep neural network for chinese medical answer selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn class hierarchy using convolutional neural networks.
<em>APIN</em>, <em>51</em>(10), 6622–6632. (<a
href="https://doi.org/10.1007/s10489-020-02103-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large amount of research on Convolutional Neural Networks (CNN) has focused on flat Classification in the multi-class domain. In the real world, many problems are naturally expressed as hierarchical classification problems, in which the classes to be predicted are organized in a hierarchy of classes. In this paper, we propose a new architecture for hierarchical classification, introducing a stack of deep linear layers using cross-entropy loss functions combined to a center loss function. The proposed architecture can extend any neural network model and simultaneously optimizes loss functions to discover local hierarchical class relationships and a loss function to discover global information from the whole class hierarchy while penalizing class hierarchy violations. We experimentally show that our hierarchical classifier presents advantages to the traditional classification approaches finding application in computer vision tasks. The same approach can also be applied to some CNN for text classification.},
  archive      = {J_APIN},
  author       = {La Grassa, Riccardo and Gallo, Ignazio and Landro, Nicola},
  doi          = {10.1007/s10489-020-02103-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6622-6632},
  shortjournal = {Appl. Intell.},
  title        = {Learn class hierarchy using convolutional neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep facial spatiotemporal network for engagement prediction
in online learning. <em>APIN</em>, <em>51</em>(10), 6609–6621. (<a
href="https://doi.org/10.1007/s10489-020-02139-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, online learning has been gradually accepted and approbated by the public. In this context, an effective prediction of students’ engagement can help teachers obtain timely feedback and make adaptive adjustments to meet learners’ needs. In this paper, we present a novel model called the Deep Facial Spatiotemporal Network (DFSTN) for engagement prediction. The model contains two modules: the pretrained SE-ResNet-50 (SENet), which is used for extracting facial spatial features, and the Long Short Term Memory (LSTM) Network with Global Attention (GALN), which is employed to generate an attentional hidden state. The training strategy of the model is different with changes of the performance metric. The DFSTN can capture facial spatial and temporal information, which is helpful for sensing the fine-grained engaged state and improving the engagement prediction performance. We evaluate the methods on the Dataset for Affective States in E-Environments (DAiSEE) and obtain an accuracy of 58.84% in four-class classification and a Mean Square Error (MSE) of 0.0422. The results show that our method outperforms many existing works in engagement prediction on DAiSEE. Additionally, the robustness of our method is also exhibited by experiments on the EmotiW-EP dataset.},
  archive      = {J_APIN},
  author       = {Liao, Jiacheng and Liang, Yan and Pan, Jiahui},
  doi          = {10.1007/s10489-020-02139-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {6609-6621},
  shortjournal = {Appl. Intell.},
  title        = {Deep facial spatiotemporal network for engagement prediction in online learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A comprehensive model and computational methods to improve
situation awareness in intelligence scenarios. <em>APIN</em>,
<em>51</em>(9), 6585–6608. (<a
href="https://doi.org/10.1007/s10489-021-02673-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive model for representing and reasoning on situations to support decision makers in Intelligence analysis activities. The main result presented in the paper stems from a work of refinement and abstraction of previous results of the authors related to the use of Situation Awareness and Granular Computing for the development of analysis methods and techniques to support Intelligence. This work made it possible to derive the characteristics of the model from previous case studies and applications with real data, and to link the reasoning techniques to concrete approaches used by intelligence analysts such as, for example, the Structured Analytic Techniques. The model allows to represent an operational situation according to three complementary perspectives: descriptive, relational and behavioral. These three perspectives are instantiated on the basis of the principles and methods of Granular Computing, mainly based on the theories of fuzzy and rough sets, and with the help of further structures such as graphs. As regards the reasoning on the situations thus represented, the paper presents four methods with related case studies and applications validated on real data.},
  archive      = {J_APIN},
  author       = {Gaeta, Angelo and Loia, Vincenzo and Orciuoli, Francesco},
  doi          = {10.1007/s10489-021-02673-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6585-6608},
  shortjournal = {Appl. Intell.},
  title        = {A comprehensive model and computational methods to improve situation awareness in intelligence scenarios},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rational verification: Game-theoretic verification of
multi-agent systems. <em>APIN</em>, <em>51</em>(9), 6569–6584. (<a
href="https://doi.org/10.1007/s10489-021-02658-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a survey of the state of the art of rational verification: the problem of checking whether a given temporal logic formula ϕ is satisfied in some or all game-theoretic equilibria of a multi-agent system – that is, whether the system will exhibit the behavior ϕ represents under the assumption that agents within the system act rationally in pursuit of their preferences. After motivating and introducing the overall framework of rational verification, we discuss key results obtained in the past few years as well as relevant related work in logic, AI, and computer science.},
  archive      = {J_APIN},
  author       = {Abate, Alessandro and Gutierrez, Julian and Hammond, Lewis and Harrenstein, Paul and Kwiatkowska, Marta and Najib, Muhammad and Perelli, Giuseppe and Steeples, Thomas and Wooldridge, Michael},
  doi          = {10.1007/s10489-021-02658-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6569-6584},
  shortjournal = {Appl. Intell.},
  title        = {Rational verification: Game-theoretic verification of multi-agent systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 30th anniversary of applied intelligence: A combination of
bibliometrics and thematic analysis using SciMAT. <em>APIN</em>,
<em>51</em>(9), 6547–6568. (<a
href="https://doi.org/10.1007/s10489-021-02584-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applied Intelligence is one of the most important international scientific journals in the field of artificial intelligence. From 1991, Applied Intelligence has been oriented to support research advances in new and innovative intelligent systems, methodologies, and their applications in solving real-life complex problems. In this way, Applied Intelligence hosts more than 2,400 publications and achieves around 31,800 citations. Moreover, Applied Intelligence is recognized by the industrial, academic, and scientific communities as a source of the latest innovative and advanced solutions in intelligent manufacturing, privacy-preserving systems, risk analysis, knowledge-based management, modern techniques to improve healthcare systems, methods to assist government, and solving industrial problems that are too complex to be solved through conventional approaches. Bearing in mind that Applied Intelligence celebrates its 30th anniversary in 2021, it is appropriate to analyze its bibliometric performance, conceptual structure, and thematic evolution. To do that, this paper conducts a bibliometric performance and conceptual structure analysis of Applied Intelligence from 1991 to 2020 using SciMAT. Firstly, the performance of the journal is analyzed according to the data retrieved from Scopus, putting the focus on the productivity of the authors, citations, countries, organizations, funding agencies, and most relevant publications. Finally, the conceptual structure of the journal is analyzed with the bibliometric software tool SciMAT, identifying the main thematic areas that have been the object of research and their composition, relationship, and evolution during the period analyzed.},
  archive      = {J_APIN},
  author       = {López-Robles, J. R. and Cobo, M. J. and Gutiérrez-Salcedo, M. and Martínez-Sánchez, M. A. and Gamboa-Rosales, N. K. and Herrera-Viedma, E.},
  doi          = {10.1007/s10489-021-02584-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6547-6568},
  shortjournal = {Appl. Intell.},
  title        = {30th anniversary of applied intelligence: A combination of bibliometrics and thematic analysis using SciMAT},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modular design patterns for hybrid learning and reasoning
systems. <em>APIN</em>, <em>51</em>(9), 6528–6546. (<a
href="https://doi.org/10.1007/s10489-021-02394-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unification of statistical (data-driven) and symbolic (knowledge-driven) methods is widely recognized as one of the key challenges of modern AI. Recent years have seen a large number of publications on such hybrid neuro-symbolic AI systems. That rapidly growing literature is highly diverse, mostly empirical, and is lacking a unifying view of the large variety of these hybrid systems. In this paper, we analyze a large body of recent literature and we propose a set of modular design patterns for such hybrid, neuro-symbolic systems. We are able to describe the architecture of a very large number of hybrid systems by composing only a small set of elementary patterns as building blocks. The main contributions of this paper are: 1) a taxonomically organised vocabulary to describe both processes and data structures used in hybrid systems; 2) a set of 15+ design patterns for hybrid AI systems organized in a set of elementary patterns and a set of compositional patterns; 3) an application of these design patterns in two realistic use-cases for hybrid AI systems. Our patterns reveal similarities between systems that were not recognized until now. Finally, our design patterns extend and refine Kautz’s earlier attempt at categorizing neuro-symbolic architectures.},
  archive      = {J_APIN},
  author       = {van Bekkum, Michael and de Boer, Maaike and van Harmelen, Frank and Meyer-Vitali, André and Teije, Annette ten},
  doi          = {10.1007/s10489-021-02394-3},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6528-6546},
  shortjournal = {Appl. Intell.},
  title        = {Modular design patterns for hybrid learning and reasoning systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A panoramic view and swot analysis of artificial
intelligence for achieving the sustainable development goals by 2030:
Progress and prospects. <em>APIN</em>, <em>51</em>(9), 6497–6527. (<a
href="https://doi.org/10.1007/s10489-021-02264-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The17 Sustainable Development Goals (SDGs) established by the United Nations Agenda 2030 constitute a global blueprint agenda and instrument for peace and prosperity worldwide. Artificial intelligence and other digital technologies that have emerged in the last years, are being currently applied in virtually every area of society, economy and the environment. Hence, it is unsurprising that their current role in the pursuance or hampering of the SDGs has become critical. This study aims at providing a snapshot and comprehensive view of the progress made and prospects in the relationship between artificial intelligence technologies and the SDGs. A comprehensive review of existing literature has been firstly conducted, after which a series SWOT (Strengths, Weaknesses, Opportunities and Threats) analyses have been undertaken to identify the strengths, weaknesses, opportunities and threats inherent to artificial intelligence-driven technologies as facilitators or barriers to each of the SDGs. Based on the results of these analyses, a subsequent broader analysis is provided, from a position vantage, to (i) identify the efforts made in applying AI technologies in SDGs, (ii) pinpoint opportunities for further progress along the current decade, and (iii) distill ongoing challenges and target areas for important advances. The analysis is organized into six categories or perspectives of human needs: life, economic and technological development, social development, equality, resources and natural environment. Finally, a closing discussion is provided about the prospects, key guidelines and lessons learnt that should be adopted for guaranteeing a positive shift of artificial intelligence developments and applications towards fully supporting the SDGs attainment by 2030.},
  archive      = {J_APIN},
  author       = {Palomares, Iván and Martínez-Cámara, Eugenio and Montes, Rosana and García-Moral, Pablo and Chiachio, Manuel and Chiachio, Juan and Alonso, Sergio and Melero, Francisco J. and Molina, Daniel and Fernández, Bárbara and Moral, Cristina and Marchena, Rosario and de Vargas, Javier Pérez and Herrera, Francisco},
  doi          = {10.1007/s10489-021-02264-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6497-6527},
  shortjournal = {Appl. Intell.},
  title        = {A panoramic view and swot analysis of artificial intelligence for achieving the sustainable development goals by 2030: Progress and prospects},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recent advances in evolutionary and bio-inspired adaptive
robotics: Exploiting embodied dynamics. <em>APIN</em>, <em>51</em>(9),
6467–6496. (<a
href="https://doi.org/10.1007/s10489-021-02275-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores current developments in evolutionary and bio-inspired approaches to autonomous robotics, concentrating on research from our group at the University of Sussex. These developments are discussed in the context of advances in the wider fields of adaptive and evolutionary approaches to AI and robotics, focusing on the exploitation of embodied dynamics to create behaviour. Four case studies highlight various aspects of such exploitation. The first exploits the dynamical properties of a physical electronic substrate, demonstrating for the first time how component-level analog electronic circuits can be evolved directly in hardware to act as robot controllers. The second develops novel, effective and highly parsimonious navigation methods inspired by the way insects exploit the embodied dynamics of innate behaviours. Combining biological experiments with robotic modeling, it is shown how rapid route learning can be achieved with the aid of navigation-specific visual information that is provided and exploited by the innate behaviours. The third study focuses on the exploitation of neuromechanical chaos in the generation of robust motor behaviours. It is demonstrated how chaotic dynamics can be exploited to power a goal-driven search for desired motor behaviours in embodied systems using a particular control architecture based around neural oscillators. The dynamics are shown to be chaotic at all levels in the system, from the neural to the embodied mechanical. The final study explores the exploitation of the dynamics of brain-body-environment interactions for efficient, agile flapping winged flight. It is shown how a multi-objective evolutionary algorithm can be used to evolved dynamical neural controllers for a simulated flapping wing robot with feathered wings. Results demonstrate robust, stable, agile flight is achieved in the face of random wind gusts by exploiting complex asymmetric dynamics partly enabled by continually changing wing and tail morphologies.},
  archive      = {J_APIN},
  author       = {Husbands, Phil and Shim, Yoonsik and Garvie, Michael and Dewar, Alex and Domcsek, Norbert and Graham, Paul and Knight, James and Nowotny, Thomas and Philippides, Andrew},
  doi          = {10.1007/s10489-021-02275-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6467-6496},
  shortjournal = {Appl. Intell.},
  title        = {Recent advances in evolutionary and bio-inspired adaptive robotics: Exploiting embodied dynamics},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated major depressive disorder detection using melamine
pattern with EEG signals. <em>APIN</em>, <em>51</em>(9), 6449–6466. (<a
href="https://doi.org/10.1007/s10489-021-02426-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD) is one of the most common modern ailments affected huge population throughout the world. The electroencephalogram (EEG) signal is widely used to screen the MDD. The manual diagnosis of MDD using EEG is time consuming, subjective and may cause human errors. Therefore, nowadays various automated systems have been developed to diagnose MDD accurately and rapidly. In this work, we have proposed a novel automated MDD detection system using EEG signals. Our proposed model has three steps: (i) Melamine pattern and discrete wavelet transform (DWT)- based multileveled feature generation, (ii) selection of most relevant features using neighborhood component analysis (NCA) and (iii) classification using support vector machine (SVM) and k nearest neighbor (kNN) classifiers. The novelty of this work is the application of melamine pattern. The molecular structure of melamine (also named chemistry spider- ChemSpider) is used to generate 1536 features. Also, various statistical features are extracted from DWT coefficients. The NCA is used to select the most relevant features and these selected features are classified using SVM and kNN classifiers. The presented model attained greater than 95% accuracies using all channels with quadratic SVM classifier. Our results obtained highest classification accuracy of 99.11% and 99.05% using Weighted kNN and Quadratic SVM respectively using A2A1 EEG channel. We have developed the automated depression model using a big dataset and yielded high classification accuracies. These results indicate that our presented model can be used in mental health clinics to confirm the manual diagnosis of psychiatrists.},
  archive      = {J_APIN},
  author       = {Aydemir, Emrah and Tuncer, Turker and Dogan, Sengul and Gururajan, Raj and Acharya, U. Rajendra},
  doi          = {10.1007/s10489-021-02426-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6449-6466},
  shortjournal = {Appl. Intell.},
  title        = {Automated major depressive disorder detection using melamine pattern with EEG signals},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The hierarchical SMAA-PROMETHEE method applied to assess the
sustainability of european cities. <em>APIN</em>, <em>51</em>(9),
6430–6448. (<a
href="https://doi.org/10.1007/s10489-021-02384-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the level of sustainability taking into account many contributing aspects is a challenge. In this paper, we apply a multiple criteria decision aiding framework, namely, the hierarchical-SMAA-PROMETHEE method, to assess the environmental, social, and economic sustainability of 20 European cities in the period going from 2012 to 2015. The application of the method is innovative for the following reasons: (i) it permits to study the sustainability of the mentioned cities not only comprehensively but also considering separately particular macro-criteria, providing in this way more specific information on their weak and strong points; (ii) the use of PROMETHEE and, in particular, of PROMETHEE II, avoids the compensation between different and heterogeneous criteria, that is arbitrarily assumed in value function aggregation models; finally, (iii) thanks to the application of the Stochastic Multicriteria Acceptability Analysis, the method provides more robust recommendations than a method based on a single instance of the considered preference model compatible with few preference information items provided by the Decision Maker.},
  archive      = {J_APIN},
  author       = {Corrente, Salvatore and Greco, Salvatore and Leonardi, Floriana and Słowiński, Roman},
  doi          = {10.1007/s10489-021-02384-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6430-6448},
  shortjournal = {Appl. Intell.},
  title        = {The hierarchical SMAA-PROMETHEE method applied to assess the sustainability of european cities},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning in multi-object detection and tracking: State
of the art. <em>APIN</em>, <em>51</em>(9), 6400–6429. (<a
href="https://doi.org/10.1007/s10489-021-02293-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection and tracking is one of the most important and challenging branches in computer vision, and have been widely applied in various fields, such as health-care monitoring, autonomous driving, anomaly detection, and so on. With the rapid development of deep learning (DL) networks and GPU’s computing power, the performance of object detectors and trackers has been greatly improved. To understand the main development status of object detection and tracking pipeline thoroughly, in this survey, we have critically analyzed the existing DL network-based methods of object detection and tracking and described various benchmark datasets. This includes the recent development in granulated DL models. Primarily, we have provided a comprehensive overview of a variety of both generic object detection and specific object detection models. We have enlisted various comparative results for obtaining the best detector, tracker, and their combination. Moreover, we have listed the traditional and new applications of object detection and tracking showing its developmental trends. Finally, challenging issues, including the relevance of granular computing, in the said domain are elaborated as a future scope of research, together with some concerns. An extensive bibliography is also provided.},
  archive      = {J_APIN},
  author       = {Pal, Sankar K. and Pramanik, Anima and Maiti, J. and Mitra, Pabitra},
  doi          = {10.1007/s10489-021-02293-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6400-6429},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning in multi-object detection and tracking: State of the art},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evidential fully convolutional network for semantic
segmentation. <em>APIN</em>, <em>51</em>(9), 6376–6399. (<a
href="https://doi.org/10.1007/s10489-021-02327-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a hybrid architecture composed of a fully convolutional network (FCN) and a Dempster-Shafer layer for image semantic segmentation. In the so-called evidential FCN (E-FCN), an encoder-decoder architecture first extracts pixel-wise feature maps from an input image. A Dempster-Shafer layer then computes mass functions at each pixel location based on distances to prototypes. Finally, a utility layer performs semantic segmentation from mass functions and allows for imprecise classification of ambiguous pixels and outliers. We propose an end-to-end learning strategy for jointly updating the network parameters, which can make use of soft (imprecise) labels. Experiments using three databases (Pascal VOC 2011, MIT-scene Parsing and SIFT Flow) show that the proposed combination improves the accuracy and calibration of semantic segmentation by assigning confusing pixels to multi-class sets.},
  archive      = {J_APIN},
  author       = {Tong, Zheng and Xu, Philippe and Denœux, Thierry},
  doi          = {10.1007/s10489-021-02327-0},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6376-6399},
  shortjournal = {Appl. Intell.},
  title        = {Evidential fully convolutional network for semantic segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous flight cycles and extreme landings of airliners
beyond the current limits and capabilities using artificial neural
networks. <em>APIN</em>, <em>51</em>(9), 6349–6375. (<a
href="https://doi.org/10.1007/s10489-021-02202-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe the Intelligent Autopilot System (IAS), a fully autonomous autopilot capable of piloting large jets such as airliners by learning from experienced human pilots using Artificial Neural Networks. The IAS is capable of autonomously executing the required piloting tasks and handling the different flight phases to fly an aircraft from one airport to another including takeoff, climb, cruise, navigate, descent, approach, and land in simulation. In addition, the IAS is capable of autonomously landing large jets in the presence of extreme weather conditions including severe crosswind, gust, wind shear, and turbulence. The IAS is a potential solution to the limitations and robustness problems of modern autopilots such as the inability to execute complete flights, the inability to handle extreme weather conditions especially during approach and landing where the aircraft’s speed is relatively low, and the uncertainty factor is high, and the pilots shortage problem compared to the increasing aircraft demand. In this paper, we present the work done by collaborating with the aviation industry to provide training data for the IAS to learn from. The training data is used by Artificial Neural Networks to generate control models automatically. The control models imitate the skills of the human pilot when executing all the piloting tasks required to pilot an aircraft between two airports. In addition, we introduce new ANNs trained to control the aircraft’s elevators, elevators’ trim, throttle, flaps, and new ailerons and rudder ANNs to counter the effects of extreme weather conditions and land safely. Experiments show that small datasets containing single demonstrations are sufficient to train the IAS and achieve excellent performance by using clearly separable and traceable neural network modules which eliminate the black-box problem of large Artificial Intelligence methods such as Deep Learning. In addition, experiments show that the IAS can handle landing in extreme weather conditions beyond the capabilities of modern autopilots and even experienced human pilots. The proposed IAS is a novel approach towards achieving full control autonomy of large jets using ANN models that match the skills and abilities of experienced human pilots and beyond.},
  archive      = {J_APIN},
  author       = {Baomar, Haitham and Bentley, Peter J.},
  doi          = {10.1007/s10489-021-02202-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6349-6375},
  shortjournal = {Appl. Intell.},
  title        = {Autonomous flight cycles and extreme landings of airliners beyond the current limits and capabilities using artificial neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards bridging the neuro-symbolic gap: Deep deductive
reasoners. <em>APIN</em>, <em>51</em>(9), 6326–6348. (<a
href="https://doi.org/10.1007/s10489-020-02165-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic knowledge representation and reasoning and deep learning are fundamentally different approaches to artificial intelligence with complementary capabilities. The former are transparent and data-efficient, but they are sensitive to noise and cannot be applied to non-symbolic domains where the data is ambiguous. The latter can learn complex tasks from examples, are robust to noise, but are black boxes; require large amounts of –not necessarily easily obtained– data, and are slow to learn and prone to adversarial examples. Either paradigm excels at certain types of problems where the other paradigm performs poorly. In order to develop stronger AI systems, integrated neuro-symbolic systems that combine artificial neural networks and symbolic reasoning are being sought. In this context, one of the fundamental open problems is how to perform logic-based deductive reasoning over knowledge bases by means of trainable artificial neural networks. This paper provides a brief summary of the authors’ recent efforts to bridge the neural and symbolic divide in the context of deep deductive reasoners. Throughout the paper we will discuss strengths and limitations of models in term of accuracy, scalability, transferability, generalizabiliy, speed, and interpretability, and finally, will talk about possible modifications to enhance desirable capabilities. More specifically, in terms of architectures, we are looking at Memory-augmented networks, Logic Tensor Networks, and compositions of LSTM models to explore their capabilities and limitations in conducting deductive reasoning. We are applying these models on Resource Description Framework (RDF), first-order logic, and the description logic $\mathcal {E}{\mathscr{L}}^{+}$ respectively.},
  archive      = {J_APIN},
  author       = {Ebrahimi, Monireh and Eberhart, Aaron and Bianchi, Federico and Hitzler, Pascal},
  doi          = {10.1007/s10489-020-02165-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6326-6348},
  shortjournal = {Appl. Intell.},
  title        = {Towards bridging the neuro-symbolic gap: Deep deductive reasoners},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The geometry of three-way decision. <em>APIN</em>,
<em>51</em>(9), 6298–6325. (<a
href="https://doi.org/10.1007/s10489-020-02142-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A theory of three-way decision concerns the art, science, and practice of thinking, problem solving, and information processing in threes. It explores the effective uses of triads of three things, for example, three elements, three parts, three perspectives, and so on. In this paper, I examine geometric structures, graphical representations, and semantical interpretations of triads in terms of basic geometric notions of dots, lines, triangles, circles, as well as more complex structures derived from these basic notions. I use examples from different disciplines and fields to illustrate the uses of these structures and their physical interpretations for triadic thinking, triadic computing, and triadic processing. Following the principles of triadic thinking, this paper blends together three common ways to think, namely, numerical thinking, textual thinking, and visual thinking.},
  archive      = {J_APIN},
  author       = {Yao, Yiyu},
  doi          = {10.1007/s10489-020-02142-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6298-6325},
  shortjournal = {Appl. Intell.},
  title        = {The geometry of three-way decision},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Editorial for the 30th anniversary special issue.
<em>APIN</em>, <em>51</em>(9), 6295–6297. (<a
href="https://doi.org/10.1007/s10489-021-02698-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Ali, Moonis and Fujita, Hamido},
  doi          = {10.1007/s10489-021-02698-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {6295-6297},
  shortjournal = {Appl. Intell.},
  title        = {Editorial for the 30th anniversary special issue},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel similarity measure for spatial entity resolution
based on data granularity model: Managing inconsistencies in place
descriptions. <em>APIN</em>, <em>51</em>(8), 6104–6123. (<a
href="https://doi.org/10.1007/s10489-020-01959-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremendous amounts of data are generated every day by different sources and stored in heterogeneous databases. Providing an integrated view by fusion of data is essential to enhance data utilization. An indispensable type of data is spatial data, with diverse application domains, including GIS, e-commerce, military, and tourism. The concept of location forms a key part of user-generated data with serious challenges, including uncertainty. A particular location may have different names, and conversely, various locations may have the same name. Furthermore, geographical coordinates of locations may not be expressed accurately in datasets. More challenges also exist that have received less attention. Various data sources might describe locations in different levels of detail. This increases data inconsistency and decreases the quality of data fusion. This paper focuses on spatial data granulation to deal with this variety. If these diversities are not taken into consideration, the different descriptions of a location may be interpreted differently and, in turn, not be fused. The contribution of this paper are: (a) Introducing a granular approach to measure the similarity between two place description for managing apparent differences. The proposed method improves the quality of the geocoding and data fusion phases, (b) Introducing a novel data blocking method to decrease pairwise comparisons based on geographical features. For result evaluation, we developed a dataset from two real aviation accident datasets. The evaluation shows that the quality of entity recognition and data fusion improved by using our proposed data granulation technique.},
  archive      = {J_APIN},
  author       = {Khodizadeh-Nahari, Mohammad and Ghadiri, Nasser and Baraani-Dastjerdi, Ahmad and Sack, Jörg-Rüdiger},
  doi          = {10.1007/s10489-020-01959-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6104-6123},
  shortjournal = {Appl. Intell.},
  title        = {A novel similarity measure for spatial entity resolution based on data granularity model: Managing inconsistencies in place descriptions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depression and anorexia detection in social media as a
one-class classification problem. <em>APIN</em>, <em>51</em>(8),
6088–6103. (<a
href="https://doi.org/10.1007/s10489-020-02131-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking advantage of the increasing amount of user-generated content in social media, some computational methods have already been proposed for detecting people suffering from depression and anorexia. Such complex tasks have been tackled as a binary classification problem using, in most cases, automatically generated training data. Despite its promising results, this approach has some important drawbacks, namely: it suffers from a severely skewed class distribution, the negative class is very diverse since it attempts to model all kinds of healthy users, and, above all, there is not a complete certainty about annotations, especially for the negative cases (i.e., healthy users). Motivated by these issues, in this paper, we propose to face the detection of these disorders following a one-class classification (OCC) approach. Particularly, we introduce two new instance-based OCC methods especially suited to manage the high diversity of content from social media documents. Taking up ideas from the gravitational attraction force, these methods evaluate the relation of documents by their strengths, considering their distances as well as their masses (relevance) with respect to the target task. Experiments were conducted on depression and anorexia benchmark datasets. The obtained results are encouraging; the overall performance was better than the results from other standard OCC methods, and competitive with regard to state-of-the-art results from binary classification approaches.},
  archive      = {J_APIN},
  author       = {Aguilera, Juan and Farías, Delia Irazú Hernández and Ortega-Mendoza, Rosa María and Montes-y-Gómez, Manuel},
  doi          = {10.1007/s10489-020-02131-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6088-6103},
  shortjournal = {Appl. Intell.},
  title        = {Depression and anorexia detection in social media as a one-class classification problem},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A probabilistic linguistic TODIM method considering
cumulative probability-based hellinger distance and its application in
waste mobile phone recycling. <em>APIN</em>, <em>51</em>(8), 6072–6087.
(<a href="https://doi.org/10.1007/s10489-021-02185-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prosperity of online mobile phone recycling platforms has brought convenience to waste mobile phone recycling. This study aims to propose a novel decision-making method to evaluate the recycling channels of mobile phones in an uncertain environment using the probabilistic linguistic term set (PLTS) to represent uncertain information. Given that each PLTS is associated with a probability distribution, the differences of different PLTSs can be represented by comparing different probability distributions. The Hellinger distance is a widely-used index to measure the similarity of two probability distributions, but it cannot be directly used to express the semantic differences of PLTSs. In this regard, we introduce a cumulative probability-based Hellinger distance measure of PLTSs. Then, we propose an extension of TODIM (an acronym in Portuguese of Interactive and Multicriteria Decision Making) method considering the bounded rationality and psychology of decision-makers based on the proposed distance measure of PLTSs and a criterion weight determination method, the Simos-Roy-Figueira method. Finally, an illustrative example of evaluating the recycling channels of mobile phones is given with sensitive and comparative analyses, showing the efficiency of the proposed method. This method can also be applied to other various fields.},
  archive      = {J_APIN},
  author       = {Chang, Jiaying and Liao, Huchang and Mi, Xiaomei and Al-Barakati, Abdullah},
  doi          = {10.1007/s10489-021-02185-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6072-6087},
  shortjournal = {Appl. Intell.},
  title        = {A probabilistic linguistic TODIM method considering cumulative probability-based hellinger distance and its application in waste mobile phone recycling},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TensSent: A tensor based sentimental word embedding method.
<em>APIN</em>, <em>51</em>(8), 6056–6071. (<a
href="https://doi.org/10.1007/s10489-020-02163-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The representation of words as vectors, conventionally known as word embeddings, has drawn considerable attention in recent years as feature learning techniques for natural language processing. The majority of these methods operate solely on the semantic and the syntactic aspects of a text, remaining oblivious to sentimental information. However, as numerous words with opposite polarities may appear in similar contexts, such as “interesting” and “boring”, the exclusive use of context-oriented information lacks the required particulars for generating word embeddings as features in sentiment analysis. Along this thread, the present study proposes two novel unsupervised models to integrating word polarity information and word co-occurrences as more tailored features for sentiment analysis. Word polarity and co-occurrence come together in the form of a tensor and tensor factorization is employed for generating the word embeddings. The experimental results on IMDB and SemEval-task2 datasets demonstrate the relatively higher performance of the proposed method compared to baseline approaches on the tasks of document-level sentiment analysis by 4.},
  archive      = {J_APIN},
  author       = {Rahimi, Zahra and Homayounpour, Mohammad Mehdi},
  doi          = {10.1007/s10489-020-02163-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6056-6071},
  shortjournal = {Appl. Intell.},
  title        = {TensSent: A tensor based sentimental word embedding method},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey for user behavior analysis based on machine
learning techniques: Current models and applications. <em>APIN</em>,
<em>51</em>(8), 6029–6055. (<a
href="https://doi.org/10.1007/s10489-020-02160-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant research has been carried out in the field of User Behavior Analysis, focused on understanding, modeling and predicting past, present and future behaviors of users. However, the heterogeneity of the approaches makes their comprehension very complicated. Thus, domain and Machine Learning experts have to work together to achieve their objectives. The main motivation for this work is to obtain an understanding of this field by providing a categorization of state-of-the-art works grouping them based on specific features. This paper presents a comprehensive survey of the existing literature in the areas of Cybersecurity, Networks, Safety and Health, and Service Delivery Improvement. The survey is organized based on four different topic-based features which categorize existing works: keywords, application domain, Machine Learning algorithm, and data type. This paper aims to thoroughly analyze the existing references, to promote the dissemination of state-of-the-art approaches discussing their strong and weak points, and to identify open challenges and prospective future research directions. In addition, 127 discussed papers have been scored and ranked according to relevance-based features: paper reputation, maximum author reputation, novelty, innovation and data quality. Both types of features, topic-based and relevance-based have been combined to build a similarity metric enabling a rich visualization of all considered publications. The obtained graphic representation provides a guide of recent advancements in User Behavior Analysis by topic, highlighting the most relevant ones.},
  archive      = {J_APIN},
  author       = {G. Martín, Alejandro and Fernández-Isabel, Alberto and Martín de Diego, Isaac and Beltrán, Marta},
  doi          = {10.1007/s10489-020-02160-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6029-6055},
  shortjournal = {Appl. Intell.},
  title        = {A survey for user behavior analysis based on machine learning techniques: Current models and applications},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential neural networks for multi-resident activity
recognition in ambient sensing smart homes. <em>APIN</em>,
<em>51</em>(8), 6014–6028. (<a
href="https://doi.org/10.1007/s10489-020-02134-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in smart home technology and IoT devices had made us capable of monitoring human activities in a non-intrusive way. This data, in turn, enables us to predict the health status and energy consumption patterns of residents of these smart homes. Machine learning has been an excellent tool for the prediction of human activities from raw sensor data of a single resident. However, Multi Resident activity recognition is still a challenging task, as there is no correlation between sensor values and resident activities. In this paper, we have applied deep learning algorithms on the real world ARAS Multi Resident data set, which consists of data from two houses, each with two residents. We have used different variations of Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), and their combination on the data set and kept the labels separate for both residents. We have evaluated the performance of models based on several metrics.},
  archive      = {J_APIN},
  author       = {Natani, Anubhav and Sharma, Abhishek and Perumal, Thinagaran},
  doi          = {10.1007/s10489-020-02134-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6014-6028},
  shortjournal = {Appl. Intell.},
  title        = {Sequential neural networks for multi-resident activity recognition in ambient sensing smart homes},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Action recognition using interrelationships of 3D joints and
frames based on angle sine relation and distance features using
interrelationships. <em>APIN</em>, <em>51</em>(8), 6001–6013. (<a
href="https://doi.org/10.1007/s10489-020-02176-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is still an uncertain computer vision problem, which could be solved by a robust action descriptor. As a solution, we proposed an action recognition descriptor using only the 3D skeleton joint’s points to perform this unsettle task. Joint’s point interrelationships and frame-frame interrelationships are presented, which is a solution backbone to achieve human action recognition. Here, many joints are related to each other, and frames depend on different frames while performing any action sequence. Joints point spatial information calculates using angle, joint’s sine relation, and distance features, whereas joints point temporal information estimates from frame-frame relations. Experiments are performed over four publicly available databases, i.e., MSR Daily Activity 3D Dataset, UTD Multimodal Human Action Dataset, KARD- Kinect Activity Recognition Dataset, and SBU Kinect Interaction Dataset, and proved that proposed descriptor outperforms as a comparison to state-of-the-art approaches on entire four datasets. Angle, Sine relation, and Distance features are extracted using interrelationships of joints and frames (ASD-R). It is all achieved due to accurately detecting spatial and temporal information of the Joint’s points. Moreover, the Support Vector Machine classifier supports the proposed descriptor to identify the right classification precisely.},
  archive      = {J_APIN},
  author       = {Islam, M. Shujah and Bakhat, Khush and Khan, Rashid and Iqbal, Mansoor and Islam, M. Mattah and Ye, Zhongfu},
  doi          = {10.1007/s10489-020-02176-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {6001-6013},
  shortjournal = {Appl. Intell.},
  title        = {Action recognition using interrelationships of 3D joints and frames based on angle sine relation and distance features using interrelationships},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated parameter tuning as a bilevel optimization problem
solved by a surrogate-assisted population-based approach. <em>APIN</em>,
<em>51</em>(8), 5978–6000. (<a
href="https://doi.org/10.1007/s10489-020-02151-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a proposal for the automated parameter tuning problem (APTP) modeled as a bilevel optimization problem. Different definitions and theoretical results are given in order to formalize the APTP in the context of this hierarchical optimization problem. The obtained bilevel optimization problem is solved via a population-based algorithm added with surrogate models to identify promising regions in the parameter search space. The approach is tested by configuring four representative metaheuristics for numerical optimization on a set of well-known and recent test problems; also a competitive algorithm for a popular combinatorial optimization problem was configured (considering a large benchmark suite). The experimental results are compared against those of a state-of-the-art parameter tuning method called IRACE. The results, validated by the Bayesian signed-rank statistical test, indicate that BCAP, even it is based on an usually costly model (i.e. a bilevel optimization problem), with only half of the calls to the target algorithm, is able to find better configurations than those obtained by the compared approach.},
  archive      = {J_APIN},
  author       = {Mejía-de-Dios, Jesús-Adolfo and Mezura-Montes, Efrén and Quiroz-Castellanos, Marcela},
  doi          = {10.1007/s10489-020-02151-y},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5978-6000},
  shortjournal = {Appl. Intell.},
  title        = {Automated parameter tuning as a bilevel optimization problem solved by a surrogate-assisted population-based approach},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phototropic algorithm for global optimisation problems.
<em>APIN</em>, <em>51</em>(8), 5965–5977. (<a
href="https://doi.org/10.1007/s10489-020-02105-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem solving and decision-making have a vital role to play in both technical and non-technical fields. Some decisions are simple while others require more effort and time to solve. This article introduces a new problem solving technique called Phototropic optimization algorithm, inspired from the optimised growth pattern in plants. It has been observed that the stem tips of a plant always grow towards sunlight. In this algorithm, the underlying hormonal mechanism of phototropism is emulated to solve computational problems. This phenomenon has indicated strong prospects of algorithmic efficiency and invites further research into prospective computational applications. Phototropic algorithm is developed as an optimization technique to solve real time application such as shortest path finding problems, travelling salesman problem, finding congestion in a network or any similar problem seen around. A prototype on finding the minimal distance between any two nodes in the physical network is modelled here. The asymptotic time complexity analysis shows the algorithm routes packages in O (n log n). Comparison with the traditional algorithms gives sufficient evidence for the efficiency of this proposal. This can be implemented over Software Defined Networks (SDN) for increasing system capabilities in route analytics and functionalities. Extension of this optimization algorithm is useful to solve various real time problems.},
  archive      = {J_APIN},
  author       = {Chandra S. S., Vinod and Hareendran S., Anand},
  doi          = {10.1007/s10489-020-02105-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5965-5977},
  shortjournal = {Appl. Intell.},
  title        = {Phototropic algorithm for global optimisation problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual generative adversarial active learning. <em>APIN</em>,
<em>51</em>(8), 5953–5964. (<a
href="https://doi.org/10.1007/s10489-020-02121-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of active learning is to significantly reduce the cost of annotation while ensuring the good performance of the model. In this paper, we propose a novel active learning method based on the combination of pool and synthesis named dual generative adversarial active learning (DGAAL), which includes the functions of image generation and representation learning. This method includes two groups of generative adversarial network composed of a generator and two discriminators. One group is used for representation learning, and then this paper performs sampling based on the predicted value of the discriminator. The other group is used for image generation. The purpose is to generate samples which are similar to those obtained from sampling, so that samples with rich information can be fully utilized. In the sampling process, the two groups of network cooperate with each other to enable the generated samples to participate in sampling process, and to enable the discriminator for sampling to co-evolve. Thus, in the later stage of sampling, the problem of insufficient information for selecting samples based on the pool method is alleviated. In this paper, DGAAL is evaluated extensively on three data sets, and the results show that DGAAL not only has certain advantages over the existing methods in terms of model performance but can also further reduces the annotation cost.},
  archive      = {J_APIN},
  author       = {Guo, Jifeng and Pang, Zhiqi and Bai, Miaoyuan and Xie, Peijiao and Chen, Yu},
  doi          = {10.1007/s10489-020-02121-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5953-5964},
  shortjournal = {Appl. Intell.},
  title        = {Dual generative adversarial active learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-sensitive hierarchical classification via multi-scale
information entropy for data with an imbalanced distribution.
<em>APIN</em>, <em>51</em>(8), 5940–5952. (<a
href="https://doi.org/10.1007/s10489-020-02089-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced distributions present a great problem in machine learning classification tasks. Various algorithms based on cost-sensitive learning have been developed to address the imbalanced distribution problem. However, classes with a hierarchical tree structure create a new challenge for cost-sensitive learning. In this paper, we propose a cost-sensitive hierarchical classification method based on multi-scale information entropy. We construct an information entropy threshold for each level in the tree structure and assign cost-sensitive weights accordingly. First, we use the class hierarchy to divide a large hierarchical classification problem into several smaller sub-classification problems. In this way, a large-scale classification task can be decomposed into multiple, controllable, small-scale classification tasks. Second, we use a logistic regression algorithm to obtain the probabilities of classes at each level. Then, we consider the information entropy at each level as a threshold, which decreases inter-level error propagation in the tree structure. Finally, we design a cost-sensitive model based on the information of each class and use hierarchical information entropy weights as cost-sensitive weights. Information entropy measures the information of the majority and minority classes and allocates them different cost weights to solve imbalanced distribution problems. Experiments on four imbalanced distribution datasets demonstrate that the cost-sensitive hierarchical classification algorithm provides excellent efficiency and effectiveness.},
  archive      = {J_APIN},
  author       = {Zheng, Weijie and Zhao, Hong},
  doi          = {10.1007/s10489-020-02089-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5940-5952},
  shortjournal = {Appl. Intell.},
  title        = {Cost-sensitive hierarchical classification via multi-scale information entropy for data with an imbalanced distribution},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-attention based semantic deep hashing for cross-modal
retrieval. <em>APIN</em>, <em>51</em>(8), 5927–5939. (<a
href="https://doi.org/10.1007/s10489-020-02137-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing is an efficient method to retrieve cross domain data. Most previous methods focused on measuring the discrepancy between intro-modality and inter-modality. However, recent researches show that semantic information is vital for cross-modal retrieval as well. As for human vision system, people establish multi-modality connections by utilizing attention mechanism with semantic information. Most of the previous methods, which are attention-based, only simply apply single modality attention, ignoring the effectiveness of multi-attention. Multi-attention is consisted of features from different semantic representation space. For better filling the gap of semantic connection among modalities, it could guides the output features to achieve alignment via utilizing attention mechanism. From this perspective, we propose a new cross-modal hashing method in this paper: 1) We design a multi-attention block to extract features effected by multi-attention. 2) We propose a correlative loss function to optimize the multi-attention matrix generated by the block, and also make hash code consistent and semantically correlated in subsequent generation. Experiments on three challenging benchmarks demonstrate the effectiveness of our method in the application of cross-modal retrieval.},
  archive      = {J_APIN},
  author       = {Zhu, Liping and Tian, Gangyi and Wang, Bingyao and Wang, Wenjie and Zhang, Di and Li, Chengyang},
  doi          = {10.1007/s10489-020-02137-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5927-5939},
  shortjournal = {Appl. Intell.},
  title        = {Multi-attention based semantic deep hashing for cross-modal retrieval},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CapsNet-based supervised hashing. <em>APIN</em>,
<em>51</em>(8), 5912–5926. (<a
href="https://doi.org/10.1007/s10489-020-02180-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet technology, an increasing amount of data enters people’s daily life, which brings great challenges when users quickly search for interesting images. The existing exact nearest neighbor retrieval methods often fail to obtain results within an acceptable retrieval time, so researchers have begun to focus on approximate nearest neighbor retrieval. Recently, the hashing-based approximate nearest neighbor retrieval method has attracted increasing attention because of its small storage space and high retrieval efficiency. At present, one of the most advanced hashing methods is to use deep neural networks, especially convolutional neural networks (CNN), to obtain image hash codes to achieve fast image retrieval. However, CNN needs a large number of images during training, so it takes a lot of time to obtain training samples. In addition, CNN cannot handle ambiguity well, and a lot of information is lost in the pooling layer; furthermore, CNN cannot learn the hierarchical structure of the image. Aiming to address these problems while making full use of data classification information to guide hash learning in a supervised form to improve retrieval efficiency, we introduce the capsule network into the hash learning and propose CapsNet-based supervised hashing (CSH) to preserve the effective information of an image as much as possible. CSH adds a hashing layer equivalent to hash mapping between the capsule network and the decoder to perform hash learning. By optimizing the objective function defined for capsule network loss, reconstruction loss and hashing quantization loss, feature representations, hashing functions and classification results can be learned from the input data at the same time. To verify the effectiveness of the method, we performed experiments on multiple datasets. The experimental results show that this method is superior to the existing hashing-based image retrieval methods and achieves satisfactory results in image classification performance.},
  archive      = {J_APIN},
  author       = {Zhang, Bolin and Qian, Jiangbo and Xie, Xijiong and Xin, Yu and Dong, Yihong},
  doi          = {10.1007/s10489-020-02180-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5912-5926},
  shortjournal = {Appl. Intell.},
  title        = {CapsNet-based supervised hashing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian regularization restoration algorithm for photon
counting images. <em>APIN</em>, <em>51</em>(8), 5898–5911. (<a
href="https://doi.org/10.1007/s10489-020-02175-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The photon counting image collected under 10− 4 lux environment has a degraded image quality due to background noises and other problems. Bayesian estimation is a classical approach for photon counting image restoration and regularization has also been widely used in image processing. However, the regularization method is not suitable for photon counting images with extremely lack of information, and the recovery effect of Bayesian estimation in images mixed with unknown noise is not ideal. The main contribution of this paper is that on the basis of Bayesian estimation, the regularization method is introduced to solve the problem of restoring photon counting images mixed with unknown noise under 10− 4 lux environment. The original part is that the gamma distribution of the expected value of photon counting is used as its prior condition, and the error function is expressed as the form of the norm to establish the objective function. Through an approximate iterative solution, the optimal estimation of the photon counting expectation is carried out to achieve the optimal restoration of the photon counting image. Experiments demonstrate that the background noise is effectively removed and the image quality is improved after restoring photon counting images. Also, the final result of the proposed method is superior to other comparative methods in multiple evaluation indexes and achieved better effects.},
  archive      = {J_APIN},
  author       = {Li, Ying and Yin, Liju and Wang, Zhenzhou and Pan, Jinfeng and Gao, Mingliang and Zou, Guofeng and Liu, Jiansi and Wang, Lei},
  doi          = {10.1007/s10489-020-02175-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5898-5911},
  shortjournal = {Appl. Intell.},
  title        = {Bayesian regularization restoration algorithm for photon counting images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-regulated differential evolution for real parameter
optimization. <em>APIN</em>, <em>51</em>(8), 5873–5897. (<a
href="https://doi.org/10.1007/s10489-020-01973-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel differential evolution (DE) framework with a self-regulated neighborhood, termed self-regulated differential evolution (SrDE), for real parameter optimization. The novelty and advantage of SrDE are to present a self-regulated neighborhood (SrN) for learning, regulating, and using the neighborhood information of the population in guiding the search process of DE. Specifically, SrDE is characterized by the following three aspects. First, a one-dimensional self-organizing map (SOM) method is employed to dynamically construct the neighborhood relationships between individuals. Second, a self-sizing technique is applied to adaptively regulate the neighborhood size of each individual based on its search state. Third, a neighborhood path-assisted strategy is proposed to utilize promising neighborhood information for guiding the mutation process. Extensive experiments on a suite of real-parameter functions and real-world problems have demonstrated the superior and competitive performance of SrDE when compared with the state-of-the-art DE variants.},
  archive      = {J_APIN},
  author       = {Cai, Yiqiao and Wu, Duanwei and Fu, Shunkai and Zeng, Shengming},
  doi          = {10.1007/s10489-020-01973-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5873-5897},
  shortjournal = {Appl. Intell.},
  title        = {Self-regulated differential evolution for real parameter optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Moth-flame optimization algorithm based on diversity and
mutation strategy. <em>APIN</em>, <em>51</em>(8), 5836–5872. (<a
href="https://doi.org/10.1007/s10489-020-02081-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, an improved moth-flame optimization algorithm is proposed to alleviate the problems of premature convergence and convergence to local minima. From the perspective of diversity, an inertia weight of diversity feedback control is introduced in the moth-flame optimization to balance the algorithm’s exploitation and global search abilities. Furthermore, a small probability mutation after the position update stage is added to improve the optimization performance. The performance of the proposed algorithm is extensively evaluated on a suite of CEC’2014 series benchmark functions and four constrained engineering optimization problems. The results of the proposed algorithm are compared with the ones of other improved algorithms presented in literatures. It is observed that the proposed method has a superior performance to improve the convergence ability of the algorithm. In addition, the proposed algorithm assists in escaping the local minima.},
  archive      = {J_APIN},
  author       = {Ma, Lei and Wang, Chao and Xie, Neng-gang and Shi, Miao and Ye, Ye and Wang, Lu},
  doi          = {10.1007/s10489-020-02081-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5836-5872},
  shortjournal = {Appl. Intell.},
  title        = {Moth-flame optimization algorithm based on diversity and mutation strategy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image rain streak removal via layer similarity prior.
<em>APIN</em>, <em>51</em>(8), 5822–5835. (<a
href="https://doi.org/10.1007/s10489-020-02056-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image rain streak removal is a significant and challenging task, which is widely applied in many artificial intelligence domains as preprocessing process. Most of existing rain streak removal works focus on designing various deraining unit (e.g., multi-stream dilation convolution) without considering the correlation between different convolution layers, which may lead to large model size. In this paper, we propose a simple and effective deep network architecture for single image rain streak removal based on deep Convolutional Neural Network (CNN). Benefit from the adjacent layers with different dilation factors have similar feature structures, we design a powerful rain streak representation network based on the Layer Similarity Prior Block (LSPB). To better cater to the property of layer similarity prior, the multi-dense-short-connection is developed and it regards every LSPB as a convolution layer, this connection style makes our overall framework to be a layer similarity prior network. To the best of our knowledge, this is the first paper to investigate the effectiveness of exploiting the layer similarity prior and the multi-dense-short-connection. Quantitative and qualitative experimental results demonstrate that the proposed method outperforms other state-of-the-art methods with the least parameters.},
  archive      = {J_APIN},
  author       = {Fan, Wanshu and Wu, Yutong and Wang, Cong},
  doi          = {10.1007/s10489-020-02056-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5822-5835},
  shortjournal = {Appl. Intell.},
  title        = {Single image rain streak removal via layer similarity prior},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stream fish detection in unconstrained underwater
videos by the fusion of two convolutional neural network detectors.
<em>APIN</em>, <em>51</em>(8), 5809–5821. (<a
href="https://doi.org/10.1007/s10489-020-02155-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, marine biologists have begun using underwater videos to study species diversity and fish abundance. These techniques generate a large amount of visual data. Automatic analysis using image processing is therefore necessary, since manual processing is time-consuming and labor-intensive. However, there are numerous challenges to implementing the automatic processing of underwater images: for example, high luminosity variation, limited visibility, complex background, free movement of fish, and high diversity of fish species. In this paper, we propose two new fusion approaches that exploit two convolutional neural network (CNN) streams to merge both appearance and motion information for automatic fish detection. These approaches consist of two Faster R-CNN models that share either the same region proposal network or the same classifier. We significantly improve the fish detection performances on the LifeClef 2015 Fish benchmark dataset not only compared with the classic Faster R-CNN but also with all the state-of-the-art approaches. The best F-score and mAP measures are 83.16% and 73.69%, respectively.},
  archive      = {J_APIN},
  author       = {Ben Tamou, Abdelouahid and Benzinou, Abdesslam and Nasreddine, Kamal},
  doi          = {10.1007/s10489-020-02155-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5809-5821},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stream fish detection in unconstrained underwater videos by the fusion of two convolutional neural network detectors},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent deep reinforcement learning with type-based
hierarchical group communication. <em>APIN</em>, <em>51</em>(8),
5793–5808. (<a
href="https://doi.org/10.1007/s10489-020-02065-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world multi-agent tasks often involve varying types and quantities of agents. These agents connected by complex interaction relationships causes great difficulty for policy learning because they need to learn various interaction types to complete a given task. Therefore, simplifying the learning process is an important issue. In multi-agent systems, agents with a similar type often interact more with each other and exhibit behaviors more similar. That means there are stronger collaborations between these agents. Most existing multi-agent reinforcement learning (MARL) algorithms expect to learn the collaborative strategies of all agents directly in order to maximize the common rewards. This causes the difficulty of policy learning to increase exponentially as the number and types of agents increase. To address this problem, we propose a type-based hierarchical group communication (THGC) model. This model uses prior domain knowledge or predefine rule to group agents, and maintains the group’s cognitive consistency through knowledge sharing. Subsequently, we introduce a group communication and value decomposition method to ensure cooperation between the various groups. Experiments demonstrate that our model outperforms state-of-the-art MARL methods on the widely adopted StarCraft II benchmarks across different scenarios, and also possesses potential value for large-scale real-world applications.},
  archive      = {J_APIN},
  author       = {Jiang, Hao and Shi, Dianxi and Xue, Chao and Wang, Yajie and Wang, Gongju and Zhang, Yongjun},
  doi          = {10.1007/s10489-020-02065-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5793-5808},
  shortjournal = {Appl. Intell.},
  title        = {Multi-agent deep reinforcement learning with type-based hierarchical group communication},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depth scale balance saliency detection with connective
feature pyramid and edge guidance. <em>APIN</em>, <em>51</em>(8),
5775–5792. (<a
href="https://doi.org/10.1007/s10489-020-02150-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have played an important role in saliency detection. How to detect a salient object as a whole is a key issue. However, most existing learning-based methods are not accurate enough to detect salient objects in complex scenes, such as easily overlooked small salient areas in a whole salient object, which is called scale imbalance problem in this paper. To address this issue, Scale Balance Network (SBN) based on fully convolutional network is proposed to accurately recognize and comprehensively detect salient objects. Firstly, to detect more small salient areas, a specially designed backbone instead of common backbone is adopted in this paper, which can capture larger resolution with more spatial features in deeper layers. Secondly, we present a novel progressive pyramid mechanism named Connective Feature Pyramid Module (CFPM), aiming to make the network focus on the balance between the large salient areas and the small ones. Finally, we present an Edge Enhancement Architecture with Various Kernels (EEAVK) to locate the saliency maps and refine the boundary features. Experimental results on five benchmark datasets show that the proposed SBN method achieves consistently superior performance in comparison with other state-of-the-art ones under different evaluation metrics.},
  archive      = {J_APIN},
  author       = {Tan, Zhenshan and Gu, Xiaodong},
  doi          = {10.1007/s10489-020-02150-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5775-5792},
  shortjournal = {Appl. Intell.},
  title        = {Depth scale balance saliency detection with connective feature pyramid and edge guidance},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving cluster recovery with feature rescaling factors.
<em>APIN</em>, <em>51</em>(8), 5759–5774. (<a
href="https://doi.org/10.1007/s10489-020-02108-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data preprocessing stage is crucial in clustering. Features may describe entities using different scales. To rectify this, one usually applies feature normalisation aiming at rescaling features so that none of them overpowers the others in the objective function of the selected clustering algorithm. In this paper, we argue that the rescaling procedure should not treat all features identically. Instead, it should favour the features that are more meaningful for clustering. With this in mind, we introduce a feature rescaling method that takes into account the within-cluster degree of relevance of each feature. Our comprehensive simulation study, carried out on real and synthetic data, with and without noise features, clearly demonstrates that clustering methods that use the proposed data normalization strategy clearly outperform those that use traditional data normalization.},
  archive      = {J_APIN},
  author       = {de Amorim, Renato Cordeiro and Makarenkov, Vladimir},
  doi          = {10.1007/s10489-020-02108-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5759-5774},
  shortjournal = {Appl. Intell.},
  title        = {Improving cluster recovery with feature rescaling factors},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direct full quantification of the left ventricle via
multitask regression and classification. <em>APIN</em>, <em>51</em>(8),
5745–5758. (<a
href="https://doi.org/10.1007/s10489-020-02130-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left ventricle (LV) quantitative indices, such as the areas of the cavity and myocardium, dimensions of the cavity, regional wall thickness, and phase classification, play a vital role in assessing the overall and local cardiac function in clinical practice. However, due to variations in the cardiac structures of different subjects and the inconvenience of manual feature extraction, it is difficult to accurately estimate left ventricle indices. To solve the above problem, this paper presents a multitask regression and classification network (MTRC-net) based on image representation to improve the accuracy of the quantified results. The network employs an autoencoder network to perform image representation, and then the representative images are fed into the multitask regression and classification network to extract the temporal and spatial features of the cardiac images, which can better help the MTRC-net to learn the potential image features, reduce the experimental errors and speed up the convergence of the model. In the multitask regression and classification network, a pseudo-3D network is used to compress the network model, and the circular hypothesis is applied to fully obtain the temporal and spatial information of the cardiac images. The proposed algorithm was verified on a dataset that includes images of 145 patients from the left ventricle quantification competition of MICCAI2018, and the results are as follows. The average absolute error of MAE in the Areas (cavity area and myocardium area) is 106 ± 88 mm2, the average absolute error of MAE in the Dims (directional dimensions of the cavity) region is 1.54 ± 1.40 mm, the average absolute error of MAE in the RWTs(regional wall thicknesses region) is 0.96 ± 0.70 mm, and the error rate of phase classification is 1.2%. The errors of each index are almost always smaller than those of the indices of the existing methods, and the experimental results illustrate the feasibility and effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Huang, Xiaoying and Tian, Yun and Zhao, Shifeng and Liu, Tao and Wang, Wei and Wang, Qingjun},
  doi          = {10.1007/s10489-020-02130-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5745-5758},
  shortjournal = {Appl. Intell.},
  title        = {Direct full quantification of the left ventricle via multitask regression and classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust predictive control of coupled water tank plant.
<em>APIN</em>, <em>51</em>(8), 5726–5744. (<a
href="https://doi.org/10.1007/s10489-020-02083-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liquid level control of the liquid tank is a basic control problem in the process industry. The liquid level control system usually has the characteristics of strong coupling, nonlinearity and large lag. Up to now, PID control is the main method of liquid level control in industry, but it is difficult to obtain high precision control performance. The difficulty of model-based liquid level control such as Robust Model Predictive Control (RMPC) is hard to obtain an accurate model of the plant or need accurate steady-state information which is hard to be obtained in practice. This paper provides a systematic modeling and RMPC method to implement a Coupled Water Tank (CWT) plant output-tracking control in the case without whole steady-state information of the system. First, a data-driven modeling technique based on the Radial Basis Function (RBF) net-type coefficients Multi-Input/Multi-Output (MIMO) AutoRegressive model with eXogenous variable (ARX) is applied to build the RBF-ARX model of the CWT plant. Using the model, a locally linearized model and a polytopic uncertain linear parameter varying (LPV) model are constructed to represent the current and future behavior of the nonlinear CWT system. Based on the two models, an RMPC method is designed to implement the system’s output tracking control under the condition without whole steady-state knowledge of the CWT plant. The stability of the RMPC strategy is guaranteed by using the time-varying parameter-dependent Lyapunov function and feasibility of the Linear Matrix Inequalities (LMIs). Real-time control experiments and comparison with other control methods are carried out on the CWT system, and the results show that the presented method is superior.},
  archive      = {J_APIN},
  author       = {Kang, Tiao and Peng, Hui and Zhou, Feng and Tian, Xiaoying and Peng, Xiaoyan},
  doi          = {10.1007/s10489-020-02083-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5726-5744},
  shortjournal = {Appl. Intell.},
  title        = {Robust predictive control of coupled water tank plant},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CAFR-CNN: Coarse-to-fine adaptive faster r-CNN for
cross-domain joint optic disc and cup segmentation. <em>APIN</em>,
<em>51</em>(8), 5701–5725. (<a
href="https://doi.org/10.1007/s10489-020-02145-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is a leading cause of blindness. Accurate and efficient segmentation of the optic disc and cup from fundus images is important for glaucoma screening. However, using off-the-shelf networks against new datasets may lead to degraded performances due to domain shift. To address this issue, in this paper, we propose a coarse-to-fine adaptive Faster R-CNN framework for cross-domain joint optic disc and cup segmentation. The proposed CAFR-CNN consists of the Faster R-CNN detector, a spatial attention-based region alignment module, a pyramid ROI alignment module and a prototype-based semantic alignment module. The Faster R-CNN detector extracts features from fundus images using a VGG16 network as a backbone. The spatial attention-based region alignment module extracts the region of interest through a spatial mechanism and aligns the feature distribution from different domains via multilayer adversarial learning to achieve a coarse-grained adaptation. The pyramid ROI alignment module learns multilevel contextual features to prevent misclassifications due to the similar appearances of the optic disc and cup. The prototype-based semantic alignment module minimizes the distance of global prototypes with the same category between the target domain and source domain to achieve a fine-grained adaptation. We evaluated the proposed CAFR-CNN framework under different scenarios constructed from four public retinal fundus image datasets (REFUGE2, DRISHTI-GS, DRIONS-DB and RIM-ONE-r3). The experimental results show that the proposed method outperforms the current state-of-the-art methods and has good accuracy and robustness: it not only avoids the adverse effects of low contrast and noise interference but also preserves the shape priors and generates more accurate contours.},
  archive      = {J_APIN},
  author       = {Guo, Yanfei and Peng, Yanjun and Zhang, Bin},
  doi          = {10.1007/s10489-020-02145-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5701-5725},
  shortjournal = {Appl. Intell.},
  title        = {CAFR-CNN: Coarse-to-fine adaptive faster R-CNN for cross-domain joint optic disc and cup segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-GPU approach to global induction of classification
trees for large-scale data mining. <em>APIN</em>, <em>51</em>(8),
5683–5700. (<a
href="https://doi.org/10.1007/s10489-020-01952-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the evolutionary induction of decision trees (DT) for large-scale data. Such a global approach is one of the alternatives to the top-down inducers. It searches for the tree structure and tests simultaneously and thus gives improvements in the prediction and size of resulting classifiers in many situations. However, it is the population-based and iterative approach that can be too computationally demanding to apply for big data mining directly. The paper demonstrates that this barrier can be overcome by smart distributed/parallel processing. Moreover, we ask the question whether the global approach can truly compete with the greedy systems for large-scale data. For this purpose, we propose a novel multi-GPU approach. It incorporates the knowledge of global DT induction and evolutionary algorithm parallelization together with efficient utilization of memory and computing GPU’s resources. The searches for the tree structure and tests are performed simultaneously on a CPU, while the fitness calculations are delegated to GPUs. Data-parallel decomposition strategy and CUDA framework are applied. Experimental validation is performed on both artificial and real-life datasets. In both cases, the obtained acceleration is very satisfactory. The solution is able to process even billions of instances in a few hours on a single workstation equipped with 4 GPUs. The impact of data characteristics (size and dimension) on convergence and speedup of the evolutionary search is also shown. When the number of GPUs grows, nearly linear scalability is observed what suggests that data size boundaries for evolutionary DT mining are fading.},
  archive      = {J_APIN},
  author       = {Jurczuk, Krzysztof and Czajkowski, Marcin and Kretowski, Marek},
  doi          = {10.1007/s10489-020-01952-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5683-5700},
  shortjournal = {Appl. Intell.},
  title        = {Multi-GPU approach to global induction of classification trees for large-scale data mining},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiview image generation for vehicle reidentification.
<em>APIN</em>, <em>51</em>(8), 5665–5682. (<a
href="https://doi.org/10.1007/s10489-020-02171-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification (ReID) with viewpoint variations is an interesting but challenging task in computer vision. Most existing vehicle ReID approaches focus on the original single view, which requires vehicle features in varying views. However, this approach limits the models’ discriminative capabilities in realistic scenarios due to the lack of visual information in arbitrary views. In this paper, we propose a multi-view generative adversarial network (MV-GAN) that can synthesize real vehicle images conditioned on arbitrary skeleton views. MV-GAN is designed specifically for viewpoint normalization in vehicle ReID. Based on the generated images, we can infer a multi-view vehicle representation to learn distance metrics for vehicle ReID from the original images that is free of the influence of viewpoint variations. We show that the features of the generated images and the original images are complementary. We demonstrate the validity of the proposed method through extensive experiments on the VeRi, VehicleID, and VRIC datasets and show the superiority of multi-view image generation for improving vehicle ReID through comparisons with the state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Zhang, Fukai and Ma, Yongqiang and Yuan, Guan and Zhang, Haiyan and Ren, Jianji},
  doi          = {10.1007/s10489-020-02171-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5665-5682},
  shortjournal = {Appl. Intell.},
  title        = {Multiview image generation for vehicle reidentification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple ant colony optimization using both novel LSTM
network and adaptive tanimoto communication strategy. <em>APIN</em>,
<em>51</em>(8), 5644–5664. (<a
href="https://doi.org/10.1007/s10489-020-02099-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant Colony Optimization (ACO) tends to fall into local optima and has insufficient convergence when solving the Traveling Salesman Problem (TSP). To overcome this problem, this paper proposes a multiple ant colony optimization (LDTACO) based on novel Long Short-Term Memory network and adaptive Tanimoto communication strategy. Firstly, we introduce an Artificial Bee Colony-based Ant Colony System (ABC-ACS), which along with the classic Ant Colony System (ACS) and Max-Min Ant System (MMAS), form the final proposed algorithm. These three types of subpopulations complement each other to improve overall optimization performance. Secondly, the evaluation reward mechanism is proposed to enhance the guiding role of the Recommended paths, which can effectively accelerate convergence speed. Besides, an adaptive Tanimoto communication strategy is put forward for interspecific communication. When the algorithm is stagnant, the homogenized information communication method is activated to help the algorithm jump out of the local optima, thus improving solution accuracy. Finally, the experimental results show that the proposed algorithm can lead to more accurate solution accuracy and faster convergence speed.},
  archive      = {J_APIN},
  author       = {Li, Shundong and You, Xiaoming and Liu, Sheng},
  doi          = {10.1007/s10489-020-02099-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5644-5664},
  shortjournal = {Appl. Intell.},
  title        = {Multiple ant colony optimization using both novel LSTM network and adaptive tanimoto communication strategy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving microelectronic thermal management problems using a
generalized spiral optimization algorithm. <em>APIN</em>,
<em>51</em>(8), 5622–5643. (<a
href="https://doi.org/10.1007/s10489-020-02164-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristics have risen as an approach for addressing diverse optimization problems by mimicking biological processes. They have proven to be effective in different fields and problems, which has skyrocketed their popularity. A recent proposal, the spiral optimization algorithm (SOA), is based on the logarithmic spiral behavior that appears in several natural scenarios. Variants of this deterministic SOA (DSOA) have emerged, seeking to improve its performance. One of them is the stochastic SOA (SSOA), which transforms a deterministic path into a random path. In this work, we make two contributions. First, we present a generalized version of the algorithm that includes the DSOA and SSOA. We use it to study the effect of allowing a random ‘reflection’ in the rotation angle of the spirals. In our proposed approach, a ‘reflection’ entails replacing the rotation angle (𝜃) with its supplementary angle (180∘− 𝜃) in the current iteration. Thus, the ‘reflection’ allows for increased diversity when exploring the search domain. To test this idea, we use several test functions, including the CEC2005 benchmark, in multiple dimensions. Finally, we use this reflection-based optimization of the stochastic spiral algorithm (ROSSA) to solve a microelectronic thermal management problem from the literature and compare its performance against some recently reported values. The data reveal that adding the proposed coefficient leads to a statistically significant improvement in performance. Most ROSSA configurations outperform all tested settings of the DSOA and SSOA, for virtually all problems. Hence, we firmly believe that the proposed generalization is adequate and that random ‘reflections’ help improve the performance of the DSOA, without increasing its computational burden.},
  archive      = {J_APIN},
  author       = {Cruz-Duarte, Jorge M. and Amaya, Iván and Ortíz-Bayliss, José Carlos and Correa, Rodrigo},
  doi          = {10.1007/s10489-020-02164-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5622-5643},
  shortjournal = {Appl. Intell.},
  title        = {Solving microelectronic thermal management problems using a generalized spiral optimization algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive fusion with multi-scale features for interactive
image segmentation. <em>APIN</em>, <em>51</em>(8), 5610–5621. (<a
href="https://doi.org/10.1007/s10489-020-02114-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale features are usually utilized to improve the performance of interactive image segmentation, however, they have varying leverages over the result of segmentation, for example, thinner segmentation results could be achieved by pixel-level features, but sensitive to image noise, and superpixel-level features could provide the semantic perception of the object, but easily lead to over-segmentations. Therefore, we propose an interactive image segmentation algorithm by adaptive fusion with multi-scale features (AFMSF). It intends on combining the multi-scale information adaptively for the segmentation via learning the influence coefficients of multi-scale features. First, multi-scale superpixel layers are generated by controlling the size of superpixels. Based on features of this multi-scale information, the similarity matrices and label priors with pixel-superpixel levels are then obtained. A fusion with diffusion strategy is designed to build the energy function by combining these multi-cues. Finally, the influence coefficient of each scale and the labeling are updated with each other until convergence. The algorithm we proposed is robust to diverse circumstances of objects, the experimental results on public interactive image segmentation datasets Graz, LHI, and MSRC validate the superior performance of the proposed method.},
  archive      = {J_APIN},
  author       = {Ding, Zongyuan and Wang, Tao and Sun, Quansen and Wang, Hongyuan},
  doi          = {10.1007/s10489-020-02114-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5610-5621},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive fusion with multi-scale features for interactive image segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensor data-driven structural damage detection based on deep
convolutional neural networks and continuous wavelet transform.
<em>APIN</em>, <em>51</em>(8), 5598–5609. (<a
href="https://doi.org/10.1007/s10489-020-02092-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural damage detection is of very importance to improve reliability and safety of civil structures. A novel sensor data-driven structural damage detection method is proposed in this paper by combining continuous wavelet transform (CWT) with deep convolutional neural network (DCNN). In this method, time-frequency images are obtained by CWT from original one-dimensional sensor signals. And, DCNN is designed to mine structural damage features from the time-frequency images and distinguish different structural damage condition. The proposed method is carried out on three-story building structure dataset and steel frame dataset. The experimental results show that the proposed method has the high accuracy and robustness of the damage detection compared with other existing machine learning methods.},
  archive      = {J_APIN},
  author       = {Chen, Zuoyi and Wang, Yanzhi and Wu, Jun and Deng, Chao and Hu, Kui},
  doi          = {10.1007/s10489-020-02092-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5598-5609},
  shortjournal = {Appl. Intell.},
  title        = {Sensor data-driven structural damage detection based on deep convolutional neural networks and continuous wavelet transform},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incomplete multi-view subspace clustering with adaptive
instance-sample mapping and deep feature fusion. <em>APIN</em>,
<em>51</em>(8), 5584–5597. (<a
href="https://doi.org/10.1007/s10489-020-02138-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering has been widely applied in practical applications. It fuses complementary information across multiple views and treats all samples of a view as a set of bases of a generalized subspace. Meanwhile, it assumes that an instance has all features corresponding to all views. However, each view may lose some features due to the malfunction, which results in an incomplete multi-view dataset. This paper presents an incomplete multi-view subspace clustering with adaptive instance-sample mapping and deep feature fusion algorithm (IMDF). Owing to the good performance of attention mechanism, we fuse deep features adaptively extracted by convolutional neural networks in a weighted way to integrate abundant information from distinctive views, reduce redundancy between features, and generalize a robust and compact representation before self-representation stage. We jointly process feature training and clustering for a specific task to weaken the sensitivity of model to pre-extracted features. At the same time, we propose a modified weighted view-specific instance-sample mapping strategy to solve the inconsistent dimensions problem induced by different numbers of samples of distinct views in the process of learning a common representation in a unified latent subspace. Experimental results demonstrate that our method outperforms five state-of-the-art methods on various real-world datasets including images and documents.},
  archive      = {J_APIN},
  author       = {Xie, Mengying and Ye, Zehui and Pan, Gan and Liu, Xiaolan},
  doi          = {10.1007/s10489-020-02138-9},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5584-5597},
  shortjournal = {Appl. Intell.},
  title        = {Incomplete multi-view subspace clustering with adaptive instance-sample mapping and deep feature fusion},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new transductive learning method with universum data.
<em>APIN</em>, <em>51</em>(8), 5571–5583. (<a
href="https://doi.org/10.1007/s10489-020-02113-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transductive learning is a generalization of semi-supervised learning which attempts to learn a distinctive classifier from large amounts of unlabeled data. In addition, Universum data can bring prior knowledge to the classifier. The Universum data mean the data which do not belong to the positive or negative classes, which can improve the performance of the learning task. In this paper, we address the problem of transductive learning with Universum data, and propose a new method, called information entropy-based transductive support vector machine with Universum data(IEB-TUSVM), which mainly consists of two steps. In the first Nstep, we propose an information entropy-based method to select the informative examples from the source Universum data to obtain the informative Universum data. In the second step, we take the selected Universum data into the semi-supervised classification, which is further solved by the Lagrange method. We further analyze the computational complexity of the proposed method. Extensive experiments have shown that IEB-TUSVM method outperforms state-of-the-art semi-supervised methods and the Universum learning methods.},
  archive      = {J_APIN},
  author       = {Xiao, Yanshan and Feng, Junyao and Liu, Bo},
  doi          = {10.1007/s10489-020-02113-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5571-5583},
  shortjournal = {Appl. Intell.},
  title        = {A new transductive learning method with universum data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EmNet: A deep integrated convolutional neural network for
facial emotion recognition in the wild. <em>APIN</em>, <em>51</em>(8),
5543–5570. (<a
href="https://doi.org/10.1007/s10489-020-02125-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, facial emotion recognition (FER) research saw tremendous progress, which led to the development of novel convolutional neural network (CNN) architectures for automatic recognition of facial emotions in static images. These networks, though, have achieved good recognition accuracy, they incur high computational costs and memory utilization. These issues restrict their deployment in real-world applications, which demands the FER systems to run on resource-constrained embedded devices in real-time. Thus, to alleviate these issues and to develop a robust and efficient method for automatic recognition of facial emotions in the wild with real-time performance, this paper presents a novel deep integrated CNN model, named EmNet (Emotion Network). The EmNet model consists of two structurally similar DCNN models and their integrated variant, jointly-optimized using a joint-optimization technique. For a given facial image, the EmNet gives three predictions, which are fused using two fusion schemes, namely average fusion and weighted maximum fusion, to obtain the final decision. To test the efficiency of the proposed FER pipeline on a resource-constrained embedded platform, we optimized the EmNet model and the face detector using TensorRT SDK and deploy the complete FER pipeline on the Nvidia Xavier device. Our proposed EmNet model with 4.80M parameters and 19.3MB model size attains notable improvement over the current state-of-the-art in terms of accuracy with multi-fold improvement in computational efficiency.},
  archive      = {J_APIN},
  author       = {Saurav, Sumeet and Saini, Ravi and Singh, Sanjay},
  doi          = {10.1007/s10489-020-02125-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5543-5570},
  shortjournal = {Appl. Intell.},
  title        = {EmNet: A deep integrated convolutional neural network for facial emotion recognition in the wild},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new dominance relation based on convergence indicators and
niching for many-objective optimization. <em>APIN</em>, <em>51</em>(8),
5525–5542. (<a
href="https://doi.org/10.1007/s10489-020-01976-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining a good balance between convergence and diversity is crucial in many-objective optimization, while most existing dominance relations can not achieve a good balance between them. In this paper, we propose a new dominance relation to better balance the convergence and diversity. In the proposed dominance relation, a convergence indicator and a niching technique based adaptive parameter are adopted to ensure the convergence and diversity of the nondominated solution set. Based on the proposed dominance relation, a new many-objective evolutionary algorithm is proposed. In the algorithm, a new distribution estimation method is proposed to obtain better solutions for mating selection. Experimental results indicate that the proposed dominance relation outperforms existing dominance relations in balancing the convergence and diversity and the proposed algorithms has a competitive performance against several state-of-art many-objective evolutionary algorithms.},
  archive      = {J_APIN},
  author       = {Yang, Feng and Xu, Liang and Chu, Xiaokai and Wang, Shenwen},
  doi          = {10.1007/s10489-020-01976-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5525-5542},
  shortjournal = {Appl. Intell.},
  title        = {A new dominance relation based on convergence indicators and niching for many-objective optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feasibility, planning and control of ground-wall transition
for a suctorial hexapod robot. <em>APIN</em>, <em>51</em>(8), 5506–5524.
(<a href="https://doi.org/10.1007/s10489-020-01955-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key factors that affect the efficiency and scope of work of wall-climbing robots is how the climbing robot can achieve autonomous transition between adjacent vertical planes. This paper studies the problem of ground-wall transition of a self-developed suctorial wall-climbing hexapod robot (WelCH). In view of the feasibility of the robot performing transition, this paper makes a detailed analysis of the number and property of degrees of freedom (DOFs) of the body and the foot based on reciprocal screw theory, and the results show that the robot can achieve transitional motion only when its home configuration is axisymmetric rather than radially symmetric. For realizing the robot’s ground-wall transition, based on a Sinusoid-Sigmoid-Shaped (SS-Shaped) interpolation function, the motion strategies of foot transferring and body pitching are firstly designed in detail. This interpolation method can effectively avoid the wear of the suction cups by relying on fewer essential path points. Then, the saturation-truncated method and mean filtering method are used to deal with joint constraints and abrupt changes in angular velocities. Finally, a kinematic-based adaptive sliding mode control (ASMC) is adapted to track the planned smooth trajectory, which can effectively resist bounded external disturbances. The successful transitions from the horizontal ground to the vertical wall for the robot WelCH in simulation and filed experiment demonstrate the effectiveness of the proposed strategy.},
  archive      = {J_APIN},
  author       = {Gao, Yong and Wei, Wu and Wang, Xinmei and Li, Yanjie and Wang, Dongliang and Yu, Qiuda},
  doi          = {10.1007/s10489-020-01955-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5506-5524},
  shortjournal = {Appl. Intell.},
  title        = {Feasibility, planning and control of ground-wall transition for a suctorial hexapod robot},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth twin bounded support vector machine with pinball
loss. <em>APIN</em>, <em>51</em>(8), 5489–5505. (<a
href="https://doi.org/10.1007/s10489-020-02085-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The twin support vector machine improves the classification performance of the support vector machine by solving two small quadratic programming problems. However, this method has the following defects: (1) For the twin support vector machine and some of its variants, the constructed models use a hinge loss function, which is sensitive to noise and unstable in resampling. (2) The models need to be converted from the original space to the dual space, and their time complexity is high. To further enhance the performance of the twin support vector machine, the pinball loss function is introduced into the twin bounded support vector machine, and the problem of the pinball loss function not being differentiable at zero is solved by constructing a smooth approximation function. Based on this, a smooth twin bounded support vector machine model with pinball loss is obtained. The model is solved iteratively in the original space using the Newton-Armijo method. A smooth twin bounded support vector machine algorithm with pinball loss is proposed, and theoretically the convergence of the iterative algorithm is proven. In the experiments, the proposed algorithm is validated on the UCI datasets and the artificial datasets. Furthermore, the performance of the presented algorithm is compared with those of other representative algorithms, thereby demonstrating the effectiveness of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Li, Kai and Lv, Zhen},
  doi          = {10.1007/s10489-020-02085-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5489-5505},
  shortjournal = {Appl. Intell.},
  title        = {Smooth twin bounded support vector machine with pinball loss},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel clustering ensemble model based on granular
computing. <em>APIN</em>, <em>51</em>(8), 5474–5488. (<a
href="https://doi.org/10.1007/s10489-020-01979-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble is one of the popular methods in the field of data mining for discovering hidden patterns in unlabeled datasets. Researches have shown that selecting base clustering results with certain differences and high quality to participate in the fusion process can improve the quality of the final result. However, the existing inherent characteristics of uncertainty, ambiguity, and overlap of the base clustering results make the selection of the base clustering members more difficult. The accuracy of the final results is easily disturbed by low-quality base clustering members. From the perspective of granular computing, a novel clustering ensemble model is proposed. The similarity among ensemble members is measured by granularity distance, so the quality of the base clustering results is ensured meanwhile the difference among them is enlarged, which is beneficial to improve the accuracy of the final result. According to the dividing ability of knowledge granularity, the method of elements generation for the co-association matrix is optimized and improved. The results obtained from the improved sample similarity measurement are more consistent with the structure of the real data. Compared with the traditional single clustering algorithm and some popular clustering ensemble methods, experiments show that the proposed model improves the quality of the final clustering result and has good expandability.},
  archive      = {J_APIN},
  author       = {Xu, Li and Ding, Shifei},
  doi          = {10.1007/s10489-020-01979-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5474-5488},
  shortjournal = {Appl. Intell.},
  title        = {A novel clustering ensemble model based on granular computing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile sensor patrol path planning in partially observable
border regions. <em>APIN</em>, <em>51</em>(8), 5453–5473. (<a
href="https://doi.org/10.1007/s10489-020-02068-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A border surveillance operation requires sophisticated sensor planning, as sensors are usually scarce and cannot cover an entire region simultaneously. Border patrol agents act as moving sensors in a border region, and the border patrol agents’ coverage moves around the region dynamically, increasing the chance of approaching a trespassing agent. Typically, the locations of trespassing agents cannot be fully observed due to the size of the border region and obstacles. In addition, intelligent trespassing agents may dynamically adjust their traveling paths so that the border patrol agents cannot predict their locations easily. Trespassing agents are assumed to leave traces that indicate their footprints, providing their estimated locations. Border patrol agents may use the trespassers’ footprints as partial information to leverage patrol path planning. We propose an adaptive border patrol process as a partially observable Markov decision process (POMDP), in which an individual border patrol agent’s decision is determined dynamically on the basis of trespassing agents’ partially observed locations. The observations are shared among individual border patrol agents, allowing the border patrol agents to cooperate. The zoning technique is used to limit the planning scope of an individual border patrol agent, and Monte Carlo simulation is applied to reduce the complexity of the POMDP planning problem. Empirical experiments are conducted by means of simulated agents. The simulation parameters are derived from the interviews with a group of border patrol experts. The results in different scenarios show that the proposed patrol path planning scheme outperforms other patrol path planning schemes in terms of the trespasser detection rate. The simulation results are validated with respect to subject matter experts (SMEs), where SMEs are the same border patrol experts who had given the interviews. The proposed method has potential in border surveillance as an assisting system for human border patrol or an automated guidance system in robots or drones.},
  archive      = {J_APIN},
  author       = {Pawgasame, Wichai and Wipusitwarakun, Komwut},
  doi          = {10.1007/s10489-020-02068-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5453-5473},
  shortjournal = {Appl. Intell.},
  title        = {Mobile sensor patrol path planning in partially observable border regions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PH-model: Enhancing multi-passage machine reading
comprehension with passage reranking and hierarchical information.
<em>APIN</em>, <em>51</em>(8), 5440–5452. (<a
href="https://doi.org/10.1007/s10489-020-02168-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine reading comprehension(MRC), which employs computers to answer questions from given passages, is a popular research field. In natural language, a natural hierarchical representation can be seen: characters, words, phrases, sentences, paragraphs, and documents. Current studies have demonstrated that hierarchical information can help machines understand natural language. However, prior works focused on the overall performance of MRC tasks without considering hierarchical information. In addition, the noise problem still has not been adequately addressed, even though many researchers have adopted the technique of passage reranking. Thus, in this paper, focusing on noise information processing and the extraction of hierarchical information, we propose a model (PH-Model) with a passage reranking framework (P) and hierarchical neural network (H) for a Chinese multi-passage MRC task. PH-Model produces more precise answers by reducing noise information and extracting hierarchical information. Experimental results on the DuReader 2.0 dataset (a large scale real-world Chinese MRC dataset) show that PH-Model outperforms the ROUGE-L and BLEU-4 baseline by 18.24% and 24.17%, respectively.},
  archive      = {J_APIN},
  author       = {Cong, Yao and Wu, Yimin and Liang, Xinbo and Pei, Jiayan and Qin, Zishan},
  doi          = {10.1007/s10489-020-02168-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5440-5452},
  shortjournal = {Appl. Intell.},
  title        = {PH-model: Enhancing multi-passage machine reading comprehension with passage reranking and hierarchical information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online deep learning based on auto-encoder. <em>APIN</em>,
<em>51</em>(8), 5420–5439. (<a
href="https://doi.org/10.1007/s10489-020-02058-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning is an important technical means for sketching massive real-time and high-speed data. Although this direction has attracted intensive attention, most of the literature in this area ignore the following three issues: (1) they think little of the underlying abstract hierarchical latent information existing in examples, even if extracting these abstract hierarchical latent representations is useful to better predict the class labels of examples; (2) the idea of preassigned model on unseen datapoints is not suitable for modeling streaming data with evolving probability distribution. This challenge is referred as “model flexibility”. And so, with this in minds, the online deep learning model we need to design should have a variable underlying structure; (3) moreover, it is of utmost importance to fusion these abstract hierarchical latent representations to achieve better classification performance, and we should give different weights to different levels of implicit representation information when dealing with the data streaming where the data distribution changes. To address these issues, we propose a two-phase Online Deep Learning based on Auto-Encoder (ODLAE). Based on auto-encoder, considering reconstruction loss, we extract abstract hierarchical latent representations of instances; Based on predictive loss, we devise two fusion strategies: the output-level fusion strategy, which is obtained by fusing the classification results of encoder’s each hidden layer; and feature-level fusion strategy, which is leveraged self-attention mechanism to fusion the every hidden layer’s output. Finally, in order to improve the robustness of the algorithm, we also try to utilize the denoising auto-encoder to yield hierarchical latent representations. Experimental results on different datasets are presented to verify the validity of our proposed algorithm (ODLAE) outperforms several baselines.},
  archive      = {J_APIN},
  author       = {Zhang, Si-si and Liu, Jian-wei and Zuo, Xin and Lu, Run-kun and Lian, Si-ming},
  doi          = {10.1007/s10489-020-02058-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5420-5439},
  shortjournal = {Appl. Intell.},
  title        = {Online deep learning based on auto-encoder},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automated fault detection system for communication
networks and distributed systems. <em>APIN</em>, <em>51</em>(8),
5405–5419. (<a
href="https://doi.org/10.1007/s10489-020-02026-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating fault detection in communication networks and distributed systems is a challenging process that usually requires the involvement of supporting tools and the expertise of system operators. Automated event monitoring and correlating systems produce event data that is forwarded to system operators for analyzing error events and creating fault reports. Machine learning methods help not only analyzing event data more precisely but also forecasting possible error events by learning from existing faults. This study introduces an automated fault detection system that assists system operators in detecting and forecasting faults. This system is characterized by the capability of exploiting bug knowledge resources at various online repositories, log events and status parameters from the monitored system; and applying bug analysis and event filtering methods for evaluating events and forecasting faults. The system contains a fault data model to collect bug reports, a feature and semantic filtering method to correlate log events, and machine learning methods to evaluate the severity, priority and relation of log events and forecast the forthcoming critical faults of the monitored system. We have evaluated the prototyping implementation of the proposed system on a high performance computing cluster system and provided analysis with lessons learned.},
  archive      = {J_APIN},
  author       = {Van Nguyen, Sinh and Tran, Ha Manh},
  doi          = {10.1007/s10489-020-02026-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5405-5419},
  shortjournal = {Appl. Intell.},
  title        = {An automated fault detection system for communication networks and distributed systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heart rate estimation based on face video under unstable
illumination. <em>APIN</em>, <em>51</em>(8), 5388–5404. (<a
href="https://doi.org/10.1007/s10489-020-02167-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) is a noncontact heart rate (HR) measurement technique. The current heart rate measurement methods based on rPPG all require ideal lighting conditions, but the lighting in real scenes is complicated, Therefore, this article proposes a robust heart rate measurement method when unstable light (time-varying light and uneven spatial illumination) exists. First, the method locates the ROI(Region of Interest) area of ​​the face, divides it into blocks, and uses the color signals of different sub-blocks to establish a three-dimensional rPPG model. Second, the method performs logarithmic operations on each frame of the image to convert the relationship between the illumination component and the reflection component from a product to a sum so that the reflected component and noise can be separated in the frequency domain. Then, the ensemble empirical mode decomposition (EEMD) is used to decompose the reflected component, and the obtained intrinsic mode function (IMF) is applied to obtain the waveform reflecting the change in the heart rate. Finally, the signal quality (SQ) of each ROI sub-block is calculated, and the high-quality signals are combined to reconstruct the heart rate signal.},
  archive      = {J_APIN},
  author       = {Yin, Ruo-Nan and Jia, Rui-Sheng and Cui, Zhe and Yu, Jin-Tao and Du, Yan-Bin and Gao, Li and Sun, Hong-Mei},
  doi          = {10.1007/s10489-020-02167-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5388-5404},
  shortjournal = {Appl. Intell.},
  title        = {Heart rate estimation based on face video under unstable illumination},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective whale optimization algorithm and
multi-objective grey wolf optimizer for solving next release problem
with developing fairness and uncertainty quality indicators.
<em>APIN</em>, <em>51</em>(8), 5358–5387. (<a
href="https://doi.org/10.1007/s10489-020-02018-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting a set of requirements to implement in the next software release is an NP-Hard problem known as NRP. We propose multi-objective versions of grey wolf optimizer and whale optimization algorithm for solving bi-objective NRP. We used these two algorithms and three other evolutionary algorithms to solve NRP problem instances from four datasets. The cost-to-score ratio and the roulette wheel are used to satisfy constraints of the NRP problem. We compare obtained Pareto fronts based on eight quality indicators. In addition to four general multi-objective optimization quality indicators, the three aspects of fairness among clients and also uncertainty are reconfigured as quality indicators. These quality indicators are computed for a Pareto front. Results show that MOWOA performs better than others and makes requirement selection fairer. MOGWO works better than the rest when budget constraints are reduced.},
  archive      = {J_APIN},
  author       = {Ghasemi, Mohsen and Bagherifard, Karamollah and Parvin, Hamid and Nejatian, Samad and Pho, Kim-Hung},
  doi          = {10.1007/s10489-020-02018-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5358-5387},
  shortjournal = {Appl. Intell.},
  title        = {Multi-objective whale optimization algorithm and multi-objective grey wolf optimizer for solving next release problem with developing fairness and uncertainty quality indicators},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency prediction on omnidirectional images with
attention-aware feature fusion network. <em>APIN</em>, <em>51</em>(8),
5344–5357. (<a
href="https://doi.org/10.1007/s10489-020-01857-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed rapid development of deep learning technology and its successful application in the saliency prediction of traditional 2D images. However, when using deep neural network (DNN) models to perform saliency prediction on omnidirectional images (ODIs), there are two critical issues: (1) The datasets for ODIs are small-scale that cannot support the training DNN-based models. (2) It is challenging to perform saliency prediction in that some ODIs contain complex background clutters. In order to solve these two problems, we propose a novel Attention-Aware Features Fusion Network (AAFFN) model which is first trained with traditional 2D images and then transferred to the ODIs for saliency prediction. Specifically, our proposed AAFFN model consists of three modules: a Part-guided Attention (PA) module, a Visibility Score (VS) module, and a Attention-Aware Features Fusion (AAFF) module. The PA module is used to extract precise features to estimate attention of the finer part on ODIs, and eliminate the influence of cluttered background. Meanwhile, the VS module is introduced to measure the proportion of the foreground and background parts and generate visibility scores in the feature learning process. Finally, in the AAFF module, we utilize the weighted fusion of attention maps and visibility scores to generate the final saliency map. Extensive experiments and ablation analysis demonstrate that the proposed model achieves superior performance and outperforms other state-of-the-art methods on public benchmark datasets.},
  archive      = {J_APIN},
  author       = {Zhu, Dandan and Chen, Yongqing and Zhao, Defang and Zhou, Qiangqiang and Yang, Xiaokang},
  doi          = {10.1007/s10489-020-01857-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5344-5357},
  shortjournal = {Appl. Intell.},
  title        = {Saliency prediction on omnidirectional images with attention-aware feature fusion network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust twin bounded support vector machines for outliers and
imbalanced data. <em>APIN</em>, <em>51</em>(8), 5314–5343. (<a
href="https://doi.org/10.1007/s10489-020-01847-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truncated loss functions are robust to class noise and outliers. A robust twin bounded support vector machine is proposed in this paper that truncates the growth of its loss functions at a pre-specified point, thus, flattens the function that pre-specified score afterwards. Moreover, to make the proposed method capable of handling datasets with different imbalance ratio, cost-sensitive learning is implemented by scaling the total error of the classes based on the number of samples of each class. However, the adopted loss functions take a non-convex structure which does not always assure global optimum. To handle this issue, we suggest concave-convex procedure (CCCP) to ensure global convergence by decomposing the cost functions into additions of one convex and one concave part. The behaviour of the proposed method for varied imbalance ratio is analysed experimentally and depicted graphically. Classification performance of the proposed method in terms of AUC, F-Measure and G-Mean is compared with other related methods, viz. Hinge loss support vector machine (SVM), Ramp loss SVM (RSVM), twin SVM (TWSVM), twin bounded SVM (TBSVM), Pinball loss SVM (pin-SVM), entropy-based fuzzy SVM (EFSVM), non-parallel hyperplane Universum SVM (U-NHSVM), stochastic gradient twin support vector machine (SGTSVM), k-nearest neighbor (KNN)-based maximum margin and minimum volume hyper-sphere machine (KNN-M3VHM) and affinity and class probability-based fuzzy SVM (ACFSVM) on several real-world datasets with imbalance ratio raging from low to high. Further, to establish the significance of the proposed method in pattern classification, pair-wise statistical comparison of the methods is performed based on their average ranks on AUC. Experimental results are convincing.},
  archive      = {J_APIN},
  author       = {Borah, Parashjyoti and Gupta, Deepak},
  doi          = {10.1007/s10489-020-01847-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5314-5343},
  shortjournal = {Appl. Intell.},
  title        = {Robust twin bounded support vector machines for outliers and imbalanced data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overlapping attributed graph clustering using mixed strategy
games. <em>APIN</em>, <em>51</em>(8), 5299–5313. (<a
href="https://doi.org/10.1007/s10489-020-02030-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike a simple network with just nodes and edges in between them, the real-world networks can contain much more, such as a set of attributes associated with every node in the network. These networks opened up a new avenue in community detection called attributed graph clustering (AGC). Furthermore, the clusters in real-world are not usually disjoint, as compared to most of the work that has been carried out in the field of AGC. This raises a need for AGC with fuzzy clusters. In this work, we try to comprehend the problem of attributed graph clustering with the help of a game-theoretic approach called dynamic cluster formation game (DCFG). To address the possibility of fuzzy clusters in a network, we model the problem of AGC as a series of coupled games involving mixed strategies, in contrast to the previous work that was primarily focused on pure strategy equilibrium. We discuss the convergence of the proposed game and the existence of Nash equilibrium at convergence. We also propose a clustering algorithm which uses a game-theoretic approach to partition a network into fuzzy clusters, giving a solution balanced in terms of topology and node attributes. We compare the the results of our work to the state-of-the-art clustering methods available in the literature.},
  archive      = {J_APIN},
  author       = {Kumar, Mayank and Gupta, Ruchir},
  doi          = {10.1007/s10489-020-02030-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5299-5313},
  shortjournal = {Appl. Intell.},
  title        = {Overlapping attributed graph clustering using mixed strategy games},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiobjective fuzzy clustering with multiple spatial
information for noisy color image segmentation. <em>APIN</em>,
<em>51</em>(8), 5280–5298. (<a
href="https://doi.org/10.1007/s10489-020-01977-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering method is a widely used and effective technique in color image segmentation. In general, traditional clustering-based image segmentation algorithms consider only one objective function and the segmentation performance is easily influenced by the noise in the image. Therefore, utilizing many clustering criteria and the neighborhood statistic information of pixels are more effective to improve the segmentation performance. In this paper, we put forward a multiobjective fuzzy clustering algorithm with multiple spatial information (MFCMSI) for noisy color image segmentation. Firstly, two conflicting fitness functions including the local spatial information with a circular neighborhood window and the non-local spatial information with circular search and similarity windows are designed to improve the noise-resistant ability and segmentation performance. In order to optimize these two fitness functions, the variable-length and cluster-center-based encoding strategy and some efficient evolutionary operations are utilized in each generation. Finally, a cluster validity index with multiple spatial information is constructed to select the best solution from the final non-dominated solution set of the last generation. Aiming to improve the robustness of MFCMSI, the ensemble strategy is introduced into MFCMSI and an ensemble version of MFCMSI is presented. Experimental results show that MFCMSI and its ensemble version behave well in evolving the number of segments and obtaining the satisfactory segmentation performance.},
  archive      = {J_APIN},
  author       = {Liu, Hanqiang and Zhao, Feng},
  doi          = {10.1007/s10489-020-01977-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5280-5298},
  shortjournal = {Appl. Intell.},
  title        = {Multiobjective fuzzy clustering with multiple spatial information for noisy color image segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Occluded object tracking using object-background prototypes
and particle filter. <em>APIN</em>, <em>51</em>(8), 5259–5279. (<a
href="https://doi.org/10.1007/s10489-020-02047-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking in a real-life scenario is very challenging due to occlusion. State-space models like Kalman and particle filters are well known to handle such a particular problem. The particle filter’s performance for solving such a problem depends on two issues - motion model and observation (i.e., likelihood) model. The question remains to exist due to the lack of useful observation and efficient motion models. This article presents an impressive observation model based on confidence (classification) score provided by introducing object-background prototypes based discriminative model. The proposed discriminative model is constructed with the prior knowledge of two classes (i.e., object and background) and tries to discriminate between three categories: an object, background, and occluded part of that object. The existing composite motion model handles the object motion and its scale. We also propose a model update technique that adapts the appearance changes of the object during tracking. We evaluate the proposed method on several challenging benchmark sequences. Analysis of the results concludes that the proposed technique can track fully (or partially) occluded object and the object in various complex environments.},
  archive      = {J_APIN},
  author       = {Mondal, Ajoy},
  doi          = {10.1007/s10489-020-02047-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5259-5279},
  shortjournal = {Appl. Intell.},
  title        = {Occluded object tracking using object-background prototypes and particle filter},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multiobjective multiperiod mean-semientropy-skewness model
for uncertain portfolio selection. <em>APIN</em>, <em>51</em>(8),
5233–5258. (<a
href="https://doi.org/10.1007/s10489-020-02079-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of the financial market, security returns are sometimes expressed by expert estimates rather than historical data. In this paper, we deal with a multiobjective multiperiod portfolio selection problem based on uncertainty theory. We propose a new uncertain multiobjective multiperiod mean-semisentropy-skewness portfolio optimization model, in which uncertain semi-entropy is used to quantify the downside risk. To be more realistic, several constraints are also considered, such as the transaction costs, cardinality, liquidity, budget, and bound constraint. Moreover, a novel hybrid technique, called the MFA-SOS algorithm, which combines the features of the firefly algorithm (FA) and symbiotic organism search algorithm (SOS) is designed to solve the proposed model. Finally, a numerical example is given to illustrate the effectiveness of the proposed approach.},
  archive      = {J_APIN},
  author       = {Lu, Shan and Zhang, Ning and Jia, Lifen},
  doi          = {10.1007/s10489-020-02079-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5233-5258},
  shortjournal = {Appl. Intell.},
  title        = {A multiobjective multiperiod mean-semientropy-skewness model for uncertain portfolio selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning domain invariant and specific representation for
cross-domain person re-identification. <em>APIN</em>, <em>51</em>(8),
5219–5232. (<a
href="https://doi.org/10.1007/s10489-020-02107-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) aims to match person images under different cameras with disjoint views. Although supervised re-ID has achieved great progress, unsupervised cross-domain re-ID remains a challenging work due to domain bias. In this work, we divide cross-domain re-ID task into two phases: domain-invariant features learning and domain-specific features learning. Our contributions are twofold. (i) To achieve domain-invariant features learning, a novel model called Pedestrian General Similarity (PGS) is proposed, which can eliminate two main factors that cause domain bias: image style and background. Compared with the existing re-ID models, PGS has better generalization ability. (ii) A novel pseudo label assignment method named Mutual Nearest Neighbors Pseudo Labeling (MNNPL) is proposed, which calculates pseudo labels based on the similarity between samples in the target domain, and the resulting pseudo labels are used to guide domain-specific feature learning. Extensive experiments are conducted on several large scale datasets, the results show that our method outperforms most published unsupervised cross-domain methods by a large margin.},
  archive      = {J_APIN},
  author       = {Chong, Yanwen and Peng, Chengwei and Zhang, Chen and Wang, Yujie and Feng, Wenqiang and Pan, Shaoming},
  doi          = {10.1007/s10489-020-02107-2},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {5219-5232},
  shortjournal = {Appl. Intell.},
  title        = {Learning domain invariant and specific representation for cross-domain person re-identification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DC-EDN: Densely connected encoder-decoder network with
reinforced depthwise convolution for face alignment. <em>APIN</em>,
<em>51</em>(7), 5025–5039. (<a
href="https://doi.org/10.1007/s10489-020-01940-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High accuracy and fast face alignment algorithms play an important role in many face-related applications. Generally, the model speed is inversely related to the number of parameters. We construct our network based on densely connected encoder-decoders, which is an efficient method to balance the parameter number and localization results. In each encoder-decoder, we introduce stacking depthwise convolution and depthwise feature fusion within the same channel, which greatly improves the performance of depthwise convolution and reduces the number of model parameters. In addition, we enhance the mean square loss function by assigning different penalty weights to each coordinate according to the distance to the position corresponding to the maximum value in the label heatmap. Experiments show that the model with the improved loss function obtains better localization results. In the experiment, we compare our method to state-of-the-art methods based on 300W and WFLW. The localization error is 2.76% with the common subset of 300W and the model size (0.7M) is small and even utilizes approximately 1% of the number of parameters of the other models. The dataset and model based on WFLW are publicly available at https://github.com/iam-zhanghongliang/DC-EDN .},
  archive      = {J_APIN},
  author       = {Yang, Lianping and Zhang, Hongliang and Wei, Panpan and Sun, Yubo and Zhang, Xiangde},
  doi          = {10.1007/s10489-020-01940-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {5025-5039},
  shortjournal = {Appl. Intell.},
  title        = {DC-EDN: Densely connected encoder-decoder network with reinforced depthwise convolution for face alignment},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment analysis of chinese stock reviews based on BERT
model. <em>APIN</em>, <em>51</em>(7), 5016–5024. (<a
href="https://doi.org/10.1007/s10489-020-02101-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of stock reviews are available on the Internet. Sentiment analysis of stock reviews has strong significance in research on the financial market. Due to the lack of a large amount of labeled data, it is difficult to improve the accuracy of Chinese stock sentiment classification using traditional methods. To address this challenge, in this paper, a novel sentiment analysis model for Chinese stock reviews based on BERT is proposed. This model relies on a pre-trained model to improve the accuracy of classification. The model use a BERT pre-training language model to perform representation of stock reviews on the sentence level, and subsequently feed the obtained feature vector into the classifier layer for classification. In the experiments, we demonstrate that our method has higher precision, recall, and F1 than TextCNN, TextRNN, Att-BLSTM and TextCRNN. Our model can obtain the best results which are indicated to be effective in Chinese stock review sentiment analysis. Meanwhile, Our model has powerful generalization capacity and can perform sentiment analysis in many fields.},
  archive      = {J_APIN},
  author       = {Li, Mingzheng and Chen, Lei and Zhao, Jing and Li, Qiang},
  doi          = {10.1007/s10489-020-02101-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {5016-5024},
  shortjournal = {Appl. Intell.},
  title        = {Sentiment analysis of chinese stock reviews based on BERT model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E-GCN: Graph convolution with estimated labels.
<em>APIN</em>, <em>51</em>(7), 5007–5015. (<a
href="https://doi.org/10.1007/s10489-020-02093-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {G raph C onvolutional N etwork (GCN) has been commonly applied for semi-supervised learning tasks. However, the established GCN frequently only considers the given labels in the topology optimization, which may not deliver the best performance for semi-supervised learning tasks. In this paper, we propose a novel G raph C onvolutional N etwork with E stimated labels (E-GCN) for semi-supervised learning. The core design of E-GCN is to learn a suitable network topology for semi-supervised learning by linking both estimated labels and given labels in a centralized network framework. The major enhancement is that both given labels and estimated labels are utilized for the topology optimization in E-GCN, which assists the graph convolution implementation for unknown labels evaluation. Experimental results demonstrate that E-GCN is significantly better than s tate-o f-t he-a rt (SOTA) baselines without estimated labels.},
  archive      = {J_APIN},
  author       = {Qin, Jisheng and Zeng, Xiaoqin and Wu, Shengli and Tang, E.},
  doi          = {10.1007/s10489-020-02093-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {5007-5015},
  shortjournal = {Appl. Intell.},
  title        = {E-GCN: Graph convolution with estimated labels},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing the competitive swarm optimizer with covariance
matrix adaptation for large scale optimization. <em>APIN</em>,
<em>51</em>(7), 4984–5006. (<a
href="https://doi.org/10.1007/s10489-020-02078-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive swarm optimizer (CSO) has been shown to be an effective optimization algorithm for large scale optimization. However, the learning strategy of a loser particle used in CSO is axis-parallel. Then, it may not be able to solve the high ill-conditioned test functions due to the lack of considering the correlation of different component. This paper presents an enhanced competitive swarm optimizer with covariance matrix adaptation to alleviate this problem. Since covariance matrix is independent of the coordinate system, covariance matrix adaptation evolution strategy (CMA-ES) is embedded into CSO. On the one hand, better particles generated by CMA-ES can provide an effective way to capture the efficient search direction. On the other hand, some high-quality particles are employed to estimate the covariance matrix of Gaussian model. Then, the evolution direction information is integrated into the learned Gaussian model to improve search efficiency of the proposed algorithm. Experimental and statistical analyses are performed on CEC2014 benchmark functions, engineering design problems and time series prediction problems. Results show that the proposed algorithm has a superior performance in comparison with other state-of-the-art optimization algorithms and some variants of CSO.},
  archive      = {J_APIN},
  author       = {Li, Wei and Lei, Zhou and Yuan, Junqing and Luo, Haonan and Xu, Qingzheng},
  doi          = {10.1007/s10489-020-02078-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4984-5006},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing the competitive swarm optimizer with covariance matrix adaptation for large scale optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An elite-guided hierarchical differential evolution
algorithm. <em>APIN</em>, <em>51</em>(7), 4962–4983. (<a
href="https://doi.org/10.1007/s10489-020-02091-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population structure has an impact on the performance of metaheuristic algorithms. To better improve the performance of differential evolution (DE), an elite-guided hierarchical differential evolution algorithm (EHDE) is proposed. First, an elite-guided hierarchical mutation mechanism is presented, which integrates elite elements into the hierarchical population structure. During each generation, the population is divided into three groups according to fitness values, each group playing a unique role in its hierarchy. The best individual on the top layer is used to avoid the local optimal by random reinitialization or Lévy flight. The (k − 1) elite individuals on the middle layer focus on the local search around the best individual. The remaining non-elite individuals on the bottom layer pay more attention to a more considerable range search by the guidance of the k elite individuals. Second, to accommodate diverse optimization problems and seek the balance between exploration and exploitation, the adaptive strategy of EHDE control parameters has added the random component and the time-varying component. Finally, for the sake of evaluating the performance of EHDE, sensitivity analysis to the size of elite individuals, efficiency analysis of the control parameters adaptive strategy, and comparisons with nine advanced DE variants and three non-DE algorithms on 29 universal benchmark function in terms of convergence accuracy and convergence speed have been taken out. All the obtained results show that the proposed EHDE has excellent optimization performance.},
  archive      = {J_APIN},
  author       = {Zhong, Xuxu and Cheng, Peng},
  doi          = {10.1007/s10489-020-02091-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4962-4983},
  shortjournal = {Appl. Intell.},
  title        = {An elite-guided hierarchical differential evolution algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic fabric defect detection using a wide-and-light
network. <em>APIN</em>, <em>51</em>(7), 4945–4961. (<a
href="https://doi.org/10.1007/s10489-020-02084-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic fabric defect detection systems improve the quality of textile production across the industry. To make these automatic systems accessible to smaller businesses, one potential solution is to use limited memory capacity chips that can be used with hardware platforms with limited resources. That is to say, the fabric defect detection algorithm must ensure high detection accuracy while maintaining a low computational cost. Therefore, we propose a wide-and-light network structure based on Faster R-CNN for detecting common fabric defects. We enhance the feature extraction capability of the feature extraction network by designing a dilated convolution module. In a dilated convolution module, a multi-scale convolution kernel is used to adapt to defects of different sizes. Dilated convolutions can increase receptive fields without increasing the number of parameters used. Therefore, we replace a subset of ordinary convolutions with dilated convolutions to learn target features and use convolution kernel decomposition and bottleneck methods to simplify the feature extraction networks. Then, high-level semantic features are fused with bottom-level detail features (via skip-connection) to obtain multi-scale fusion features. Finally, a series of anchor frames (of different sizes) is designed to suit multi-scale fabric defect detection. Experiments show that compared with various mainstream target detection algorithms, our proposed algorithm can improve the accuracy of fabric defect detection and reduce the size of the model.},
  archive      = {J_APIN},
  author       = {Wu, Jun and Le, Juan and Xiao, Zhitao and Zhang, Fang and Geng, Lei and Liu, Yanbei and Wang, Wen},
  doi          = {10.1007/s10489-020-02084-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4945-4961},
  shortjournal = {Appl. Intell.},
  title        = {Automatic fabric defect detection using a wide-and-light network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TBTF: An effective time-varying bias tensor factorization
algorithm for recommender system. <em>APIN</em>, <em>51</em>(7),
4933–4944. (<a
href="https://doi.org/10.1007/s10489-020-02035-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context-aware processing is a research hotspot in the recommendation area, which achieves better recommendation accuracy by considering more context information such as time, location and etc. besides the information of the users, items and ratings. Tensor factorization is an effective algorithm in context-aware recommendation and current approaches show that adding bias to the tensor factorization model can improve the accuracy. However, users’ rating preferences fluctuate greatly over time, which makes bias fluctuate with time too. Current context-aware recommendation algorithms ignore this problem, and usually use the same bias for a user or an item in different time. Aiming at this problem, this paper first considers the time-varying effect on user bias and item bias in context-aware recommendation, and proposes a time-varying bias tensor factorization recommendation algorithm based on the bias tensor factorization model (BiasTF). We experiment on two real datasets, and the experimental results show that the proposed algorithms get better accuracy than other algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Jianli and Yang, Shangcheng and Huo, Huan and Sun, Qiuxia and Geng, Xijiao},
  doi          = {10.1007/s10489-020-02035-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4933-4944},
  shortjournal = {Appl. Intell.},
  title        = {TBTF: An effective time-varying bias tensor factorization algorithm for recommender system},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-sensitive probability for weighted voting in an
ensemble model for multi-class classification problems. <em>APIN</em>,
<em>51</em>(7), 4908–4932. (<a
href="https://doi.org/10.1007/s10489-020-02106-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning is an algorithm that utilizes various types of classification models. This algorithm can enhance the prediction efficiency of component models. However, the efficiency of combining models typically depends on the diversity and accuracy of the predicted results of ensemble models. However, the problem of multi-class data is still encountered. In the proposed approach, cost-sensitive learning was implemented to evaluate the prediction accuracy for each class, which was used to construct a cost-sensitivity matrix of the true positive (TP) rate. This TP rate can be used as a weight value and combined with a probability value to drive ensemble learning for a specified class. We proposed an ensemble model, which was a type of heterogenous model, namely, a combination of various individual classification models (support vector machine, Bayes, K-nearest neighbour, naïve Bayes, decision tree, and multi-layer perceptron) in experiments on 3-, 4-, 5- and 6-classifier models. The efficiencies of the propose models were compared to those of the individual classifier model and homogenous models (Adaboost, bagging, stacking, voting, random forest, and random subspaces) with various multi-class data sets. The experimental results demonstrate that the cost-sensitive probability for the weighted voting ensemble model that was derived from 3 models provided the most accurate results for the dataset in multi-class prediction. The objective of this study was to increase the efficiency of predicting classification results in multi-class classification tasks and to improve the classification results.},
  archive      = {J_APIN},
  author       = {Rojarath, Artittayapron and Songpan, Wararat},
  doi          = {10.1007/s10489-020-02106-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4908-4932},
  shortjournal = {Appl. Intell.},
  title        = {Cost-sensitive probability for weighted voting in an ensemble model for multi-class classification problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive diagnosis of DC motors using r-WDCNN classifiers
based on VMD-SVD. <em>APIN</em>, <em>51</em>(7), 4888–4907. (<a
href="https://doi.org/10.1007/s10489-020-02087-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fault diagnosis methods of DC (direct current) motors require high expertise and human labor. However, the other disadvantages of these methods are low efficiency and poor accuracy. To address these problems, a new adaptive and intelligent mechanical fault diagnosis method for DC motors based on variational mode decomposition (VMD), singular value decomposition (SVD), and residual deep convolutional neural networks with wide first-layer kernels (R-WDCNN) was proposed. First, the vibration signals of a DC motor were collected by a designed acquisition system. Subsequently, VMD was employed to decompose the raw signals adaptively into several intrinsic mode functions (IMFs). Moreover, the transient frequency means method, which can quickly and accurately obtain the optimal value of K, is proposed. SVD was applied to reduce the dimensionality of the IMF matrix for further feature extraction. Finally, the reconstructed matrix containing the main fault feature information was used to train and test the R-WDCNN. Based on residual learning, identification and classification of four types of vibration signals were achieved, while the R-WDCNN was optimized by the adaptive batch normalization algorithm (AdaBN). The recognition rate and the convergence were improved by this classifier. The results show that the method proposed in this paper has better adaptability and intelligence than other methods, and the R-WDCNN can reach a 94% recognition rate on unknown samples. Therefore, the proposed method is more intelligent and accurate than other methods.},
  archive      = {J_APIN},
  author       = {Qin, Huabin and Liu, Mingliang and Wang, Jian and Guo, Zijian and Liu, Junbo},
  doi          = {10.1007/s10489-020-02087-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4888-4907},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive diagnosis of DC motors using R-WDCNN classifiers based on VMD-SVD},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anomaly detection via a combination model in time series
data. <em>APIN</em>, <em>51</em>(7), 4874–4887. (<a
href="https://doi.org/10.1007/s10489-020-02041-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the time series data have the characteristics of a large amount of data and non-stationarity, we usually cannot obtain a satisfactory result by a single-model-based method to detect anomalies in time series data. To overcome this problem, in this paper, a combination-model-based approach is proposed by combining a similarity-measurement-based method and a model-based method for anomaly detection. First, the process of data representation is performed to generate a new data form to arrive at the purpose of reducing data volume. Furthermore, due to the anomalies being generally caused by changes in amplitude and shape, we take both the original time series data and their amplitude change data into consideration of the process of data representation to capture the shape and morphological features. Then, the results of data representation are employed to establish a model for anomaly detection. Compared with the state-of-the-art methods, experimental studies on a large number of datasets show that the proposed method can significantly improve the performance of anomaly detection with higher data anomaly resolution.},
  archive      = {J_APIN},
  author       = {Zhou, Yanjun and Ren, Huorong and Li, Zhiwu and Wu, Naiqi and Al-Ahmari, Abdulrahman M.},
  doi          = {10.1007/s10489-020-02041-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4874-4887},
  shortjournal = {Appl. Intell.},
  title        = {Anomaly detection via a combination model in time series data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weighted intrusion detection model of dynamic selection.
<em>APIN</em>, <em>51</em>(7), 4860–4873. (<a
href="https://doi.org/10.1007/s10489-020-02090-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the difficulty of existing intrusion detection methods in dealing with new forms, large scale, and high concealment of network intrusion behaviors, this paper presents a weighted intrusion detection model of the dynamic selection (WIDMoDS) based on data features. The aim is to customize intrusion detection models for network intrusion data sets of different types, sizes and structures. First, according to data features, single classifiers are clustered using a hierarchical clustering algorithm based on the classifiers evaluation indicators, and then, the classifiers selection is by means of accuracy of the single classifiers, in addition, the data-classifier applicable indicators (DCAI) and of the classifiers performances are used for calculating the weights of subjective and objective, and then calculating combined weight ranks. Finally, a custom intrusion detection model is generated by the Weight-voting (W-voting) algorithm. Our experiments show that this model can optimize the number of classifiers based on the data sets features, reduce the problem of redundant or insufficient classifiers in the ensemble process. A new network intrusion detection model of combining the classifier characteristics with the dataset attributes can improve the accuracy of intrusion detection.},
  archive      = {J_APIN},
  author       = {Feng, Tao and Dou, Manfang},
  doi          = {10.1007/s10489-020-02090-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4860-4873},
  shortjournal = {Appl. Intell.},
  title        = {A weighted intrusion detection model of dynamic selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel binary farmland fertility algorithm for feature
selection in analysis of the text psychology. <em>APIN</em>,
<em>51</em>(7), 4824–4859. (<a
href="https://doi.org/10.1007/s10489-020-02038-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a key role in data mining and machine learning algorithms to reduce the processing time and increase the accuracy of classification of high dimensional datasets. One of the most common feature selection methods is the wrapper method that works on the feature set to reduce the number of features while improving the accuracy of the classification. In this paper, two different wrapper feature selection approaches are proposed based on Farmland Fertility Algorithm (FFA). Two binary versions of the FFA algorithm are proposed, denoted as BFFAS and BFFAG. The first version is based on the sigmoid function. In the second version, new operators called Binary Global Memory Update (BGMU) and Binary Local Memory Update (BLMU) and a dynamic mutation (DM) operator are used for binarization. Furthermore, the new approach (BFFAG) reduces the three parameters of the base algorithm (FFA) that are dynamically adjusted to maintain exploration and efficiency. Two proposed approaches have been compared with the basic meta-heuristic algorithms used in feature selection on 18 standard datasets. The results show better performance of the proposed approaches compared with the competing methods in terms of objective function value, the average number of selected features, and the classification accuracy. Also, the experiments on the emotion analysis dataset demonstrate the satisfactory results.},
  archive      = {J_APIN},
  author       = {Hosseinalipour, Ali and Gharehchopogh, Farhad Soleimanian and Masdari, Mohammad and Khademi, Ali},
  doi          = {10.1007/s10489-020-02038-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4824-4859},
  shortjournal = {Appl. Intell.},
  title        = {A novel binary farmland fertility algorithm for feature selection in analysis of the text psychology},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linguistic frequent pattern mining using a compressed
structure. <em>APIN</em>, <em>51</em>(7), 4806–4823. (<a
href="https://doi.org/10.1007/s10489-020-02080-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional association-rule mining (ARM) considers only the frequency of items in a binary database, which provides insufficient knowledge for making efficient decisions and strategies. The mining of useful information from quantitative databases is not a trivial task compared to conventional algorithms in ARM. Fuzzy-set theory was invented to represent a more valuable form of knowledge for human reasoning, which can also be applied and utilized for quantitative databases. Many approaches have adopted fuzzy-set theory to transform the quantitative value into linguistic terms with its corresponding degree based on defined membership functions for the discovery of FFIs, also known as fuzzy frequent itemsets. Only linguistic terms with maximal scalar cardinality are considered in traditional fuzzy frequent itemset mining, but the uncertainty factor is not involved in past approaches. In this paper, an efficient fuzzy mining (EFM) algorithm is presented to quickly discover multiple FFIs from quantitative databases under type-2 fuzzy-set theory. A compressed fuzzy-list (CFL)-structure is developed to maintain complete information for rule generation. Two pruning techniques are developed for reducing the search space and speeding up the mining process. Several experiments are carried out to verify the efficiency and effectiveness of the designed approach in terms of runtime, the number of examined nodes, memory usage, and scalability under different minimum support thresholds and different linguistic terms used in the membership functions.},
  archive      = {J_APIN},
  author       = {Lin, Jerry Chun-Wei and Ahmed, Usman and Srivastava, Gautam and Wu, Jimmy Ming-Tai and Hong, Tzung-Pei and Djenouri, Youcef},
  doi          = {10.1007/s10489-020-02080-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4806-4823},
  shortjournal = {Appl. Intell.},
  title        = {Linguistic frequent pattern mining using a compressed structure},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel social network community discovery method combined
local distance with node rank optimization function. <em>APIN</em>,
<em>51</em>(7), 4788–4805. (<a
href="https://doi.org/10.1007/s10489-020-02040-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of that most of the current community discovery methods in social network do not consider node self-transfer and node bias, so that it is not possible to extract the graph features effectively, which leads to the ineffective problem by the community discovery, this paper proposes a novel social network community discovery algorithm (Local Distance Laplace, LDL). First, a Laplace matrix decomposition model is constructed based on the principle of matrix decomposition. Secondly, considering the high cost of global social network information acquisition and calculation, a community discovery model based on local distance is proposed. Finally, the optimal community structure is selected by using the NRO (Node Rank Optimization) function. A comprehensive comparative analysis is made on eleven real and synthetic networks. At the same time, validation analysis is conducted on eleven different social networks (Karate, Dolphins, Lemis, Public book, Football, Celegansnertal, Email, Public blogs, Netscience, Power, Hep_th). The experimental simulation results show that: in the real network, the proposed LDL algorithm improved the overall performance by nearly 7% compared with the seven state of art optimal methods (CoVeC, EDBC, JNMF, EADP, LPANNI, LSA, SCFS). The novel algorithm is reasonable and effective and it can also be extended to multi-scale community discovery.},
  archive      = {J_APIN},
  author       = {Liu, Xiaoyang and Ding, Nan and Liu, Chao and Zhang, Yihao and Tang, Ting},
  doi          = {10.1007/s10489-020-02040-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4788-4805},
  shortjournal = {Appl. Intell.},
  title        = {Novel social network community discovery method combined local distance with node rank optimization function},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of rumor conversations in twitter using graph
convolutional networks. <em>APIN</em>, <em>51</em>(7), 4774–4787. (<a
href="https://doi.org/10.1007/s10489-020-02036-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of the social network Twitter and its use to propagate information, it is of vital importance to detect rumors prior to their dissemination on Twitter. In the present paper, a model to detect rumor conversations is proposed using graph convolutional networks. A reply tree and user graph were extracted for each conversation. The reply trees were created according to the source tweet and the reply tweets. By modeling this graph on graph convolutional networks, structural information of the graph and the contents of conversation tweets were obtained. The user graphs were created based on the users participating in the conversation and the tweets exchanged among them. Information regarding the users and how they interacted in the conversations were obtained through modeling this graph on the graph convolutional networks. The outputs of the two above-mentioned modules were combined to detect the rumor. Experimental results on the public dataset show that the proposed method has a better performance than baseline methods.},
  archive      = {J_APIN},
  author       = {Lotfi, Serveh and Mirzarezaee, Mitra and Hosseinzadeh, Mehdi and Seydi, Vahid},
  doi          = {10.1007/s10489-020-02036-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4774-4787},
  shortjournal = {Appl. Intell.},
  title        = {Detection of rumor conversations in twitter using graph convolutional networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning-based consensus decision-making support for
crowd-scale deliberation. <em>APIN</em>, <em>51</em>(7), 4762–4773. (<a
href="https://doi.org/10.1007/s10489-020-02118-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet, the online discussion system or social democratic system has become an important and effective vehicle for group decision-making support since it can continue collecting the opinions from the public at anytime. To reach a consensus in crowd-scale deliberation, the existing online discussion systems require an experienced human facilitator to navigate and guild the discussion. When human facilitator performs the required facilitation there are several issues such as heavy burden on decision-making, the 24/7 online facilitation, bias on the social issues, etc. To address these issues it is necessary and inevitable to explore intelligent facilitation. For this purpose, we propose a novel machine learning-based method for smart facilitation, in particular the intelligent consensus decision-making support (CDMS) for crowd-scale deliberation. After presenting an overview of the crowd-scale deliberation and the COLLAGREE, the paper details the proposed approach, a machine learning-based framework for CDMS in crowd-scale deliberation. To validate the developed methods the offline evaluation experiments were conducted with the online discussion platform, COLLAGREE. The preliminary experimental results obtained from offline validation demonstrated the feasibility and usefulness of the developed machine learning-based methods for CDMS.},
  archive      = {J_APIN},
  author       = {Yang, Chunsheng and Gu, Wen and Ito, Takayuki and Yang, Xiaohua},
  doi          = {10.1007/s10489-020-02118-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4762-4773},
  shortjournal = {Appl. Intell.},
  title        = {Machine learning-based consensus decision-making support for crowd-scale deliberation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdaDT: An adaptive decision tree for addressing local class
imbalance based on multiple split criteria. <em>APIN</em>,
<em>51</em>(7), 4744–4761. (<a
href="https://doi.org/10.1007/s10489-020-02061-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As it is well known, decision tree is a kind of data-driven classification model, and its primary core is the split criterion. Although a great deal of split criteria have been proposed so far, almost all of them focus on the global class distribution of the training data. However, they ignored the local class imbalance problem that commonly appears during the decision tree induction over balanced or roughly balanced binary class data sets. In the present study, this problem is investigated in detail and an adaptive approach based on multiple existing split criteria is proposed. In the proposed scheme, the local class imbalanced ratio is considered as the weight factor to weigh the importance between these split criteria so as to determine the optimal splitting point at each internal node. In order to evaluate the effectiveness of the proposed method, it is applied on twenty roughly balanced real-world binary class data sets. Experimental results show that the proposed method not only outperforms all other methods, but also improves the prediction accuracy of each class.},
  archive      = {J_APIN},
  author       = {Yan, Jianjian and Zhang, Zhongnan and Dong, Huailin},
  doi          = {10.1007/s10489-020-02061-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4744-4761},
  shortjournal = {Appl. Intell.},
  title        = {AdaDT: An adaptive decision tree for addressing local class imbalance based on multiple split criteria},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GRU-based capsule network with an improved loss for
personnel performance prediction. <em>APIN</em>, <em>51</em>(7),
4730–4743. (<a
href="https://doi.org/10.1007/s10489-020-02039-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personnel performance is a key factor to maintain core competitive advantages. Thus, predicting personnel future performance is a significant research domain in human resource management (HRM). In this paper, to improve the performance, we propose a novel method for personnel performance prediction which helps decision-makers select high-potential talents. Specifically, for modeling the personnel performance, we first devise a GRU model to learn sequential information from personnel performance data without any expertise. Then, to better cluster the features, we exploit capsule network. Finally, to precisely make predictions, we further design one strategy, i.e., an improved loss function, and embed it into the capsule network. In addition, by introducing this strategy, our proposed model can well deal with the imbalanced data problem. Extensive experiments on real-world data clearly demonstrate the effectiveness of the proposed approach.},
  archive      = {J_APIN},
  author       = {Xue, Xia and Gao, Yi and Liu, Meng and Sun, Xia and Zhang, Wenyu and Feng, Jun},
  doi          = {10.1007/s10489-020-02039-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4730-4743},
  shortjournal = {Appl. Intell.},
  title        = {GRU-based capsule network with an improved loss for personnel performance prediction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inception single shot multi-box detector with affinity
propagation clustering and their application in multi-class vehicle
counting. <em>APIN</em>, <em>51</em>(7), 4714–4729. (<a
href="https://doi.org/10.1007/s10489-020-02127-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-class vehicle detection and counting in video-based traffic surveillance systems with real-time performance and acceptable precision are challenging. This paper proposes a modified single shot multi-box convolutional neural network named Inception-SSD (ISSD) for vehicle detection and a centroid matching algorithm for vehicle counting. An Inception-like block is introduced to replace the extra feature layers in the original SSD to deal with the multi-scale vehicle detection to enhance smaller vehicles’ detection. Non-Maximum Suppression (NMS) is replaced with Affinity Propagation Clustering (APC) to improve the detection of nearby occluded vehicles. For a 300 × 300 input image, on PASCAL VOC 2007 test data set, the proposed ISSD achieved 79.3 mean Average Precision (mAP) and ran on an NVIDIA RTX2080Ti; the network attains a speed of 52.3 frames per second. ISSD with APC generates 2.7% improvement in mAP over original SSD300 while almost retaining its time efficiency. By centroid matching algorithm, the vehicles are counted class-wise with a weighted F1 of 98.5%, which is quite superior to the other recent existing research works.},
  archive      = {J_APIN},
  author       = {Harikrishnan, P. M. and Thomas, Anju and Gopi, Varun P. and Palanisamy, P. and Wahid, Khan A.},
  doi          = {10.1007/s10489-020-02127-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4714-4729},
  shortjournal = {Appl. Intell.},
  title        = {Inception single shot multi-box detector with affinity propagation clustering and their application in multi-class vehicle counting},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A high speed roller dung beetles clustering algorithm and
its architecture for real-time image segmentation. <em>APIN</em>,
<em>51</em>(7), 4682–4713. (<a
href="https://doi.org/10.1007/s10489-020-02067-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several practical applications like disaster detection, remote surveillance, object recognition using remote sensing satellite images, object monitoring and tracking using radar images etc. essentially require real-time image segmentation. In these applications computational complexity of the algorithm play a vital role along with accuracy. In this work image segmentation is dealt as a clustering problem and a bio-inspired algorithm based on the behavior of ‘Roller Dung Beetles (RDB)’ is proposed to determine effective solutions. The beauty of this proposed RDB Clustering architecture is its lower computational complexity O(N). The software implementation of the proposed algorithm is carried out in MATLAB environment and a hardware architecture is developed in Verilog HDL using Modelsim, Xilinx ISE for FPGA environment. The architecture has a comparison free sorting module, two data storage modules and a parallel threshold comparator unit, all of which use fewer mathematical operations. The performance of the proposed architecture is validated on many synthetic and standard benchmark color images. Further application of the proposed architecture is carried out for real-time segmentation of 8 NASA LANDSAT / ESA satellite images. Performance comparison has been carried out with other existing architectures based on Artificial Immune System (AIS), Genetic, K-means clustering, CNN etc. Simulation results reveal that the proposed RDBC architecture is 42% faster than the K-Means implementation with a clock frequency of 230.52MHz with an increased PSNR of 6.825% and SSIM of 15.55%. Statistical analysis and silhouette index also confirms the superiority of new clustering architecture over existing implementations.Compared to CNN the RDBC architecture is economical both in terms of lower chip-area and power consumption.},
  archive      = {J_APIN},
  author       = {Ratnakumar, Rahul and Nanda, Satyasai Jagannath},
  doi          = {10.1007/s10489-020-02067-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4682-4713},
  shortjournal = {Appl. Intell.},
  title        = {A high speed roller dung beetles clustering algorithm and its architecture for real-time image segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bio-inspired self-organized cooperative control consensus
for crowded UUV swarm based on adaptive dynamic interaction topology.
<em>APIN</em>, <em>51</em>(7), 4664–4681. (<a
href="https://doi.org/10.1007/s10489-020-02104-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative control is currently a challenging topic of crowded unmanned underwater vehicle (UUV) swarm. However, individual behavior conflict and chain-avalanche collision involved in this swarm are easily triggered due to the fluctuations and disturbances. In order to address the two problems, a bio-inspired self-organized cooperative control consensus derived from adaptive dynamic interaction topology is investigated in this paper. Firstly, a novel following-interaction framework incorporating the topological interaction and visual interaction is devised to ensure the minimum number and optimal distribution for neighborhoods. Then, an adaptive dynamic computing model inspired by single-nearest-neighbor following and weighted- multiple-nearest-neighbors following is proposed to steer a sensitive following behavior, in which the influence of each individual on this following behavior is described by a nonlinear weight. Finally, a distributed control protocol is put forward by using the proposed following model and mathematics-based potential fields to achieve the cohesive flocking and avoiding collision, and its sufficient conditions is proven by Laypunov and LaSalle invariance principle to accomplish a self- organized cooperative control. Simulation results are presented for illustrating the feasibility and effectiveness of our proposed control approach.},
  archive      = {J_APIN},
  author       = {Liang, Hongtao and Fu, Yanfang and Gao, Jie},
  doi          = {10.1007/s10489-020-02104-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4664-4681},
  shortjournal = {Appl. Intell.},
  title        = {Bio-inspired self-organized cooperative control consensus for crowded UUV swarm based on adaptive dynamic interaction topology},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-utility and diverse itemset mining. <em>APIN</em>,
<em>51</em>(7), 4649–4663. (<a
href="https://doi.org/10.1007/s10489-020-02063-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility Itemset Mining (HUIM) finds patterns from a transaction database with their utility no less than a user-defined threshold. The utility of an itemset is defined as the sum of the utilities of its items. The utility notion enables a data analyst to associate a profit score with each item and thereof to a pattern. We extend the notion of high-utility with diversity to define a new pattern type called High-utility and Diverse pattern (HUD). The notion of diversity of a pattern captures the extent of the different categories covered by the selected items in the pattern. An application of diverse-pattern lies in the recommendation task where a system can recommend to a customer a set of items from a new class based on her previously bought items. Our notion of diversity is easy to compute and also captures the basic essence of a previously proposed diversity notion. The existing algorithm to compute frequent-diverse patterns is 2-phase, i.e., in the first phase, frequent patterns are computed, out of which diverse patterns are filtered out in the second phase. We, in this paper, give an integrated algorithm that efficiently computes high-utility and diverse patterns in a single phase. Our experimental study shows that our proposed algorithm is very efficient as compared to a 2-phase algorithm that extracts high-utility itemsets in the first phase and filters out the diverse itemsets in the second phase.},
  archive      = {J_APIN},
  author       = {Verma, Amit and Dawar, Siddharth and Kumar, Raman and Navathe, Shamkant and Goyal, Vikram},
  doi          = {10.1007/s10489-020-02063-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4649-4663},
  shortjournal = {Appl. Intell.},
  title        = {High-utility and diverse itemset mining},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BiTE: A dynamic bi-level traffic engineering model for load
balancing and energy efficiency in data center networks. <em>APIN</em>,
<em>51</em>(7), 4623–4648. (<a
href="https://doi.org/10.1007/s10489-020-02003-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent significant growth of virtualization and cloud services, the data center network (DCN) as the underlying infrastructure is more important. The increasing and changing volume of workloads highlights critical issues such as load balancing and energy efficiency in data centers. Large path diversity in DCNs introduces multipath forwarding as a promising approach to improve load distribution. However, the over-provisioned DCNs consume large amounts of power while the network is under full capacity most of the time. Accordingly, this paper proposes BiTE, a dynamic bi-level traffic engineering (TE) scheme in a hierarchical Software Defined Networking (SDN)-based DCN to strike a balance between load balancing and energy efficiency objectives. BiTE consists of decision-making problems at two levels modeled as a multi-period bi-level optimization problem, where each decision maker optimizes one of objectives. According to the inherent complexity of bi-level programming, a co-evolutionary metaheuristic algorithm is proposed for solving BiTE. BiTE performance is evaluated in comparison to NSGA-II algorithm and four previously proposed TE schemes in terms of several load balancing and energy saving metrics under different scenarios. The results show that BiTE performs well in traffic load balancing while preserves the energy efficiency. We apply the Analytic Hierarchy Process (AHP) method to multi-criteria analyze and rank the performance of studied TE mechanisms. AHP results for different scenarios indicate that BiTE is in first or second place in terms of the overall performance score among six studied approaches.},
  archive      = {J_APIN},
  author       = {Rikhtegar, Negar and Keshtgari, Manijeh and Bushehrian, Omid and Pujolle, Guy},
  doi          = {10.1007/s10489-020-02003-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4623-4648},
  shortjournal = {Appl. Intell.},
  title        = {BiTE: A dynamic bi-level traffic engineering model for load balancing and energy efficiency in data center networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-parallel text style transfer with domain adaptation and
an attention model. <em>APIN</em>, <em>51</em>(7), 4609–4622. (<a
href="https://doi.org/10.1007/s10489-020-02077-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text style transfer, the aim of which is to convert a specific style in a given sentence to another target style while maintaining the style-independent content information of the original sentence, can face challenges when applied to non-parallel text. In this paper, we combine domain adaptation learning and an attention model to propose a new framework to accomplish the task. Domain adaptation can leverage relative information from the source domain to improve the generative model’s capacity for reconstructing data. The attention model can give the importance weights of generated words for the target style in a sentence; therefore, the generative model can concentrate on generating words with higher importance weights to accomplish text style transfer effectively. We evaluate our framework using Yelp, Amazon and Captions corpora. The results of automatic and human evaluation demonstrate the effectiveness of our framework compared with previous works under non-parallel and limited training data. The available codes are in https://github.com/mingxuan007/text-style-transfer-with-adversarial-network-and-domain-adaptation .},
  archive      = {J_APIN},
  author       = {Hu, Mingxuan and He, Min},
  doi          = {10.1007/s10489-020-02077-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4609-4622},
  shortjournal = {Appl. Intell.},
  title        = {Non-parallel text style transfer with domain adaptation and an attention model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-based approaches for eliminating premature
convergence in particle swarm optimization. <em>APIN</em>,
<em>51</em>(7), 4575–4608. (<a
href="https://doi.org/10.1007/s10489-020-02045-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Swarm Optimization (PSO) is a computational method in which a group of particles moves in search space in search of an optimal solution. During this movement, each particle updates its position and velocity with its best previous position and best position found by the swarm. Though PSO is considered as a potential solution and applied in many areas, it suffers from premature convergence in which all the particles are converged too early, resulting in sub-optimal results. Although there are several techniques to address premature convergence, achieving a higher convergence rate while avoiding premature convergence is still challenging. In this paper, we present two new memory-based variants of PSO for preventing premature convergence. The first technique (PSOMR), augments memory by leveraging the concepts of the Ebbinghaus forgetting curve. The second technique (MS-PSOMR) divides swarm into multiple subswarms. Both techniques use memory to store promising historical values and use them later to avoid premature convergence. The proposed approaches are compared with existing algorithms belonging to a similar category and evaluations on CEC 2010 and CEC 2017 benchmark functions. The results show that both the approaches performed significantly better for the measured metrics and discouraged premature convergence.},
  archive      = {J_APIN},
  author       = {Chaitanya, K. and Somayajulu, D. V. L. N and Krishna, P. Radha},
  doi          = {10.1007/s10489-020-02045-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4575-4608},
  shortjournal = {Appl. Intell.},
  title        = {Memory-based approaches for eliminating premature convergence in particle swarm optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MXQN: Mixed quantization for reducing bit-width of weights
and activations in deep convolutional neural networks. <em>APIN</em>,
<em>51</em>(7), 4561–4574. (<a
href="https://doi.org/10.1007/s10489-020-02109-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization, which involves bit-width reduction, is considered as one of the most effective approaches to rapidly and energy-efficiently deploy deep convolutional neural networks (DCNNs) on resource-constrained embedded hardware. However, bit-width reduction on the weights and activations of DCNNs seriously degrades accuracy. To solve this problem, in this paper we propose a mixed hardware-friendly quantization (MXQN) method that applies fixed-point quantization and logarithmic quantization for DCNNs without the necessity to retrain and fine-tune the DCNN. Our MXQN algorithm is a multi-staged process where, first, we employ a signal-to-quantization-noise ratio (SQNR) process as the metric to estimate the interplay between the parameter quantization errors of each layer and the overall model prediction accuracy. Then, we utilize a fixed-point quantization process to quantize weights, and depending on the SQNR metric we empirically select either a logarithmic or a fixed-point quantization process to quantize activations. For improved accuracy, we propose an optimized logarithmic quantization scheme that affords a fine-grained step size. We evaluate the performance of MXQN utilizing the VGG16 network on the MNIST, CIFAR-10, CIFAR-100, and the ImageNet datasets, as well as VGG19 and ResNet (ResNet18, ResNet34, ResNet50) networks on the ImageNet, and demonstrate that the MXQN-quantized DCNN despite not being retrained and fine-tuned, it still achieves high accuracy close to the original DCNN.},
  archive      = {J_APIN},
  author       = {Huang, Chenglong and Liu, Puguang and Fang, Liang},
  doi          = {10.1007/s10489-020-02109-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4561-4574},
  shortjournal = {Appl. Intell.},
  title        = {MXQN: Mixed quantization for reducing bit-width of weights and activations in deep convolutional neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An RBF-LVQPNN model and its application to time-varying
signal classification. <em>APIN</em>, <em>51</em>(7), 4548–4560. (<a
href="https://doi.org/10.1007/s10489-020-02094-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel technique is proposed for maintaining the diversity of sample features and modeling imbalanced datasets in multi-channel time-varying signal classification. The RBF-LVQPNN consists of a time-varying signal input layer, an RBF process neuron hidden layer, an LVQ competition layer, a pattern layer, and a classifier. Dynamic clustering was used to divide samples in each pattern class into several pattern subclasses with similar features. Typical signal samples in the pattern subclasses are then used as the kernel centers for RBFPNs, to achieve embedding of the diversity signal class features. The output of the RBFPN layer was used as input to the LVQN. In the competition layer, the ‘winning’ neurons were selected to represent a pattern subclass. In the process of reorganizing pattern subclasses into pattern classes, subclass boundaries were combined into irregular class boundaries to reduce the overlap of decision spaces for different pattern classes. The RBF-LVQNN could improve the memory ability of typical signal features and discriminability of signals, realize structural and data constraints of the model, and improve the modeling property of imbalanced datasets. In this paper, the properties of the RBF-LVQPNN are analyzed and a comprehensive training algorithm is established. Experimental validation was performed with classification diagnoses from seven types of cardiovascular diseases based on 12-lead ECG signals. Results demonstrated that the proposed technique significantly improved both classification accuracy and generalizability comparing with other methods in the experiment.},
  archive      = {J_APIN},
  author       = {Wu, Lu and Wang, Yinglong and Xu, Shaohua and Liu, Kun and Li, Xuegui},
  doi          = {10.1007/s10489-020-02094-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4548-4560},
  shortjournal = {Appl. Intell.},
  title        = {An RBF-LVQPNN model and its application to time-varying signal classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast detector generation algorithm for negative selection.
<em>APIN</em>, <em>51</em>(7), 4525–4547. (<a
href="https://doi.org/10.1007/s10489-020-02001-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by biological immune systems, the field of artificial immune system (AIS), particularly the negative selection algorithm (NSA), has been proved effective in solving computational problems. However in practical applications, NSA still encounter challenges, such as noise in training data leading to imprecise classifications, the lack of sufficient samples for detector maturation, and potential overlap among detectors. Address to these problems, we propose a novel hybrid detector generation algorithm based on fast clustering for artificial immune systems, namely FCAIS-HD. It primarily consists of two stages: first it utilizes a fast clustering algorithm to generate self-samples to decrease the effect of noise in the data. FCAIS-HD replaces the self-samples with a small number of self-detectors to reduce the time of generating non-self detectors; in the second stage, it utilizes a novel variable-radius non-self detector generation algorithm to generate a small number of non-self detectors with small overlap rates. Finally, both self-detectors as well as non-self detectors are used to implement hybrid detection (HD). Comprehensive experiments are conducted on both simulation and real world data sets to compare classification performance with baselines. The results demonstrate that FCAIS-HD outperforms other algorithms with well excluded low-level noise interference, higher rate of detection and less parameter sensitivity. Additionally, experiments are carried out on some real word data sets demonstrate that FCAIS-HD also performs well in high-dimensional data sets.},
  archive      = {J_APIN},
  author       = {Chen, Jinyin and Wang, Xueke and Su, Mengmeng and Lin, Xiang},
  doi          = {10.1007/s10489-020-02001-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4525-4547},
  shortjournal = {Appl. Intell.},
  title        = {A fast detector generation algorithm for negative selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A reasoning enhance network for muti-relation question
answering. <em>APIN</em>, <em>51</em>(7), 4515–4524. (<a
href="https://doi.org/10.1007/s10489-020-02111-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-relation Question Answering is an important task of knowledge base over question answering (KBQA), multi-relation means that the question contains multiple relations and entity information, so it needs to use the fact triples in the knowledge base to analyze and reasoning the question in more detail. In this paper, we propose a novel model called Reasoning Enhance Network that uses context information, enhance the accuracy of relation and entity predicted in each hop. The model obtains the relation by analyzing the context information before each hop start, and then reasons the answer by the previous information; update question representation and reasoning state through predicted relation and entity, then promote the next hop reasoning starts. Our experiments clearly show that our method achieves good results on four datasets. Also, since we use attention mechanisms, our method offers better interpretability.},
  archive      = {J_APIN},
  author       = {Wu, Wenqing and Zhu, Zhenfang and Zhang, Guangyuan and Kang, Shiyong and Liu, Peiyu},
  doi          = {10.1007/s10489-020-02111-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4515-4524},
  shortjournal = {Appl. Intell.},
  title        = {A reasoning enhance network for muti-relation question answering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Label flipping attacks against naive bayes on spam filtering
systems. <em>APIN</em>, <em>51</em>(7), 4503–4514. (<a
href="https://doi.org/10.1007/s10489-020-02086-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label flipping attack is a poisoning attack that flips the labels of training samples to reduce the classification performance of the model. Robustness is used to measure the applicability of machine learning algorithms to adversarial attack. Naive Bayes (NB) algorithm is a anti-noise and robust machine learning technique. It shows good robustness when dealing with issues such as document classification and spam filtering. Here we propose two novel label flipping attacks to evaluate the robustness of NB under label noise. For the three datasets of Spambase, TREC 2006c and TREC 2007 in the spam classification domain, our attack goal is to increase the false negative rate of NB under the influence of label noise without affecting normal mail classification. Our evaluation shows that at a noise level of 20%, the false negative rate of Spambase and TREC 2006c has increased by about 20%, and the test error of the TREC 2007 dataset has increased to nearly 30%. We compared the classification accuracy of five classic machine learning algorithms (random forest(RF), support vector machine(SVM), decision tree(DT), logistic regression(LR), and NB) and two deep learning models(AlexNet, LeNet) under the proposed label flipping attacks. The experimental results show that two label noises are suitable for various classification models and effectively reduce the accuracy of the models.},
  archive      = {J_APIN},
  author       = {Zhang, Hongpo and Cheng, Ning and Zhang, Yang and Li, Zhanbo},
  doi          = {10.1007/s10489-020-02086-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4503-4514},
  shortjournal = {Appl. Intell.},
  title        = {Label flipping attacks against naive bayes on spam filtering systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistent scale normalization for object perception.
<em>APIN</em>, <em>51</em>(7), 4490–4502. (<a
href="https://doi.org/10.1007/s10489-020-02070-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, object detection has been a vital aspect in the vision community, while scale variation of objects in images or videos usually brings challenge for performance improvement. To combat this problem, conventional paradigms generally adopt image pyramid or Feature Pyramid Network (FPN) to process objects at different scales. However, existing multi-scale deep convolution neural networks mostly set different scales in a heuristic way, which may introduce inconsistency between the region of interest and the semantic scope. In this paper, we propose an innovative paradigm called Consistent Scale Normalization (CSN) to weaken the influence of scale variation for object detection. The proposed CSN can realize a consistent compression for the scale space of objects, in both training and testing phases. Extensive experimental testing is performed on COCO object detection benchmark in comparison with several state-of-the-art methods. In addition to object detection, experiments on instance segmentation and multi-task human pose estimation are also conducted. Furthermore, the CSN paradigm is beneficial to reduce the difficulty of network learning. The results verify the effectiveness and superiority of the CSN paradigm.},
  archive      = {J_APIN},
  author       = {He, Zewen and Huang, He and Wu, Yudong and Yang, Xuebing and Zhang, Wensheng},
  doi          = {10.1007/s10489-020-02070-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4490-4502},
  shortjournal = {Appl. Intell.},
  title        = {Consistent scale normalization for object perception},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel order evaluation model with nested
probabilistic-numerical linguistic information applied to traditional
order grabbing mode. <em>APIN</em>, <em>51</em>(7), 4470–4489. (<a
href="https://doi.org/10.1007/s10489-020-02088-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of information technology and the acceleration of the people’s pace of life, the takeout food industry is prevailing. The choice of order allocation mode plays an important role in order delivery efficiencies. This paper firstly reviews the whole process of the order grabbing mode and its internal logic, and then analyzes the qualitative and quantitative factors that influence the order distribution efficiency. Next, the order evaluation model is established based on the nested probabilistic-numerical linguistic information. After that, influencing factors of the order allocation modes are established, and the weights of the factors are determined by the AHP method. Finally, the order distribution results are obtained by traditional mode and the novel mode respectively. The comparative analysis and further analysis verify the validity and operability of the novel mode. By comparing the final values of multi-criteria functions between two modes, we conclude that the novel mode improves the allocation efficiency of order grabbing mode. In addition, the proposed mode significantly reduces the service distance and the standard deviation of service distance. The completion rate of delivery orders and the consistency of service level are also greatly improved. The takeout order allocation problem is optimized through the order evaluation model based on the nested probabilistic-numerical linguistic information. The proposed method has guiding effect on similar platforms.},
  archive      = {J_APIN},
  author       = {Ge, Zijing and Wang, Xinxin and Xu, Zeshui},
  doi          = {10.1007/s10489-020-02088-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4470-4489},
  shortjournal = {Appl. Intell.},
  title        = {A novel order evaluation model with nested probabilistic-numerical linguistic information applied to traditional order grabbing mode},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved multi-focus image fusion algorithm based on
multi-scale weighted focus measure. <em>APIN</em>, <em>51</em>(7),
4453–4469. (<a
href="https://doi.org/10.1007/s10489-020-02066-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on developing an improved multi-focus image fusion (MFIF) algorithm. Existing spatial domain algorithms dependent on the obtained fusion decision map still lead to unexpected ghosting, blurred, edges as well as blocking effects such that the visual effect of image fusion is seriously degraded. To overcome these shortages, an improved MFIF algorithm is developed with the help of a novel multi-scale weighted focus measure and a decision map optimization technique. First, a novel multi-scale measurement template is designed in order to effectively extract the gradient information of rich texture regions, smooth regions as well as transitional regions between the aforementioned regions simultaneously. Then, an improved calculation scheme of the focus score matrix is designed based on the weighted sum of the focus measure maps in each region window centered on a concerned pixel, under which the advantage of pixel-by-pixel weighting is employed. In what follows, an initial decision map is obtained in light of the focus score matrix combined with threshold filtering, which is employed to eliminate the small isolated regions caused by some misclassified pixels. Furthermore, an accurate decision map is received with the help of the optimization capability of guided filtering to avoid edge unexpected artificial textures. In comparison with block-based fusion algorithms, our algorithm developed in this paper extracts the focus regions pixel-by-pixel, thereby helping to reduce the blocking effects that appear in the fusion image. Finally, some intensive comparison analysis based on common datasets is performed to verify the superiority over state-of-the-art methods in both visual qualitative and quantitative evaluations.},
  archive      = {J_APIN},
  author       = {Hu, Zhanhui and Liang, Wei and Ding, Derui and Wei, Guoliang},
  doi          = {10.1007/s10489-020-02066-8},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4453-4469},
  shortjournal = {Appl. Intell.},
  title        = {An improved multi-focus image fusion algorithm based on multi-scale weighted focus measure},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bottom-up multi-agent reinforcement learning by reward
shaping for cooperative-competitive tasks. <em>APIN</em>,
<em>51</em>(7), 4434–4452. (<a
href="https://doi.org/10.1007/s10489-020-02034-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-agent system (MAS) is expected to be applied to various real-world problems where a single agent cannot accomplish given tasks. Due to the inherent complexity in the real-world MAS, however, manual design of group behaviors of agents is intractable. Multi-agent reinforcement learning (MARL), which is a framework for multiple agents in the same environment to learn their policies adaptively by using reinforcement learning, would be a promising methodology for such complexity in the MAS. To acquire the group behaviors by MARL, all the agents are required to understand how to achieve the respective tasks cooperatively. So far, we have proposed “bottom-up MARL”, which is a decentralized system to manage real and large-scale MARL, with a reward shaping algorithm to represent the group behaviors. The reward shaping algorithm, however, assumes that all the agents are in cooperative relationships to some extent. In this paper, therefore, we extend this algorithm to allow the agents not to know the interests between them. The interests are regarded as correlation coefficients derived from the agents’ rewards, which are numerically estimated in an online manner. Actually, in both simulations and real experiments without knowledge of the interests between the agents, they correctly estimated their interests, thereby allowing them to derive their new rewards to represent the feasible group behaviors in the decentralized manner. As a result, our extended algorithm succeeded in acquiring the group behaviors from cooperative tasks to competitive tasks.},
  archive      = {J_APIN},
  author       = {Aotani, Takumi and Kobayashi, Taisuke and Sugimoto, Kenji},
  doi          = {10.1007/s10489-020-02034-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4434-4452},
  shortjournal = {Appl. Intell.},
  title        = {Bottom-up multi-agent reinforcement learning by reward shaping for cooperative-competitive tasks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved direction-of-arrival estimation method based on
LSTM neural networks with robustness to array imperfections.
<em>APIN</em>, <em>51</em>(7), 4420–4433. (<a
href="https://doi.org/10.1007/s10489-020-02124-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Array imperfections severely degrade the performance of most physics-driven direction-of-arrival (DOA) methods. Deep learning-based methods do not rely on any assumptions, can learn the latent data features of a given dataset, and are expected to adapt better to array imperfections compared with existing physics-driven methods. Hence, an improved DOA estimation method based on long short-term memory (LSTM) neural networks for situations with array imperfections is proposed in this paper. Various analyses given by this paper demonstrate that the phase features are the key to DOA estimation. Considering the sequential characteristics of the moving target and the correlation of multi-frame data features, the LSTM neural networks are used to learn and enhance the phase features of sampled data. The DOA estimation accuracy and generalization capability are improved by mitigating the phase distortion using LSTM. Numerical simulations and statistical results show that the proposed method is satisfactory in terms of both the generalization capability and imperfection adaptability compared with state-of-the-art physics-driven and data-driven methods.},
  archive      = {J_APIN},
  author       = {Xiang, Houhong and Chen, Baixiao and Yang, Minglei and Xu, Saiqin and Li, Zhengjie},
  doi          = {10.1007/s10489-020-02124-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4420-4433},
  shortjournal = {Appl. Intell.},
  title        = {Improved direction-of-arrival estimation method based on LSTM neural networks with robustness to array imperfections},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aspect-gated graph convolutional networks for aspect-based
sentiment analysis. <em>APIN</em>, <em>51</em>(7), 4408–4419. (<a
href="https://doi.org/10.1007/s10489-020-02095-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis aims to predict the sentiment polarity of each specific aspect term in a given sentence. However, the previous models ignore syntactical constraints and long-range sentiment dependencies and mistakenly identify irrelevant contextual words as clues for judging aspect sentiment. In addition, these models usually use aspect-independent encoders to encode sentences, which can lead to a lack of aspect information. In this paper, we propose an aspect-gated graph convolutional network (AGGCN), that includes a special aspect gate designed to guide the encoding of aspect-specific information from the outset and construct a graph convolution network on the sentence dependency tree to make full use of the syntactical information and sentiment dependencies. The experimental results on multiple SemEval datasets demonstrate the effectiveness of the proposed approach, and our model outperforms the strong baseline models.},
  archive      = {J_APIN},
  author       = {Lu, Qiang and Zhu, Zhenfang and Zhang, Guangyuan and Kang, Shiyong and Liu, Peiyu},
  doi          = {10.1007/s10489-020-02095-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4408-4419},
  shortjournal = {Appl. Intell.},
  title        = {Aspect-gated graph convolutional networks for aspect-based sentiment analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ranking influential nodes in complex networks based on local
and global structures. <em>APIN</em>, <em>51</em>(7), 4394–4407. (<a
href="https://doi.org/10.1007/s10489-020-02132-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying influential nodes in complex networks is an open and challenging issue. Many measures have been proposed to evaluate the influence of nodes and improve the accuracy of measuring influential nodes. In this paper, a new method is proposed to identify and rank the influential nodes in complex networks. The proposed method determines the influence of a node based on its local location and global location. It considers both the local and global structure of the network. Traditional degree centrality is improved and combined with the notion of the local clustering coefficient to measure the local influence of nodes, and the classical k-shell decomposition method is improved to measure the global influence of nodes. To evaluate the performance of the proposed method, the susceptible-infected-recovered (SIR) model is utilized to examine the spreading capability of nodes. A number of experiments are conducted on 11 real-world networks to compare the proposed method with other methods. The experimental results show that the proposed method can identify the influential nodes more accurately than other methods.},
  archive      = {J_APIN},
  author       = {Qiu, Liqing and Zhang, Jianyi and Tian, Xiangbo},
  doi          = {10.1007/s10489-020-02132-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4394-4407},
  shortjournal = {Appl. Intell.},
  title        = {Ranking influential nodes in complex networks based on local and global structures},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-temporal attention network for multistep-ahead
forecasting of chlorophyll. <em>APIN</em>, <em>51</em>(7), 4381–4393.
(<a href="https://doi.org/10.1007/s10489-020-02143-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multistep-ahead prediction of chlorophyll provides an effective means for early warning of red tide. However, since multistep-ahead forecasting presents challenges, such as vague interactive relationships among ocean factors, long-term dependence modeling, and accumulative errors, existing methods mostly concentrate on the current time or one-step-ahead forecasting. In this paper, a hierarchical multistep-ahead forecasting model spatial-temporal attention network(STAN), which integrates the spatial context extractor network(SCE-net), long short-term memory network(LSTM), and the temporal attention mechanism, is proposed for the prediction of chlorophyll. In STAN, the input layer utilizes SCE-net to excavate relationships among ocean factors and generate high-level semantic via embedding factors into a continuous low-dimensional space. The middle layer applies LSTM to build the long-term dependencies of corresponding semantic representations. The output layer uses another LSTM with temporal attention to reduce accumulative errors and maintain temporal continuity. The attention can assign different weights to the middle layer’s hidden state and generate a context vector. Then the context vector and the final predicted value are considered as the current input for better forecasting. The buoy observation data of the Xiamen coastal area monitored in 2009–2011 is used to verify the efficiency of STAN. Experimental results prove that STAN outperforms the state-of-the-art methods of multistep-ahead prediction. When using 7 observation steps to forecast 15 steps, the MAPE of STAN is 0.3209, and the MAE is 0.1 lower than the values of the baselines approaches.},
  archive      = {J_APIN},
  author       = {He, Xiaoyu and Shi, Suixiang and Geng, Xiulin and Xu, Lingyu and Zhang, Xiaolin},
  doi          = {10.1007/s10489-020-02143-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4381-4393},
  shortjournal = {Appl. Intell.},
  title        = {Spatial-temporal attention network for multistep-ahead forecasting of chlorophyll},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image super-resolution reconstruction based on feature map
attention mechanism. <em>APIN</em>, <em>51</em>(7), 4367–4380. (<a
href="https://doi.org/10.1007/s10489-020-02116-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the issue of low-frequency and high-frequency components from feature maps being treated equally in existing image super-resolution reconstruction methods, the paper proposed an image super-resolution reconstruction method using attention mechanism with feature map to facilitate reconstruction from original low-resolution images to multi-scale super-resolution images. The proposed model consists of a feature extraction block, an information extraction block, and a reconstruction module. Firstly, the extraction block is used to extract useful features from low-resolution images, with multiple information extraction blocks being combined with the feature map attention mechanism and passed between feature channels. Secondly, the interdependence is used to adaptively adjust the channel characteristics to restore more details. Finally, the reconstruction module reforms different scales high-resolution images. The experimental results can demonstrate that the proposed method can effectively improve not only the visual effect of images but also the results on the Set5, Set14, Urban100, and Manga109. The results can demonstrate the proposed method has structurally similarity to the image reconstruction methods. Furthermore, the evaluating indicator of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM) has been improved to a certain degree, while the effectiveness of using feature map attention mechanism in image super-resolution reconstruction applications is useful and effective.},
  archive      = {J_APIN},
  author       = {Chen, Yuantao and Liu, Linwu and Phonevilay, Volachith and Gu, Ke and Xia, Runlong and Xie, Jingbo and Zhang, Qian and Yang, Kai},
  doi          = {10.1007/s10489-020-02116-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4367-4380},
  shortjournal = {Appl. Intell.},
  title        = {Image super-resolution reconstruction based on feature map attention mechanism},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DcaNAS: Efficient convolutional network design for desktop
CPU platforms. <em>APIN</em>, <em>51</em>(7), 4353–4366. (<a
href="https://doi.org/10.1007/s10489-020-02133-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hardware platform is a significant consideration in efficient CNN model design. Most lightweight networks are based on GPUs and mobile devices. However, they are usually not efficient nor fast enough for desktop CPU platforms. In this paper, we aim to explore the design of highly-efficient convolutional architectures for desktop CPU platforms. To achieve our goal, we first derive a series of CNN model design guidelines for CPU-based devices by comparing different computing platforms. Based on these proposed guidelines, we further present a Desktop CPU-Aware network architecture search (DcaNAS) to search for the optimal network structure with lower CPU latency. By combining automatic search and manual design, our DcaNAS achieves better flexibility and efficiency. On the ImageNet benchmark, we employ DcaNAS to produce two CPU-based lightweight CNN models: DcaNAS-L for higher accuracy and DcaNAS-S for faster speed. On a single CPU core, DcaNAS-L achieves 78.8% Top-1 (94.6% Top-5) accuracy at 13.6 FPS (73.5 ms), and our DcaNAS-S achieves extremely low CPU latency (43.1 ms). The results show that our DcaNAS method can obtain new state-of-the-art CPU-based networks.},
  archive      = {J_APIN},
  author       = {Chen, Dong and Shen, Hao and Shen, Yuchen},
  doi          = {10.1007/s10489-020-02133-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4353-4366},
  shortjournal = {Appl. Intell.},
  title        = {DcaNAS: Efficient convolutional network design for desktop CPU platforms},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid greedy indicator- and pareto-based many-objective
evolutionary algorithm. <em>APIN</em>, <em>51</em>(7), 4330–4352. (<a
href="https://doi.org/10.1007/s10489-020-02025-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As most of Multi-Objective Evolutionary Algorithms (MOEAs) scale quite poorly when the number of objective functions increases, new strategies have been proposed to face this limitation. Considered one of the most well-succeeded examples of such new strategies, the third version of Non-dominated Sorting Genetic Algorithm (NSGA-III) uses a set of reference points placed on a normalized hyperplane to solve Many Objective Optimization Problems (MaOPs). Despite the good results of NSGA-III, the shape of the hypersurface can influence the algorithm’s performance and it has not been deeply explored in the recent literature. This work aims therefore at proposing a new hybrid algorithm and investigating nonlinear transformations applied to the NSGA-III hyperplane. We categorize our proposed approach as a hybrid MOEA (Indicator- and Pareto-based), and analyze the influence of the modified set of reference points on the proposed algorithm while solving MaOPs. The paper presents as its main contributions: (i) the proposal of a greedy indicator-based variant of NSGA-III that uses math transformations on a subset of chosen reference points; (ii) the use of Vector Guided Adaptation procedure to modify original NSGA-III hyperplane at every few iterations. A set of experiments addressing several benchmark problems is performed to evaluate the performance of the original and adapted NSGA-III versions, and also to compare them with two well known MOEAs, observing both, convergence and diversity of the final Pareto fronts. Based on the analysis of statistical tests, we conclude that the proposed approach is competitive and can provide an interesting alternative to the original NSGA-III conception.},
  archive      = {J_APIN},
  author       = {de Oliveira, Matheus Carvalho and Delgado, Myriam Regattieri and Britto, André},
  doi          = {10.1007/s10489-020-02025-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4330-4352},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid greedy indicator- and pareto-based many-objective evolutionary algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Deep bi-directional interaction network for sentence
matching. <em>APIN</em>, <em>51</em>(7), 4305–4329. (<a
href="https://doi.org/10.1007/s10489-020-02156-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of sentence matching is to determine the semantic relation between two sentences, which is the basis of many downstream tasks in natural language processing, such as question answering and information retrieval. Recent studies using attention mechanism to align the elements of two sentences have shown promising results in capturing semantic similarity/relevance. Most existing methods mainly focus on the design of multi-layer attention network, however, some critical issues have not been dealt with well: 1) the higher attention layer is easily affected by error propagation because it relies on the alignment results of preceding attentions; 2) models have the risk of losing low-layer semantic features with the increase of network depth; and 3) the approach of capturing global matching information brings about large computing complexity for model training. To this end, we propose a Deep Bi-Directional Interaction Network (DBDIN) to solve these issues, which captures semantic relatedness from two directions and each direction employs multiple attention-based interaction units. To be specific, the attention of each interaction unit will repeatedly focus on the original sentence representation of another one for semantic alignment, which alleviates the error propagation problem by attending to a fixed semantic representation. Then we design deep fusion to aggregate and propagate attention information from low layers to high layers, which effectively retains low-layer semantic features for subsequential interactions. Moreover, we introduce a self-attention mechanism at last to enhance global matching information with smaller model complexity. We conduct experiments on natural language inference and paraphrase identification tasks with three benchmark datasets SNLI, SciTail and Quora. Experimental results demonstrate that our proposed method can achieve significant improvements over baseline systems without using any external knowledge. Additionally, we conduct interpretable study to disclose how our deep interaction network with attention can benefit sentence matching, which provides a reference for future model design. Ablation studies and visualization analyses further verify that our model can better capture interactive information between two sentences, and the proposed components are indeed able to help modeling semantic relation more precisely.},
  archive      = {J_APIN},
  author       = {Liu, Mingtong and Zhang, Yujie and Xu, Jinan and Chen, Yufeng},
  doi          = {10.1007/s10489-020-02156-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4305-4329},
  shortjournal = {Appl. Intell.},
  title        = {Deep bi-directional interaction network for sentence matching},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory network with hierarchical multi-head attention for
aspect-based sentiment analysis. <em>APIN</em>, <em>51</em>(7),
4287–4304. (<a
href="https://doi.org/10.1007/s10489-020-02069-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis is a challenging subtask of sentiment analysis, which aims to identify the sentiment polarities of the given aspect terms in sentences. Previous studies have demonstrated the remarkable progress achieved by memory networks. However, current memory-network-based models cannot fully exploit long-term semantic relationships to the given aspect terms in sentences, which may lead to the loss of aspect information. In this paper, we propose a novel memory network with hierarchical multi-head attention (MNHMA) for aspect-based sentiment analysis. First, we introduce a semantic information extraction strategy based on the rotational unit of memory to acquire long-term semantic information in context and build memory for the memory network. Second, we propose a hierarchical multi-head attention mechanism to preserve aspect information and enable MNHMA to focus on the critical context words to the given aspect terms in sentences. Third, we employ a fully connected layer in each attention layer of the hierarchical multi-head attention layer to simulate the nonlinear transformation of sentiments, thereby acquiring a comprehensive context representation for aspect-level sentiment classification. Experimental results on three commonly used benchmark datasets demonstrate that our MNHMA model outperforms other state-of-the-art models for aspect-based sentiment analysis.},
  archive      = {J_APIN},
  author       = {Chen, Yuzhong and Zhuang, Tianhao and Guo, Kun},
  doi          = {10.1007/s10489-020-02069-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4287-4304},
  shortjournal = {Appl. Intell.},
  title        = {Memory network with hierarchical multi-head attention for aspect-based sentiment analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Averaged tree-augmented one-dependence estimators.
<em>APIN</em>, <em>51</em>(7), 4270–4286. (<a
href="https://doi.org/10.1007/s10489-020-02064-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ever since the success of naive Bayes (NB) in achieving excellent classification performance and the least computational overhead, more and more researchers have focused their attention on the Bayesian network classifiers (BNCs). Among numerous approaches to refining NB, averaged one-dependence estimators (AODE) achieves excellent classification performance although its discriminative independence assumption for each member rarely holds in practice. Robust AODE with high expressivity and low bias is in urgent need with the ever increasing data quantity. In this paper, the log likelihood function $LL({\mathscr{B}}|D)$ is introduced to measure the number of bits which is encoded in the network topology ${\mathscr{B}}$ for describing training data D. An efficient heuristic search strategy is applied to maximize $LL({\mathscr{B}}|D)$ and relax the independence assumption of AODE by exploring higher-order conditional dependencies between attributes. The proposed approach, averaged tree-augmented one-dependence estimators (ATODE), inherits the effectiveness of AODE and gains more flexibility for modelling higher-order dependencies. The extensive experimental comparison results on 36 datasets demonstrate that, compared to state-of-the-art learners including single-model BNCs (e.g., CFWNB and SKDB) and variants of AODE (e.g., TAODE), our proposed out-of-core learner can achieve competitive or better classification performance.},
  archive      = {J_APIN},
  author       = {Kong, He and Shi, Xiaohu and Wang, Limin and Liu, Yang and Mammadov, Musa and Wang, Gaojie},
  doi          = {10.1007/s10489-020-02064-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4270-4286},
  shortjournal = {Appl. Intell.},
  title        = {Averaged tree-augmented one-dependence estimators},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modified non-dominated sorting genetic algorithm III with
fine final level selection. <em>APIN</em>, <em>51</em>(7), 4236–4269.
(<a href="https://doi.org/10.1007/s10489-020-02053-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dominance resistance is a challenge for Pareto-based multi-objective evolutionary algorithms to solve the high-dimensional optimization problems. The Non-dominated Sorting Genetic Algorithm III (NSGA-III) still has such disadvantage even though it is recognized as an algorithm with good performance for many-objective problems. Thus, a variation of NSGA-III algorithm based on fine final level selection is proposed to improve convergence. The fine final level selection is designed in this way. The θ-dominance relation is used to sort the solutions in the critical layer firstly. Then ISDE index and favor convergence are employed to evaluate convergence of individuals for different situations. And some better solutions are selected finally. The effectiveness of our proposed algorithm is validated by comparing with nine state-of-the-art algorithms on the Deb-Thiele-Laumanns-Zitzler and Walking-Fish-Group test suits. And the optimization objectives are varying from 3 to 15. The performance is evaluated by the inverted generational distance (IGD), hypervolume (HV), generational distance (GD). The simulation results show that the proposed algorithm has an average improvement of 55.4%, 60.0%, 63.1% of 65 test instances for IGD, HV, GD indexes over the original NSGA-III algorithm. Besides, the proposed algorithm obtains the best performance by comparing 9 state-of-art algorithms in HV, GD indexes and ranks third for IGD indicator. Therefore, the proposed algorithm can achieve the advantages over the benchmarks.},
  archive      = {J_APIN},
  author       = {Gu, Qinghua and Wang, Rui and Xie, Haiyan and Li, Xuexian and Jiang, Song and Xiong, Naixue},
  doi          = {10.1007/s10489-020-02053-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4236-4269},
  shortjournal = {Appl. Intell.},
  title        = {Modified non-dominated sorting genetic algorithm III with fine final level selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified approach for detection of clickbait videos on
YouTube using cognitive evidences. <em>APIN</em>, <em>51</em>(7),
4214–4235. (<a
href="https://doi.org/10.1007/s10489-020-02057-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clickbait is one of the form of false content, purposely designed to attract the user’s attention and make them curious to follow the link and read, view, or listen to the attached content. The teaser aim behind this is to exploit the curiosity gap by giving information within the short statement. Still, the given statement is not sufficient enough to satisfy the curiosity without clicking through the linked content and lure the user to get into the respective page via playing with human psychology and degrades the user experience. To counter this problem, we develop a Clickbait Video Detector (CVD) scheme. The scheme leverages to learn three sets of latent features based on User Profiling, Video-Content, and Human Consensus, these are further used to retrieve cognitive evidence for the detection of clickbait videos on YouTube. The first step is to extract audio from the videos, which is further transformed to textual data, and later on, it is utilized for the extraction of video content-based features. Secondly, the comments are analyzed, and features are extracted based on human responses/reactions over the posted content. Lastly, user profile based features are extracted. Finally, all these features are fed into the classifier. The proposed method is tested on the publicly available fake video corpus [FVC], [FVC-2018] dataset, and a self-generated misleading video dataset [MVD]. The achieved result is compared with other state-of-the-art methods and demonstrates superior performance.},
  archive      = {J_APIN},
  author       = {Varshney, Deepika and Vishwakarma, Dinesh Kumar},
  doi          = {10.1007/s10489-020-02057-9},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4214-4235},
  shortjournal = {Appl. Intell.},
  title        = {A unified approach for detection of clickbait videos on YouTube using cognitive evidences},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A-DBNF: Adaptive deep belief network framework for
regression and classification tasks. <em>APIN</em>, <em>51</em>(7),
4199–4213. (<a
href="https://doi.org/10.1007/s10489-020-02050-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many machine learning methods and models have been proposed for multivariate data regression and classification in recent years. Most of them are supervised learning methods, which require a large number of labeled data. Moreover, current methods need exclusive human labor and supervision to fine-tune the model hyperparameters. In this paper, we propose an adaptive deep belief network framework (A-DBNF) that can adapt to different datasets with minimum human labor. The proposed framework employs a deep belief network (DBN) to extract representative features of the datasets in the unsupervised learning phase and then fine-tune the network parameters by using few labeled data in the supervised learning phase. We integrate the DBN model with a genetic algorithm (GA) to select and optimize the model hyperparameters and further improve the network performance. We validate the performance of the proposed framework on several benchmark datasets, comparing the regression and classification accuracy with state-of-the-art methods. A-DBNF showed a noticeable performance improvement on three regression tasks using only 40–50% of labeled data. Our model outperformed most of the related methods in classification tasks by using 23–48% of labeled data.},
  archive      = {J_APIN},
  author       = {Ibrokhimov, Bunyodbek and Hur, Cheonghwan and Kim, Hyunseok and Kang, Sanggil},
  doi          = {10.1007/s10489-020-02050-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4199-4213},
  shortjournal = {Appl. Intell.},
  title        = {A-DBNF: Adaptive deep belief network framework for regression and classification tasks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new SEAIRD pandemic prediction model with clinical and
epidemiological data analysis on COVID-19 outbreak. <em>APIN</em>,
<em>51</em>(7), 4162–4198. (<a
href="https://doi.org/10.1007/s10489-020-01938-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the spread of disease during a pandemic is critically important for accurately and promptly applying various lockdown strategies, so to prevent the collapse of the medical system. The latest pandemic of COVID-19 that hits the world death tolls and economy loss very hard, is more complex and contagious than its precedent diseases. The complexity comes mostly from the emergence of asymptomatic patients and relapse of the recovered patients which were not commonly seen during SARS outbreaks. These new characteristics pertaining to COVID-19 were only discovered lately, adding a level of uncertainty to the traditional SEIR models. The contribution of this paper is that for the COVID-19 epidemic, which is infectious in both the incubation period and the onset period, we use neural networks to learn from the actual data of the epidemic to obtain optimal parameters, thereby establishing a nonlinear, self-adaptive dynamic coefficient infectious disease prediction model. On the basis of prediction, we considered control measures and simulated the effects of different control measures and different strengths of the control measures. The epidemic control is predicted as a continuous change process, and the epidemic development and control are integrated to simulate and forecast. Decision-making departments make optimal choices. The improved model is applied to simulate the COVID-19 epidemic in the United States, and by comparing the prediction results with the traditional SEIR model, SEAIRD model and adaptive SEAIRD model, it is found that the adaptive SEAIRD model’s prediction results of the U.S. COVID-19 epidemic data are in good agreement with the actual epidemic curve. For example, from the prediction effect of these 3 different models on accumulative confirmed cases, in terms of goodness of fit, adaptive SEAIRD model (0.99997) ≈ SEAIRD model (0.98548) &gt; Classical SEIR model (0.66837); in terms of error value: adaptive SEAIRD model (198.6563) &lt; &lt; SEAIRD model(4739.8577) &lt; &lt; Classical SEIR model (22,652.796); The objective of this contribution is mainly on extending the current spread prediction model. It incorporates extra compartments accounting for the new features of COVID-19, and fine-tunes the new model with neural network, in a bid of achieving a higher level of prediction accuracy. Based on the SEIR model of disease transmission, an adaptive model called SEAIRD with internal source and isolation intervention is proposed. It simulates the effects of the changing behaviour of the SARS-CoV-2 in U.S. Neural network is applied to achieve a better fit in SEAIRD. Unlike the SEIR model, the adaptive SEAIRD model embraces multi-group dynamics which lead to different evolutionary trends during the epidemic. Through the risk assessment indicators of the adaptive SEAIRD model, it is convenient to measure the severity of the epidemic situation for consideration of different preventive measures. Future scenarios are projected from the trends of various indicators by running the adaptive SEAIRD model.},
  archive      = {J_APIN},
  author       = {Liu, Xian-Xian and Fong, Simon James and Dey, Nilanjan and Crespo, Rubén González and Herrera-Viedma, Enrique},
  doi          = {10.1007/s10489-020-01938-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4162-4198},
  shortjournal = {Appl. Intell.},
  title        = {A new SEAIRD pandemic prediction model with clinical and epidemiological data analysis on COVID-19 outbreak},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi objective volleyball premier league algorithm for
green scheduling identical parallel machines with splitting jobs.
<em>APIN</em>, <em>51</em>(7), 4143–4161. (<a
href="https://doi.org/10.1007/s10489-020-02027-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel machine scheduling is one of the most common studied problems in recent years, however, this classic optimization problem has to achieve two conflicting objectives, i.e. minimizing the total tardiness and minimizing the total wastes, if the scheduling is done in the context of plastic injection industry where jobs are splitting and molds are important constraints. This paper proposes a mathematical model for scheduling parallel machines with splitting jobs and resource constraints. Two minimization objectives - the total tardiness and the number of waste - are considered, simultaneously. The obtained model is a bi-objective integer linear programming model that is shown to be of NP-hard class optimization problems. In this paper, a novel Multi-Objective Volleyball Premier League (MOVPL) algorithm is presented for solving the aforementioned problem. This algorithm uses the crowding distance concept used in NSGA-II as an extension of the Volleyball Premier League (VPL) that we recently introduced. Furthermore, the results are compared with six multi-objective metaheuristic algorithms of MOPSO, NSGA-II, MOGWO, MOALO, MOEA/D, and SPEA2. Using five standard metrics and ten test problems, the performance of the Pareto-based algorithms was investigated. The results demonstrate that in general, the proposed algorithm has supremacy than the other four algorithms.},
  archive      = {J_APIN},
  author       = {Salimifard, Khodakaram and Li, Jingpeng and Mohammadi, Davood and Moghdani, Reza},
  doi          = {10.1007/s10489-020-02027-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4143-4161},
  shortjournal = {Appl. Intell.},
  title        = {A multi objective volleyball premier league algorithm for green scheduling identical parallel machines with splitting jobs},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel mathematical formulation for solving the dynamic and
discrete berth allocation problem by using the bee colony optimisation
algorithm. <em>APIN</em>, <em>51</em>(7), 4127–4142. (<a
href="https://doi.org/10.1007/s10489-020-02062-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Berth allocation is one of the crucial points for efficient management of ports. This problem is complex due to all possible combinations for assigning ships to available compatible berths. This paper focuses on solving the Berth Allocation Problem (BAP) by optimising port operations using an innovative model. The problem analysed in this work deals with the Discrete and Dynamic Berth Allocation Problem (DDBAP). We propose a novel mathematical formulation expressed as a Mixed Integer Linear Programming (MILP) for solving the DDBAP. Furthermore, we adapted a metaheuristic solution approach based on the Bee Colony Optimisation (BCO) for solving large-sized combinatorial BAPs. In order to assess the solution performance and efficiency of the proposed model, we introduce a new set of instances based on real data of the Livorno port (Italy), and a comparison between the BCO algorithm and CPLEX in solving the DDBAP is performed. Additionally, the application of the proposed model to a real berth scheduling (Livorno port data) and a comparison with the Ant Colony Optimisation (ACO) metaheuristic are carried out. Results highlight the feasibility of the proposed model and the effectiveness of BCO when compared to both CPLEX and ACO, achieving computation times that ensure a real-time application of the method.},
  archive      = {J_APIN},
  author       = {Prencipe, Luigi Pio and Marinelli, Mario},
  doi          = {10.1007/s10489-020-02062-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {4127-4142},
  shortjournal = {Appl. Intell.},
  title        = {A novel mathematical formulation for solving the dynamic and discrete berth allocation problem by using the bee colony optimisation algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A rule-based heuristic algorithm for joint order batching
and delivery planning of online retailers with multiple order pickers.
<em>APIN</em>, <em>51</em>(6), 3917–3935. (<a
href="https://doi.org/10.1007/s10489-020-01843-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today with the rapid improvement of new technologies, people tend to buy various products from online retailers which facilitate the purchasing process and save their valuable limited time. Two important and interconnected operations of each online retailing system are order picking and delivery planning. In an online system, lots of small orders including different products arrive dynamically and must be delivered on time, so there is a limited time to retrieve products from their storage locations, pack them, load onto trucks, and deliver to the destinations. In this study, we deal with these two problems of an online retailer that stores a variety of products in a warehouse and sells them online through their website. A rule-based heuristic algorithm is proposed which integrates decisions of order batching, picking schedule of batches, and assigning orders to trucks as well as, scheduling and routing of trucks. Three different batching methods including two well- known heuristics and a genetic algorithm have been used. An extensive numerical experiment is carried out to show the efficiency of the rule-based algorithm and investigate the results of using each batching method for different problem sizes. It is demonstrated that while the algorithm has efficient performance with three used batching methods, the genetic algorithm can lead to less system cost and more order pickers productivity.},
  archive      = {J_APIN},
  author       = {Hossein Nia Shavaki, Fahimeh and Jolai, Fariborz},
  doi          = {10.1007/s10489-020-01843-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3917-3935},
  shortjournal = {Appl. Intell.},
  title        = {A rule-based heuristic algorithm for joint order batching and delivery planning of online retailers with multiple order pickers},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection with multi-objective genetic algorithm
based on a hybrid filter and the symmetrical complementary coefficient.
<em>APIN</em>, <em>51</em>(6), 3899–3916. (<a
href="https://doi.org/10.1007/s10489-020-02028-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of data size and data dimension, feature selection attracts more and more attention. In this paper, we propose a novel feature selection algorithm, namely, Hybrid filter and Symmetrical Complementary Coefficient based Multi-Objective Genetic Algorithm feature selection (HSMOGA). HSMOGA contains a new hybrid filter, Symmetrical Complementary Coefficient which is a well-performed metric of feature interactions proposed recently, and a novel way to limit feature subset’s size. A new Pareto-based ranking function is proposed when solving multi-objective problems. Besides, HSMOGA starts with a novel step called knowledge reserve, which precalculate the knowledge required for fitness function calculation and initial population generation. In this way, HSMOGA is classifier-independent in each generation, and its initial population generation makes full use of the knowledge of data set which makes solutions converge faster. Compared with other GA-based feature selection methods, HSMOGA has a much lower time complexity. According to experimental results, HSMOGA outperforms other nine state-of-art feature selection algorithms including five classic and four more recent algorithms in terms of kappa coefficient, accuracy, and G-mean for the data sets tested.},
  archive      = {J_APIN},
  author       = {Zhang, Rui and Zhang, Zuoquan and Wang, Di and Du, Marui},
  doi          = {10.1007/s10489-020-02028-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3899-3916},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection with multi-objective genetic algorithm based on a hybrid filter and the symmetrical complementary coefficient},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IOSUDA: An unsupervised domain adaptation with input and
output space alignment for joint optic disc and cup segmentation.
<em>APIN</em>, <em>51</em>(6), 3880–3898. (<a
href="https://doi.org/10.1007/s10489-020-01956-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of the optic disc (OD) and the optic cup (OC) is an important step for glaucoma diagnosis. Conventional deep neural network models appear good performance, but degradation when facing domain shift. In this paper, we propose a novel unsupervised domain adaptation framework, called Input and Output Space Unsupervised Domain Adaptation (IOSUDA), to reduce the performance degradation in joint OD and OC segmentation. Our framework achieves both the input and output space alignments. Precisely, we extract the shared content features and the style features of each domain through image translation. The shared content features are input to the segmentation network, then we conduct adversarial learning to promote the similarity of segmentation maps from different domains. Results of the comparative experiments on three different fundus image datasets show that our IOSUDA outperforms the other tested methods in unsupervised domain adaptation. The code of the proposed model is available at https://github.com/EdisonCCL/IOSUDA .},
  archive      = {J_APIN},
  author       = {Chen, Chonglin and Wang, Gang},
  doi          = {10.1007/s10489-020-01956-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3880-3898},
  shortjournal = {Appl. Intell.},
  title        = {IOSUDA: An unsupervised domain adaptation with input and output space alignment for joint optic disc and cup segmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Core-guided method for constraint-based multi-objective
combinatorial optimization. <em>APIN</em>, <em>51</em>(6), 3865–3879.
(<a href="https://doi.org/10.1007/s10489-020-01998-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Objective Combinatorial Optimization(MOCO), which consists of several conflicting objectives to be optimized, finds an ever-increasing number of uses in many real-world applications. In past years, the research of MOCO mainly focuses on evolutionary algorithms. Recently, constraint-based methods come into the view and have been proved to be effective on MOCO problems. This paper builds on the previous works of constraint-based algorithm MCSEnumPD(AAAI-18) using path diversification method. Due to the inadequacy that the original method fails to prune the redundant search space effectively, this paper proposes the definition of infeasible path and develops a novel algorithm that exploits the properties of unsatisfiable cores, referred as CgPDMCS. The approach extends MCSEnumPD algorithm with a core-guided path diversification method, which avoids solving infeasible paths representing the supersets of the unsatisfiable cores. Experimental results show that the novel approach provides further performance gains over the previous constraint-based algorithms, especially for the instances tightly constrained.},
  archive      = {J_APIN},
  author       = {Tian, Naiyu and Ouyang, Dantong and Wang, Yiyuan and Hou, Yimou and Zhang, Liming},
  doi          = {10.1007/s10489-020-01998-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3865-3879},
  shortjournal = {Appl. Intell.},
  title        = {Core-guided method for constraint-based multi-objective combinatorial optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-covariance based affinity for graphs. <em>APIN</em>,
<em>51</em>(6), 3844–3864. (<a
href="https://doi.org/10.1007/s10489-020-01986-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of graph based learning techniques relies on the underlying topological structure and affinity between data points, which are assumed to lie on a smooth Riemannian manifold. However, the assumption of local linearity in a neighborhood does not always hold true. Hence, the Euclidean distance based affinity that determines the graph edges may fail to represent the true connectivity strength between data points. Moreover, the affinity between data points is influenced by the distribution of the data around them and must be considered in the affinity measure. In this paper, we propose two techniques, CCGAL and CCGAN that use cross-covariance based graph affinity (CCGA) to represent the relation between data points in a local region. CCGAL also explores the additional connectivity between data points which share a common local neighborhood. CCGAN considers the influence of respective neighborhoods of the two immediately connected data points, which further enhance the affinity measure. Experimental results of manifold learning on synthetic datasets show that CCGA is able to represent the affinity measure between data points more accurately. This results in better low dimensional representation. Manifold regularization experiments on standard image dataset further indicate that the proposed CCGA based affinity is able to accurately identify and include the influence of the data points and its common neighborhood that increase the classification accuracy. The proposed method outperforms the existing state-of-the-art manifold regularization methods by a significant margin.},
  archive      = {J_APIN},
  author       = {Yadav, Rakesh Kumar and Abhishek and Verma, Shekhar and Venkatesan, S},
  doi          = {10.1007/s10489-020-01986-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3844-3864},
  shortjournal = {Appl. Intell.},
  title        = {Cross-covariance based affinity for graphs},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised learning model for identifying illegal activities
in bitcoin. <em>APIN</em>, <em>51</em>(6), 3824–3843. (<a
href="https://doi.org/10.1007/s10489-020-02048-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its inception in 2009, Bitcoin is mired in controversies for providing a haven for illegal activities. Several types of illicit users hide behind the blanket of anonymity. Uncovering these entities is key for forensic investigations. Current methods utilize machine learning for identifying these illicit entities. However, the existing approaches only focus on a limited category of illicit users. The current paper proposes to address the issue by implementing an ensemble of decision trees for supervised learning. More parameters allow the ensemble model to learn discriminating features that can categorize multiple groups of illicit users from licit users. To evaluate the model, a dataset of 1216 real-life entities on Bitcoin was extracted from the Blockchain. Nine Features were engineered to train the model for segregating 16 different licit-illicit categories of users. The proposed model provided a reliable tool for forensic study. Empirical evaluation of the proposed model vis-a-vis three existing benchmark models was performed to highlight its efficacy. Experiments showed that the specificity and sensitivity of the proposed model were comparable to other models. Due to higher parameters of the ensemble tree model, the classification accuracy was 0.91, with 95% CI - 0.8727, 0.9477. This was better than SVM and Logistic Regression, the two popular models in the literature and comparable to the Random Forest and XGBOOST model. CPU and RAM utilization were also monitored to demonstrate the usefulness of the proposed work for real-world deployment. RAM utilization for the proposed model was higher by 30-45% compared to the other three models. Hence, the proposed model is resource-intensive as it has higher parameters than the other three models. Higher parameters also result in higher accuracy of predictions.},
  archive      = {J_APIN},
  author       = {Nerurkar, Pranav and Bhirud, Sunil and Patel, Dhiren and Ludinard, Romaric and Busnel, Yann and Kumari, Saru},
  doi          = {10.1007/s10489-020-02048-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3824-3843},
  shortjournal = {Appl. Intell.},
  title        = {Supervised learning model for identifying illegal activities in bitcoin},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptively weighted decomposition based multi-objective
evolutionary algorithm. <em>APIN</em>, <em>51</em>(6), 3801–3823. (<a
href="https://doi.org/10.1007/s10489-020-01969-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithm based on Decomposition (MOEA/D) decomposes a multi-objective problem into a number of scalar optimization problems using uniformly distributed weight vectors. However, uniformly distributed weight vectors do not guarantee uniformity of solutions on approximated Pareto-Front. This study proposes an adaptive strategy to modify these scalarizing weights after regular intervals by assessing the crowdedness of solutions using crowding distance operator. Experiments carried out over several benchmark problems with complex Pareto-Fronts show that such a strategy helps in improving the convergence and diversity of solutions on approximated Pareto-Front. Proposed algorithm also shows better performance when compared with other state-of-the-art multi-objective algorithms over most of the benchmark problems.},
  archive      = {J_APIN},
  author       = {Meghwani, Suraj S. and Thakur, Manoj},
  doi          = {10.1007/s10489-020-01969-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3801-3823},
  shortjournal = {Appl. Intell.},
  title        = {Adaptively weighted decomposition based multi-objective evolutionary algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid ant colony system algorithm for solving the ring
star problem. <em>APIN</em>, <em>51</em>(6), 3789–3800. (<a
href="https://doi.org/10.1007/s10489-020-02072-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ring star problem (RSP) involves finding a minimum-length cycle over a set of nodes, for which each node that is non-visited on a cycle is assigned to its closest node on the cycle. The goal is to minimize routing and assignment costs. This study proposes a mathematical model to formulate RSP by using bi-level programming ideas, which consist of one leader and one follower sub-problems. The leader sub-problem refers to constructing a cycle over a subset of nodes in the network, whereas the follower sub-problem is related to assigning the remaining nodes to the nodes on the cycle. An efficient hybrid ant colony system (ACS) algorithm is developed on the basis of the bi-level programming formulations, in which ACS with assignment pheromone is proposed to solve the leader sub-problem. Lastly, the hybrid ACS is tested on 153 benchmark instances. Results show the good performance of the proposed approach.},
  archive      = {J_APIN},
  author       = {Zang, Xiaoning and Jiang, Li and Ding, Bin and Fang, Xiang},
  doi          = {10.1007/s10489-020-02072-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3789-3800},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid ant colony system algorithm for solving the ring star problem},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward perfect neural cascading architecture for grammatical
error correction. <em>APIN</em>, <em>51</em>(6), 3775–3788. (<a
href="https://doi.org/10.1007/s10489-020-01980-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grammatical Error Correction (GEC) is the task of correcting several diverse errors in a text such as spelling, punctuation, morphological, and word choice typos or mistakes. Expressed as a sentence correction task, models such as neural-based sequence-to-sequence (seq2seq) GECs have emerged to offer solutions to the task. However, neural-based seq2seq grammatical error correction models are computationally expensive both in training and in translation inference. Also, they tend to suffer from poor generalization and arrive at inept capabilities due to limited error-corrected data, and thus, incapable of effectively correcting grammar. In this work, we propose the use of Neural Cascading Architecture and different techniques in enhancing the effectiveness of neural sequence-to-sequence grammatical error correction models as inspired by post-editing processes of Neural Machine Translations (NMTs). The findings of our experiments show that, in low-resource NMT models, adapting the presented cascading techniques unleashes performances that is comparable to high setting NMT models, with improvements on state-of-the-art (SOTA) JHU FLuency- Extended GUG corpus (JFLEG) parallel corpus for developing and evaluating GEC model systems. We extensively exploit and evaluate multiple cascading learning strategies and establish best practices toward improving neural seq2seq GECs.},
  archive      = {J_APIN},
  author       = {Acheampong, Kingsley Nketia and Tian, Wenhong},
  doi          = {10.1007/s10489-020-01980-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3775-3788},
  shortjournal = {Appl. Intell.},
  title        = {Toward perfect neural cascading architecture for grammatical error correction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting ontology information in fuzzy SVM social media
profile classification. <em>APIN</em>, <em>51</em>(6), 3757–3774. (<a
href="https://doi.org/10.1007/s10489-020-01939-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, social media like Twitter, Facebook, blogs, and LinkedIn are considered the most used sources of information, while at the same time being the most visited and most used sources of disinformation. These can have a negative impact on several areas and on our minds, hence on our behavior. It is obvious that this disinformation is closely related to the profiles of the authors of this information. The purpose of author profiling is to analyze the texts published by the authors in order to determine their profile category. A wide range of methods for selecting statistical characteristics and machine learning has been studied in recent years in order to automatically classify this information. However, these main methods of selecting statistical characteristics and machine learning used for this purpose have not proven their great performance in the processing of data from social networks. The main contribution of this article consists in integrating the semantic component, which has not been taken into account in the main approaches studied in the literature, as additional functionalities enabling the identification of relevant information. Our hypothesis is that the concepts and the relationships between these concepts tend to have a more coherent correlation with relevant and irrelevant information, and can therefore increase the discriminating power of classifiers. The semantic approach proposed revolves around an ontology combined with the linear SVM classifier and then with the fuzzy SVM classifier. The experimental study carried out, on the different collections of Twitter profiles. On our approach and on the main approaches to the literature that we have studied, as well as the analysis of the results obtained. The results we have clearly show the limits of these studied approaches and confirm the performance of our approach, as well as the efficiency of the integration of the semantic component in the categorization of Twitter profiles.},
  archive      = {J_APIN},
  author       = {Mabrouk, Olfa and Hlaoua, Lobna and Omri, Mohamed Nazih},
  doi          = {10.1007/s10489-020-01939-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3757-3774},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting ontology information in fuzzy SVM social media profile classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable selection for linear regression in large databases:
Exact methods. <em>APIN</em>, <em>51</em>(6), 3736–3756. (<a
href="https://doi.org/10.1007/s10489-020-01927-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes the variable selection problem in the context of Linear Regression for large databases. The problem consists of selecting a small subset of independent variables that can perform the prediction task optimally. This problem has a wide range of applications. One important type of application is the design of composite indicators in various areas (sociology and economics, for example). Other important applications of variable selection in linear regression can be found in fields such as chemometrics, genetics, and climate prediction, among many others. For this problem, we propose a Branch &amp; Bound method. This is an exact method and therefore guarantees optimal solutions. We also provide strategies that enable this method to be applied in very large databases (with hundreds of thousands of cases) in a moderate computation time. A series of computational experiments shows that our method performs well compared to well-known methods in the literature and with commercial software.},
  archive      = {J_APIN},
  author       = {Pacheco, Joaquín and Casado, Silvia},
  doi          = {10.1007/s10489-020-01927-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3736-3756},
  shortjournal = {Appl. Intell.},
  title        = {Variable selection for linear regression in large databases: Exact methods},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved approach to generate generalized basic
probability assignment based on fuzzy sets in the open world and its
application in multi-source information fusion. <em>APIN</em>,
<em>51</em>(6), 3718–3735. (<a
href="https://doi.org/10.1007/s10489-020-01989-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized evidence theory (GET) is an efficient mathematical methodology to deal with multi-source information fusion problems. The GET has the capability of handling uncertain problems even in the open world. In real world applications, some noise or other disturbance often makes the multi-source information have uncertainty. Thus, how to reliably generate the generalized basic probability assignment (GBPA) is a key problem of GET, especially under the noisy environment. Therefore, in this paper, we propose a novel approach to generate GBPA with high robustness by using a cluster method. In this way, the proposed model has the ability to correctly identify the target even under a noisy environment. In particular, the k-means++ algorithm based on triangular fuzzy number is applied to build the GBPA generation model. According to the proposed GBPA generation model, the related similarity degree is calculated for each test instance. After resolving the existing conflicts, the final GBPAs are obtained by using the generalized combination rule. To demonstrate the effectiveness of the proposed method, we compare the proposed approach with related work in the applications of classification and fault diagnosis problems, respectively. Through experimental analysis, it is verified that the proposed approach has the best robustness to generate the GBPAs and maintain a high recognition rate under both noisy and noiseless environments.},
  archive      = {J_APIN},
  author       = {Fan, Yi and Ma, Tianshuo and Xiao, Fuyuan},
  doi          = {10.1007/s10489-020-01989-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3718-3735},
  shortjournal = {Appl. Intell.},
  title        = {An improved approach to generate generalized basic probability assignment based on fuzzy sets in the open world and its application in multi-source information fusion},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparative approach on detecting multi-lingual and
multi-oriented text in natural scene images. <em>APIN</em>,
<em>51</em>(6), 3696–3717. (<a
href="https://doi.org/10.1007/s10489-020-01972-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text helps to convey the intended message to users very accurately. Detecting text from natural scene images for quadrilateral-type and polygon-type datasets is the primary scope of this work. A regression-based method using modified You Only Look Once YOLOv4 network is used for quadrilateral-type datasets. Hyperparameters for training the network are optimized using the Genetic Algorithm which proves to be a suitable candidate than traditional methods. The Pixels-IoU (PIoU) loss is introduced to derive an accurate bounding box and it seems to be productive under various challenging scenarios with high aspect ratios and complex background. This yielded quick results for quadrilateral-type datasets but did not scale for arbitrarily-shaped and curved scene text. So the approach is changed to segmentation based for enhancing the results. This introduces binarization operation in a segmentation network to boost its detection accuracy for polygon-type datasets. The introduction of a new module DiffBiSeg (Differentiable Binarization in Segmentation network) facilitates post-processing and text detection performance by setting the thresholds flexibly for binarization in the segmentation network. The efficacy of both approaches is clearly seen in their respective experimental results.},
  archive      = {J_APIN},
  author       = {Yegnaraman, Aparna and Valli, S.},
  doi          = {10.1007/s10489-020-01972-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3696-3717},
  shortjournal = {Appl. Intell.},
  title        = {A comparative approach on detecting multi-lingual and multi-oriented text in natural scene images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-spike timing-dependent plasticity learning mechanism for
memristive neural networks. <em>APIN</em>, <em>51</em>(6), 3684–3695.
(<a href="https://doi.org/10.1007/s10489-020-01985-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristive neural networks (MNNs) attract the attention of many researchers because memristor can mimic the learning mechanism of biologic neuron, spike timing-dependent plasticity (STDP). While STDP brings huge potentials on many applications for memristive neural networks, it also gives complex calculation process for hardware implement. In this work, a non-STDP learning mechanism is proposed, which is implemented in two common frameworks including feedforward neural network and crossbar. The non-STDP learning mechanism relies on the linear relationship between the value of memristor and area of input spikes, which gives the proposed method a simple calculation process and better hardware compatibility. Experimental results show that the non-STDP learning mechanism can help to achieve good hardware performance in both feedforward neural network and crossbar frameworks. Compared with STDP based memristive neural networks, the proposed method can save 2.19%-24.4% hardware resource (ALMs) and improve 1.56-12.25 MHz processing speed under a set of different network scales. In future, some other complex memristor models with non-STDP learning mechanism should be taken into consideration, which will give more room for practical applications of memristive neural networks.},
  archive      = {J_APIN},
  author       = {Tang, Zhiri and Chen, Yanhua and Wang, Zhihua and Hu, Ruihan and Wu, Edmond Q.},
  doi          = {10.1007/s10489-020-01985-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3684-3695},
  shortjournal = {Appl. Intell.},
  title        = {Non-spike timing-dependent plasticity learning mechanism for memristive neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ultrarobust support vector registration. <em>APIN</em>,
<em>51</em>(6), 3664–3683. (<a
href="https://doi.org/10.1007/s10489-020-01967-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An iterativeframework based on finding point correspondences and estimating the transformation function is widely adopted for nonrigid point set registration. However, correspondences established based on feature descriptors are likely to be inaccurate. In this paper, we propose a novel transformation model that can learn from such correspondences. The model is built by means of weighted support vector (SV) regression with a quadratic ε-insensitive loss and manifold regularization. The loss is insensitive to noise, and the regularization forces the transformation function to preserve the intrinsic geometry of the input data. To assess the confidences of correspondences, we introduce a probabilistic model that is solved using the expectation maximization (EM) algorithm. Then, we input the confidences into the transformation model as instance weights to guide model training. We use the coordinate descent method to solve the transformation model in a reproducing kernel Hilbert space and accelerate its speed by means of sparse approximation. Extensive experiments show that our approach is efficient and outperforms other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yin, Lei and Yu, Chong and Wang, Yuyi and Zou, Bin and Tang, Yuan Yan},
  doi          = {10.1007/s10489-020-01967-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3664-3683},
  shortjournal = {Appl. Intell.},
  title        = {Ultrarobust support vector registration},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain-computer interface for human-multirobot strategic
consensus with a differential world model. <em>APIN</em>,
<em>51</em>(6), 3645–3663. (<a
href="https://doi.org/10.1007/s10489-020-01963-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a distributed multi-robot system, the world model maintained by each robot is inconsistent due to measurement errors from onboard sensors, which will produce different and even incorrect strategies. In this paper, we propose an advanced interaction approach for human-multirobot strategic consensus. First, an opinion dynamics model is used to find the consistent multi-robot strategy, which is not necessarily the best choice due to the inaccurate world model. When the human receives the strategy from the robots, he/she can accept or reject it and reselect the strategy via a brain-computer interface (BCI). Of course, human judgment may be incorrect, and the BCI has false detections. Thus, the robots do not directly accept the human strategy but add it to the opinion dynamics model as a new node and recalculate the final consistent strategy. In addition, we developed a custom-designed simulation system based on the Robot Operating System and Gazebo to realize and evaluate the human-multirobot interaction. The extensive simulation results show that the proposed approach can significantly improve the correct rate of strategy selection compared with robot-only or human-only control, as well as the traditional human-robot interaction methods and other strategic consensus models.},
  archive      = {J_APIN},
  author       = {Liu, Yaru and Dai, Wei and Lu, Huimin and Liu, Yadong and Zhou, Zongtan},
  doi          = {10.1007/s10489-020-01963-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3645-3663},
  shortjournal = {Appl. Intell.},
  title        = {Brain-computer interface for human-multirobot strategic consensus with a differential world model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). An empirical study of ensemble techniques for software
fault prediction. <em>APIN</em>, <em>51</em>(6), 3615–3644. (<a
href="https://doi.org/10.1007/s10489-020-01935-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previously, many researchers have performed analysis of various techniques for the software fault prediction (SFP). Oddly, the majority of such studies have shown the limited prediction capability and their performance for given software fault datasets was not persistent. In contrast to this, recently, ensemble techniques based SFP models have shown promising and improved results across different software fault datasets. However, many new as well as improved ensemble techniques have been introduced, which are not explored for SFP. Motivated by this, the paper performs an investigation on ensemble techniques for SFP. We empirically assess the performance of seven ensemble techniques namely, Dagging, Decorate, Grading, MultiBoostAB, RealAdaBoost, Rotation Forest, and Ensemble Selection. We believe that most of these ensemble techniques are not used before for SFP. We conduct a series of experiments on the benchmark fault datasets and use three distinct classification algorithms, namely, naive Bayes, logistic regression, and J48 (decision tree) as base learners to the ensemble techniques. Experimental analysis revealed that rotation forest with J48 as the base learner achieved the highest precision, recall, and G-mean 1 values of 0.995, 0.994, and 0.994, respectively and Decorate achieved the highest AUC value of 0.986. Further, results of statistical tests showed used ensemble techniques demonstrated a statistically significant difference in their performance among the used ones for SFP. Additionally, the cost-benefit analysis showed that SFP models based on used ensemble techniques might be helpful in saving software testing cost and effort for twenty out of twenty-eight used fault datasets.},
  archive      = {J_APIN},
  author       = {Rathore, Santosh S. and Kumar, Sandeep},
  doi          = {10.1007/s10489-020-01935-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3615-3644},
  shortjournal = {Appl. Intell.},
  title        = {An empirical study of ensemble techniques for software fault prediction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Research progress of zero-shot learning. <em>APIN</em>,
<em>51</em>(6), 3600–3614. (<a
href="https://doi.org/10.1007/s10489-020-02075-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there have been encouraging breakthroughs in supervised learning since the renaissance of deep learning, the recognition of large-scale object classes remains a challenge, especially when some classes have no or few training samples. In this paper, the development of ZSL is reviewed comprehensively, including the evolution, key technologies, mainstream models, current research hotspots and future research directions. First, the evolution process is introduced from the perspectives of multi-shot, few-shot to zero-shot learning. Second, the key techniques of ZSL are analyzed in detail in terms of three aspects: visual feature extraction, semantic representation and visual-semantic mapping. Third, some typical models are interpreted in chronological order. Finally, closely related articles from the last three years are collected to analyze the current research hotspots and list future research directions.},
  archive      = {J_APIN},
  author       = {Sun, Xiaohong and Gu, Jinan and Sun, Hongying},
  doi          = {10.1007/s10489-020-02075-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3600-3614},
  shortjournal = {Appl. Intell.},
  title        = {Research progress of zero-shot learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast, yet robust end-to-end camera pose estimation for
robotic applications. <em>APIN</em>, <em>51</em>(6), 3581–3599. (<a
href="https://doi.org/10.1007/s10489-020-01982-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera pose estimation in robotic applications is paramount. Most of recent algorithms based on convolutional neural networks demonstrate that they are able to predict the camera pose adequately. However, they usually suffer from the computational complexity which prevent them from running in real-time. Additionally, they are not robust to perturbations such as partial occlusion while they have not been trained on such cases beforehand. To study these limitations, this paper presents a fast and robust end-to-end Siamese convolutional model for robot-camera pose estimation. Two colored-frames are fed to the model at the same time, and the generic features are produced mainly based on the transfer learning. The extracted features are then concatenated, from which the relative pose is directly obtained at the output. Furthermore, a new dataset is generated, which includes several videos taken at various situations for the model evaluation. The proposed technique shows a robust performance even in challenging scenes, which have not been rehearsed during the training phase. Through the experiments conducted with an eye-in-hand KUKA robotic arm, the presented network renders fairly accurate results on camera pose estimation despite scene-illumination changes. Also, the pose estimation is conducted with reasonable accuracy in presence of partial camera occlusion. The results are enhanced by defining a new dynamic weighted loss function. The proposed method is further exploited in visual servoing scenario.},
  archive      = {J_APIN},
  author       = {Kamranian, Zahra and Sadeghian, Hamid and Naghsh Nilchi, Ahmad Reza and Mehrandezh, Mehran},
  doi          = {10.1007/s10489-020-01982-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3581-3599},
  shortjournal = {Appl. Intell.},
  title        = {Fast, yet robust end-to-end camera pose estimation for robotic applications},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved limited random walk approach for identification
of overlapping communities in complex networks. <em>APIN</em>,
<em>51</em>(6), 3561–3580. (<a
href="https://doi.org/10.1007/s10489-020-01999-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of community structures in complex networks provides an effective tool for studying the relationships between nodes and revealing the hidden structures. In many real applications, the communities overlap, which means that the nodes can belong to different communities. Recently, random walk methods have been successfully applied for identification of overlapping communities in social networks. However, most of the existing random-walk-based methods require information on the entire network, which is difficult to obtain. Moreover, most networks have gained very large scales with the rapid development of Internet technologies, and it is impractical to scale the existing works for online social networks. Another issue concerning the existing methods is the need for information on the number of communities before the algorithm begins, which is impossible to meet for most real-world networks. To resolve the above issues, a random walk method is proposed in this paper for detection of overlapping community structures in complex networks. The proposed method employs the Markov transition matrix to calculate the transferability of the agent from one node to the others. These probabilities are then used as an attribute vector and feature set for each node, and the feature sets are used for identification of initial communities. Nodes that are available in the feature sets of more than one community, or exhibit high ratios of neighborhood with the other communities are identified as overlapping nodes. The proposed method is examined on various real-world and synthetic datasets. The results reported in terms of various evaluation metrics demonstrate its high efficiency as compared to the existing works.},
  archive      = {J_APIN},
  author       = {Bahadori, Sondos and Moradi, Parham and Zare, Hadi},
  doi          = {10.1007/s10489-020-01999-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3561-3580},
  shortjournal = {Appl. Intell.},
  title        = {An improved limited random walk approach for identification of overlapping communities in complex networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clothing fashion style recognition with design issue graph.
<em>APIN</em>, <em>51</em>(6), 3548–3560. (<a
href="https://doi.org/10.1007/s10489-020-01950-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion style recognition of clothing images facilitates the clothing retrieval and recommendation in E-commerce. It is still a challenging task because the clothing images of same style may have diverse visual appearances. Existing fashion style recognition methods utilize deep neural networks to classify clothing images based on pixel-level or region-level features. However, these features of local regions lack the semantics of fashion issues and make the style recognition sensitive to clothing appearance changing. To tackle this problem, we construct Design Issue Graphs (DIGs) with clothing attributes to form global and semantic representations of fashion styles, and propose a joint fashion style recognition model which consists of two convolutional neural networks based on clothing images and DIGs. The experiments on DeepFashion data sets validate that the proposed model is effective to recognize the clothing fashion styles of diverse appearances. The integration of DIGs into Deep Convolutional Neural Networks (DCNNs) achieves 1.75%, 0.99%, 1.03%, 1.53% improvements for multi-style recognition and 1.22%, 2.06%, 1.58%, 2.20% improvements for certain style recognition in the evaluations of accuracy, precision, recall and F1-score on average respectively.},
  archive      = {J_APIN},
  author       = {Yue, Xiaodong and Zhang, Cheng and Fujita, Hamido and Lv, Ying},
  doi          = {10.1007/s10489-020-01950-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3548-3560},
  shortjournal = {Appl. Intell.},
  title        = {Clothing fashion style recognition with design issue graph},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilinear subspace learning using handcrafted and deep
features for face kinship verification in the wild. <em>APIN</em>,
<em>51</em>(6), 3534–3547. (<a
href="https://doi.org/10.1007/s10489-020-02044-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new multilinear and multiview subspace learning method called Tensor Cross-view Quadratic Discriminant Analysis for face kinship verification in the wild. Most of the existing multilinear subspace learning methods straightforwardly focus on learning a single set of projection matrices, making it difficult to separate different classes. To address this issue, the proposed approach mutually learns multi-view representations for multidimensional cross-view matching. In order to decrease the effect of the within class variations for each mode of the tensor data, the proposed approach integrates the Within Class Covariance Normalization. Moreover, we propose a new tensor face descriptor based on the Gabor wavelets. Besides, we investigate the complementarity of handcrafted and deep face tensor features via their fusion at score level using the Logistic Regression method. Our extensive experiments demonstrate that the proposed kinship verification framework outperforms the state of the art, achieving 95.14%, 91.83% and 93.58% verification accuracies on Cornell KinFace, UB KinFace and TSKinFace face kinship datasets, respectively.},
  archive      = {J_APIN},
  author       = {Bessaoudi, Mohcene and Chouchane, Ammar and Ouamane, Abdelmalik and Boutellaa, Elhocine},
  doi          = {10.1007/s10489-020-02044-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3534-3547},
  shortjournal = {Appl. Intell.},
  title        = {Multilinear subspace learning using handcrafted and deep features for face kinship verification in the wild},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved aspect-category sentiment analysis model for
text sentiment analysis based on RoBERTa. <em>APIN</em>, <em>51</em>(6),
3522–3533. (<a
href="https://doi.org/10.1007/s10489-020-01964-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aspect-category sentiment analysis can provide more and deeper information than the document-level sentiment analysis, because it aims to predict the sentiment polarities of different aspect categories in the same text. The main challenge of aspect-category sentiment analysis is that different aspect categories may present different polarities in the same text. Previous studies combine the Long Short-Term Memory (LSTM) and attention mechanism to predict the sentiment polarity of the given aspect category, but the LSTM-based methods are not really bidirectional text feature extraction methods. In this paper, we propose a multi-task aspect-category sentiment analysis model based on RoBERTa (Robustly Optimized BERT Pre-training Approach). Treating each aspect category as a subtask, we employ the RoBERTa based on deep bidirectional Transformer to extract features from both text and aspect tokens, and apply the cross-attention mechanism to guide the model to focus on the features most relevant to the given aspect category. According to the experimental results, the proposed model outperforms other models for comparison in aspect-category sentiment analysis. Furthermore, the influencing factors of our proposed model are also analyzed.},
  archive      = {J_APIN},
  author       = {Liao, Wenxiong and Zeng, Bi and Yin, Xiuwen and Wei, Pengfei},
  doi          = {10.1007/s10489-020-01964-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3522-3533},
  shortjournal = {Appl. Intell.},
  title        = {An improved aspect-category sentiment analysis model for text sentiment analysis based on RoBERTa},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Usr-mtl: An unsupervised sentence representation learning
framework with multi-task learning. <em>APIN</em>, <em>51</em>(6),
3506–3521. (<a
href="https://doi.org/10.1007/s10489-020-02042-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing the utilized intelligent systems is increasingly important to learn effective text representations, especially extract the sentence features. Numerous previous studies have been concentrated on the task of sentence representation learning based on deep learning approaches. However, the present approaches are mostly proposed with the single task or replied on the labeled corpus when learning the embedding of the sentences. In this paper, we assess the factors in learning sentence representation and propose an efficient unsupervised learning framework with multi-task learning (USR-MTL), in which various text learning tasks are merged into the unitized framework. With the syntactic and semantic features of sentences, three different factors to some extent are reflected in the task of the sentence representation learning that is the wording, or the ordering of the neighbored sentences of a target sentence in other words. Hence, we integrate the word-order learning task, word prediction task, and the sentence-order learning task into the proposed framework to attain meaningful sentence embeddings. Here, the process of sentence embedding learning is reformulated as a multi-task learning framework of the sentence-level task and the two word-level tasks. Moreover, the proposed framework is motivated by an unsupervised learning algorithm utilizing the unlabeled corpus. Based on the experimental results, our approach achieves the state-of-the-art performances on the downstream natural language processing tasks compared to the popular unsupervised representation learning techniques. The experiments on representation visualization and task analysis demonstrate the effectiveness of the tasks in the proposed framework in creating reasonable sentence representations proving the capacity of the proposed unsupervised multi-task framework for the sentence representation learning.},
  archive      = {J_APIN},
  author       = {Xu, Wenshen and Li, Shuangyin and Lu, Yonghe},
  doi          = {10.1007/s10489-020-02042-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3506-3521},
  shortjournal = {Appl. Intell.},
  title        = {Usr-mtl: An unsupervised sentence representation learning framework with multi-task learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale spatiotemporal graph convolution network for air
quality prediction. <em>APIN</em>, <em>51</em>(6), 3491–3505. (<a
href="https://doi.org/10.1007/s10489-020-02054-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution is a serious environmental problem that has attracted much attention. Air quality prediction can provide useful information for urban environmental governance decision-making and residents’ daily health control. However, existing research methods have suffered from a weak ability to capture the spatial correlations and fail to model the long-term temporal dependencies of air quality. To overcome these limitations, we propose a multi-scale spatiotemporal graph convolution network (MST-GCN), which consists of a multi-scale block, several spatial-temporal blocks and a fusion block. We first divide the extracted features into several groups based on their domain categories, and represent the spatial correlations across stations as two graphs. Then we combine the grouped features and the constructed graphs in pairs to form a multi-scale block that feeds into spatial-temporal blocks. Each spatial-temporal block contains a graph convolution layer and a temporal convolution layer, which can model the spatial correlations and long-term temporal dependencies. To capture the group interactions, we use a fusion block to fuse multiple groups. Extensive experiments on a real-world dataset demonstrate that our model achieves the highest performance compared with state-of-the-art and baseline models for air quality prediction.},
  archive      = {J_APIN},
  author       = {Ge, Liang and Wu, Kunyan and Zeng, Yi and Chang, Feng and Wang, Yaqian and Li, Siyu},
  doi          = {10.1007/s10489-020-02054-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3491-3505},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale spatiotemporal graph convolution network for air quality prediction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Correction to: Batch mode active learning via adaptive
criteria weights. <em>APIN</em>, <em>51</em>(6), 3490. (<a
href="https://doi.org/10.1007/s10489-020-02146-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correction to this paper has been published: https://doi.org/10.1007/s10489-020-02146-9},
  archive      = {J_APIN},
  author       = {Li, Hao and Wang, Yongli and Li, Yanchao and Xiao, Gang and Hu, Peng and Zhao, Ruxin},
  doi          = {10.1007/s10489-020-02146-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3490},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Batch mode active learning via adaptive criteria weights},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Batch mode active learning via adaptive criteria weights.
<em>APIN</em>, <em>51</em>(6), 3475–3489. (<a
href="https://doi.org/10.1007/s10489-020-01953-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch mode active learning (BMAL) is absorbed in training reliable classifier with deficient labeled examples by efficiently querying the most valuable unlabeled examples for supervision. In particular, BMAL always selects examples based on the decent-designed criteria, such as (un)certainty and representativeness, etc. However, existing BMAL approaches make a naive trade-off between the criteria and simply combine them with fixed weights, which may yield suboptimal batch selection since the criteria of unlabeled examples would fluctuate after retraining classifier with the newly augmented training set as the learning of classifier progresses. Instead, the weights of the criteria should be assigned properly. To overcome this problem, this paper proposes a novel A daptive C riteria W eights active learning method, abbreviated ACW, which dynamically combines the example selection criteria together to select critical examples for semi-supervised classification. Concretely, we first assign an initial value to each criterion weight, then the current optimal batch is picked from unlabeled pool. Thereafter, the criteria weights are learned and adjusted adaptively by minimizing the objective function with the selected batch at each round. To the best of our knowledge, this work is the first attempt to explore adaptive criteria weights in the context of active learning. The superiority of ACW against the existing state-of-the-art BMAL approaches has also been validated by extensive experimental results on widely used datasets.},
  archive      = {J_APIN},
  author       = {Li, Hao and Wang, Yongli and Li, Yanchao and Xiao, Gang and Hu, Peng and Zhao, Ruxin},
  doi          = {10.1007/s10489-020-01953-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3475-3489},
  shortjournal = {Appl. Intell.},
  title        = {Batch mode active learning via adaptive criteria weights},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Research on image inpainting algorithm of improved GAN
based on two-discriminations networks. <em>APIN</em>, <em>51</em>(6),
3460–3474. (<a
href="https://doi.org/10.1007/s10489-020-01971-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All existing image inpainting methods based on neural network models are affected by structural distortions and blurred textures on visible connectivity, such that overfitting and overlearning phenomena can easily emerge in the image inpainting processing procedure. Accordingly, in an attempt to address the defects of image inpainting algorithm, such as long iteration time, poor adaptability and unsatisfactory repairing effects, the image inpainting algorithm of improved Generative Adversarial Networks based on deep learning method of Two-Discriminations Network has been proposed in the paper. The proposed method uses image inpainting network, global discrimination network and local discrimination network to create a fusion network to apply computational images. In the training procedure of proposed algorithm, the network of image inpainting algorithm uses similar patching method to fill the broken area in image and set it as input training objects, which greatly improves the speed and quality of image inpainting. The global discrimination network uses global structure with marginal information and feature information to judge the completed image, meaning that it comprehensively achieves visible connectivity. As local discrimination network can judge the computational images, it has also been trained with assisted feature patches found on multiple images. Furthermore, the proposed method can enhance the discriminant capability and solve the problem that the image inpainting network has easily been overfitting when the features are too concentrated and limited in number to process. Our results of designed experiments demonstrate that proposed algorithm has better adaptive capability on several image categories than those state-of-the-arts.},
  archive      = {J_APIN},
  author       = {Chen, Yuantao and Zhang, Haopeng and Liu, Linwu and Chen, Xi and Zhang, Qian and Yang, Kai and Xia, Runlong and Xie, Jingbo},
  doi          = {10.1007/s10489-020-01971-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3460-3474},
  shortjournal = {Appl. Intell.},
  title        = {Research on image inpainting algorithm of improved GAN based on two-discriminations networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STA-net: Spatial-temporal attention network for video
salient object detection. <em>APIN</em>, <em>51</em>(6), 3450–3459. (<a
href="https://doi.org/10.1007/s10489-020-01961-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper conducts a systematic study on the role of spatial and temporal attention mechanism in the video salient object detection (VSOD) task. We present a two-stage spatial-temporal attention network, named STA-Net, which makes two major contributions. In the first stage, we devise a Multi-Scale-Spatial-Attention (MSSA) module to reduce calculation cost on non-salient regions while exploiting multi-scale saliency information. Such a sliced attention method offers an individual way to efficiently exploit the high-level features of the network with an enlarged receptive field. The second stage is to propose a Pyramid-Saliency-Shift-Aware (PSSA) module, which puts emphasis on the importance of dynamic object information since it offers a valid shift cue to confirm salient object and capture temporal information. Such a temporal detection module is able to encourage precise salient region detection. Exhaustive experiments show that the proposed STA-Net is effective for video salient object detection task, and achieves compelling performance in comparison with state-of-the-art.},
  archive      = {J_APIN},
  author       = {Bi, Hong-Bo and Lu, Di and Zhu, Hui-Hui and Yang, Li-Na and Guan, Hua-Ping},
  doi          = {10.1007/s10489-020-01961-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3450-3459},
  shortjournal = {Appl. Intell.},
  title        = {STA-net: Spatial-temporal attention network for video salient object detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank and sparse matrix factorization with prior
relations for recommender systems. <em>APIN</em>, <em>51</em>(6),
3435–3449. (<a
href="https://doi.org/10.1007/s10489-020-02023-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth of data has caused users to spend considerable time and effort finding the items they need. Various recommender systems have been created to provide convenience for users. This paper proposes a low-rank and sparse matrix factorization with prior relations (LSMF-PR) recommendation model, which predicts users’ ratings for items through a sum of the learned low-rank matrix and sparse matrix. Thus, unlike traditional matrix factorization approaches, our method can alleviate the error propagation produced by intermediate outputs. The LSMF-PR integrates user relationships and item relationships as prior information. User relationships in different recommendation scenarios are extracted by the corresponding social relations of the users, and item relationships are obtained from the similarity of the item content. Therefore, the sparsity and cold start problems can be effectively reduced with prior information. Furthermore, our model has better interpretability since it reveals the low-rank and sparse features of the ratings. Experiments are conducted on four real-world datasets to validate the performance of our proposed method.},
  archive      = {J_APIN},
  author       = {Wang, Jie and Zhu, Li and Dai, Tao and Xu, Qiannan and Gao, Tianyu},
  doi          = {10.1007/s10489-020-02023-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3435-3449},
  shortjournal = {Appl. Intell.},
  title        = {Low-rank and sparse matrix factorization with prior relations for recommender systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new multi-task learning method with universum data.
<em>APIN</em>, <em>51</em>(6), 3421–3434. (<a
href="https://doi.org/10.1007/s10489-020-01954-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning (MTL) obtains a better classifier than single-task learning (STL) by sharing information between tasks within the multi-task models. Most existing multi-task learning models only focus on the data of the target tasks during training, and ignore the data of non-target tasks that may be contained in the target tasks. In this way, Universum data can be added to classifier training as prior knowledge, and these data do not belong to any indicated categories. In this paper, we address the problem of multi-task learning with Universum data, which improves utilization of non-target task data. We introduce Universum learning to make non-target task data act as prior knowledge and propose a novel multi-task support vector machine with Universum data (U-MTLSVM). Based on the characteristics of MTL, each task have corresponding Universum data to provide prior knowledge. We then utilize the Lagrange method to solve the optimization problem so as to obtain the multi-task classifiers. Then, conduct experiments to compare the performance of the proposed method with several baslines on different data sets. The experimental results demonstrate the effectiveness of the proposed methods for multi-task classification.},
  archive      = {J_APIN},
  author       = {Xiao, Yanshan and Wen, Jing and Liu, Bo},
  doi          = {10.1007/s10489-020-01954-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3421-3434},
  shortjournal = {Appl. Intell.},
  title        = {A new multi-task learning method with universum data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive adjustment strategy for bolt posture errors
based on an improved reinforcement learning algorithm. <em>APIN</em>,
<em>51</em>(6), 3405–3420. (<a
href="https://doi.org/10.1007/s10489-020-01906-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing an intelligent and autonomous system remains a great challenge in the assembly field. Most reinforcement learning (RL) methods are applied to experiments with relatively small state spaces. However, the complicated situation and high-dimensional spaces of the assembly environment cause traditional RL methods to behave poorly in terms of their efficiency and accuracy. In this paper, a model-driven adaptive proximal proximity optimization (MAPPO) method was presented to make the assembly system autonomously rectify the bolt posture error. In the MAPPO method, a probabilistic tree and adaptive reward mechanism were used to improve the calculation efficiency and accuracy of the traditional PPO method. The size of the action space was reduced by establishing a hierarchical logical relationship for each parameter with a probabilistic tree. Based on an adaptive reward mechanism, the phenomenon that the algorithm easily falls into local minima could be improved. Finally, the proposed method was verified based on the Unity simulation engine. The advancement and robustness of the proposed model were also validated by comparing different cases in simulations and experiments. The results revealed that MAPPO has better algorithm efficiency and accuracy compared with other state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Luo, Wentao and Zhang, Jianfu and Feng, Pingfa and Liu, Haochen and Yu, Dingwen and Wu, Zhijun},
  doi          = {10.1007/s10489-020-01906-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3405-3420},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive adjustment strategy for bolt posture errors based on an improved reinforcement learning algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lung cancer detection using enhanced segmentation accuracy.
<em>APIN</em>, <em>51</em>(6), 3391–3404. (<a
href="https://doi.org/10.1007/s10489-020-02046-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is currently one of the most common causes of cancer-related death. Detecting and providing an accurate diagnosis of potentially cancerous lung nodules at an early stage of their development would increase treatment efficacy and so reduce lung cancer mortality. A key barrier to early detection is the absence of noticeable symptoms until the lung cancer has already spread. Diagnosis and screening using non-invasive imaging such as computed tomography (CT) is a potential solution. However, to realize the potential of this approach an accurate automated analysis of these high-resolution images needed. Image segmentation is an important stage of that process. Fuzzy-based image segmentation schemes use the maximum of each row and minimum of each column. Our study developed an algorithm that employs median values measured along each row and column, in addition to the maxima and minima values, and found that this approach increased segmenting accuracy of these images,. In the next phase of analysis, a neuro-fuzzy classifier classified those segmented lung nodules into malignant and benign nodules. Sensitivity, specificity and accuracy were used as performance assessment parameters. The proposed methodology resulted in sensitivity, specificity, precision and accuracy of 100%, 81%, 86% and 90%, respectively, with a reduced false positive rate. In sum, our improved algorithm can give significantly improved accuracy of diagnosis in early-stage patients from CT imaging. Thus, our methodology could contribute to better clinical outcomes for lung cancer patients.},
  archive      = {J_APIN},
  author       = {Akter, Onika and Moni, Mohammad Ali and Islam, Mohammad Mahfuzul and Quinn, Julian M. W. and Kamal, A. H. M.},
  doi          = {10.1007/s10489-020-02046-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3391-3404},
  shortjournal = {Appl. Intell.},
  title        = {Lung cancer detection using enhanced segmentation accuracy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label classification by formulating label-specific
features from simultaneous instance level and feature level.
<em>APIN</em>, <em>51</em>(6), 3375–3390. (<a
href="https://doi.org/10.1007/s10489-020-02008-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning (MLL) trains a classification model from multiple labelled datasets, where each training instance is annotated with a set of class labels simultaneously. Following the binary relevance MLL paradigm, a recently effective spirit is to constructing specific features for each label, instead of training over the original feature space. Existing label-specific methods, however, only consider the information from instance distributions, making the reconstructed features poorly discriminative. In this paper, we propose the generation of Label-spEcific feaTures by simultaneously exploring insTance distributions and fEatuRe distributions, and suggest a new method named Letter. Letter reconstructs two subsets of new features from the instance level and feature level, respectively. More concretely, from the instance level, Letter incorporates a sparse constraint, and from the feature level, we cluster the original features to construct new features as an extension. The combination of these two new feature subsets is the final set of label-specific features. Extensive experiments on a total of 14 benchmark datasets verify the competitive performance of Letter against the existing state-of-the-art MLL methods.},
  archive      = {J_APIN},
  author       = {Guan, Yuanyuan and Li, Wenhui and Zhang, Boxiang and Han, Bing and Ji, Manglai},
  doi          = {10.1007/s10489-020-02008-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3375-3390},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label classification by formulating label-specific features from simultaneous instance level and feature level},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recommending content using side information. <em>APIN</em>,
<em>51</em>(6), 3353–3374. (<a
href="https://doi.org/10.1007/s10489-020-01945-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Filtering methods predict user interests and make recommendations just by using the rating matrix. However, in practice there is extensive side information about users and items, such as the age of the user, the actors in a movie, or the abstract of a journal article. In this paper, a novel model called Collaborative Poisson Factorization with Side-information (CPFS) is proposed which extends CTPF by incorporating richer kinds of side information conditionally as a prior to the model. CPFS is a monolithic hybridization model that combines features from different data sources into a single recommendation algorithm. We develop a Gibbs sampler and also a Variational method with closed-form updates for the inference of CPFS and demonstrate its applicability on a range of datasets including movies, books, academic papers, and travel. The extension improves prediction quality, especially in the cold start scenario. The connections between side information and topics are also intuitive.},
  archive      = {J_APIN},
  author       = {Ravanifard, Rabeh and Buntine, Wray and Mirzaei, Abdolreza},
  doi          = {10.1007/s10489-020-01945-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3353-3374},
  shortjournal = {Appl. Intell.},
  title        = {Recommending content using side information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment analysis for customer relationship management: An
incremental learning approach. <em>APIN</em>, <em>51</em>(6), 3339–3352.
(<a href="https://doi.org/10.1007/s10489-020-01984-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years there has been a significant rethinking of corporate management, which is increasingly based on customer orientation principles. As a matter of fact, customer relationship management processes and systems are ever more popular and crucial to facing today’s business challenges. However, the large number of available customer communication stimuli coming from different (direct and indirect) channels, require automatic language processing techniques to help filter and qualify such stimuli, determine priorities, facilitate the routing of requests and reduce the response times. In this scenario, sentiment analysis plays an important role in measuring customer satisfaction, tracking consumer opinion, interacting with consumers and building customer loyalty. The research described in this paper proposes an approach based on Hierarchical Attention Networks for detecting the sentiment polarity of customer communications. Unlike other existing approaches, after initial training, the defined model can improve over time during system operation using the feedback provided by CRM operators thanks to an integrated incremental learning mechanism. The paper also describes the developed prototype as well as the dataset used for training the model which includes over 30.000 annotated items. The results of two experiments aimed at measuring classifier performance and validating the retraining mechanism are also presented and discussed. In particular, the classifier accuracy turned out to be better than that of other algorithms for the supported languages (macro-averaged f1-score of 0.89 and 0.79 for Italian and English respectively) and the retraining mechanism was able to improve the classification accuracy on new samples without degrading the overall system performance.},
  archive      = {J_APIN},
  author       = {Capuano, Nicola and Greco, Luca and Ritrovato, Pierluigi and Vento, Mario},
  doi          = {10.1007/s10489-020-01984-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3339-3352},
  shortjournal = {Appl. Intell.},
  title        = {Sentiment analysis for customer relationship management: An incremental learning approach},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of incremental support vector regression based
on optimal training subset and improved particle swarm optimization
algorithm in real-time sensor fault diagnosis. <em>APIN</em>,
<em>51</em>(6), 3323–3338. (<a
href="https://doi.org/10.1007/s10489-020-01916-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attracted by the advantages of support vector regression and incremental learning approach, it is proposed in this work that an incremental support vector regression (ISVR) model optimized by particle swarm optimization (PSO) algorithm, and some improvements are made to be more suitable for sensor faults on-line diagnosis. To reducethe training time of ISVR model, an optimal training subset (OTS) method is adopted to reduce the size of training data set of the model. Then, in order to solve the problem of slow convergence of standard PSO algorithm, an incremental PSO (IPSO) algorithm is proposed to accelerate the model convergence through adjusting the inertial weight of each particle, which is gained by comparing the current position of each particle and the optimal position of the last incremental training. Based on the above improvements, a hybrid model, IPSO-OTS-ISVR model is presented finally. Experimental results based on actual operational data of a gas turbine shows that, under the premise of ensuring accuracy, the proposed IPSO-OTS-ISVR has much better performance in model response time and convergence performance over the comparison models. The experimental results based on an UCI data set indicate that the proposed hybrid model can also be extended to solve other prediction problems.},
  archive      = {J_APIN},
  author       = {Zhang, Dongdong and Xiang, Wenguo and Cao, Qiwei and Chen, Shiyi},
  doi          = {10.1007/s10489-020-01916-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3323-3338},
  shortjournal = {Appl. Intell.},
  title        = {Application of incremental support vector regression based on optimal training subset and improved particle swarm optimization algorithm in real-time sensor fault diagnosis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mask-guided SSD for small-object detection. <em>APIN</em>,
<em>51</em>(6), 3311–3322. (<a
href="https://doi.org/10.1007/s10489-020-01949-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small objects is a challenging job for the single-shot multibox detector (SSD) model due to the limited information contained in features and complex background interference. Here, we increased the performance of the SSD for detecting target objects with small size by enhancing detection features with contextual information and introducing a segmentation mask to eliminate background regions. The proposed model is referred to as a “guided SSD” (Mask-SSD) and includes two branches: a detection branch and a segmentation branch. We created a feature-fusion module to allow the detection branch to exploit contextual information for feature maps with large resolution, with the segmentation branch primarily built with atrous convolution to provide additional contextual information to the detection branch. The input of the segmentation branch was also the output of the detection branch, and output segmentation features were fused with detection features in order to classify and locate target objects. Additionally, segmentation features were applied to generate the mask, which was utilized to guide the detection branch to find objects in potential foreground regions. Evaluation of Mask-SSD on the Tsinghua-Tencent 100K and Caltech pedestrian datasets demonstrated its effectiveness at detecting small objects and comparable performance relative to other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Sun, Chang and Ai, Yibo and Wang, Sheng and Zhang, Weidong},
  doi          = {10.1007/s10489-020-01949-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3311-3322},
  shortjournal = {Appl. Intell.},
  title        = {Mask-guided SSD for small-object detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A many-objective optimized task allocation scheduling model
in cloud computing. <em>APIN</em>, <em>51</em>(6), 3293–3310. (<a
href="https://doi.org/10.1007/s10489-020-01887-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characteristics of randomness, running style, and unpredictability of user requirements in the cloud environment, brings great challenges to task scheduling. Meanwhile, the scheduling efficiency of cloud task allocation is an important factor affecting cloud resource systems. Therefore, this paper takes into account the characteristics of tasks, systems and users, a many-objective task scheduling model was constructed in cloud computing. In order to better solve the proposed many-objective task scheduling model, a reference vector guided evolutionary algorithm based on angle-penalty distance of normal distribution (RVEA-NDAPD) is proposed, and compared with the existing standard many-objective evolutionary algorithms (MaOEAs). Simulation results show that the algorithm can effectively improve the performance of the proposed model in cloud computing and obtain a suitable task allocation strategy.},
  archive      = {J_APIN},
  author       = {Xu, Jialei and Zhang, Zhixia and Hu, Zhaoming and Du, Lei and Cai, Xingjuan},
  doi          = {10.1007/s10489-020-01887-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3293-3310},
  shortjournal = {Appl. Intell.},
  title        = {A many-objective optimized task allocation scheduling model in cloud computing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel metaheuristic based on multiverse theory for
optimization problems in emerging systems. <em>APIN</em>,
<em>51</em>(6), 3275–3292. (<a
href="https://doi.org/10.1007/s10489-020-01920-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding an optimal solution for emerging cyber physical systems (CPS) for better efficiency and robustness is one of the major issues. Meta-heuristic is emerging as a promising field of study for solving various optimization problems applicable to different CPS systems. In this paper, we propose a new meta-heuristic algorithm based on Multiverse Theory, named MVA, that can solve NP-hard optimization problems such as non-linear and multi-level programming problems as well as applied optimization problems for CPS systems. MVA algorithm inspires the creation of the next population to be very close to the solution of initial population, which mimics the nature of parallel worlds in multiverse theory. Additionally, MVA distributes the solutions in the feasible region similarly to the nature of big bangs. To illustrate the effectiveness of the proposed algorithm, a set of test problems is implemented and measured in terms of feasibility, efficiency of their solutions and the number of iterations taken in finding the optimum solution. Numerical results obtained from extensive simulations have shown that the proposed algorithm outperforms the state-of-the-art approaches while solving the optimization problems with large feasible regions.},
  archive      = {J_APIN},
  author       = {Hosseini, Eghbal and Ghafoor, Kayhan Zrar and Emrouznejad, Ali and Sadiq, Ali Safaa and Rawat, Danda B.},
  doi          = {10.1007/s10489-020-01920-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3275-3292},
  shortjournal = {Appl. Intell.},
  title        = {Novel metaheuristic based on multiverse theory for optimization problems in emerging systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection based on term frequency deviation rate for
text classification. <em>APIN</em>, <em>51</em>(6), 3255–3274. (<a
href="https://doi.org/10.1007/s10489-020-01937-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a technique to select a subset of the most relevant features for modeling training. In this paper, a new concept of TDR is firstly proposed to improve the classification accuracy. Then, a TDR-based algorithm for text classification is advanced. Finally, the extensive experiments are made on seven datasets (K1a, K1b, WAP, R52, R8, 20NewGroups, and Cade12) for two classifiers of Naive Bayes and Support Vector Machine. The experimental results indicate that the new approach can improve the classification accuracy by an average percent of 7.9%.},
  archive      = {J_APIN},
  author       = {Zhou, Hongfang and Ma, Yiming and Li, Xiang},
  doi          = {10.1007/s10489-020-01937-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3255-3274},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection based on term frequency deviation rate for text classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixture of experts with convolutional and variational
autoencoders for anomaly detection. <em>APIN</em>, <em>51</em>(6),
3241–3254. (<a
href="https://doi.org/10.1007/s10489-020-01944-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focused on the problem of anomaly detection (AD) by means of mixture-of-experts network. Most of the existing AD methods solely based on the reconstruction errors or latent representation using a single low-dimensional manifold are often not ideal for the image objects with complex background. However, modeling the data as a mixture of low-dimensional nonlinear manifolds is natural and promising for the classification of anomalies. In this study to realize the promise of multi-manifold latent information for AD, we propose a mixture of experts ensemble with two convolutional variational autoencoders (CVAEs) and convolution network (MEx-CVAEC) which explicitly learns manifold relationships of data that make use of multiple encoded detections. Additionally, we integrate a linear-based CAE as a gating network which optimizes the expert structures for efficient data characterization based on the manifold of the latent space. In the expert structure the data is re-encoded after each decoder to enhance the latent detection performance and the VAE is used as a core element in the encoder-decoder-encode (EDE) pipeline. To the best of our knowledge, this is the first study suggesting a mixture of CVAEs-based models for AD. The performance of the MEx-CVAE with EDE pipeline which we names as (MEx-CVAEC) compared over two basic MEx-CVAE model with ED pipeline based on logistic regression (MEx-L) and based on CAE (MEx-C) structures. In addition, the performance of the proposed model on three different datasets show the highest average AUC value than that of the state-of-the-art for image anomalies detection task.},
  archive      = {J_APIN},
  author       = {Yu, Qien and Kavitha, Muthu Subash and Kurita, Takio},
  doi          = {10.1007/s10489-020-01944-5},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3241-3254},
  shortjournal = {Appl. Intell.},
  title        = {Mixture of experts with convolutional and variational autoencoders for anomaly detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new efficient decision making algorithm based on
interval-valued fuzzy soft set. <em>APIN</em>, <em>51</em>(6),
3226–3240. (<a
href="https://doi.org/10.1007/s10489-020-01915-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued fuzzy soft set is an extended model of soft set. It is a new mathematical tool that has great advantages in dealing with uncertain information and is proposed by combining soft sets and interval-valued fuzzy sets. The two existing fuzzy decision making algorithms based on interval-valued fuzzy soft sets were given. However, the two existing methods involve the high computational complexity and do not consider the added objects. In order to solve these problems, in this paper, we propose a new efficient decision making algorithm for interval-valued fuzzy soft sets. The comparison results among three methods on one real-life application and 30 synthetic generated datasets show that, the proposed algorithm involves relatively less computation and considers the added objects. Due to relatively less computation, our proposed algorithm has the much higher scalability for the large scale datasets compared with the two existing algorithms. Due to considering the added objects, our proposed algorithm has the much higher flexibility and is beneficial to the extension of interval-valued fuzzy soft set and combination of multiple interval-valued fuzzy soft sets.},
  archive      = {J_APIN},
  author       = {Ma, Xiuqin and Fei, Qinghua and Qin, Hongwu and Li, Huifang and Chen, Wanghu},
  doi          = {10.1007/s10489-020-01915-w},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3226-3240},
  shortjournal = {Appl. Intell.},
  title        = {A new efficient decision making algorithm based on interval-valued fuzzy soft set},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A co-training method based on entropy and multi-criteria.
<em>APIN</em>, <em>51</em>(6), 3212–3225. (<a
href="https://doi.org/10.1007/s10489-020-02014-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-training method is a branch of semi-supervised learning, which improves the performance of classifier through the complementary effect of two views. In co-training algorithm, the selection of unlabeled data often adopts the high confidence degree strategy. Obviously, the higher confidence of data signifies the higher accuracy of prediction. Unfortunately, high confidence selection strategy is not always effective in improving classifier performance. In this paper, a co-training method based on entropy and multi-criteria is proposed. Firstly, the data set is divided into two views with the same amount of information by entropy. Then, the clustering criterion and confidence criterion are adopted to select unlabeled data in view 1 and view 2, respectively. It can solve the problem that high confidence criterion is not always valid. Different choices can better play the complementary role of co-training, thus supplement what the other view does not have. In addition, the role of labeled data is fully considered in multi-criteria in order to select more valuable unlabeled data. Experimental results on several UCI data sets and one artificial data set show the effectiveness of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Lu, Jia and Gong, Yanlu},
  doi          = {10.1007/s10489-020-02014-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3212-3225},
  shortjournal = {Appl. Intell.},
  title        = {A co-training method based on entropy and multi-criteria},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical correlation siamese network for real-time
object tracking. <em>APIN</em>, <em>51</em>(6), 3202–3211. (<a
href="https://doi.org/10.1007/s10489-020-01992-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the influence of deep learning, many trackers have emerged recently. Among them, Siamese network reaches a pleasant balance between accuracy and speed, but its tracking performance still lags behind other trackers. In this paper, we have proposed a Hierarchical Correlation Siamese Network (HC-Siam) for object tracking. The tracker uses convolutional features of each layer to compare the correlation and identifies the position of the tracking object depending on the greatest correlation. Meanwhile, we have designed a Correlation Attention Module (CA-Module). For various objects, this module can assign different weights to the hierarchical correlation and help the network choose the distinct correlation from the hierarchical correlation. Besides, objects’ size and scale constantly varied during tracking, we claimed to use the separate scale factor in the wide and high directions to decrease the deformation of bounding boxes and increase the accuracy of our tracker. On the OTB dataset, the accuracy of HC-Siam is 6.5% higher than the baseline, and the speed of our tracker can reach 85 fps. On the VOT dataset, HC-Siam also has better performance in speed and accuracy.},
  archive      = {J_APIN},
  author       = {Meng, Yu and Deng, Zaixu and Zhao, Kun and Xu, Yan and Liu, Hao},
  doi          = {10.1007/s10489-020-01992-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3202-3211},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical correlation siamese network for real-time object tracking},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling low- and high-order feature interactions with FM
and self-attention network. <em>APIN</em>, <em>51</em>(6), 3189–3201.
(<a href="https://doi.org/10.1007/s10489-020-01951-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Click-Through Rate (CTR) prediction has always been a very popular topic. In many online applications, such as online advertising and product recommendation, a small increase in CTR will bring great returns. However, CTR prediction has always faced several challenges. A large number of users and items and the different sizes of the feature space of different data types lead to high-dimensional and sparse input, and high-order feature interactions rely too much on expert knowledge and are very time-consuming. In this paper, we build a novel model called multi-order interactive features aware factorization machine (MoFM) for CTR prediction. To effectively capturing both low-order and high-order interactive features, three different types of prediction models are integrated, of which logistic regression (LR) and factorization machine (FM) model the original features and 2-order interactive features respectively, and a multi-head self-attention network with residual connections is used to automatically identify high-value high-order feature combinations. There is also an embedding layer in the model to realize a unified embedding processing of different data types, avoiding diversification, sparsity, and high dimensionality of features. Since, feature engineering is not required, we can carry out end-to-end model learning. Experiments on three public datasets show the superiority of the proposed model over the state-of-the-art models, and the flexibility and scalability of the model structure have also been verified.},
  archive      = {J_APIN},
  author       = {Yan, Cairong and Chen, Yizhou and Wan, Yongquan and Wang, Pengwei},
  doi          = {10.1007/s10489-020-01951-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3189-3201},
  shortjournal = {Appl. Intell.},
  title        = {Modeling low- and high-order feature interactions with FM and self-attention network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative attention neural network for multi-domain
sentiment classification. <em>APIN</em>, <em>51</em>(6), 3174–3188. (<a
href="https://doi.org/10.1007/s10489-020-02021-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-domain sentiment classification is a challenging topic in natural language processing, where data from multiple domains are applied to improve the performance of classification. Recently, it has been demonstrated that attention neural networks exhibit powerful performance in this task. In the present study, we propose a collaborative attention neural network (CAN). A self-attention module and domain attention module work together in our approach, where the hidden states generated in the self-attention module are fed into both the domain sub-module and sentiment sub-module in the domain attention module. Compared with other attention neural networks, we use two types of attention modules to conduct the auxiliary and main sentiment classification tasks. The experimental results showed that CAN outperforms other state-of-the-art sentiment classification approaches in terms of the overall accuracy based on both English (Amazon) and Chinese (JD) multi-domain sentiment analysis data sets.},
  archive      = {J_APIN},
  author       = {Yue, Chunyi and Cao, Hanqiang and Xu, Guoping and Dong, Youli},
  doi          = {10.1007/s10489-020-02021-7},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3174-3188},
  shortjournal = {Appl. Intell.},
  title        = {Collaborative attention neural network for multi-domain sentiment classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective dynamic spatiotemporal framework with external
features information for traffic prediction. <em>APIN</em>,
<em>51</em>(6), 3159–3173. (<a
href="https://doi.org/10.1007/s10489-020-02043-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is necessary for management departments to dispatch vehicles and for drivers to avoid congested roads. Many traffic forecasting methods based on deep learning have been proposed in recent years, and their main aim is to solve the problem of spatial dependencies and temporal dynamics. This paper proposes a useful dynamic model to predict the urban traffic volume by combining fully bidirectional LSTM, a complex attention mechanism, and external features, including weather conditions and events. First, we adopt bidirectional LSTM to obtain temporal dependencies of traffic volume dynamically in each layer, which is different from the hybrid methods combining bidirectional and unidirectional approaches. Second, we use a more elaborate attention mechanism to learn short-term and long-term periodic temporal dependencies. Finally, we collect weather condition and event information as external features to further improve the prediction precision. The experimental results show that the proposed model improves the prediction precision by approximately 3-7 percent on the NYC-Taxi and NYC-Bike datasets compared to the most recently developed method and is therefore a useful tool for urban traffic prediction.},
  archive      = {J_APIN},
  author       = {Wang, Jichen and Zhu, Weiguo and Sun, Yongqi and Tian, Chunzi},
  doi          = {10.1007/s10489-020-02043-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3159-3173},
  shortjournal = {Appl. Intell.},
  title        = {An effective dynamic spatiotemporal framework with external features information for traffic prediction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid discriminant embedding with feature selection:
Application to image categorization. <em>APIN</em>, <em>51</em>(6),
3142–3158. (<a
href="https://doi.org/10.1007/s10489-020-02009-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, feature extraction was the focus of many researches due to its usefulness in the machine learning and pattern recognition fields. Feature extraction mainly aims to extract informative representations from the original set of features. This can be carried out using various ways. The proposed method is targeting a hybrid linear feature extraction scheme for supervised multi-class classification problems. Inspired by recent robust sparse LDA and Inter-class sparsity frameworks, we will propose a unifying criterion that is able to retain these two powerful linear discriminant method’s advantages. Thus, the obtained transformation encapsulates two different types of discrimination, the inter-class sparsity and robust Linear Discriminant Analysis with feature selection. We will introduce an iterative alternating minimization scheme in order to estimate the linear transform and the orthogonal matrix. The linear transform is efficiently updated via the steepest descent gradient technique. We will also introduce two initialization schemes for the linear transform. The proposed framework is generic in the sense that it allows the combination and tuning of other linear discriminant embedding methods. According to the experiments which have been carried out on several datasets including faces, objects and digits, the proposed method was able to outperform the competing methods in most cases.},
  archive      = {J_APIN},
  author       = {Khoder, A. and Dornaika, F.},
  doi          = {10.1007/s10489-020-02009-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3142-3158},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid discriminant embedding with feature selection: Application to image categorization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stratified and time-aware sampling based adaptive ensemble
learning for streaming recommendations. <em>APIN</em>, <em>51</em>(6),
3121–3141. (<a
href="https://doi.org/10.1007/s10489-020-01851-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have played an increasingly important role in providing users with tailored suggestions based on their preferences. However, the conventional offline recommender systems cannot handle the ubiquitous data stream well. To address this issue, Streaming Recommender Systems (SRSs) have emerged in recent years, which incrementally train recommendation models on newly received data for effective real-time recommendations. Focusing on new data only benefits addressing concept drift, i.e., the changing user preferences towards items. However, it impedes capturing long-term user preferences. In addition, the commonly existing underload and overload problems should be well tackled for higher accuracy of streaming recommendations. To address these problems, we propose a S tratified and T ime-aware S ampling based A daptive E nsemble L earning framework, called STS-AEL, to improve the accuracy of streaming recommendations. In STS-AEL, we first devise stratified and time-aware sampling to extract representative data from both new data and historical data to address concept drift while capturing long-term user preferences. Also, incorporating the historical data benefits utilizing the idle resources in the underload scenario more effectively. After that, we propose adaptive ensemble learning to efficiently process the overloaded data in parallel with multiple individual recommendation models, and then effectively fuse the results of these models with a sequential adaptive mechanism. Extensive experiments conducted on three real-world datasets demonstrate that STS-AEL, in all the cases, significantly outperforms the state-of-the-art SRSs.},
  archive      = {J_APIN},
  author       = {Zhao, Yan and Wang, Shoujin and Wang, Yan and Liu, Hongwei},
  doi          = {10.1007/s10489-020-01851-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {3121-3141},
  shortjournal = {Appl. Intell.},
  title        = {Stratified and time-aware sampling based adaptive ensemble learning for streaming recommendations},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning-based improved snapshot ensemble technique for
COVID-19 chest x-ray classification. <em>APIN</em>, <em>51</em>(5),
3104–3120. (<a
href="https://doi.org/10.1007/s10489-021-02199-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 has proven to be a deadly virus, and unfortunately, it triggered a worldwide pandemic. Its detection for further treatment poses a severe threat to researchers, scientists, health professionals, and administrators worldwide. One of the daunting tasks during the pandemic for doctors in radiology is the use of chest X-ray or CT images for COVID-19 diagnosis. Time is required to inspect each report manually. While a CT scan is the better standard, an X-ray is still useful because it is cheaper, faster, and more widely used. To diagnose COVID-19, this paper proposes to use a deep learning-based improved Snapshot Ensemble technique for efficient COVID-19 chest X-ray classification. In addition, the proposed method takes advantage of the transfer learning technique using the ResNet-50 model, which is a pre-trained model. The proposed model uses the publicly accessible COVID-19 chest X-ray dataset consisting of 2905 images, which include COVID-19, viral pneumonia, and normal chest X-ray images. For performance evaluation, the model applied the metrics such as AU-ROC, AU-PR, and Jaccard Index. Furthermore, it also obtained a multi-class micro-average of 97% specificity, 95% f1-score, and 95% classification accuracy. The obtained results demonstrate that the performance of the proposed method outperformed those of several existing methods. This method appears to be a suitable and efficient approach for COVID-19 chest X-ray classification.},
  archive      = {J_APIN},
  author       = {P, Samson Anosh Babu and Annavarapu, Chandra Sekhara Rao},
  doi          = {10.1007/s10489-021-02199-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3104-3120},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning-based improved snapshot ensemble technique for COVID-19 chest X-ray classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using artificial intelligence techniques for COVID-19 genome
analysis. <em>APIN</em>, <em>51</em>(5), 3086–3103. (<a
href="https://doi.org/10.1007/s10489-021-02193-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The genome of the novel coronavirus (COVID-19) disease was first sequenced in January 2020, approximately a month after its emergence in Wuhan, capital of Hubei province, China. COVID-19 genome sequencing is critical to understanding the virus behavior, its origin, how fast it mutates, and for the development of drugs/vaccines and effective preventive strategies. This paper investigates the use of artificial intelligence techniques to learn interesting information from COVID-19 genome sequences. Sequential pattern mining (SPM) is first applied on a computer-understandable corpus of COVID-19 genome sequences to see if interesting hidden patterns can be found, which reveal frequent patterns of nucleotide bases and their relationships with each other. Second, sequence prediction models are applied to the corpus to evaluate if nucleotide base(s) can be predicted from previous ones. Third, for mutation analysis in genome sequences, an algorithm is designed to find the locations in the genome sequences where the nucleotide bases are changed and to calculate the mutation rate. Obtained results suggest that SPM and mutation analysis techniques can reveal interesting information and patterns in COVID-19 genome sequences to examine the evolution and variations in COVID-19 strains respectively.},
  archive      = {J_APIN},
  author       = {Nawaz, M. Saqib and Fournier-Viger, Philippe and Shojaee, Abbas and Fujita, Hamido},
  doi          = {10.1007/s10489-021-02193-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3086-3103},
  shortjournal = {Appl. Intell.},
  title        = {Using artificial intelligence techniques for COVID-19 genome analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The analysis of isolation measures for epidemic control of
COVID-19. <em>APIN</em>, <em>51</em>(5), 3074–3085. (<a
href="https://doi.org/10.1007/s10489-021-02239-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a susceptible exposed infectious recovered model (SEIR) with isolation measures to evaluate the COVID-19 epidemic based on the prevention and control policy implemented by the Chinese government on February 23, 2020. According to the Chinese government’s immediate isolation and centralized diagnosis of confirmed cases, and the adoption of epidemic tracking measures on patients to prevent further spread of the epidemic, we divide the population into susceptible, exposed, infectious, quarantine, confirmed and recovered. This paper proposes an SEIR model with isolation measures that simultaneously investigates the infectivity of the incubation period, reflects prevention and control measures and calculates the basic reproduction number of the model. According to the data released by the National Health Commission of the People’s Republic of China, we estimated the parameters of the model and compared the simulation results of the model with actual data. We have considered the trend of the epidemic under different incubation periods of infectious capacity. When the incubation period is not contagious, the peak number of confirmed in the model is 33,870; and when the infectious capacity is 0.1 times the infectious capacity in the infectious period, the peak number of confirmed in the model is 57,950; when the infectious capacity is doubled, the peak number of confirmed will reach 109,300. Moreover, by changing the contact rate in the model, we found that as the intensity of prevention and control measures increase, the peak of the epidemic will come earlier, and the peak number of confirmed will also be significantly reduced. Under extremely strict prevention and control measures, the peak number of confirmed cases has dropped by nearly 50%. In addition, we use the EEMD method to decompose the time series data of the epidemic, and then combine the LSTM model to predict the trend of the epidemic. Compared with the method of directly using LSTM for prediction, more detailed information can be obtained.},
  archive      = {J_APIN},
  author       = {Huang, Bo and Zhu, Yimin and Gao, Yongbin and Zeng, Guohui and Zhang, Juan and Liu, Jin and Liu, Li},
  doi          = {10.1007/s10489-021-02239-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3074-3085},
  shortjournal = {Appl. Intell.},
  title        = {The analysis of isolation measures for epidemic control of COVID-19},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multilingual topic modeling for tracking COVID-19 trends
based on facebook data analysis. <em>APIN</em>, <em>51</em>(5),
3052–3073. (<a
href="https://doi.org/10.1007/s10489-020-02033-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social data has shown important role in tracking, monitoring and risk management of disasters. Indeed, several works focused on the benefits of social data analysis for the healthcare practices and curing domain. Similarly, these data are exploited now for tracking the COVID-19 pandemic but the majority of works exploited Twitter as source. In this paper, we choose to exploit Facebook, rarely used, for tracking the evolution of COVID-19 related trends. In fact, a multilingual dataset covering 7 languages (English (EN), Arabic (AR), Spanish (ES), Italian (IT), German (DE), French (FR) and Japanese (JP)) is extracted from Facebook public posts. The proposal is an analytics process including a data gathering step, pre-processing, LDA-based topic modeling and presentation module using graph structure. Data analysing covers the duration spanned from January 1st, 2020 to May 15, 2020 divided on three periods in cumulative way: first period January-February, second period March-April and the last one to 15 May. The results showed that the extracted topics correspond to the chronological development of what has been circulated around the pandemic and the measures that have been taken according to the various languages under discussion representing several countries.},
  archive      = {J_APIN},
  author       = {Amara, Amina and Hadj Taieb, Mohamed Ali and Ben Aouicha, Mohamed},
  doi          = {10.1007/s10489-020-02033-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3052-3073},
  shortjournal = {Appl. Intell.},
  title        = {Multilingual topic modeling for tracking COVID-19 trends based on facebook data analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Densely connected convolutional networks-based COVID-19
screening model. <em>APIN</em>, <em>51</em>(5), 3044–3051. (<a
href="https://doi.org/10.1007/s10489-020-02149-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensively utilized tool to detect novel coronavirus (COVID-19) is a real-time polymerase chain reaction (RT-PCR). However, RT-PCR kits are costly and consume critical time, around 6 to 9 hours to classify the subjects as COVID-19(+) or COVID-19(-). Due to the less sensitivity of RT-PCR, it suffers from high false-negative results. To overcome these issues, many deep learning models have been implemented in the literature for the early-stage classification of suspected subjects. To handle the sensitivity issue associated with RT-PCR, chest CT scans are utilized to classify the suspected subjects as COVID-19 (+), tuberculosis, pneumonia, or healthy subjects. The extensive study on chest CT scans of COVID-19 (+) subjects reveals that there are some bilateral changes and unique patterns. But the manual analysis from chest CT scans is a tedious task. Therefore, an automated COVID-19 screening model is implemented by ensembling the deep transfer learning models such as Densely connected convolutional networks (DCCNs), ResNet152V2, and VGG16. Experimental results reveal that the proposed ensemble model outperforms the competitive models in terms of accuracy, f-measure, area under curve, sensitivity, and specificity.},
  archive      = {J_APIN},
  author       = {Singh, Dilbag and Kumar, Vijay and Kaur, Manjit},
  doi          = {10.1007/s10489-020-02149-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3044-3051},
  shortjournal = {Appl. Intell.},
  title        = {Densely connected convolutional networks-based COVID-19 screening model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Corona-nidaan: Lightweight deep convolutional neural network
for chest x-ray based COVID-19 infection detection. <em>APIN</em>,
<em>51</em>(5), 3026–3043. (<a
href="https://doi.org/10.1007/s10489-020-01978-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus COVID-19 pandemic is today’s major public health crisis, we have faced since the Second World War. The pandemic is spreading around the globe like a wave, and according to the World Health Organization’s recent report, the number of confirmed cases and deaths are rising rapidly. COVID-19 pandemic has created severe social, economic, and political crises, which in turn will leave long-lasting scars. One of the countermeasures against controlling coronavirus outbreak is specific, accurate, reliable, and rapid detection technique to identify infected patients. The availability and affordability of RT-PCR kits remains a major bottleneck in many countries, while handling COVID-19 outbreak effectively. Recent findings indicate that chest radiography anomalies can characterize patients with COVID-19 infection. In this study, Corona-Nidaan, a lightweight deep convolutional neural network (DCNN), is proposed to detect COVID-19, Pneumonia, and Normal cases from chest X-ray image analysis; without any human intervention. We introduce a simple minority class oversampling method for dealing with imbalanced dataset problem. The impact of transfer learning with pre-trained CNNs on chest X-ray based COVID-19 infection detection is also investigated. Experimental analysis shows that Corona-Nidaan model outperforms prior works and other pre-trained CNN based models. The model achieved 95% accuracy for three-class classification with 94% precision and recall for COVID-19 cases. While studying the performance of various pre-trained models, it is also found that VGG19 outperforms other pre-trained CNN models by achieving 93% accuracy with 87% recall and 93% precision for COVID-19 infection detection. The model is evaluated by screening the COVID-19 infected Indian Patient chest X-ray dataset with good accuracy.},
  archive      = {J_APIN},
  author       = {Chakraborty, Mainak and Dhavale, Sunita Vikrant and Ingole, Jitendra},
  doi          = {10.1007/s10489-020-01978-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3026-3043},
  shortjournal = {Appl. Intell.},
  title        = {Corona-nidaan: Lightweight deep convolutional neural network for chest X-ray based COVID-19 infection detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cropping and attention based approach for masked face
recognition. <em>APIN</em>, <em>51</em>(5), 3012–3025. (<a
href="https://doi.org/10.1007/s10489-020-02100-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global epidemic of COVID-19 makes people realize that wearing a mask is one of the most effective ways to protect ourselves from virus infections, which poses serious challenges for the existing face recognition system. To tackle the difficulties, a new method for masked face recognition is proposed by integrating a cropping-based approach with the Convolutional Block Attention Module (CBAM). The optimal cropping is explored for each case, while the CBAM module is adopted to focus on the regions around eyes. Two special application scenarios, using faces without mask for training to recognize masked faces, and using masked faces for training to recognize faces without mask, have also been studied. Comprehensive experiments on SMFRD, CISIA-Webface, AR and Extend Yela B datasets show that the proposed approach can significantly improve the performance of masked face recognition compared with other state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Li, Yande and Guo, Kun and Lu, Yonggang and Liu, Li},
  doi          = {10.1007/s10489-020-02100-9},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {3012-3025},
  shortjournal = {Appl. Intell.},
  title        = {Cropping and attention based approach for masked face recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new clustering method for the diagnosis of CoVID19 using
medical images. <em>APIN</em>, <em>51</em>(5), 2988–3011. (<a
href="https://doi.org/10.1007/s10489-020-02122-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the spread of COVID-19, there is an urgent need for a fast and reliable diagnostic aid. For the same, literature has witnessed that medical imaging plays a vital role, and tools using supervised methods have promising results. However, the limited size of medical images for diagnosis of CoVID19 may impact the generalization of such supervised methods. To alleviate this, a new clustering method is presented. In this method, a novel variant of a gravitational search algorithm is employed for obtaining optimal clusters. To validate the performance of the proposed variant, a comparative analysis among recent metaheuristic algorithms is conducted. The experimental study includes two sets of benchmark functions, namely standard functions and CEC2013 functions, belonging to different categories such as unimodal, multimodal, and unconstrained optimization functions. The performance comparison is evaluated and statistically validated in terms of mean fitness value, Friedman test, and box-plot. Further, the presented clustering method tested against three different types of publicly available CoVID19 medical images, namely X-ray, CT scan, and Ultrasound images. Experiments demonstrate that the proposed method is comparatively outperforming in terms of accuracy, precision, sensitivity, specificity, and F1-score.},
  archive      = {J_APIN},
  author       = {Mittal, Himanshu and Pandey, Avinash Chandra and Pal, Raju and Tripathi, Ashish},
  doi          = {10.1007/s10489-020-02122-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2988-3011},
  shortjournal = {Appl. Intell.},
  title        = {A new clustering method for the diagnosis of CoVID19 using medical images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convalescent-plasma-transfusion intelligent framework for
rescuing COVID-19 patients across centralised/decentralised telemedicine
hospitals based on AHP-group TOPSIS and matching component.
<em>APIN</em>, <em>51</em>(5), 2956–2987. (<a
href="https://doi.org/10.1007/s10489-020-02169-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As coronavirus disease 2019 (COVID-19) spreads across the world, the transfusion of efficient convalescent plasma (CP) to the most critical patients can be the primary approach to preventing the virus spread and treating the disease, and this strategy is considered as an intelligent computing concern. In providing an automated intelligent computing solution to select the appropriate CP for the most critical patients with COVID-19, two challenges aspects are bound to be faced: (1) distributed hospital management aspects (including scalability and management issues for prioritising COVID-19 patients and donors simultaneously), and (2) technical aspects (including the lack of COVID-19 dataset availability of patients and donors and an accurate matching process amongst them considering all blood types). Based on previous reports, no study has provided a solution for CP-transfusion-rescue intelligent framework during this pandemic that has addressed said challenges and issues. This study aimed to propose a novel CP-transfusion intelligent framework for rescuing COVID-19 patients across centralised/decentralised telemedicine hospitals based on the matching component process to provide an efficient CP from eligible donors to the most critical patients using multicriteria decision-making (MCDM) methods. A dataset, including COVID-19 patients/donors that have met the important criteria in the virology field, must be augmented to improve the developed framework. Four consecutive phases conclude the methodology. In the first phase, a new COVID-19 dataset is generated on the basis of medical-reference ranges by specialised experts in the virology field. The simulation data are classified into 80 patients and 80 donors on the basis of the five biomarker criteria with four blood types (i.e., A, B, AB, and O) and produced for COVID-19 case study. In the second phase, the identification scenario of patient/donor distributions across four centralised/decentralised telemedicine hospitals is identified ‘as a proof of concept’. In the third phase, three stages are conducted to develop a CP-transfusion-rescue framework. In the first stage, two decision matrices are adopted and developed on the basis of the five ‘serological/protein biomarker’ criteria for the prioritisation of patient/donor lists. In the second stage, MCDM techniques are analysed to adopt individual and group decision making based on integrated AHP-TOPSIS as suitable methods. In the third stage, the intelligent matching components amongst patients/donors are developed on the basis of four distinct rules. In the final phase, the guideline of the objective validation steps is reported. The intelligent framework implies the benefits and strength weights of biomarker criteria to the priority configuration results and can obtain efficient CPs for the most critical patients. The execution of matching components possesses the scalability and balancing presentation within centralised/decentralised hospitals. The objective validation results indicate that the ranking is valid.},
  archive      = {J_APIN},
  author       = {Mohammed, Thura J. and Albahri, A. S. and Zaidan, A. A. and Albahri, O. S. and Al-Obaidi, Jameel R. and Zaidan, B. B. and Larbani, Moussa and Mohammed, R. T. and Hadi, Suha M.},
  doi          = {10.1007/s10489-020-02169-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2956-2987},
  shortjournal = {Appl. Intell.},
  title        = {Convalescent-plasma-transfusion intelligent framework for rescuing COVID-19 patients across centralised/decentralised telemedicine hospitals based on AHP-group TOPSIS and matching component},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A method based on graph theory and three way decisions to
evaluate critical regions in epidemic diffusion: <em>APIN</em>,
<em>51</em>(5), 2939–2955. (<a
href="https://doi.org/10.1007/s10489-020-02173-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper reports the results of an analysis of COVID-19 diffusion in Italy. The analysis was carried out with a new method based on the combined use of a 3 Way Decisions model and graph theory. Specifically, the data about infected people in the Italian regions is assessed by means of an evaluation function which allows the tri-partitioning of Italy and the identification of high, medium or low critical regions. The tri-partition is performed, along the temporal evolution of the COVID-19 diffusion, by calculating two threshold values which take into account the containment actions that, from time to time, the decision makers have implemented. The effects of a containment action are related to a reduction in the centrality value of a region. To estimate the effect of containment actions, we evaluated two approaches. The first is based on a uniform reduction in the centrality values of the regions, the second estimates the effects of containment actions starting from the mobility changes data provided by the Google Community Mobility reports. The results of our evaluation based on real data of the COVID-19 diffusion in Italy are encouraging and represent a good starting point for future extensions of the method.},
  archive      = {J_APIN},
  author       = {Gaeta, Angelo and Loia, Vincenzo and Orciuoli, Francesco},
  doi          = {10.1007/s10489-020-02173-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2939-2955},
  shortjournal = {Appl. Intell.},
  title        = {A method based on graph theory and three way decisions to evaluate critical regions in epidemic diffusion: },
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent system for COVID-19 prognosis: A
state-of-the-art survey. <em>APIN</em>, <em>51</em>(5), 2908–2938. (<a
href="https://doi.org/10.1007/s10489-020-02102-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This 21st century is notable for experiencing so many disturbances at economic, social, cultural, and political levels in the entire world. The outbreak of novel corona virus 2019 (COVID-19) has been treated as a Public Health crisis of global Concern by the World Health Organization (WHO). Various outbreak models for COVID-19 are being utilized by researchers throughout the world to get well-versed decisions and impose significant control measures. Amid the standard methods for COVID-19 worldwide epidemic prediction, easy statistical, as well as epidemiological methods have got more consideration by researchers and authorities. One main difficulty in controlling the spreading of COVID-19 is the inadequacy and lack of medical tests for detecting as well as identifying a solution. To solve this problem, a few statistical-based advances are being enhanced and turn into a partial resolution up-to some level. To deal with the challenges of the medical field, a broad range of intelligent based methods, frameworks, and equipment have been recommended by Machine Learning (ML) and Deep Learning. As ML and DL have the ability of identifying and predicting patterns in complex large datasets, they are recognized as a suitable procedure for producing effective solutions for the diagnosis of COVID-19. In this paper, a perspective research has been conducted in the applicability of intelligent systems such as ML, DL and others in solving COVID-19 related outbreak issues. The main intention behind this study is (i) to understand the importance of intelligent approaches such as ML and DL for COVID-19 pandemic, (ii) discussing the efficiency and impact of these methods in the prognosis of COVID-19, (iii) the growth in the development of type of ML and advanced ML methods for COVID-19 prognosis,(iv) analyzing the impact of data types and the nature of data along with challenges in processing the data for COVID-19,(v) to focus on some future challenges in COVID-19 prognosis to inspire the researchers for innovating and enhancing their knowledge and research on other impacted sectors due to COVID-19.},
  archive      = {J_APIN},
  author       = {Nayak, Janmenjoy and Naik, Bighnaraj and Dinesh, Paidi and Vakula, Kanithi and Rao, B. Kameswara and Ding, Weiping and Pelusi, Danilo},
  doi          = {10.1007/s10489-020-02102-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2908-2938},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent system for COVID-19 prognosis: A state-of-the-art survey},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). “Fast deep learning computer-aided diagnosis of COVID-19
based on digital chest x-ray images.” <em>APIN</em>, <em>51</em>(5),
2890–2907. (<a
href="https://doi.org/10.1007/s10489-020-02076-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) is a novel harmful respiratory disease that has rapidly spread worldwide. At the end of 2019, COVID-19 emerged as a previously unknown respiratory disease in Wuhan, Hubei Province, China. The world health organization (WHO) declared the coronavirus outbreak a pandemic in the second week of March 2020. Simultaneous deep learning detection and classification of COVID-19 based on the full resolution of digital X-ray images is the key to efficiently assisting patients by enabling physicians to reach a fast and accurate diagnosis decision. In this paper, a simultaneous deep learning computer-aided diagnosis (CAD) system based on the YOLO predictor is proposed that can detect and diagnose COVID-19, differentiating it from eight other respiratory diseases: atelectasis, infiltration, pneumothorax, masses, effusion, pneumonia, cardiomegaly, and nodules. The proposed CAD system was assessed via five-fold tests for the multi-class prediction problem using two different databases of chest X-ray images: COVID-19 and ChestX-ray8. The proposed CAD system was trained with an annotated training set of 50,490 chest X-ray images. The regions on the entire X-ray images with lesions suspected of being due to COVID-19 were simultaneously detected and classified end-to-end via the proposed CAD predictor, achieving overall detection and classification accuracies of 96.31% and 97.40%, respectively. Most test images from patients with confirmed COVID-19 and other respiratory diseases were correctly predicted, achieving average intersection over union (IoU) greater than 90%. Applying deep learning regularizers of data balancing and augmentation improved the COVID-19 diagnostic performance by 6.64% and 12.17% in terms of the overall accuracy and the F1-score, respectively. It is feasible to achieve a diagnosis based on individual chest X-ray images with the proposed CAD system within 0.0093 s. Thus, the CAD system presented in this paper can make a prediction at the rate of 108 frames/s (FPS), which is close to real-time. The proposed deep learning CAD system can reliably differentiate COVID-19 from other respiratory diseases. The proposed deep learning model seems to be a reliable tool that can be used to practically assist health care systems, patients, and physicians.},
  archive      = {J_APIN},
  author       = {Al-antari, Mugahed A. and Hua, Cam-Hao and Bang, Jaehun and Lee, Sungyoung},
  doi          = {10.1007/s10489-020-02076-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2890-2907},
  shortjournal = {Appl. Intell.},
  title        = {“Fast deep learning computer-aided diagnosis of COVID-19 based on digital chest x-ray images”},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic approach based on CNN architecture to detect
covid-19 disease from chest x-ray images. <em>APIN</em>, <em>51</em>(5),
2864–2889. (<a
href="https://doi.org/10.1007/s10489-020-02010-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel coronavirus (COVID-19) is started from Wuhan (City in China), and is rapidly spreading among people living in other countries. Today, around 215 countries are affected by COVID-19 disease. WHO announced approximately number of cases 11,274,600 worldwide. Due to rapidly rising cases daily in the hospitals, there are a limited number of resources available to control COVID-19 disease. Therefore, it is essential to develop an accurate diagnosis of COVID-19 disease. Early diagnosis of COVID-19 patients is important for preventing the disease from spreading to others. In this paper, we proposed a deep learning based approach that can differentiate COVID- 19 disease patients from viral pneumonia, bacterial pneumonia, and healthy (normal) cases. In this approach, deep transfer learning is adopted. We used binary and multi-class dataset which is categorized in four types for experimentation: (i) Collection of 728 X-ray images including 224 images with confirmed COVID-19 disease and 504 normal condition images (ii) Collection of 1428 X-ray images including 224 images with confirmed COVID-19 disease, 700 images with confirmed common bacterial pneumonia, and 504 normal condition images. (iii) Collections of 1442 X- ray images including 224 images with confirmed COVID-19 disease, 714 images with confirmed bacterial and viral pneumonia, and 504 images of normal conditions (iv) Collections of 5232 X- ray images including 2358 images with confirmed bacterial and 1345 with viral pneumonia, and 1346 images of normal conditions. In this paper, we have used nine convolutional neural network based architecture (AlexNet, GoogleNet, ResNet-50, Se-ResNet-50, DenseNet121, Inception V4, Inception ResNet V2, ResNeXt-50, and Se-ResNeXt-50). Experimental results indicate that the pre trained model Se-ResNeXt-50 achieves the highest classification accuracy of 99.32% for binary class and 97.55% for multi-class among all pre-trained models.},
  archive      = {J_APIN},
  author       = {Hira, Swati and Bai, Anita and Hira, Sanchit},
  doi          = {10.1007/s10489-020-02010-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2864-2889},
  shortjournal = {Appl. Intell.},
  title        = {An automatic approach based on CNN architecture to detect covid-19 disease from chest X-ray images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based VGG-16 model for COVID-19 chest x-ray image
classification. <em>APIN</em>, <em>51</em>(5), 2850–2863. (<a
href="https://doi.org/10.1007/s10489-020-02055-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis (CAD) methods such as Chest X-rays (CXR)-based method is one of the cheapest alternative options to diagnose the early stage of COVID-19 disease compared to other alternatives such as Polymerase Chain Reaction (PCR), Computed Tomography (CT) scan, and so on. To this end, there have been few works proposed to diagnose COVID-19 by using CXR-based methods. However, they have limited performance as they ignore the spatial relationships between the region of interests (ROIs) in CXR images, which could identify the likely regions of COVID-19’s effect in the human lungs. In this paper, we propose a novel attention-based deep learning model using the attention module with VGG-16. By using the attention module, we capture the spatial relationship between the ROIs in CXR images. In the meantime, by using an appropriate convolution layer (4th pooling layer) of the VGG-16 model in addition to the attention module, we design a novel deep learning model to perform fine-tuning in the classification process. To evaluate the performance of our method, we conduct extensive experiments by using three COVID-19 CXR image datasets. The experiment and analysis demonstrate the stable and promising performance of our proposed method compared to the state-of-the-art methods. The promising classification performance of our proposed method indicates that it is suitable for CXR image classification in COVID-19 diagnosis.},
  archive      = {J_APIN},
  author       = {Sitaula, Chiranjibi and Hossain, Mohammad Belayet},
  doi          = {10.1007/s10489-020-02055-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2850-2863},
  shortjournal = {Appl. Intell.},
  title        = {Attention-based VGG-16 model for COVID-19 chest X-ray image classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FaNet: Fast assessment network for the novel coronavirus
(COVID-19) pneumonia based on 3D CT imaging and clinical symptoms.
<em>APIN</em>, <em>51</em>(5), 2838–2849. (<a
href="https://doi.org/10.1007/s10489-020-01965-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus (COVID-19) pneumonia has become a serious health challenge in countries worldwide. Many radiological findings have shown that X-ray and CT imaging scans are an effective solution to assess disease severity during the early stage of COVID-19. Many artificial intelligence (AI)-assisted diagnosis works have rapidly been proposed to focus on solving this classification problem and determine whether a patient is infected with COVID-19. Most of these works have designed networks and applied a single CT image to perform classification; however, this approach ignores prior information such as the patient’s clinical symptoms. Second, making a more specific diagnosis of clinical severity, such as slight or severe, is worthy of attention and is conducive to determining better follow-up treatments. In this paper, we propose a deep learning (DL) based dual-tasks network, named FaNet, that can perform rapid both diagnosis and severity assessments for COVID-19 based on the combination of 3D CT imaging and clinical symptoms. Generally, 3D CT image sequences provide more spatial information than do single CT images. In addition, the clinical symptoms can be considered as prior information to improve the assessment accuracy; these symptoms are typically quickly and easily accessible to radiologists. Therefore, we designed a network that considers both CT image information and existing clinical symptom information and conducted experiments on 416 patient data, including 207 normal chest CT cases and 209 COVID-19 confirmed ones. The experimental results demonstrate the effectiveness of the additional symptom prior information as well as the network architecture designing. The proposed FaNet achieved an accuracy of 98.28% on diagnosis assessment and 94.83% on severity assessment for test datasets. In the future, we will collect more covid-CT patient data and seek further improvement.},
  archive      = {J_APIN},
  author       = {Huang, Zhenxing and Liu, Xinfeng and Wang, Rongpin and Zhang, Mudan and Zeng, Xianchun and Liu, Jun and Yang, Yongfeng and Liu, Xin and Zheng, Hairong and Liang, Dong and Hu, Zhanli},
  doi          = {10.1007/s10489-020-01965-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2838-2849},
  shortjournal = {Appl. Intell.},
  title        = {FaNet: Fast assessment network for the novel coronavirus (COVID-19) pneumonia based on 3D CT imaging and clinical symptoms},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEIAQRDT model for the spread of novel coronavirus
(COVID-19): A case study in india. <em>APIN</em>, <em>51</em>(5),
2818–2837. (<a
href="https://doi.org/10.1007/s10489-020-01929-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a global pandemic declared by WHO. This pandemic requires the execution of planned control strategies, incorporating quarantine, self-isolation, and tracing of asymptomatic cases. Mathematical modeling is one of the prominent techniques for predicting and controlling the spread of COVID-19. The predictions of earlier proposed epidemiological models (e.g. SIR, SEIR, SIRD, SEIRD, etc.) are not much accurate due to lack of consideration for transmission of the epidemic during the latent period. Moreover, it is important to classify infected individuals to control this pandemic. Therefore, a new mathematical model is proposed to incorporate infected individuals based on whether they have symptoms or not. This model forecasts the number of cases more accurately, which may help in better planning of control strategies. The model consists of eight compartments: susceptible (S), exposed (E), infected (I), asymptomatic (A), quarantined (Q), recovered (R), deaths (D), and insusceptible (T), accumulatively named as SEIAQRDT. This model is employed to predict the pandemic results for India and its majorly affected states. The estimated number of cases using the SEIAQRDT model is compared with SIRD, SEIR, and LSTM models. The relative error square analysis is used to verify the accuracy of the proposed model. The simulation is done on real datasets and results show the effectiveness of the proposed approach. These results may help the government and individuals to make the planning in this pandemic situation.},
  archive      = {J_APIN},
  author       = {Kumari, Preety and Singh, Harendra Pal and Singh, Swarn},
  doi          = {10.1007/s10489-020-01929-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2818-2837},
  shortjournal = {Appl. Intell.},
  title        = {SEIAQRDT model for the spread of novel coronavirus (COVID-19): A case study in india},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stacked-autoencoder-based model for COVID-19 diagnosis on CT
images. <em>APIN</em>, <em>51</em>(5), 2805–2817. (<a
href="https://doi.org/10.1007/s10489-020-02002-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the outbreak of COVID-19, medical imaging such as computed tomography (CT) based diagnosis is proved to be an effective way to fight against the rapid spread of the virus. Therefore, it is important to study computerized models for infectious detection based on CT imaging. New deep learning-based approaches are developed for CT assisted diagnosis of COVID-19. However, most of the current studies are based on a small size dataset of COVID-19 CT images as there are less publicly available datasets for patient privacy reasons. As a result, the performance of deep learning-based detection models needs to be improved based on a small size dataset. In this paper, a stacked autoencoder detector model is proposed to greatly improve the performance of the detection models such as precision rate and recall rate. Firstly, four autoencoders are constructed as the first four layers of the whole stacked autoencoder detector model being developed to extract better features of CT images. Secondly, the four autoencoders are cascaded together and connected to the dense layer and the softmax classifier to constitute the model. Finally, a new classification loss function is constructed by superimposing reconstruction loss to enhance the detection accuracy of the model. The experiment results show that our model is performed well on a small size COVID-2019 CT image dataset. Our model achieves the average accuracy, precision, recall, and F1-score rate of 94.7%, 96.54%, 94.1%, and 94.8%, respectively. The results reflect the ability of our model in discriminating COVID-19 images which might help radiologists in the diagnosis of suspected COVID-19 patients.},
  archive      = {J_APIN},
  author       = {Li, Daqiu and Fu, Zhangjie and Xu, Jun},
  doi          = {10.1007/s10489-020-02002-w},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2805-2817},
  shortjournal = {Appl. Intell.},
  title        = {Stacked-autoencoder-based model for COVID-19 diagnosis on CT images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis of a large-scale COVID-19 tweets
dataset. <em>APIN</em>, <em>51</em>(5), 2790–2804. (<a
href="https://doi.org/10.1007/s10489-020-02029-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As of July 17, 2020, more than thirteen million people have been diagnosed with the Novel Coronavirus (COVID-19), and half a million people have already lost their lives due to this infectious disease. The World Health Organization declared the COVID-19 outbreak as a pandemic on March 11, 2020. Since then, social media platforms have experienced an exponential rise in the content related to the pandemic. In the past, Twitter data have been observed to be indispensable in the extraction of situational awareness information relating to any crisis. This paper presents COV19Tweets Dataset (Lamsal 2020a), a large-scale Twitter dataset with more than 310 million COVID-19 specific English language tweets and their sentiment scores. The dataset’s geo version, the GeoCOV19Tweets Dataset (Lamsal 2020b), is also presented. The paper discusses the datasets’ design in detail, and the tweets in both the datasets are analyzed. The datasets are released publicly, anticipating that they would contribute to a better understanding of spatial and temporal dimensions of the public discourse related to the ongoing pandemic. As per the stats, the datasets (Lamsal 2020a, 2020b) have been accessed over 74.5k times, collectively.},
  archive      = {J_APIN},
  author       = {Lamsal, Rabindra},
  doi          = {10.1007/s10489-020-02029-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2790-2804},
  shortjournal = {Appl. Intell.},
  title        = {Design and analysis of a large-scale COVID-19 tweets dataset},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural network to detect COVID-19: One architecture for
both CT scans and chest x-rays. <em>APIN</em>, <em>51</em>(5),
2777–2789. (<a
href="https://doi.org/10.1007/s10489-020-01943-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since December 2019, the novel COVID-19’s spread rate is exponential, and AI-driven tools are used to prevent further spreading [1]. They can help predict, screen, and diagnose COVID-19 positive cases. Within this scope, imaging with Computed Tomography (CT) scans and Chest X-rays (CXRs) are widely used in mass triage situations. In the literature, AI-driven tools are limited to one data type either CT scan or CXR to detect COVID-19 positive cases. Integrating multiple data types could possibly provide more information in detecting anomaly patterns due to COVID-19. Therefore, in this paper, we engineered a Convolutional Neural Network (CNN) -tailored Deep Neural Network (DNN) that can collectively train/test both CT scans and CXRs. In our experiments, we achieved an overall accuracy of 96.28% (AUC = 0.9808 and false negative rate = 0.0208). Further, major existing DNNs provided coherent results while integrating CT scans and CXRs to detect COVID-19 positive cases.},
  archive      = {J_APIN},
  author       = {Mukherjee, Himadri and Ghosh, Subhankar and Dhar, Ankita and Obaidullah, Sk Md and Santosh, K. C. and Roy, Kaushik},
  doi          = {10.1007/s10489-020-01943-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2777-2789},
  shortjournal = {Appl. Intell.},
  title        = {Deep neural network to detect COVID-19: One architecture for both CT scans and chest X-rays},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Differentiation of COVID-19 conditions in
planar chest radiographs using optimized convolutional neural networks.
<em>APIN</em>, <em>51</em>(5), 2776. (<a
href="https://doi.org/10.1007/s10489-021-02343-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Figure 4 in the original article unfortunately contained an error.},
  archive      = {J_APIN},
  author       = {Govindarajan, Satyavratan and Swaminathan, Ramakrishnan},
  doi          = {10.1007/s10489-021-02343-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2776},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Differentiation of COVID-19 conditions in planar chest radiographs using optimized convolutional neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Differentiation of COVID-19 conditions in planar chest
radiographs using optimized convolutional neural networks.
<em>APIN</em>, <em>51</em>(5), 2764–2775. (<a
href="https://doi.org/10.1007/s10489-020-01941-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an attempt has been made to differentiate Novel Coronavirus-2019 (COVID-19) conditions from healthy subjects in Chest radiographs using a simplified end-to-end Convolutional Neural Network (CNN) model and occlusion sensitivity maps. Early detection and faster automated screening of the COVID-19 patients is essential. For this, the images are considered from publicly available datasets. Significant biomarkers representing critical image features are extracted from CNN by experimentally investigating on cross-validation methods and hyperparameter settings. The performance of the network is evaluated using standard metrics. Perturbation based occlusion sensitivity maps are employed on the features obtained from the classification model to visualise the localization of abnormal areas. Results demonstrate that the simplified CNN model with optimised parameters is able to extract significant features with a sensitivity of 97.35% and F-measure of 96.71% to detect COVID-19 images. The algorithm achieves an Area Under the Curve-Receiver Operating Characteristic score of 99.4% with Matthews correlation coefficient of 0.93. High value of Diagnostic odds ratio is also obtained. Occlusion sensitivity maps provide precise localization of abnormal regions by identifying COVID-19 conditions. As early detection through chest radiographic images are useful for automated screening of the disease, this method appears to be clinically relevant in providing a visual diagnostic solution using a simplified and efficient model.},
  archive      = {J_APIN},
  author       = {Govindarajan, Satyavratan and Swaminathan, Ramakrishnan},
  doi          = {10.1007/s10489-020-01941-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2764-2775},
  shortjournal = {Appl. Intell.},
  title        = {Differentiation of COVID-19 conditions in planar chest radiographs using optimized convolutional neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new deep learning pipeline to detect covid-19 on chest
x-ray images using local binary pattern, dual tree complex wavelet
transform and convolutional neural networks. <em>APIN</em>,
<em>51</em>(5), 2740–2763. (<a
href="https://doi.org/10.1007/s10489-020-02019-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, which aims at early diagnosis of Covid-19 disease using X-ray images, the deep-learning approach, a state-of-the-art artificial intelligence method, was used, and automatic classification of images was performed using convolutional neural networks (CNN). In the first training-test data set used in the study, there were 230 X-ray images, of which 150 were Covid-19 and 80 were non-Covid-19, while in the second training-test data set there were 476 X-ray images, of which 150 were Covid-19 and 326 were non-Covid-19. Thus, classification results have been provided for two data sets, containing predominantly Covid-19 images and predominantly non-Covid-19 images, respectively. In the study, a 23-layer CNN architecture and a 54-layer CNN architecture were developed. Within the scope of the study, the results were obtained using chest X-ray images directly in the training-test procedures and the sub-band images obtained by applying dual tree complex wavelet transform (DT-CWT) to the above-mentioned images. The same experiments were repeated using images obtained by applying local binary pattern (LBP) to the chest X-ray images. Within the scope of the study, four new result generation pipeline algorithms having been put forward additionally, it was ensured that the experimental results were combined and the success of the study was improved. In the experiments carried out in this study, the training sessions were carried out using the k-fold cross validation method. Here the k value was chosen as 23 for the first and second training-test data sets. Considering the average highest results of the experiments performed within the scope of the study, the values of sensitivity, specificity, accuracy, F-1 score, and area under the receiver operating characteristic curve (AUC) for the first training-test data set were 0,9947, 0,9800, 0,9843, 0,9881 and 0,9990 respectively; while for the second training-test data set, they were 0,9920, 0,9939, 0,9891, 0,9828 and 0,9991; respectively. Within the scope of the study, finally, all the images were combined and the training and testing processes were repeated for a total of 556 X-ray images comprising 150 Covid-19 images and 406 non-Covid-19 images, by applying 2-fold cross. In this context, the average highest values of sensitivity, specificity, accuracy, F-1 score, and AUC for this last training-test data set were found to be 0,9760, 1,0000, 0,9906, 0,9823 and 0,9997; respectively.},
  archive      = {J_APIN},
  author       = {Yasar, Huseyin and Ceylan, Murat},
  doi          = {10.1007/s10489-020-02019-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2740-2763},
  shortjournal = {Appl. Intell.},
  title        = {A new deep learning pipeline to detect covid-19 on chest X-ray images using local binary pattern, dual tree complex wavelet transform and convolutional neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A gradient boosting machine learning approach in modeling
the impact of temperature and humidity on the transmission rate of
COVID-19 in india. <em>APIN</em>, <em>51</em>(5), 2727–2739. (<a
href="https://doi.org/10.1007/s10489-020-01997-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meteorological parameters were crucial and effective factors in past infectious diseases, like influenza and severe acute respiratory syndrome (SARS), etc. The present study targets to explore the association between the coronavirus disease 2019 (COVID-19) transmission rates and meteorological parameters. For this purpose, the meteorological parameters and COVID-19 infection data from 28th March 2020 to 22nd April 2020 of different states of India have been compiled and used in the analysis. The gradient boosting model (GBM) has been implemented to explore the effect of the minimum temperature, maximum temperature, minimum humidity, and maximum humidity on the infection count of COVID-19. The optimal performance of the GBM model has been achieved after tuning its parameters. The GBM results in the best accuracy of R2 = 0.95 for prediction of active cases in Maharashtra, and R2 = 0.98 for prediction of recovered cases of COVID-19 in Kerala and Rajasthan, India.},
  archive      = {J_APIN},
  author       = {Shrivastav, Lokesh Kumar and Jha, Sunil Kumar},
  doi          = {10.1007/s10489-020-01997-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2727-2739},
  shortjournal = {Appl. Intell.},
  title        = {A gradient boosting machine learning approach in modeling the impact of temperature and humidity on the transmission rate of COVID-19 in india},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kalman filter based short term prediction model for COVID-19
spread. <em>APIN</em>, <em>51</em>(5), 2714–2726. (<a
href="https://doi.org/10.1007/s10489-020-01948-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corona Virus Disease 2019 (COVID19) has emerged as a global medical emergency in the contemporary time. The spread scenario of this pandemic has shown many variations. Keeping all this in mind, this article is written after various studies and analysis on the latest data on COVID19 spread, which also includes the demographic and environmental factors. After gathering data from various resources, all data is integrated and passed into different Machine Learning Models in order to check its appropriateness. Ensemble Learning Technique, Random Forest, gives a good evaluation score on the tested data. Through this technique, various important factors are recognized and their contribution to the spread is analyzed. Also, linear relationships between various features are plotted through the heat map of Pearson Correlation matrix. Finally, Kalman Filter is used to estimate future spread of SARS-Cov-2, which shows good results on the tested data. The inferences from the Random Forest feature importance and Pearson Correlation gives many similarities and few dissimilarities, and these techniques successfully identify the different contributing factors. The Kalman Filter gives a satisfying result for short term estimation, but not so good performance for long term forecasting. Overall, the analysis, plots, inferences and forecast are satisfying and can help a lot in fighting the spread of the virus.},
  archive      = {J_APIN},
  author       = {Singh, Koushlendra Kumar and Kumar, Suraj and Dixit, Prachi and Bajpai, Manish Kumar},
  doi          = {10.1007/s10489-020-01948-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2714-2726},
  shortjournal = {Appl. Intell.},
  title        = {Kalman filter based short term prediction model for COVID-19 spread},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting COVID-19 outbreak progression using hybrid
polynomial-bayesian ridge regression model. <em>APIN</em>,
<em>51</em>(5), 2703–2713. (<a
href="https://doi.org/10.1007/s10489-020-01942-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, Coronavirus Disease 2019 (COVID-19), caused by the SARS-CoV-2 (Severe Acute Respiratory Syndrome Corona Virus 2) Coronavirus, unforeseen pandemic put humanity at big risk and health professionals are facing several kinds of problem due to rapid growth of confirmed cases. That is why some prediction methods are required to estimate the magnitude of infected cases and masses of studies on distinct methods of forecasting are represented so far. In this study, we proposed a hybrid machine learning model that is not only predicted with good accuracy but also takes care of uncertainty of predictions. The model is formulated using Bayesian Ridge Regression hybridized with an n-degree Polynomial and uses probabilistic distribution to estimate the value of the dependent variable instead of using traditional methods. This is a completely mathematical model in which we have successfully incorporated with prior knowledge and posterior distribution enables us to incorporate more upcoming data without storing previous data. Also, L2 (Ridge) Regularization is used to overcome the problem of overfitting. To justify our results, we have presented case studies of three countries, −the United States, Italy, and Spain. In each of the cases, we fitted the model and estimate the number of possible causes for the upcoming weeks. Our forecast in this study is based on the public datasets provided by John Hopkins University available until 11th May 2020. We are concluding with further evolution and scope of the proposed model.},
  archive      = {J_APIN},
  author       = {Saqib, Mohd},
  doi          = {10.1007/s10489-020-01942-7},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2703-2713},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting COVID-19 outbreak progression using hybrid polynomial-bayesian ridge regression model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated diagnosis of COVID-19 with limited posteroanterior
chest x-ray images using fine-tuned deep neural networks. <em>APIN</em>,
<em>51</em>(5), 2689–2702. (<a
href="https://doi.org/10.1007/s10489-020-01900-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that resembles pneumonia. The current diagnostic procedure of COVID-19 follows reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which however is less sensitive to identify the virus at the initial stage. Hence, a more robust and alternate diagnosis technique is desirable. Recently, with the release of publicly available datasets of corona positive patients comprising of computed tomography (CT) and chest X-ray (CXR) imaging; scientists, researchers and healthcare experts are contributing for faster and automated diagnosis of COVID-19 by identifying pulmonary infections using deep learning approaches to achieve better cure and treatment. These datasets have limited samples concerned with the positive COVID-19 cases, which raise the challenge for unbiased learning. Following from this context, this article presents the random oversampling and weighted class loss function approach for unbiased fine-tuned learning (transfer learning) in various state-of-the-art deep learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2, DenseNet169, and NASNetLarge to perform binary classification (as normal and COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia, and normal case) of posteroanterior CXR images. Accuracy, precision, recall, loss, and area under the curve (AUC) are utilized to evaluate the performance of the models. Considering the experimental results, the performance of each model is scenario dependent; however, NASNetLarge displayed better scores in contrast to other architectures, which is further compared with other recently proposed approaches. This article also added the visual explanation to illustrate the basis of model classification and perception of COVID-19 in CXR images.},
  archive      = {J_APIN},
  author       = {Punn, Narinder Singh and Agarwal, Sonali},
  doi          = {10.1007/s10489-020-01900-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2689-2702},
  shortjournal = {Appl. Intell.},
  title        = {Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Editorial for the COVID special issue. <em>APIN</em>,
<em>51</em>(5), 2687–2688. (<a
href="https://doi.org/10.1007/s10489-021-02432-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Ali, Moonis and Fujita, Hamido},
  doi          = {10.1007/s10489-021-02432-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {2687-2688},
  shortjournal = {Appl. Intell.},
  title        = {Editorial for the COVID special issue},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A feature selection algorithm based on redundancy analysis
and interaction weight. <em>APIN</em>, <em>51</em>(4), 2672–2686. (<a
href="https://doi.org/10.1007/s10489-020-01936-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of some three-dimensional mutual information-based algorithms can be affected, since only relevance and interaction are considered. Aiming at solving the problem, a feature selection algorithm based on redundancy analysis and interaction weight is proposed in this paper. The proposed algorithm adopts three-way interaction information to measure the interaction among the class label and features, and processes features for interaction weight analysis. Then, it employs symmetric uncertainty to measure the relevance between features and the class label as well as the redundancy between features, and selects the features with greater relevance and interaction as well as smaller redundancy. To validate the performance, the proposed algorithm is compared with several feature selection algorithms. Since relevance, redundancy, and interaction analysis are all presented, the proposed algorithm can obtain better feature selection performance.},
  archive      = {J_APIN},
  author       = {Gu, Xiangyuan and Guo, Jichang and Li, Chongyi and Xiao, Lijun},
  doi          = {10.1007/s10489-020-01936-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2672-2686},
  shortjournal = {Appl. Intell.},
  title        = {A feature selection algorithm based on redundancy analysis and interaction weight},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weight and bias initialization routines for sigmoidal
feedforward network. <em>APIN</em>, <em>51</em>(4), 2651–2671. (<a
href="https://doi.org/10.1007/s10489-020-01960-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of the Sigmoidal Feedforward Networks in the solution of complex learning task can be attributed to their Universal Approximation Property. These networks are trained using non-linear iterative optimization method (of first-order or second-order) to solve a learning task. The convergence rate in Sigmoidal Feedforward Network training is affected by the initial choice of weights, therefore, in this paper, we propose two new weight initialization routines (Routine-1 and Routine-2) using characteristics of input and output data and property of activation function. Routine-1 uses the linear dependency of weight update step size on derivative of activation function and thus, initialize weights and bias to activate the activation function region near zero (input), where the derivative is maximum, therefore, increasing the weight update step size, and hence, the convergence speed. The same principle is used to derive Routine-2, that initialize weights and bias to activate distinct point in the significant range of activation function (where significant range defines the non-saturated region in activation function), such that, each node evolves independently of each other, and act as distinct feature identifier. Initializing weights in significant range reduces chances of (hidden) nodes getting stuck in saturated state. The networks initialized using proposed routines has higher convergence and higher probability to achieve deeper minima. The efficiency of proposed routines is evaluated by comparing them to conventional random weight initialization routine and 11 weight initialization routines proposed in literature (4 well established routines and 7 recently proposed routines) for several benchmark problems. The proposed routine is also tested for larger networks sizes and larger datasets such as MNIST. The results show that the performance of proposed routines is better than conventional random weight initialization routine and 11 established weight initialization routines.},
  archive      = {J_APIN},
  author       = {Mittal, Apeksha and Singh, Amit Prakash and Chandra, Pravin},
  doi          = {10.1007/s10489-020-01960-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2651-2671},
  shortjournal = {Appl. Intell.},
  title        = {Weight and bias initialization routines for sigmoidal feedforward network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classifying univariate uncertain data. <em>APIN</em>,
<em>51</em>(4), 2622–2650. (<a
href="https://doi.org/10.1007/s10489-020-01911-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature, univariate uncertain data has a quantitative interval for each attribute in each transaction, which is accompanied by a probability density function indicating the probability that each value in the interval exists and appears. To the best of our knowledge, classifying univariate uncertain data has thus far seldom been addressed in the literature. Here, we propose the AssoU2Classifier algorithm to address this research gap. The AssoU2Classifier algorithm retrieves association rules from the univariate uncertain data to serve as a classification model. In addition, the U2Pruning procedure is developed to prune the association rules. The U2Pruning procedure not only reduces the number of association rules, which considerably accelerates the classification process, but also achieves high classification accuracies. In the experiments, the AssoU2Classifier algorithm was compared with 14 existing algorithms on 12 modified UCI datasets. The AssoU2Classifier algorithm obtained better classification accuracy than the compared algorithms on most of the datasets. Statistical tests (Friedman test and pairwise Wilcoxon test) also justified the advantage of the AssoU2Classifier algorithm. In addition, the AssoU2Classifier algorithm also had average learning time.},
  archive      = {J_APIN},
  author       = {Liu, Ying-Ho and Fan, Huei-Yu},
  doi          = {10.1007/s10489-020-01911-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2622-2650},
  shortjournal = {Appl. Intell.},
  title        = {Classifying univariate uncertain data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A classification method to detect faults in a rotating
machinery based on kernelled support tensor machine and multilinear
principal component analysis. <em>APIN</em>, <em>51</em>(4), 2609–2621.
(<a href="https://doi.org/10.1007/s10489-020-02011-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotatingmachinery is the main component of mechanical equipment. Nevertheless, due to variation of operating condition results in important detection performance deterioration. Therefore, fault detection and diagnosis of rotating machines is very critical for the reliable operation. In this paper, a novel classification technique is employed for fault detection of rotating machines based on kernelled support tensor machine (KSTM) and multilinear principal component analysis (MPCA). The vibration signal is firstly formulated as a 3-way tensor using trial, condition and channel. In order to process the rotating machines faults and identify the information classes in tensor space, the KSTM is then introduced from sets of binary support tensor machine classifiers by the one-against-one parallel strategy. The MPCA is utilized for reduction dimensionality of the high-dimensional signature space and reservation the tensorial structure information. The performance of the developed technique in classification faults of rotating machinery has been thoroughly evaluated through collecting signals on bearing and gear test-rigs. Experimental results showed that the proposed method can achieve the highest classification results among the six classification techniques investigated in this study.},
  archive      = {J_APIN},
  author       = {Hu, Chaofan and He, Shuilong and Wang, Yanxue},
  doi          = {10.1007/s10489-020-02011-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2609-2621},
  shortjournal = {Appl. Intell.},
  title        = {A classification method to detect faults in a rotating machinery based on kernelled support tensor machine and multilinear principal component analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video sketch: A middle-level representation for action
recognition. <em>APIN</em>, <em>51</em>(4), 2589–2608. (<a
href="https://doi.org/10.1007/s10489-020-01905-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different modalities extracted from videos, such as RGB and optical flows, may provide complementary cues for improving video action recognition. In this paper, we introduce a new modality named video sketch, which implies the human shape information, as a complementary modality for video action representation. We show that video action recognition can be enhanced by using the proposed video sketch. More specifically, we first generate video sketch with class distinctive action areas and then employ a two-stream network to combine the shape information extracted from image-based sketch and point-based sketch, followed by fusing the classification scores of two streams to generate shape representation for videos. Finally, we use the shape representation as the complementary one for the traditional appearance (RGB) and motion (optical flow) representations for the final video classification. We conduct extensive experiments on four human action recognition datasets – KTH, HMDB51, UCF101, Something-Something and UTI. The experimental results show that the proposed method outperforms the existing state-of-the-art action recognition methods.},
  archive      = {J_APIN},
  author       = {Zhang, Xing-Yuan and Huang, Ya-Ping and Mi, Yang and Pei, Yan-Ting and Zou, Qi and Wang, Song},
  doi          = {10.1007/s10489-020-01905-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2589-2608},
  shortjournal = {Appl. Intell.},
  title        = {Video sketch: A middle-level representation for action recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QSIM: A novel approach to node proximity estimation based on
discrete-time quantum walk. <em>APIN</em>, <em>51</em>(4), 2574–2588.
(<a href="https://doi.org/10.1007/s10489-020-01970-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node proximity estimation studies structural similarity between nodes and is the key issue of network analysis. It can exist as the node recommendation task and is a fundamental basis of other graph mining techniques. Although Discrete-time quantum walk (DTQW), a promising new technique with distinctive characters, is widely used in many graph mining problems such as graph isomorphism and graph kernel, there are only a few works estimating proximity via DTQW, limiting the further application of DTQW in graph mining. In this paper, we study the capability of DTQW for proximity estimation and propose QSIM to estimate node proximity by DTQW. By analyzing the diffusion process of biased walks, we discover two influential effects that are beneficial to proximity estimation. The Diminishing Effect shows that a node close to the starting node can generally have a high average probability during the diffusion process, which serves as the basis of QSIM. The Returning Effect shows the probability has a tendency to stay around the starting node during the diffusion, which enhances the capability for mining local information especially in densely-connected structures. Benefited from the two effects, QSIM faithfully reveals node proximity and comprehensively unifies different kinds of node proximity. QSIM is the first mature quantum-walk-based method for proximity estimation. Extensive experiments validate the effectiveness of QSIM and show that QSIM outperforms state-of-the-art methods in the node recommendation task, significantly surpassing Refex, Node2vec, and Role2vec, by up to 1094.2% in the first-order node proximity and 18.8% in the second-order node proximity.},
  archive      = {J_APIN},
  author       = {Wang, Xin and Lu, Kai and Zhang, Yi and Liu, Kai},
  doi          = {10.1007/s10489-020-01970-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2574-2588},
  shortjournal = {Appl. Intell.},
  title        = {QSIM: A novel approach to node proximity estimation based on discrete-time quantum walk},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A group decision making method with intuitionistic
triangular fuzzy preference relations and its application.
<em>APIN</em>, <em>51</em>(4), 2556–2573. (<a
href="https://doi.org/10.1007/s10489-020-01879-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to offer an intuitionistic triangular fuzzy group decision making method by preference relations. For this purpose, the concept of intuitionistic triangular fuzzy preference relations (ITFPRs) is first offered. Then, an additive consistency concept for ITFPRs is introduced. Meanwhile, a programming model is built to check the consistency of ITFPRs. Considering the case where incomplete ITFPRs are obtained, two programming models are constructed, which aim at maximizing the consistency and minimizing the uncertainty of missing information. To achieve the goals of the minimum total adjustment and the smallest number of adjusted elements, two programming models are established to repair inconsistent ITFPRs. In addition, the weights of decision makers are considered, and the consensus levels of individual ITFPRs are studied to ensure the representativeness of decision results. When individual ITFPRs do not meet the consensus requirement, a programming model to reach the consensus threshold is constructed, which permits different intuitionistic triangular fuzzy variables (ITFVs) to have different adjustments and minimizes the total adjustment. Finally, a group decision making algorithm with ITFPRs is proposed, and its feasibility and efficiency are demonstrated through an example of evaluating the intelligent traditional Chinese medicine decocting centers.},
  archive      = {J_APIN},
  author       = {Zhang, Shaolin and Meng, Fanyong},
  doi          = {10.1007/s10489-020-01879-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2556-2573},
  shortjournal = {Appl. Intell.},
  title        = {A group decision making method with intuitionistic triangular fuzzy preference relations and its application},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection of key features for PM2.5 prediction using a
wavelet model and RBF-LSTM. <em>APIN</em>, <em>51</em>(4), 2534–2555.
(<a href="https://doi.org/10.1007/s10489-020-02031-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PM2.5 prediction has received much attention from researchers in recent years, as PM2.5 has been proven to have a major impact on human health. High-precision PM2.5 predictions would greatly benefit both the general public and governments. However, most of the existing studies on predicting PM2.5 or other air pollutants focus on model design and rarely discuss feature selection. Conventionally, researchers either simply choose a few types of meteorological data based on suggestions from experts or conduct simple correlation analyses to identify the types of meteorological data that are most correlated with the predicted pollutant. However, these methods suffer from two shortcomings. (1) Changes in PM2.5 values are influenced by low-frequency pollution from other places and high-frequency pollution from the local area. Furthermore, the meteorological data associated with the two pollution sources are generally different. In this case, changes with different frequencies in each type of meteorological data should be considered separately to precisely identify the correlated meteorological data of pollutants with different frequencies. (2) Datasets used for PM2.5 predictions generally contain high-dimensional meteorological data. Conventional correlation analysis methods are not effective at identifying features in such datasets, making it difficult to select useful features and negatively affecting prediction accuracy. This study therefore proposes two concepts to address this issue: (1) a wavelet model to decompose each type of meteorological data into multiple sub-time series with different frequencies and (2) the design of a novel radial basis function long short-term memory model that can analyze the outputs of the radial basis function to extract key features identified by deep learning models. This approach is faster and simpler than other methods using deep learning models to extract key features. Application of these concepts will greatly enhance prediction accuracy and reduce costs regardless of the prediction model used. We use three years of historical meteorological data from Central Taiwan to demonstrate the effectiveness of the proposed methods.},
  archive      = {J_APIN},
  author       = {Chen, Yi-Chung and Li, Dong-Chi},
  doi          = {10.1007/s10489-020-02031-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2534-2555},
  shortjournal = {Appl. Intell.},
  title        = {Selection of key features for PM2.5 prediction using a wavelet model and RBF-LSTM},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EfficientPose: Scalable single-person pose estimation.
<em>APIN</em>, <em>51</em>(4), 2518–2533. (<a
href="https://doi.org/10.1007/s10489-020-01918-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-person human pose estimation facilitates markerless movement analysis in sports, as well as in clinical applications. Still, state-of-the-art models for human pose estimation generally do not meet the requirements of real-life applications. The proliferation of deep learning techniques has resulted in the development of many advanced approaches. However, with the progresses in the field, more complex and inefficient models have also been introduced, which have caused tremendous increases in computational demands. To cope with these complexity and inefficiency challenges, we propose a novel convolutional neural network architecture, called EfficientPose, which exploits recently proposed EfficientNets in order to deliver efficient and scalable single-person pose estimation. EfficientPose is a family of models harnessing an effective multi-scale feature extractor and computationally efficient detection blocks using mobile inverted bottleneck convolutions, while at the same time ensuring that the precision of the pose configurations is still improved. Due to its low complexity and efficiency, EfficientPose enables real-world applications on edge devices by limiting the memory footprint and computational cost. The results from our experiments, using the challenging MPII single-person benchmark, show that the proposed EfficientPose models substantially outperform the widely-used OpenPose model both in terms of accuracy and computational efficiency. In particular, our top-performing model achieves state-of-the-art accuracy on single-person MPII, with low-complexity ConvNets.},
  archive      = {J_APIN},
  author       = {Groos, Daniel and Ramampiaro, Heri and Ihlen, Espen AF},
  doi          = {10.1007/s10489-020-01918-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2518-2533},
  shortjournal = {Appl. Intell.},
  title        = {EfficientPose: Scalable single-person pose estimation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complement component face space for 3D face recognition from
range images. <em>APIN</em>, <em>51</em>(4), 2500–2517. (<a
href="https://doi.org/10.1007/s10489-020-02012-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a mathematical model for decomposing a range face image into four basic components (named ‘complement components’) in conjunction with a simple approach for data-level fusion to generate thirty-six additional hybrid components. These forty component faces composing a new face image space called the ‘complement component face space.’ The main challenge of this work was to extract relevant features from the vast face space. Features are extracted from the four basic components and four selected hybrid components using singular value decomposition. To introduce diversity, the extracted feature vectors are fused by applying the crossover operation of the genetic algorithm using a Hamming distance-based fitness measure. Particle swarm optimization-based feature selection is employed on the fused features to discard redundant feature values and to maximize the face recognition performance. The recognition performances of the proposed feature set with a support vector machine-based classifier on three accessible and well-known 3D face databases, namely, Frav3D, Bosphorus, and Texas3D, show significant improvements over those achieved by state-of-the-art methods. This work also studies the feasibility of utilizing the component images in the complement component face space for data augmentation in convolutional neural network (CNN)-based frameworks.},
  archive      = {J_APIN},
  author       = {Dutta, Koushik and Bhattacharjee, Debotosh and Nasipuri, Mita and Krejcar, Ondrej},
  doi          = {10.1007/s10489-020-02012-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2500-2517},
  shortjournal = {Appl. Intell.},
  title        = {Complement component face space for 3D face recognition from range images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolutional residual network to short-term load
forecasting. <em>APIN</em>, <em>51</em>(4), 2485–2499. (<a
href="https://doi.org/10.1007/s10489-020-01932-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since their inception, convolutional neural networks (CNNs) have been shown to have powerful feature extraction and learning capabilities, and the creation of deep residual networks (DRNs) was a milestone in the development of CNNs. However, residual networks mostly use convolution structures, which are widely applied to image recognition and classification problems. Therefore, when facing a load forecasting problem that involves nonlinear regression, will a DRN using a convolution structure still achieve great results? To answer this question, we present a network based on a DRN with a convolution structure to carry out short-term load forecasting, and we mainly focus on the effects of DRNs with different depths, widths and block structures for dealing with nonlinear regression problems. Through multiple sets of controlled experiments, we obtain the best network architecture and the corresponding hyperparameters for short-term load forecasting. The experimental results demonstrate that the model has higher prediction accuracy than existing models, and the DRN with a convolution structure can handle load forecasting while still achieving state-of-the-art results.},
  archive      = {J_APIN},
  author       = {Sheng, Ziyu and Wang, Huiwei and Chen, Guo and Zhou, Bo and Sun, Jian},
  doi          = {10.1007/s10489-020-01932-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2485-2499},
  shortjournal = {Appl. Intell.},
  title        = {Convolutional residual network to short-term load forecasting},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic prognosis of lung cancer using heterogeneous deep
learning models for nodule detection and eliciting its morphological
features. <em>APIN</em>, <em>51</em>(4), 2471–2484. (<a
href="https://doi.org/10.1007/s10489-020-01990-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among cancers, lung cancer has the highest morbidity, and mortality rate. The survival probability of lung cancer patients depends largely on an early diagnosis. For predicting lung cancer from low-dose Computed Tomography (LDCT) scans, computer-aided diagnosis (CAD) system needs to detect all pulmonary nodules, and combine their morphological features to assess the risk of cancer. An automatic lung cancer prognosis system is proposed. The existing CAD system is only for nodule detection. Actually, presence of a nodule does not mean cancer. Depending on its morphological features, the risk that it eventually would develop into cancer, is different. The motivation of the work is to propose a complete lung cancer prognosis system. It consists of 2 cascaded modules: nodule detection module and cancer risk evaluation module. In nodule detection module, two object detection algorithms are ensembled to minimize missing detection, i.e., maximize recall performance. They are based on 3D convolutional neural network (3D-CNN), and our recently proposed model of recurrent neural network (RNN). As they extract features in completely different ways, we call them heterogeneous deep learning models. By ensembing them, we could achieve much better recall performance compared to individual detectors. In cancer risk evaluation module, 3D-CNN based models are trained to evaluate the grade of malady of morphological features of pulmonary nodules. It will also provide medically interpretable intermediate information. Finally, a regression model is trained to match the ground truth labels describing morbidity grade of the CT-Scan. In this work, 13 features from the highest risk nodule is used to evaluate the risk of lung cancer. We also identify the subset of structural and morphological features which are strongly related to grading decision, labelled by oncologist. The final system could obtain a low logloss of 0.408.},
  archive      = {J_APIN},
  author       = {Wang, Weilun and Charkborty, Goutam},
  doi          = {10.1007/s10489-020-01990-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2471-2484},
  shortjournal = {Appl. Intell.},
  title        = {Automatic prognosis of lung cancer using heterogeneous deep learning models for nodule detection and eliciting its morphological features},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning flat representations with artificial neural
networks. <em>APIN</em>, <em>51</em>(4), 2456–2470. (<a
href="https://doi.org/10.1007/s10489-020-02032-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a method of learning representation layers with squashing activation functions within a deep artificial neural network which directly addresses the vanishing gradients problem. The proposed solution is derived from solving the maximum likelihood estimator for components of the posterior representation, which are approximately Beta-distributed, formulated in the context of variational inference. This approach not only improves the performance of deep neural networks with squashing activation functions on some of the hidden layers - including in discriminative learning - but can be employed towards producing sparse codes.},
  archive      = {J_APIN},
  author       = {Constantinescu, Vlad and Chiru, Costin and Boloni, Tudor and Florea, Adina and Tacutu, Robi},
  doi          = {10.1007/s10489-020-02032-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2456-2470},
  shortjournal = {Appl. Intell.},
  title        = {Learning flat representations with artificial neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast multi-resolution occlusion: A method for explaining and
understanding deep neural networks. <em>APIN</em>, <em>51</em>(4),
2431–2455. (<a
href="https://doi.org/10.1007/s10489-020-01946-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolutional Neural Networks (DCNNs) contain a high level of complexity and nonlinearity, so it is not clear based on what features DCNN models make decisions and how they can reach such promising results. There are two types of visualization techniques to interpret and explain the deep models: Backpropagation-based and Perturbation-based algorithms. The most notable drawback of the backpropagation-based visualization is that they cannot be applied for all architectures, whereas Perturbation-based visualizations are totally independent of the architectures. These methods, however, take a lot of computation and memory resources which make them slow and expensive, thereby unsuitable for many real-world applications. To cope with these problems, in this paper, a perturbation-based visualization method called Fast Multi-resolution Occlusion (FMO) are presented which is efficient in terms of time and resource consumption and can be considered in real-world applications. In order to compare the FMO with five well-known Perturbation-based visualizations methods such as Occlusion Test, Super-pixel perturbation (LIME), Randomized Input Sampling (RISE), Meaningful Perturbation and Extremal Perturbation, different experiments are designed in terms of time-consumption, visualization quality and localization accuracy. All methods are applied on 5 well-known DCNNs DenseNet121, InceptionV3, InceptionResnetV2, MobileNet and ResNet50 using common benchmark datasets ImageNet, PASCAL VOC07 and COCO14. According to the experimental results, FMO is averagely 2.32 times faster than LIME on five models DenseNet121, InceptionResnetV2, InceptionV3, MobileNet and ResNet50 with images of ILSVRC2012 dataset as well as 24.84 times faster than Occlusion Test, 11.87 times faster than RISE, 8.72 times faster than Meaningful Perturbation and 10.03 times faster than Extremal Perturbation on all of the five used models with images of common dataset ImageNet without scarifying visualization quality. Moreover, the methods are evaluated in terms of localization accuracy on two hard common datasets of PASCAL VOC07 and COCO14. The results show that FMO outperforms the compared relevant methods in terms of localization accuracy. Also, FMO extends the superimposing process of the Occlusion Test method, which yields a heatmap with more visualization quality than the Occlusion Test on many colorful images.},
  archive      = {J_APIN},
  author       = {Behzadi-Khormouji, Hamed and Rostami, Habib},
  doi          = {10.1007/s10489-020-01946-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2431-2455},
  shortjournal = {Appl. Intell.},
  title        = {Fast multi-resolution occlusion: A method for explaining and understanding deep neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge workers mental workload prediction using optimised
ELANFIS. <em>APIN</em>, <em>51</em>(4), 2406–2430. (<a
href="https://doi.org/10.1007/s10489-020-01928-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The competitive society in the new era calls for more research to improve the well-being of workers as well as to improve their productivity. Knowledge workers face a high mental workload in terms of planning and coordination. One solution is to predict the mental workload of knowledge workers. Some machine learning models have been implemented for mental workload prediction, but deep learning models are yet to be introduced for this purpose. Deep learning models are superior to machine learning models because of their ability to correct inaccurate predictions if they ever occur. Therefore, this study aims to optimize the extreme learning adaptive neuro-fuzzy inference system (ELANFIS) by integrating particle swarm optimization into a micro-genetic algorithm to predict the mental workload of knowledge workers. Although the adaptive neuro-fuzzy inference system (ANFIS) shows reasonable prediction performance, it also suffers from the curse of dimensionality and has a poor computation time. Thus, ELANFIS is introduced because its curse of dimensionality is less severe when solving problems with a high number of input dimensions. The integration of the advantages of a micro-genetic algorithm and particle swarm optimization is suggested to optimize the premise parameters of ELANFIS, as this can allow better solutions to be located at a faster rate. The proposed model yields promising prediction results, with improvements of 6.0665 in the Mean Squared Error(MSE) and 1.279 in the Root Mean Squared Error (RMSE) for regression; the proposed model even surpasses the prediction results of ELANFIS optimized with PSO alone, with improvements of 1.5369 in MSE and 0.4094 in RMSE for regression. The findings are expected to assist employers in determining an appropriate working lifestyle for their employees.},
  archive      = {J_APIN},
  author       = {Teoh Yi Zhe, Isaac and Keikhosrokiani, Pantea},
  doi          = {10.1007/s10489-020-01928-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2406-2430},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge workers mental workload prediction using optimised ELANFIS},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved integrate-and-fire neuron models for inference
acceleration of spiking neural networks. <em>APIN</em>, <em>51</em>(4),
2393–2405. (<a
href="https://doi.org/10.1007/s10489-020-02017-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effects of different bio-synaptic membrane potential mechanisms on the inference speed of both spiking feed-forward neural networks and spiking convolutional neural networks. These mechanisms are inspired by biological neuron phenomena include electronic conduction in neurons and chemical neurotransmitter attenuation between presynaptic and postsynaptic neurons. In the area of spiking neural networks, we model some biological neural membrane potential updating strategies based on integrate-and-fire (I&amp;F) spiking neurons. These include the spiking neuron model with membrane potential decay (MemDec), the spiking neuron model with synaptic input current superposition at spiking time (SynSup), and the spiking neuron model with synaptic input current accumulation (SynAcc). Experiment results show that compared with the general I&amp;F model (one of the most commonly used spiking neuron models), SynSup and SynAcc can effectively improve the spiking inference speed of spiking feed-forward neural networks and spiking convolutional neural networks.},
  archive      = {J_APIN},
  author       = {Zhou, Yongcheng and Zhang, Anguo},
  doi          = {10.1007/s10489-020-02017-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2393-2405},
  shortjournal = {Appl. Intell.},
  title        = {Improved integrate-and-fire neuron models for inference acceleration of spiking neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust dialog state tracker with contextual-feature
augmentation. <em>APIN</em>, <em>51</em>(4), 2377–2392. (<a
href="https://doi.org/10.1007/s10489-020-01991-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialog state tracking (DST), which estimates dialog states given a dialog context, is a core component in task-oriented dialog systems. Existing data-driven methods usually extract features automatically through deep learning. However, most of these models have limitations. First, compared with hand-crafted delexicalization features, such features in deep learning approaches are not universal. However, they are important for tracking unseen slot values. Second, such models do not work well in situations where noisy labels are ubiquitous in datasets. To address these challenges, we propose a robust dialog state tracker with contextual-feature augmentation. Contextual-feature augmentation is used to extract generalized features; hence, it is capable of solving the unseen slot value tracking problem. We apply a simple but effective deep learning paradigm to train our DST model with noisy labels. The experimental results show that our model achieves state-of-the-art scores in terms of joint accuracy on the MultiWOZ 2.0 dataset. In addition, we show its performance in tracking unseen slot values by simulating unseen domain dialog state tracking.},
  archive      = {J_APIN},
  author       = {Zhang, Xuejun and Zhao, Xuemin and Tan, Tian},
  doi          = {10.1007/s10489-020-01991-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2377-2392},
  shortjournal = {Appl. Intell.},
  title        = {Robust dialog state tracker with contextual-feature augmentation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightning search algorithm: A comprehensive survey.
<em>APIN</em>, <em>51</em>(4), 2353–2376. (<a
href="https://doi.org/10.1007/s10489-020-01947-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lightning search algorithm (LSA) is a novel meta-heuristic optimization method, which is proposed in 2015 to solve constraint optimization problems. This paper presents a comprehensive survey of the applications, variants, and results of the so-called LSA. In LSA, the best-obtained solution is defined to improve the effectiveness of the fitness function through the optimization process by finding the minimum or maximum costs to solve a specific problem. Meta-heuristics have grown the focus of researches in the optimization domain, because of the foundation of decision-making and assessment in addressing various optimization problems. A review of LSA variants is displayed in this paper, such as the basic, binary, modification, hybridization, improved, and others. Moreover, the classes of the LSA’s applications include the benchmark functions, machine learning applications, network applications, engineering applications, and others. Finally, the results of the LSA is compared with other optimization algorithms published in the literature. Presenting a survey and reviewing the LSA applications is the chief aim of this survey paper.},
  archive      = {J_APIN},
  author       = {Abualigah, Laith and Elaziz, Mohamed Abd and Hussien, Abdelazim G. and Alsalibi, Bisan and Jalali, Seyed Mohammad Jafar and Gandomi, Amir H.},
  doi          = {10.1007/s10489-020-01947-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2353-2376},
  shortjournal = {Appl. Intell.},
  title        = {Lightning search algorithm: A comprehensive survey},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Link traffic speed forecasting using convolutional
attention-based gated recurrent unit. <em>APIN</em>, <em>51</em>(4),
2331–2352. (<a
href="https://doi.org/10.1007/s10489-020-02020-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic speed forecasting becomes a thriving research area in modern transportation systems. The intensification of travel flow volumes due to fast urbanization, vehicle path planning, demands on efficient transport planning policies, commercial objectives, and many other factors contribute to fuel this revival dynamics. Moreover, predicting vehicle speed is of paramount importance in congestion management to help transport authorities as well as network users to handle congestion over road infrastructures or to provide a global overview of daily passenger flow. In this work, we propose a novel approach to forecast the future traffic speed of the road segments (links) based on traffic flow data without the need for previous traffic speed as input. To do this, we first pre-process floating car data of several million vehicles for multiples network links spread all over the Greater Paris area from 2016 to 2017. A convolutional attention-based recurrent neural network is used to capture the local-temporal features of traffic data to unveil the underlying pattern between the traffic flow and speed sequences for all links over the network. While the convolutional layer captures the local dependency, the attention layer learns patterns from weights of near-term traffic flow. It extracts the inherent interdependency of traffic speed due to many factors such as incidents, rush hour, land use, to cite a few, in non-free-flow conditions. The efficiency of the proposed model is evaluated using several metrics in traffic speed forecasting excluding additional data such as historical traffic speed and network graph contrary to cutting-edge work in the field. This is a substantial property since it allows avoiding the cumbersomeness in data mixing and facilitating resource availability. The proposed model is also evaluated on several roads located in the Greater Paris area separately on weekdays and weekends.},
  archive      = {J_APIN},
  author       = {Khodabandelou, Ghazaleh and Kheriji, Walid and Selem, Fouad Hadj},
  doi          = {10.1007/s10489-020-02020-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2331-2352},
  shortjournal = {Appl. Intell.},
  title        = {Link traffic speed forecasting using convolutional attention-based gated recurrent unit},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dictionary adaptation and variational mode decomposition for
gyroscope signal enhancement. <em>APIN</em>, <em>51</em>(4), 2312–2330.
(<a href="https://doi.org/10.1007/s10489-020-01958-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes an approach to signal denoising based on a combination of Variational Mode Decomposition with the Split Augmented Lagrangian Shrinkage Algorithm. In our research, we found that the proposed approach gives a great improvement of denoising gyroscopic signals. In turn, the results for the synthetic signals are not straightforward. For the bumps synthetic signals, the proposed algorithm gives the best results for different levels of signal degradation. While for the Doppler and blocks synthetic signals the reference methods give better results. However, for heavisine test signal the proposed algorithm gives better results in almost all cases. A weak point of the presented algorithm is its time complexity. The proposed approach is based on the Split Augmented Lagrangian Shrinkage Algorithm, which is the iterative optimization method since the time of computation strongly depends on the number of iterations. The presented results show that the proposed approach gives a great improvement in signal denoising and it is a promising direction of future research.},
  archive      = {J_APIN},
  author       = {Brzostowski, Krzysztof and Świa̧tek, Jerzy},
  doi          = {10.1007/s10489-020-01958-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2312-2330},
  shortjournal = {Appl. Intell.},
  title        = {Dictionary adaptation and variational mode decomposition for gyroscope signal enhancement},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bangla-meitei mayek scripts handwritten character
recognition using convolutional neural network. <em>APIN</em>,
<em>51</em>(4), 2291–2311. (<a
href="https://doi.org/10.1007/s10489-020-01901-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of handwritten characters in two Indic scripts Bangla and Meitei Mayek is one of the challenging responsibilities due to intricate patterns and scarcity of standard datasets. Convolutional Neural Network (CNN) is one of the stablest well-known techniques for classifying objects in distinctive specialties as it has an extraordinary capability of discovering complex patterns. In this paper, we hook a different layout and obtain a unique CNN architecture from scratch, which has manifold advantages over classical machine learning (ML) approaches, and it has a unique ability to consolidate feature extraction and classification altogether. Further, we stretch our work to uncover the mathematical rationale for using non-linearity in the deep learning (DL) model. Our proposed CNN architecture consists of four layers, including convolutional layer (CL), nonlinear activation layer (AL), pooling layer (PL), and fully connected layer (FCL), which are used in the existing two accessible Bangla datasets named cMATERdb and ISI Bangla datasets. The identical model also validates on proposed Manipuri Character dataset, called “Mayek27”. Moreover, we perform an in-depth comparison with different batch sizes and optimization techniques over all the datasets for understanding their functionality. We conceive a novel benchmark performance that has delivered state-of-the-art decisions on two regional handwritten character identifications.},
  archive      = {J_APIN},
  author       = {Hazra, Abhishek and Choudhary, Prakash and Inunganbi, Sanasam and Adhikari, Mainak},
  doi          = {10.1007/s10489-020-01901-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2291-2311},
  shortjournal = {Appl. Intell.},
  title        = {Bangla-meitei mayek scripts handwritten character recognition using convolutional neural network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-parameter safe screening rule for hinge-optimal margin
distribution machine. <em>APIN</em>, <em>51</em>(4), 2279–2290. (<a
href="https://doi.org/10.1007/s10489-020-02024-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal margin distribution machine (ODM) is an efficient algorithm for classification problems. ODM attempts to optimize the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously, so it can achieve a better generalization performance. However, it is relatively time-consuming for large-scale problems. In this paper, we propose a hinge loss-based optimal margin distribution machine (Hinge-ODM), which derives a simplified substitute formulation. It can speed up the solving process without affecting the optimal accuracy obviously. Besides, inspired by its sparse solution, we put forward a multi-parameter safe screening rule for Hinge-ODM, called MSSR-Hinge-ODM. Based on the MSSR, most non-support vectors can be identified and deleted beforehand so the scale of dual problem will be greatly reduced. Moreover, our MSSR is safe, that is, it can get the exactly same optimal solutions as the original one. Furthermore, a fast algorithm DCDM is introduced to further solve the reduced Hinge-ODM. Finally, we integrate the MSSR into grid search method to accelerate the whole training process. Experimental results on twenty data sets demonstrate the superiority of the proposed methods.},
  archive      = {J_APIN},
  author       = {Ma, Mengdan and Xu, Yitian},
  doi          = {10.1007/s10489-020-02024-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2279-2290},
  shortjournal = {Appl. Intell.},
  title        = {Multi-parameter safe screening rule for hinge-optimal margin distribution machine},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Robustness comparison between the capsule network and the
convolutional network for facial expression recognition. <em>APIN</em>,
<em>51</em>(4), 2269–2278. (<a
href="https://doi.org/10.1007/s10489-020-01895-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important part of human-computer interactions, facial expression recognition has become a popular research topic in computer vision, pattern recognition, artificial intelligence and other fields. With the development of deep learning and convolutional neural networks, research on facial expression recognition has also made considerable progress. Because facial expressions vary in real environments, such as rotation, shifting, brightness changes, partial occlusion and noise with different intensities, research on the robustness of facial expression recognition is very important. A capsule network consists of capsules, which are groups of neurons, and these capsules can learn posture information through the dynamic routing mechanism. The length of a capsule represents the existence probability, and each neuron in a capsule represents posture information (e.g., position, size, orientation or a combination of these properties). Therefore, in this study, the robustness of the emerging capsule network (CapsNet) is comprehensively compares with that of the traditional convolutional neural network (CNN) and fully convolutional network (FCN) in facial expression recognition tasks. The simulation results based on the Cohn-Kanade (CK+) databases show that the capsule network is more robust than the other networks. Therefore, the capsule network has significant advantages over the other networks in facial expression recognition task in complex real-world environments.},
  archive      = {J_APIN},
  author       = {Li, Donghui and Zhao, Xingcong and Yuan, Guangjie and Liu, Ying and Liu, Guangyuan},
  doi          = {10.1007/s10489-020-01895-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2269-2278},
  shortjournal = {Appl. Intell.},
  title        = {Robustness comparison between the capsule network and the convolutional network for facial expression recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SL2E-AFRE: Personalized 3D face reconstruction using
autoencoder with simultaneous subspace learning and landmark estimation.
<em>APIN</em>, <em>51</em>(4), 2253–2268. (<a
href="https://doi.org/10.1007/s10489-020-02000-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D face reconstruction from single face image has received much attention in the past decade, as it has been used widely in many applications in the field of computer vision. Despite more accurate solutions by 3D scanners and several commercial systems, they have drawbacks such as the need for manual initialization, time and economy constraints. In this paper, a novel framework for 3D face reconstruction is presented. Firstly, landmarks are localized on the database faces with the proposed landmark-mapping strategy employing a model template. Then, an autoencoder assisted by the proposed energy function to simultaneously learn the facial patch subspace and the keypoints positions is employed to predict the landmarks. Finally, an unique 3D reconstruction is obtained with the proposed predicted landmark based deformation. Meta-parameters are incorporated into the energy function during the training phase to enhance the performance of the autoencoder network in reconstructing the face model. The experiments are carried out on two databases namely the USF Human ID 3-D Database and the Bosphorus 3D face database. The experimental results show that the Autoencoder based Face REconstruction with Simultaneous patch Learning and Landmark Estimation method (SL2E-AFRE) is efficient and the performance of the same is significantly upgraded in each iteration.},
  archive      = {J_APIN},
  author       = {Devi, P. R. Suganya and Baskaran, R.},
  doi          = {10.1007/s10489-020-02000-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2253-2268},
  shortjournal = {Appl. Intell.},
  title        = {SL2E-AFRE: Personalized 3D face reconstruction using autoencoder with simultaneous subspace learning and landmark estimation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TenLa: An approach based on controllable tensor
decomposition and optimized lasso regression for judgement prediction of
legal cases. <em>APIN</em>, <em>51</em>(4), 2233–2252. (<a
href="https://doi.org/10.1007/s10489-020-01912-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of big data and artificial intelligence technology, the computer-assisted judgment of legal cases has become an inevitable trend in the intersection of computer science and law. Judgment prediction methods of legal cases mainly consist of two parts: (1) modeling of legal cases and (2) construction of judgment prediction algorithms. Previous methods for the judgment prediction of legal cases are mainly based on feature models and classification algorithms. Traditional feature models require extensive expert knowledge and manual annotation. They are highly dependent on vocabulary and grammatical information in databases, which are not conducive to the improvement of accuracy and universality of subsequent prediction algorithms. In addition, prediction results obtained by classification algorithms are coarse in granularity and low in accuracy. In general, judgments in similar legal cases are similar. This article proposes a new method for the judgment prediction of legal cases, namely, TenLa, which is based on a controllable algorithm of tensor decomposition and an optimized Lasso regression model. TenLa takes similarities between legal cases as an important indicator of judgment prediction and is mainly divided into three parts: (1) ModTen; we propose a modeling method for legal cases, namely, ModTen, which represents legal cases as three-dimensional tensors. (2) ConTen; we propose a new tensor decomposition algorithm, namely, ConTen, which decomposes tensors obtained by ModTen into core tensors through the intermediary tensor. Core tensors greatly reduce the dimensions of original tensors. (3) OLass; we propose an optimized Lasso regression algorithm, namely, OLass. Core tensors obtained by ConTen are used to train OLass. Specifically, we propose an optimization algorithm for OLass with respect to the intermediary tensor in ConTen; thus, the core tensors obtained by ConTen carry tensor elements and tensor structure information that is most conducive to the improvement of the accuracy of OLass. Experiments show that TenLa has higher accuracy than traditional judgment prediction algorithms.},
  archive      = {J_APIN},
  author       = {Guo, Xiaoding and Zhang, Hongli and Ye, Lin and Li, Shang},
  doi          = {10.1007/s10489-020-01912-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2233-2252},
  shortjournal = {Appl. Intell.},
  title        = {TenLa: An approach based on controllable tensor decomposition and optimized lasso regression for judgement prediction of legal cases},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-sensitive feature selection on multi-label data via
neighborhood granularity and label enhancement. <em>APIN</em>,
<em>51</em>(4), 2210–2232. (<a
href="https://doi.org/10.1007/s10489-020-01993-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection, which is an efficient and effective pre-processing step in machine learning and data mining, can select a feature subset that contains more contributions for multi-label classification while improving the performance of the classifiers. In real-world applications, an instance may be associated with multiple related labels with different relative importances, and the process of obtaining different features usually requires different costs, containing money, and time, etc. However, most existing works with regard to multi-label feature selection do not take into consideration the above two critical issues simultaneously. Therefore, in this paper, we exploit the idea of neighborhood granularity to enhance the traditional logical labels into label distribution forms to excavate the deeper supervised information hidden in multi-label data, and further consider the effect of the test cost under three different distributions, simultaneously. Motivated by these issues, a novel test cost multi-label feature selection algorithm with label enhancement and neighborhood granularity is designed. Moreover, the proposed algorithm is tested upon ten publicly available benchmark multi-label datasets with six widely-used metrics from two different aspects. Finally, two groups of experimental results demonstrate that the proposed algorithm achieves the satisfactory and superior performance over other four state-of-the-art comparing algorithms, and it is effective for improving the learning performance and decreasing the total test costs of the selected feature subset.},
  archive      = {J_APIN},
  author       = {Long, Xuandong and Qian, Wenbin and Wang, Yinglong and Shu, Wenhao},
  doi          = {10.1007/s10489-020-01993-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2210-2232},
  shortjournal = {Appl. Intell.},
  title        = {Cost-sensitive feature selection on multi-label data via neighborhood granularity and label enhancement},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting unusual input to neural networks. <em>APIN</em>,
<em>51</em>(4), 2198–2209. (<a
href="https://doi.org/10.1007/s10489-020-01925-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating a neural network on an input that differs markedly from the training data might cause erratic and flawed predictions. We study a method that judges the unusualness of an input by evaluating its informative content compared to the learned parameters. This technique can be used to judge whether a network is suitable for processing a certain input and to raise a red flag that unexpected behavior might lie ahead. We compare our approach to various methods for uncertainty evaluation from the literature for various datasets and scenarios. Specifically, we introduce a simple, effective method that allows to directly compare the output of such metrics for single input points even if these metrics live on different scales.},
  archive      = {J_APIN},
  author       = {Martin, Jörg and Elster, Clemens},
  doi          = {10.1007/s10489-020-01925-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2198-2209},
  shortjournal = {Appl. Intell.},
  title        = {Detecting unusual input to neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-y network: Infrared-visible image patches matching via
semi-supervised transfer learning. <em>APIN</em>, <em>51</em>(4),
2188–2197. (<a
href="https://doi.org/10.1007/s10489-020-01996-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared-visible image patches matching has many applications, such as target recognition, vision-based navigation, and others. At present, deep learning has achieved excellent performance in visible image patches matching. Due to imaging differences in infrared-visible images and the small number of supervised samples, the existing networks cannot solve them well. We propose a Dual-Y network based on semi-supervised transfer learning to address the challenges. Since the infrared and visible image patches have similarities in the same scene, adversarial domain adaptation uses visible images as the source domain and infrared images as the target domain to solve different imaging principles. Through a large number of unannotated samples training, the two convolutional autoencoders in our network can reconstruct the infrared-visible image patches respectively to improve feature representation ability. In the high-level convolution layers, the cross-domain features are extracted by sharing weights in two domains. The adversarial domain adaptation achieves the domain confusion in the high layers. In the low-level convolutional layers, independent weights in two branches can ensure that the domain-related imaging principles can be preserved. Finally, the annotated data are used for supervised training in infrared-visible image patches matching. With the improvements above, the matching accuracy increase by 10.78%, compared with the fine-tuning based on the pre-training module.},
  archive      = {J_APIN},
  author       = {Mao, Yuanhong and He, Zhanzhuang},
  doi          = {10.1007/s10489-020-01996-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2188-2197},
  shortjournal = {Appl. Intell.},
  title        = {Dual-Y network: Infrared-visible image patches matching via semi-supervised transfer learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WOLIF: An efficiently tuned classifier that learns to
classify non-linear temporal patterns without hidden layers.
<em>APIN</em>, <em>51</em>(4), 2173–2187. (<a
href="https://doi.org/10.1007/s10489-020-01934-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present in this paper a computationally efficient and biologically plausible classifier WOLIF, using Grey Wolf Optimizer (GWO) tuned error function obtained from Leaky-Integrate-and-Fire (LIF) spiking neuron. Unlike traditional artificial neuron, spiking neuron is capable of intelligently classifying non-linear temporal patterns without hidden layer(s), which makes a Spiking Neural Network (SNN) computationally efficient. There is no additional cost of adding hidden layer(s) in SNN, it is also biologically plausible, and energy efficient. Since supervised learning rule for SNN is still in infancy stage, we introduced WOLIF classifier and its supervised learning rule based on GWO algorithm. WOLIF uses a single LIF neuron thereby use less network parameters, and homo-synaptic static long-term synaptic weights (both excitatory and inhibitory). Note that, WOLIF also reduces the total simulation time which improves computational efficiency. It is benchmarked on seven different datasets drawn from the UCI machine learning repository and found better results both in terms of accuracy and computational cost than state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Hussain, Irshed and Thounaojam, Dalton Meitei},
  doi          = {10.1007/s10489-020-01934-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2173-2187},
  shortjournal = {Appl. Intell.},
  title        = {WOLIF: An efficiently tuned classifier that learns to classify non-linear temporal patterns without hidden layers},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning approach for person identification using ear
biometrics. <em>APIN</em>, <em>51</em>(4), 2161–2172. (<a
href="https://doi.org/10.1007/s10489-020-01995-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic person identification from ear images is an active field of research within the biometric community. Similar to other biometrics such as face, iris and fingerprints, ear also has a large amount of specific and unique features that allow for person identification. In this current worldwide outbreak of COVID-19 situation, most of the face identification systems fail due to the mask wearing scenario. The human ear is a perfect source of data for passive person identification as it does not involve the cooperativeness of the human whom we are trying to recognize and the structure of ear does not change drastically over time. Acquisition of a human ear is also easy as the ear is visible even in the mask wearing scenarios. Ear biometric system can complement the other biometric systems in automatic human recognition system and provides identity cues when the other system information is unreliable or even unavailable. In this work, we propose a six layer deep convolutional neural network architecture for ear recognition. The potential efficiency of the deep network is tested on IITD-II ear dataset and AMI ear dataset. The deep network model achieves a recognition rate of 97.36% and 96.99% for the IITD-II dataset and AMI dataset respectively. The robustness of the proposed system is validated in uncontrolled environment using AMI Ear dataset. This system can be useful in identifying persons in a massive crowd when combined with a proper surveillance system.},
  archive      = {J_APIN},
  author       = {Ahila Priyadharshini, Ramar and Arivazhagan, Selvaraj and Arun, Madakannu},
  doi          = {10.1007/s10489-020-01995-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2161-2172},
  shortjournal = {Appl. Intell.},
  title        = {A deep learning approach for person identification using ear biometrics},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent fault diagnosis of rolling bearings using a
semi-supervised convolutional neural network. <em>APIN</em>,
<em>51</em>(4), 2144–2160. (<a
href="https://doi.org/10.1007/s10489-020-02006-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of convolutional neural networks (CNNs) in intelligent fault diagnosis is largely dependent on massive amounts of labelled data. In a real-world case, however, massive amounts of labelled data are difficult or costly to collect, whereas abundant unlabelled data are often available. To utilize such unlabelled data, a novel method using a semi-supervised convolutional neural network (SSCNN) for intelligent fault diagnosis of bearings is proposed. First, a 1-d CNN is applied to learn class space features and generate class probabilities of unlabelled samples, based on which a class probability maximum margin criterion (CPMMC) method is used to construct the loss function of unlabelled samples. Then, the constructed loss function, which aims to maximise the inter-class distance of class space features and minimise the intra-class distance of class space features, is integrated into the cross-entropy loss function of the CNN, and the SSCNN is established. Finally, the SSCNN model is applied to analyse the vibration signals collected from rolling bearings, and a novel intelligent fault diagnosis method using the SSCNN is proposed. Two datasets are employed to validate the effectiveness of the proposed methodology. The results show that the established SSCNN can effectively utilise unlabelled samples to train the model and enhance its fault diagnosis performance. Through a comparison with commonly used semi-supervised deep learning methods, the superiority of the proposed method is validated.},
  archive      = {J_APIN},
  author       = {Wu, Yaochun and Zhao, Rongzhen and Jin, Wuyin and He, Tianjing and Ma, Sencai and Shi, Mingkuan},
  doi          = {10.1007/s10489-020-02006-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2144-2160},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent fault diagnosis of rolling bearings using a semi-supervised convolutional neural network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vehicle theft recognition from surveillance video based on
spatiotemporal attention. <em>APIN</em>, <em>51</em>(4), 2128–2143. (<a
href="https://doi.org/10.1007/s10489-020-01933-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent vehicle thefts have a highly detrimental impact on public safety. Thanks to surveillance equipment distributed throughout a city, a large number of videos that can be used to recognize vehicle theft are available. However, vehicle theft behavior has the characteristics of a small criminal target and small movement. Hence, the existing action recognition algorithms cannot be directly applied for the recognition of vehicle theft. In this paper, we propose a method for vehicle theft recognition based on a spatiotemporal attention mechanism. First, a database of vehicle theft is established by collecting videos from the Internet and an existing dataset. Then, we establish a vehicle theft recognition network and introduce a spatiotemporal attention mechanism for application when extracting the spatiotemporal features of theft. Through the learning of adaptive feature weights, the features that contribute most greatly to recognition are emphasized. Simulation experiments show that our proposed algorithm can achieve 97.04% accuracy on the collected vehicle theft database.},
  archive      = {J_APIN},
  author       = {He, Lijun and Wen, Shuai and Wang, Liejun and Li, Fan},
  doi          = {10.1007/s10489-020-01933-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2128-2143},
  shortjournal = {Appl. Intell.},
  title        = {Vehicle theft recognition from surveillance video based on spatiotemporal attention},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel agent-based, evolutionary model for expressing the
dynamics of creative open-problem solving in small groups.
<em>APIN</em>, <em>51</em>(4), 2094–2127. (<a
href="https://doi.org/10.1007/s10489-020-01919-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the process of producing creative responses to open-ended problems solved in small groups is important for many modern domains, like health care, manufacturing, education, banking, and investment. Some of the main theoretical challenges include characterizing and measuring the dynamics of responses, relating social and individual aspects in group problem solving, incorporating soft skills (e.g., experience, social aspects, and emotions) to the theory of decision making in groups, and understanding the evolution of processes guided by soft utilities (hard-to-quantify utilities), e.g., social interactions and emotional rewards. This paper presents a novel theoretical model (TM) that describes the process of solving open-ended problems in small groups. It mathematically presents the connection between group member characteristics, interactions in a group, group knowledge evolution, and overall novelty of the responses created by a group as a whole. Each member is modeled as an agent with local knowledge, a way of interpreting the knowledge, resources, social skills, and emotional levels associated to problem goals and concepts. Five solving strategies can be employed by an agent to generate new knowledge. Group responses form a solution space, in which responses are grouped into categories based on their similarity and organized in abstraction levels. The solution space includes concrete features and samples, as well as the causal sequences that logically connect concepts with each other. The model was used to explain how member characteristics, e.g., the degree to which their knowledge is similar, relate to the solution novelty of the group. Model validation compared model simulations against results obtained through behavioral experiments with teams of human subjects, and suggests that TMs are a useful tool in improving the effectiveness of small teams.},
  archive      = {J_APIN},
  author       = {Doboli, Alex and Doboli, Simona},
  doi          = {10.1007/s10489-020-01919-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2094-2127},
  shortjournal = {Appl. Intell.},
  title        = {A novel agent-based, evolutionary model for expressing the dynamics of creative open-problem solving in small groups},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast top-k association rule mining using rule generation
property pruning. <em>APIN</em>, <em>51</em>(4), 2077–2093. (<a
href="https://doi.org/10.1007/s10489-020-01994-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional association rule mining algorithms can have a long runtime, high memory consumption, and generate a huge number of rules. Browsing through numerous rules and adjusting parameters to find just enough rules is a tedious task for users, who are often only interested in finding the strongest rules. Hence, many recent studies have focused on mining the top-k most frequent association rules that have a minimum confidence so as to limit the number of rules by ranking them by frequency. Though this redefined task has many applications, the performance of current algorithms remains an issue. To address this issue, this paper presents a novel algorithm named FTARM (Fast Top-K Association Rule Miner) to efficiently find the set of top-k association rules using a novel technique called Rule Generation Property Pruning (RGPP). This technique reduces the search space by analyzing the internal relationships between items of the database to be mined and the parameters set by users. Furthermore, a novel candidate pruning property is used by this technique to speed up the mining process. FTARM’s efficiency was evaluated on various public benchmark datasets. A substantial reduction of the association rule mining time and memory usage was observed, and that FTARM has good scalability, which can benefit to many applications.},
  archive      = {J_APIN},
  author       = {Liu, Xiangyu and Niu, Xinzheng and Fournier-Viger, Philippe},
  doi          = {10.1007/s10489-020-01994-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2077-2093},
  shortjournal = {Appl. Intell.},
  title        = {Fast top-K association rule mining using rule generation property pruning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PointFusionNet: Point feature fusion network for 3D point
clouds analysis. <em>APIN</em>, <em>51</em>(4), 2063–2076. (<a
href="https://doi.org/10.1007/s10489-020-02004-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D point clouds is an important type of geometric data structure, and the analysis of 3D point clouds based on deep learning is a very challenging task due to the disorder and irregularity. In existing research, RS-CNN provides an effective and promising method to obtain shape features on disordered point clouds directly, which encodes local features effectively. However, RS-CNN fails to consider point-wise features and global features, which are conducive to point clouds better. In this paper, we proposed PointFusionNet, which solves these problems effectively by fusing point-wise features, local features, and global features. We have designed Feature Fusion Convolution (FF-Conv) and Global Relationship Reasoning Module (GRRM) to build PointFusionNet. The point-wise features were fused with their corresponding local features in the FF-Conv and then mapped into a high-dimensional space to extract richer local features. The GRRM inferred the relationship between various parts, in order to capture global features for enriching the content of the feature descriptor. Therefore the PointFusionNet is suitable for point clouds classification and semantic segmentation by using the two distinctive modules. The PointFusionNet has been tested on ModelNet40 and ShapeNet part datasets, and the experiments show that PointFusionNet has a competitive advantage in shape classification and part segmentation tasks.},
  archive      = {J_APIN},
  author       = {Liang, Pan and Fang, Zhijun and Huang, Bo and Zhou, Heng and Tang, Xianhua and Zhong, Cengsi},
  doi          = {10.1007/s10489-020-02004-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2063-2076},
  shortjournal = {Appl. Intell.},
  title        = {PointFusionNet: Point feature fusion network for 3D point clouds analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolutionary many-objective optimization algorithm based on
angle and clustering. <em>APIN</em>, <em>51</em>(4), 2045–2062. (<a
href="https://doi.org/10.1007/s10489-020-01874-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In evolutionary multi-objective optimization, maintaining a well balance of convergence and diversity is particularly important for the performance of evolutionary algorithms. Considering the convergence and diversity at the same time, a many-objective optimization algorithm combining angle-based selection strategy and clustering strategy is proposed. In the former strategy, the whole population is divided into several partitions to ensure the diversity of the population, and superior individuals are selected to ensure the convergence of the population. The latter strategy, the individual vector angle is used to reflect the similarity and the individuals are divided into some clusters, which helps to describe the population distribution. The performance of this algorithm is compared with five state-of-the-art evolutionary many-objective optimization algorithms on a variety of benchmark test problems with 5, 10 and 15 objectives. The results suggest that the algorithm can slightly better competitive performance.},
  archive      = {J_APIN},
  author       = {Xiong, Zhijian and Yang, Jingming and Hu, Ziyu and Zhao, Zhiwei and Wang, Xiaojing},
  doi          = {10.1007/s10489-020-01874-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2045-2062},
  shortjournal = {Appl. Intell.},
  title        = {Evolutionary many-objective optimization algorithm based on angle and clustering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chameleon algorithm based on mutual k-nearest neighbors.
<em>APIN</em>, <em>51</em>(4), 2031–2044. (<a
href="https://doi.org/10.1007/s10489-020-01926-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a typical unsupervised data analysis method, which divides a given data set without label information into multiple clusters. The data on each cluster has a great deal of association, which can be used as the preprocessing stage of other algorithms or for further association analysis. Therefore, clustering plays an important role in a wide range of fields. Chameleon is a clustering algorithm that combines the relative interconnectivity and relative closeness to find clusters of arbitrary shape with high quality. However, the graph-partitioning technology hMETIS algorithm used in the algorithm is difficult to operate and easy to cause uncertainty of results. In addition, the final number of clusters need to be specified by user as a parameter to stop merging, which is difficult to determine without prior information. Aiming at these shortcomings, Chameleon algorithm based on mutual k-nearest neighbors (MChameleon) is proposed. Firstly, the idea of mutual k-nearest neighbors is introduced to directly generate sub-clusters, which omits the process of partitioning graph. Then, the concept of MC modularity is introduced, which is used to objectively identify the final clustering results. By experiments on artificial data sets and UCI data sets, we compared MChameleon with the original Chameleon algorithm, the improved AChameleon algorithm and the classic K-Means, DBSCAN, BIRCH algorithm in accuracy. Experimental results on data sets show that Chameleon algorithm based on mutual k-nearest neighbors has great advantages and is feasible.},
  archive      = {J_APIN},
  author       = {Zhang, Yuru and Ding, Shifei and Wang, Lijuan and Wang, Yanru and Ding, Ling},
  doi          = {10.1007/s10489-020-01926-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2031-2044},
  shortjournal = {Appl. Intell.},
  title        = {Chameleon algorithm based on mutual k-nearest neighbors},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infrared image super-resolution reconstruction by using
generative adversarial network with an attention mechanism.
<em>APIN</em>, <em>51</em>(4), 2018–2030. (<a
href="https://doi.org/10.1007/s10489-020-01987-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of infrared imaging principles and imaging systems, many problems are typically encountered with collected infrared images, such as low resolution, insufficient detail information, and blurred edges. In response to these problems, a method of infrared image super-resolution reconstruction that uses recursive attention and is based on a generative adversarial network is proposed. First, according to the characteristics of low-resolution infrared images such as uniform pixel distributions, low contrast, and poor perceived quality, a deep generator structure with a recursive-attention network is designed in this article. The recursive-attention module is used to extract high-frequency information from the feature maps, suppress useless information, and enhance the expressiveness of the features, which facilitates the reconstruction of texture details of infrared images. Then, to better distinguish the reconstructed images from the original high-resolution images, we designed a discriminator that was composed of a deep convolutional neural network. In addition, targeted improvements were made to the content loss function of GAN. We used the pre-trained VGG-19 network features before activation to calculate the perceptual loss, which helps recover the texture details of the infrared images. The experimental results on infrared image datasets demonstrated that the reconstruction performance of the proposed method is higher than those of several typical methods, and it realizes higher image visual quality.},
  archive      = {J_APIN},
  author       = {Liu, Qing-Ming and Jia, Rui-Sheng and Liu, Yan-Bo and Sun, Hai-Bin and Yu, Jian-Zhi and Sun, Hong-Mei},
  doi          = {10.1007/s10489-020-01987-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {2018-2030},
  shortjournal = {Appl. Intell.},
  title        = {Infrared image super-resolution reconstruction by using generative adversarial network with an attention mechanism},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel bat algorithm with dynamic membrane structure for
optimization problems. <em>APIN</em>, <em>51</em>(4), 1992–2017. (<a
href="https://doi.org/10.1007/s10489-020-01898-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the optimization efficiency for different optimization problems and take advantage of the dynamic membrane computing framework, this paper proposes an improved bat algorithm, namely, Dynamic Membrane-driven Bat Algorithm (DMBA). The dynamic construction of the DMBA algorithm aims at enhancing population diversity by balancing the exploration-exploitation tradeoff. Unlike the static membrane algorithms, the membranes in DMBA will be dynamically evolved by using merging and separation rules which help in maintaining the diversity of the population. The experimental results on a set of well-known benchmark functions including CEC 2005, CEC 2011, and CEC 2017 clearly prove the effectiveness of the proposed DMBA algorithm in terms of maintaining the diversity and exploitation capabilities compared to that of the others. It is shown that the proposed DMBA algorithm is superior to recent variants of the bat algorithm and other well-known algorithms in terms of solution accuracy and convergence speed.},
  archive      = {J_APIN},
  author       = {Alsalibi, Bisan and Abualigah, Laith and Khader, Ahamad Tajudin},
  doi          = {10.1007/s10489-020-01898-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1992-2017},
  shortjournal = {Appl. Intell.},
  title        = {A novel bat algorithm with dynamic membrane structure for optimization problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstruction of gene regulatory networks with
multi-objective particle swarm optimisers. <em>APIN</em>,
<em>51</em>(4), 1972–1991. (<a
href="https://doi.org/10.1007/s10489-020-01891-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational reconstruction of Gene Regulatory Networks (GRNs) from gene expression data has been modelled as a complex optimisation problem, which enables the use of sophisticated search methods to address it. Among these techniques, particle swarm optimisation based algorithms stand out as prominent techniques with fast convergence and accurate network inferences. A multi-objective approach for the inference of GRNs consists of optimising a given network’s topology while tuning the kinetic order parameters in an S-System, thus preventing the use of unnecessary penalty weights and enables the adoption of Pareto optimality based algorithms. In this study, we empirically assess the behaviour of a set of multi-objective particle swarm optimisers based on different archiving and leader selection strategies in the scope of the inference of GRNs. The main goal is to provide system biologists with experimental evidence about which optimisation technique performs with higher success for the inference of consistent GRNs. The experiments conducted involve time-series datasets of gene expression taken from the DREAM3/4 standard benchmarks, as well as in vivo datasets from IRMA and Melanoma cancer samples. Our study shows that multi-objective particle swarm optimiser OMOPSO obtains the best overall performance. Inferred networks show biological consistency in accordance with in vivo studies in the literature.},
  archive      = {J_APIN},
  author       = {Hurtado, Sandro and García-Nieto, José and Navas-Delgado, Ismael and Nebro, Antonio J. and Aldana-Montes, José F.},
  doi          = {10.1007/s10489-020-01891-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1972-1991},
  shortjournal = {Appl. Intell.},
  title        = {Reconstruction of gene regulatory networks with multi-objective particle swarm optimisers},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automatic framework for endoscopic image restoration and
enhancement. <em>APIN</em>, <em>51</em>(4), 1959–1971. (<a
href="https://doi.org/10.1007/s10489-020-01923-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its success in the field of minimally invasive surgery, endoscopy image analysis remains challenging due to limited image settings and control conditions. The low resolution and existence of large number of reflections in endoscopy images are the major problems in the automatic detection of objects. To address these issues, we presented a novel framework based on the convolutional neural networks. The proposed approach consists of three major parts. First, a deep learning (DL)-based image evaluation method is used to classify the input images into two groups, namely, specular highlights and weakly illuminated groups. Second, the specular highlight is detected using the DL-based method, and the reflected areas are recovered through a patch-based restoration operation. Lastly, gamma correction with optimized reflectance and illumination estimation is adopted to enhance the weakly illuminated images. The proposed method is compared against the existing ones, and the experimental results demonstrate that the former outperforms the latter in terms of subjective and objective assessments. This finding indicates that the proposed approach can serve as a potential tool for improving the quality of the endoscopy images used to examine internal body organs.},
  archive      = {J_APIN},
  author       = {Asif, Muhammad and Chen, Lei and Song, Hong and Yang, Jian and Frangi, Alejandro F.},
  doi          = {10.1007/s10489-020-01923-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1959-1971},
  shortjournal = {Appl. Intell.},
  title        = {An automatic framework for endoscopic image restoration and enhancement},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local-CycleGAN: A general end-to-end network for visual
enhancement in complex deep-water environment. <em>APIN</em>,
<em>51</em>(4), 1947–1958. (<a
href="https://doi.org/10.1007/s10489-020-01931-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image analysis is crucial for many applications such as seafloor survey, biological and environment monitoring, underwater vehicle navigation, inspection and maintenance of underwater infrastructure etc. However, due to light absorption and scattering, the images acquired underwater are always blurry and distorted in color. Most existing image enhancement algorithms typically focus on a few features of the imaging environments, and enhanced results depend on the characteristics of original images. In this study, a local cycle-consistent generative adversarial network is proposed to enhance images acquired in a complex deep-water environment. The proposed network uses a combination of a local discriminator and a global discriminator. Additionally, quality-monitor loss is adopted to evaluate the effect of the generated images. Experimental results show that the local cycle-consistent generative adversarial network is robust and can be generalized for many different image enhancement tasks in different types of complex deep-water environment with varied turbidity.},
  archive      = {J_APIN},
  author       = {Zong, Xianhui and Chen, Zhehan and Wang, Dadong},
  doi          = {10.1007/s10489-020-01931-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1947-1958},
  shortjournal = {Appl. Intell.},
  title        = {Local-CycleGAN: A general end-to-end network for visual enhancement in complex deep-water environment},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting and simulation of cutting force in virtual
surgery based on particle filtering. <em>APIN</em>, <em>51</em>(4),
1934–1946. (<a
href="https://doi.org/10.1007/s10489-020-01910-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate and realistic force feedback is very important in determining the realism of virtual surgery. In order to improve the accuracy of force simulation in cutting procedures, we proposed a novel method for forecasting and simulating the cutting force based on a particle filtering (PF) technique. Since the probability density function (PDF) is represented by particles, it is able to estimate accurately the interaction force between the surgical tool and soft tissue during cutting processes. The root mean square error (RMSE) of the PF-based method ranges from 0.0014 to 0.0034, and the mean absolute error (MAE) is less than 0.0399. Comparison of the experiment results with other methods demonstrated that the PF-based method can achieve a higher accuracy with different cutting speeds and angles. The application of the PF-based method to a virtual liver cutting procedure confirmed the effectiveness and accuracy of this method.},
  archive      = {J_APIN},
  author       = {Cheng, Qiangqiang and Sun, Pengyu and Yang, Chunsheng and Yu, Runqiao and Liu, Peter Xiaoping},
  doi          = {10.1007/s10489-020-01910-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1934-1946},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting and simulation of cutting force in virtual surgery based on particle filtering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). An effective method using clustering-based adaptive
decomposition and editing-based diversified oversamping for multi-class
imbalanced datasets. <em>APIN</em>, <em>51</em>(4), 1918–1933. (<a
href="https://doi.org/10.1007/s10489-020-01883-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multi-class imbalanced classification tasks that occur in many real-world applications, the class imbalance, which is caused by the case that some classes are not as frequent as other classes, and class overlap, which is caused by the case that some classes contains a similar number of data, are the major challenges. Both of them make the classification task complicated. The decomposition-based strategy is an effective way to improve the performance of multi-class imbalanced classification tasks. However, current studies based on this strategy have failed to solve the problems of class imbalance and overlapping simultaneously. Therefore, we propose an effective method , namely clustering-based adaptive decomposition and editing-based diversified oversamping procedure(CluAD-EdiDO), to solve the above problems in this paper. The proposed CluAD-EdiDO consists of two key components: the clustering-based adaptive decomposition and the editing-based diversified oversampling technique. The former is applied to group similar data samples of the data set into clusters(i.e., “sub-problems”). The latter is applied independently in different clusters to combat the imbalance and overlap, reducing the impact of the majority classes in overlapping region and oversampling the minority classes appropriately. Furthermore, a diversified ensemble learning framework is adopted to select the best classification algorithm for different sub-problems. Extensive experiments on 17 real-world datasets demonstrate that our method outperforms for multi-class imbalanced datasets.},
  archive      = {J_APIN},
  author       = {Chen, Xiangtao and Zhang, Lan and Wei, Xiaohui and Lu, Xinguo},
  doi          = {10.1007/s10489-020-01883-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1918-1933},
  shortjournal = {Appl. Intell.},
  title        = {An effective method using clustering-based adaptive decomposition and editing-based diversified oversamping for multi-class imbalanced datasets},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting multi-attention network with contextual influence
for point-of-interest recommendation. <em>APIN</em>, <em>51</em>(4),
1904–1917. (<a
href="https://doi.org/10.1007/s10489-020-01868-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-of-Interest (POI) recommendation has become an important service on Location-Based Social Networks (LBSNs). In order to improve the performance of recommendation, besides the check-in data generated in LBSNs, researchers are striving to exploit various auxiliary information such as social relation among users and geographical influence among neighbourhood POIs. However, existing works cannot effectively study the diverse degrees of influence from user’s friends, neither are they able to capture the feature impacts of POIs in the preference modelling process. To overcome these challenges, by making use of a M ulti-A ttention N etwork to learn the C ontextual influence of both users and POIs, this paper presents a model named MANC for POI recommendation. The MANC model consists of two parts: a user-friend module and a POI neighbourhood module. Unlike existing works which treat the influences from different friends of a user equally, the user-friend module in MANC applies an attention-based memory component to generate specific relation vectors which can differentiate the influence from the aspect of interest, and applies a friend-level attention network to adaptively capture the preferences of users. For the POI contextual information, the POI neighbourhood module in MANC applies a feature-level attention network to capture the latent features of neighbourhood POIs, and applies a POI-level attention network to capture the geographical influence among POIs. Extensive experiments are carried out, and it is shown that the MANC model achieves better performance than other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Chang, Liang and Chen, Wei and Huang, Jianbo and Bin, Chenzhong and Wang, Wenkai},
  doi          = {10.1007/s10489-020-01868-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1904-1917},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting multi-attention network with contextual influence for point-of-interest recommendation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster-based information retrieval using pattern mining.
<em>APIN</em>, <em>51</em>(4), 1888–1903. (<a
href="https://doi.org/10.1007/s10489-020-01922-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of responding to user queries by fetching the most relevant object from a clustered set of objects. It addresses the common drawbacks of cluster-based approaches and targets fast, high-quality information retrieval. For this purpose, a novel cluster-based information retrieval approach is proposed, named Cluster-based Retrieval using Pattern Mining (CRPM). This approach integrates various clustering and pattern mining algorithms. First, it generates clusters of objects that contain similar objects. Three clustering algorithms based on k-means, DBSCAN (Density-based spatial clustering of applications with noise), and Spectral are suggested to minimize the number of shared terms among the clusters of objects. Second, frequent and high-utility pattern mining algorithms are performed on each cluster to extract the pattern bases. Third, the clusters of objects are ranked for every query. In this context, two ranking strategies are proposed: i) Score Pattern Computing (SPC), which calculates a score representing the similarity between a user query and a cluster; and ii) Weighted Terms in Clusters (WTC), which calculates a weight for every term and uses the relevant terms to compute the score between a user query and each cluster. Irrelevant information derived from the pattern bases is also used to deal with unexpected user queries. To evaluate the proposed approach, extensive experiments were carried out on two use cases: the documents and tweets corpus. The results showed that the designed approach outperformed traditional and cluster-based information retrieval approaches in terms of the quality of the returned objects while being very competitive in terms of runtime.},
  archive      = {J_APIN},
  author       = {Djenouri, Youcef and Belhadi, Asma and Djenouri, Djamel and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s10489-020-01922-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1888-1903},
  shortjournal = {Appl. Intell.},
  title        = {Cluster-based information retrieval using pattern mining},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FB-GSA: A fuzzy bi-level programming based gravitational
search algorithm for unconstrained optimization. <em>APIN</em>,
<em>51</em>(4), 1857–1887. (<a
href="https://doi.org/10.1007/s10489-020-01884-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gravitational Search Algorithm (GSA) which is a prominent nature-inspired computing technique outperforms in the exploration stage, but its performance degrades in the exploitation stage. A fuzzy bi-level programming based gravitational search algorithm (FB-GSA) is proposed in this study. The basic concept to create FB-GSA is the iterative fuzzy decision-making operation. FB-GSA accompanies the algorithms such as Chaotic Gravitational Search Algorithm (CGSA), and the proposed local search using spectral Polak-Ribire-Polyak-3 (spectral PRP-3) method. Initially, the adaptive parameters, for the fuzzy decision-making process, are determined. Then, the controlled operation of constituent algorithms is executed using fuzzy Bi-level logic, which leads to an optimal solution. Experimental evaluation of FB-GSA is performed using several unimodal and multi-modal benchmark functions. Experimental results illustrate that FB-GSA outperforms other state-of-art works for most benchmarks. The simulation results for FB-GSA also presents a significant improvement in the convergence speed. The fuzzy-based adaptive control employed in FB-GSA makes it devoid of premature convergence.},
  archive      = {J_APIN},
  author       = {Das, Nitish and P., Aruna Priya},
  doi          = {10.1007/s10489-020-01884-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1857-1887},
  shortjournal = {Appl. Intell.},
  title        = {FB-GSA: A fuzzy bi-level programming based gravitational search algorithm for unconstrained optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale fractal residual network for image
super-resolution. <em>APIN</em>, <em>51</em>(4), 1845–1856. (<a
href="https://doi.org/10.1007/s10489-020-01909-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that the use of deep convolutional neural networks (CNNs) can improve the performance of single image super-resolution reconstruction (SISR) methods. However, the existing CNN-based SISR model ignores the multi-scale features and shallow and deep features of the image, resulting in relatively low image reconstruction performance. To address these issues, this paper proposes a new multi-scale fractal residual network (MSFRN) for image super-resolution. On the basis of residual learning, a multi-scale fractal residual block (MSFRB) is designed. This block uses convolution kernels of different sizes to extract image multi-scale features and uses multiple paths to extract and fuse image features of different depths. Then, the shallow features extracted at the shallow feature extraction stage and the local features output by all MSFRBs are used to perform global hierarchical feature fusion. Finally, through sub-pixel convolution, the fused global features are used to reconstruct high-resolution images from low-resolution images. The experimental results on the five standard benchmark datasets show that MSFRN improved subjective visual effects and objective image quality evaluation indicators, and is superior to other state-of-the-art SISR methods.},
  archive      = {J_APIN},
  author       = {Feng, Xinxin and Li, Xianguo and Li, Jianxiong},
  doi          = {10.1007/s10489-020-01909-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1845-1856},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale fractal residual network for image super-resolution},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural attention model for recommendation based on
factorization machines. <em>APIN</em>, <em>51</em>(4), 1829–1844. (<a
href="https://doi.org/10.1007/s10489-020-01921-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommendation systems, it is of vital importance to comprehensively consider various aspects of information to make accurate recommendations for users. When the low-order feature interactions between items are insufficient, it is necessary to mine information to learn higher-order feature interactions. In addition, to distinguish the different importance levels of feature interactions, larger weights should be assigned to features with larger contributions to predictions, and smaller weights to those with smaller contributions. Therefore, this paper proposes a neural attention model for recommendation (NAM), which deepens factorization machines (FMs) by adding an attention mechanism and fully connected layers. Through the attention mechanism, NAM can learn the different importance levels of low-order feature interactions. By adding fully connected layers on top of the attention component, NAM can model high-order feature interactions in a nonlinear way. Experiments on two real-world datasets demonstrate that NAM has excellent performance and is superior to FM and other state-of-the-art models. The results demonstrate the effectiveness of the proposed model and the potential of using neural networks for prediction under sparse data.},
  archive      = {J_APIN},
  author       = {Wen, Peng and Yuan, Weihua and Qin, Qianqian and Sang, Sheng and Zhang, Zhijun},
  doi          = {10.1007/s10489-020-01921-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1829-1844},
  shortjournal = {Appl. Intell.},
  title        = {Neural attention model for recommendation based on factorization machines},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group competition-cooperation optimization algorithm.
<em>APIN</em>, <em>51</em>(4), 1813–1828. (<a
href="https://doi.org/10.1007/s10489-020-01913-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve complex practical problems, the model of deep learning can not be limited to models such as deep neural networks. To deepen the learning model, we must actively explore various depth models. Based on this, we propose a deep evolutionary algorithm, that is group competition cooperation optimization (GCCO) algorithm. Unlike the deep learning, in the GCCO algorithm, depth is mainly reflected in multi-step iterations, feature transformation, and models are complex enough. Firstly, the bio-group model is introduced to simulate the behavior that the animals hunt for the food. Secondly, according to the rules of mutual benefit and survival of the fittest in nature, the competition model and cooperation model are introduced. Furthermore, in the individual mobility strategy, the wanderers adopt stochastic movement strategy based on feature transformation to avoid local optimization. The followers adopt the variable step size region replication method to balance the convergence speed and optimization precision. Finally, the GCCO algorithm and the other three comparison algorithms are used to test the performance of the algorithm on ten optimization functions. At the same time, in the actual problem of setting up the Shanghai gas station the to improve the timely rate, GCCO algorithm achieves better performance than the other three algorithms. Moreover, Compared to the Global Search, the GCCO algorithm takes less time to achieve similar effects to the Global Search.},
  archive      = {J_APIN},
  author       = {Chen, Haijuan and Feng, Xiang and Yu, Huiqun},
  doi          = {10.1007/s10489-020-01913-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1813-1828},
  shortjournal = {Appl. Intell.},
  title        = {Group competition-cooperation optimization algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic ranking structure preserving for cross-modal
retrieval. <em>APIN</em>, <em>51</em>(3), 1802–1812. (<a
href="https://doi.org/10.1007/s10489-020-01930-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval not only needs to eliminate the heterogeneity of modalities, but also needs to constrain the return order of retrieval results. Accordingly, we propose a novel common representation space learning method, called Semantic Ranking Structure Preserving (SRSP) for Cross-modal Retrieval in this paper. First, the dependency relationship between labels is used to minimize the discriminative loss of multi-modal data and mine potential relationships between samples to get richer semantic information in the common space. Second, we constrain the correlation ranking of representations in common space, so as to break the modal gap and promote the multi-modal correlation learning. The comprehensive experimental comparison results show that our algorithm substantially enhances the performance and consistently outperforms very recent algorithms in terms of widely used cross-modal benchmark datasets.},
  archive      = {J_APIN},
  author       = {Liu, Hui and Feng, Yong and Zhou, Mingliang and Qiang, Baohua},
  doi          = {10.1007/s10489-020-01930-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1802-1812},
  shortjournal = {Appl. Intell.},
  title        = {Semantic ranking structure preserving for cross-modal retrieval},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trainable activation function with differentiable negative
side and adaptable rectified point. <em>APIN</em>, <em>51</em>(3),
1784–1801. (<a
href="https://doi.org/10.1007/s10489-020-01885-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activation function is one of the keys for Artificial Neural Network in learning complex mapping function. A recently proposed activation function called Scaled Exponential Linear Unit (SELU) has a unique characteristic in the ability to automatically normalize its output toward predefined mean and variance. In this paper, we introduce Parametric Scaled Exponential Linear Unit (PSELU), a modification of SELU where the parameters are adaptively learned during the training phase via backpropagation algorithm. We then add further modifications with the aim of having a stronger gradient in the negative part and produce more negative output from our proposed method. Our proposed method is evaluated using various artificial neural network models on diverse image classification tasks. In addition, we also measure the performance of our proposed method in comparison with some other popular activation functions. The evaluation results in this paper present some empirical proof of improvement in term of network generalization performance. Moreover, our experiments strengthen our initial hypothesis regarding the advantages obtained from using our proposed method. Finally, the behavior of the trained parameters along with the training phase dynamic is observed in order to better understand the increased performance of our proposed method},
  archive      = {J_APIN},
  author       = {Pratama, Kevin and Kang, Dae-Ki},
  doi          = {10.1007/s10489-020-01885-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1784-1801},
  shortjournal = {Appl. Intell.},
  title        = {Trainable activation function with differentiable negative side and adaptable rectified point},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel framework based on wavelet transform and principal
component for face recognition under varying illumination.
<em>APIN</em>, <em>51</em>(3), 1762–1783. (<a
href="https://doi.org/10.1007/s10489-020-01924-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most primitive problems in face recognition is illumination variation, which is of significance for image processing and pattern recognition. Existing studies concentrate on the wavelet transform (WT) without elaborately considering high-frequency information and most do not efficiently tackle a computation explosion of facial dimensions in classification. Therefore, a novel framework based on wavelet transform and principal component is proposed to improve the accuracy under illumination variation in this paper. In the proposed framework, low-frequency sub-band (LFSB) and high-frequency sub-band (HFSB) images from wavelet transform are simultaneously enhanced and denoised, unlike previous studies that usually cause loss of details due to less consideration of HFSB. For LFSB image, a multiple scale Retinex-based steering kernel is designed to enhance more details, and then an adaptive strategy of gamma correction is developed to automatically expand gray-dynamic range. For HFSB image, a non-local mean filtering is established to suppress the noise and subsequently, more image details are preserved by local of mean of local variance. Moreover, the principal component technique based Fisherface and virtual auxiliary sample strategy is developed in order to overcome the computation explosion of facial dimensions, in which a sample strategy with interpolation mechanism is employed to avoid the complicated singularity and Fisherface analysis is further applied to extract features and dimensionality reduction. In addition, the particle swarm optimization-neural network (PSO-NN) is employed to perform classification in the framework. Experimental results prove that the proposed framework can effectively obtain the robust visual effect under varying illumination and significantly improve the recognition performance in comparison to existing methods.},
  archive      = {J_APIN},
  author       = {Liang, Hongtao and Gao, Jie and Qiang, Ning},
  doi          = {10.1007/s10489-020-01924-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1762-1783},
  shortjournal = {Appl. Intell.},
  title        = {A novel framework based on wavelet transform and principal component for face recognition under varying illumination},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive evolutionary generative adversarial network.
<em>APIN</em>, <em>51</em>(3), 1747–1761. (<a
href="https://doi.org/10.1007/s10489-020-01917-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) is an effective method to learn generative models from real data. But there are some drawbacks such as instability, mode collapse and low computational efficiency in the existing GANs. In this paper, attentive evolutionary generative adversarial network (AEGAN) model is proposed in order to improve these disadvantages of GANs. The modified evolutionary algorithm is designed for the AEGAN. In the AEGAN the generator evolves continuously to resist the discriminator by three independent mutations at every batch and only the well-performing offspring (i.e.,the generators) can be preserved at next batch. Furthermore, a normalized self-attention (SA) mechanism is embedded in the discriminator and generator of AEGAN to adaptively assign weights according to the importance of features. We propose careful regulation of the generators evolution and an effective weight assignment to improve diversity and long-range dependence. We also propose a superior training algorithm for AEGAN. With the algorithm, the AEGAN overcomes the shortcomings of traditional GANs brought by single loss function and deep convolution and it greatly improves the training stability and statistical efficiency. Extensive image synthesis experiments on CIFAR-10, CelebA and LSUN datasets are presented to validate the performance of AEGAN. Experimental results and comparisons with other GANs show that the proposed model is superior to the existing models.},
  archive      = {J_APIN},
  author       = {Wu, Zhongze and He, Chunmei and Yang, Liwen and Kuang, Fangjun},
  doi          = {10.1007/s10489-020-01917-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1747-1761},
  shortjournal = {Appl. Intell.},
  title        = {Attentive evolutionary generative adversarial network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speeding up distributed pseudo-tree optimization procedures
with cross edge consistency to solve DCOPs. <em>APIN</em>,
<em>51</em>(3), 1733–1746. (<a
href="https://doi.org/10.1007/s10489-020-01860-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Distributed Pseudo-tree Optimization Procedure (DPOP) is a well-known message passing algorithm that provides optimal solutions to Distributed Constraint Optimization Problems (DCOPs) in cooperative multi-agent systems. However, the traditional DCOP formulation does not consider constraints that must be satisfied (hard constraints), rather it concentrates only on constraints that place no restriction on satisfaction (soft constraints). This is a serious shortcoming as many real-world applications involve both types of constraints. Traditional DPOP algorithms are not able to benefit from the existence of hard constraints, where an additional calculation is required to handle such constraints. This results in longer runtimes. Thus scalability remains an issue. Additionally, in the standard DPOP, the agents are arranged as a Depth First Search (DFS) pseudo-tree, but recent work has shown that the construction of pseudo-trees in this way often leads to chain-like communication structures that greatly impair the algorithm’s performance. To address these issues, we develop an algorithm that speeds up the DPOP algorithm by reducing the size of the messages exchanged and increases parallelism in the pseudo tree. For this purpose, initially, we improve the path for exchanging messages. Next, we introduce a new form of constraint propagation, which we call cross-edge consistency. Our theoretical evaluation shows that our proposed algorithm is complete and correct. In empirical evaluations, our algorithm achieves a significant reduction in the runtime, ranging from 4% to 96%, compared to the state-of-the-art.},
  archive      = {J_APIN},
  author       = {Rashik, Mashrur and Rahman, Md. Musfiqur and Khan, Md. Mosaddek and Mamun-or-Rashid, Md. and Tran-Thanh, Long and Jennings, Nicholas R.},
  doi          = {10.1007/s10489-020-01860-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1733-1746},
  shortjournal = {Appl. Intell.},
  title        = {Speeding up distributed pseudo-tree optimization procedures with cross edge consistency to solve DCOPs},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An intelligent and blind dual color image watermarking for
authentication and copyright protection. <em>APIN</em>, <em>51</em>(3),
1701–1732. (<a
href="https://doi.org/10.1007/s10489-020-01903-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a blind dual watermarking scheme for color images is proposed by embedding an invisible robust watermark to protect copyright, as well as a fragile watermark is embedded for image authentication. For the purpose of copyright protection, the robust watermark is embedded into the blue channel of RGB color space based on DWT, HVS and SVD domains using a specialized PSO optimization to balance the trade-off between robustness and imperceptibility. In addition, the robust watermarking capacity in SVD is doubled by inserting two robust watermark bits into each selected blocks and the robust watermark can be extracted blindly. For the purpose of authentication, a fragile watermark is embedded into all channels of RGB color space using a new way to manipulate the diagonal singular values. The authenticity of a suspected image can be verified in the absence of original watermark and host images. The combination of robust and fragile watermarking in the proposed scheme provides a suitable mechanism to protect valuable and original color images. According to the experimental and comparative results, the proposed scheme provides superior outcomes with high robustness, imperceptibility, and capacity along with a good accuracy rate in locating the tampered area of an image.},
  archive      = {J_APIN},
  author       = {Ahmadi, Sajjad Bagheri Baba and Zhang, Gongxuan and Rabbani, Mahdi and Boukela, Lynda and Jelodar, Hamed},
  doi          = {10.1007/s10489-020-01903-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1701-1732},
  shortjournal = {Appl. Intell.},
  title        = {An intelligent and blind dual color image watermarking for authentication and copyright protection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning based detection and analysis of COVID-19 on
chest x-ray images. <em>APIN</em>, <em>51</em>(3), 1690–1700. (<a
href="https://doi.org/10.1007/s10489-020-01902-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covid-19 is a rapidly spreading viral disease that infects not only humans, but animals are also infected because of this disease. The daily life of human beings, their health, and the economy of a country are affected due to this deadly viral disease. Covid-19 is a common spreading disease, and till now, not a single country can prepare a vaccine for COVID-19. A clinical study of COVID-19 infected patients has shown that these types of patients are mostly infected from a lung infection after coming in contact with this disease. Chest x-ray (i.e., radiography) and chest CT are a more effective imaging technique for diagnosing lunge related problems. Still, a substantial chest x-ray is a lower cost process in comparison to chest CT. Deep learning is the most successful technique of machine learning, which provides useful analysis to study a large amount of chest x-ray images that can critically impact on screening of Covid-19. In this work, we have taken the PA view of chest x-ray scans for covid-19 affected patients as well as healthy patients. After cleaning up the images and applying data augmentation, we have used deep learning-based CNN models and compared their performance. We have compared Inception V3, Xception, and ResNeXt models and examined their accuracy. To analyze the model performance, 6432 chest x-ray scans samples have been collected from the Kaggle repository, out of which 5467 were used for training and 965 for validation. In result analysis, the Xception model gives the highest accuracy (i.e., 97.97%) for detecting Chest X-rays images as compared to other models. This work only focuses on possible methods of classifying covid-19 infected patients and does not claim any medical accuracy.},
  archive      = {J_APIN},
  author       = {Jain, Rachna and Gupta, Meenu and Taneja, Soham and Hemanth, D. Jude},
  doi          = {10.1007/s10489-020-01902-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1690-1700},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning based detection and analysis of COVID-19 on chest X-ray images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel online sequential extreme learning machine with
l2,1-norm regularization for prediction problems. <em>APIN</em>,
<em>51</em>(3), 1669–1689. (<a
href="https://doi.org/10.1007/s10489-020-01890-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s world, data is produced at a very high speed and used in a large number of prediction problems. Therefore, the sequential nature of learning algorithms is in demand for batch learning algorithms. This paper presents a novel online sequential algorithm for extreme learning machine with l2,1-norm regularization (LR21OS-ELM) to handle the real-time sequential data. Wang et al. have given ELM with l2,1-norm based regularization namely LR21-ELM. This method is a batch processing model which takes data in a single chunk. So, whenever a new chunk of data arrives the model has to be retrained which takes a lot of time and memory. The proposed sequential algorithm does not require building a new model each time data arrives. This will update the previous model with new data that will save time and memory. The l2,1-norm regularization is a structural sparse-inducing norm which is integrated with an online sequential learning algorithm to diminish the complexity of the learning model by eliminating the redundant neurons of OS-ELM model. This paper proposes an iterative bi-objective optimization algorithm to solve l2,1 norm-based minimization problem and to handle the real time sequential data. The proposed model can learn sequentially arriving data in the form of chunks where chunk size can be fixed or varying. The experimental study has been conducted on several benchmark datasets collected from different research domains to prove the generalization ability of the proposed algorithm. The obtained results show that LR21OS-ELM combines the advantages of l2,1-norm regularization and online sequential learning of data and improves the prediction performance of the system.},
  archive      = {J_APIN},
  author       = {Preeti and Bala, Rajni and Dagar, Ankita and Singh, Ram Pal},
  doi          = {10.1007/s10489-020-01890-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1669-1689},
  shortjournal = {Appl. Intell.},
  title        = {A novel online sequential extreme learning machine with l2,1-norm regularization for prediction problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An enhanced particle swarm optimization algorithm to solve
probabilistic load flow problem in a micro-grid. <em>APIN</em>,
<em>51</em>(3), 1645–1668. (<a
href="https://doi.org/10.1007/s10489-020-01872-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an Enhanced PSO (EPSO) method to address the weaknesses associated with the traditional PSO such as high sensitivity to the initial conditions, fast convergence, decline of solutions’ variety, and trapping in the local optimum. For this, several strategies are suggested including segmentation of search space, modification of solution’s updating rule, accepting poor solutions using an intelligent probabilistic function, searching in the regions with poor solutions, gradually removing the regions with poor solutions, and focusing the solutions on the local search after removing all regions with poor solutions. The performance of the EPSO was investigated using the 30 CEC 2014 test functions, 30 CEC 2017 test functions, 10 standard optimization algorithms, and a challenging optimization problem called Probabilistic Load Flow (PLF) in a distribution network. The results of the Wilcoxon signed-rank test on the 2014 and 2017 test functions revealed the superiority of the EPSO over nearly 80% of cases compared to the other algorithms. The obtained results of solving 10 benchmark functions by the proposed EPSO and the other six improved PSO algorithms indicated advantages of the proposed EPSO compared to the other algorithms in finding the optimal value. Meanwhile, the proposed EPSO took the average time of 0.66 s to solve the 10 test functions; it was the shortest time compared to other improved PSO algorithms. According to the results of implementing the EPSO algorithm and other algorithms with random agents for 61 times, in more than 65% of the test functions, the proposed EPSO could find the global optimal solution in a shorter time than the other algorithms. The results of solving the probabilistic load flow problem indicated 89% similarity of the results of the proposed EPSO to those of the most accurate method i.e. Monte Carlo Simulation (MCS). Comparison of the obtained results with the other algorithms as well as outcomes of several improved versions of the PSO indicated the competitive proficiency of the proposed EPSO in various optimization circumstances.},
  archive      = {J_APIN},
  author       = {Bagheri Tolabi, Hajar and Lashkar Ara, Afshin and Hosseini, Rahil},
  doi          = {10.1007/s10489-020-01872-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1645-1668},
  shortjournal = {Appl. Intell.},
  title        = {An enhanced particle swarm optimization algorithm to solve probabilistic load flow problem in a micro-grid},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-effective workflow scheduling approach on cloud under
deadline constraint using firefly algorithm. <em>APIN</em>,
<em>51</em>(3), 1629–1644. (<a
href="https://doi.org/10.1007/s10489-020-01875-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing, a novel and promising methodology in the distributed computing domain, provides a pay-per-use framework to solve large-scale scientific and business workflow applications. These workflow applications have a constraint that each of them must completed within the limited time (deadline constraint). Therefore, scheduling a workflow with deadline constraints is increasingly becoming a crucial research issue. However, many analytical reviews on scheduling problems reveal that existing solutions fail to provide cost-effective solutions and they do not consider the parameters like CPU performance variation, delay in acquisition and termination of Virtual Machines (VMs). This paper presents a Cost-Effective Firefly based Algorithm (CEFA) to solve workflow scheduling problems that can occur in an Infrastructure as a Service (IaaS) platform. The proposed CEFA uses a novel method for problem encoding, population initialization and fitness evaluation with an objective to provide cost-effective and optimized workflow execution within the time limit. The performance of the proposed CEFA is compared with the state-of-the-art algorithms such as IaaS Cloud-Partial Critical Path (IC-PCP), Particle Swarm Optimization (PSO), Robustness-Cost-Time (RCT), Robustness-Time-Cost (RTC), and Regressive Whale Optimization (RWO). Our experimental results demonstrate that the proposed CEFA outperforms current state-of-the-art heuristics with the criteria of achieving the deadline constraint and minimizing the cost of execution.},
  archive      = {J_APIN},
  author       = {Chakravarthi, Koneti Kalyan and Shyamala, L. and Vaidehi, V.},
  doi          = {10.1007/s10489-020-01875-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1629-1644},
  shortjournal = {Appl. Intell.},
  title        = {Cost-effective workflow scheduling approach on cloud under deadline constraint using firefly algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthetic CT images for semi-sequential detection and
segmentation of lung nodules. <em>APIN</em>, <em>51</em>(3), 1616–1628.
(<a href="https://doi.org/10.1007/s10489-020-01914-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately detecting and segmenting lung nodules from CT images play a critical role in the earlier diagnosis of lung cancer and thus have attracted much interest from the research community. However, due to the irregular shapes of nodules, and the low-intensity contrast between the nodules and other lung areas, precisely segmenting nodules from lung CT images is a very challenging task. In this paper, we propose a highly effective and robust solution to this problem by innovatively utilizing the changes of nodule shapes over continuous slices (inter-slice changes) and develop a deep learning based end-to-end system. Different from the existing 2.5D or 3D methods that attempt to explore the inter-slice features, we propose to create a novel synthetic image to depict the unique changing pattern of nodules between slices in distinctive colour patterns. Based on the new synthetic images, we then adopt the deep learning based image segmentation techniques and develop a modified U-Net architecture to learn the unique color patterns formed by nodules. With our proposed approach, the detection and segmentation of nodules can be achieved simultaneously with an accuracy significantly higher than the state of the arts by 10% without introducing high computation cost. By taking advantage of inter-slice information and form the proposed synthetic image, the task of lung nodule segmentation is done more accurately and effectively.},
  archive      = {J_APIN},
  author       = {Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Wang, Qingqing and Kennedy, Paul J.},
  doi          = {10.1007/s10489-020-01914-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1616-1628},
  shortjournal = {Appl. Intell.},
  title        = {Synthetic CT images for semi-sequential detection and segmentation of lung nodules},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernelized fuzzy rough sets based online streaming feature
selection for large-scale hierarchical classification. <em>APIN</em>,
<em>51</em>(3), 1602–1615. (<a
href="https://doi.org/10.1007/s10489-020-01863-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many online streaming feature selection approaches focus on flat data, which means that all data are taken as a whole. However, in the era of big data, not only the feature space of data has unknown and evolutionary characteristics, but also the label space of data exists hierarchical structure. To address this problem, an online streaming feature selection framework for large-scale hierarchical classification task is proposed. The framework consists of three parts: (1) a new hierarchical data-oriented kernelized fuzzy rough model with sibling strategy is constructed, (2) the online important feature is selected based on feature correlation analysis, and (3) the online redundant feature is deleted based on feature redundancy. Finally, an empirical study using several hierarchical classification data sets manifests that the proposed method outperforms other state-of-the-art online streaming feature selection methods.},
  archive      = {J_APIN},
  author       = {Bai, Shengxing and Lin, Yaojin and Lv, Yan and Chen, Jinkun and Wang, Chenxi},
  doi          = {10.1007/s10489-020-01863-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1602-1615},
  shortjournal = {Appl. Intell.},
  title        = {Kernelized fuzzy rough sets based online streaming feature selection for large-scale hierarchical classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Proof searching and prediction in HOL4 with
evolutionary/heuristic and deep learning techniques. <em>APIN</em>,
<em>51</em>(3), 1580–1601. (<a
href="https://doi.org/10.1007/s10489-020-01837-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive theorem provers (ITPs), also known as proof assistants, allow human users to write and verify formal proofs. The proof development process in ITPs can be a cumbersome and time-consuming activity due to manual user interactions. This makes proof guidance and proof automation the two most desired features for ITPs. In this paper, we first provide two evolutionary and heuristic-based proof searching approaches for the HOL4 proof assistant, where a genetic algorithm (GA) and simulated annealing (SA) is used to search and optimize the proofs in different HOL theories. In both approaches, random proof sequences are first generated from a population of frequently occurring HOL4 proof steps that are discovered with sequential pattern mining. Generated proof sequences are then evolved using GA operators (crossover and mutation) and by applying the annealing process of SA till their fitness match the fitness of the target proof sequences. Experiments were done to compare the performance of SA with that of GA. Results have shown that the two proof searching approaches can be used to efficiently evolve the random sequences to obtain the target sequences. However, these approaches lack the ability to learn the proof process, that is important for the prediction of new proof sequences. For this purpose, we propose to use a deep learning technique known as long short-term memory (LSTM). LSTM is trained on various HOL4 theories for proof learning and prediction. Experimental results suggest that combining evolutionary/heuristic and deep learning techniques with proof assistants can greatly facilitate proof finding/optimization and proof prediction.},
  archive      = {J_APIN},
  author       = {Nawaz, M. Saqib and Nawaz, M. Zohaib and Hasan, Osman and Fournier-Viger, Philippe and Sun, Meng},
  doi          = {10.1007/s10489-020-01837-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1580-1601},
  shortjournal = {Appl. Intell.},
  title        = {Proof searching and prediction in HOL4 with evolutionary/heuristic and deep learning techniques},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel first order bayesian optimization with an application
to reinforcement learning. <em>APIN</em>, <em>51</em>(3), 1565–1579. (<a
href="https://doi.org/10.1007/s10489-020-01896-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zeroth Order Bayesian Optimization (ZOBO) methods optimize an unknown function based on its black-box evaluations at the query locations. Unlike most optimization procedures, ZOBO methods fail to utilize gradient information even when it is available. On the other hand, First Order Bayesian Optimization (FOBO) methods exploit the available gradient information to arrive at better solutions faster. However, the existing FOBO methods do not utilize a crucial information that the gradient is zero at the optima. Further, the inherent sequential nature of the FOBO methods incur high computational cost limiting their wide applicability. To alleviate the aforementioned difficulties of FOBO methods, we propose a relaxed statistical model to leverage the gradient information that directly searches for points where gradient vanishes. To accomplish this, we develop novel acquisition algorithms that search for global optima effectively. Unlike the existing FOBO methods, the proposed methods are parallelizable. Through extensive experimentation on standard test functions, we compare the performance of our methods over the existing methods. Furthermore, we explore an application of the proposed FOBO methods in the context of policy gradient reinforcement learning.},
  archive      = {J_APIN},
  author       = {J., Prabuchandran K. and Penubothula, Santosh and Kamanchi, Chandramouli and Bhatnagar, Shalabh},
  doi          = {10.1007/s10489-020-01896-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1565-1579},
  shortjournal = {Appl. Intell.},
  title        = {Novel first order bayesian optimization with an application to reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond missing: Weakly-supervised multi-label learning with
incomplete and noisy labels. <em>APIN</em>, <em>51</em>(3), 1552–1564.
(<a href="https://doi.org/10.1007/s10489-020-01878-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised multi-label learning has received much attention more recently, and most of the existing methods focus on such problem with either missing or noisy labels, while the issue with both missing and noisy labels has not been well investigated. In this paper, we propose a novel COst-sensitive label Ranking Approach with Low-rank and Sparse constraints (CORALS) to enrich the missing labels and remove the noisy labels simultaneously. Unlike most existing studies that an indicator matrix needs to be given in advance which may not be available in reality, a label confidence matrix is constructed to reflect the relevance between the labels and the corresponding instances, and then the relevance ordering of all possible labels including both missing and noisy labels on each instance is optimized by minimizing a cost-sensitive ranking loss. By considering the dependencies in both feature space and label space, we exploit the dual low-rank regularization terms to capture the corresponding correlations. Afterwards, noticing the fact that both missing and noisy labels are rare, the sparse regularization term is encoded to constrain such noisy information to be sparse. Comprehensive experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_APIN},
  author       = {Sun, Lijuan and Lyu, Gengyu and Feng, Songhe and Huang, Xiankai},
  doi          = {10.1007/s10489-020-01878-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1552-1564},
  shortjournal = {Appl. Intell.},
  title        = {Beyond missing: Weakly-supervised multi-label learning with incomplete and noisy labels},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Archimedes optimization algorithm: A new metaheuristic
algorithm for solving optimization problems. <em>APIN</em>,
<em>51</em>(3), 1531–1551. (<a
href="https://doi.org/10.1007/s10489-020-01893-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty and complexity of the real-world numerical optimization problems has grown manifold, which demands efficient optimization methods. To date, various metaheuristic approaches have been introduced, but only a few have earned recognition in research community. In this paper, a new metaheuristic algorithm called Archimedes optimization algorithm (AOA) is introduced to solve the optimization problems. AOA is devised with inspirations from an interesting law of physics Archimedes’ Principle. It imitates the principle of buoyant force exerted upward on an object, partially or fully immersed in fluid, is proportional to weight of the displaced fluid. To evaluate performance, the proposed AOA algorithm is tested on CEC’17 test suite and four engineering design problems. The solutions obtained with AOA have outperformed well-known state-of-the-art and recently introduced metaheuristic algorithms such genetic algorithms (GA), particle swarm optimization (PSO), differential evolution variants L-SHADE and LSHADE-EpSin, whale optimization algorithm (WOA), sine-cosine algorithm (SCA), Harris’ hawk optimization (HHO), and equilibrium optimizer (EO). The experimental results suggest that AOA is a high-performance optimization tool with respect to convergence speed and exploration-exploitation balance, as it is effectively applicable for solving complex problems. The source code is currently available for public from: https://www.mathworks.com/matlabcentral/fileexchange/79822-archimedes-optimization-algorithm},
  archive      = {J_APIN},
  author       = {Hashim, Fatma A. and Hussain, Kashif and Houssein, Essam H. and Mabrouk, Mai S. and Al-Atabany, Walid},
  doi          = {10.1007/s10489-020-01893-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1531-1551},
  shortjournal = {Appl. Intell.},
  title        = {Archimedes optimization algorithm: A new metaheuristic algorithm for solving optimization problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel competitive-cooperative learning models (cclms) based
on higher order information sets. <em>APIN</em>, <em>51</em>(3),
1513–1530. (<a
href="https://doi.org/10.1007/s10489-020-01881-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two novel competitive-cooperative learning models (CCLM) for achieving goals by human contenders. These models have two phases, viz., Competition phase and Cooperation phase. CCLM based on Hanman Transform (HT) is called HT-CCLM and that using a new concept termed Pervasive Information set is called PIS-CCLM. In the competition phase of HT-CCLM, each contender emulates the effort of best achiever by taking the difference of Hanman Transform values associated with the efforts of an individual and the best achiever whereas in the cooperation phase the differential of HT values of efforts of two random contenders is considered. In PIS-CCLM pervasive information value obtained from hesitancy values are used in the competition phase only. We have also carried out Wilcoxon test to establish the superiority of the proposed HT-CCLM and PIS-CCLM.},
  archive      = {J_APIN},
  author       = {Grover, Jyotsana and Hanmandlu, Madasu},
  doi          = {10.1007/s10489-020-01881-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1513-1530},
  shortjournal = {Appl. Intell.},
  title        = {Novel competitive-cooperative learning models (cclms) based on higher order information sets},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced gaussian process regression-based forecasting model
for COVID-19 outbreak and significance of IoT for its detection.
<em>APIN</em>, <em>51</em>(3), 1492–1512. (<a
href="https://doi.org/10.1007/s10489-020-01889-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virus based epidemic is one of the speedy and widely spread infectious disease which can affect the economy of the country as well as it is life-threatening too. So, there is a need to forecast the epidemic lifespan, which can help us in taking preventive measures and remedial action on time. These preventive measures and corrective action may consist of closing schools, closing malls, closing theaters, sealing of borders, suspension of public services, and suspension of traveling. Resuming such restrictions is depends upon the outbreak momentum and its decay rate. The accurate forecasting of the epidemic lifespan is one of the enormously essential and challenging tasks. It is a challenging task because the lack of knowledge about the novel virus-based diseases and its consequences with complicated societal-governmental factors can influence the widespread of this newly born disease. At this stage, any forecasting can play a vital role, and it will be reliable too. As we know, the novel virus-based diseases are in a growing phase, and we also do not have real-time data samples. Thus, the biggest challenge is to find out the machine learning-based best forecasting model, which could offer better forecasting with the limited training samples. In this paper, the Multi-Task Gaussian Process (MTGP) regression model with enhanced predictions of novel coronavirus (COVID-19) outbreak is proposed. The purpose of the proposed MTGP regression model is to predict the COVID-19 outbreak worldwide. It will help the countries in planning their preventive measures to reduce the overall impact of the speedy and widely spread infectious disease. The result of the proposed model has been compared with the other prediction model to find out its suitability and correctness. In subsequent analysis, the significance of IoT based devices in COVID-19 detection and prevention has been discussed.},
  archive      = {J_APIN},
  author       = {Ketu, Shwet and Mishra, Pramod Kumar},
  doi          = {10.1007/s10489-020-01889-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1492-1512},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced gaussian process regression-based forecasting model for COVID-19 outbreak and significance of IoT for its detection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A divide-and-unite deep network for person
re-identification. <em>APIN</em>, <em>51</em>(3), 1479–1491. (<a
href="https://doi.org/10.1007/s10489-020-01880-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (person re-ID) is one of the most challenging tasks in the field of computer vision as it involves large variations in human appearances, human poses, background illuminations, camera views, etc. In recent literature, using part-level features for the person re-ID task provides fine-grained information, and has been proven to be effective. Instead of relying on additional skeleton key points or pose estimation models, this paper proposes a Divide-and-Unite Network to obtain feature embedding end-to-end. We design a deep network guided by image contents, which divides pedestrians into parts and obtains the part features with different contributions. These part features and the global feature are united to obtain the pedestrian descriptor for person re-ID. To summarize, the contributions of this work are two-fold. Firstly, a novel architecture of discriminative descriptor learning is proposed, which is based on the global feature and supplemented by part features. Secondly, a Feature Division Network is constructed to generate the part features with different contributions, where the divided parts maintain the consistency of content between different images. Extensive experiments are conducted on three widely-used benchmarks including Market1501, CUHK03, and DukeMTMC-reID. The results have demonstrated that the proposed model can achieve remarkable performance against numerous state-of-the-arts.},
  archive      = {J_APIN},
  author       = {Li, Rui and Zhang, Baopeng and Teng, Zhu and Fan, Jianping},
  doi          = {10.1007/s10489-020-01880-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1479-1491},
  shortjournal = {Appl. Intell.},
  title        = {A divide-and-unite deep network for person re-identification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LipschitzLR: Using theoretically computed adaptive learning
rates for fast convergence. <em>APIN</em>, <em>51</em>(3), 1460–1478.
(<a href="https://doi.org/10.1007/s10489-020-01892-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel theoretical framework for computing large, adaptive learning rates. Our framework makes minimal assumptions on the activations used and exploits the functional properties of the loss function. Specifically, we show that the inverse of the Lipschitz constant of the loss function is an ideal learning rate. We analytically compute formulas for the Lipschitz constant of several loss functions, and through extensive experimentation, demonstrate the strength of our approach using several architectures and datasets. In addition, we detail the computation of learning rates when other optimizers, namely, SGD with momentum, RMSprop, and Adam, are used. Compared to standard choices of learning rates, our approach converges faster, and yields better results.},
  archive      = {J_APIN},
  author       = {Yedida, Rahul and Saha, Snehanshu and Prashanth, Tejas},
  doi          = {10.1007/s10489-020-01892-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1460-1478},
  shortjournal = {Appl. Intell.},
  title        = {LipschitzLR: Using theoretically computed adaptive learning rates for fast convergence},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining frequent weighted closed itemsets using the WN-list
structure and an early pruning strategy. <em>APIN</em>, <em>51</em>(3),
1439–1459. (<a
href="https://doi.org/10.1007/s10489-020-01899-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of mining frequent weighted itemsets (FWIs) is an extension of the mining frequent itemsets (FIs), which considers not only the frequent occurrence of items but also their relative importance in a dataset. However, like mining FIs, mining FWIs usually produces a large result set, which makes it difficult to extract rules and creates redundancy. The problem of mining frequent weighted closed itemsets (FWCIs) has been proposed as a solution to this issue, which produces a smaller result set while preserving sufficient information to extract rules. The weighted node-list (WN-list) structure is currently considered the state-of-the-art structure for mining FWIs. In this study, we first propose the definition of WN-list ancestral operation and a theorem as the theoretical basis for eliminating unsatisfactory candidates, then propose an efficient algorithm, namely NFWCI, for mining FWCIs using the WN-list and an early pruning strategy. The experimental results on many sparse and dense datasets show that the proposed algorithm outperforms the-state-of-the-art algorithm for mining FWCIs.},
  archive      = {J_APIN},
  author       = {Bui, Huong and Vo, Bay and Nguyen-Hoang, Tu-Anh and Yun, Unil},
  doi          = {10.1007/s10489-020-01899-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1439-1459},
  shortjournal = {Appl. Intell.},
  title        = {Mining frequent weighted closed itemsets using the WN-list structure and an early pruning strategy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HPSO-SA: Hybrid particle swarm optimization-simulated
annealing algorithm for relay node selection in wireless body area
networks. <em>APIN</em>, <em>51</em>(3), 1410–1438. (<a
href="https://doi.org/10.1007/s10489-020-01834-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern world, wireless body area networks (WBANs) play an essential role in psychological and biomedical applications. The use of WBANs in medical applications is limited due to various issues related to the sensors, viz., irregularity in data production, replacement and recharging of their batteries and the energy consumed by the networks. This manuscript addresses how these problems can be solved along with optimization of the energy consumption through efficient design of the system by applying routing protocols and heuristic-based optimization algorithms. In this paper, the particle swarm optimization (PSO) algorithm is a heuristic search algorithm that relies on an upgrade mechanism of the velocity and position of swarms. Although PSO has excellent exploration capability in global search, it becomes quickly stuck in local minima. To enhance the local search function of the current PSO algorithm, a simulated annealing (SA) algorithm has been incorporated in the exploitation phase. The newly developed hybrid PSO-SA (hPSO-SA) algorithm is validated with other state-of-the-art nature-inspired algorithms on eighteen benchmarks and five real engineering design problems. The statistical results of the proposed hPSO-SA algorithm are promising and indicate very good efficiency. The paper also aims at the application of the proposed algorithm to the WBAN design problem for minimization of the energy consumption through better selection of the relay node. The proposed hPSO-SA algorithm outperforms twelve other metaheuristic algorithms, taking hybrid variants for comparison.},
  archive      = {J_APIN},
  author       = {Bilandi, Naveen and Verma, Harsh K. and Dhir, Renu},
  doi          = {10.1007/s10489-020-01834-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1410-1438},
  shortjournal = {Appl. Intell.},
  title        = {HPSO-SA: Hybrid particle swarm optimization-simulated annealing algorithm for relay node selection in wireless body area networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SMOTE-WENN: Solving class imbalance and small sample
problems by oversampling and distance scaling. <em>APIN</em>,
<em>51</em>(3), 1394–1409. (<a
href="https://doi.org/10.1007/s10489-020-01852-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many practical applications suffer from imbalanced data classification, in which case the minority class has degraded recognition rate. The primary causes are the sample scarcity of the minority class and the intrinsic complex distribution characteristics of imbalanced datasets. The imbalanced classification problem is more serious on small sample datasets. To solve the problems of small sample and class imbalance, a hybrid resampling method is proposed. The proposed method combines an oversampling approach (synthetic minority oversampling technique, SMOTE) and a novel data cleaning approach (weighted edited nearest neighbor rule, WENN). First, SMOTE generates synthetic minority class examples using linear interpolation. Then, WENN detects and deletes unsafe majority and minority class examples using weighted distance function and k-nearest neighbor (kNN) rule. The weighted distance function scales up a commonly used distance by considering local imbalance and spacial sparsity. Extensive experiments over synthetic and real datasets validate the superiority of the proposed SMOTE-WENN compared with three state-of-the-art resampling methods.},
  archive      = {J_APIN},
  author       = {Guan, Hongjiao and Zhang, Yingtao and Xian, Min and Cheng, H. D. and Tang, Xianglong},
  doi          = {10.1007/s10489-020-01852-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1394-1409},
  shortjournal = {Appl. Intell.},
  title        = {SMOTE-WENN: Solving class imbalance and small sample problems by oversampling and distance scaling},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete-valued belief structures combination and
normalization using evidential reasoning rule. <em>APIN</em>,
<em>51</em>(3), 1379–1393. (<a
href="https://doi.org/10.1007/s10489-020-01897-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete-valued belief structures (DBSs) (known as discrete belief structures) are universal in real life, differ from precise-valued belief structures, and interval-valued belief structures (IBSs). However, the combination of different discrete belief structures presents a problem that has yet to be solved. Therefore, this study investigated the respective counter-intuitive types of behavior associated with the combination of discrete belief structures within the frameworks of the Dempster-Shafer theory. (DST) evidential reasoning (ER) for the purpose of constructing a more general method for the combination and normalization of discrete evidence. Finally, an experimental application is provided to indicate that the proposed method is suitable for combining and normalizing conflict-free/conflicting discrete evidence, and can effectively solve problems involving group decision-making (GDM) with uncertain preference ordinals, such as in a software selection problem.},
  archive      = {J_APIN},
  author       = {Zhang, Xing-Xian and Wang, Ying-Ming and Chen, Sheng-Qun and Chen, Lei},
  doi          = {10.1007/s10489-020-01897-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1379-1393},
  shortjournal = {Appl. Intell.},
  title        = {Discrete-valued belief structures combination and normalization using evidential reasoning rule},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A jumping mining attack and solution. <em>APIN</em>,
<em>51</em>(3), 1367–1378. (<a
href="https://doi.org/10.1007/s10489-020-01866-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining is the important part of the blockchain used the proof of work (PoW) on its consensus, looking for the matching block through testing a number of hash calculations. In order to attract more hash computing power, the miner who finds the proper block can obtain some rewards. Actually, these hash calculations ensure that the data of the blockchain is not easily tampered. Thus, the incentive mechanism for mining affects the security of the blockchain directly. This paper presents an approach to attack against the difficulty adjustment algorithm (abbreviated as DAA) used in blockchain mining, which has a direct impact on miners’ earnings. In this method, the attack miner jumps between different blockchains to get more benefits than the honest miner who keep mining on only one blockchain. We build a probabilistic model to simulate the time to obtain the next block at different hash computing power called hashrate. Based on this model, we analyze the DAAs of the major cryptocurrencies, including Bitcoin, Bitcoin Cash, Zcash, and Bitcoin Gold. We further verify the effectiveness of this attack called jumping mining through simulation experiments, and also get the characters for the attack in the public block data of Bitcoin Gold. Finally, we give an improved DAA scheme against this attack. Extensive experiments are provided to support the efficiency of our designed scheme.},
  archive      = {J_APIN},
  author       = {Hu, Muchuang and Chen, Jiahui and Gan, Wensheng and Chen, Chien-Ming},
  doi          = {10.1007/s10489-020-01866-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1367-1378},
  shortjournal = {Appl. Intell.},
  title        = {A jumping mining attack and solution},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OptCoNet: An optimized convolutional neural network for an
automatic diagnosis of COVID-19. <em>APIN</em>, <em>51</em>(3),
1351–1366. (<a
href="https://doi.org/10.1007/s10489-020-01904-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quick spread of coronavirus disease (COVID-19) has become a global concern and affected more than 15 million confirmed patients as of July 2020. To combat this spread, clinical imaging, for example, X-ray images, can be utilized for diagnosis. Automatic identification software tools are essential to facilitate the screening of COVID-19 using X-ray images. This paper aims to classify COVID-19, normal, and pneumonia patients from chest X-ray images. As such, an Optimized Convolutional Neural network (OptCoNet) is proposed in this work for the automatic diagnosis of COVID-19. The proposed OptCoNet architecture is composed of optimized feature extraction and classification components. The Grey Wolf Optimizer (GWO) algorithm is used to optimize the hyperparameters for training the CNN layers. The proposed model is tested and compared with different classification strategies utilizing an openly accessible dataset of COVID-19, normal, and pneumonia images. The presented optimized CNN model provides accuracy, sensitivity, specificity, precision, and F1 score values of 97.78%, 97.75%, 96.25%, 92.88%, and 95.25%, respectively, which are better than those of state-of-the-art models. This proposed CNN model can help in the automatic screening of COVID-19 patients and decrease the burden on medicinal services frameworks.},
  archive      = {J_APIN},
  author       = {Goel, Tripti and Murugan, R. and Mirjalili, Seyedali and Chakrabartty, Deba Kumar},
  doi          = {10.1007/s10489-020-01904-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1351-1366},
  shortjournal = {Appl. Intell.},
  title        = {OptCoNet: An optimized convolutional neural network for an automatic diagnosis of COVID-19},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel active multi-source transfer learning algorithm for
time series forecasting. <em>APIN</em>, <em>51</em>(3), 1326–1350. (<a
href="https://doi.org/10.1007/s10489-020-01871-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Time Series Forecasting (TSF), researchers usually assume that there is enough training data can be obtained, with the old a`nd new data satisfying the same distribution. However, time series data always produces some time-varying characteristics over time, which will lead to relatively large differences between old and new data. As we all know, single-source TSF Transfer Learning (TL) faces the problem of negative transfer. Addressing this issue, this paper proposes a new Multi-Source TL algorithm, abbreviated as the MultiSrcTL algorithm, and a novel Active Multi-Source Transfer Learning, abbreviated as the AcMultiSrcTL algorithm, with the latter one integrating Multi-Source TL with Active Learning (AL), and taking the former one as its sub-algorithm. We introduce domain adaptation theory into this work, and analyze the expected target risk of TSF under the multi-source setting, accordingly. For the development of MultiSrcTL, we make full use of source similarity and domain dependability, using the Maximum Mean Discrepancy statistical indicator to measure the similarity between domains, so as to promote better transfer. A domain relation matrix is constructed to describe the relationship between source domains, so that the source-source and source-target relations are adequately considered. In the design of AcMultiSrcTL, Kullback-Leibler divergence is used to measure the similarity of related indicators to select the appropriate source domain. The uncertainty sampling method and the distribution match weighting technique are integrated, obtaining a new sample selection scheme. The empirical results on six benchmark datasets demonstrate the applicability and effectiveness of the two proposed algorithms for multi-source TSF TL.},
  archive      = {J_APIN},
  author       = {Gu, Qitao and Dai, Qun},
  doi          = {10.1007/s10489-020-01871-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1326-1350},
  shortjournal = {Appl. Intell.},
  title        = {A novel active multi-source transfer learning algorithm for time series forecasting},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 open source data sets: A comprehensive survey.
<em>APIN</em>, <em>51</em>(3), 1296–1325. (<a
href="https://doi.org/10.1007/s10489-020-01862-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In December 2019, a novel virus named COVID-19 emerged in the city of Wuhan, China. In early 2020, the COVID-19 virus spread in all continents of the world except Antarctica, causing widespread infections and deaths due to its contagious characteristics and no medically proven treatment. The COVID-19 pandemic has been termed as the most consequential global crisis since the World Wars. The first line of defense against the COVID-19 spread are the non-pharmaceutical measures like social distancing and personal hygiene. The great pandemic affecting billions of lives economically and socially has motivated the scientific community to come up with solutions based on computer-aided digital technologies for diagnosis, prevention, and estimation of COVID-19. Some of these efforts focus on statistical and Artificial Intelligence-based analysis of the available data concerning COVID-19. All of these scientific efforts necessitate that the data brought to service for the analysis should be open source to promote the extension, validation, and collaboration of the work in the fight against the global pandemic. Our survey is motivated by the open source efforts that can be mainly categorized as (a) COVID-19 diagnosis from CT scans, X-ray images, and cough sounds, (b) COVID-19 case reporting, transmission estimation, and prognosis from epidemiological, demographic, and mobility data, (c) COVID-19 emotional and sentiment analysis from social media, and (d) knowledge-based discovery and semantic analysis from the collection of scholarly articles covering COVID-19. We survey and compare research works in these directions that are accompanied by open source data and code. Future research directions for data-driven COVID-19 research are also debated. We hope that the article will provide the scientific community with an initiative to start open source extensible and transparent research in the collective fight against the COVID-19 pandemic.},
  archive      = {J_APIN},
  author       = {Shuja, Junaid and Alanazi, Eisa and Alasmary, Waleed and Alashaikh, Abdulaziz},
  doi          = {10.1007/s10489-020-01862-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1296-1325},
  shortjournal = {Appl. Intell.},
  title        = {COVID-19 open source data sets: A comprehensive survey},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Batch bayesian optimization via adaptive local search.
<em>APIN</em>, <em>51</em>(3), 1280–1295. (<a
href="https://doi.org/10.1007/s10489-020-01790-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesianoptimization (BO) provides an efficient tool for solving the black-box global optimization problems. Under situations where multiple points can be evaluated simultaneously, batch Bayesian optimization has been a popular extension by taking full use of the computational and experimental resources. In this paper, an adaptive local search strategy is investigated to select batch points for Bayesian optimization. First, multi-start strategy and gradient-based optimization method are combined to maximize the acquisition function. Then, an automatic cluster approach (e.g., X-means) is applied to adaptively identify the acquisition function’s local maxima from the gradient-based optimization results. Third, the Bayesian stopping criterion is utilized to guarantee all the local maxima can be obtained theoretically. Moreover, the lower bound confidence criterion and frontend truncation operation are employed to select the most promising local maxima as batch points. Extensive evaluations on various synthetic functions and two hyperparameter tuning problems for deep learning models are utilized to verify the proposed method.},
  archive      = {J_APIN},
  author       = {Liu, Jingfei and Jiang, Chao and Zheng, Jing},
  doi          = {10.1007/s10489-020-01790-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1280-1295},
  shortjournal = {Appl. Intell.},
  title        = {Batch bayesian optimization via adaptive local search},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning-based approach for the automated surface
inspection of copper clad laminate images. <em>APIN</em>,
<em>51</em>(3), 1262–1279. (<a
href="https://doi.org/10.1007/s10489-020-01877-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface quality inspection and control are extremely important for electronic manufacturing. The use of machine vision technology to automatically detect the defects of products has become an indispensable means for better quality control. A machine vision-based surface quality inspection system is usually composed of two processes: image acquisition and automatic defect detection. In this paper, we propose a deep learning-based approach for the defect detection of Copper Clad Laminate (CCL) images acquired from an industrial CCL production line. In the proposed approach, a new convolutional neural network (CNN) that realizes fast defect detection while maintaining high accuracy is designed. Our proposed approach makes four contributions. First, we introduce the depthwise separable convolution to reduce the calculation time. Second, we improve the squeeze-and-excitation block to improve network performance. Third, we introduce the squeeze-and-expand mechanism to reduce the computation cost. Fourth, we employ a smoother activation function (Mish) to allow improved information flow. The proposed network is compared with the benchmark CNNs (including Inception, ResNet and MobileNet). The experimental results show that compared with the benchmark networks, our proposed network has achieved the best results regarding the accuracy and suboptimal results in terms of the speed compared with the benchmark networks. Therefore, our proposed method has been integrated into an industrial CCL production line as a guideline for online defective product rejection.},
  archive      = {J_APIN},
  author       = {Zheng, Xiaoqing and Chen, Jie and Wang, Hongcheng and Zheng, Song and Kong, Yaguang},
  doi          = {10.1007/s10489-020-01877-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1262-1279},
  shortjournal = {Appl. Intell.},
  title        = {A deep learning-based approach for the automated surface inspection of copper clad laminate images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TIRNet: Object detection in thermal infrared images for
autonomous driving. <em>APIN</em>, <em>51</em>(3), 1244–1261. (<a
href="https://doi.org/10.1007/s10489-020-01882-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present study, towards reliable and efficient object detection in thermal infrared (TIR) images, we put forward a novel object detection approach, termed TIRNet, which is built upon convolutional neural network (CNN). Instead of using the deep CNN backbone (ResNet, ResNeXt) which suffers low speed and high computational cost, the lightweight feature extractor (VGG) is adopted. To get the robust and discriminating features for accurate box regression and classification, the Residual Branch is introduced. More uniquely, it only exists in the training phase, so no any additional time is increased when inference. All the computation is encapsulated in a single network, so our TIRNet can be optimized and tested in the manner of end-to-end. Furthermore, the continuous information fusion strategy is proposed for improving detection performance, which can effectively solve the problems such as complex background, occlusion, and get more accurate and smoother detection results. To get the real-world dataset and effectively evaluate the effectiveness, a China Thermal Infrared (CTIR) dataset is collected. Besides, we also evaluate our proposed approach on the public KAIST Multispectral dataset. As demonstrated in the comparative experiments, our approach gets the state-of-the-art detection accuracy while maintains high detection efficiency.},
  archive      = {J_APIN},
  author       = {Dai, Xuerui and Yuan, Xue and Wei, Xueye},
  doi          = {10.1007/s10489-020-01882-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1244-1261},
  shortjournal = {Appl. Intell.},
  title        = {TIRNet: Object detection in thermal infrared images for autonomous driving},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving matrix games based on ambika method with hesitant
fuzzy information and its application in the counter-terrorism issue.
<em>APIN</em>, <em>51</em>(3), 1227–1243. (<a
href="https://doi.org/10.1007/s10489-020-01759-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hesitant fuzzy set has been studied as a powerful tool to describe the decision makers’ judgements under uncertain environment and applied to many domains. For solving the matrix games whose payoffs are expressed by the hesitant fuzzy information, the paper proposes the Ambika method of hesitant fuzzy matrix games (HFMGs). In this paper, firstly, the formal representation of HFMGs is established to meet the conditions of two-person finite zero-sum games. Secondly, after a new method of adding elements to the shorter hesitant fuzzy elements (HFEs), i.e. the hesitant fuzzy elements with possibility, is developed to keep the same length of HFEs, a weighting method based on the position of element in the HFEs is proposed. Then the hesitant fuzzy bi-objective nonlinear programming models for both players are established for HFMGs. Thirdly, according to the proposed value and ambiguity indexes, the Ambika method of HFMGs is developed to find the optimal solutions of mixed strategies by solving the converted linear programming models. Finally, as the illustration of the proposed method, a numerical example about how to choose the optimal solutions for a state security department is given in the counter-terrorism issue.},
  archive      = {J_APIN},
  author       = {Xue, Wenting and Xu, Zeshui and Zeng, Xiao-Jun},
  doi          = {10.1007/s10489-020-01759-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1227-1243},
  shortjournal = {Appl. Intell.},
  title        = {Solving matrix games based on ambika method with hesitant fuzzy information and its application in the counter-terrorism issue},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVIDetectioNet: COVID-19 diagnosis system based on x-ray
images using features selected from pre-learned deep features ensemble.
<em>APIN</em>, <em>51</em>(3), 1213–1226. (<a
href="https://doi.org/10.1007/s10489-020-01888-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent novel coronavirus (also known as COVID-19) has rapidly spread worldwide, causing an infectious respiratory disease that has killed hundreds of thousands and infected millions. While test kits are used for diagnosis of the disease, the process takes time and the test kits are limited in their availability. However, the COVID-19 disease is also diagnosable using radiological images taken through lung X-rays. This process is known to be both faster and more reliable as a form of identification and diagnosis. In this regard, the current study proposes an expert-designed system called COVIDetectioNet model, which utilizes features selected from combination of deep features for diagnosis of COVID-19. For this purpose, a pretrained Convolutional Neural Network (CNN)-based AlexNet architecture that employed the transfer learning approach, was used. The effective features that were selected using the Relief feature selection algorithm from all layers of the architecture were then classified using the Support Vector Machine (SVM) method. To verify the validity of the model proposed, a total of 6092 X-ray images, classified as Normal (healthy), COVID-19, and Pneumonia, were obtained from a combination of public datasets. In the experimental results, an accuracy of 99.18% was achieved using the model proposed. The results demonstrate that the proposed COVIDetectioNet model achieved a superior level of success when compared to previous studies.},
  archive      = {J_APIN},
  author       = {Turkoglu, Muammer},
  doi          = {10.1007/s10489-020-01888-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1213-1226},
  shortjournal = {Appl. Intell.},
  title        = {COVIDetectioNet: COVID-19 diagnosis system based on X-ray images using features selected from pre-learned deep features ensemble},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view clustering via adversarial view embedding and
adaptive view fusion. <em>APIN</em>, <em>51</em>(3), 1201–1212. (<a
href="https://doi.org/10.1007/s10489-020-01864-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which explores complementarity and consistency among multiple distinct feature sets to boost clustering performance, is becoming more and more useful in many real-world applications. Traditional approaches usually map multiple views to a unified embedding, in which some weighted mechanisms are utilized to measure the importance of each view. The embedding, serving as a clustering friendly representation, is then sent to extra clustering algorithms. However, a unified embedding cannot cover both complementarity and consistency among views and the weighted scheme measuring the importance of each view as a whole ignores the differences of features in each view. Moreover, because of lacking in proper grouping structure constraint imposed on the unified embedding, it will lead to just multi-view representation learned, which is not clustering friendly. In this paper, we propose a novel multi-view clustering method to alleviate the above problems. By dividing the embedding of a view into unified and view-specific vectors explicitly, complementarity and consistency can be reflected. Besides, an adversarial learning process is developed to force the above embeddings to be non-trivial. Then a fusion strategy is automatically learned, which will adaptively adjust weights for all the features in each view. Finally, a Kullback-Liebler (KL) divergence based objective is developed to constrain the fused embedding for clustering friendly representation learning and to conduct clustering. Extensive experiments have been conducted on various datasets, performing better than the state-of-the-art clustering approaches.},
  archive      = {J_APIN},
  author       = {Li, Yongzhen and Liao, Husheng},
  doi          = {10.1007/s10489-020-01864-4},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1201-1212},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view clustering via adversarial view embedding and adaptive view fusion},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic multi-objective evolutionary algorithm for IoT
services. <em>APIN</em>, <em>51</em>(3), 1177–1200. (<a
href="https://doi.org/10.1007/s10489-020-01861-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of the Internet of things(IoT) is to provide people with anywhere services in real life. But intelligent IoT shouldn’t only provide services, but also consider how to allocate heterogeneous resources reasonably, which has become a very challenging problem. To obtain the best resource allocation scheme, it is crucial to minimize the service cost and service time. Since the two objectives are contradictory, we have modelled IoT services as a dynamic multi-objective optimization problem. Then a dynamic multi-objective evolutionary algorithm for dynamic IoT services(dMOEA/DI) is proposed. In dMOEA/DI, we have designed operators such as the appropriate encoding method, dynamic detection operator, filtering strategy, differential evolution, and polynomial mutation. Based on the single service strategy and collaborative service strategy, experimental research is performed on the agricultural IoT services with dynamic requests under different distributions. The simulation experimental results prove that dMOEA/DI performs better than the contrasted algorithms on the IoT service optimization problems.},
  archive      = {J_APIN},
  author       = {Fang, Shun-shun and Chai, Zheng-yi and Li, Ya-lun},
  doi          = {10.1007/s10489-020-01861-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {1177-1200},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic multi-objective evolutionary algorithm for IoT services},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmentation mask-guided person image generation.
<em>APIN</em>, <em>51</em>(2), 1161–1176. (<a
href="https://doi.org/10.1007/s10489-020-01907-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background clutters and pose variation are the key factors which prevents the network from learning a robust Person re-identification (Re-ID) model. To address the problem above, we first introduce the binary segmentation mask to construct the body region served as the input of the generator, then design a segmentation mask-guided person image generation network for the pose transfer. The binary segmentation mask has the capability of removing the background clutters in pixel-level, and contains more details about the edge information, where better shape consistency can be achieved for the generated image with the input image. Compared with the previous methods, the proposed method can dramatically improve the model adaptive ability and deal with the diversity of postures. In addition, we design a lightweight attention mechanism module as a guider module, which can assist the generator to focus on the discriminative features of pedestrians. The experiment results are introduced to demonstrate the effectiveness of the proposed method and the superiority performance over most state-of-the-art methods without over-computing in the design process of the Re-ID model. It is worth mentioning that our ideas can be easily combined with other fields to solve the phenomenon of the current situation with insufficient pose variations in the datasets.},
  archive      = {J_APIN},
  author       = {Liu, Meichen and Yan, Xin and Wang, Chenhui and Wang, Kejun},
  doi          = {10.1007/s10489-020-01907-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1161-1176},
  shortjournal = {Appl. Intell.},
  title        = {Segmentation mask-guided person image generation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pruning filters with l1-norm and capped l1-norm for CNN
compression. <em>APIN</em>, <em>51</em>(2), 1152–1160. (<a
href="https://doi.org/10.1007/s10489-020-01894-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The blistering progress of convolutional neural networks (CNNs) in numerous applications of the real-world usually obstruct by a surge in network volume and computational cost. Recently, researchers concentrate on eliminating these issues by compressing the CNN models, such as pruning filters and weights. In comparison with the technique of pruning weights, the technique of pruning filters doesn’t effect in sparse connectivity patterns. In this article, we have proposed a fresh new technique to estimate the significance of filters. More precisely, we combined L1-norm with capped L1-norm to represent the amount of information extracted by the filter and control regularization. In the process of pruning, the insignificant filters remove directly without any loss in the test accuracy, providing much slimmer and compact models with comparable accuracy and this process is iterated a few times. To validate the effectiveness of our algorithm. We experimentally determine the usefulness of our approach with several advanced CNN models on numerous standard data sets. Particularly, data sets CIFAR-10 is used on VGG-16 and prunes 92.7% parameters with float-point-operations (FLOPs) reduction of 75.8% without loss of accuracy and has achieved advancement in state-of-art.},
  archive      = {J_APIN},
  author       = {Kumar, Aakash and Shaikh, Ali Muhammad and Li, Yun and Bilal, Hazrat and Yin, Baoqun},
  doi          = {10.1007/s10489-020-01894-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1152-1160},
  shortjournal = {Appl. Intell.},
  title        = {Pruning filters with l1-norm and capped l1-norm for CNN compression},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I-SiamIDS: An improved siam-IDS for handling class imbalance
in network-based intrusion detection systems. <em>APIN</em>,
<em>51</em>(2), 1133–1151. (<a
href="https://doi.org/10.1007/s10489-020-01886-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-based Intrusion Detection Systems (NIDSs) identify malicious activities by analyzing network traffic. NIDSs are trained with the samples of benign and intrusive network traffic. Training samples belong to either majority or minority classes depending upon the number of available instances. Majority classes consist of abundant samples for the normal traffic as well as for recurrent intrusions. Whereas, minority classes include fewer samples for unknown events or infrequent intrusions. NIDSs trained on such imbalanced data tend to give biased predictions against minority attack classes, causing undetected or misclassified intrusions. Past research works handled this class imbalance problem using data-level approaches that either increase minority class samples or decrease majority class samples in the training data set. Although these data-level balancing approaches indirectly improve the performance of NIDSs, they do not address the underlying issue in NIDSs i.e. they are unable to identify attacks having limited training data only. This paper proposes an algorithm-level approach called Improved Siam-IDS (I-SiamIDS), which is a two-layer ensemble for handling class imbalance problem. I-SiamIDS identifies both majority and minority classes at the algorithm-level without using any data-level balancing techniques. The first layer of I-SiamIDS uses an ensemble of binary eXtreme Gradient Boosting (b-XGBoost), Siamese Neural Network (Siamese-NN) and Deep Neural Network (DNN) for hierarchical filtration of input samples to identify attacks. These attacks are then sent to the second layer of I-SiamIDS for classification into different attack classes using multi-class eXtreme Gradient Boosting classifier (m-XGBoost). As compared to its counterparts, I-SiamIDS showed significant improvement in terms of Accuracy, Recall, Precision, F1-score and values of Area Under the Curve (AUC) for both NSL-KDD and CIDDS-001 datasets. To further strengthen the results, computational cost analysis was also performed to study the acceptability of the proposed I-SiamIDS.},
  archive      = {J_APIN},
  author       = {Bedi, Punam and Gupta, Neha and Jindal, Vinita},
  doi          = {10.1007/s10489-020-01886-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1133-1151},
  shortjournal = {Appl. Intell.},
  title        = {I-SiamIDS: An improved siam-IDS for handling class imbalance in network-based intrusion detection systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HFADE-FMD: A hybrid approach of fireworks algorithm and
differential evolution strategies for functional module detection in
protein-protein interaction networks. <em>APIN</em>, <em>51</em>(2),
1118–1132. (<a
href="https://doi.org/10.1007/s10489-020-01791-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional module detection in protein-protein interaction (PPI) network is one important content of the proteomics research in the post-genomic era. Nowadays the swarm intelligence and evolutionary based approaches have become effective ways for detecting functional modules. This paper proposes a novel hybrid approach of fireworks algorithm and differential evolution strategies for functional module detection in PPI networks (called HFADE-FMD). HFADE-FMD first initializes each firework individual into a candidate functional module partition based on label propagation according to the topological and functional information between protein nodes. Then HFADE-FMD uses the explosion operator of firework algorithm, and mutation, crossover and selection strategies of differential evolution algorithm to iteratively search for better functional module partitions. To verify the performance of HFADE-FMD, this paper compared it with ten competitive methods on four public PPI datasets. The experimental results show that HFADE-FMD achieves prominent performance with respective to Recall, Sn, PPV, and ACC metrics while performing well in terms of Precision and F-measure metrics. Thus, it is able to more accurately detect functional modules and help biologists to find some novel biological insights.},
  archive      = {J_APIN},
  author       = {Ji, Junzhong and Xiao, Hanghang and Yang, Cuicui},
  doi          = {10.1007/s10489-020-01791-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1118-1132},
  shortjournal = {Appl. Intell.},
  title        = {HFADE-FMD: A hybrid approach of fireworks algorithm and differential evolution strategies for functional module detection in protein-protein interaction networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image robust recognition based on feature-entropy-oriented
differential fusion capsule network. <em>APIN</em>, <em>51</em>(2),
1108–1117. (<a
href="https://doi.org/10.1007/s10489-020-01873-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In solving the black box attribute problem of neural networks, how to extract feature information in data and generalize inherent features of data are the focus of artificial intelligence research. Aiming at the problem of the weak generalization ability of large image transformation under deep convolutional networks, a new method for image robust recognition based on a feature-entropy-oriented differential fusion capsule network (DFC) is proposed, the core of which is feature entropy approximation. First, convolution feature entropy is introduced as the transformation metric at the feature extraction level, and a convolution difference scale space is constructed using a residual network to approximate the similar entropy. Then, based on this scale feature, convolution feature extraction in a lower scale space is carried out and fused with the last scale feature to form a convolution differential fusion feature. Finally, a capsule network is used to autonomously cluster using dynamic routing to complete the semantic learning of various high-dimensional features, thereby further enhancing the recognition robustness. Experimental results show that feature entropy can effectively evaluate the transformation image recognition effect, and the DFC is effective for robust recognition with large image transformations such as image translation, rotation, and scale transformation.},
  archive      = {J_APIN},
  author       = {Qian, Kui and Tian, Lei and Liu, Yiting and Wen, Xiulan and Bao, Jiatong},
  doi          = {10.1007/s10489-020-01873-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1108-1117},
  shortjournal = {Appl. Intell.},
  title        = {Image robust recognition based on feature-entropy-oriented differential fusion capsule network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and slow curiosity for high-level exploration in
reinforcement learning. <em>APIN</em>, <em>51</em>(2), 1086–1107. (<a
href="https://doi.org/10.1007/s10489-020-01849-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) algorithms rely on carefully designed environment rewards that are extrinsic to the agent. However, in many real-world scenarios rewards are sparse or delayed, motivating the need for discovering efficient exploration strategies. While intrinsically motivated agents hold promise of better local exploration, solving problems that require coordinated decisions over long-time horizons remains an open problem. We postulate that to discover such strategies, a DRL agent should be able to combine local and high-level exploration behaviors. To this end, we introduce the concept of fast and slow curiosity that aims to incentivize long-time horizon exploration. Our method decomposes the curiosity bonus into a fast reward that deals with local exploration and a slow reward that encourages global exploration. We formulate this bonus as the error in an agent’s ability to reconstruct the observations given their contexts. We further propose to dynamically weight local and high-level strategies by measuring state diversity. We evaluate our method on a variety of benchmark environments, including Minigrid, Super Mario Bros, and Atari games. Experimental results show that our agent outperforms prior approaches in most tasks in terms of exploration efficiency and mean scores.},
  archive      = {J_APIN},
  author       = {Bougie, Nicolas and Ichise, Ryutaro},
  doi          = {10.1007/s10489-020-01849-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1086-1107},
  shortjournal = {Appl. Intell.},
  title        = {Fast and slow curiosity for high-level exploration in reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of coordinated behavior structures with multi-agent
deep reinforcement learning. <em>APIN</em>, <em>51</em>(2), 1069–1085.
(<a href="https://doi.org/10.1007/s10489-020-01832-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperation and coordination are major issues in studies on multi-agent systems because the entire performance of such systems is greatly affected by these activities. The issues are challenging however, because appropriate coordinated behaviors depend on not only environmental characteristics but also other agents’ strategies. On the other hand, advances in multi-agent deep reinforcement learning (MADRL) have recently attracted attention, because MADRL can considerably improve the entire performance of multi-agent systems in certain domains. The characteristics of learned coordination structures and agent’s resulting behaviors, however, have not been clarified sufficiently. Therefore, we focus here on MADRL in which agents have their own deep Q-networks (DQNs), and we analyze their coordinated behaviors and structures for the pickup and floor laying problem, which is an abstraction of our target application. In particular, we analyze the behaviors around scarce resources and long narrow passages in which conflicts such as collisions are likely to occur. We then indicated that different types of inputs to the networks exhibit similar performance but generate various coordination structures with associated behaviors, such as division of labor and a shared social norm, with no direct communication.},
  archive      = {J_APIN},
  author       = {Miyashita, Yuki and Sugawara, Toshiharu},
  doi          = {10.1007/s10489-020-01832-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1069-1085},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of coordinated behavior structures with multi-agent deep reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new base basic probability assignment approach for
conflict data fusion in the evidence theory. <em>APIN</em>,
<em>51</em>(2), 1056–1068. (<a
href="https://doi.org/10.1007/s10489-020-01876-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster-Shafer evidence theory (D-S theory) is applied to process uncertain information in different scenarios. However, traditional Dempster combination rule may produce counterintuitive results while dealing with highly conflicting data. Inspired by a perspective of constructing base belief function for conflicting data processing in D-S theory, a new base basic probability assignment (bBPA) method is proposed to process the potential conflict before data fusion. Instead of assigning initial belief on the whole power set space, the new method assigns the base belief to basic events in the frame of discernment. Consequently, the bBPA is consistent with the classical probability theory. Several numerical examples are adopted to verify the reliability and accuracy of the method in processing highly conflicting data. The data sets in the University of California Irvine (UCI) Machine Learning Repository are used to verity the availability of the new method in classification problem. Experimental result shows that the new method has some superiority in dealing with highly conflicting data.},
  archive      = {J_APIN},
  author       = {Jing, Ming and Tang, Yongchuan},
  doi          = {10.1007/s10489-020-01876-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1056-1068},
  shortjournal = {Appl. Intell.},
  title        = {A new base basic probability assignment approach for conflict data fusion in the evidence theory},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Natural object manipulation using anthropomorphic robotic
hand through deep reinforcement learning and deep grasping probability
network. <em>APIN</em>, <em>51</em>(2), 1041–1055. (<a
href="https://doi.org/10.1007/s10489-020-01870-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human hands can perform complex manipulation of various objects. It is beneficial if anthropomorphic robotic hands can manipulate objects like human hands. However, it is still a challenge due to the high dimensionality and a lack of machine intelligence. In this work, we propose a novel framework based on Deep Reinforcement Learning (DRL) with Deep Grasping Probability Network (DGPN) to grasp and relocate various objects with an anthropomorphic robotic hand much like a human hand. DGPN is used to predict the probability of successful human-like natural grasping based on the priors of human grasping hand poses and object touch areas. Thus, our DRL with DGPN rewards natural grasping hand poses according to object geometry for successful human-like manipulation of objects. The proposed DRL with DGPN is evaluated by grasping and relocating five objects including apple, light bulb, cup, bottle, and can. The performance of our DRL with DGPN is compared with the standard DRL without DGPN. The results show that the standard DRL only achieves an average success rate of 22.60%, whereas our DRL with DGPN achieves 89.40% for the grasping and relocation tasks of the objects.},
  archive      = {J_APIN},
  author       = {Valarezo Añazco, Edwin and Rivera Lopez, Patricio and Park, Nahyeon and Oh, Jiheon and Ryu, Gahyeon and Al-antari, Mugahed A. and Kim, Tae-Seong},
  doi          = {10.1007/s10489-020-01870-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1041-1055},
  shortjournal = {Appl. Intell.},
  title        = {Natural object manipulation using anthropomorphic robotic hand through deep reinforcement learning and deep grasping probability network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An enhanced class topper algorithm based on particle swarm
optimizer for global optimization. <em>APIN</em>, <em>51</em>(2),
1022–1040. (<a
href="https://doi.org/10.1007/s10489-020-01856-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class topper optimization (CTO) algorithm divides the initial swarm into several sub-swarms, and this causes it to possess a strong exploration ability throughout optimization. It however randomly selects best-ranked particles as section toppers (ST’s) and class topper (CT), and the inability of every particle to directly learn from the CT causes slow convergence during the latter stages of iterations. To overcome the algorithm’s deficiency and find a good balance between exploration and exploitation, this study proposes an enhanced CTO (ECTPSO) based on the social learning characteristics of particle swarm optimization (PSO). We created an external archive called the assertive repository (AR) to store best-ranked particles and employed the Karush-Kuhn-Tucker (KKT) proximity measure to assist in the selection of STs and CT. Also, the intensive crowded sorting (ICS) is developed to truncate the AR when it exceeds its maximum size limit. To further encourage exploitation and avert particles from getting trapped in local optimum, we incorporated an adaptive performance adjustment strategy (APA) into our framework to activate particles when they are stagnated. The CEC2017 test suite is employed to evaluate the effectiveness of the proposed algorithm and four other benchmark peer algorithms. The results show that our proposed method possesses a better capability to elude local optima with faster convergence than the other peer algorithms. Furthermore, the algorithms were applied to economic load dispatch (ELD), of which our proposed algorithm demonstrated its effectiveness and competitiveness to address optimization problems.},
  archive      = {J_APIN},
  author       = {Amponsah, Alfred Adutwum and Han, Fei and Ling, Qing-Hua and Kudjo, Patrick Kwaku},
  doi          = {10.1007/s10489-020-01856-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1022-1040},
  shortjournal = {Appl. Intell.},
  title        = {An enhanced class topper algorithm based on particle swarm optimizer for global optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COVID-19 detection and disease progression visualization:
Deep learning on chest x-rays for classification and coarse
localization. <em>APIN</em>, <em>51</em>(2), 1010–1021. (<a
href="https://doi.org/10.1007/s10489-020-01867-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-rays are playing an important role in the testing and diagnosis of COVID-19 disease in the recent pandemic. However, due to the limited amount of labelled medical images, automated classification of these images for positive and negative cases remains the biggest challenge in their reliable use in diagnosis and disease progression. We implemented a transfer learning pipeline for classifying COVID-19 chest X-ray images from two publicly available chest X-ray datasets1,2. The classifier effectively distinguishes inflammation in lungs due to COVID-19 and Pneumonia from the ones with no infection (normal). We have used multiple pre-trained convolutional backbones as the feature extractor and achieved an overall detection accuracy of 90%, 94.3%, and 96.8% for the VGG16, ResNet50, and EfficientNetB0 backbones respectively. Additionally, we trained a generative adversarial framework (a CycleGAN) to generate and augment the minority COVID-19 class in our approach. For visual explanations and interpretation purposes, we implemented a gradient class activation mapping technique to highlight the regions of the input image that are important for predictions. Additionally, these visualizations can be used to monitor the affected lung regions during disease progression and severity stages.},
  archive      = {J_APIN},
  author       = {Zebin, Tahmina and Rezvy, Shahadate},
  doi          = {10.1007/s10489-020-01867-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {1010-1021},
  shortjournal = {Appl. Intell.},
  title        = {COVID-19 detection and disease progression visualization: Deep learning on chest X-rays for classification and coarse localization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel discretization algorithm based on multi-scale and
information entropy. <em>APIN</em>, <em>51</em>(2), 991–1009. (<a
href="https://doi.org/10.1007/s10489-020-01850-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discretization is one of the data preprocessing topics in the field of data mining, and is a critical issue to improve the efficiency and quality of data mining. Multi-scale can reveal the structure and hierarchical characteristics of data objects, the representation of the data in different granularities will be obtained if we make a reasonable hierarchical division for a research object. The multi-scale theory is introduced into the process of data discretization and a data discretization method based on multi-scale and information entropy called MSE is proposed. MSE first conducts scale partition on the domain attribute to obtain candidate cut point set with different granularity. Then, the information entropy is applied to the candidate cut point set, and the candidate cut point with the minimum information entropy is selected and detected in turn to determine the final cut point set using the MDLPC criterion. In such way, MSE avoids the problem that the candidate cut points are limited to only certain limited attribute values caused by considering only the statistical attribute values in the traditional discretization methods, and reduces the number of candidates by controlling the data division hierarchy to an optimal range. Finally, the extensive experiments show that MSE achieves high performance in terms of discretization efficiency and classification accuracy, especially when it is applied to support vector machines, random forest, and decision trees.},
  archive      = {J_APIN},
  author       = {Xun, Yaling and Yin, Qingxia and Zhang, Jifu and Yang, Haifeng and Cui, Xiaohui},
  doi          = {10.1007/s10489-020-01850-w},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {991-1009},
  shortjournal = {Appl. Intell.},
  title        = {A novel discretization algorithm based on multi-scale and information entropy},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid nonlinear convolution filters for image recognition.
<em>APIN</em>, <em>51</em>(2), 980–990. (<a
href="https://doi.org/10.1007/s10489-020-01845-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical convolutional filter only extract features linearly. Although nonlinearities are introduced into the feature extraction layer by using activation functions and pooling operations, they can only provide point-wise nonlinearity. In this paper, a Gaussian convolution for extracting nonlinear features is proposed, and a hybrid nonlinear convolution filter consisting of baseline convolution, Gaussian convolution and other nonlinear convolutions is designed. It can efficiently achieve the fusion of linear features and nonlinear features while preserving the advantages of traditional linear convolution filter in feature extraction. Extensive experiments on the benchmark datasets MNIST, CIFAR10, and CIFAR100 show that the hybrid nonlinear convolutional neural network has faster convergence and higher image recognition accuracy than the traditional baseline convolutional neural network.},
  archive      = {J_APIN},
  author       = {Zhang, Xiuling and Wei, Kailun and Kang, Xuenan and Li, Jinxiang},
  doi          = {10.1007/s10489-020-01845-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {980-990},
  shortjournal = {Appl. Intell.},
  title        = {Hybrid nonlinear convolution filters for image recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label text classification with latent word-wise label
information. <em>APIN</em>, <em>51</em>(2), 966–979. (<a
href="https://doi.org/10.1007/s10489-020-01838-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification (MLTC) is a significant task that aims to assign multiple labels to each given text. There are usually correlations between the labels in the dataset. However, traditional machine learning methods tend to ignore the label correlations. To capture the dependencies between the labels, the sequence-to-sequence (Seq2Seq) model is applied to MLTC tasks. Moreover, to reduce the incorrect penalty caused by the Seq2Seq model due to the inconsistent order of the generated labels, a deep reinforced sequence-to-set (Seq2Set) model is proposed. However, the label generation of the Seq2Set model still relies on a sequence decoder, which cannot eliminate the influence of the predefined label order and exposure bias. Therefore, we propose an MLTC model with latent word-wise label information (MLC-LWL), which constructs effective word-wise labeled information using a labeled topic model and incorporates the label information carried by the word and label context information through a gated network. With the word-wise label information, our model captures the correlations between the labels via a label-to-label structure without being affected by the predefined label order or exposure bias. Extensive experimental results illustrate the effectiveness and significant advantages of our model compared with the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Chen, Ziheng and Ren, Jiangtao},
  doi          = {10.1007/s10489-020-01838-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {966-979},
  shortjournal = {Appl. Intell.},
  title        = {Multi-label text classification with latent word-wise label information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forecasting of e-commerce transaction volume using a hybrid
of extreme learning machine and improved moth-flame optimization
algorithm. <em>APIN</em>, <em>51</em>(2), 952–965. (<a
href="https://doi.org/10.1007/s10489-020-01840-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of e-commerce has resulted in optimization of the industrial structure of Chinese enterprises and has improved the Chinese economy. E-commerce transaction volume is an evaluation index used to determine the development level of e-commerce. This study proposed a model for forecasting e-commerce transaction volume. First, a hybrid moth–flame optimization algorithm (HMFO) was proposed. The convergence ability of the HMFO algorithm was analyzed on the basis of test functions. Second, using data provided by the China Internet Network Information Center, factors influencing e-commerce transaction volume were analyzed. The input variables of the e-commerce transaction volume prediction model were selected by analyzing correlation coefficients. Finally, a hybrid extreme learning machine and hybrid-strategy-based HMFO (ELM-HMFO) method was proposed to predict the volume of e-commerce transactions. The prediction results revealed that the root mean square error of the proposed ELM-HMFO model was smaller than 0.5, and the determination coefficient was 0.99, which indicated that the forecast e-commerce transaction volume was satisfactory. The proposed ELM-HMFO model can promote the industrial upgrading and development of e-commerce in China.},
  archive      = {J_APIN},
  author       = {Zhang, Bo and Tan, Runhua and Lin, Cheng-Jian},
  doi          = {10.1007/s10489-020-01840-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {952-965},
  shortjournal = {Appl. Intell.},
  title        = {Forecasting of e-commerce transaction volume using a hybrid of extreme learning machine and improved moth-flame optimization algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention augmented multi-scale network for single image
super-resolution. <em>APIN</em>, <em>51</em>(2), 935–951. (<a
href="https://doi.org/10.1007/s10489-020-01869-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale convolution can be used in a deep neural network (DNN) to obtain a set of features in parallel with different perceptive fields, which is beneficial to reduce network depth and lower training difficulty. Also, the attention mechanism has great advantages to strengthen representation power of a DNN. In this paper, we propose an attention augmented multi-scale network (AAMN) for single image super-resolution (SISR), in which deep features from different scales are discriminatively aggregated to improve performance. Specifically, the statistics of features at different scales are first computed by global average pooling operation, and then used as a guidance to learn the optimal weight allocation for the subsequent feature recalibration and aggregation. Meanwhile, we adopt feature fusion at two levels to further boost reconstruction power, one of which is intra-group local hierarchical feature fusion (LHFF), and the other is inter-group global hierarchical feature fusion (GHFF). Extensive experiments on public standard datasets indicate the superiority of our AAMN over the state-of-the-art models, in terms of not only quantitative and qualitative evaluation but also model complexity and efficiency.},
  archive      = {J_APIN},
  author       = {Xiong, Chengyi and Shi, Xiaodi and Gao, Zhirong and Wang, Ge},
  doi          = {10.1007/s10489-020-01869-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {935-951},
  shortjournal = {Appl. Intell.},
  title        = {Attention augmented multi-scale network for single image super-resolution},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metric transfer learning via geometric knowledge embedding.
<em>APIN</em>, <em>51</em>(2), 921–934. (<a
href="https://doi.org/10.1007/s10489-020-01853-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usefulness of metric learning in image classification has been proven and has attracted increasing attention in recent research. In conventional metric learning, it is assumed that the source and target instances are distributed identically, however, real-world problems may not have such an assumption. Therefore, for better classifying, we need abundant labeled images, which are inaccessible due to the high cost of labeling. In this way, the knowledge transfer could be utilized. In this paper, we present a metric transfer learning approach entitled as “Metric Transfer Learning via Geometric Knowledge Embedding (MTL-GKE)” to actuate metric learning in transfer learning. Specifically, we learn two projection matrices for each domain to project the source and target domains to a new feature space. In the new shared sub-space, Mahalanobis distance metric is learned to maximize inter-class and minimize intra-class distances in target domain, while a novel instance reweighting scheme based on the graph optimization is applied, simultaneously, to employ the weights of source samples for distribution matching. The results of different experiments on several datasets on object and handwriting recognition tasks indicate the effectiveness of the proposed MTL-GKE compared to other state-of-the-arts methods.},
  archive      = {J_APIN},
  author       = {Ahmadvand, Mahya and Tahmoresnezhad, Jafar},
  doi          = {10.1007/s10489-020-01853-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {921-934},
  shortjournal = {Appl. Intell.},
  title        = {Metric transfer learning via geometric knowledge embedding},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-aware sequence model for next-item recommendation.
<em>APIN</em>, <em>51</em>(2), 906–920. (<a
href="https://doi.org/10.1007/s10489-020-01820-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequences of users’ behaviors generally indicate their preferences, and they can be used to improve next-item prediction in sequential recommendation. Unfortunately, users’ behaviors may change over time, making it difficult to capture users’ dynamic preferences directly from recent sequences of behaviors. Traditional methods such as Markov Chains (MC), Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks only consider the relative order of items in a sequence and ignore important time information such as the time interval and duration in the sequence. In this paper, we propose a novel sequential recommendation model, named Interval- and Duration-aware LSTM with Embedding layer and Coupled input and forget gate (IDLSTM-EC), which leverages time interval and duration information to accurately capture users’ long-term and short-term preferences. In particular, the model incorporates global context information about sequences in the input layer to make better use of long-term memory. Furthermore, the model introduces the coupled input and forget gate and embedding layer to further improve efficiency and effectiveness. Experiments on real-world datasets show that the proposed approaches outperform the state-of-the-art baselines and can handle the problem of data sparsity effectively.},
  archive      = {J_APIN},
  author       = {Wang, Dongjing and Xu, Dengwei and Yu, Dongjin and Xu, Guandong},
  doi          = {10.1007/s10489-020-01820-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {906-920},
  shortjournal = {Appl. Intell.},
  title        = {Time-aware sequence model for next-item recommendation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-layer and multi-ensemble stock trader using deep
learning and deep reinforcement learning. <em>APIN</em>, <em>51</em>(2),
889–905. (<a href="https://doi.org/10.1007/s10489-020-01839-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of computer-aided stock trading methods is gaining popularity in recent years, mainly because of their ability to process efficiently past information through machine learning to predict future market behavior. Several approaches have been proposed to this task, with the most effective ones using fusion of a pile of classifiers decisions to predict future stock values. However, using prices information in single supervised classifiers has proven to lead to poor results, mainly because market history is not enough to be an indicative of future market behavior. In this paper, we propose to tackle this issue by proposing a multi-layer and multi-ensemble stock trader. Our method starts by pre-processing data with hundreds of deep neural networks. Then, a reward-based classifier acts as a meta-learner to maximize profit and generate stock signals through different iterations. Finally, several metalearner trading decisions are fused in order to get a more robust trading strategy, using several trading agents to take a final decision. We validate the effectiveness of the approach in a real-world trading scenario, by extensively testing it on the Standard &amp; Poor’s 500 future market and the J.P. Morgan and Microsoft stocks. Experimental results show that the proposed method clearly outperforms all the considered baselines (which still performs very well in the analysed period), and even the conventional Buy-and-Hold strategy, which replicates the market behaviour.},
  archive      = {J_APIN},
  author       = {Carta, Salvatore and Corriga, Andrea and Ferreira, Anselmo and Podda, Alessandro Sebastian and Recupero, Diego Reforgiato},
  doi          = {10.1007/s10489-020-01839-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {889-905},
  shortjournal = {Appl. Intell.},
  title        = {A multi-layer and multi-ensemble stock trader using deep learning and deep reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-guided multi-granularity selector for attribute
reduction. <em>APIN</em>, <em>51</em>(2), 876–888. (<a
href="https://doi.org/10.1007/s10489-020-01846-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presently, the greedy searching strategy has been widely accepted for obtaining reduct in the field of rough set. In the framework of greedy searching, the evaluation of the candidate attribute is crucial, because the evaluation can determine the final result of reduct to a large extent. However, most of the previous evaluations are designed by considering one and only one fixed granularity, which fails to make the multi-view based evaluation possible. To fill such gap, a Parameterized Multi-granularity Attribute Selector is proposed for obtaining reduct in this paper. Our attribute selector consists of two parts: one is the multi-granularity attribute selector which evaluates and selects attributes through using the information provided by multiple different granularities; the other is the data-guided parameterized granularity selector which generates multiple different parameterized granularities through taking the characteristics of data into account. The experimental results over 15 UCI data sets show the following: 1) compared with the state of the art approaches for obtaining reducts, our proposed attribute selector can contribute to reduct with higher stability; 2) our proposed attribute selector will not provide the reduct with poorer classification performance. This research suggests a new trend for the multi-granularity mechanism in the problem of attribute reduction.},
  archive      = {J_APIN},
  author       = {Jiang, Zehua and Dou, Huili and Song, Jingjing and Wang, Pingxin and Yang, Xibei and Qian, Yuhua},
  doi          = {10.1007/s10489-020-01846-6},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {876-888},
  shortjournal = {Appl. Intell.},
  title        = {Data-guided multi-granularity selector for attribute reduction},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A context-aware recommendation approach based on feature
selection. <em>APIN</em>, <em>51</em>(2), 865–875. (<a
href="https://doi.org/10.1007/s10489-020-01835-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information can be used in recommender systems to make recommendation more efficient. Recent research has made progress in combining contextual information into representation models for recommendations. However, the existing approaches do not well address the problem of data sparsity, and they suffer from context redundancy. To deal with these problems, this paper proposes a context-aware recommendation approach based on embedded feature selection. It gets rid of context redundancy by generating a minimum subset of all contextual information and allocates the weight to each context appropriately. Experiments on the restaurant recommendation shows that the proposed approach has better performance.},
  archive      = {J_APIN},
  author       = {Chen, Lei and Xia, Meimei},
  doi          = {10.1007/s10489-020-01835-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {865-875},
  shortjournal = {Appl. Intell.},
  title        = {A context-aware recommendation approach based on feature selection},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of COVID-19 in chest x-ray images using
DeTraC deep convolutional neural network. <em>APIN</em>, <em>51</em>(2),
854–864. (<a href="https://doi.org/10.1007/s10489-020-01829-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural networks (CNN s) for image recognition and classification. However, due to the limited availability of annotated medical images, the classification of medical images remains the biggest challenge in medical diagnosis. Thanks to transfer learning, an effective mechanism that can provide a promising solution by transferring knowledge from generic object recognition tasks to domain-specific tasks. In this paper, we validate and a deep CNN, called Decompose, Transfer, and Compose (DeTraC), for the classification of COVID-19 chest X-ray images. DeTraC can deal with any irregularities in the image dataset by investigating its class boundaries using a class decomposition mechanism. The experimental results showed the capability of DeTraC in the detection of COVID-19 cases from a comprehensive image dataset collected from several hospitals around the world. High accuracy of 93.1% (with a sensitivity of 100%) was achieved by DeTraC in the detection of COVID-19 X-ray images from normal, and severe acute respiratory syndrome cases.},
  archive      = {J_APIN},
  author       = {Abbas, Asmaa and Abdelsamea, Mohammed M. and Gaber, Mohamed Medhat},
  doi          = {10.1007/s10489-020-01829-7},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {854-864},
  shortjournal = {Appl. Intell.},
  title        = {Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep network compression with teacher latent subspace
learning and LASSO. <em>APIN</em>, <em>51</em>(2), 834–853. (<a
href="https://doi.org/10.1007/s10489-020-01858-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been shown to excel in understanding multimedia by using latent representations to learn complex and useful abstractions. However, they remain unpractical for embedded devices due to memory constraints, high latency, and considerable power consumption at runtime. In this paper, we propose the compression of deep models based on learning lower dimensional subspaces from their latent representations while maintaining a minimal loss of performance. We leverage on the premise that deep convolutional neural networks extract many redundant features to learn new subspaces for feature representation. We construct a compressed model by reconstruction from representations captured by an already trained large model. As compared to state-of-the-art, the proposed approach does not rely on labeled data. Moreover, it allows the use of sparsity inducing LASSO parameter penalty to achieve better compression results than when used to train models from scratch. We perform extensive experiments using VGG-16 and wide ResNet models on CIFAR-10, CIFAR-100, MNIST and SVHN datasets. For instance, VGG-16 with 8.96M parameters trained on CIFAR-10 was pruned by 81.03 % with only 0.26 % generalization performance loss. Correspondingly, the size of the VGG-16 model is reduced from 35MB to 6.72MB to facilitate compact storage. Furthermore, the associated inference time for the same VGG-16 model is reduced from 1.1 secs to 0.6 secs so that inference is accelerated. Particularly, the proposed student models outperform state-of-the-art approaches and the same models trained from scratch.},
  archive      = {J_APIN},
  author       = {Oyedotun, Oyebade K. and Shabayek, Abd El Rahman and Aouada, Djamila and Ottersten, Björn},
  doi          = {10.1007/s10489-020-01858-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {834-853},
  shortjournal = {Appl. Intell.},
  title        = {Deep network compression with teacher latent subspace learning and LASSO},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust semi-supervised support vector machines with laplace
kernel-induced correntropy loss functions. <em>APIN</em>,
<em>51</em>(2), 819–833. (<a
href="https://doi.org/10.1007/s10489-020-01865-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The insufficiency and contamination of supervision information are the main factors affecting the performance of support vector machines (SVMs) in real-world applications. To address this issue, novel correntropy loss functions and Laplacian SVM (LapSVM) are utilized for robust semi-supervised classification. It is known that correntropy loss functions have been used in robust learning and achieved promising results. However, the potential for more diverse priors has not been extensively explored. In this paper, a correntropy loss function induced from Laplace kernel function, called LK-loss, is applied to LapSVM for the construction of robust semi-supervised classifier. Properties of LK-loss are demonstrated including robustness, symmetry, boundedness, Fisher consistency and asymptotic approximation behaviors. Moreover, the asymmetric version of LK-loss is introduced to further improve the performance. Concave-convex procedure (CCCP) technique is used to handle the non-convexity of Laplace kernel-induced correntropy loss functions iteratively. Experimental results show that in most cases, the proposed methods have better generalization performance than the comparing ones, which demonstrate the feasibility and effectiveness of the proposed semi-supervised classification framework.},
  archive      = {J_APIN},
  author       = {Dong, Hongwei and Yang, Liming and Wang, Xue},
  doi          = {10.1007/s10489-020-01865-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {819-833},
  shortjournal = {Appl. Intell.},
  title        = {Robust semi-supervised support vector machines with laplace kernel-induced correntropy loss functions},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of fault diagnosis algorithm for electric fan based
on LSSVM and kd-tree. <em>APIN</em>, <em>51</em>(2), 804–818. (<a
href="https://doi.org/10.1007/s10489-020-01830-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the complexity of mechanical equipment is increasing rapidly together with the poor working environment. If a fault occurs, how to find the fault in time becomes a poser. Motivated by this existing problem, based on the analysis of the fault characteristics of electric fans, a fault diagnosis algorithm model based on Least Square Support Vector Machine (LSSVM) and Kd-Tree was proposed. This algorithm was based on the LSSVM optimized by the Cuckoo Search (CS). This paper used the “one-to-many” principle and the sigma threshold method to introduce k-Nearest Neighbor (kNN) which was implemented by Kd-Tree as a secondary classifier to optimize the model. In data preprocessing, the data based on time series was first processed by Empirical Mode Decomposition (EMD) and the energy ratios were calculated, and the the above results were degraded by Principal Component Analysis (PCA) and normalized. On top of that, in case of the uncertain fault types, the Fuzzy C-Means clustering algorithm (FCM) optimized by Particle Swarm Optimization (PSO) was proposed to provide a priori knowledge for the model. In this paper, the algorithm model, FCM and other parts were verified to prove that the performance and generality of the algorithm were better than those of general classification algorithms, and relevant experiments were conducted for different data processing methods to expand the universality of the algorithm.},
  archive      = {J_APIN},
  author       = {Hu, Kongzhi and Jiang, Ming and Zhang, Haifeng and Cao, Sheng and Guo, Ziyi},
  doi          = {10.1007/s10489-020-01830-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {804-818},
  shortjournal = {Appl. Intell.},
  title        = {Design of fault diagnosis algorithm for electric fan based on LSSVM and kd-tree},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative feature extraction for video person
re-identification via multi-task network. <em>APIN</em>, <em>51</em>(2),
788–803. (<a href="https://doi.org/10.1007/s10489-020-01844-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of video-based person re-identification is to match different pedestrians in various image sequences across non-overlapping cameras. A critical issue of this task is how to exploit the useful information provided by videos. To solve this problem, we propose a novel feature learning framework for video-based person re-identification. The proposed method aims at capturing the most significant information in the spatial and temporal domains and then building a discriminative and robust feature representation for each sequence. More specifically, to learn more effective frame-wise features, we apply several attributes to the video-based task and build a multi-task network for the identity and attribute classifications. In the training phase, we present a multi-loss function to minimize intra-class variances and maximize inter-class differences. After that, the feature aggregation network is employed to aggregate frame-wise features and extract the temporal information from the video. Furthermore, considering that adjacent frames typically have similar appearance features, we propose the concept of “non-redundant appearance feature extraction” to obtain the sequence-level appearance descriptors of pedestrians. Based on the complementarity between the temporal feature and the non-redundant appearance feature, we combine them in the distance learning phase by assigning them different distance-weighted coefficients. Extensive experiments are conducted on three video-based datasets and the results demonstrate the superiority and effectiveness of our method.},
  archive      = {J_APIN},
  author       = {Song, Wanru and Zheng, Jieying and Wu, Yahong and Chen, Changhong and Liu, Feng},
  doi          = {10.1007/s10489-020-01844-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {788-803},
  shortjournal = {Appl. Intell.},
  title        = {Discriminative feature extraction for video person re-identification via multi-task network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State-transition simulated annealing algorithm for
constrained and unconstrained multi-objective optimization problems.
<em>APIN</em>, <em>51</em>(2), 775–787. (<a
href="https://doi.org/10.1007/s10489-020-01836-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel multi-objective optimization algorithm based on a state-transition simulated annealing algorithm (MOSTASA) is proposed, in which four state-transition operators for generating candidate solutions and the Pareto optimal solution is obtained by combining it with the concept of Pareto dominance and then storing it in a Pareto archive. To ensure the uniform distribution of the Pareto optimal solution, we define a crowded comparison operator to update the Pareto archive. Simulation experiments were conducted on several standard constrained and unconstrained multi-objective problems, in which convergence and spacing metrics were used to assess the performance of the MOSTASA. The test results manifest that the MOSTASA can converge to the true Pareto-optimal front, and the solution distribution is uniform. Compared to the performance of other multi-objective optimization algorithms, the proposed algorithm is more efficient and reliable.},
  archive      = {J_APIN},
  author       = {Han, Xiaoxia and Dong, Yingchao and Yue, Lin and Xu, Quanxi and Xie, Gang and Xu, Xinying},
  doi          = {10.1007/s10489-020-01836-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {775-787},
  shortjournal = {Appl. Intell.},
  title        = {State-transition simulated annealing algorithm for constrained and unconstrained multi-objective optimization problems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pearson correlation coefficient-based pheromone refactoring
mechanism for multi-colony ant colony optimization. <em>APIN</em>,
<em>51</em>(2), 752–774. (<a
href="https://doi.org/10.1007/s10489-020-01841-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of falling into local optimum and poor convergence speed in large Traveling Salesman Problem (TSP), this paper proposes a Pearson correlation coefficient-based Pheromone refactoring mechanism for multi-colony Ant Colony Optimization (PPACO). First, the dynamic guidance mechanism is introduced to dynamically adjust the pheromone concentration on the path of the maximum and minimum spanning tree, which can effectively balance the diversity and convergence of the algorithm. Secondly, the frequency of communication between colonies is adjusted adaptively according to a criterion based on the similarity between the minimum spanning tree and the optimal solution. Besides, the pheromone matrix of the colony is reconstructed according to the Pearson correlation coefficient or information entropy to help the algorithm jump out of the local optimum, thus improving the accuracy of the solution. These strategies greatly improve the adaptability of the algorithm and ensure the effectiveness of the interaction. Finally, the experimental results indicate that the proposed algorithm could improve the solution accuracy and accelerate the convergence speed, especially for large-scale TSP instances.},
  archive      = {J_APIN},
  author       = {Pan, Han and You, Xiaoming and Liu, Sheng and Zhang, Dehui},
  doi          = {10.1007/s10489-020-01841-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {752-774},
  shortjournal = {Appl. Intell.},
  title        = {Pearson correlation coefficient-based pheromone refactoring mechanism for multi-colony ant colony optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end CNN + LSTM deep learning approach for bearing
fault diagnosis. <em>APIN</em>, <em>51</em>(2), 736–751. (<a
href="https://doi.org/10.1007/s10489-020-01859-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnostics and prognostics are important topics both in practice and research. There is an intense pressure on industrial plants to continue reducing unscheduled downtime, performance degradation, and safety hazards, which requires detecting and recovering potential faults in its early stages. Intelligent fault diagnosis is a promising tool due to its ability to rapidly and efficiently processing collected signals and providing accurate diagnosis results. Although many studies have developed machine leaning (M.L) and deep learning (D.L) algorithms for detecting the bearing fault, the results have generally been limited to relatively small train and test datasets and the input data has been manipulated (selective features used) to reach high accuracy. In this work, the raw data, collected from accelerometers (time-domain features) are taken as the input of a novel temporal sequence prediction algorithm to present an end-to-end method for fault detection. We use equivalent temporal sequences as the input of a novel Convolutional Long-Short-Term-Memory Recurrent Neural Network (CRNN) to detect the bearing fault with the highest accuracy in the shortest possible time. The method can reach the highest accuracy in the literature, to the best knowledge of the authors of the present paper, voiding any sort of pre-processing or manipulation of the input data. Effectiveness and feasibility of the fault diagnosis method are validated by applying it to two commonly used benchmark real vibration datasets and comparing the result with the other intelligent fault diagnosis methods.},
  archive      = {J_APIN},
  author       = {Khorram, Amin and Khalooei, Mohammad and Rezghi, Mansoor},
  doi          = {10.1007/s10489-020-01859-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {736-751},
  shortjournal = {Appl. Intell.},
  title        = {End-to-end CNN + LSTM deep learning approach for bearing fault diagnosis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated method for multi-criteria decision-making
based on the best-worst method and dempster-shafer evidence theory under
double hierarchy hesitant fuzzy linguistic environment. <em>APIN</em>,
<em>51</em>(2), 713–735. (<a
href="https://doi.org/10.1007/s10489-020-01777-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double hierarchy hesitant fuzzy linguistic term set (DHHFLTS) is a newly developed complex linguistic expression model, and has been well applied to multi-criteria decision-making (MCDM) problems. However, the determination of criteria weights and the innovation of decision-making methods are still two issues that worth exploring in this field. At present, conventional weight-determination methods sometimes have the disadvantages of complicated calculation and low consistency of the obtained results. On the other hand, the existing methods for linguistic information sometimes cannot consider the uncertainty of information caused by ignorance. Considering that the best-worst method (BWM) is a weight-determination method, which can not only greatly simplify the calculation process, but also improve the consistency degree of the results. Dempster-Shafer evidence theory (DSET) can better deal with information uncertainty caused by ignorance. Therefore, this paper extends the BWM and DSET to double hierarchy hesitant fuzzy linguistic environment to solve the above two problems respectively, and the DHHFL-BWM-DSET method is proposed. First, the weight of each criterion is derived based on the BWM-based weight-determination method. Then, inspired by DSET, we propose a DSET-based MCDM method which can not only obtain the decision results of a single decision maker, but also integrate the decision information of multiple decision makers to obtain more rational results. Therefore, decision makers can choose the method based on the specific situation. Finally, taking the selection of financial products as an example, it shows that the method proposed in this paper has some breakthroughs and advantages.},
  archive      = {J_APIN},
  author       = {Zhang, Ruichen and Xu, Zeshui and Gou, Xunjie},
  doi          = {10.1007/s10489-020-01777-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {713-735},
  shortjournal = {Appl. Intell.},
  title        = {An integrated method for multi-criteria decision-making based on the best-worst method and dempster-shafer evidence theory under double hierarchy hesitant fuzzy linguistic environment},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A combined multiple action recognition and summarization for
surveillance video sequences. <em>APIN</em>, <em>51</em>(2), 690–712.
(<a href="https://doi.org/10.1007/s10489-020-01823-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition and video summarization represent challenging tasks for several computer vision applications including video surveillance, criminal investigations, and sports applications. For long videos, it is difficult to search within a video for a specific action and/or person. Usually, human action recognition approaches presented in the literature deal with videos that contain only a single person, and they are able to recognize his action. This paper proposes an effective approach to multiple human action detection, recognition, and summarization. The multiple action detection extracts human bodies’ silhouette, then generates a specific sequence for each one of them using motion detection and tracking method. Each of the extracted sequences is then divided into shots that represent homogeneous actions in the sequence using the similarity between each pair frames. Using the histogram of the oriented gradient (HOG) of the Temporal Difference Map (TDMap) of the frames of each shot, we recognize the action by performing a comparison between the generated HOG and the existed HOGs in the training phase which represents all the HOGs of many actions using a set of videos for training. Also, using the TDMap images we recognize the action using a proposed CNN model. Action summarization is performed for each detected person. The efficiency of the proposed approach is shown through the obtained results for mainly multi-action detection and recognition.},
  archive      = {J_APIN},
  author       = {Elharrouss, Omar and Almaadeed, Noor and Al-Maadeed, Somaya and Bouridane, Ahmed and Beghdadi, Azeddine},
  doi          = {10.1007/s10489-020-01823-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {690-712},
  shortjournal = {Appl. Intell.},
  title        = {A combined multiple action recognition and summarization for surveillance video sequences},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterated graph cut method for automatic and accurate
segmentation of finger-vein images. <em>APIN</em>, <em>51</em>(2),
673–689. (<a href="https://doi.org/10.1007/s10489-020-01828-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in computer vision and machine intelligence have facilitated biometric technologies, which increasingly rely on image data in security practices. As an important biometric identifier, the near-infrared (NIR) finger-vein pattern is favoured by non-contact, high accuracy, and enhanced security systems. However, large stacks of low-contrast and complex finger-vein images present barriers to manual image segmentation, which locates the objects of interest. Although some headway in computer-aided segmentation has been made, state-of-the-art approaches often require user interaction or prior training, which are tedious, time-consuming and prone to operator bias. Recognizing this deficiency, the present study exploits structure-specific contextual clues and proposes an iterated graph cut (IGC) method for automatic and accurate segmentation of finger-vein images. To this end, the geometric structures of the image-acquisition system and the fingers provide the hard (centreline along the finger) and shape (rectangle around the finger) constraints. A node-merging scheme is applied to reduce the computational burden. The Gaussian probability model determines the initial labels. Finally, the maximum a posteriori Markov random field (MAP-MRF) framework is tasked with iteratively updating the data models of the object and the background. Our approach was extensively evaluated on 4 finger-vein databases and compared with some benchmark methods. The experimental results indicate that the proposed IGC method outperforms the state-of-the-practice approaches in finger-vein image segmentation. Specifically, the IGC method, relative to its level set deep learning (LSDL) counterpart, can increase the average F-measure value by 5.03%, 6.56%, 49.91%, and 22.89% when segmenting images from four different finger-vein databases. Therefore, this work can provide a feasible path towards fully automatic image segmentation.},
  archive      = {J_APIN},
  author       = {Lei, Lei and Xi, Feng and Chen, Shengyao and Liu, Zhong},
  doi          = {10.1007/s10489-020-01828-8},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {673-689},
  shortjournal = {Appl. Intell.},
  title        = {Iterated graph cut method for automatic and accurate segmentation of finger-vein images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preference relation based collaborative filtering with graph
aggregation for group recommender system. <em>APIN</em>, <em>51</em>(2),
658–672. (<a href="https://doi.org/10.1007/s10489-020-01848-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the group recommender systems (GRS) apply some aggregation strategy to the ratings given by the group members for generating recommendations. But this can be highly influenced by a few members of the group, which can lead to poor group recommendation. Further, rating based aggregation strategies do not provide efficient ranking of items. Keeping these things in mind, this paper proposes a preference relation (PR) based GRS, that uses matrix factorization (MF) for predicting unknown PRs for group members. The aggregation of preferences is done using a novel virtual user based weight aggregation strategy. The weight aggregation concept is derived from the graph aggregation process. The advantage of this process is that it does not ignore weak preferences and also contributes towards group recommendation. The proposed model is evaluated and compared using standard ranking measures for MovieLens and NetFlix datasets. Experimental results obtained using Top-K recommendation task indicates the superiority of the proposed GRS method over the others. The proposed GRS model provides the best performance when we balance the number of member in a group and the number of recommended items.},
  archive      = {J_APIN},
  author       = {Pujahari, Abinash and Sisodia, Dilip Singh},
  doi          = {10.1007/s10489-020-01848-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {658-672},
  shortjournal = {Appl. Intell.},
  title        = {Preference relation based collaborative filtering with graph aggregation for group recommender system},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-heuristic algorithms for resource management in crisis
based on OWA approach. <em>APIN</em>, <em>51</em>(2), 646–657. (<a
href="https://doi.org/10.1007/s10489-020-01808-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crisis management, Threat Evaluation (TE) and Resource Allocation (RA) are two key components. To build an automated system in this area after modelling Threat Evaluation and Resource Allocation processes, solving these models and finding the optimal solution are further important issues. In this paper, Non-dominated Sorting Genetic Algorithm-II (NSGA-II) and Strength Pareto Evolutionary Algorithms (SPEA-II) are employed to solve a multi-objective multi-stage Resource Allocation problem. These Algorithms have been compared using normalized values of the objectives by generational distance, spread, hyper-volume, cardinality and actual computational times. It is found that the non-dominated solutions obtained by SPEA-II are better than NSGA-II both in terms of convergence and diversity but at the expense of computational time. Here, the fuzzy inference systems and the decision tree have been used to conduct threat evaluation process. Finally, Ordered Weighted Averaging (OWA) with maximum Bayesian entropy method for determining the operator weights has been used to pick the final choice among optimal options. We plan to use the proposed method in this paper for crisis management in Iranian Red Crescent organization during fire fighting. Two real studies have been done and results have been presented.},
  archive      = {J_APIN},
  author       = {Ghanbari, Abdolreza Asadi and Alaei, Hossein},
  doi          = {10.1007/s10489-020-01808-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {646-657},
  shortjournal = {Appl. Intell.},
  title        = {Meta-heuristic algorithms for resource management in crisis based on OWA approach},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EnsPKDE&amp;IncLKDE: A hybrid time series prediction
algorithm integrating dynamic ensemble pruning, incremental learning,
and kernel density estimation. <em>APIN</em>, <em>51</em>(2), 617–645.
(<a href="https://doi.org/10.1007/s10489-020-01802-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble pruning can effectively overcome several shortcomings of the classical ensemble learning paradigm, such as the relatively high time and space complexity. However, each predictor has its own unique ability. One predictor may not perform well on some samples, but it will perform very well on other samples. Blindly underestimating the power of specific predictors is unreasonable. Choosing the best predictor set for each query sample is exactly what dynamic ensemble pruning techniques address. This paper proposes a hybrid Time Series Prediction (TSP) algorithm to implement one-step-ahead prediction task, integrating Dynamic Ensemble Pruning (DEP), Incremental Learning (IL), and Kernel Density Estimation (KDE), abbreviated as the EnsPKDE&amp;IncLKDE algorithm. It dynamically selects proper predictor sets based on the kernel density distribution of all base learners’ prediction values. Due to the characteristic of TSP problems that samples arrive in chronological order, the idea of IL is naturally introduced into EnsPKDE&amp;IncLKDE, while DEP is a common technology to address the concept drift issue inherent in IL. The algorithm is divided into three subprocesses: 1) Overproduction, which generates the original ensemble learning system; 2) Dynamic Ensemble Pruning (DEP), achieved by one subalgorithm called EnsPKDE; 3) Incremental Learning (IL), realized by one subalgorithm termed IncLKDE. Benefited from the advantages of integrating Dynamic Ensemble Pruning scheme, Incremental Learning paradigm and Kernel Density Estimation, in the experimental results, EnsPKDE&amp;IncLKDE demonstrates superior prediction performance to several other state-of-the-art algorithms in fulfilling time series forecasting tasks.},
  archive      = {J_APIN},
  author       = {Zhu, Gangliang and Dai, Qun},
  doi          = {10.1007/s10489-020-01802-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {617-645},
  shortjournal = {Appl. Intell.},
  title        = {EnsPKDE&amp;IncLKDE: A hybrid time series prediction algorithm integrating dynamic ensemble pruning, incremental learning, and kernel density estimation},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new user similarity measure in a new prediction model for
collaborative filtering. <em>APIN</em>, <em>51</em>(1), 586–615. (<a
href="https://doi.org/10.1007/s10489-020-01811-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Recommender Systems (RSs) based on the performance of Collaborative filtering (CF) depends on similarities among users or items obtained by a user-item rating matrix. The conventional measures such as the Pearson correlation coefficient (PCC), cosine (COS), and Jaccard (JACC) provide a varied and dissimilar value when the ratings between the users lie in the positive and negative side of the rating scale. These measures are also not very effective when there is sparsity in the rating matrix of the user-item. These problems are addressed by the Proximity-Impact-Popularity (PIP) similarity measure. Even though the PIP method provides an improved solution for this problem, the range of values for each component in PIP is very high. To address this issue and to improve the performance of a CF-based RS, a modified proximity-impact-popularity (MPIP) similarity measure is introduced. The expression is designed to get PIP values within the range of 0 to 1. A modified prediction expression is proposed to predict the available and unavailable ratings by combining user- and item-related components. The proposed method is tested by using various benchmark datasets. The size of the user-item sparse matrix varies to compare the performance of the methods in terms of mean absolute error, root mean squared error, precision, recall, and F1-measure. The performance of the proposed method is statistically tested through the Friedman and McNemer test. The results obtained by using the evaluation criteria indicate that the proposed method provides a better solution than the conventional methods. The statistical analysis reveals that the proposed method provides minimum MAE and RMSE values. Similarly, it also provides a maximum F1-measure for all the sub-problems.},
  archive      = {J_APIN},
  author       = {Manochandar, S. and Punniyamoorthy, M.},
  doi          = {10.1007/s10489-020-01811-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {586-615},
  shortjournal = {Appl. Intell.},
  title        = {A new user similarity measure in a new prediction model for collaborative filtering},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep transfer learning-based automated detection of COVID-19
from lung CT scan slices. <em>APIN</em>, <em>51</em>(1), 571–585. (<a
href="https://doi.org/10.1007/s10489-020-01826-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung abnormality is one of the common diseases in humans of all age group and this disease may arise due to various reasons. Recently, the lung infection due to SARS-CoV-2 has affected a larger human community globally, and due to its rapidity, the World-Health-Organisation (WHO) declared it as pandemic disease. The COVID-19 disease has adverse effects on the respiratory system, and the infection severity can be detected using a chosen imaging modality. In the proposed research work; the COVID-19 is detected using transfer learning from CT scan images decomposed to three-level using stationary wavelet. A three-phase detection model is proposed to improve the detection accuracy and the procedures are as follows; Phase1- data augmentation using stationary wavelets, Phase2- COVID-19 detection using pre-trained CNN model and Phase3- abnormality localization in CT scan images. This work has considered the well known pre-trained architectures, such as ResNet18, ResNet50, ResNet101, and SqueezeNet for the experimental evaluation. In this work, 70% of images are considered to train the network and 30% images are considered to validate the network. The performance of the considered architectures is evaluated by computing the common performance measures. The result of the experimental evaluation confirms that the ResNet18 pre-trained transfer learning-based model offered better classification accuracy (training = 99.82%, validation = 97.32%, and testing = 99.4%) on the considered image dataset compared with the alternatives.},
  archive      = {J_APIN},
  author       = {Ahuja, Sakshi and Panigrahi, Bijaya Ketan and Dey, Nilanjan and Rajinikanth, Venkatesan and Gandhi, Tapan Kumar},
  doi          = {10.1007/s10489-020-01826-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {571-585},
  shortjournal = {Appl. Intell.},
  title        = {Deep transfer learning-based automated detection of COVID-19 from lung CT scan slices},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal attention on manifold space for 3D human
action recognition. <em>APIN</em>, <em>51</em>(1), 560–570. (<a
href="https://doi.org/10.1007/s10489-020-01803-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, skeleton-based action recognition has become increasingly prevalent in computer vision due to its wide range of applications, and many approaches have been proposed to address this task. Among these methods, manifold space is widely used to deal with the relative geometric relationships between different body parts in human skeletons. Existing studies treat all geometric relationships as having the same degree of importance; thus, they cannot focus on significant information. In addition, the traditional attention mechanism aims mostly to solve the attention problems in Euclidean space, and is not applicable in manifold space. To investigate these issues, we propose a spatial and temporal attention mechanism on Lie groups for 3D human action recognition. We build our network architecture with a generalized attention mechanism that extends the scope of attention from traditional Euclidean space to manifold space. In addition, our model can learn to identify the significant spatial features and temporal stages with effective attention modules, which focus on discriminative transformation relationships between different rigid bodies within each frame and allocate different levels of attention to different frames. Extensive experiments are conducted on standard datasets and the experimental results demonstrate the effectiveness of the proposed network architecture.},
  archive      = {J_APIN},
  author       = {Ding, Chongyang and Liu, Kai and Cheng, Fei and Belyaev, Evgeny},
  doi          = {10.1007/s10489-020-01803-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {560-570},
  shortjournal = {Appl. Intell.},
  title        = {Spatio-temporal attention on manifold space for 3D human action recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E-FCNN for tiny facial expression recognition.
<em>APIN</em>, <em>51</em>(1), 549–559. (<a
href="https://doi.org/10.1007/s10489-020-01855-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a hot issue in recent years, facial expression recognition(FER) has been widely applied in many fields, but it still faces great challenges in tiny facial expression recognition. Currently, most of the FER networks only consider images of ideal sizes. Their recognition accuracy would significantly decrease as the image resolution decreases. However, images captured by surveillance cameras often contain tiny faces with low-resolution. This paper proposes an edge-aware feedback convolutional neural network(E-FCNN) for tiny FER, which associates image super-resolution and facial expression recognition together. To effectively leverage the texture information of faces, we design a novel three-stream super-resolution network, which is embedded with an edge-enhancement block as one branch. The other two branches are the up-sampling branch and SR(Super-Resolution) primary branch. Specifically, visual features are extracted from tiny images based on a hierarchical strategy, and then put into a feedback block with fused results of the three branches. Experiments are performed on down-sampled images in four facial expression datasets: CK+, FER2013, BU-3DFE, RAF-DB. The results demonstrate the favorable performance of our network.},
  archive      = {J_APIN},
  author       = {Shao, Jie and Cheng, Qiyu},
  doi          = {10.1007/s10489-020-01855-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {549-559},
  shortjournal = {Appl. Intell.},
  title        = {E-FCNN for tiny facial expression recognition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive inverse multilayer fuzzy control for uncertain
nonlinear system optimizing with differential evolution algorithm.
<em>APIN</em>, <em>51</em>(1), 527–548. (<a
href="https://doi.org/10.1007/s10489-020-01819-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel adaptive inverse multilayer T-S fuzzy controller (AIMFC) optimally identified with an optimization soft computing algorithm available for a class of robust control applied in uncertain nonlinear SISO systems. The parameters of multilayer T-S fuzzy model are optimally identified by the differential evolution (DE) algorithm to create offline the inverse nonlinear plant with uncertain coefficients. Then, the adaptive fuzzy-based sliding mode surface is applied to ensure that the closed-loop system is asymptotically stable in which the stability is satisfied using Lyapunov stability concept. The control quality of the proposed AIMFC algorithm is compared with the three recent advanced control algorithms applied in the Spring-Mass-Damper (SMD) benchmark system. Simulation and experiment results with different control parameters show that the proposed algorithm is better than the inverse fuzzy controller and the conventional adaptive fuzzy controller comparatively applied in both SMD system and the coupled-liquid tank system with the performance index using the least mean squares (LMS) error, which is investigated to demonstrate the efficiency and the robustness of the proposed AIMFC control approach.},
  archive      = {J_APIN},
  author       = {Van Kien, Cao and Anh, Ho Pham Huy and Son, Nguyen Ngoc},
  doi          = {10.1007/s10489-020-01819-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {527-548},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive inverse multilayer fuzzy control for uncertain nonlinear system optimizing with differential evolution algorithm},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-valued and sequential-labeled decision tree method
for recommending sequential patterns in cold-start situations.
<em>APIN</em>, <em>51</em>(1), 506–526. (<a
href="https://doi.org/10.1007/s10489-020-01806-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We plan to recommend some initial suitable single-itemed sequences like a flight itinerary based on a preference pattern in the form of personalized sequential pattern to each cold-start user. However, sequential pattern mining has never treated a conventional sequential pattern as a personalized pattern. Besides, as a cold-start user lacks the personalized sequential pattern, collaborative filtering cannot recommend one any single-itemed sequences. Thus, we first design such a preference pattern, namely representative sequential pattern, which reflects one’s main frequently recurring buying behavior mined from the item-sequences during a time period. After sampling a training-set from non-cold-start users who prefer similar items, we propose an auxiliary algorithm to mine the representative sequential pattern as the sequential class labels of each training instance. A multi-label classifier seems therefore be trained to predict the sequential-label for each cold-start user based on one’s features. However, most multi-label classification methods are designed to classify data whose class labels are non-sequential. Besides, some of the predictor attributes would be multi-valued in the real world. Aiming to handle such data, we have developed a novel algorithm, named MSDT (Multi-valued and Sequential-labeled Decision Tree). Experimental results indicate it outperforms all the baseline multi-label algorithms in accuracy even if three of them are deep learning algorithms.},
  archive      = {J_APIN},
  author       = {Hsu, Chang-Ling},
  doi          = {10.1007/s10489-020-01806-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {506-526},
  shortjournal = {Appl. Intell.},
  title        = {A multi-valued and sequential-labeled decision tree method for recommending sequential patterns in cold-start situations},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autoencoder-based unsupervised clustering and hashing.
<em>APIN</em>, <em>51</em>(1), 493–505. (<a
href="https://doi.org/10.1007/s10489-020-01797-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Faced with a large amount of data and high-dimensional data information in a database, the existing exact nearest neighbor retrieval methods cannot obtain ideal retrieval results within an acceptable retrieval time. Therefore, researchers have begun to focus on approximate nearest neighbor retrieval. Recently, the hashing-based approximate nearest neighbor retrieval method has attracted increasing attention because of its small storage space and high retrieval efficiency. The development of neural networks has also promoted progress in hash learning. However, these methods are mostly supervised. In practical applications, annotating large amounts of data is a very time-consuming and laborious task. Furthermore, efficiently using a large amount of unlabeled data for hash learning is challenging. In this paper, we create a new autoencoder variant to efficiently capture the features of high-dimensional data, and propose an unsupervised deep hashing method for large-scale data retrieval, named as Autoencoder-based Unsupervised Clustering and Hashing (AUCH). By constructing a hashing layer as a hidden layer of the autoencoder, hash learning is performed together with unsupervised clustering by minimizing the overall loss. AUCH can unify unsupervised clustering and retrieval tasks into a single learning model. In addition, the method can use a deep neural network to simultaneously learn feature representations, hashing functions and cluster assignments. Experimental results on standard datasets indicate that AUCH achieves competitive results compared to state-of-the-art models for retrieval and clustering tasks.},
  archive      = {J_APIN},
  author       = {Zhang, Bolin and Qian, Jiangbo},
  doi          = {10.1007/s10489-020-01797-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {493-505},
  shortjournal = {Appl. Intell.},
  title        = {Autoencoder-based unsupervised clustering and hashing},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid algorithm for the university course timetabling
problem using the improved parallel genetic algorithm and local search.
<em>APIN</em>, <em>51</em>(1), 467–492. (<a
href="https://doi.org/10.1007/s10489-020-01833-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling is one of the problems that has attracted the attention of many researchers over the years. The University Course Timetabling Problem (UCTP) is a highly constrained real-world combinatorial optimization task. Designing course timetables for academic institutions has always been challenging, because it is a non-deterministic polynomial-time hardness (NP-hard) problem. This problem attempts to assign specific timeslots and rooms to the events considering a number of hard and soft constraints. All hard constraints must be satisfied to achieve a feasible solution, whereas satisfying all soft constraints is not necessary. Although the quality of the solution is directly related to the number of soft constraints that are satisfied. One of the recent innovative methodologies for solving UCTP is the hybrid algorithm, which attempts to automate the timetabling design process so that it would be able to work with different instances of problem domains. In this paper, we present a hybrid method based on the Improved Parallel Genetic Algorithm and Local Search (IPGALS) to solve the course timetabling problem. The Local Search (LS) approach is used to strengthen the Genetic Algorithm (GA). The IPGALS has applied a representation of the timetable, which ensure the hard constraints would never be violated. Hard constraints are measured by Distance to Feasibility (DF) criterion. In fact, applying the DF criterion leads to achieving feasible solutions and promotes the performance of our algorithm. Due to the wide range of problem constraints, the proposed algorithm is performed in parallel to improve the GA searching process. The IPGALS algorithm is tested over BenPaechter and ITC-2007 standard benchmarks and compared with the state-of-the-art techniques in this literature. The experimental results confirm the effectiveness and the superiority of the proposed algorithm compared to other prominent methods for solving UCTP.},
  archive      = {J_APIN},
  author       = {Rezaeipanah, Amin and Matoori, Samaneh Sechin and Ahmadi, Gholamreza},
  doi          = {10.1007/s10489-020-01833-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {467-492},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid algorithm for the university course timetabling problem using the improved parallel genetic algorithm and local search},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft-margin convex polyhedron classifier for nonlinear
task with noise tolerance. <em>APIN</em>, <em>51</em>(1), 453–466. (<a
href="https://doi.org/10.1007/s10489-020-01854-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special form of piecewise linear classifier, the convex polyhedron classifier is simple to implement and achieves rapid response in real-time classification. However, it usually performs badly in the case of high noise where severe boundary intrusion exists. Inspired by the scheme of soft margin in support vector machine, in this paper we propose a soft-margin convex polyhedron classifier for nonlinear classification task. The base (linear) classifier is first generalized to its soft-margin version through kernelization process and slack variables. In each local region, the soft-margin base classifier learns a decision hyperplane with noise tolerance. Then, a series of learned hyperplanes are structurally integrated into a convex polyhedron classifier, which is essentially a convex polyhedron that encloses one class and excludes the other class outside. Experimental results on fifteen benchmark datasets show the proposed soft-margin convex polyhedron classifier is comparable to linear support vector machine and four piecewise linear classifiers, but does not perform as well as the support vector machine with radial basis function kernel in general. When random noises are added to datasets, the soft-margin convex polyhedron classifier achieves similar or better accuracies with the well-known classifiers used for comparison, implying its promising ability of noise tolerance.},
  archive      = {J_APIN},
  author       = {Leng, Qiangkui and He, Zuowei and Liu, Yuqing and Qin, Yuping and Li, Yujian},
  doi          = {10.1007/s10489-020-01854-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {453-466},
  shortjournal = {Appl. Intell.},
  title        = {A soft-margin convex polyhedron classifier for nonlinear task with noise tolerance},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Block-sparse CNN: Towards a fast and memory-efficient
framework for convolutional neural networks. <em>APIN</em>,
<em>51</em>(1), 441–452. (<a
href="https://doi.org/10.1007/s10489-020-01815-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a block-sparse convolutional neural network (BSCNN) architecture that converts a dense convolution kernel into a sparse one. Traditional convolutional neural networks (CNNs) face the problem that an increase in the number of network parameters will lead to more model and floating-point computations, and a higher risk of network overfitting. The block-sparse convolution uses sparse factor pairs to randomize a sparse convolution kernel, which can introduce mixed information and thereby enabling the extraction of more diverse features. In the meantime, a SUMMA-based parallel computing method is adopted to achieve a lightweight storage and a fast calculation of the convolution kernel. Experimental results show that, compared with current sparse networks, the proposed framework achieves better prediction accuracy than the classical backbone networks in terms of faster floating-point operation and less storage space requirements.},
  archive      = {J_APIN},
  author       = {Wen, Nu and Guo, Renzhong and He, Biao and Fan, Yong and Ma, Ding},
  doi          = {10.1007/s10489-020-01815-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {441-452},
  shortjournal = {Appl. Intell.},
  title        = {Block-sparse CNN: Towards a fast and memory-efficient framework for convolutional neural networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd counting method based on the self-attention residual
network. <em>APIN</em>, <em>51</em>(1), 427–440. (<a
href="https://doi.org/10.1007/s10489-020-01842-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the crowd density in surveillance videos is a hot issue in the field of computer vision and has become the basis of data processing and analysis of public transport services, commercial passenger flow analysis, public security protection and other industries. However, in terms of practical applications, due to the problems of pedestrian occlusion and scale changes, existing methods are inadequate with regard to the acquisition of the human head, which affects the accuracy of counting. To solve this problem, a crowd counting method based on a self-attention residual network is proposed. First, a multiscale convolution module composed of dilated convolution and deformation convolution is used. To avoid losing image resolution, some of the sampling positions are shifted to the occluded crowd by shifting the sampling points, which solves the problem of crowd occlusion. Then, a self-attention residual module is designed to score and classify the feature map, which allows all pixels in the feature map to be classified. The corresponding weight is generated, and the population scale is determined by the weight, which solves the problem of crowd scale changes. The algorithm is applied in ShanghaiTech and the UCF_CC_50 and WorldExpo’10 datasets are tested. The experimental results show that the mean absolute error (MAE) and mean square error (MSE) of this algorithm are significantly reduced compared with those of a comparative algorithm.},
  archive      = {J_APIN},
  author       = {Liu, Yan-Bo and Jia, Rui-Sheng and Liu, Qing-Ming and Zhang, Xing-Li and Sun, Hong-Mei},
  doi          = {10.1007/s10489-020-01842-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {427-440},
  shortjournal = {Appl. Intell.},
  title        = {Crowd counting method based on the self-attention residual network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attributed network representation learning via improved
graph attention with robust negative sampling. <em>APIN</em>,
<em>51</em>(1), 416–426. (<a
href="https://doi.org/10.1007/s10489-020-01825-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed network representation learning is to embed graphs in low dimensional vector space such that the embedded vectors follow the differences and similarities of the source graphs. To capture structural features and node attributes of attributed network, we propose a novel graph auto-encoder method which is stacked encoder-decoder layers based on graph attention with robust negative sampling. Here, minimize the negative log-likelihood, triplet distance, and weighted neighborhood attributes are proposed as the loss function. To alleviate the over-fitting on reconstruct graph structural features or node attributes, a trade off algorithm between reconstruction loss of node attributes and reconstruction loss of structural features is proposed. Furthermore, to alleviate the impact of random sampling, we propose additional constraints on negative sampling based on node degree. Experimental results on several benchmark datasets for transductive and inductive learning tasks show that the proposed model is competitive against well-known methods in node classification and link prediction.},
  archive      = {J_APIN},
  author       = {Fan, Huilian and Zhong, Yuanchang and Zeng, Guangpu and Sun, Lili},
  doi          = {10.1007/s10489-020-01825-x},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {416-426},
  shortjournal = {Appl. Intell.},
  title        = {Attributed network representation learning via improved graph attention with robust negative sampling},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the one-against-all binary approach for multiclass
classification using balancing techniques. <em>APIN</em>,
<em>51</em>(1), 396–415. (<a
href="https://doi.org/10.1007/s10489-020-01805-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-against-one and one-against-all are common approaches to break down multiclass classification problems into binary classification problems and build a multiclass classifier. The former approach often yields better multiclass classifiers than the latter due to its structure. The one-against-all approach strengthens or sometimes creates linear inseparability and class imbalance in the binary classifiers during the training phase. In this sense, balancing techniques can be applied to handle the binary imbalance problem and motivate the use of the computationally simpler approach. The one-against-all approach with balancing techniques proposed in this work reaches better accuracy values than the pure one-against-all approach for 7 out of 8 datasets and shows a considerable increase in the weighted recall value for 4 out of 8 datasets. Besides, the accuracy values of the one-against-all approach with balancing techniques are considerably closer to the ones found by the one-against-one approach with less computational efforts.},
  archive      = {J_APIN},
  author       = {Silva, Warley Almeida and Villela, Saulo Moraes},
  doi          = {10.1007/s10489-020-01805-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {396-415},
  shortjournal = {Appl. Intell.},
  title        = {Improving the one-against-all binary approach for multiclass classification using balancing techniques},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving covariance-regularized discriminant analysis for
EHR-based predictive analytics of diseases. <em>APIN</em>,
<em>51</em>(1), 377–395. (<a
href="https://doi.org/10.1007/s10489-020-01810-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Discriminant Analysis (LDA) is a well-known technique for feature extraction and dimension reduction. The performance of classical LDA however, significantly degrades on the High Dimension Low Sample Size (HDLSS) data for the ill-posed inverse problem. Existing approaches for HDLSS data classification typically assume the data in question are with Gaussian distribution and deal the HDLSS classification problem with regularization. However, these assumptions are too strict to hold in many emerging real-life applications, such as enabling personalized predictive analysis using Electronic Health Records (EHRs) data collected from an extremely limited number of patients who have been diagnosed with or without the target disease for prediction. In this paper, we revised the problem of predictive analysis of disease using personal EHR data and LDA classifier. To fill the gap, in this paper, we first studied an analytical model that understands the accuracy of LDA for classifying data with arbitrary distribution. The model gives a theoretical upper bound of LDA error rate that is controlled by two factors: (1) the statistical convergence rate of (inverse) covariance matrix estimators and (2) the divergence of the training/testing datasets to fitted distributions. To this end, we could lower the error rate by balancing the two factors for better classification performance. Hereby, we further proposed a novel LDA classifier De-Sparse that leverages De-sparsified Graphical Lasso to improve the estimation of LDA, which outperforms state-of-the-art LDA approaches developed for HDLSS data. Such advances and effectiveness are further demonstrated by both theoretical analysis and extensive experiments on EHR datasets https://www.overleaf.com/project/5d2728c718f6ff3b2bcf5991 .},
  archive      = {J_APIN},
  author       = {Yang, Sijia and Xiong, Haoyi and Xu, Kaibo and Wang, Licheng and Bian, Jiang and Sun, Zeyi},
  doi          = {10.1007/s10489-020-01810-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {377-395},
  shortjournal = {Appl. Intell.},
  title        = {Improving covariance-regularized discriminant analysis for EHR-based predictive analytics of diseases},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JDF-DE: A differential evolution with jrand number
decreasing mechanism and feedback guide technique for global numerical
optimization. <em>APIN</em>, <em>51</em>(1), 359–376. (<a
href="https://doi.org/10.1007/s10489-020-01795-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential Evolution (DE) is a readily comprehensible and highly powerful intelligent optimized method for numerical optimization. The performance of DE significantly depends on its parameters and strategies generating both mutation vector and trial vector. To further enhance its exhibition, we propose a new DE variant called JDF-DE based on JADE by introducing the improved parameter approach with weight and crossover strategy with Jrand number decreasing mechanism and feedback guide technique. The new way for updating parameter μCR and μF brings fitness value to generate more reasonable parameters with the fixed orientation during evolution. Meanwhile, Levy distribution is used to complete the adaptive distribution of CR when the population has a high clustering intensity so that solutions escape from the local optimal value. Jrand number decreasing mechanism is embedded to crossover operation to strengthen population diversity instead the number of Jrand equals 1 in primary DE algorithm. Feedback guide method is utilized to determine the step size for Jrand number to advance the ability that JDF-DE searches the optimum value. In order to investigate performance of JDF-DE, In order to analyze performance of JDF-DE, 29 benchmark functions from CEC2017 on real parameter optimization are employed to verify the validity of JDF-DE for solving complex high-dimensional problems. The experimental results show that JDF-DE is better than, or at least comparable with several state-of-the-art DE variants including DE variants JADE, SinDE, TSDE, AGDE, and EFADE and non-DE variants TSA, SHO, GWO, MVO, SCA, and GSA in the global numerical optimization problems.},
  archive      = {J_APIN},
  author       = {Deng, LiBao and Sun, Haili and Li, Chunlei},
  doi          = {10.1007/s10489-020-01795-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {359-376},
  shortjournal = {Appl. Intell.},
  title        = {JDF-DE: A differential evolution with jrand number decreasing mechanism and feedback guide technique for global numerical optimization},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of COVID-19 using CXR and CT images using transfer
learning and haralick features. <em>APIN</em>, <em>51</em>(1), 341–358.
(<a href="https://doi.org/10.1007/s10489-020-01831-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of COVID-19 is a challenging task which consistently requires taking a gander at clinical images of patients. In this paper, the transfer learning technique has been applied to clinical images of different types of pulmonary diseases, including COVID-19. It is found that COVID-19 is very much similar to pneumonia lung disease. Further findings are made to identify the type of pneumonia similar to COVID-19. Transfer Learning makes it possible for us to find out that viral pneumonia is same as COVID-19. This shows the knowledge gained by model trained for detecting viral pneumonia can be transferred for identifying COVID-19. Transfer Learning shows significant difference in results when compared with the outcome from conventional classifications. It is obvious that we need not create separate model for classifying COVID-19 as done by conventional classifications. This makes the herculean work easier by using existing model for determining COVID-19. Second, it is difficult to detect the abnormal features from images due to the noise impedance from lesions and tissues. For this reason, texture feature extraction is accomplished using Haralick features which focus only on the area of interest to detect COVID-19 using statistical analyses. Hence, there is a need to propose a model to predict the COVID-19 cases at the earliest possible to control the spread of disease. We propose a transfer learning model to quicken the prediction process and assist the medical professionals. The proposed model outperforms the other existing models. This makes the time-consuming process easier and faster for radiologists and this reduces the spread of virus and save lives.},
  archive      = {J_APIN},
  author       = {Perumal, Varalakshmi and Narayanan, Vasumathi and Rajasekar, Sakthi Jaya Sundar},
  doi          = {10.1007/s10489-020-01831-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {341-358},
  shortjournal = {Appl. Intell.},
  title        = {Detection of COVID-19 using CXR and CT images using transfer learning and haralick features},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature selection for multi-label classification by
maximizing full-dimensional conditional mutual information.
<em>APIN</em>, <em>51</em>(1), 326–340. (<a
href="https://doi.org/10.1007/s10489-020-01822-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional mutual information (CMI) maximization is a promising criterion for feature selection in a computationally efficient stepwise way, but it is hard to be applied comprehensively because of imprecise probability calculation and heavy computational load. Many dimension-reduced CMI-based and mutual information (MI)-based methods have been reported to achieve state-of-art performances in terms of classification. However, model deviations are introduced into the CMI and MI formulations in these methods during dimension reduction. In this paper, we start with the full-dimensional CMI to deal with the feature selection problem, so as to retain full inter-feature and feature-label mutual information when selecting new features. The cost function is approximated and simplified from a mathematical perspective to overcome the difficulties for maximizing the original full-dimensional CMI. A relationship is established between the proposed feature selection criterion and the one based on Hilbert-Schmidt independence, which explains qualitatively how the new criterion succeeds to achieve relevance maximization and redundance minimization simultaneously. Experiments on real-world datasets demonstrate the predominance of the proposed method over the existing ones.},
  archive      = {J_APIN},
  author       = {Sha, Zhi-Chao and Liu, Zhang-Meng and Ma, Chen and Chen, Jun},
  doi          = {10.1007/s10489-020-01822-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {326-340},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection for multi-label classification by maximizing full-dimensional conditional mutual information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient stacking model with label selection for
multi-label classification. <em>APIN</em>, <em>51</em>(1), 308–325. (<a
href="https://doi.org/10.1007/s10489-020-01807-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary relevance (BR) is one of the most popular frameworks in multi-label learning. It constructs a group of binary classifiers, one for each label. BR is a simple and intuitive way to deal with multi-label problem, but fails to utilize label correlations. To deal with this problem, dependent binary relevance (DBR) and other works employ stacking learning paradigm for BR, in which all labels are viewed as additional features. Those works may be suboptimal as each label has its own most related label subset. In this paper, a novel two-layer stacking based approach, which is named a Stacking Model with Label Selection (SMLS), is induced to exploit proper label correlations for improving the performance of DBR. At the first layer, we construct several binary classifiers in the way of BR. At the second layer, we find the specific label subset through label selection for each labels , and expand them into feature space. The final binary classifiers are constructed based on their corresponding augmented feature space. Comprehensive experiments are conducted on a collection of benchmark data sets. Comparison results with the state-of-the-art approaches validate the competitive performance of our proposed approach. Comparison results with DBR shows that our approach is not only more time efficient but also more robust.},
  archive      = {J_APIN},
  author       = {Chen, Yan-Nan and Weng, Wei and Wu, Shun-Xiang and Chen, Bai-Hua and Fan, Yu-Ling and Liu, Jing-Hua},
  doi          = {10.1007/s10489-020-01807-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {308-325},
  shortjournal = {Appl. Intell.},
  title        = {An efficient stacking model with label selection for multi-label classification},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decision-making method based on new entropy and refined
single-valued neutrosophic sets and its application in typhoon disaster
assessment. <em>APIN</em>, <em>51</em>(1), 283–307. (<a
href="https://doi.org/10.1007/s10489-020-01706-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a multi-attribute decision-making method for the decision-making problems with attributes and sub-attribute where the attribute weight is unknown, based on information entropy and the evaluation based on distance from average solution (EDAS) method under a refined single-valued neutrosophic set environment. First, the new distance measure, similarity measure, and neutrosophic entropy based on refined single-valued neutrosophic sets are defined. Further, the relationship between them is discussed and the attribute weights are determined based on the new neutrosophic entropy. Then, the EDAS method is used to rank and select the best alternative. Finally, two illustrative examples of typhoon disaster assessment (typhoon disaster assessment with multi-layer indicators and dynamic assessment of typhoon disaster) are presented to demonstrate the feasibility, effectiveness, and practicality of the proposed method. The advantages of the proposed method are illustrated by sensitive analysis and comparative analysis with other methods.},
  archive      = {J_APIN},
  author       = {Tan, Rui-pu and Zhang, Wen-de},
  doi          = {10.1007/s10489-020-01706-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {283-307},
  shortjournal = {Appl. Intell.},
  title        = {Decision-making method based on new entropy and refined single-valued neutrosophic sets and its application in typhoon disaster assessment},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple delay-dependent noise-to-state stability for a
class of uncertain switched random nonlinear systems with intermittent
sensor and actuator faults. <em>APIN</em>, <em>51</em>(1), 265–282. (<a
href="https://doi.org/10.1007/s10489-020-01753-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on multiple delay-dependent noise-to-state stability (NSS) for a class of switched random nonlinear systems against uncertainty terms and intermittent sensor and actuator faults. External disturbances, nonlinear functions, as well as measurement noise, are also taken into account. This is the first attempt to achieve dynamic output feedback controller design for uncertain switched random nonlinear systems subject to intermittent sensor and actuator faults. First, a controller is established to perform passive fault-tolerant control (FTC). Random systems are more common than Itô stochastic systems. Thus, compared with the previous works, the proposed controller has a wider application scope and is more feasible. Next, an augmented closed-loop system is exhibited to realize NSS. Moreover, a piecewise Lyapunov function is utilized with less conservatism than common Lyapunov function. The delay dependent stability conditions are gathered via linear matrix inequalities (LMIs) and controller matrices are earned. At last, the novelty and validity of the approach suggested in this paper are demonstrated through two simulation examples.},
  archive      = {J_APIN},
  author       = {Sun, Shaoxin and Zhang, Huaguang and Han, Jian and Gao, Zhiyun},
  doi          = {10.1007/s10489-020-01753-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {265-282},
  shortjournal = {Appl. Intell.},
  title        = {Multiple delay-dependent noise-to-state stability for a class of uncertain switched random nonlinear systems with intermittent sensor and actuator faults},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised multi-view representation learning with
proximity guided representation and generalized canonical correlation
analysis. <em>APIN</em>, <em>51</em>(1), 248–264. (<a
href="https://doi.org/10.1007/s10489-020-01821-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data can collaborate with each other to provide more comprehensive information than single-view data. Although there exist a few unsupervised multi-view representation learning methods taking both the discrepancies and incorporating complementary information from different views into consideration, they always ignore the use of inner-view discriminant information. It remains challenging to learn a meaningful shared representation of multiple views. To overcome this difficulty, this paper proposes a novel unsupervised multi-view representation learning model, MRL. Unlike most state-of-art multi-view representation learning, which only can be used for clustering or classification task, our method explores the proximity guided representation from inner-view and complete the task of multi-label classification and clustering by the discrimination fusion representation simultaneously. MRL consists of three parts. The first part is a deep representation learning for each view and then aims to represent the latent specific discriminant characteristic of each view, the second part builds a proximity guided dynamic routing to preserve its inner features of direction,location and etc. At last, the third part, GCCA-based fusion, exploits the maximum correlations among multiple views based on Generalized Canonical Correlation Analysis (GCCA). To the best of our knowledge, the proposed MRL could be one of the first unsupervised multi-view representation learning models that work in proximity guided dynamic routing and GCCA modes. The proposed model MRL is tested on five multi-view datasets for two different tasks. In the task of multi-label classification, the results show that our model is superior to the state-of-the-art multi-view learning methods in precision, recall, F1 and accuracy. In clustering task, its performance is better than the latest related popular algorithms. And the performance varies w.r.t. the dimensionality of G is also made to explore the characteristics of MRL.},
  archive      = {J_APIN},
  author       = {Zheng, Tingyi and Ge, Huibin and Li, Jiayi and Wang, Li},
  doi          = {10.1007/s10489-020-01821-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {248-264},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised multi-view representation learning with proximity guided representation and generalized canonical correlation analysis},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning and control algorithms of direct perception
for autonomous driving. <em>APIN</em>, <em>51</em>(1), 237–247. (<a
href="https://doi.org/10.1007/s10489-020-01827-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an end-to-end machine learning model that integrates multi-task (MT) learning, convolutional neural networks (CNNs), and control algorithms to achieve efficient inference and stable driving for self-driving cars. The CNN-MT model can simultaneously perform regression and classification tasks for estimating perception indicators and driving decisions, respectively, based on the direct perception paradigm of autonomous driving. The model can also be used to evaluate the inference efficiency and driving stability of different CNNs on the metrics of CNN’s size, complexity, accuracy, processing speed, and collision number, respectively, in a dynamic traffic. We also propose new algorithms for controllers to drive a car using the indicators and its short-range sensory data to avoid collisions in real-time testing. We collect a set of images from a camera of The Open Racing Car Simulator in various driving scenarios, train the model using this dataset, test it in unseen traffics, and find that it outperforms earlier models in highway traffic. The stability of end-to-end learning and self driving depends crucially on the dynamic interplay between CNN and control algorithms. The source code and data of this work are available on our website, which can be used as a simulation platform to evaluate different learning models on equal footing and quantify collisions precisely for further studies on autonomous driving.},
  archive      = {J_APIN},
  author       = {Lee, Der-Hau and Chen, Kuan-Lin and Liou, Kuan-Han and Liu, Chang-Lun and Liu, Jinn-Liang},
  doi          = {10.1007/s10489-020-01827-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {237-247},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning and control algorithms of direct perception for autonomous driving},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diagnosis of complications of type 2 diabetes based on
weighted multi-label small sphere and large margin machine.
<em>APIN</em>, <em>51</em>(1), 223–236. (<a
href="https://doi.org/10.1007/s10489-020-01824-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, type 2 diabetes mellitus (T2DM) is one of the most serious and critical health problems. Persistent hyperglycemia of diabetic patients can lead to other complications, such as macrovascular, microvascular, neuropathy, which are the main cause of death in diabetic patients. Therefore, it is an urgent task to diagnose the complications. To address the above issue, we turn it into a multi-label classification problem by taking macrovascular, microvascular, neuropathy as three labels. Furthermore, we find that it is an imbalanced classification problem for each label. Thus, a novel weighted multi-label small sphere and large margin machine (WML-SSLM) is proposed to diagnose the complications from T2DM in this paper, which is constructed by introducing the binary relevance (BR) method to SSLM. Compared with the BR method, WML-SSLM considers the relevance of labels by giving different weights for different instances. Taking the diabetes dataset from the Chinese PLA General Hospital as the research object, the diagnosis of the macrovascular, microvascular, and neuropathy from T2DM are studied by using our proposed WML-SSLM. The experimental results show that WML-SSLM can effectively deal with the prediction of complications of T2DM. Besides, the relevant features of each complication are analyzed by using the student’s t-test.},
  archive      = {J_APIN},
  author       = {Wang, Hongmei and Xu, Yitian and Chen, Qian and Wang, Xinye},
  doi          = {10.1007/s10489-020-01824-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {223-236},
  shortjournal = {Appl. Intell.},
  title        = {Diagnosis of complications of type 2 diabetes based on weighted multi-label small sphere and large margin machine},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive enhancement method for low illumination color
images. <em>APIN</em>, <em>51</em>(1), 202–222. (<a
href="https://doi.org/10.1007/s10489-020-01792-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to effectively improve the visual effect and image quality of color images under low illumination conditions, we propose an image enhancement method based on HSV and CIEL*a*b* color spaces for adaptively enhancing color image under low illumination conditions. The proposed method takes into account the characteristics of low illumination color images, and has the strategies of contrast, brightness enhancement, and color saturation correction. We utilize our proposed adaptive chaotic particle swarm optimization algorithm in this paper combined with gamma correction to improve the overall brightness of the image, and generate the best brightness adjustment effect in the proposed algorithm. In addition, our improved adaptive stretching function is used to enhance the image saturation. The experimental results show that compared with other traditional and latest color image enhancement algorithms, the proposed algorithm significantly enhances the visual effect of the low illumination color images. It can not only improve the contrast of low illumination color images and avoid color distortion, but also effectively improve the brightness of the image and provide more detail enhancement while maintaining the naturalness of the image.},
  archive      = {J_APIN},
  author       = {Li, Canlin and Liu, Jinhua and Wu, Qinge and Bi, Lihua},
  doi          = {10.1007/s10489-020-01792-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {202-222},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive enhancement method for low illumination color images},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). SLER: Self-generated long-term experience replay for
continual reinforcement learning. <em>APIN</em>, <em>51</em>(1),
185–201. (<a href="https://doi.org/10.1007/s10489-020-01786-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning has achieved significant success in various domains. However, it still faces a huge challenge when learning multiple tasks in sequence. This is because the interaction in a complex setting involves continual learning that results in the change in data distributions over time. A continual learning system should ensure that the agent acquires new knowledge without forgetting the previous one. However, catastrophic forgetting may occur as the new experience can overwrite previous experience due to limited memory size. The dual experience replay algorithm which retains previous experience is widely applied to reduce forgetting, but it cannot be applied in scalable tasks when the memory size is constrained. To alleviate the constrained by the memory size, we propose a new continual reinforcement learning algorithm called Self-generated Long-term Experience Replay (SLER). Our method is different from the standard dual experience replay algorithm, which uses short-term experience replay to retain current task experience, and the long-term experience replay retains all past tasks’ experience to achieve continual learning. In this paper, we first trained an environment sample model called Experience Replay Mode (ERM) to generate the simulated state sequence of the previous tasks for knowledge retention. Then combined the ERM with the experience of the new task to generate the simulation experience all previous tasks to alleviate forgetting. Our method can effectively decrease the requirement of memory size in multiple tasks, reinforcement learning. We show that our method in StarCraft II and the GridWorld environments performs better than the state-of-the-art deep learning method and achieve a comparable result to the dual experience replay method, which retains the experience of all the tasks.},
  archive      = {J_APIN},
  author       = {Li, Chunmao and Li, Yang and Zhao, Yinliang and Peng, Peng and Geng, Xupeng},
  doi          = {10.1007/s10489-020-01786-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {185-201},
  shortjournal = {Appl. Intell.},
  title        = {SLER: Self-generated long-term experience replay for continual reinforcement learning},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic feature scaling and selection for support vector
machine classification with functional data. <em>APIN</em>,
<em>51</em>(1), 161–184. (<a
href="https://doi.org/10.1007/s10489-020-01765-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FunctionalData Analysis (FDA) has become a very important field in recent years due to its wide range of applications. However, there are several real-life applications in which hybrid functional data appear, i.e., data with functional and static covariates. The classification of such hybrid functional data is a challenging problem that can be handled with the Support Vector Machine (SVM). Moreover, the selection of the most informative features may yield to drastic improvements in the classification rates. In this paper, an embedded feature selection approach for SVM classification is proposed, in which the isotropic Gaussian kernel is modified by associating a bandwidth to each feature. The bandwidths are jointly optimized with the SVM parameters, yielding an alternating optimization approach. The effectiveness of our methodology was tested on benchmark data sets. Indeed, the proposed method achieved the best average performance when compared to 17 other feature selection and SVM classification approaches. A comprehensive sensitivity analysis of the parameters related to our proposal was also included, confirming its robustness.},
  archive      = {J_APIN},
  author       = {Jiménez-Cordero, Asunción and Maldonado, Sebastián},
  doi          = {10.1007/s10489-020-01765-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {161-184},
  shortjournal = {Appl. Intell.},
  title        = {Automatic feature scaling and selection for support vector machine classification with functional data},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving multi-objective optimization problem using cuckoo
search algorithm based on decomposition. <em>APIN</em>, <em>51</em>(1),
143–160. (<a href="https://doi.org/10.1007/s10489-020-01816-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cuckoo search (CS) algorithm has been successfully applied in single-objective optimization problems. In addition, decomposition-based multi-objective evolutionary algorithms (MOEA/D) have high performance for multi-objective optimization problems (MOPs). Inspired by this, a new decomposition-based multi-objective CS algorithm is proposed in this paper. Two reproduction operators with different characteristics derived from the CS algorithm are constructed and they compose an operator pool. Then, a bandit-based adaptive operator selection method is used to determine the application of different operators. An angle-based selection strategy that achieves a better balance between convergence and diversity is adopted to preserve diversity. Compared with other improved strategies designed for MOEA/D on two suits of test instances, the proposed algorithm was demonstrated to be effective and competitive for MOPs.},
  archive      = {J_APIN},
  author       = {Chen, Liang and Gan, Wenyan and Li, Hongwei and Cheng, Kai and Pan, Darong and Chen, Li and Zhang, Zili},
  doi          = {10.1007/s10489-020-01816-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {143-160},
  shortjournal = {Appl. Intell.},
  title        = {Solving multi-objective optimization problem using cuckoo search algorithm based on decomposition},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer learning based hybrid 2D-3D CNN for traffic sign
recognition and semantic road detection applied in advanced driver
assistance systems. <em>APIN</em>, <em>51</em>(1), 124–142. (<a
href="https://doi.org/10.1007/s10489-020-01801-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annually, deep learning algorithms have proven their effectiveness in many vision-based applications, such as autonomous driving, traffic, and congestion monitoring, and so on. In computer vision, accurate traffic sign recognition and semantic road detection are vital challenges for increased safety, which are becoming a major research topic for intelligent transport systems community. In this paper, a deep learning-based driving assistance system has been proposed. To this end, we present hybrid 2D-3D CNN models based on the transfer learning paradigm to achieve better performance on benchmark real-world datasets. The primary goal of transfer learning is to improve the learning process in the target domain while transferring relevant knowledge from the source domain. We combine a pre-trained deep 2D CNN and a shallow 3D CNN to significantly reduce complexity and speed-up the training algorithm. The first model, called Hybrid-TSR, is intended to effectively address the task of traffic sign recognition. Hybrid-SRD is the second architecture that allows the semantic detection of road space through a combination of up-sampling and deconvolutional operations. The experimental results show that the proposed methods have considerable relevance in terms of efficiency and accuracy.},
  archive      = {J_APIN},
  author       = {Bayoudh, Khaled and Hamdaoui, Fayçal and Mtibaa, Abdellatif},
  doi          = {10.1007/s10489-020-01801-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {124-142},
  shortjournal = {Appl. Intell.},
  title        = {Transfer learning based hybrid 2D-3D CNN for traffic sign recognition and semantic road detection applied in advanced driver assistance systems},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evacuation simulation method based on an improved
artificial bee colony algorithm and a social force model. <em>APIN</em>,
<em>51</em>(1), 100–123. (<a
href="https://doi.org/10.1007/s10489-020-01711-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation modeling is an important tool for simulating crowd behavior and studying the law of crowd evacuation. It is of great significance for exploring evacuation management methods in emergency situations. The real-time change of evacuation is the main challenge of simulation modeling. In the evacuation simulation, it is difficult for people to choose a suitable route according to the change of evacuation dynamics. This paper proposes a new evacuation simulation method which combines an improved artificial bee colony algorithm for dynamic path planning and SFM (Social Force Model) for simulating the movement of pedestrians, to providing pedestrians with timely route selection. In the path planning layer, we developed a MABCM (Multiple-subpopulations Artificial Bee Colony with Memory) algorithm and proposed a new exit evaluation strategy. These methods can plan a route with the shortest evacuation time for pedestrians according to the dynamic changes of evacuation and improve evacuation efficiency. In the simulated motion layer, we use the SFM to avoid collisions and achieve the reproduction of the evacuation scene. We verified the performance of the proposed MABCM on the CEC 2014 benchmark suite, and the results show that it is superior to the four existing artificial bee colony algorithms in most cases. The proposed crowd evacuation method is verified on an existing SFM platform. The experimental results indicate that the proposed method can efficiently evacuate a dense crowd in multiple scenes and can effectively shorten evacuation time.},
  archive      = {J_APIN},
  author       = {Zhao, Yuan and Liu, Hong and Gao, Kaizhou},
  doi          = {10.1007/s10489-020-01711-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {100-123},
  shortjournal = {Appl. Intell.},
  title        = {An evacuation simulation method based on an improved artificial bee colony algorithm and a social force model},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual attention network using multi-channel dense
connections for image super-resolution. <em>APIN</em>, <em>51</em>(1),
85–99. (<a href="https://doi.org/10.1007/s10489-020-01723-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the methods based on deep convolutional neural networks (DCNN) have greatly promoted the development of image super-resolution (SR). However, deeper and wider SR networks are more difficult to train. For the SR task, the low-frequency information contained in low-resolution images is very important, and the neglect of exploring the feature information across channels hinders the representational capability of DCNN. To address these problems, we enhance the representational capability of DCNN by utilizing the interactive relationship among multiple channels. In this paper, a residual attention network using multi-channel dense connections (MCRAN) is proposed to improve the image super-resolution significantly. This method can make full use of multi-channel information for more effective feature expression and learning. In MCRAN, a multi-channel residual attention (MCRA) module is designed to coalesce the features of multiple different channels and the attention mechanism is applied to adjust the channel features adaptively. Accordingly, the channel features own more discriminative representation. In addition, the multi-source residual group (MSRG) structure is developed to construct a deeper network and simplify the training of network, which contains several long non-local skip connections (L-NLSC) to capture global low-frequency information in remote space. Besides, MSRG contains some short local-source skip connections (S-LSSC) to enhance the information interaction of local network. Extensive experimental evaluation on benchmark datasets on single image super-resolution proves the superiority of the proposed MCRAN.},
  archive      = {J_APIN},
  author       = {Liu, Zhiwei and Huang, Ji and Zhu, Chengjia and Peng, Xiaoyu and Du, Xinyu},
  doi          = {10.1007/s10489-020-01723-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {85-99},
  shortjournal = {Appl. Intell.},
  title        = {Residual attention network using multi-channel dense connections for image super-resolution},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new irregular cellular learning automata-based
evolutionary computation for time series link prediction in social
networks. <em>APIN</em>, <em>51</em>(1), 71–84. (<a
href="https://doi.org/10.1007/s10489-020-01685-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction (LP), as an attempt to predict event-based future connections within a network, is the main task of social network analysis (SNA). Accordingly, common LP approaches to forecast future connections utilize similarity metrics of non-connected links in a static network representation. A general shortcoming of most existing research studies in this field is that they tap the present condition of a system and fail to take any temporal events into account. Moreover; social networks are innately evolutionary since they are assumed to be online, non-deterministic, and unforeseeable in most applications. Consequently, it is not appropriate to employ deterministic models for examining actual social network problems. With regard to time-series LP (TSLP) problems, temporal evolution of connection incidence is correspondingly exploited to predict connection chances at a particular time. In this paper, a new TSLP method based on irregular cellular learning automaton (ICLA) and evolutionary computation (EC) is proposed. In the evolutionary procedure suggested here, each vertex (i.e. cell) includes a genome as well as a set of learning automata (LAs). Accordingly, the genome residing in a cell represents predicted links for the corresponding cell. Local information among cells in successive time 1 to T in the network is then analyzed to predict future connections in time T + 1. According to the distributed feature of the recommended approach, each genome is locally developed by a local search. The experiments in this study via e-mail and co-authorship networks ultimately show that the proposed algorithm leads to remarkable outcomes in predicting future connections.},
  archive      = {J_APIN},
  author       = {Khaksar Manshad, Mozhdeh and Meybodi, Mohammad Reza and Salajegheh, Afshin},
  doi          = {10.1007/s10489-020-01685-5},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {71-84},
  shortjournal = {Appl. Intell.},
  title        = {A new irregular cellular learning automata-based evolutionary computation for time series link prediction in social networks},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sentiment analysis: Dynamic and temporal clustering of
product reviews. <em>APIN</em>, <em>51</em>(1), 51–70. (<a
href="https://doi.org/10.1007/s10489-020-01668-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased availability of online reviews requires a relevant solution to draw chronological insights from review streams. This paper introduces temporal sentiment analysis by adopting the automatic contextual analysis and ensemble clustering (ACAEC) algorithm. ACAEC is a clustering algorithm which utilizes contextual analysis and a clustering ensemble learning. We propose chronological sentiment analysis using window sequential clustering (WSC) and segregated window clustering (SWC). WSC is a dynamic analysis, whereas SWC is solely based on the temporal characteristic of reviews. ACAEC is the base learning algorithm of WSC and SWC. ACAEC’s ensemble approach is enhanced using an additional weight scheme and an additional learner to improve WSC’s outcome. To understand the produced sentiment pattern, an unsupervised review selection is introduced which is based on review polarity. We also introduce consistency, a free-label measure to assess the algorithm’s performance. For this study, new sets of reviews are introduced, these being four airlines and an Australian property agent. In terms of accuracy and stability, the proposed methods are effective in processing a review series. Experiments show that the average accuracy rates of SWC and WSC reach 87.54% and 83.87%, respectively. In addition, it is robust against the so-called imbalanced windows problem. The suggested solutions are unsupervised i. e. domain-independent and suitable for the analysis of a large review series.},
  archive      = {J_APIN},
  author       = {AL-Sharuee, Murtadha Talib and Liu, Fei and Pratama, Mahardhika},
  doi          = {10.1007/s10489-020-01668-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {51-70},
  shortjournal = {Appl. Intell.},
  title        = {Sentiment analysis: Dynamic and temporal clustering of product reviews},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonlinear system modeling and application based on
restricted boltzmann machine and improved BP neural network.
<em>APIN</em>, <em>51</em>(1), 37–50. (<a
href="https://doi.org/10.1007/s10489-019-01614-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the complexity, nonlinearity and difficulty in modeling of nonlinear system. In this paper, an improved back-propagation(BP) neural network based on restricted boltzmann machine(RBM-IBPNN) is proposed for nonlinear systems modeling. First, the structure of BP neural network(BPNN) is optimized by using sensitivity analysis(SA) and mutual information(MI) of the hidden neurons. Namely when the SA value and the MI value of the hidden neurons satisfy the set standard, the corresponding neurons will be pruned, split or merged. second, the restricted boltzmann machine(RBM) is employed to perform parameters initialization of training on the IBPNN. Finally, the proposed RBM-IBPNN is evaluated on nonlinear system identification, lorenz chaotic time series prediction and the total phosphorus prediction problems. The experimental results demonstrate that the proposed RBM-IBPNN not only has faster convergence speed and higher prediction accuracy, but also realizes a more compact network structure.},
  archive      = {J_APIN},
  author       = {Qiao, Junfei and Wang, Longyang},
  doi          = {10.1007/s10489-019-01614-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {37-50},
  shortjournal = {Appl. Intell.},
  title        = {Nonlinear system modeling and application based on restricted boltzmann machine and improved BP neural network},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A synthesis approach of fast robust MPC with RBF-ARX model
to nonlinear system with uncertain steady status information.
<em>APIN</em>, <em>51</em>(1), 19–36. (<a
href="https://doi.org/10.1007/s10489-019-01555-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mechanical model of a plant in real industry is usually difficult to obtain. This paper integrates the data-driven RBF-ARX modeling method and a fast Robust Model Predictive Control (RMPC) approach to achieving output-tracking control of a nonlinear system with unknown steady status information. Considering the large online computational burden of online RMPC, this paper proposes a RBF-ARX model-based efficient robust predictive control (RBF-ARX-ERPC) approach. First, based on the RBF-ARX model, a polytopic uncertain linear parameter varying (LPV) state-space model is built to represent the dynamic behavior of the system; next, two convex polytopic sets are constructed to wrap the globally nonlinear behavior of the system. Then, an optimization problem including several linear matrix inequalities (LMIs) is formulated, which is solved offline to synthesize a sequence of explicit control laws corresponding to a sequence of asymptotically stable invariant ellipsoids in the state space, of which all the optimization results are stored in a look-up table. For the real-time control online, it only involves simple state-vector computation and bisection search. Two simulation examples, i.e. the modeling and control of a widely used continuously stirred tank reactor (CSTR) and a linear one-stage inverted pendulum (LOSIP) system, and the real-time control experiments on an actual LOSIP plant are provided to demonstrate the effectiveness of the proposed RBF-ARX model-based efficient RPC approach.},
  archive      = {J_APIN},
  author       = {Tian, Xiaoying and Peng, Hui and Zhou, Feng and Peng, Xiaoyan},
  doi          = {10.1007/s10489-019-01555-9},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {19-36},
  shortjournal = {Appl. Intell.},
  title        = {A synthesis approach of fast robust MPC with RBF-ARX model to nonlinear system with uncertain steady status information},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying twins based on ocular region features using deep
representations. <em>APIN</em>, <em>51</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10489-019-01562-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the twins identification problem; a challenging task in biometric authentication. The experiments are carried out using deep learning techniques and for that we have proposed a Convolutional Siamese Network (CNN-Siamese) and a multiscale Convolutional Siamese network (MCNN-Siamese). We have also explored and evaluated three pre-trained CNNs namely, ResNet-50, VGG-16 and NASNet-Large, and utilized them in Siamese network after appropriate modifications. In addition, a simple 5-layer neural network (sNN) is also utilised as Siamese subnetwork in the experiments. We have presented compelling experimental evidences in terms of correct classification accuracy (CCR) on CASIA-IrisV4 Twins’ dataset that manifest the effectiveness of ocular biometrics in identifying twins. The results achieve the existing state-of-the-art human level accuracy. Also, the proximity of CCRs of all the models asserts that the ocular regions in twins hold a significant correlation to label them as twins. We have also quantified the cross-domain capability of the proposed subnetworks (i.e. CNN and MCNN) on ND-GFI dataset that outperforms the state-of-the-art methods. Notably, for the ocular biometrics, to the best of our knowledge, there is currently no literature available as of date that explores the association between twins and subsequently unriddles the classification using Deep Learning.},
  archive      = {J_APIN},
  author       = {Gautam, Gunjan and Raj, Aditya and Mukhopadhyay, Susanta},
  doi          = {10.1007/s10489-019-01562-w},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Identifying twins based on ocular region features using deep representations},
  volume       = {51},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
