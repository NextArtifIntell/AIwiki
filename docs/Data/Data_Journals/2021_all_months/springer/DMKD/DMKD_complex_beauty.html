<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---82">DMKD - 82</h2>
<ul>
<li><details>
<summary>
(2021). Unsupervised domain adaptation with non-stochastic missing
data. <em>DMKD</em>, <em>35</em>(6), 2714–2755. (<a
href="https://doi.org/10.1007/s10618-021-00775-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider unsupervised domain adaptation (UDA) for classification problems in the presence of missing data in the unlabelled target domain. More precisely, motivated by practical applications, we analyze situations where distribution shift exists between domains and where some components are systematically absent on the target domain without available supervision for imputing the missing target components. We propose a generative approach for imputation. Imputation is performed in a domain-invariant latent space and leverages indirect supervision from a complete source domain. We introduce a single model performing joint adaptation, imputation and classification which, under our assumptions, minimizes an upper bound of its target generalization error and performs well under various representative divergence families ( $$\mathscr {H}$$ -divergence, Optimal Transport). Moreover, we compare the target error of our adaptation-imputation framework and the “ideal” target error of a UDA classifier without missing target components. Our model is further improved with self-training, to bring the learned source and target class posterior distributions closer. We perform experiments on three families of datasets of different modalities: a classical digit classification benchmark, the Amazon product reviews dataset both commonly used in UDA and real-world digital advertising datasets. We show the benefits of jointly performing adaptation, classification and imputation on these datasets.},
  archive      = {J_DMKD},
  author       = {Kirchmeyer, Matthieu and Gallinari, Patrick and Rakotomamonjy, Alain and Mantrach, Amin},
  doi          = {10.1007/s10618-021-00775-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2714-2755},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Unsupervised domain adaptation with non-stochastic missing data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VFC-SMOTE: Very fast continuous synthetic minority
oversampling for evolving data streams. <em>DMKD</em>, <em>35</em>(6),
2679–2713. (<a
href="https://doi.org/10.1007/s10618-021-00786-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is constantly changing, and so are the massive amount of data produced. However, only a few studies deal with online class imbalance learning that combines the challenges of class-imbalanced data streams and concept drift. In this paper, we propose the very fast continuous synthetic minority oversampling technique (VFC-SMOTE). It is a novel meta-strategy to be prepended to any streaming machine learning classification algorithm aiming at oversampling the minority class using a new version of Smote and Borderline-Smote inspired by Data Sketching. We benchmarked VFC-SMOTE pipelines on synthetic and real data streams containing different concept drifts, imbalance levels, and class distributions. We bring statistical evidence that VFC-SMOTE pipelines learn models whose minority class performances are better than state-of-the-art. Moreover, we analyze the time/memory consumption and the concept drift recovery speed.},
  archive      = {J_DMKD},
  author       = {Bernardo, Alessio and Della Valle, Emanuele},
  doi          = {10.1007/s10618-021-00786-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2679-2713},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {VFC-SMOTE: Very fast continuous synthetic minority oversampling for evolving data streams},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CURIE: A cellular automaton for concept drift detection.
<em>DMKD</em>, <em>35</em>(6), 2655–2678. (<a
href="https://doi.org/10.1007/s10618-021-00776-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream mining extracts information from large quantities of data flowing fast and continuously (data streams). They are usually affected by changes in the data distribution, giving rise to a phenomenon referred to as concept drift. Thus, learning models must detect and adapt to such changes, so as to exhibit a good predictive performance after a drift has occurred. In this regard, the development of effective drift detection algorithms becomes a key factor in data stream mining. In this work we propose $$\textit{CURIE}$$ , a drift detector relying on cellular automata. Specifically, in $$\textit{CURIE}$$ the distribution of the data stream is represented in the grid of a cellular automata, whose neighborhood rule can then be utilized to detect possible distribution changes over the stream. Computer simulations are presented and discussed to show that $$\textit{CURIE}$$ , when hybridized with other base learners, renders a competitive behavior in terms of detection metrics and classification accuracy. $$\textit{CURIE}$$ is compared with well-established drift detectors over synthetic datasets with varying drift characteristics.},
  archive      = {J_DMKD},
  author       = {Lobo, Jesus L. and Del Ser, Javier and Osaba, Eneko and Bifet, Albert and Herrera, Francisco},
  doi          = {10.1007/s10618-021-00776-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2655-2678},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {CURIE: A cellular automaton for concept drift detection},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MultiETSC: Automated machine learning for early time series
classification. <em>DMKD</em>, <em>35</em>(6), 2602–2654. (<a
href="https://doi.org/10.1007/s10618-021-00781-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, “naïve” fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36).},
  archive      = {J_DMKD},
  author       = {Ottervanger, Gilles and Baratchi, Mitra and Hoos, Holger H.},
  doi          = {10.1007/s10618-021-00781-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2602-2654},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MultiETSC: Automated machine learning for early time series classification},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Early abandoning and pruning for elastic distances including
dynamic time warping. <em>DMKD</em>, <em>35</em>(6), 2577–2601. (<a
href="https://doi.org/10.1007/s10618-021-00782-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest neighbor search under elastic distances is a key tool for time series analysis, supporting many applications. However, straightforward implementations of distances require $$O(n^2)$$ space and time complexities, preventing these applications from scaling to long series. Much work has been devoted to speeding up the NN search process, mostly with the development of lower bounds, allowing to avoid costly distance computations when a given threshold is exceeded. This threshold, provided by the similarity search process, also allows to early abandon the computation of a distance itself. Another approach, is to prune parts of the computation. All these techniques are orthogonal to each other. In this work, we develop a new generic strategy, “EAPruned”, that tightly integrates pruning with early abandoning. We apply it to six elastic distance measures: DTW, CDTW, WDTW, ERP, MSM and TWE, showing substantial speedup in NN search applications. Pruning alone also shows substantial speedup for some distances, benefiting applications beyond the scope of NN search (e.g. requiring all pairwise distances), and hence where early abandoning is not applicable. We release our implementation as part of a new C++ library for time series classification, along with easy to use Python/Numpy bindings.},
  archive      = {J_DMKD},
  author       = {Herrmann, Matthieu and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-021-00782-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2577-2601},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Early abandoning and pruning for elastic distances including dynamic time warping},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BROCCOLI: Overlapping and outlier-robust biclustering
through proximal stochastic gradient descent. <em>DMKD</em>,
<em>35</em>(6), 2542–2576. (<a
href="https://doi.org/10.1007/s10618-021-00787-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix tri-factorization subject to binary constraints is a versatile and powerful framework for the simultaneous clustering of observations and features, also known as biclustering. Applications for biclustering encompass the clustering of high-dimensional data and explorative data mining, where the selection of the most important features is relevant. Unfortunately, due to the lack of suitable methods for the optimization subject to binary constraints, the powerful framework of biclustering is typically constrained to clusterings which partition the set of observations or features. As a result, overlap between clusters cannot be modelled and every item, even outliers in the data, have to be assigned to exactly one cluster. In this paper we propose Broccoli, an optimization scheme for matrix factorization subject to binary constraints, which is based on the theoretically well-founded optimization scheme of proximal stochastic gradient descent. Thereby, we do not impose any restrictions on the obtained clusters. Our experimental evaluation, performed on both synthetic and real-world data, and against 6 competitor algorithms, show reliable and competitive performance, even in presence of a high amount of noise in the data. Moreover, a qualitative analysis of the identified clusters shows that Broccoli may provide meaningful and interpretable clustering structures.},
  archive      = {J_DMKD},
  author       = {Hess, Sibylle and Pio, Gianvito and Hochstenbach, Michiel and Ceci, Michelangelo},
  doi          = {10.1007/s10618-021-00787-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2542-2576},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {BROCCOLI: Overlapping and outlier-robust biclustering through proximal stochastic gradient descent},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the special issue of the ECML PKDD 2021
journal track. <em>DMKD</em>, <em>35</em>(6), 2540–2541. (<a
href="https://doi.org/10.1007/s10618-021-00792-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Appice, Annalisa and Escalera, Sergio and Gámez, Jose A. and Trautmann, Heike},
  doi          = {10.1007/s10618-021-00792-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2540-2541},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Introduction to the special issue of the ECML PKDD 2021 journal track},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Characterizing attitudinal network graphs through
frustration cloud. <em>DMKD</em>, <em>35</em>(6), 2498–2539. (<a
href="https://doi.org/10.1007/s10618-021-00795-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attitudinal network graphs are signed graphs where edges capture an expressed opinion; two vertices connected by an edge can be agreeable (positive) or antagonistic (negative). A signed graph is called balanced if each of its cycles includes an even number of negative edges. Balance is often characterized by the frustration index or by finding a single convergent balanced state of network consensus. In this paper, we propose to expand the measures of consensus from a single balanced state associated with the frustration index to the set of nearest balanced states. We introduce the frustration cloud as a set of all nearest balanced states and use a graph-balancing algorithm to find all nearest balanced states in a deterministic way. Computational concerns are addressed by measuring consensus probabilistically, and we introduce new vertex and edge metrics to quantify status, agreement, and influence. We also introduce a new global measure of controversy for a given signed graph and show that vertex status is a zero-sum game in the signed network. We propose an efficient scalable algorithm for calculating frustration cloud-based measures in social network and survey data of up to 80,000 vertices and half-a-million edges. We also demonstrate the power of the proposed approach to provide discriminant features for community discovery when compared to spectral clustering and to automatically identify dominant vertices and anomalous decisions in the network.},
  archive      = {J_DMKD},
  author       = {Rusnak, Lucas and Tešić, Jelena},
  doi          = {10.1007/s10618-021-00795-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2498-2539},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Characterizing attitudinal network graphs through frustration cloud},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous treatment effect estimation via generative
adversarial de-confounding. <em>DMKD</em>, <em>35</em>(6), 2467–2497.
(<a href="https://doi.org/10.1007/s10618-021-00797-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental problem in causal inference is the treatment effect estimation in observational studies, and its key challenge is to handle the confounding bias induced by the associations between covariates and treatment variable. In this paper, we study the problem of effect estimation on continuous treatment from observational data, going beyond previous work on binary treatments. Previous work on binary treatment focuses on de-confounding by balancing the distribution of covariates between the treated and control groups with either propensity score or confounder balancing techniques. In the continuous setting, those methods would fail as we can hardly evaluate the distribution of covariates under each treatment status. To tackle the case of continuous treatments, we propose a novel Generative Adversarial De-confounding (GAD) algorithm to eliminate the associations between covariates and treatment variable with two main steps: (1) generating an “calibration” distribution without associations between covariates and treatment by randomly perturbation on treatment variable; (2) learning sample weights that transfer the distribution of observed data to the “calibration” distribution for de-confounding with a Generative Adversarial Network. We show, both theoretically and with empirical experiments, that our GAD algorithm can remove the associations between covariates and treatment, hence, precisely estimating the causal effect of continuous treatment. Extensive experiments on both synthetic and real-world datasets demonstrate that our algorithm outperforms the state-of-the-art methods for effect estimation of continuous treatment with observational data.},
  archive      = {J_DMKD},
  author       = {Kuang, Kun and Li, Yunzhe and Li, Bo and Cui, Peng and Yang, Hongxia and Tao, Jianrong and Wu, Fei},
  doi          = {10.1007/s10618-021-00797-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2467-2497},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Continuous treatment effect estimation via generative adversarial de-confounding},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chebyshev approaches for imbalanced data streams regression
models. <em>DMKD</em>, <em>35</em>(6), 2389–2466. (<a
href="https://doi.org/10.1007/s10618-021-00793-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years data stream mining and learning from imbalanced data have been active research areas. Even though solutions exist to tackle these two problems, most of them are not designed to handle challenges inherited from both problems. As far as we are aware, the few approaches in the area of learning from imbalanced data streams fall in the context of classification, and no efforts on the regression domain have been reported yet. This paper proposes a technique that uses sampling strategies to cope with imbalanced data streams in a regression setting, where the most important cases have rare and extreme target values. Specifically, we employ under-sampling and over-sampling strategies that resort to Chebyshev’s inequality value as a heuristic to disclose the type of incoming cases (i.e. frequent or rare). We have evaluated our proposal by applying it in the training of models by four well-known regression algorithms over fourteen benchmark data sets. We conducted a series of experiments with different setups on both synthetic and real-world data sets. The experimental results confirm our approach’s effectiveness by showing the models’ superior performance trained by each of the sampling strategies compared with their baseline pairs.},
  archive      = {J_DMKD},
  author       = {Aminian, Ehsan and Ribeiro, Rita P. and Gama, João},
  doi          = {10.1007/s10618-021-00793-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2389-2466},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Chebyshev approaches for imbalanced data streams regression models},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series clustering in linear time complexity.
<em>DMKD</em>, <em>35</em>(6), 2369–2388. (<a
href="https://doi.org/10.1007/s10618-021-00798-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing power of data storage and advances in data generation and collection technologies, large volumes of time series data become available and the content is changing rapidly. This requires data mining methods to have low time complexity to handle the huge and fast-changing data. This article presents a novel time series clustering algorithm that has linear time complexity. The proposed algorithm partitions the data by checking some randomly selected symbolic patterns in the time series. We provide theoretical analysis to show that group structures in the data can be revealed from this process. We evaluate the proposed algorithm extensively on all 128 datasets from the well-known UCR time series archive, and compare with the state-of-the-art approaches with statistical analysis. The results show that the proposed method achieves better accuracy compared with other rival methods. We also conduct experiments to explore how the parameters and configuration of the algorithm can affect the final clustering results.},
  archive      = {J_DMKD},
  author       = {Li, Xiaosheng and Lin, Jessica and Zhao, Liang},
  doi          = {10.1007/s10618-021-00798-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2369-2388},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Time series clustering in linear time complexity},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lagrangian-based score for assessing the quality of
pairwise constraints in semi-supervised clustering. <em>DMKD</em>,
<em>35</em>(6), 2341–2368. (<a
href="https://doi.org/10.1007/s10618-021-00794-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms help identify homogeneous subgroups from data. In some cases, additional information about the relationship among some subsets of the data exists. When using a semi-supervised clustering algorithm, an expert may provide additional information to constrain the solution based on that knowledge and, in doing so, guide the algorithm to a more useful and meaningful solution. Such additional information often takes the form of a cannot-link constraint (i.e., two data points cannot be part of the same cluster) or a must-link constraint (i.e., two data points must be part of the same cluster). A key challenge for users of such constraints in semi-supervised learning algorithms, however, is that the addition of inaccurate or conflicting constraints can decrease accuracy and little is known about how to detect whether expert-imposed constraints are likely incorrect. In the present work, we introduce a method to score each must-link and cannot-link pairwise constraint as likely incorrect. Using synthetic experimental examples and real data, we show that the resulting impact score can successfully identify individual constraints that should be removed or revised.},
  archive      = {J_DMKD},
  author       = {Randel, Rodrigo and Aloise, Daniel and Blanchard, Simon J. and Hertz, Alain},
  doi          = {10.1007/s10618-021-00794-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2341-2368},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A lagrangian-based score for assessing the quality of pairwise constraints in semi-supervised clustering},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicit consensus clustering from multiple graphs.
<em>DMKD</em>, <em>35</em>(6), 2313–2340. (<a
href="https://doi.org/10.1007/s10618-021-00788-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with relational learning generally relies on tools modeling relational data. An undirected graph can represent these data with vertices depicting entities and edges describing the relationships between the entities. These relationships can be well represented by multiple undirected graphs over the same set of vertices with edges arising from different graphs catching heterogeneous relations. The vertices of those networks are often structured in unknown clusters with varying properties of connectivity. These multiple graphs can be structured as a three-way tensor, where each slice of tensor depicts a graph which is represented by a count data matrix. To extract relevant clusters, we propose an appropriate model-based co-clustering capable of dealing with multiple graphs. The proposed model can be seen as a suitable tensor extension of mixture models of graphs, while the obtained co-clustering can be treated as a consensus clustering of nodes from multiple graphs. Applications on real datasets and comparisons with multi-view clustering and tensor decomposition methods show the interest of our contribution.},
  archive      = {J_DMKD},
  author       = {Boutalbi, Rafika and Labiod, Lazhar and Nadif, Mohamed},
  doi          = {10.1007/s10618-021-00788-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2313-2340},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Implicit consensus clustering from multiple graphs},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Isolation kernel: The x factor in efficient and effective
large scale online kernel learning. <em>DMKD</em>, <em>35</em>(6),
2282–2312. (<a
href="https://doi.org/10.1007/s10618-021-00785-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale online kernel learning aims to build an efficient and scalable kernel-based predictive model incrementally from a sequence of potentially infinite data points. Current state-of-the-art large scale online kernel learning focuses on improving efficiency. Two key approaches to gain efficiency through approximation are (1) limiting the number of support vectors, and (2) using an approximate feature map. They often employ a kernel with a feature map with intractable dimensionality. While these approaches can deal with large scale datasets efficiently, this outcome is achieved by compromising predictive accuracy because of the approximation. We offer an alternative approach that puts the kernel used at the heart of the approach. It focuses on creating a sparse and finite-dimensional feature map of a kernel called Isolation Kernel. Using this new approach, to achieve the above aim of large scale online kernel learning becomes extremely simple—simply use Isolation Kernel instead of a kernel having a feature map with intractable dimensionality. We show that, using Isolation Kernel, large scale online kernel learning can be achieved efficiently without sacrificing accuracy.},
  archive      = {J_DMKD},
  author       = {Ting, Kai Ming and Wells, Jonathan R. and Washio, Takashi},
  doi          = {10.1007/s10618-021-00785-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2282-2312},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Isolation kernel: The x factor in efficient and effective large scale online kernel learning},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social media as author-audience games. <em>DMKD</em>,
<em>35</em>(6), 2251–2281. (<a
href="https://doi.org/10.1007/s10618-021-00783-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach for the prediction of user authorship and feedback behavior with shared content. We consider that users use models of other users and their feedback to choose what to publish next. We look at the problem as a game between authors and audiences and relate it to current content-based user modeling solutions with no prior strategic models. As applications, we consider the large-scale authorship of Wikipedia pages, movies and food recipes. We demonstrate analytic properties, authorship and feedback prediction results, and an overall framework to study content authorship regularities in social media.},
  archive      = {J_DMKD},
  author       = {Ribeiro, Andre F.},
  doi          = {10.1007/s10618-021-00783-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2251-2281},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Social media as author-audience games},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting house price predictions using geo-spatial network
embedding. <em>DMKD</em>, <em>35</em>(6), 2221–2250. (<a
href="https://doi.org/10.1007/s10618-021-00789-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real estate contributes significantly to all major economies around the world. In particular, house prices have a direct impact on stakeholders, ranging from house buyers to financing companies. Thus, a plethora of techniques have been developed for real estate price prediction. Most of the existing techniques rely on different house features to build a variety of prediction models to predict house prices. Perceiving the effect of spatial dependence on house prices, some later works focused on introducing spatial regression models for improving prediction performance. However, they fail to take into account the geo-spatial context of the neighborhood amenities such as how close a house is to a train station, or a highly-ranked school, or a shopping center. Such contextual information may play a vital role in users’ interests in a house and thereby has a direct influence on its price. In this paper, we propose to leverage the concept of graph neural networks to capture the geo-spatial context of the neighborhood of a house. In particular, we present a novel method, the geo-spatial network embedding (GSNE), that learns the embeddings of houses and various types of points of interest (POIs) in the form of multipartite networks, where the houses and the POIs are represented as attributed nodes and the relationships between them as edges. Extensive experiments with a large number of regression techniques show that the embeddings produced by our proposed GSNE technique consistently and significantly improve the performance of the house price prediction task regardless of the downstream regression model. Relevant source code for GSNE is available at: https://github.com/sarathismg/gsne .},
  archive      = {J_DMKD},
  author       = {Das, Sarkar Snigdha Sarathi and Ali, Mohammed Eunus and Li, Yuan-Fang and Kang, Yong-Bin and Sellis, Timos},
  doi          = {10.1007/s10618-021-00789-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {2221-2250},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Boosting house price predictions using geo-spatial network embedding},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). K-plex cover pooling for graph neural networks.
<em>DMKD</em>, <em>35</em>(5), 2200–2220. (<a
href="https://doi.org/10.1007/s10618-021-00779-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph pooling methods provide mechanisms for structure reduction that are intended to ease the diffusion of context between nodes further in the graph, and that typically leverage community discovery mechanisms or node and edge pruning heuristics. In this paper, we introduce a novel pooling technique which borrows from classical results in graph theory that is non-parametric and generalizes well to graphs of different nature and connectivity patterns. Our pooling method, named KPlexPool, builds on the concepts of graph covers and k-plexes, i.e. pseudo-cliques where each node can miss up to k links. The experimental evaluation on benchmarks on molecular and social graph classification shows that KPlexPool achieves state of the art performances against both parametric and non-parametric pooling methods in the literature, despite generating pooled graphs based solely on topological information.},
  archive      = {J_DMKD},
  author       = {Bacciu, Davide and Conte, Alessio and Grossi, Roberto and Landolfi, Francesco and Marino, Andrea},
  doi          = {10.1007/s10618-021-00779-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2200-2220},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {K-plex cover pooling for graph neural networks},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Link prediction in dynamic networks using random dot product
graphs. <em>DMKD</em>, <em>35</em>(5), 2168–2199. (<a
href="https://doi.org/10.1007/s10618-021-00784-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of predicting links in large networks is an important task in a variety of practical applications, including social sciences, biology and computer security. In this paper, statistical techniques for link prediction based on the popular random dot product graph model are carefully presented, analysed and extended to dynamic settings. Motivated by a practical application in cyber-security, this paper demonstrates that random dot product graphs not only represent a powerful tool for inferring differences between multiple networks, but are also efficient for prediction purposes and for understanding the temporal evolution of the network. The probabilities of links are obtained by fusing information at two stages: spectral methods provide estimates of latent positions for each node, and time series models are used to capture temporal dynamics. In this way, traditional link prediction methods, usually based on decompositions of the entire network adjacency matrix, are extended using temporal information. The methods presented in this article are applied to a number of simulated and real-world graphs, showing promising results.},
  archive      = {J_DMKD},
  author       = {Sanna Passino, Francesco and Bertiger, Anna S. and Neil, Joshua C. and Heard, Nicholas A.},
  doi          = {10.1007/s10618-021-00784-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2168-2199},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Link prediction in dynamic networks using random dot product graphs},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selego: Robust variate selection for accurate time series
forecasting. <em>DMKD</em>, <em>35</em>(5), 2141–2167. (<a
href="https://doi.org/10.1007/s10618-021-00777-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naïve extensions of uni-variate prediction techniques lead to an unwelcome increase in the cost of multi-variate model learning and significant deteriorations in the model performance. In this paper, we first argue that (a) one can learn a more accurate forecasting model by leveraging temporal alignments among variates to quantify the importance of the recorded variates with respect to a target variate. We further argue that, (b) for this purpose we need to quantify temporal correlation, not in terms of series similarity, but in terms of temporal alignments of key “events” impacting these series. Finally, we argue that (c) while learning a temporal model using recurrence based techniques (such as RNN and LSTM—even when leveraging attention strategies) is difficult and costly, we can achieve better performance by coupling simpler CNNs with an adaptive variate selection strategy. Relying on these arguments, we propose a Selego framework (Selego is a word of latin origin meaning “selection”) for variate selection and experimentally evaluate the performance of the proposed approach on various forecasting models, such as LSTM, RNN, and CNN, for different top-X% variates and different forecasting time in the future (lead) on multiple real-world datasets. Experiments show that the proposed framework can offer significant ( $$90-98\%$$ ) drops in the number of recorded variates that are needed to train predictive models, while simultaneously boosting accuracy.},
  archive      = {J_DMKD},
  author       = {Tiwaskar, Manoj and Garg, Yash and Li, Xinsheng and Candan, K. Selçuk and Sapino, Maria Luisa},
  doi          = {10.1007/s10618-021-00777-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2141-2167},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Selego: Robust variate selection for accurate time series forecasting},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention based adversarially regularized learning for
network embedding. <em>DMKD</em>, <em>35</em>(5), 2112–2140. (<a
href="https://doi.org/10.1007/s10618-021-00780-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding, also known as graph embedding and network representation learning, is an effective method for representing graphs or network data in a low-dimensional space. Most existing methods focus on preserving network topology and minimizing the reconstruction errors to learn a low-dimensional embedding vector representation of the network. In addition, some researchers are devoted to the embedding learning of attribute networks. These researchers usually study the two matrices of network structure and network attributes separately, and then merge them to realize the embedding learning representation of attribute networks. These studies have different performances on a variety of downstream tasks. However, most of these methods have two problems: first, these methods mostly use shallow model to learn structure or attribute embedding, which do not make full use of the rich information contained in the network, such as the neighborhood information of nodes; second, the distribution of the learned network low-dimensional vector representation is overlooked, which leads to poor generalization ability of the model in some real-world network data. Therefore, this paper proposes an adversarially regularized network representation learning model based on attention mechanism, which encodes the topology features and content information of the network into a low-dimensional embedding vector representation through a graph attention autoencoder. Meanwhile, through an adversarial training schema, the learned low-dimensional vector representation could circumvent the requirement of an explicit prior distribution, and thus obtain better generalization ability. Extensive experiments on tasks of link prediction and node clustering demonstrate the effectiveness of learned network embeddings.},
  archive      = {J_DMKD},
  author       = {He, Jieyue and Wang, Jinmeng and Yu, Zhizhou},
  doi          = {10.1007/s10618-021-00780-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2112-2140},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Attention based adversarially regularized learning for network embedding},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure learning for relational logistic regression: An
ensemble approach. <em>DMKD</em>, <em>35</em>(5), 2089–2111. (<a
href="https://doi.org/10.1007/s10618-021-00770-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning Relational Logistic Regression (RLR). Unlike standard logistic regression, the features of RLR are first-order formulae with associated weight vectors instead of scalar weights. We turn the problem of learning RLR to learning these vector-weighted formulae and develop a learning algorithm based on the recently successful functional-gradient boosting methods for probabilistic logic models. We derive the functional gradients and show how weights can be learned simultaneously in an efficient manner. Our empirical evaluation on standard data sets demonstrates the superiority of our approach over other methods for learning RLR.},
  archive      = {J_DMKD},
  author       = {Ramanan, Nandini and Kunapuli, Gautam and Khot, Tushar and Fatemi, Bahare and Kazemi, Seyed Mehran and Poole, David and Kersting, Kristian and Natarajan, Sriraam},
  doi          = {10.1007/s10618-021-00770-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2089-2111},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Structure learning for relational logistic regression: An ensemble approach},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentially private distance learning in categorical
data. <em>DMKD</em>, <em>35</em>(5), 2050–2088. (<a
href="https://doi.org/10.1007/s10618-021-00778-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most privacy-preserving machine learning methods are designed around continuous or numeric data, but categorical attributes are common in many application scenarios, including clinical and health records, census and survey data. Distance-based methods, in particular, have limited applicability to categorical data, since they do not capture the complexity of the relationships among different values of a categorical attribute. Although distance learning algorithms exist for categorical data, they may disclose private information about individual records if applied to a secret dataset. To address this problem, we introduce a differentially private family of algorithms for learning distances between any pair of values of a categorical attribute according to the way they are co-distributed with the values of other categorical attributes forming the so-called context. We define different variants of our algorithm and we show empirically that our approach consumes little privacy budget while providing accurate distances, making it suitable in distance-based applications, such as clustering and classification.},
  archive      = {J_DMKD},
  author       = {Battaglia, Elena and Celano, Simone and Pensa, Ruggero G.},
  doi          = {10.1007/s10618-021-00778-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2050-2088},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Differentially private distance learning in categorical data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven detection of counterpressing in professional
football. <em>DMKD</em>, <em>35</em>(5), 2009–2049. (<a
href="https://doi.org/10.1007/s10618-021-00763-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting counterpressing is an important task for any professional match-analyst in football (soccer), but is being done exclusively manually by observing video footage. The purpose of this paper is not only to automatically identify this strategy, but also to derive metrics that support coaches with the analysis of transition situations. Additionally, we want to infer objective influence factors for its success and assess the validity of peer-created rules of thumb established in by practitioners. Based on a combination of positional and event data we detect counterpressing situations as a supervised machine learning task. Together, with professional match-analysis experts we discussed and consolidated a consistent definition, extracted 134 features and manually labeled more than 20, 000 defensive transition situations from 97 professional football matches. The extreme gradient boosting model—with an area under the curve of $$87.4\%$$ on the labeled test data—enabled us to judge how quickly teams can win the ball back with counterpressing strategies, how many shots they create or allow immediately afterwards and to determine what the most important success drivers are. We applied this automatic detection on all matches from six full seasons of the German Bundesliga and quantified the defensive and offensive consequences when applying counterpressing for each team. Automating the task saves analysts a tremendous amount of time, standardizes the otherwise subjective task, and allows to identify trends within larger data-sets. We present an effective way of how the detection and the lessons learned from this investigation are integrated effectively into common match-analysis processes.},
  archive      = {J_DMKD},
  author       = {Bauer, Pascal and Anzer, Gabriel},
  doi          = {10.1007/s10618-021-00763-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {2009-2049},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Data-driven detection of counterpressing in professional football},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An alternating nonmonotone projected barzilai–borwein
algorithm of nonnegative factorization of big matrices. <em>DMKD</em>,
<em>35</em>(5), 1972–2008. (<a
href="https://doi.org/10.1007/s10618-021-00773-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new alternating nonmonotone projected Barzilai–Borwein (BB) algorithm is developed for solving large scale problems of nonnegative matrix factorization. Unlike the existing algorithms available in the literature, a nonmonotone line search strategy is proposed to find suitable step lengths, and an adaptive BB spectral parameter is employed to generate search directions such that the constructed subproblems are efficiently solved. Apart from establishment of global convergence for this algorithm, numerical tests on three synthetic datasets, four public face image datasets and a real-world transcriptomic dataset are conducted to show advantages of the developed algorithm in this paper. It is concluded that in terms of numerical efficiency, noise robustness and quality of matrix factorization, our algorithm is promising and applicable to face image reconstruction, and deep mining of transcriptomic profiles of the sub-genomes in hybrid fish lineage, compared with the state-of-the-art algorithms.},
  archive      = {J_DMKD},
  author       = {Li, Ting and Tang, Jiayi and Wan, Zhong},
  doi          = {10.1007/s10618-021-00773-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1972-2008},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An alternating nonmonotone projected Barzilai–Borwein algorithm of nonnegative factorization of big matrices},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TSK-streams: Learning TSK fuzzy systems for regression on
data streams. <em>DMKD</em>, <em>35</em>(5), 1941–1971. (<a
href="https://doi.org/10.1007/s10618-021-00769-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of adaptive learning from evolving and possibly non-stationary data streams has attracted a lot of interest in machine learning in the recent past, and also stimulated research in related fields, such as computational intelligence and fuzzy systems. In particular, several rule-based methods for the incremental induction of regression models have been proposed. In this paper, we develop a method that combines the strengths of two existing approaches rooted in different learning paradigms. More concretely, our method adopts basic principles of the state-of-the-art learning algorithm AMRules and enriches them by the representational advantages of fuzzy rules. In a comprehensive experimental study, TSK-Streams is shown to be highly competitive in terms of performance.},
  archive      = {J_DMKD},
  author       = {Shaker, Ammar and Hüllermeier, Eyke},
  doi          = {10.1007/s10618-021-00769-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1941-1971},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TSK-streams: Learning TSK fuzzy systems for regression on data streams},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperbolic node embedding for temporal networks.
<em>DMKD</em>, <em>35</em>(5), 1906–1940. (<a
href="https://doi.org/10.1007/s10618-021-00774-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating general-purpose vector representations of networks allows us to analyze them without the need for extensive feature-engineering. Recent works have shown that the hyperbolic space can naturally represent the structure of networks, and that embedding networks into hyperbolic space is extremely efficient, especially in low dimensions. However, the existing hyperbolic embedding methods apply to static networks and cannot capture the dynamic evolution of the nodes and edges of a temporal network. In this paper, we present an unsupervised framework that uses temporal random walks to obtain training samples with both temporal and structural information to learn hyperbolic embeddings from continuous-time dynamic networks. We also show how the framework extends to attributed and heterogeneous information networks. Through experiments on five publicly available real-world temporal datasets, we show the efficacy of our model in embedding temporal networks in low-dimensional hyperbolic space compared to several other unsupervised baselines. We show that our model obtains state-of-the-art performance in low dimensions, outperforming all baselines, and has competitive performance in higher dimensions, outperforming the baselines in three of the five datasets. Our results show that embedding temporal networks in hyperbolic space is extremely effective when necessitating low dimensions.},
  archive      = {J_DMKD},
  author       = {Wang, Lili and Huang, Chenghan and Ma, Weicheng and Liu, Ruibo and Vosoughi, Soroush},
  doi          = {10.1007/s10618-021-00774-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1906-1940},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Hyperbolic node embedding for temporal networks},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AURORA: A unified fRamework fOR anomaly detection on
multivariate time series. <em>DMKD</em>, <em>35</em>(5), 1882–1905. (<a
href="https://doi.org/10.1007/s10618-021-00771-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to accurately and consistently discover anomalies in time series is important in many applications. Fields such as finance (fraud detection), information security (intrusion detection), healthcare, and others all benefit from anomaly detection. Intuitively, anomalies in time series are time points or sequences of time points that deviate from normal behavior characterized by periodic oscillations and long-term trends. For example, the typical activity on e-commerce websites exhibits weekly periodicity and grows steadily before holidays. Similarly, domestic usage of electricity exhibits daily and weekly oscillations combined with long-term season-dependent trends. How can we accurately detect anomalies in such domains while simultaneously learning a model for normal behavior? We propose a robust offline unsupervised framework for anomaly detection in seasonal multivariate time series, called AURORA. A key innovation in our framework is a general background behavior model that unifies periodicity and long-term trends. To this end, we leverage a Ramanujan periodic dictionary and a spline-based dictionary to capture both seasonal and trend patterns. We conduct experiments on both synthetic and real-world datasets and demonstrate the effectiveness of our method. AURORA has significant advantages over existing models for anomaly detection, including high accuracy (AUC of up to 0.98), interpretability of recovered normal behavior ( $$100\%$$ accuracy in period detection), and the ability to detect both point and contextual anomalies. In addition, AURORA is orders of magnitude faster than baselines.},
  archive      = {J_DMKD},
  author       = {Zhang, Lin and Zhang, Wenyu and McNeil, Maxwell J. and Chengwang, Nachuan and Matteson, David S. and Bogdanov, Petko},
  doi          = {10.1007/s10618-021-00771-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1882-1905},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {AURORA: A unified fRamework fOR anomaly detection on multivariate time series},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fake review detection on online e-commerce platforms: A
systematic literature review. <em>DMKD</em>, <em>35</em>(5), 1830–1881.
(<a href="https://doi.org/10.1007/s10618-021-00772-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing popularity of online review systems motivates malevolent intent in competing sellers and service providers to manipulate consumers by fabricating product/service reviews. Immoral actors use Sybil accounts, bot farms, and purchase authentic accounts to promote products and vilify competitors. Facing the continuous advancement of review spamming techniques, the research community should step back, assess the approaches explored to date to combat fake reviews, and regroup to define new ones. This paper reviews the literature on Fake Review Detection (FRD) on online platforms. It covers both basic research and commercial solutions, and discusses the reasons behind the limited level of success that the current approaches and regulations have had in preventing damage due to deceptive reviews.},
  archive      = {J_DMKD},
  author       = {Paul, Himangshu and Nikolaev, Alexander},
  doi          = {10.1007/s10618-021-00772-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1830-1881},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fake review detection on online E-commerce platforms: A systematic literature review},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effective social post classifiers on top of search
interfaces. <em>DMKD</em>, <em>35</em>(5), 1809–1829. (<a
href="https://doi.org/10.1007/s10618-021-00768-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying text classification to find social media posts relevant to a topic of interest is the focus of a substantial amount of research. A key challenge is how to select a good training set of posts to label. This problem has traditionally been solved using active learning. However, this assumes access to all posts of the collection, which is not realistic in many cases, as social networks impose constraints on the number of posts that can be retrieved through their search APIs. To address this problem, which we refer as the training post retrieval over constrained search interfaces problem, we propose several keyword selection algorithms that, given a topic, generate an effective set of keyword queries to submit to the search API. The returned posts are labeled and used as a training dataset to train post classifiers. Our experiments compare our proposed keyword selection algorithms to several baselines across various topics from three sources. The results show that the proposed methods generate superior training sets, which is measured by the balanced accuracy of the trained classifiers.},
  archive      = {J_DMKD},
  author       = {Rivas, Ryan and Hristidis, Vagelis},
  doi          = {10.1007/s10618-021-00768-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1809-1829},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Effective social post classifiers on top of search interfaces},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive modeling of infant mortality. <em>DMKD</em>,
<em>35</em>(4), 1785–1807. (<a
href="https://doi.org/10.1007/s10618-020-00728-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Infant Mortality Rate (IMR) is defined as the number of infants for every thousand infants that do not survive until their first birthday. IMR is an important metric not only because it provides information about infant births in an area, but it also measures the general societal health status. In the United States of America, the IMR is higher than many other developed countries, despite the high level of prosperity. It is important to note here that the U.S.A. exhibits strong and persistent inequalities in the IMR across different racial and ethnic groups (Kochanek et al. in Natl Vital Stat Rep 65(4):1–122, 2006). In this paper, we study predictive models in the problem of infant mortality. We implement traditional machine learning models and state-of-the-art neural network models with various combinations of features extracted from birth certificates. Those combinations include features that can be summed as socio-economic and ethical features related to the mother and the father of the infant and medical measurements during the pregnancy and the delivery. We approach the classification problem of infant mortality, whether an infant will survive until her first birthday or not, both as binary and multi-class based on the time of death. We focus on understanding and exploring the importance of features extracted from the birth certificates. For example, we test the performance of models trained on the general population to models trained in subsets of the population, e.g., for individual races. We show in our experimental evaluation comparisons between different predictive models (including those used by epidemiology researchers), various combinations of features, different distributions in the training set and features’ importance.},
  archive      = {J_DMKD},
  author       = {Saravanou, Antonia and Noelke, Clemens and Huntington, Nicholas and Acevedo-Garcia, Dolores and Gunopulos, Dimitrios},
  doi          = {10.1007/s10618-020-00728-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1785-1807},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Predictive modeling of infant mortality},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature extraction from unequal length heterogeneous EHR
time series via dynamic time warping and tensor decomposition.
<em>DMKD</em>, <em>35</em>(4), 1760–1784. (<a
href="https://doi.org/10.1007/s10618-020-00724-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic Health Records (EHR) data is routinely generated patient data that can provide useful information for analytical tasks such as disease detection and clinical event prediction. However, temporal EHR data such as physiological vital signs and lab test results are particularly challenging. Temporal EHR features typically have different sampling frequencies; such examples include heart rate (measured almost continuously) and blood test results (a few times during a patient’s entire stay). Different patients also have different length of stays. Existing approaches for temporal EHR sequence extraction either ignore the temporal pattern within features, or use a predefined window to select a section of the sequences without taking into account all the information. We propose a novel approach to tackle the issue of irregularly sampled, unequal length EHR time series using dynamic time warping and tensor decomposition. We use DTW to learn the pairwise distances for each temporal feature among the patient cohort and stack the distance matrices into a tensor. We then decompose the tensor to learn the latent structure, which is consequently used for patient representation. Finally, we use the patient representation for in-hospital mortality prediction. We illustrate our method on two cohorts from the MIMIC-III database: the sepsis and the acute kidney failure cohorts. We show that our method produces outstanding classification performance in terms of AUROC, AUPRC and accuracy compared with the baseline methods: LSTM and DTW-KNN. In the end we provide a detailed analysis on the feature importance for the interpretability of our method.},
  archive      = {J_DMKD},
  author       = {Zhang, Chi and Fanaee-T, Hadi and Thoresen, Magne},
  doi          = {10.1007/s10618-020-00724-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1760-1784},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Feature extraction from unequal length heterogeneous EHR time series via dynamic time warping and tensor decomposition},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Affinity analysis for studying physicians’ prescription
behavior. <em>DMKD</em>, <em>35</em>(4), 1739–1759. (<a
href="https://doi.org/10.1007/s10618-021-00758-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study refers to the affinity analysis of a prescription and diagnosis dataset and focuses on how prescription behavior changes after the detection of a disorder. Using a subset of patients that have been diagnosed, at least once, with hypertension and hyperlipidemia, examines the prescription behavior of the doctor after the detection of gastroesophageal reflux (K21), or insulin-dependent diabetes mellitus (E10). Diagnosis and prescription data were collected during consecutive visits of 4473 patients in a 3 years period. The analysis of the prescription data before and after the diagnosis of K21 and E10 reveals the popular substances for each disorder, such as Metformin for E10, or Omeprazole for K21 and substances that are discontinued including the same popular substances as well. It also reveals that substances that treat the main disorders of the group, such as Simvastatin and Hydrochlorothiazide are discontinued after the diagnosis of K21 and E10. Apart from the medical findings, which must be subject of further research, the proposed methodology can be employed for studying the prescription behavior of physicians and how it changes after specific diagnoses.},
  archive      = {J_DMKD},
  author       = {Varlamis, Iraklis},
  doi          = {10.1007/s10618-021-00758-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1739-1759},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Affinity analysis for studying physicians’ prescription behavior.},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial balancing-based representation learning for
causal effect inference with observational data. <em>DMKD</em>,
<em>35</em>(4), 1713–1738. (<a
href="https://doi.org/10.1007/s10618-021-00759-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning causal effects from observational data greatly benefits a variety of domains such as health care, education, and sociology. For instance, one could estimate the impact of a new drug on specific individuals to assist clinical planning and improve the survival rate. In this paper, we focus on studying the problem of estimating the Conditional Average Treatment Effect (CATE) from observational data. The challenges for this problem are two-fold: on the one hand, we have to derive a causal estimator to estimate the causal quantity from observational data, in the presence of confounding bias; on the other hand, we have to deal with the identification of the CATE when the distributions of covariates over the treatment group units and the control units are imbalanced. To overcome these challenges, we propose a neural network framework called Adversarial Balancing-based representation learning for Causal Effect Inference (ABCEI), based on recent advances in representation learning. To ensure the identification of the CATE, ABCEI uses adversarial learning to balance the distributions of covariates in the treatment and the control group in the latent representation space, without any assumptions on the form of the treatment selection/assignment function. In addition, during the representation learning and balancing process, highly predictive information from the original covariate space might be lost. ABCEI can tackle this information loss problem by preserving useful information for predicting causal effects under the regularization of a mutual information estimator. The experimental results show that ABCEI is robust against treatment selection bias, and matches/outperforms the state-of-the-art approaches. Our experiments show promising results on several datasets, encompassing several health care (and other) domains.},
  archive      = {J_DMKD},
  author       = {Du, Xin and Sun, Lei and Duivesteijn, Wouter and Nikolaev, Alexander and Pechenizkiy, Mykola},
  doi          = {10.1007/s10618-021-00759-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1713-1738},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Adversarial balancing-based representation learning for causal effect inference with observational data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on mining for health.
<em>DMKD</em>, <em>35</em>(4), 1710–1712. (<a
href="https://doi.org/10.1007/s10618-021-00767-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Spiliopoulou, Myra and Papapetrou, Panagiotis},
  doi          = {10.1007/s10618-021-00767-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1710-1712},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Guest editorial: Special issue on mining for health},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrashNet: An encoder–decoder architecture to predict crash
test outcomes. <em>DMKD</em>, <em>35</em>(4), 1688–1709. (<a
href="https://doi.org/10.1007/s10618-021-00761-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Destructive car crash tests are an elaborate, time-consuming, and expensive necessity of the automotive development process. Today, finite element method (FEM) simulations are used to reduce costs by simulating car crashes computationally. We propose CrashNet, an encoder–decoder deep neural network architecture that reduces costs further and models specific outcomes of car crashes very accurately. We achieve this by formulating car crash events as time series prediction enriched with a set of scalar features. Traditional sequence-to-sequence models are usually composed of convolutional neural network (CNN) and CNN transpose layers. We propose to concatenate those with an MLP capable of learning how to inject the given scalars into the output time series. In addition, we replace the CNN transpose with 2D CNN transpose layers in order to force the model to process the hidden state of the set of scalars as one time series. The proposed CrashNet model can be trained efficiently and is able to process scalars and time series as input in order to infer the results of crash tests. CrashNet produces results faster and at a lower cost compared to destructive tests and FEM simulations. Moreover, it represents a novel approach in the car safety management domain.},
  archive      = {J_DMKD},
  author       = {Belaid, Mohamed Karim and Rabus, Maximilian and Krestel, Ralf},
  doi          = {10.1007/s10618-021-00761-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1688-1709},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {CrashNet: An encoder–decoder architecture to predict crash test outcomes},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An overlap sensitive neural network for class imbalanced
data. <em>DMKD</em>, <em>35</em>(4), 1654–1687. (<a
href="https://doi.org/10.1007/s10618-021-00766-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is one of the well-known challenges in machine learning. Class imbalance occurs when one class dominates the other class in terms of the number of observations. Due to this imbalance, conventional classifiers fail to classify the minority class correctly. The challenges become even more severe when class overlap occurs in imbalanced data. Though literature is available to sequentially deal with class imbalance and class overlap, these methods are quite complex and not so efficient. In this paper, we propose an overlap-sensitive artificial neural network that can handle the problem of class overlapping and class imbalance simultaneously, along with noisy and outlier observations. The strength of this method lies in identifying the overlapping observations rather than the region and in not using multiple classifiers unlike the other existing methods. The key idea of the proposed method is in weighing the observations based on its location in the feature space before training the neural network. The performance of the proposed method is evaluated on 12 simulated data sets and 23 real-life data sets and compared with other well known methods.The results clearly indicate the strength and ability of the proposed method for a wide variety of imbalance ratio and levels of overlapping. Also, it is shown that the proposed method is statistically superior to the other methods in terms of different performance measures.},
  archive      = {J_DMKD},
  author       = {Shahee, Shaukat Ali and Ananthakumar, Usha},
  doi          = {10.1007/s10618-021-00766-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1654-1687},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An overlap sensitive neural network for class imbalanced data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlations between random projections and the bivariate
normal. <em>DMKD</em>, <em>35</em>(4), 1622–1653. (<a
href="https://doi.org/10.1007/s10618-021-00764-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random projections is a technique primarily used in dimension reduction by mapping high dimensional data to a low dimensional space, preserving pairwise distances in expectation, such as the Euclidean distance, inner product, angular distance, and $$l_p$$ distance for values of p which are even. These estimated pairwise distances between observations in the low dimensional space can be rapidly computed to be used for nearest neighbor searches, clustering, or even classification. This paper highlights how these two disparate topics have a common thread, and expand upon two computational statistical techniques in recent random projection literature to further improve the accuracy of the estimate of the inner product between vectors under random projection by making use of the properties of the respective dataset, as well as limitations of these methods.},
  archive      = {J_DMKD},
  author       = {Kang, Keegan},
  doi          = {10.1007/s10618-021-00764-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1622-1653},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correlations between random projections and the bivariate normal},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handling imbalance in hierarchical classification problems
using local classifiers approaches. <em>DMKD</em>, <em>35</em>(4),
1564–1621. (<a
href="https://doi.org/10.1007/s10618-021-00762-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of learning from imbalanced datasets has been widely investigated in the binary, multi-class and multi-label classification scenarios. Although this problem also affects hierarchical datasets, there are few work in the literature dealing with it. Meanwhile, the local classifier approaches are the most used techniques in the literature to deal with Hierarchical Classification problems. In this paper, we present new ways to handle data imbalance in hierarchical classification problems when using local classifiers approaches. We propose three different resampling schemas, according to the local classification approach: (1) Local Classifiers per Node; (2) Local Classifiers per Parent Node; and (3) Local Classifiers per Level. In order to define how imbalanced a certain hierarchical dataset is, we also propose three novel metrics to measure the imbalance in hierarchical datasets considering the different local classification approaches. The experimental evaluation in eight well-known datasets showed that the imbalance metrics can indeed measure the datasets imbalance and the proposed resampling schemas are able to improve the classification results when compared to baselines, state-of-the-art and related work approaches.},
  archive      = {J_DMKD},
  author       = {Pereira, Rodolfo M. and Costa, Yandre M. G. and Silla, Carlos N.},
  doi          = {10.1007/s10618-021-00762-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1564-1621},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Handling imbalance in hierarchical classification problems using local classifiers approaches},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What’s in a name? – gender classification of names with
character based machine learning models. <em>DMKD</em>, <em>35</em>(4),
1537–1563. (<a
href="https://doi.org/10.1007/s10618-021-00748-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gender information is no longer a mandatory input when registering for an account at many leading Internet companies. However, prediction of demographic information such as gender and age remains an important task, especially in intervention of unintentional gender/age bias in recommender systems. Therefore it is necessary to infer the gender of those users who did not to provide this information during registration. We consider the problem of predicting the gender of registered users based on their declared name. By analyzing the first names of 100M+ users, we found that genders can be very effectively classified using the composition of the name strings. We propose a number of character based machine learning models, and demonstrate that our models are able to infer the gender of users with much higher accuracy than baseline models. Moreover, we show that using the last names in addition to the first names improves classification performance further.},
  archive      = {J_DMKD},
  author       = {Hu, Yifan and Hu, Changwei and Tran, Thanh and Kasturi, Tejaswi and Joseph, Elizabeth and Gillingham, Matt},
  doi          = {10.1007/s10618-021-00748-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1537-1563},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {What’s in a name? – gender classification of names with character based machine learning models},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relational learning analysis of social politics using
knowledge graph embedding. <em>DMKD</em>, <em>35</em>(4), 1497–1536. (<a
href="https://doi.org/10.1007/s10618-021-00760-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graphs (KGs) have gained considerable attention recently from both academia and industry. In fact, incorporating graph technology and the copious of various graph datasets have led the research community to build sophisticated graph analytics tools, which has extended the application of KGs to tackle a plethora of real-life problems in dissimilar domains. Despite the abundance of the currently proliferated generic KGs, there is a vital need to construct domain-specific KGs. Further, quality and credibility should be assimilated in the process of constructing and augmenting KGs, particularly those propagated from mixed-quality resources such as social media data. For example, the amount of the political discourses in social media is overwhelming yet can be hijacked and misused by spammers to spread misinformation and false news. This paper presents a novel credibility domain-based KG Embedding framework. This framework involves capturing a fusion of data related to politics domain and obtained from heterogeneous resources into a formal KG representation depicted by a politics domain ontology. The proposed approach makes use of various knowledge-based repositories to enrich the semantics of the textual contents, thereby facilitating the interoperability of information. The proposed framework also embodies a domain-based social credibility module to ensure data quality and trustworthiness. The utility of the proposed framework is verified by means of experiments conducted on two constructed KGs. The KGs are then embedded in low-dimensional semantically-continuous space using several embedding techniques. The effectiveness of embedding techniques and social credibility module is further demonstrated and substantiated on link prediction, clustering, and visualisation tasks.},
  archive      = {J_DMKD},
  author       = {Abu-Salih, Bilal and Al-Tawil, Marwan and Aljarah, Ibrahim and Faris, Hossam and Wongthongtham, Pornpit and Chan, Kit Yan and Beheshti, Amin},
  doi          = {10.1007/s10618-021-00760-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1497-1536},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Relational learning analysis of social politics using knowledge graph embedding},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smoothed dilated convolutions for improved dense prediction.
<em>DMKD</em>, <em>35</em>(4), 1470–1496. (<a
href="https://doi.org/10.1007/s10618-021-00765-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dilated convolutions, also known as atrous convolutions, have been widely explored in deep convolutional neural networks (DCNNs) for various dense prediction tasks. However, dilated convolutions suffer from the gridding artifacts, which hampers the performance. In this work, we propose two simple yet effective degridding methods by studying a decomposition of dilated convolutions. Unlike existing models, which explore solutions by focusing on a block of cascaded dilated convolutional layers, our methods address the gridding artifacts by smoothing the dilated convolution itself. In addition, we point out that the two degridding approaches are intrinsically related and define separable and shared (SS) operations, which generalize the proposed methods. We further explore SS operations in view of operations on graphs and propose the SS output layer, which is able to smooth the entire DCNNs by only replacing the output layer. We evaluate our degridding methods and the SS output layer thoroughly, and visualize the smoothing effect through effective receptive field analysis. Results show that our methods degridding yield consistent improvements on the performance of dense prediction tasks, while adding negligible amounts of extra training parameters. And the SS output layer improves the performance by 3.3% and contains only 9% training parameters of the original output layer.},
  archive      = {J_DMKD},
  author       = {Wang, Zhengyang and Ji, Shuiwang},
  doi          = {10.1007/s10618-021-00765-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1470-1496},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Smoothed dilated convolutions for improved dense prediction},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient set-valued prediction in multi-class
classification. <em>DMKD</em>, <em>35</em>(4), 1435–1469. (<a
href="https://doi.org/10.1007/s10618-021-00751-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cases of uncertainty, a multi-class classifier preferably returns a set of candidate classes instead of predicting a single class label with little guarantee. More precisely, the classifier should strive for an optimal balance between the correctness (the true class is among the candidates) and the precision (the candidates are not too many) of its prediction. We formalize this problem within a general decision-theoretic framework that unifies most of the existing work in this area. In this framework, uncertainty is quantified in terms of conditional class probabilities, and the quality of a predicted set is measured in terms of a utility function. We then address the problem of finding the Bayes-optimal prediction, i.e., the subset of class labels with the highest expected utility. For this problem, which is computationally challenging as there are exponentially (in the number of classes) many predictions to choose from, we propose efficient algorithms that can be applied to a broad family of utility functions. Our theoretical results are complemented by experimental studies, in which we analyze the proposed algorithms in terms of predictive accuracy and runtime efficiency.},
  archive      = {J_DMKD},
  author       = {Mortier, Thomas and Wydmuch, Marek and Dembczyński, Krzysztof and Hüllermeier, Eyke and Waegeman, Willem},
  doi          = {10.1007/s10618-021-00751-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1435-1469},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient set-valued prediction in multi-class classification},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending greedy feature selection algorithms to multiple
solutions. <em>DMKD</em>, <em>35</em>(4), 1393–1434. (<a
href="https://doi.org/10.1007/s10618-020-00731-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most feature selection methods identify only a single solution. This is acceptable for predictive purposes, but is not sufficient for knowledge discovery if multiple solutions exist. We propose a strategy to extend a class of greedy methods to efficiently identify multiple solutions, and show under which conditions it identifies all solutions. We also introduce a taxonomy of features that takes the existence of multiple solutions into account. Furthermore, we explore different definitions of statistical equivalence of solutions, as well as methods for testing equivalence. A novel algorithm for compactly representing and visualizing multiple solutions is also introduced. In experiments we show that (a) the proposed algorithm is significantly more computationally efficient than the TIE* algorithm, the only alternative approach with similar theoretical guarantees, while identifying similar solutions to it, and (b) that the identified solutions have similar predictive performance.},
  archive      = {J_DMKD},
  author       = {Borboudakis, Giorgos and Tsamardinos, Ioannis},
  doi          = {10.1007/s10618-020-00731-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1393-1434},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Extending greedy feature selection algorithms to multiple solutions},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep multimodal model for bug localization. <em>DMKD</em>,
<em>35</em>(4), 1369–1392. (<a
href="https://doi.org/10.1007/s10618-021-00755-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bug localization utilizes the collected bug reports to locate the buggy source files. The state of the art falls short in handling the following three aspects, including (L1) the subtle difference between natural language and programming language, (L2) the noise in the bug reports and (L3) the multi-grained nature of programming language. To overcome these limitations, we propose a novel deep multimodal model named DeMoB for bug localization. It embraces three key features, each of which is tailored to address each of the three limitations. To be specific, the proposed DeMoB generates the multimodal coordinated representations for both bug reports and source files for addressing L1. It further incorporates the AttL encoder to process bug reports for addressing L2, and the MDCL encoder to process source files for addressing L3. Extensive experiments on four large-scale real-world data sets demonstrate that the proposed DeMoB significantly outperforms existing techniques.},
  archive      = {J_DMKD},
  author       = {Zhu, Ziye and Li, Yun and Wang, Yu and Wang, Yaojing and Tong, Hanghang},
  doi          = {10.1007/s10618-021-00755-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1369-1392},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A deep multimodal model for bug localization},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast computation of katz index for efficient processing of
link prediction queries. <em>DMKD</em>, <em>35</em>(4), 1342–1368. (<a
href="https://doi.org/10.1007/s10618-021-00754-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network proximity computations are among the most common operations in various data mining applications, including link prediction and collaborative filtering. A common measure of network proximity is Katz index, which has been shown to be among the best-performing path-based link prediction algorithms. With the emergence of very large network databases, such proximity computations become an important part of query processing in these databases. Consequently, significant effort has been devoted to developing algorithms for efficient computation of Katz index between a given pair of nodes or between a query node and every other node in the network. Here, we present LRC-Katz, an algorithm based on indexing and low rank correction to accelerate Katz index based network proximity queries. Using a variety of very large real-world networks, we show that LRC-Katzoutperforms the fastest existing method, Conjugate Gradient, for a wide range of parameter values. Taking advantage of the acceleration in the computation of Katz index, we propose a new link prediction algorithm that exploits locality of networks that are encountered in practical applications. Our experiments show that the resulting link prediction algorithm drastically outperforms state-of-the-art link prediction methods based on the vanilla and truncated Katz.},
  archive      = {J_DMKD},
  author       = {Coşkun, Mustafa and Baggag, Abdelkader and Koyutürk, Mehmet},
  doi          = {10.1007/s10618-021-00754-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1342-1368},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fast computation of katz index for efficient processing of link prediction queries},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudoinverse graph convolutional networks. <em>DMKD</em>,
<em>35</em>(4), 1318–1341. (<a
href="https://doi.org/10.1007/s10618-021-00752-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have proven to be successful tools for semi-supervised classification on graph-based datasets. We propose a new GCN variant whose three-part filter space is targeted at dense graphs. Our examples include graphs generated from 3D point clouds with an increased focus on non-local information, as well as hypergraphs based on categorical data of real-world problems. These graphs differ from the common sparse benchmark graphs in terms of the spectral properties of their graph Laplacian. Most notably we observe large eigengaps, which are unfavorable for popular existing GCN architectures. Our method overcomes these issues by utilizing the pseudoinverse of the Laplacian. Another key ingredient is a low-rank approximation of the convolutional matrix, ensuring computational efficiency and increasing accuracy at the same time. We outline how the necessary eigeninformation can be computed efficiently in each applications and discuss the appropriate choice of the only metaparameter, the approximation rank. We finally showcase our method’s performance regarding runtime and accuracy in various experiments with real-world datasets.},
  archive      = {J_DMKD},
  author       = {Alfke, Dominik and Stoll, Martin},
  doi          = {10.1007/s10618-021-00752-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1318-1341},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Pseudoinverse graph convolutional networks},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Streaming changepoint detection for
transition matrices. <em>DMKD</em>, <em>35</em>(4), 1317. (<a
href="https://doi.org/10.1007/s10618-021-00756-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correction to this paper has been published: https://doi.org/10.1007/s10618-021-00756-6},
  archive      = {J_DMKD},
  author       = {Plasse, Joshua and Hoeltgebaum, Henrique and Adams, Niall M.},
  doi          = {10.1007/s10618-021-00756-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1317},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction to: Streaming changepoint detection for transition matrices},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Streaming changepoint detection for transition matrices.
<em>DMKD</em>, <em>35</em>(4), 1287–1316. (<a
href="https://doi.org/10.1007/s10618-021-00747-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequentially detecting multiple changepoints in a data stream is a challenging task. Difficulties relate to both computational and statistical aspects, and in the latter, specifying control parameters is a particular problem. Choosing control parameters typically relies on unrealistic assumptions, such as the distributions generating the data, and their parameters, being known. This is implausible in the streaming paradigm, where several changepoints will exist. Further, current literature is mostly concerned with streams of continuous-valued observations, and focuses on detecting a single changepoint. There is a dearth of literature dedicated to detecting multiple changepoints in transition matrices, which arise from a sequence of discrete states. This paper makes the following contributions: a complete framework is developed for adaptively and sequentially estimating a Markov transition matrix in the streaming data setting. A change detection method is then developed, using a novel moment matching technique, which can effectively monitor for multiple changepoints in a transition matrix. This adaptive detection and estimation procedure for transition matrices, referred to as ADEPT-M, is compared to several change detectors on synthetic data streams, and is implemented on two real-world data streams – one consisting of over nine million HTTP web requests, and the other being a well-studied electricity market data set.},
  archive      = {J_DMKD},
  author       = {Plasse, Joshua and Hoeltgebaum, Henrique and Adams, Niall M.},
  doi          = {10.1007/s10618-021-00747-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1287-1316},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Streaming changepoint detection for transition matrices},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Widening: Using parallel resources to improve model quality.
<em>DMKD</em>, <em>35</em>(4), 1258–1286. (<a
href="https://doi.org/10.1007/s10618-021-00749-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a unified description of Widening, a framework for the use of parallel (or otherwise abundant) computational resources to improve model quality. We discuss different theoretical approaches to Widening with and without consideration of diversity. We then soften some of the underlying constraints so that Widening can be implemented in real world algorithms. We summarize earlier experimental results demonstrating the potential impact as well as promising implementation strategies before concluding with a survey of related work.},
  archive      = {J_DMKD},
  author       = {Berthold, Michael R. and Fillbrunn, Alexander and Siebes, Arno},
  doi          = {10.1007/s10618-021-00749-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1258-1286},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Widening: Using parallel resources to improve model quality},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining full, inner and tail periodic patterns with perfect,
imperfect and asynchronous periodicity simultaneously. <em>DMKD</em>,
<em>35</em>(4), 1225–1257. (<a
href="https://doi.org/10.1007/s10618-021-00753-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodic pattern has been utilized in many real life applications, such as weather conditions in a particular season, transactions in a superstore, power consumption, computer network fault analysis, and analysis of DNA and protein sequences. Periodic pattern mining is a popular though challenging research field in data mining because periodic patterns are of different types (namely full, inner, and tail patterns) and varied periodicities (namely perfect, imperfect, and asynchronous periodicity). Previous periodic pattern mining methods have some disadvantages: (1) Previous methods have to find different patterns separately; (2) They require postprocessing such as level-by-level join strategies for mining complex periodic patterns which have wildcards between two items. They cannot mine full, tail, and inner periodic patterns with perfect, imperfect, and asynchronous periodicities simultaneously. Therefore, an effective and comprehensive approach capable of discovering the above specified kinds of periodic patterns is needed. We propose a novel suffix tree-based algorithm, Mining dIfferent kinds of Periodic Patterns Simultaneously, MIPPS, to address the above issues. MIPPS finds different kinds of periodic patterns with different periodicities simultaneously without level-by-level join techniques using a novel incremental propagation generator. In addition, MIPPS mines periodic patterns efficiently using some pruning strategies. For the performance evaluation, we use both synthetic and real data to confirm good performance and scalability with complex periodic patterns.},
  archive      = {J_DMKD},
  author       = {Huang, Jen-Wei and Jaysawal, Bijay Prasad and Wang, Cheng-Chung},
  doi          = {10.1007/s10618-021-00753-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1225-1257},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mining full, inner and tail periodic patterns with perfect, imperfect and asynchronous periodicity simultaneously},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Homophily outlier detection in non-IID categorical data.
<em>DMKD</em>, <em>35</em>(4), 1163–1224. (<a
href="https://doi.org/10.1007/s10618-021-00750-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing outlier detection methods assume that the outlier factors (i.e., outlierness scoring measures) of data entities (e.g., feature values and data objects) are Independent and Identically Distributed (IID). This assumption does not hold in real-world applications where the outlierness of different entities is dependent on each other and/or taken from different probability distributions (non-IID). This may lead to the failure of detecting important outliers that are too subtle to be identified without considering the non-IID nature. The issue is even intensified in more challenging contexts, e.g., high-dimensional data with many noisy features. This work introduces a novel outlier detection framework and its two instances to identify outliers in categorical data by capturing non-IID outlier factors. Our approach first defines and incorporates distribution-sensitive outlier factors and their interdependence into a value-value graph-based representation. It then models an outlierness propagation process in the value graph to learn the outlierness of feature values. The learned value outlierness allows for either direct outlier detection or outlying feature selection. The graph representation and mining approach is employed here to well capture the rich non-IID characteristics. Our empirical results on 15 real-world data sets with different levels of data complexities show that (i) the proposed outlier detection methods significantly outperform five state-of-the-art methods at the 95%/99% confidence level, achieving 10–28% AUC improvement on the 10 most complex data sets; and (ii) the proposed feature selection methods significantly outperform three competing methods in enabling subsequent outlier detection of two different existing detectors.},
  archive      = {J_DMKD},
  author       = {Pang, Guansong and Cao, Longbing and Chen, Ling},
  doi          = {10.1007/s10618-021-00750-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1163-1224},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Homophily outlier detection in non-IID categorical data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tackling ordinal regression problem for heterogeneous data:
Sparse and deep multi-task learning approaches. <em>DMKD</em>,
<em>35</em>(3), 1134–1161. (<a
href="https://doi.org/10.1007/s10618-021-00746-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets are labeled with natural orders, i.e., ordinal labels. Ordinal regression is a method to predict ordinal labels that finds a wide range of applications in data-rich domains, such as natural, health and social sciences. Most existing ordinal regression approaches work well for independent and identically distributed (IID) instances via formulating a single ordinal regression task. However, for heterogeneous non-IID instances with well-defined local geometric structures, e.g., subpopulation groups, multi-task learning (MTL) provides a promising framework to encode task (subgroup) relatedness, bridge data from all tasks, and simultaneously learn multiple related tasks in efforts to improve generalization performance. Even though MTL methods have been extensively studied, there is barely existing work investigating MTL for heterogeneous data with ordinal labels. We tackle this important problem via sparse and deep multi-task approaches. Specifically, we develop a regularized multi-task ordinal regression (MTOR) model for smaller datasets and a deep neural networks based MTOR model for large-scale datasets. We evaluate the performance using three real-world healthcare datasets with applications to multi-stage disease progression diagnosis. Our experiments indicate that the proposed MTOR models markedly improve the prediction performance comparing with single-task ordinal regression models.},
  archive      = {J_DMKD},
  author       = {Wang, Lu and Zhu, Dongxiao},
  doi          = {10.1007/s10618-021-00746-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1134-1161},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Tackling ordinal regression problem for heterogeneous data: Sparse and deep multi-task learning approaches},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential recommendation with metric models based on
frequent sequences. <em>DMKD</em>, <em>35</em>(3), 1087–1133. (<a
href="https://doi.org/10.1007/s10618-021-00744-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling user preferences (long-term history) and user dynamics (short-term history) is of greatest importance to build efficient sequential recommender systems. The challenge lies in the successful combination of the whole user’s history and his recent actions (sequential dynamics) to provide personalized recommendations. Existing methods capture the sequential dynamics of a user using fixed-order Markov chains (usually first order chains) regardless of the user, which limits both the impact of the past of the user on the recommendation and the ability to adapt its length to the user profile. In this article, we propose to use frequent sequences to identify the most relevant part of the user history for the recommendation. The most salient items are then used in a unified metric model that embeds items based on user preferences and sequential dynamics. Extensive experiments demonstrate that our method outperforms state-of-the-art, especially on sparse datasets. We show that considering sequences of varying lengths improves the recommendations and we also emphasize that these sequences provide explanations on the recommendation.},
  archive      = {J_DMKD},
  author       = {Lonjarret, Corentin and Auburtin, Roch and Robardet, Céline and Plantevit, Marc},
  doi          = {10.1007/s10618-021-00744-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1087-1133},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sequential recommendation with metric models based on frequent sequences},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-label learning with missing and completely unobserved
labels. <em>DMKD</em>, <em>35</em>(3), 1061–1086. (<a
href="https://doi.org/10.1007/s10618-021-00743-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning deals with data examples which are associated with multiple class labels simultaneously. Despite the success of existing approaches to multi-label learning, there is still a problem neglected by researchers, i.e., not only are some of the values of observed labels missing, but also some of the labels are completely unobserved for the training data. We refer to the problem as multi-label learning with missing and completely unobserved labels, and argue that it is necessary to discover these completely unobserved labels in order to mine useful knowledge and make a deeper understanding of what is behind the data. In this paper, we propose a new approach named MCUL to solve multi-label learning with Missing and Completely Unobserved Labels. We try to discover the unobserved labels of a multi-label data set with a clustering based regularization term and describe the semantic meanings of them based on the label-specific features learned by MCUL, and overcome the problem of missing labels by exploiting label correlations. The proposed method MCUL can predict both the observed and newly discovered labels simultaneously for unseen data examples. Experimental results validated over ten benchmark datasets demonstrate that the proposed method can outperform other state-of-the-art approaches on observed labels and obtain an acceptable performance on the new discovered labels as well.},
  archive      = {J_DMKD},
  author       = {Huang, Jun and Xu, Linchuan and Qian, Kun and Wang, Jing and Yamanishi, Kenji},
  doi          = {10.1007/s10618-021-00743-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1061-1086},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multi-label learning with missing and completely unobserved labels},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series extrinsic regression. <em>DMKD</em>,
<em>35</em>(3), 1032–1060. (<a
href="https://doi.org/10.1007/s10618-021-00745-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies time series extrinsic regression (TSER): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification (TSC), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting, relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of TSC algorithms on a novel archive of 19 TSER datasets which we have assembled. Our results show that the state-of-the-art TSC algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other TSC algorithms and state-of-the-art machine learning (ML) algorithms such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of ML models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, François and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-021-00745-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1032-1060},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Time series extrinsic regression},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse randomized shortest paths routing with tsallis
divergence regularization. <em>DMKD</em>, <em>35</em>(3), 986–1031. (<a
href="https://doi.org/10.1007/s10618-021-00742-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work elaborates on the important problem of (1) designing optimal randomized routing policies for reaching a target node t from a source note s on a weighted directed graph G and (2) defining distance measures between nodes interpolating between the least-cost (based on optimal movements) and the commute cost (based on a random walk on G), depending on a positive temperature parameter T. To this end, the randomized shortest path (RSP) formalism is rephrased in terms of Tsallis divergence regularization, instead of Kullback–Leibler divergence. The main consequence of this change is that the resulting routing policy (local transition probabilities) becomes sparser when T decreases, therefore inducing a sparse random walk on G converging to the least-cost directed acyclic graph when $$T \rightarrow 0$$ . Experimental comparisons on node clustering and semi-supervised classification tasks show that the derived dissimilarity measures based on expected routing costs provide state-of-the-art results. The sparse RSP is therefore a promising model of movements on a graph, balancing sparse exploitation and exploration.},
  archive      = {J_DMKD},
  author       = {Leleux, Pierre and Courtain, Sylvain and Guex, Guillaume and Saerens, Marco},
  doi          = {10.1007/s10618-021-00742-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {986-1031},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sparse randomized shortest paths routing with tsallis divergence regularization},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dataset2Vec: Learning dataset meta-features. <em>DMKD</em>,
<em>35</em>(3), 964–985. (<a
href="https://doi.org/10.1007/s10618-021-00737-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning, or learning to learn, is a machine learning approach that utilizes prior learning experiences to expedite the learning process on unseen tasks. As a data-driven approach, meta-learning requires meta-features that represent the primary learning tasks or datasets, and are estimated traditonally as engineered dataset statistics that require expert domain knowledge tailored for every meta-task. In this paper, first, we propose a meta-feature extractor called Dataset2Vec that combines the versatility of engineered dataset meta-features with the expressivity of meta-features learned by deep neural networks. Primary learning tasks or datasets are represented as hierarchical sets, i.e., as a set of sets, esp. as a set of predictor/target pairs, and then a DeepSet architecture is employed to regress meta-features on them. Second, we propose a novel auxiliary meta-learning task with abundant data called dataset similarity learning that aims to predict if two batches stem from the same dataset or different ones. In an experiment on a large-scale hyperparameter optimization task for 120 UCI datasets with varying schemas as a meta-learning task, we show that the meta-features of Dataset2Vec outperform the expert engineered meta-features and thus demonstrate the usefulness of learned meta-features for datasets with varying schemas for the first time.},
  archive      = {J_DMKD},
  author       = {Jomaa, Hadi S. and Schmidt-Thieme, Lars and Grabocka, Josif},
  doi          = {10.1007/s10618-021-00737-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {964-985},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Dataset2Vec: Learning dataset meta-features},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Word-class embeddings for multiclass text classification.
<em>DMKD</em>, <em>35</em>(3), 911–963. (<a
href="https://doi.org/10.1007/s10618-020-00735-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained word embeddings encode general word semantics and lexical regularities of natural language, and have proven useful across many NLP tasks, including word sense disambiguation, machine translation, and sentiment analysis, to name a few. In supervised tasks such as multiclass text classification (the focus of this article) it seems appealing to enhance word representations with ad-hoc embeddings that encode task-specific information. We propose (supervised) word-class embeddings (WCEs), and show that, when concatenated to (unsupervised) pre-trained word embeddings, they substantially facilitate the training of deep-learning models in multiclass classification by topic. We show empirical evidence that WCEs yield a consistent improvement in multiclass classification accuracy, using six popular neural architectures and six widely used and publicly available datasets for multiclass text classification. One further advantage of this method is that it is conceptually simple and straightforward to implement. Our code that implements WCEs is publicly available at https://github.com/AlexMoreo/word-class-embeddings .},
  archive      = {J_DMKD},
  author       = {Moreo, Alejandro and Esuli, Andrea and Sebastiani, Fabrizio},
  doi          = {10.1007/s10618-020-00735-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {911-963},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Word-class embeddings for multiclass text classification},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time series motifs discovery under DTW allows more robust
discovery of conserved structure. <em>DMKD</em>, <em>35</em>(3),
863–910. (<a href="https://doi.org/10.1007/s10618-021-00740-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, time series motif discovery has emerged as perhaps the most important primitive for many analytical tasks, including clustering, classification, rule discovery, segmentation, and summarization. In parallel, it has long been known that Dynamic Time Warping (DTW) is superior to other similarity measures such as Euclidean Distance under most settings. However, due to the computational complexity of both DTW and motif discovery, virtually no research efforts have been directed at combining these two ideas. The current best mechanisms to address their lethargy appear to be mutually incompatible. In this work, we present the first efficient, scalable and exact method to find time series motifs under DTW. Our method automatically performs the best trade-off of time-to-compute versus tightness-of-lower-bounds for a novel hierarchy of lower bounds that we introduce. As we shall show through extensive experiments, our algorithm prunes up to 99.99% of the DTW computations under realistic settings and is up to three to four orders of magnitude faster than the brute force search, and two orders of magnitude faster than the only other competitor algorithm. This allows us to discover DTW motifs in massive datasets for the first time. As we will show, in many domains, DTW-based motifs represent semantically meaningful conserved behavior that would escape our attention using all existing Euclidean distance-based methods.},
  archive      = {J_DMKD},
  author       = {Alaee, Sara and Mercer, Ryan and Kamgar, Kaveh and Keogh, Eamonn},
  doi          = {10.1007/s10618-021-00740-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {863-910},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Time series motifs discovery under DTW allows more robust discovery of conserved structure},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FuseRec: Fusing user and item homophily modeling with
temporal recommender systems. <em>DMKD</em>, <em>35</em>(3), 837–862.
(<a href="https://doi.org/10.1007/s10618-021-00738-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems can benefit from a plethora of signals influencing user behavior such as her past interactions, her social connections, as well as the similarity between different items. However, existing methods are challenged when taking all this data into account and often do not exploit all available information. This is primarily due to the fact that it is non-trivial to combine the various information as they mutually influence each other. To address this shortcoming, here, we propose a ‘Fusion Recommender’ (FuseRec), which models each of these factors separately and later combines them in an interpretable manner. We find this general framework to yield compelling results on all three investigated datasets, Epinions, Ciao, and CiaoDVD, outperforming the state-of-the-art by more than 14% for Ciao and Epinions. In addition, we provide a detailed ablation study, showing that our combined model achieves accurate results, often better than any of its components individually. Our model also provides insights on the importance of each of the factors in different datasets.},
  archive      = {J_DMKD},
  author       = {Narang, Kanika and Song, Yitong and Schwing, Alexander and Sundaram, Hari},
  doi          = {10.1007/s10618-021-00738-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {837-862},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {FuseRec: Fusing user and item homophily modeling with temporal recommender systems},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurring concept memory management in data streams:
Exploiting data stream concept evolution to improve performance and
transparency. <em>DMKD</em>, <em>35</em>(3), 796–836. (<a
href="https://doi.org/10.1007/s10618-021-00736-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data stream is a sequence of observations produced by a generating process which may evolve over time. In such a time-varying stream the relationship between input features and labels, or concepts, can change. Adapting to changes in concept is most often done by destroying and incrementally rebuilding the current classifier. Many systems additionally store and reuse previously built models to more efficiently adapt when stream conditions drift to a previously seen state. Reusing a model offers increased classification performance over rebuilding, and provides an indicator, or transparency, into the hidden state of the generating process. When only a subset of past models can be stored for reuse, for example due to memory constraints, the choice of which models to store for optimal future reuse is an important problem. Current methods of evaluating which models to store use valuation policies such as age, time since last use, accuracy and diversity. These policies are often not optimal, losing predictive performance by undervaluing complex models. We propose a new valuation policy based on advantage, the misclassifications avoided by reusing a model rather than training a new model, which more accurately reflects the true value of model storage. We evaluate our method on synthetic and real world data, including a real world air pollution dataset. Our results show accuracy increases of up to 6% using our valuation policy, while preserving transparency.},
  archive      = {J_DMKD},
  author       = {Halstead, Ben and Koh, Yun Sing and Riddle, Patricia and Pears, Russel and Pechenizkiy, Mykola and Bifet, Albert},
  doi          = {10.1007/s10618-021-00736-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {796-836},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Recurring concept memory management in data streams: Exploiting data stream concept evolution to improve performance and transparency},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ForestDSH: A universal hash design for discrete probability
distributions. <em>DMKD</em>, <em>35</em>(3), 748–795. (<a
href="https://doi.org/10.1007/s10618-020-00732-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of classification of high dimensional queries to high dimensional classes from discrete alphabets where the probabilistic model that relates data to the classes is known. This problem has applications in various fields including the database search problem in mass spectrometry. The problem is analogous to the nearest neighbor search problem, where the goal is to find the data point in a database that is the most similar to a query point. The state of the art method for solving an approximate version of the nearest neighbor search problem in high dimensions is locality sensitive hashing (LSH). LSH is based on designing hash functions that map near points to the same buckets with a probability higher than random (far) points. To solve our high dimensional classification problem, we introduce distribution sensitive hashes that map jointly generated pairs to the same bucket with probability higher than random pairs. We design distribution sensitive hashes using a forest of decision trees and we analytically derive the complexity of search. We further show that the proposed hashes perform faster than state of the art approximate nearest neighbor search methods for a range of probability distributions, in both theory and simulations. Finally, we apply our method to the spectral library search problem in mass spectrometry, and show that it is an order of magnitude faster than the state of the art methods.},
  archive      = {J_DMKD},
  author       = {Davoodi, Arash Gholami and Chang, Sean and Yoo, Hyun Gon and Baweja, Anubhav and Mongia, Mihir and Mohimani, Hosein},
  doi          = {10.1007/s10618-020-00732-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {748-795},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ForestDSH: A universal hash design for discrete probability distributions},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting virtual concept drift of regressors without ground
truth values. <em>DMKD</em>, <em>35</em>(3), 726–747. (<a
href="https://doi.org/10.1007/s10618-021-00739-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression analysis is a standard supervised machine learning method used to model an outcome variable in terms of a set of predictor variables. In most real-world applications the true value of the outcome variable we want to predict is unknown outside the training data, i.e., the ground truth is unknown. Phenomena such as overfitting and concept drift make it difficult to directly observe when the estimate from a model potentially is wrong. In this paper we present an efficient framework for estimating the generalization error of regression functions, applicable to any family of regression functions when the ground truth is unknown. We present a theoretical derivation of the framework and empirically evaluate its strengths and limitations. We find that it performs robustly and is useful for detecting concept drift in datasets in several real-world domains.},
  archive      = {J_DMKD},
  author       = {Oikarinen, Emilia and Tiittanen, Henri and Henelius, Andreas and Puolamäki, Kai},
  doi          = {10.1007/s10618-021-00739-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {726-747},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detecting virtual concept drift of regressors without ground truth values},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep graph similarity learning: A survey. <em>DMKD</em>,
<em>35</em>(3), 688–725. (<a
href="https://doi.org/10.1007/s10618-020-00733-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many domains where data are represented as graphs, learning a similarity metric among graphs is considered a key problem, which can further facilitate various learning tasks, such as classification, clustering, and similarity search. Recently, there has been an increasing interest in deep graph similarity learning, where the key idea is to learn a deep learning model that maps input graphs to a target space such that the distance in the target space approximates the structural distance in the input space. Here, we provide a comprehensive review of the existing literature of deep graph similarity learning. We propose a systematic taxonomy for the methods and applications. Finally, we discuss the challenges and future directions for this problem.},
  archive      = {J_DMKD},
  author       = {Ma, Guixiang and Ahmed, Nesreen K. and Willke, Theodore L. and Yu, Philip S.},
  doi          = {10.1007/s10618-020-00733-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {688-725},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Deep graph similarity learning: A survey},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining communities and their descriptions on attributed
graphs: A survey. <em>DMKD</em>, <em>35</em>(3), 661–687. (<a
href="https://doi.org/10.1007/s10618-021-00741-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding communities that are not only relatively densely connected in a graph but that also show similar characteristics based on attribute information has drawn strong attention in the last years. There exists already a remarkable body of work that attempts to find communities in vertex-attributed graphs that are relatively homogeneous with respect to attribute values. Yet, it is scattered through different research fields and most of those publications fail to make the connection. In this paper, we identify important characteristics of the different approaches and place them into three broad categories: those that select descriptive attributes, related to clustering approaches, those that enumerate attribute-value combinations, related to pattern mining techniques, and those that identify conditional attribute weights, allowing for post-processing. We point out that the large majority of these techniques treat the same problem in terms of attribute representation, and are therefore interchangeable to a certain degree. In addition, different authors have found very similar algorithmic solutions to their respective problem.},
  archive      = {J_DMKD},
  author       = {Atzmueller, Martin and Günnemann, Stephan and Zimmermann, Albrecht},
  doi          = {10.1007/s10618-021-00741-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {661-687},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mining communities and their descriptions on attributed graphs: A survey},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning tractable probabilistic models for moral
responsibility and blame. <em>DMKD</em>, <em>35</em>(2), 621–659. (<a
href="https://doi.org/10.1007/s10618-020-00726-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moral responsibility is a major concern in autonomous systems, with applications ranging from self-driving cars to kidney exchanges. Although there have been recent attempts to formalise responsibility and blame, among similar notions, the problem of learning within these formalisms has been unaddressed. From the viewpoint of such systems, the urgent questions are: (a) How can models of moral scenarios and blameworthiness be extracted and learnt automatically from data? (b) How can judgements be computed effectively and efficiently, given the split-second decision points faced by some systems? By building on constrained tractable probabilistic learning, we propose and implement a hybrid (between data-driven and rule-based methods) learning framework for inducing models of such scenarios automatically from data and reasoning tractably from them. We report on experiments that compare our system with human judgement in three illustrative domains: lung cancer staging, teamwork management, and trolley problems.},
  archive      = {J_DMKD},
  author       = {Hammond, Lewis and Belle, Vaishak},
  doi          = {10.1007/s10618-020-00726-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {621-659},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Learning tractable probabilistic models for moral responsibility and blame},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for deep constrained clustering. <em>DMKD</em>,
<em>35</em>(2), 593–620. (<a
href="https://doi.org/10.1007/s10618-020-00734-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area of constrained clustering has been extensively explored by researchers and used by practitioners. Constrained clustering formulations exist for popular algorithms such as k-means, mixture models, and spectral clustering but have several limitations. A fundamental strength of deep learning is its flexibility, and here we explore a deep learning framework for constrained clustering and in particular explore how it can extend the field of constrained clustering. We show that our framework can not only handle standard together/apart constraints (without the well documented negative effects reported earlier) generated from labeled side information but more complex constraints generated from new types of side information such as continuous values and high-level domain knowledge. Furthermore, we propose an efficient training paradigm that is generally applicable to these four types of constraints. We validate the effectiveness of our approach by empirical results on both image and text datasets. We also study the robustness of our framework when learning with noisy constraints and show how different components of our framework contribute to the final performance. Our source code is available at: http://github.com/blueocean92 .},
  archive      = {J_DMKD},
  author       = {Zhang, Hongjing and Zhan, Tianyang and Basu, Sugato and Davidson, Ian},
  doi          = {10.1007/s10618-020-00734-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {593-620},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A framework for deep constrained clustering},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User preference and embedding learning with implicit
feedback for recommender systems. <em>DMKD</em>, <em>35</em>(2),
568–592. (<a href="https://doi.org/10.1007/s10618-020-00730-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel ranking framework for collaborative filtering with the overall aim of learning user preferences over items by minimizing a pairwise ranking loss. We show the minimization problem involves dependent random variables and provide a theoretical analysis by proving the consistency of the empirical risk minimization in the worst case where all users choose a minimal number of positive and negative items. We further derive a Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items. The learning objective is based on three scenarios of ranking losses that control the ability of the model to maintain the ordering over the items induced from the users’ preferences, as well as, the capacity of the dot-product defined in the learned embedded space to produce the ordering. The proposed model is by nature suitable for implicit feedback and involves the estimation of only very few parameters. Through extensive experiments on several real-world benchmarks on implicit data, we show the interest of learning the preference and the embedding simultaneously when compared to learning those separately. We also demonstrate that our approach is very competitive with the best state-of-the-art collaborative filtering techniques proposed for implicit feedback.},
  archive      = {J_DMKD},
  author       = {Sidana, Sumit and Trofimov, Mikhail and Horodnytskyi, Oleh and Laclau, Charlotte and Maximov, Yury and Amini, Massih-Reza},
  doi          = {10.1007/s10618-020-00730-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {568-592},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {User preference and embedding learning with implicit feedback for recommender systems},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social explorative attention based recommendation for
content distribution platforms. <em>DMKD</em>, <em>35</em>(2), 533–567.
(<a href="https://doi.org/10.1007/s10618-020-00729-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern social media platforms, an effective content recommendation should benefit both creators to bring genuine benefits to them and consumers to help them get really interesting content. To address the limitations of existing methods for social recommendation, we propose Social Explorative Attention Network (SEAN), a social recommendation framework that uses a personalized content recommendation model to encourage personal interests driven recommendation. SEAN has two versions: (1) SEAN-END2END allows user’s attention vector to attend their personalized interested points in the documents. (2) SEAN-KEYWORD extracts keywords from users’ historical readings to capture their long-term interests. It is much faster than the first version, more suitable for practical usage, while SEAN-END2END is more effective. Both versions allow the personalization factors to attend to users’ higher-order friends on the social network to improve the accuracy and diversity of recommendation results. Constructing two datasets in two languages, English and Spanish, from a popular decentralized content distribution platform, Steemit, we compare SEAN models with state-of-the-art collaborative filtering (CF) and content based recommendation approaches. Experimental results demonstrate the effectiveness of SEAN in terms of both Gini coefficients for recommendation equality and F1 scores for recommendation accuracy.},
  archive      = {J_DMKD},
  author       = {Xiao, Wenyi and Zhao, Huan and Pan, Haojie and Song, Yangqiu and Zheng, Vincent W. and Yang, Qiang},
  doi          = {10.1007/s10618-020-00729-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {533-567},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Social explorative attention based recommendation for content distribution platforms},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational auto-encoder based bayesian poisson tensor
factorization for sparse and imbalanced count data. <em>DMKD</em>,
<em>35</em>(2), 505–532. (<a
href="https://doi.org/10.1007/s10618-020-00723-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative tensor factorization models enable predictive analysis on count data. Among them, Bayesian Poisson–Gamma models can derive full posterior distributions of latent factors and are less sensitive to sparse count data. However, current inference methods for these Bayesian models adopt restricted update rules for the posterior parameters. They also fail to share the update information to better cope with the data sparsity. Moreover, these models are not endowed with a component that handles the imbalance in count data values. In this paper, we propose a novel variational auto-encoder framework called VAE-BPTF which addresses the above issues. It uses multi-layer perceptron networks to encode and share complex update information. The encoded information is then reweighted per data instance to penalize common data values before aggregated to compute the posterior parameters for the latent factors. Under synthetic data evaluation, VAE-BPTF tended to recover the right number of latent factors and posterior parameter values. It also outperformed current models in both reconstruction errors and latent factor (semantic) coherence across five real-world datasets. Furthermore, the latent factors inferred by VAE-BPTF are perceived to be meaningful and coherent under a qualitative analysis.},
  archive      = {J_DMKD},
  author       = {Jin, Yuan and Liu, Ming and Li, Yunfeng and Xu, Ruohua and Du, Lan and Gao, Longxiang and Xiang, Yong},
  doi          = {10.1007/s10618-020-00723-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {505-532},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Variational auto-encoder based bayesian poisson tensor factorization for sparse and imbalanced count data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting singleton spams in reviews via learning deep
anomalous temporal aspect-sentiment patterns. <em>DMKD</em>,
<em>35</em>(2), 450–504. (<a
href="https://doi.org/10.1007/s10618-020-00725-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer reviews are an essential source of information to consumers. Meanwhile, opinion spams spread widely and the detection of spam reviews becomes critically important for ensuring the integrity of the echo system of online reviews. Singleton spam reviews—one-time reviews—have spread widely of late as spammers can create multiple accounts to purposefully cheat the system. Most available techniques fail to detect this cunning form of malicious reviews, mainly due to the scarcity of behaviour trails left behind by singleton spammers. Available approaches also require extensive feature engineering, expensive manual annotation and are less generalizable. Based on our thorough study of spam reviews, it was found that genuine opinions are usually directed uniformly towards important aspects of entities. In contrast, spammers attempt to counter the consensus towards these aspects while covering their malicious intent by adding more text but on less important aspects. Additionally, spammers usually target specific time periods along products’ lifespan to cause maximum bias to the public opinion. Based on these observations, we present an unsupervised singleton spam review detection model that runs in two steps. Unsupervised deep aspect-level sentiment model employing deep Boltzmann machines first learns fine-grained opinion representations from review texts. Then, an LSTM network is trained on opinion learned representation to track the evolution of opinions through the fluctuation of sentiments in a temporal context, followed by the application of a Robust Variational Autoencoder to identify spam instances. Experiments on three benchmark datasets widely used in the literature showed that our approach outperforms strong state-of-the-art baselines.},
  archive      = {J_DMKD},
  author       = {Shaalan, Yassien and Zhang, Xiuzhen and Chan, Jeffrey and Salehi, Mahsa},
  doi          = {10.1007/s10618-020-00725-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {450-504},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detecting singleton spams in reviews via learning deep anomalous temporal aspect-sentiment patterns},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The great multivariate time series classification bake off:
A review and experimental evaluation of recent algorithmic advances.
<em>DMKD</em>, <em>35</em>(2), 401–449. (<a
href="https://doi.org/10.1007/s10618-020-00727-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.},
  archive      = {J_DMKD},
  author       = {Ruiz, Alejandro Pasos and Flynn, Michael and Large, James and Middlehurst, Matthew and Bagnall, Anthony},
  doi          = {10.1007/s10618-020-00727-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {401-449},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The great multivariate time series classification bake off: A review and experimental evaluation of recent algorithmic advances},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SMILE: A feature-based temporal abstraction framework for
event-interval sequence classification. <em>DMKD</em>, <em>35</em>(1),
372–399. (<a href="https://doi.org/10.1007/s10618-020-00719-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of classification of sequences of temporal intervals. Our main contribution is a novel framework, which we call SMILE, for extracting relevant features from interval sequences to construct classifiers.SMILE introduces the notion of utilizing random temporal abstraction features, we define as e-lets, as a means to capture information pertaining to class-discriminatory events which occur across the span of complete interval sequences. Our empirical evaluation is applied to a wide array of benchmark data sets and fourteen novel datasets for adverse drug event detection. We demonstrate how the introduction of simple sequential features, followed by progressively more complex features each improve classification performance. Importantly, this investigation demonstrates that SMILE significantly improves AUC performance over the current state-of-the-art. The investigation also reveals that the selection of underlying classification algorithm is important to achieve superior predictive performance, and how the number of features influences the performance of our framework.},
  archive      = {J_DMKD},
  author       = {Rebane, Jonathan and Karlsson, Isak and Bornemann, Leon and Papapetrou, Panagiotis},
  doi          = {10.1007/s10618-020-00719-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {372-399},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SMILE: A feature-based temporal abstraction framework for event-interval sequence classification},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining explainable local and global subgraph patterns with
surprising densities. <em>DMKD</em>, <em>35</em>(1), 321–371. (<a
href="https://doi.org/10.1007/s10618-020-00721-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The connectivity structure of graphs is typically related to the attributes of the vertices. In social networks for example, the probability of a friendship between any pair of people depends on a range of attributes, such as their age, residence location, workplace, and hobbies. The high-level structure of a graph can thus possibly be described well by means of patterns of the form ‘the subgroup of all individuals with certain properties X are often (or rarely) friends with individuals in another subgroup defined by properties Y’, ideally relative to their expected connectivity. Such rules present potentially actionable and generalizable insight into the graph. Prior work has already considered the search for dense subgraphs (‘communities’) with homogeneous attributes. The first contribution in this paper is to generalize this type of pattern to densities between a pair of subgroups, as well as between all pairs from a set of subgroups that partition the vertices. Second, we develop a novel information-theoretic approach for quantifying the subjective interestingness of such patterns, by contrasting them with prior information an analyst may have about the graph’s connectivity. We demonstrate empirically that in the special case of dense subgraphs, this approach yields results that are superior to the state-of-the-art. Finally, we propose algorithms for efficiently finding interesting patterns of these different types.},
  archive      = {J_DMKD},
  author       = {Deng, Junning and Kang, Bo and Lijffijt, Jefrey and De Bie, Tijl},
  doi          = {10.1007/s10618-020-00721-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {321-371},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mining explainable local and global subgraph patterns with surprising densities},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Natural language techniques supporting decision modelers.
<em>DMKD</em>, <em>35</em>(1), 290–320. (<a
href="https://doi.org/10.1007/s10618-020-00718-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision Model and Notation (DMN) has become a relevant topic for organizations since it allows users to control their processes and organizational decisions. The increasing use of DMN decision tables to capture critical business knowledge raises the need for supporting analysis tasks such as the extraction of inputs, outputs and their relations from natural language descriptions. In this paper, we create a stepping stone towards implementing a Natural Language Processing framework to model decisions based on the DMN standard. Our proposal contributes to the generation of decision rules and tables from a single sentence analysis. This framework comprises three phases: (1) discourse and semantic analysis, (2) syntactic analysis and (3) decision table construction. To the best of our knowledge, this is the first attempt devoted to automatically discovering decision rules according to the DMN terminology from natural language descriptions. Aiming at assessing the quality of the resultant decision tables, we have conducted a survey involving 16 DMN experts. The results have shown that our framework is able to generate semantically correct tables. It is convenient to mention that our proposal does not aim to replace analysts but support them in creating better models with less effort.},
  archive      = {J_DMKD},
  author       = {Arco, Leticia and Nápoles, Gonzalo and Vanhoenshoven, Frank and Lara, Ana Laura and Casas, Gladys and Vanhoof, Koen},
  doi          = {10.1007/s10618-020-00718-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {290-320},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Natural language techniques supporting decision modelers},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An exemplar-based clustering using efficient variational
message passing. <em>DMKD</em>, <em>35</em>(1), 248–289. (<a
href="https://doi.org/10.1007/s10618-020-00720-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a crucial step in scientific data analysis and engineering systems. Thus, an efficient cluster analysis method often remains a key challenge. In this paper, we introduce a general purpose exemplar-based clustering method called (MEGA), which performs a novel message-passing strategy based on variational expectation–maximization and generalized arc-consistency techniques. Unlike message passing clustering methods, MEGA formulates the message-passing schema as E- and M-steps of variational expectation–maximization based on a reparameterized factor graph. It also exploits an adaptive variant of generalized arc consistency technique to perform a variational mean-field approximation in E-step to minimize a Kullback–Leibler divergence on the model evidence. Dissimilar to density-based clustering methods, MEGA has no sensitivity to initial parameters. In contrast to partition-based clustering methods, MEGA does not require pre-specifying the number of clusters. We focus on the binary-variable factor graph to model the clustering problem but MEGA is applicable to other graphical models in general. Our experiments on real-world problems demonstrate the efficiency of MEGA over existing prominent clustering algorithms such as Affinity propagation, Agglomerative, DBSCAN, K-means, and EM.},
  archive      = {J_DMKD},
  author       = {Ibrahim, Mohamed Hamza and Missaoui, Rokia},
  doi          = {10.1007/s10618-020-00720-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {248-289},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An exemplar-based clustering using efficient variational message passing},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The network-untangling problem: From interactions to
activity timelines. <em>DMKD</em>, <em>35</em>(1), 213–247. (<a
href="https://doi.org/10.1007/s10618-020-00717-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study a problem of determining when entities are active based on their interactions with each other. We consider a set of entities V and a sequence of time-stamped edges E among the entities. Each edge $$(u,v,t)\in E$$ denotes an interaction between entities u and v at time t. We assume an activity model where each entity is active during at most k time intervals. An interaction (u, v, t) can be explained if at least one of u or v are active at time t. Our goal is to reconstruct the activity intervals for all entities in the network, so as to explain the observed interactions. This problem, the network-untangling problem, can be applied to discover event timelines from complex entity interactions. We provide two formulations of the network-untangling problem: (i) minimizing the total interval length over all entities (sum version), and (ii) minimizing the maximum interval length (max version). We study separately the two problems for $$k=1$$ and $$k&gt;1$$ activity intervals per entity. For the case $$k=1$$ , we show that the sum problem is NP-hard, while the max problem can be solved optimally in linear time. For the sum problem we provide efficient algorithms motivated by realistic assumptions. For the case of $$k&gt;1$$ , we show that both formulations are inapproximable. However, we propose efficient algorithms based on alternative optimization. We complement our study with an evaluation on synthetic and real-world datasets, which demonstrates the validity of our concepts and the good performance of our algorithms.},
  archive      = {J_DMKD},
  author       = {Rozenshtein, Polina and Tatti, Nikolaj and Gionis, Aristides},
  doi          = {10.1007/s10618-020-00717-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {213-247},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The network-untangling problem: From interactions to activity timelines},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). For real: A thorough look at numeric attributes in subgroup
discovery. <em>DMKD</em>, <em>35</em>(1), 158–212. (<a
href="https://doi.org/10.1007/s10618-020-00703-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgroup discovery (SD) is an exploratory pattern mining paradigm that comes into its own when dealing with large real-world data, which typically involves many attributes, of a mixture of data types. Essential is the ability to deal with numeric attributes, whether they concern the target (a regression setting) or the description attributes (by which subgroups are identified). Various specific algorithms have been proposed in the literature for both cases, but a systematic review of the available options is missing. This paper presents a generic framework that can be instantiated in various ways in order to create different strategies for dealing with numeric data. The bulk of the work in this paper describes an experimental comparison of a considerable range of numeric strategies in SD, where these strategies are organised according to four central dimensions. These experiments are furthermore repeated for both the classification task (target is nominal) and regression task (target is numeric), and the strategies are compared based on the quality of the top subgroup, and the quality and redundancy of the top-k result set. Results of three search strategies are compared: traditional beam search, complete search, and a variant of diverse subgroup set discovery called cover-based subgroup selection. Although there are various subtleties in the outcome of the experiments, the following general conclusions can be drawn: it is often best to determine numeric thresholds dynamically (locally), in a fine-grained manner, with binary splits, while considering multiple candidate thresholds per attribute.},
  archive      = {J_DMKD},
  author       = {Meeng, Marvin and Knobbe, Arno},
  doi          = {10.1007/s10618-020-00703-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {158-212},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {For real: A thorough look at numeric attributes in subgroup discovery},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recency-based sequential pattern mining in multiple event
sequences. <em>DMKD</em>, <em>35</em>(1), 127–157. (<a
href="https://doi.org/10.1007/s10618-020-00715-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard sequential pattern mining scheme hardly considers the positions of events in a sequence, and therefore it is difficult to focus on more interesting patterns that represent better the causal relationships between events. Without quantifying how close two events are in a sequence, we may fail to evaluate how likely an event is caused by the others from the pattern, which is a severe drawback for some applications like prediction. Motivated by this, we propose the recency-based sequential pattern mining scheme together with a novel measure of pattern interestingness to effectively capture recency as well as frequency. To efficiently extract all the recency-based sequential patterns, we devise a mining algorithm, called Recency-based Frequent pattern Miner (RF-Miner), together with an effective prediction method to evaluate the quality of recency-based patterns in terms of their prediction power. The experimental results show that our RF-Miner algorithm can extract more diverse and important patterns that can be used to make prediction of the next event, and can be more efficiently performed by using the upper bounds of our measure than baseline algorithms.},
  archive      = {J_DMKD},
  author       = {Kim, Hakkyu and Choi, Dong-Wan},
  doi          = {10.1007/s10618-020-00715-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {127-157},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Recency-based sequential pattern mining in multiple event sequences},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online summarization of dynamic graphs using subjective
interestingness for sequential data. <em>DMKD</em>, <em>35</em>(1),
88–126. (<a href="https://doi.org/10.1007/s10618-020-00714-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world phenomena can be represented as dynamic graphs, i.e., networks that change over time. The problem of dynamic graph summarization, i.e., to succinctly describe the evolution of a dynamic graph, has been widely studied. Existing methods typically use objective measures to find fixed structures such as cliques, stars, and cores. Most of the methods, however, do not consider the problem of online summarization, where the summary is incrementally conveyed to the analyst as the graph evolves, and (thus) do not take into account the knowledge of the analyst at a specific moment in time. We address this gap in the literature through a novel, generic framework for subjective interestingness for sequential data. Specifically, we iteratively identify atomic changes, called ‘actions’, that provide most information relative to the current knowledge of the analyst. For this, we introduce a novel information gain measure, which is motivated by the minimum description length (MDL) principle. With this measure, our approach discovers compact summaries without having to decide on the number of patterns. As such, we are the first to combine approaches for data mining based on subjective interestingness (using the maximum entropy principle) with pattern-based summarization (using the MDL principle). We instantiate this framework for dynamic graphs and dense subgraph patterns, and present DSSG, a heuristic algorithm for the online summarization of dynamic graphs by means of informative actions, each of which represents an interpretable change to the connectivity structure of the graph. The experiments on real-world data demonstrate that our approach effectively discovers informative summaries. We conclude with a case study on data from an airline network to show its potential for real-world applications.},
  archive      = {J_DMKD},
  author       = {Kapoor, Sarang and Saxena, Dhish Kumar and van Leeuwen, Matthijs},
  doi          = {10.1007/s10618-020-00714-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {88-126},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Online summarization of dynamic graphs using subjective interestingness for sequential data},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of deep network techniques all classifiers can
adopt. <em>DMKD</em>, <em>35</em>(1), 46–87. (<a
href="https://doi.org/10.1007/s10618-020-00722-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have introduced novel and useful tools to the machine learning community. Other types of classifiers can potentially make use of these tools as well to improve their performance and generality. This paper reviews the current state of the art for deep learning classifier technologies that are being used outside of deep neural networks. Non-neural network classifiers can employ many components found in DNN architectures. In this paper, we review the feature learning, optimization, and regularization methods that form a core of deep network technologies. We then survey non-neural network learning algorithms that make innovative use of these methods to improve classification performance. Because many opportunities and challenges still exist, we discuss directions that can be pursued to expand the area of deep learning for a variety of classification algorithms.},
  archive      = {J_DMKD},
  author       = {Ghods, Alireza and Cook, Diane J.},
  doi          = {10.1007/s10618-020-00722-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {46-87},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A survey of deep network techniques all classifiers can adopt},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of community detection methods in multilayer
networks. <em>DMKD</em>, <em>35</em>(1), 1–45. (<a
href="https://doi.org/10.1007/s10618-020-00716-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is one of the most popular researches in a variety of complex systems, ranging from biology to sociology. In recent years, there’s an increasing focus on the rapid development of more complicated networks, namely multilayer networks. Communities in a single-layer network are groups of nodes that are more strongly connected among themselves than the others, while in multilayer networks, a group of well-connected nodes are shared in multiple layers. Most traditional algorithms can rarely perform well on a multilayer network without modifications. Thus, in this paper, we offer overall comparisons of existing works and analyze several representative algorithms, providing a comprehensive understanding of community detection methods in multilayer networks. The comparison results indicate that the promoting of algorithm efficiency and the extending for general multilayer networks are also expected in the forthcoming studies.},
  archive      = {J_DMKD},
  author       = {Huang, Xinyu and Chen, Dongming and Ren, Tao and Wang, Dongqi},
  doi          = {10.1007/s10618-020-00716-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-45},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A survey of community detection methods in multilayer networks},
  volume       = {35},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
