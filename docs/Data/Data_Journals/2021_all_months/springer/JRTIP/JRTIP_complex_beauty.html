<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---188">JRTIP - 188</h2>
<ul>
<li><details>
<summary>
(2021). Probabilistic object tracking by low power microcontrollers.
<em>JRTIP</em>, <em>18</em>(6), 2539–2550. (<a
href="https://doi.org/10.1007/s11554-021-01139-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low power microcontrollers have become widely available. Hence, they have been used in several stand-alone applications in which the developed system depends on battery or energy harvesting module. One such application is surveillance aiming to observe a selected region or target in time. Due to the complexity of the problem and real-time constraints in operation, several object trackers have been proposed in literature. An object tracker produces the trajectory of an object from a given image sequence. To do so, two major steps are taken as object representation and trajectory prediction. Here, the computation load for tracking and object representation strength plays adversary effects most of the time. Moreover, the overall system to be deployed in a remote location casts serious limitations on the tracking method to be used. Therefore, we propose a probabilistic object representation-based object tracking method to work on low power Arm Cortex-M4 and -M7 core microcontrollers in this study. The proposed method aims to represent the object to be tracked as simple as possible. On the other hand, the method provides an effective way of describing the object to be tracked. Therefore, the novelty of the proposed method is adding a simple yet flexible probabilistic object representation method to the tracking framework. The probabilistic object representation method can be easily merged with the Bayesian framework which is extensively used in trajectory prediction. To do so, we use the particle filter based Bayesian tracking method. As we form the overall system for object tracking, we compare it with similar methods in the literature under real-time constraints. We provide experimental results to show the strengths and weaknesses of the proposed method in comparison with the existing ones.},
  archive      = {J_JRTIP},
  author       = {Büyükeşmeli, Hüseyin and Masazade, Engin and Ünsalan, Cem},
  doi          = {10.1007/s11554-021-01139-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2539-2550},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Probabilistic object tracking by low power microcontrollers},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). L-net: Lightweight and fast object detector-based
ShuffleNetV2. <em>JRTIP</em>, <em>18</em>(6), 2527–2538. (<a
href="https://doi.org/10.1007/s11554-021-01145-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection algorithms based on deep learning have made continuous progress in recent years. On the premise of ensuring the accuracy of object detection, reducing model complexity and improving detection speed have always been the goals pursued by current object detection algorithms. A lightweight object detection model its backbone based on ShuffleNetV2 network structure named L-Net is presented in this paper. A suitable backbone network was obtained by changing from 3 × 3 depth convolution to 5 × 5 depth convolution and reducing the number of input channels. In order to obtain a more discriminative image feature description, Pyramid Pooling Module and Attention Pyramid Module are added after the backbone network. Experimental results show that the L-Net model only uses 1.54B FLOPs (floating point operations) to achieve 70.2% mAP (mean average precision) on PASCAL VOC2007 and 21.8% mAP on the MS COCO dataset. The model has achieved competitive results in terms of accuracy and speed while being lightweight.},
  archive      = {J_JRTIP},
  author       = {Han, Jin and Yang, Yonghao},
  doi          = {10.1007/s11554-021-01145-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2527-2538},
  shortjournal = {J. Real-Time Image Process.},
  title        = {L-net: Lightweight and fast object detector-based ShuffleNetV2},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved single image dehazing methods for
resource-constrained platforms. <em>JRTIP</em>, <em>18</em>(6),
2511–2525. (<a
href="https://doi.org/10.1007/s11554-021-01143-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is an increasingly widespread approach to address the degradation of images of the natural environment by low-visibility weather, dust and other phenomena. Advances in autonomous systems and platforms have increased the need for low-complexity, high-performing dehazing techniques. However, while recent learning-based image dehazing approaches have significantly increased the dehazing performance, this has often been at the expense of complexity and hence the use of prior-based approaches persists, despite their lower performance. This paper addresses both these aspects and focuses on single image dehazing, the most practical class of techniques. A new Dark Channel Prior-based single image dehazing algorithm is presented that has an improved atmospheric light estimation method and a low-complexity morphological reconstruction. In addition, a novel, lightweight end-to-end network is proposed, that avoids information loss and significant computational effort by eliminating the pooling and fully connected layers. Qualitative and quantitative evaluations show that our proposed algorithms are competitive with, or outperform, state-of-the-art techniques with significantly lower complexity, demonstrating their suitability for use in resource-constrained platforms.},
  archive      = {J_JRTIP},
  author       = {Yang, Gengqian and Evans, Adrian N.},
  doi          = {10.1007/s11554-021-01143-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2511-2525},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved single image dehazing methods for resource-constrained platforms},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-energy motion estimation memory system with dynamic
management. <em>JRTIP</em>, <em>18</em>(6), 2495–2510. (<a
href="https://doi.org/10.1007/s11554-021-01138-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital video coding process imposes severe pressure on memory traffic, leading to considerable power consumption related to frequent DRAM accesses. External off-chip memory demand needs to be minimized by clever architecture/algorithm co-design, thus saving energy and extending battery lifetime during video encoding. To exploit temporal redundancies among neighboring frames, the motion estimation (ME) algorithm searches for good matching between the current block and blocks within reference frames stored in external memory. To save energy during ME, this work performs memory accesses distribution analysis of the test zone search (TZS) ME algorithm and, based on this analysis, proposes both a multi-sector scratchpad memory design and dynamic management for the TZS memory access. Our dynamic memory management, called neighbor management, reduces both static consumption—by employing sector-level power gating—and dynamic consumption—by reducing the number of accesses for ME execution. Additionally, our dynamic management was integrated with two previously proposed solutions: a hardware reference frame compressor and the Level C data reuse scheme (using a scratchpad memory). This system achieves a memory energy consumption savings of $$99.8\%$$ and, when compared to the baseline solution composed of a reference frame compressor and data reuse scheme, the memory energy consumption was reduced by $$44.1\%$$ at a cost of just $$0.35\%$$ loss in coding efficiency, on average. When compared with related works, our system presents better memory bandwidth/energy savings and coding efficiency results.},
  archive      = {J_JRTIP},
  author       = {Silveira, Dieison Soares and Amaral, Lívia and Povala, Guilherme and Zatt, Bruno and Agostini, Luciano Volcan and Porto, Marcelo Schiavon and Bampi, Sergio},
  doi          = {10.1007/s11554-021-01138-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2495-2510},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-energy motion estimation memory system with dynamic management},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time visible and near-infrared video fusion:
Architecture and implementation. <em>JRTIP</em>, <em>18</em>(6),
2479–2493. (<a
href="https://doi.org/10.1007/s11554-020-01068-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-infrared (NIR) band sensors capture achromatic images that contain complementary details of a scene which are diminished in visible (VS) band images when the scene is obscured by haze, mist, or fog. To exploit these complementary details, an integrated FPGA architecture and implementation of a video processing system are proposed in this paper. This system performs VS-NIR video fusion and produces an enhanced VS video in real-time. The proposed FPGA architecture and implementation effectively handle the challenges associated with the simultaneous processing of video signals obtained from different sources such as the inevitable delay among corresponding frames and time-varying deviation among frame rates. Moreover, the proposed implementation is efficiently designed and able to produce the fused video at the same frame rate as the input videos, i.e. in real-time, regardless of the resolution of the input videos while the consumed FPGA resources are kept small. This is achieved by data and calculations reuse, besides performing operations concurrently in parallel and pipelined fashions at both the data and task levels. The proposed implementation is synthesized, validated on a low-end FPGA device, and compared to three other implementations. The comparison shows the superiority of the proposed implementation in terms of the consumed resources which have a direct industrial impact in the case of integration in modern smart-phones and cameras.},
  archive      = {J_JRTIP},
  author       = {Awad, Mohamed and Elliethy, Ahmed and Aly, Hussein A.},
  doi          = {10.1007/s11554-020-01068-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2479-2493},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time visible and near-infrared video fusion: Architecture and implementation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image normalization in embedded systems. <em>JRTIP</em>,
<em>18</em>(6), 2469–2478. (<a
href="https://doi.org/10.1007/s11554-021-01098-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalization is a step in image processing that is used to reduce lighting and contrast effects, significantly increasing the accuracy of the entire solution. The face detection algorithm proposed by Viola-Jones popularized the image normalization process. The basis of this process consists in obtaining the integral image and its square integral. This process requires a greater amount of computational resources, being then avoided, particularly in embedded systems, even if it compromises the result of the solution. In this article we propose a way to implement the image normalization process from the integral of that image already stored in memory, without increasing the amount of external memory, accessing each value of integral stored only twice.},
  archive      = {J_JRTIP},
  author       = {Monteiro, Heron Aragão and Brito, Alisson Vasconcelos de and Melcker, Elmar Uwe Kurt},
  doi          = {10.1007/s11554-021-01098-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2469-2478},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Image normalization in embedded systems},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mode skipping for screen content coding based on neural
network classifier. <em>JRTIP</em>, <em>18</em>(6), 2453–2468. (<a
href="https://doi.org/10.1007/s11554-021-01137-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Screen Content Coding Extension in High-Efficiency Video Coding standard (HEVC-SCC) promotes the capabilities of HEVC in coding screen content videos (SCVs) using new techniques, which improves coding efficiency dramatically. These new techniques depend on the distinguished features of SCV such as repeated patterns, limited number of colors, sharp edges, and non-noisy regions. Nonetheless, this coding efficiency comes at the cost of enormous computational complexity. In this paper, a new technique is proposed to save encoding time while conserving coding efficiency. The proposed algorithm selects the suitable mode for each Coding Unit (CU) and skips unhelpful modes by two methods. Two methods depend on skipping unwanted modes by Neural Network Classifiers. The first classifier is Neural Network Classifier Based on Current Depth Features (NNC_CF), which depends on the CU current depth features. The second one is Neural Network Classifier Based on Parent Depth Features (NNC_PF); the Parent depth features are considered the input of this classifier. The simulation results demonstrate the efficacy of the proposed scheme.},
  archive      = {J_JRTIP},
  author       = {Elsawy, Nabila and Sayed, Mohammed S. and Farag, Fathi},
  doi          = {10.1007/s11554-021-01137-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2453-2468},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Mode skipping for screen content coding based on neural network classifier},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel algorithm for fringe pattern demodulation.
<em>JRTIP</em>, <em>18</em>(6), 2441–2451. (<a
href="https://doi.org/10.1007/s11554-021-01129-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a parallel implementation of a fixed-point algorithm for finding the solution of the total variation model for phase demodulation. The total variation model is efficient in estimating discontinuous phase maps, background illumination, and amplitude modulation from a single fringe pattern. The implementations include execution in a multi-core CPU and a GPU using OpenMP and CUDA, respectively. We show performance comparisons of the parallel implementations with 64-bit and 32-bit precision floating-point numbers using synthetic and real experimental data. Results show that our parallel implementations achieve speedups over the serial implementation of 9x for multi-core CPU and 103x for GPU.},
  archive      = {J_JRTIP},
  author       = {Hernandez-Lopez, Francisco J. and Legarda-Sáenz, Ricardo and Brito-Loeza, Carlos},
  doi          = {10.1007/s11554-021-01129-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2441-2451},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel algorithm for fringe pattern demodulation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA-accelerated anisotropic diffusion filter based on
SW/HW-codesign for medical images. <em>JRTIP</em>, <em>18</em>(6),
2429–2440. (<a
href="https://doi.org/10.1007/s11554-021-01100-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical imaging, denoising is very important for the analysis of images and the diagnosis and treatment of diseases. Currently, the image denoising methods based on anisotropic diffusion are efficient. However, the methods have been limited as regards the processing time. In recent computing systems, the FPGA-based acceleration has been highly competitive for GPU-based one due to its high computation capabilities and lower energy consumption. In this paper, we present a high-level synthesis implementation on a SOC-FPGA of an anisotropic diffusion algorithm dedicated to medical applications. We choose an oriented speckle reducing anisotropic diffusion denoising filter, which provides robust performance but requires a significant computation on the embedded CPU since it is iterative. Moreover, we optimize the performance by modifying the original algorithm, automizing it by controlling the diffusion process at each iteration, and accelerating the processing operations by providing a hardware/software description. The evaluation is performed using different medical images. The efficiency and relevance of the proposed filter is demonstrated through segmentation. The design is validated on FPGA XC7Z020CLG484-1 with a frequency of 255 MHz and a PSNR of about 30 dB.},
  archive      = {J_JRTIP},
  author       = {Hadj Fredj, Amira and Malek, Jihene},
  doi          = {10.1007/s11554-021-01100-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2429-2440},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-accelerated anisotropic diffusion filter based on SW/HW-codesign for medical images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A zoom tracking algorithm based on defocus difference.
<em>JRTIP</em>, <em>18</em>(6), 2417–2428. (<a
href="https://doi.org/10.1007/s11554-021-01133-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image clarity evaluation function was commonly used by the autofocus algorithm to evaluate the clarity of the image. And autofocus algorithms like peak search algorithm and zoom tracking algorithm are based on the evaluation value. This paper proposes an Improved Feedback Zoom Tracking method (IFZT) based on defocus difference, using the amount of defocus difference as the degree of image blur. IFZT algorithm modifies the revision criterion for feedback revision point and removes the relatively complex PID algorithm. IFZT determines the orientation of the in-focus motor position according to the amount of defocus difference and uses the depth of defocus method to estimate the ideal focus position. In this paper, the calculation formula of defocus difference and ideal focus position are deduced. Finally, the algorithm was experimented on an integrated camera; the experimental results show that: IFZT algorithm using the amount of defocus has good tracking accuracy, and has a larger promotion compared with other zoom tracking algorithms. And the overall performance of IFZT algorithm is in line with the requirements of zoom tracking algorithm.},
  archive      = {J_JRTIP},
  author       = {Wang, Xuanyin and Zhu, Yanyu and Ji, Jiayu},
  doi          = {10.1007/s11554-021-01133-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2417-2428},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A zoom tracking algorithm based on defocus difference},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and implementation of a radar and camera-based
obstacle classification system using machine-learning techniques.
<em>JRTIP</em>, <em>18</em>(6), 2403–2415. (<a
href="https://doi.org/10.1007/s11554-021-01117-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road safety is an essential issue of modern life that must be tackled and resolved. Using AI technology to develop autonomous vehicles and driver-assistant systems is a promising approach to reduce accidents and preserve user’s security. In this regard, obstacle detection and identification have been a topic of much concern for researchers over the last few years. In this paper, we propose an embedded system that operates on low-level, lightweight algorithms, based on two types of data, namely, radar signals and camera images with the purpose of identifying and classifying obstacles on the road. The proposed system has two major contributions. The first is the use of machine-learning methods alongside signal processing techniques to optimize the overall computing performance and efficiency. Then, the second contribution consists of the use of the dynamic reconfiguration feature using DSP48 instead of standard CLBs to improve surface usage. The overall system was developed on Xilinx Zedboard Zynq-7000 FPGA.},
  archive      = {J_JRTIP},
  author       = {Dhouioui, Mohamed and Frikha, Tarek},
  doi          = {10.1007/s11554-021-01117-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2403-2415},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Design and implementation of a radar and camera-based obstacle classification system using machine-learning techniques},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lightweight tiny-YOLOv3 vehicle detection approach.
<em>JRTIP</em>, <em>18</em>(6), 2389–2401. (<a
href="https://doi.org/10.1007/s11554-021-01131-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, vehicle detection from video sequences has been one of the important tasks in intelligent transportation systems and is used for detection and tracking of the vehicles, capturing their violations, and controlling the traffic. This paper focuses on a lightweight real-time vehicle detection model developed to run on common computing devices. This method can be developed on low power systems (e.g. devices without GPUs or low power GPU modules), relying on the proposed real-time lightweight algorithm. The system employs an end-to-end approach for identifying, locating, and classifying vehicles in the images. The pre-trained Tiny-YOLOv3 network is adopted as the main reference model and subsequently pruned and simplified by training on the BIT-vehicle dataset, and excluding some of the unnecessary layers. The results indicated advantages of the proposed method in terms of accuracy and speed. Also, the network is capable to detect and classify six different types of vehicles with MAP = 95.05%, at the speed of 17 fps. Hence, it is about two times faster than the original Tiny-YOLOv3 network.},
  archive      = {J_JRTIP},
  author       = {Taheri Tajar, Alireza and Ramazani, Abbas and Mansoorizadeh, Muharram},
  doi          = {10.1007/s11554-021-01131-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2389-2401},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight tiny-YOLOv3 vehicle detection approach},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient real-time defect detection for spillway tunnel
using deep learning. <em>JRTIP</em>, <em>18</em>(6), 2377–2387. (<a
href="https://doi.org/10.1007/s11554-021-01130-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spillway tunnel eroded by high-speed water for a long time is prone to the rebar-exposed defects. Therefore, regular defect detection is very important for the safety of the hydropower station. The images of spillway tunnel are obtained by erecting scaffolding, and then the defects are manually recognized. This traditional method has some disadvantages such as high risk, inefficiently, time consumption and strong subjectivity. To improve the efficiency of defect detection, a real-time method is proposed for spillway tunnel defect detection (STDD) using deep learning. First, images of a spillway tunnel are collected by an Unmanned Aerial Vehicle (UAV) system and raw images are cropped and labeled to create a dataset of rebar-exposed defects. Then, the lightweight STDD network is developed using separable convolution and asymmetric convolution, and the network is trained and tested on the dataset. To evaluate the performance of STDD network, a comparative experiment is conducted with other networks. The results show that the STDD network has better detection performance. For defect segmentation, the recall, precision, F1 and mean intersection over union (mIoU) are 89.92%, 93.48%, 91.59%, and 91.73%, respectively. The STDD network has 1.7 M parameters, and the average inference time is 14.08 ms. In summary, the proposed STDD network achieves accurate and real-time defect detection for spillway tunnel, which can provide reliable support for the structure safety evaluation.},
  archive      = {J_JRTIP},
  author       = {Feng, Chuncheng and Zhang, Hua and Li, Yonglong and Wang, Shuang and Wang, Haoran},
  doi          = {10.1007/s11554-021-01130-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2377-2387},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient real-time defect detection for spillway tunnel using deep learning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerated superpixel image segmentation with a
parallelized DBSCAN algorithm. <em>JRTIP</em>, <em>18</em>(6),
2361–2376. (<a
href="https://doi.org/10.1007/s11554-021-01128-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of an image into superpixel clusters is a necessary part of many imaging pathways. In this article, we describe a new routine for superpixel image segmentation (F-DBSCAN) based on the DBSCAN algorithm that is six times faster than previous existing methods, while being competitive in terms of segmentation quality and resistance to noise. The gains in speed are achieved through efficient parallelization of the cluster search process by limiting the size of each cluster thus enabling the processes to operate in parallel without duplicating search areas. Calculations are performed in large consolidated memory buffers which eliminate fragmentation and maximize memory cache hits thus improving performance. When tested on the Berkeley Segmentation Dataset, the average processing speed is 175 frames/s with a Boundary Recall of 0.797 and an Achievable Segmentation Accuracy of 0.944.},
  archive      = {J_JRTIP},
  author       = {Loke, Seng Cheong and MacDonald, Bruce A. and Parsons, Matthew and Wünsche, Burkhard Claus},
  doi          = {10.1007/s11554-021-01128-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2361-2376},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accelerated superpixel image segmentation with a parallelized DBSCAN algorithm},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time implementation of fast discriminative scale space
tracking algorithm. <em>JRTIP</em>, <em>18</em>(6), 2347–2360. (<a
href="https://doi.org/10.1007/s11554-021-01119-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object tracking is an important step of many modern image processing applications. The efficient hardware design of real-time object tracker must achieve the desired accuracy while satisfying the frame rate requirements for a variety of image sizes. The existing methods of visual tracking employ sophisticated algorithms and challenge the capabilities of most embedded architectures. Discriminative scale space tracking is one algorithm that is capable of demonstrating good performance with affordable complexity. It has a high degree of parallelism which can be exploited for efficient implementation of reconfigurable hardware architectures. This paper proposes a real-time implementation of the discriminative scale-space tracker on FPGA for the major blocks. A careful design exploration of core mathematical operations of the tracking algorithm is performed to improve their hardware utilization and timing performance. Among the core functional units optimized in this work, the discrete Fourier transform achieves a computational time improvement of 92% relative to existing works, QR factorization achieves a 2.3 $$\times$$ reduction in resource utilization, and singular value decomposition yields a 3.8 $$\times$$ improvement in processing time. The proposed data path architecture is designed using Vivado HLS tool set and implemented for Zync Zed Board (xc7z020clg484-1). For an input image size of 320 $$\times$$ 240, the proposed architecture achieves a mean 25.38 fps.},
  archive      = {J_JRTIP},
  author       = {Walid, Walid and Awais, Muhammad and Ahmed, Ashfaq and Masera, Guido and Martina, Maurizio},
  doi          = {10.1007/s11554-021-01119-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2347-2360},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time implementation of fast discriminative scale space tracking algorithm},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedded real-time infrared and visible image fusion for UAV
surveillance. <em>JRTIP</em>, <em>18</em>(6), 2331–2345. (<a
href="https://doi.org/10.1007/s11554-021-01111-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion is a beneficial processing task for Unmanned Aerial Vehicle (UAV) surveillance, which can improve visibility by combining the advantages of the infrared camera and the visible light camera. An embedded onboard solution is necessary for UAV-based surveillance missions because it reduces the amount of data that are transmitted to the ground. In this paper, we propose an infrared and visible light image fusion method and implement it on two platforms with commonly used HW accelerators for embedded vision applications: Zedboard (ARM + FPGA) and NVIDIA TX1 (ARM + GPU), and compare their performances. To verify the usefulness of image fusion, we carry out sufficient experiments to prove that image fusion can improve the target detection ability of a UAV in different scenes. The detection rate for target detection is up to 0.926 in our experiments. The execution times on the ZedBoard and the TX1 are, respectively, 205.3 FPS and 36.6 FPS (38 $$\times$$ and 6.7 $$\times$$ in comparison to an ARM Cortex-A9 processor). Our results also show that the ZedBoard achieves an energy/frame reduction ratio of 7.1 $$\times$$ and 18.9 $$\times$$ respectively compared to the TX1 and the ARM CPU. This work is based on a UAV platform designed by ourselves, and all image sets are real scenes that we have captured. This demonstrates that the proposed method is viable and reflects the actual needs of real UAV surveillance systems.},
  archive      = {J_JRTIP},
  author       = {Li, Jun and Peng, Yuanxi and Jiang, Tian},
  doi          = {10.1007/s11554-021-01111-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2331-2345},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Embedded real-time infrared and visible image fusion for UAV surveillance},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time deep learning forest fire monitoring algorithm
based on an improved pruned + KD model. <em>JRTIP</em>, <em>18</em>(6),
2319–2329. (<a
href="https://doi.org/10.1007/s11554-021-01124-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the needs of embedded intelligent forest fire monitoring systems using an unmanned aerial vehicles (UAV), a deep learning fire recognition algorithm based on model compression and lightweight requirements is proposed in this study. The algorithm for the lightweight MobileNetV3 model was developed to reduce the complexity of the conventional YOLOv4 network structure. The redundant channels are eliminated through channel-level sparsity-induced regularization. The knowledge distillation algorithm is used to improve the detection accuracy of the pruned model. The experimental results reveal that the number of model parameters for the proposed architecture is only 2.64 million—compared with YOLOv4, this represents a reduction of nearly 95.87%. The inference time decreased from 153.8 to 37.4 ms, a reduction of nearly 75.68%. Our approach shows the advantages of a model with a smaller number of parameters, low memory requirements and fast inference speed compared with existing algorithms. The method presented in this paper is specifically tailored for use as a deep learning forest fire monitoring system on a UAV platform.},
  archive      = {J_JRTIP},
  author       = {Wang, Shengying and Zhao, Jing and Ta, Na and Zhao, Xiaoye and Xiao, Mingxia and Wei, Haicheng},
  doi          = {10.1007/s11554-021-01124-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2319-2329},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time deep learning forest fire monitoring algorithm based on an improved pruned + KD model},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual models in 3D digital reconstruction: Detection and
analysis of symmetry. <em>JRTIP</em>, <em>18</em>(6), 2301–2318. (<a
href="https://doi.org/10.1007/s11554-021-01115-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cultural Heritage (CH) artifacts generally possess symmetry of reflection, rotation, translation and glide reflection in their shape. Similarity measures are used to determine complex 3D models where symmetry is considered to be one of the similarity signatures. This work presents the methodology of detecting symmetry in 3D objects based on the three techniques: (1) Eigenvalues and Eigenvectors, (2) local surface discontinuity, and (3) pixel orientation. In this work, these methods have been modified suitably for the detection of symmetry in CH artifacts. Among these methods, it is found that the first two methods yield better performance on the symmetry signature estimation of 98 percent for complex models and up to 100% for primitive models. The execution time of the proposed methods is compared with the state-of-the-art approaches available in the literature. Three levels of random 3D models available in the internet repository are analyzed for efficiency, performance and robustness. At each level, the accuracy of the Eigenvalue method and the local discontinuity method is found to be better than the pixel orientation method. The modified algorithms have been tested for better performance with F-score, robustness, and execution time with 3D benchmark dataset and cultural heritage dataset available in the literature. Future work shall be extended by applying the symmetry features as constraints for the effective search of CH artifacts in digital repositories.},
  archive      = {J_JRTIP},
  author       = {Gothandaraman, Rajkumar and Muthuswamy, Sreekumar},
  doi          = {10.1007/s11554-021-01115-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2301-2318},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Virtual models in 3D digital reconstruction: Detection and analysis of symmetry},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal and spatial feature based approaches in drowsiness
detection using deep learning technique. <em>JRTIP</em>, <em>18</em>(6),
2287–2299. (<a
href="https://doi.org/10.1007/s11554-021-01114-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drowsiness is a term which seems to be very simple but for a moment, it becomes a critical issue for many drivers and workers while they are performing their duty. Many people’s lives may collapse into trouble because of drowsiness. Therefore, such a real-time system is needed which can be easy to develop and configure for early as well as accurate drowsiness detection. As per requisite, we have adopted a large realistic dataset which includes 30 h video of 60 different participants in three classes, i.e. alert, low vigilant and drowsy. In our proposed work, we have selected the videos with extreme classes, i.e. alert and drowsy only. Further, we have designed two different models based on temporal and spatial feature by employing computer vision as well as deep-learning approach. In one model, temporal features are obtained by computer vision techniques followed by long short-term memory (LSTM) and the second model adopts spatial features extraction through convolution neural network (CNN) followed by LSTM. Although the temporal model is more complex and has less accuracy than spatial model, in spite of this, the study shows that the temporal model is far better in terms of training time than spatial model by establishing the comparison using confusion metrics and Area under Curve (AUC)–Receiver-Operating Characteristic Curve (ROC) score.},
  archive      = {J_JRTIP},
  author       = {Pandey, Nageshwar Nath and Muppalaneni, Naresh Babu},
  doi          = {10.1007/s11554-021-01114-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2287-2299},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Temporal and spatial feature based approaches in drowsiness detection using deep learning technique},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast SHVC inter-coding based on bayesian decision with
coding depth estimation. <em>JRTIP</em>, <em>18</em>(6), 2269–2285. (<a
href="https://doi.org/10.1007/s11554-021-01112-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scalable extension of the high efficiency video coding standard named SHVC supports flexible access for various terminals in heterogeneous networks. However, it is difficult to use in real-time scenarios because of the high complexity of the hierarchical coding structure. In this paper, a novel method for SHVC inter-coding is proposed to reduce the coding complexity in a manner that is compatible with quality scalability and spatial scalability. First, the depth range of the coding tree units is estimated from a reference table generated from a statistical probability distribution based on the correlation between the current coding unit (CU) and its adjacent CUs. Within this depth range, a fast CU partitioning method based on Bayesian minimum risk and a fast prediction unit (PU) selection method based on Bayesian maximum probability are adopted to improve time efficiency. Three different methods, namely, histogram estimation, Gaussian modelling and neighbouring prediction, are used to calculate the conditional probabilities for discrete or continuous features in the Bayesian decision method. The significant advantage of the proposed method is that the time savings in the enhancement layer for each sequence exceeds 60% with negligible quality loss.},
  archive      = {J_JRTIP},
  author       = {Lu, Yu and Huang, Xudong and Liu, Huaping and Yin, Haibing and Shen, Liquan},
  doi          = {10.1007/s11554-021-01112-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2269-2285},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast SHVC inter-coding based on bayesian decision with coding depth estimation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time face alignment: Evaluation methods, training
strategies and implementation optimization. <em>JRTIP</em>,
<em>18</em>(6), 2239–2267. (<a
href="https://doi.org/10.1007/s11554-021-01107-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face alignment is a crucial component in most face analysis systems. It focuses on identifying the location of several keypoints of the human faces in images or videos. Although several methods and models are available to developers in popular computer vision libraries, they still struggle with challenges such as insufficient illumination, extreme head poses, or occlusions, especially when they are constrained by the needs of real-time applications. Throughout this article, we propose a set of training strategies and implementations based on data augmentation, software optimization techniques that help in improving a large variety of models belonging to several real-time algorithms for face alignment. We propose an extended set of evaluation metrics that allow novel evaluations to mitigate the typical problems found in real-time tracking contexts. The experimental results show that the generated models using our proposed techniques are faster, smaller, more accurate, more robust in specific challenging conditions and smoother in tracking systems. In addition, the training strategy shows to be applicable across different types of devices and algorithms, making them versatile in both academic and industrial uses.},
  archive      = {J_JRTIP},
  author       = {Álvarez Casado, Constantino and Bordallo López, Miguel},
  doi          = {10.1007/s11554-021-01107-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2239-2267},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time face alignment: Evaluation methods, training strategies and implementation optimization},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and efficient recursive algorithm of meixner
polynomials. <em>JRTIP</em>, <em>18</em>(6), 2225–2237. (<a
href="https://doi.org/10.1007/s11554-021-01093-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meixner polynomials (MNPs) and their moments are considered significant feature extraction tools because of their salient representation in signal processing and computer vision. However, the existing recurrence algorithm of MNPs exhibits numerical instabilities of coefficients for high-order polynomials. This paper proposed a new recurrence algorithm to compute the coefficients of MNPs for high-order polynomials. The proposed algorithm is based on a derived identity for MNPs that reduces the number of the utilized recurrence times and the computed number of MNPs coefficients. To minimize the numerical errors, a new form of the recurrence algorithm is presented. The proposed algorithm computes $$\sim $$ 50% of the MNP coefficients. A comparison with different state-of-the-art algorithms is performed to evaluate the performance of the proposed recurrence algorithm in terms of computational cost and reconstruction error. In addition, an investigation is performed to find the maximum generated size. The results show that the proposed algorithm remarkably reduces the computational cost and increases the generated size of the MNPs. The proposed algorithm shows an average improvement of $$\sim $$ 77% in terms of computation cost. In addition, the proposed algorithm exhibits an improvement of $$\sim $$ 1269% in terms of generated size.},
  archive      = {J_JRTIP},
  author       = {Abdulhussain, Sadiq H. and Mahmmod, Basheera M.},
  doi          = {10.1007/s11554-021-01093-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2225-2237},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast and efficient recursive algorithm of meixner polynomials},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast SSD model based on parameter reduction and dilated
convolution. <em>JRTIP</em>, <em>18</em>(6), 2211–2224. (<a
href="https://doi.org/10.1007/s11554-021-01108-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning networks always compromise between speed and accuracy for their in-depth feature extraction. In this paper, we present a modified single shot multibox detector (SSD) model to achieve high speed while maintaining satisfactory accuracy for target detection. Firstly, the operational parameters are reduced by deleting the convolution layers and reducing the channels within. Thus, the parameters are reduced by 50% with a permissible precision loss, and the detection speed of the model is significantly improved. Secondly, a light multiple dilated convolution (LMDC) operator is introduced to compensate for the precision loss. The LMDC functions as a filter to extract global and semantic information from the feature map, thereby making feature information completer and more accurate. Moreover, to reduce the computation quantity and increase the computation efficiency of the network, the feature extraction and fusion of the convolution layer are separated. It transforms the complex multiplication into addition among the parameters. Finally, the LMDC-SSD is evaluated on 3 datasets for 300 × 300-sized inputs. It yields 98.99% mean average precision (mAP) and 85 frames per second for the apple datasets. The speed and accuracy are improved by 44% and 8.1%, respectively, compared to the original model. The speed and accuracy are improved by 0.99% and 65.71%, respectively, for the bicycle and person datasets.The speed and accuracy are improved by 0.26% and 112.9%, respectively, for the vehicle datasets. The experimental results have shown that the proposed LMDC-SSD is rather promising for detection with high detection speed and accuracy performance.},
  archive      = {J_JRTIP},
  author       = {Zhang, Xinliang and Xie, Heng and Zhao, Yunji and Qian, Wei and Xu, Xiaozhuo},
  doi          = {10.1007/s11554-021-01108-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2211-2224},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A fast SSD model based on parameter reduction and dilated convolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time automotive night-vision system for drivers to
inhibit headlight glare of the oncoming vehicles and enhance road
visibility. <em>JRTIP</em>, <em>18</em>(6), 2193–2209. (<a
href="https://doi.org/10.1007/s11554-021-01104-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vision problem of drivers during the night is mainly owing to the high-intensity headlight-beam of the oncoming vehicle from the reverse direction. It causes temporary blindness to the driver. To overcome this situation, people generally use color windshield glass, sun visor, and night-vision glass. But, these are not considered as the best solution since it decreases the light intensity of the entire view including the road. Many researchers used various image enhancement techniques to overcome this situation. But, the existing approaches are unable to dim the high-beam headlights of oncoming vehicles without affecting the road view. In this paper, a novel night-vision system is proposed to resolve the problem in real-time for manual-driving vehicles and autonomous vehicles. The proposed method includes region segmentation of frames, local enhancement techniques in different regions followed by adaptive Gaussian filtering. Pixels masking, gamma correction, and low-light pixel enhancement are applied to three distinct regions. Both autonomous vehicles and manual drivers can get a bright and a prominent view of the road with dim headlights of oncoming vehicles in real-time. Numerous heuristic real-time test reveals the performance superiority of the projected system compared to state-of-art methods in quantitative as well as qualitative point of view.},
  archive      = {J_JRTIP},
  author       = {Mandal, Gouranga and Bhattacharya, Diptendu and De, Parthasarathi},
  doi          = {10.1007/s11554-021-01104-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2193-2209},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time automotive night-vision system for drivers to inhibit headlight glare of the oncoming vehicles and enhance road visibility},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time traffic sign detection network using DS-DetNet and
lite fusion FPN. <em>JRTIP</em>, <em>18</em>(6), 2181–2191. (<a
href="https://doi.org/10.1007/s11554-021-01102-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection (TSD) using convolutional neural networks (CNN) is promising and intriguing for autonomous driving. Especially, with sophisticated large-scale CNN models, TSD can be performed with high accuracy. However, the conventional CNN models suffer the drawbacks of being time-consuming and resource-hungry, which limit their application and deployments in various platforms of limited resources. In this paper, we propose a novel real-time traffic sign detection system with a lightweight backbone network named Depth Separable DetNet (DS-DetNet) and a lite fusion feature pyramid network (LFFPN) for efficient feature fusion. The new model can achieve a performance trade-off between speed and accuracy using a depthwise separable bottleneck block, a lite fusion module, and an improved SSD detection front-end. The testing results on the MS COCO and the GTSDB datasets reveal that 23.1% mAP with 6.39 M parameters and only 1.08B FLOPs on MSCOCO, 81.35% mAP with 5.78 M parameters on GTSDB. With our model, the run speed is 61 frames per second (fps) on GTX 1080ti, 12 fps on Nvidia Jetson Nano and 16 fps on Nvidia Jetson Xavier NX.},
  archive      = {J_JRTIP},
  author       = {Ren, Kun and Huang, Long and Fan, Chunqi and Han, Honggui and Deng, Hai},
  doi          = {10.1007/s11554-021-01102-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2181-2191},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time traffic sign detection network using DS-DetNet and lite fusion FPN},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regressive rate-distortion trade-off with weighted entropy
coding for HEVC encoding. <em>JRTIP</em>, <em>18</em>(6), 2165–2180. (<a
href="https://doi.org/10.1007/s11554-021-01096-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-Efficiency Video Coding (HEVC) is the standard employed for the subsequent compressing of the video without degrading the quality of the image. HEVC renders effective performance compared to the existing compression standards as the encoding time is minimal. With the concern for quality and video compression, this paper proposes a modified version for HEVC encoding using the Ordered Tree-based Hex-Octagon based block Search and Rate-Distortion trade-off (OrTHO-Search-based RD) for motion estimation and weighted Context-Adaptive Binary Arithmetic Coding (CABAC). The proposed OrTHO-Search-based RD-dependent HEVC renders a good-quality video after compression with half the compression standard when compared with the other existing compression standards. In the motion estimation block, the OrTHO-Search is employed with a new RD trade-off in such a way that the Conditional Autoregressive Value at Risk concept modifies the existing RD trade-off measure. The bit rates of the proposed method are reduced with effective coding. The experimentation and analysis of the methods are performed using four videos from CIPR SIF Sequences and one video from Xiph.org Video Test Media datasets and the analysis based on Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), reveals that the proposed method acquired the maximal PSNR of 45.1132 dB, maximal SSIM of 0. 9918, and minimal computational time of 0.2135 min.},
  archive      = {J_JRTIP},
  author       = {Korishetti, Anilkumar Chandrashekhar and Malemath, V. S.},
  doi          = {10.1007/s11554-021-01096-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2165-2180},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Regressive rate-distortion trade-off with weighted entropy coding for HEVC encoding},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complexity-scalable HEVC-to-AV1 video transcoding based on
partition inheritance. <em>JRTIP</em>, <em>18</em>(6), 2151–2163. (<a
href="https://doi.org/10.1007/s11554-021-01101-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After the AOMedia Video 1 (AV1) bitstream specification was launched by the Alliance for Open Media, the need for converting legacy content encoded with the state-of-the-art High Efficiency Video Coding (HEVC) standard to the new format has arisen in several scenarios. However, transcoding is a complex task composed of a decoding and an encoding process in sequence, which requires long processing times and high energy consumption. This paper presents a complexity-scalable HEVC-to-AV1 transcoding scheme, which comprises 25 configuration modes chosen based on a Pareto Optimization strategy and built upon the correlation between block size decisions in HEVC and AV1. The configurations allow the AV1 encoder to inherit partitioning information from the HEVC bitstream to constrain the AV1 reencoding process, thus speeding up the transcoding operation. Experimental results showed that the proposed transcoding complexity reduction ranges from 0.41 up to 35.06%, on average, with coding efficiency losses that vary between 0.0536 and 5.3801%.},
  archive      = {J_JRTIP},
  author       = {Borges, Alex and Zatt, Bruno and Porto, Marcelo and Correa, Guilherme},
  doi          = {10.1007/s11554-021-01101-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2151-2163},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Complexity-scalable HEVC-to-AV1 video transcoding based on partition inheritance},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cryptanalysis of a secure image encryption scheme based on a
novel 2D sine–cosine cross‑chaotic map. <em>JRTIP</em>, <em>18</em>(6),
2135–2149. (<a
href="https://doi.org/10.1007/s11554-021-01091-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a new secure image encryption scheme was proposed based on a novel 2D sine–cosine cross‑chaotic map. The authors claimed that the encryption scheme they proposed was secure enough to resist all the existing cryptanalytic attacks. However, the multi-round original cryptosystem with permutation–bit diffusion structure (PBDS) was found to have three security vulnerabilities and was proved to be equivalent to the one-round cryptosystem with PBDS, which would lead to the original cryptosystem being successfully cracked by chosen-plaintext attack (CPA). Subsequently, two efficient and feasible attack schemes were proposed in this paper, and they require $$\left\lceil {\log_{2} W \times H} \right\rceil + 1$$ and $$\left\lceil {\log_{256} W \times H} \right\rceil + 1$$ times of CPA in total, respectively. Simulation and experimental results have verified the effectiveness and feasibility of the two attack schemes.},
  archive      = {J_JRTIP},
  author       = {Li, Ming and Wang, Pengcheng and Yue, Yange and Liu, Yanfang},
  doi          = {10.1007/s11554-021-01091-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2135-2149},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Cryptanalysis of a secure image encryption scheme based on a novel 2D sine–cosine cross‑chaotic map},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time FPGA-based implementation of the AKAZE algorithm
with nonlinear scale space generation using image partitioning.
<em>JRTIP</em>, <em>18</em>(6), 2123–2134. (<a
href="https://doi.org/10.1007/s11554-021-01089-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first step in a scale invariant image matching system is scale space generation. Nonlinear scale space generation algorithms such as AKAZE, reduce noise and distortion in different scales while retaining the borders and key-points of the image. An FPGA-based hardware architecture for AKAZE nonlinear scale space generation is proposed to speed up this algorithm for real-time applications. The three contributions of this work are (1) mapping the two passes of the AKAZE algorithm onto a hardware architecture that realizes parallel processing of multiple sections, (2) multi-scale line buffers which can be used for different scales, and (3) a time-sharing mechanism in the memory management unit to process multiple sections of the image in parallel. We propose a time-sharing mechanism for memory management to prevent artifacts as a result of separating the process of image partitioning. We also use approximations in the algorithm to make hardware implementation more efficient while maintaining the repeatability of the detection. A frame rate of 304 frames per second for a $$1280 \times 768$$ image resolution is achieved which is favorably faster in comparison with other work.},
  archive      = {J_JRTIP},
  author       = {Soleimani, Parastoo and Capson, David W. and Li, Kin Fun},
  doi          = {10.1007/s11554-021-01089-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2123-2134},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time FPGA-based implementation of the AKAZE algorithm with nonlinear scale space generation using image partitioning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient convolutional neural network with multi-kernel
enhancement features for real-time facial expression recognition.
<em>JRTIP</em>, <em>18</em>(6), 2111–2122. (<a
href="https://doi.org/10.1007/s11554-021-01088-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are the most direct external manifestation of personal emotions. Different from other pattern recognition problems, the feature difference between facial expressions is smaller. The general methods are difficult to effectively characterize the feature difference, or their parameters are too large to realize real-time processing. This paper proposes a lightweight mobile architecture and a multi-kernel feature facial expression recognition network, which can take into account the speed and accuracy of real-time facial expression recognition. First, a multi-kernel convolution block is designed by using three depthwise separable convolution kernels of different sizes in parallel. The small and the large kernels can extract local details and edge contour information of facial expressions, respectively. Then, the multi-channel information is fused to obtain multi-kernel enhancement features to better describe the differences between facial expressions. Second, a &quot;Channel Split&quot; operation is performed on the input of the multi-kernel convolution block, which can avoid repeated extraction of invalid information and reduce the amount of parameters to one-third of the original. Finally, a lightweight multi-kernel feature expression recognition network is designed by alternately using multi-kernel convolution blocks and depthwise separable convolutions to further improve the feature representation ability. Experimental results show that the proposed network achieves high accuracy of 73.3 and 99.5% on FER-2013 and CK + datasets, respectively. Furthermore, it achieves a speed of 78 frames per second on 640 × 480 video. It is superior to other state-of-the-art methods in terms of speed and accuracy.},
  archive      = {J_JRTIP},
  author       = {Li, Minze and Li, Xiaoxia and Sun, Wei and Wang, Xueyuan and Wang, Shunli},
  doi          = {10.1007/s11554-021-01088-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2111-2122},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient convolutional neural network with multi-kernel enhancement features for real-time facial expression recognition},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Passive synthetic aperture radar imaging using kalman
reflection coefficients estimation algorithm with DVB-t signal.
<em>JRTIP</em>, <em>18</em>(6), 2097–2109. (<a
href="https://doi.org/10.1007/s11554-021-01075-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new method is presented for image formation in a Multistatic Passive Synthetic Aperture Radar (SAR), whose receiver is moving and transmitters are stationary in a multistatic structure, and the signal used to create the image is DVB-T. The proposed method uses multiple transmitters of opportunity to improve the range resolution of a Passive SAR. First, a linear observation model is developed for multistatic structure of the Passive SAR system. Then, the image of the underlying scene is obtained through a new Kalman-based estimation algorithm. In this algorithm, range migration error is completely corrected, and the use of Kalman recursive filter reduces the computational complexity of the reflection coefficients estimation algorithm. Experimental results indicate that the proposed algorithm is effective.},
  archive      = {J_JRTIP},
  author       = {Ansari, Farzad and Samadi, Sadegh and Mohseni, Reza},
  doi          = {10.1007/s11554-021-01075-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2097-2109},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Passive synthetic aperture radar imaging using kalman reflection coefficients estimation algorithm with DVB-T signal},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time video smoke detection algorithm based on kalman
filter and CNN. <em>JRTIP</em>, <em>18</em>(6), 2085–2095. (<a
href="https://doi.org/10.1007/s11554-021-01094-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoke detection represents a critical task for avoiding large scale fire disaster in industrial environment and cities. Including intelligent video-based techniques in existing camera infrastructure enables faster response time if compared to traditional analog smoke detectors. In this work presents a hybrid approach to assess the rapid and precise identification of smoke in a video sequence. The algorithm combines a traditional feature detector based on Kalman filtering and motion detection, and a lightweight shallow convolutional neural network. This technique allows the automatic selection of specific regions of interest within the image by the generation of bounding boxes for gray colored moving objects. In the final step the convolutional neural network verifies the actual presence of smoke in the proposed regions of interest. The algorithm provides also an alarm generator that can trigger an alarm signal if the smoke is persistent in a time window of 3 s. The proposed technique has been compared to the state of the art methods available in literature by using several videos of public and non-public dataset showing an improvement in the metrics. Finally, we developed a portable solution for embedded systems and evaluated its performance for the Raspberry Pi 3 and the Nvidia Jetson Nano.},
  archive      = {J_JRTIP},
  author       = {Gagliardi, Alessio and de Gioia, Francesco and Saponara, Sergio},
  doi          = {10.1007/s11554-021-01094-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2085-2095},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time video smoke detection algorithm based on kalman filter and CNN},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variant center-symmetric census transform for real-time
stereo vision architecture on chip. <em>JRTIP</em>, <em>18</em>(6),
2073–2083. (<a
href="https://doi.org/10.1007/s11554-021-01087-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo vision is one of the most crucial operations in many computer vision applications, and stereo matching is its most important step. In recent years, stereo matching has developed in the direction of increasing accuracy and higher computational speed, and the real-time stereo matching algorithm based on hardware architecture has been increasingly emphasized due to its applicability in embedded systems. The most frequently used method on FPGA is the census transform (CT) because of its simple structure and easy parallelization and the high quality of the generated disparity maps, but CT has the drawbacks of mismatches in some regions and dependence on a central pixel. In this paper, an improved CT-based semi-global stereo matching algorithm with pipeline and parallel operation based on FPGA is proposed in order to increase matching accuracy in specific regions while satisfying real-time constraints. In the matching cost step, pixels are divided into two parts, with the two methods calculated to generate the bit-vector feature. A four-path semi-global matching algorithm with twice aggregate is proposed in the cost aggregation stage. The left–right check, mismatching point filling, and the median filter to enhance the final disparity map are also required. The novel algorithm is evaluated on the Middleburry benchmark and implemented on Xilinx ZYNQ-7000 SoCs, which results in a throughput of 640 × 480/60 fps, with 64 disparity levels at 100 MHz. Compared with the related work, we improve the average accuracy by 1.61%, making our approach suitable for real-time embedded systems.},
  archive      = {J_JRTIP},
  author       = {Zhao, Chenyuan and Li, Wenxin and Zhang, Qingxi},
  doi          = {10.1007/s11554-021-01087-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2073-2083},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Variant center-symmetric census transform for real-time stereo vision architecture on chip},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smart and real-time image dehazing on mobile devices.
<em>JRTIP</em>, <em>18</em>(6), 2063–2072. (<a
href="https://doi.org/10.1007/s11554-021-01085-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze is one of the common factors that degrades the visual quality of the images and videos. This diminishes contrast and reduces visual efficiency. The ALS (Atmospheric light scattering) model which has two unknowns to be estimated from the scene: atmospheric light and transmission map, is commonly used for dehazing. The process of modelling the atmospheric light scattering is complex and estimation of scattering is time consuming. This condition makes dehazing in real-time difficult. In this work, a new approach is employed for dehazing in real time which reads the orientation sensor of mobile device and compares the amount of rotation with a pre-specified threshold. The system decides whether to recalculate the atmospheric light or not. When the amount of rotation is little means there are only subtle changes to the scene, it uses the pre-estimated atmospheric light. Therefore, the system does not need to recalculate it at each time instant and this approach accelerates the overall dehazing process. 0.07 s fps (frame per second) per frame processing time (~ 15 fps) is handled for 360p imagery. Frame processing time results show that our approach is superior to the state-of-the-art real-time dehazing implementations on mobile operating systems.},
  archive      = {J_JRTIP},
  author       = {Cimtay, Yucel},
  doi          = {10.1007/s11554-021-01085-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2063-2072},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Smart and real-time image dehazing on mobile devices},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Free-size accelerated kuwahara filter. <em>JRTIP</em>,
<em>18</em>(6), 2049–2062. (<a
href="https://doi.org/10.1007/s11554-021-01081-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kuwahara filter is a smoothing filter used in image processing for adaptive noise reduction that has the ability to preserve object edges. Applications for this filter exist in fields such as medical imaging and artistic imaging. However, it has a very high computational cost, especially when filter size is large. In this paper, we propose an efficient algorithm to accelerate this filter regardless of filter size. Our method uses Summed-area Table (SAT) to gain fast computation of mean and variance values in Kuwahara filter. After that, three acceleration methods, namely caching mean and variance values, memory access optimization and flexible-format SATs are proposed to optimize the SAT-based Kuwahara filter. The experiments show that our method achieves the lowest possible big-O time complexity and performs at around the same level regardless of filter size, while producing the exact same output image as the original method. The achieved speedup ratio grows quadratically with the filter size, ranging from 10x to 1000x and even more. As a result, our optimized filter can run at a high frame rate even when the filter is large.},
  archive      = {J_JRTIP},
  author       = {Le, Huy Duc and Tran, Giang Son},
  doi          = {10.1007/s11554-021-01081-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2049-2062},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Free-size accelerated kuwahara filter},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA implementation of the JPEG XR for onboard
earth-observation applications. <em>JRTIP</em>, <em>18</em>(6),
2037–2048. (<a
href="https://doi.org/10.1007/s11554-021-01078-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a more recent option for low-complexity compression, the transform-based JPEG XR standard accepts an extended range of input formats and provides good low-loss compression. Based on the performance in software of the JPEG XR compression to process remote-sensing images and foreseeing applications onboard small earth-observation satellites, the JPEG XR algorithm was implemented and evaluated in FPGA. The VHDL code was developed and evaluated in two parts, the transform/quantization/prediction and the DC/LP/HP encoding, which were then merged into the complete JPEG XR description. The results of the real-time evaluation were compared to the predictive-differential JPEG LS compression, in a similar hardware test setup, and also to other JPEG XR hardware implementations, and showed up a good trade-off between a throughput of 16 Mpix/s and a power consumption of about 180 mW, at a 50 MHz clock frequency. Better results were estimated for a more recent FPGA version at a higher frequency.},
  archive      = {J_JRTIP},
  author       = {Lopes Filho, Antonio and d’Amore, Roberto},
  doi          = {10.1007/s11554-021-01078-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2037-2048},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of the JPEG XR for onboard earth-observation applications},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine vision for low-cost remote control of mosquitoes by
power laser. <em>JRTIP</em>, <em>18</em>(6), 2027–2036. (<a
href="https://doi.org/10.1007/s11554-021-01079-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an innovative and effective method for remote monitoring of mosquitoes and their neutralization. We explain in detail how we leverage modern advances in neural networks to use a powerful laser to neutralize mosquitoes. The paper presented the experimental low-cost prototype for mosquito control, which uses a powerful laser to thermally neutralize the mosquitoes. The developed device is controlled by a single-board computer based on the neural network. The paper demonstrated experimental research for mosquito neutralization during which, to maximize approximation to natural conditions, simulation of various working conditions was conducted. The manuscript showed that a low-cost device can be used to kill mosquitoes with a powerful laser.},
  archive      = {J_JRTIP},
  author       = {Ildar, Rakhmatulin},
  doi          = {10.1007/s11554-021-01079-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2027-2036},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Machine vision for low-cost remote control of mosquitoes by power laser},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time FPGA implementation of a secure chaos-based
digital crypto-watermarking system in the DWT domain using co-design
approach. <em>JRTIP</em>, <em>18</em>(6), 2009–2025. (<a
href="https://doi.org/10.1007/s11554-021-01073-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper a new approach for designing invisible non-blind full crypto-watermarking system targeting images security on FPGA platform is presented. This new design is based on the Hardware-Software co-design approach using the High-Level Synthesis (HLS) tool of Xilinx which allows a good compromise between development time and performances. For a better authentication and robustness of the proposed system, the Discrete Wavelet Transform (DWT) is employed. To more enhance the security level, a new chaos-based generator proposed is integrated into a stream cipher algorithm in order to encrypt and decrypt the watermark during the insertion and extraction phases.This approach allows a better secure access at the positions of the watermark and to distribute the watermark evenly throughout the image. Three novel customized Intellectual Property (IP) cores designed under HLS tool, implementing Haar DWT and the new chaos-based key generator, have been generated, tested, and validated. The generated Register Transfer Level-IP (RTL-IP) cores are integrated into a Vivado library that achieves real-time secured watermarking operations for both embedding and extraction processes. The system has been evaluated using the main metrics in terms of imperceptibility of the produced watermarked images achieving a Peak Signal to Noise Ratio (PSNR) of 47 dB, robustness against most geometric and image processing attacks achieving a Normalized Cross-Correlation (NCC) of 0.99. The proposed crypt-watermarking system allows a good solution against brute force attack which produce a huge key-space of $$2^{768}$$ . Finally, the implementation offers a good efficiency value of 0.19 MHz/LUT in terms of FPGA resource consumption and speed, making the system a reliable choice for real sensitive embedded applications.},
  archive      = {J_JRTIP},
  author       = {Kaibou, Redouane and Azzaz, Mohamed Salah and Benssalah, Mustapha and Teguig, Djamel and Hamil, Hocine and Merah, Amira and Akrour, Meriam Tinhinane},
  doi          = {10.1007/s11554-021-01073-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2009-2025},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time FPGA implementation of a secure chaos-based digital crypto-watermarking system in the DWT domain using co-design approach},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smart surveillance system for real-time multi-person
multi-camera tracking at the edge. <em>JRTIP</em>, <em>18</em>(6),
1993–2007. (<a
href="https://doi.org/10.1007/s11554-020-01066-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we have presented an end-to-end multi-person multi-camera tracking (MPMCT) surveillance system and implemented it on edge analytics platform for real-time performance. The proposed MPMCT framework is both privacy-aware and scalable supporting a processing pipeline on the edge consisting of person detection, tracking and robust person re-identification. A realistic and large dataset has been created to train and evaluate the surveillance system that has been employed to track people inside the institute campus throughout the entire day. Appropriate deep-learning algorithms and real-time implementation strategies have been employed to realize the MPMCT system on NVIDIA Jetson TX2 embedded platform with real-time performance. The proposed system has an IDF1 score of 90.97 on our dataset and outperforms the current state-of-the-art real-time algorithms. The performance up to 30 FPS is achieved for the person detection algorithm, whereas an average latency of 90 ms is achieved for the re-identification algorithm.},
  archive      = {J_JRTIP},
  author       = {Gaikwad, Bipin and Karmakar, Abhijit},
  doi          = {10.1007/s11554-020-01066-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1993-2007},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Smart surveillance system for real-time multi-person multi-camera tracking at the edge},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating iterative CT reconstruction algorithms using
tensor cores. <em>JRTIP</em>, <em>18</em>(6), 1979–1991. (<a
href="https://doi.org/10.1007/s11554-020-01069-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor Cores are specialized hardware units added to recent NVIDIA GPUs to speed up matrix multiplication-related tasks, such as convolutions and densely connected layers in neural networks. Due to their specific hardware implementation and programming model, Tensor Cores cannot be straightforwardly applied to other applications outside machine learning. In this paper, we demonstrate the feasibility of using NVIDIA Tensor Cores for the acceleration of a non-machine learning application: iterative Computed Tomography (CT) reconstruction. For large CT images and real-time CT scanning, the reconstruction time for many existing iterative reconstruction methods is relatively high, ranging from seconds to minutes, depending on the size of the image. Therefore, CT reconstruction is an application area that could potentially benefit from Tensor Core hardware acceleration. We first studied the reconstruction algorithm’s performance as a function of the hardware related parameters and proposed an approach to accelerate reconstruction on Tensor Cores. The results show that the proposed method provides about 5 $$\times $$ increase in speed and energy saving using the NVIDIA RTX 2080 Ti GPU for the parallel projection of 32 images of size $$512\times 512$$ . The relative reconstruction error due to the mixed-precision computations was almost equal to the error of single-precision (32-bit) floating-point computations. We then presented an approach for real-time and memory-limited applications by exploiting the symmetry of the system (i.e., the acquisition geometry). As the proposed approach is based on the conjugate gradient method, it can be generalized to extend its application to many research and industrial fields.},
  archive      = {J_JRTIP},
  author       = {Nourazar, Mohsen and Goossens, Bart},
  doi          = {10.1007/s11554-020-01069-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1979-1991},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accelerating iterative CT reconstruction algorithms using tensor cores},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved one-stage pedestrian detection method based on
multi-scale attention feature extraction. <em>JRTIP</em>,
<em>18</em>(6), 1965–1978. (<a
href="https://doi.org/10.1007/s11554-021-01074-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the performance of the convolutional neural network-based pedestrian detection method has improved significantly. However, an imbalance remains between detection accuracy and speed. In this paper, we employ a one-stage object detection framework and propose a pedestrian detection method based on the multi-scale attention mechanism of a convolutional neural network to improve the imbalance between accuracy and speed. First, a multi-scale convolution module is designed to extract corresponding features at different scales. Second, using the attention module, association information between features is mined from space and channel perspectives to strengthen the original features. Then, the enhanced features are passed through a classification and regression module to perform object positioning and bounding box regression. Finally, to learn more pedestrian location information, we improve the loss function to realise better network training. The proposed method achieved considerable results on the challenging CityPersons and Caltech pedestrian detection datasets.},
  archive      = {J_JRTIP},
  author       = {Ma, Jun and Wan, Honglin and Wang, Junxia and Xia, Hao and Bai, Chengjie},
  doi          = {10.1007/s11554-021-01074-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1965-1978},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved one-stage pedestrian detection method based on multi-scale attention feature extraction},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction to: A compression pipeline for one-stage object
detection model. <em>JRTIP</em>, <em>18</em>(6), 1963–1964. (<a
href="https://doi.org/10.1007/s11554-021-01082-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The original article can be found online.},
  archive      = {J_JRTIP},
  author       = {Li, Zhishan and Sun, Yiran and Tian, Guanzhong and Xie, Lei and Liu, Yong and Su, Hongye and He, Yifan},
  doi          = {10.1007/s11554-021-01082-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1963-1964},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: A compression pipeline for one-stage object detection model},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A compression pipeline for one-stage object detection
model. <em>JRTIP</em>, <em>18</em>(6), 1949–1962. (<a
href="https://doi.org/10.1007/s11554-020-01053-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have strong fitting ability on a variety of computer vision tasks, but they also require intensive computing power and large storage space, which are not always available in portable smart devices. Although a lot of studies have contributed to the compression of image classification networks, there are few model compression algorithms for object detection models. In this paper, we propose a general compression pipeline for one-stage object detection networks to meet the real-time requirements. Firstly, we propose a softer pruning strategy on the backbone to reduce the number of filters. Compared with original direct pruning, our method can maintain the integrity of network structure and reduce the drop of accuracy. Secondly, we transfer the knowledge of the original model to the small model by knowledge distillation to reduce the accuracy drop caused by pruning. Finally, as edge devices are more suitable for integer operations, we further transform the 32-bit floating point model into the 8-bit integer model through quantization. With this pipeline, the model size and inference time are compressed to 10% or less of the original, while the mAP is only reduced by 2.5% or less. We verified that performance of the compression pipeline on the Pascal VOC dataset.},
  archive      = {J_JRTIP},
  author       = {Li, Zhishan and Sun, Yiran and Tian, Guanzhong and Xie, Lei and Liu, Yong and Su, Hongye and He, Yifan},
  doi          = {10.1007/s11554-020-01053-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1949-1962},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A compression pipeline for one-stage object detection model},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Implementing a real-time, AI-based, people detection and
social distancing measuring system for covid-19. <em>JRTIP</em>,
<em>18</em>(6), 1937–1947. (<a
href="https://doi.org/10.1007/s11554-021-01070-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a disease caused by a severe respiratory syndrome coronavirus. It was identified in December 2019 in Wuhan, China. It has resulted in an ongoing pandemic that caused infected cases including many deaths. Coronavirus is primarily spread between people during close contact. Motivating to this notion, this research proposes an artificial intelligence system for social distancing classification of persons using thermal images. By exploiting YOLOv2 (you look at once) approach, a novel deep learning detection technique is developed for detecting and tracking people in indoor and outdoor scenarios. An algorithm is also implemented for measuring and classifying the distance between persons and to automatically check if social distancing rules are respected or not. Hence, this work aims at minimizing the spread of the COVID-19 virus by evaluating if and how persons comply with social distancing rules. The proposed approach is applied to images acquired through thermal cameras, to establish a complete AI system for people tracking, social distancing classification, and body temperature monitoring. The training phase is done with two datasets captured from different thermal cameras. Ground Truth Labeler app is used for labeling the persons in the images. The proposed technique has been deployed in a low-cost embedded system (Jetson Nano) which is composed of a fixed camera. The proposed approach is implemented in a distributed surveillance video system to visualize people from several cameras in one centralized monitoring system. The achieved results show that the proposed method is suitable to set up a surveillance system in smart cities for people detection, social distancing classification, and body temperature analysis.},
  archive      = {J_JRTIP},
  author       = {Saponara, Sergio and Elhanashi, Abdussalam and Gagliardi, Alessio},
  doi          = {10.1007/s11554-021-01070-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1937-1947},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Implementing a real-time, AI-based, people detection and social distancing measuring system for covid-19},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RT-BVE—real-time no-reference blocking visibility estimation
in video frames. <em>JRTIP</em>, <em>18</em>(6), 1921–1936. (<a
href="https://doi.org/10.1007/s11554-020-01065-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During lossy video compression, a blocking artifact is introduced into a video. It can significantly reduce the quality of a compressed video, and consequently end-user Quality of Experience. Hence, it is very important to measure and monitor the quality of compressed videos delivered to the end-user, which can be performed only by using the no-reference (NR) approach, without the original video signal available. Many NR video quality assessment (VQA) metrics are based on measurements of different artifacts visibility, which are then used as an input for an objective NR VQA metric. Thus, reliable artifacts detection and their visibility estimation are the first step in a reliable VQA process. In this paper, a new algorithm for Real-Time NR Blocking Visibility Estimation (RT-BVE) in video frames is proposed. The performance of the proposed RT-BVE algorithm is compared to the performance of two freely available algorithms, i.e., BBT-BDA and MSU Blocking, using videos from the well-known databases (156 video sequences in total and more than 48,000 frames). While RT-BVE achieves performance comparable to BBT-BDA and MSU Blocking algorithms for MPEG-2 videos, it outperforms both algorithms for H.264 videos. The results show that the proposed RT-BVE algorithm can precisely estimate blocking visibility in video frames for videos compressed according to different compression standards by using a fixed set of algorithm parameter values. Additionally, RT-BVE is suitable for usage in real-time applications since it is capable of processing approximately 100 Full HD video frames per second on the current midrange × 86–64 platform.},
  archive      = {J_JRTIP},
  author       = {Jurić, Tomislav and Vranješ, Mario and Grbić, Ratko and Brisinello, Matteo},
  doi          = {10.1007/s11554-020-01065-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1921-1936},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RT-BVE—Real-time no-reference blocking visibility estimation in video frames},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Live user-guided depth map estimation for single images.
<em>JRTIP</em>, <em>18</em>(6), 1907–1919. (<a
href="https://doi.org/10.1007/s11554-020-01055-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of depth information in an image enables the simulation of distinct visual effects (e.g., refocus, desaturation, haze) that are related to the distance of the camera to the objects in the scene. To generate depth from color data in single images, existing techniques typically use learning-based strategies or require user-guided depth annotations. Learning-based techniques suffer from generality issues, while user-guided techniques solve a costly optimization problem that prevents a real-time feedback of the depth map generated from the user annotation. In this paper, we overcome the latter problem and propose a GPU-based algorithm that provides live feedback on the output depth map estimated during the user annotation. We follow previous work and treat the depth map estimation as a 2D Poisson problem that can be optimized using a sparse linear solver. However, we change the way that the sparse linear coefficients are computed to favor a more smooth, spatially coherent depth map, able to provide the desired visual effects. Moreover, our approach is designed to run almost entirely on the GPU, achieving real-time performance even for high-resolution images.},
  archive      = {J_JRTIP},
  author       = {Macedo, Márcio C. F. and Apolinário, Antônio L.},
  doi          = {10.1007/s11554-020-01055-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1907-1919},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Live user-guided depth map estimation for single images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel approaches to improve the speed of
chaotic-maps-based encryption using GPU. <em>JRTIP</em>, <em>18</em>(6),
1897–1906. (<a
href="https://doi.org/10.1007/s11554-020-01064-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaos-based cryptography has become a major interest for providing effective, fast and secure encryption in the recent few years. Chaos-based encryption applies basically repetitive steps of more than one chaotic map to increase the strength of the security. However, this repetition unfortunately increases the processing time especially for videos. With the advent of the graphical processing units (GPU), many encryption algorithms were applied for image and video encryption using GPU to gain a speedup for the whole process. In this paper, we aim to introduce parallel implementation for chaos-based encryption techniques which enables using GPUs to perform the encryption and decryption process more efficient than using the traditional methods in processing both images and videos. The simulation results of the proposed algorithms on GPU using CUDA-OpenGL demonstrate an execution time reduction of almost 75% of encryption speed which encourages exploring more chaotic maps for video encryption.},
  archive      = {J_JRTIP},
  author       = {Elrefaey, Amany and Sarhan, Amany and El-Shennawy, Nada M.},
  doi          = {10.1007/s11554-020-01064-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1897-1906},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel approaches to improve the speed of chaotic-maps-based encryption using GPU},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coding mode decision algorithm for fast HEVC transrating
using heuristics and machine learning. <em>JRTIP</em>, <em>18</em>(6),
1881–1896. (<a
href="https://doi.org/10.1007/s11554-020-01063-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes a framework to speed up the HEVC encoding decisions for on-demand transrating of bitstreams. The methods proposed collect information from a high-quality reference bitstream which after processing is used to limit the number of modes evaluated in subsequent re-encodings at different bitrates. In this way, the time required to process re-encode-time computing-intensive decisions, such as partitioning and motion estimation is significantly reduced. The methods proposed are a combination of heuristics with a statistical basis and fast decision techniques trained using automatic learning methodologies. Experimental results using the HEVC reference encoder show that jointly the methods proposed reduce the transcoding computational complexity by up to 78.8%, with Bjontegaard bitrate deltas penalties smaller than 1.06%. A comparison with related works showed that the proposed method is able to outperform state-of-the-art solutions in terms of combined rate-distortion–complexity performance indicators.},
  archive      = {J_JRTIP},
  author       = {Grellert, Mateus and da Silva Cruz, Luis A. and Zatt, Bruno and Bampi, Sergio},
  doi          = {10.1007/s11554-020-01063-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1881-1896},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Coding mode decision algorithm for fast HEVC transrating using heuristics and machine learning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A compact and recursive riemannian motion descriptor for
untrimmed activity recognition. <em>JRTIP</em>, <em>18</em>(6),
1867–1880. (<a
href="https://doi.org/10.1007/s11554-020-01057-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A very low dimension frame-level motion descriptor is herein proposed with the capability to represent incomplete dynamics, thus allowing online action prediction. At each frame, a set of local trajectory kinematic cues are spatially pooled using a covariance matrix. The set of frame-level covariance matrices forms a Riemannian manifold that describes motion patterns. A set of statistic measures are computed over this manifold to characterize the sequence dynamics, either globally, or instantaneously from a motion history. Regarding the Riemannian metrics, two different versions are proposed: (1) by considering tangent projections with respect to updated recursive statistics, and (2) by mapping the covariance onto a linear matrix using as reference the identity matrix. The proposed approach was evaluated for two different tasks: (1) for action classification on complete video sequences and (2) for online action recognition, in which the activity is predicted at each frame. The method was evaluated using two public datasets: KTH and UT-interaction. For action classification, the method achieved an average accuracy of 92.27 and 81.67%, for KTH and UT-interaction, respectively. In partial recognition task, the proposed method achieved similar classification rate as for the whole sequence using only the 40 and 70% on KTH and UT sequences, respectively. The code of this work is available at [code].},
  archive      = {J_JRTIP},
  author       = {Martı́nez Carrillo, Fabio and Gouiffès, Michèle and Garzón Villamizar, Gustavo and Manzanera, Antoine},
  doi          = {10.1007/s11554-020-01057-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1867-1880},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A compact and recursive riemannian motion descriptor for untrimmed activity recognition},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Journal of real-time image processing: Sixth issue of volume
18. <em>JRTIP</em>, <em>18</em>(6), 1865–1866. (<a
href="https://doi.org/10.1007/s11554-021-01179-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Carlsohn, Matthias F. and Kehtarnavaz, Nasser},
  doi          = {10.1007/s11554-021-01179-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1865-1866},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: Sixth issue of volume 18},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recognizing human violent action using drone surveillance
within real-time proximity. <em>JRTIP</em>, <em>18</em>(5), 1851–1863.
(<a href="https://doi.org/10.1007/s11554-021-01171-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the world is witnessing a significant rise in the cases of both reported and unnoticed violations. As an answer to this rising menace, video surveillance can fill the gap of covering untapped actions which lead to violence, while also ensuring a secure life. In our everyday life, surveillance can be accomplished efficiently by activity classification from drone videos. The prominent fields that have employed this technology are police work, video categorization, biometrics, and human–computer interaction. So far, no public dataset is available for violent activity classification using drone surveillance. Hence, this work aims to look into the domain of machine-driven recognition and classification of human actions from drone videos. In this study, the dataset is created using drones from different heights for an unconstrained environment. The study begins by performing key-point extraction and generate 2D skeletons for the persons in the frame. These extracted key points are given as features in the classification module to recognize the actions. The classification models used in the proposed method are SVM (support vector machine) and Random Forest. Experimental results show that the SVM model with RBF (radial basis function) kernel for activity classification is more efficient when compared to the prior proposed approaches and other experimented models. The research work has also analyzed the run time performance of the proposed system and achieve its real-time performance.},
  archive      = {J_JRTIP},
  author       = {Srivastava, Anugrah and Badal, Tapas and Garg, Apar and Vidyarthi, Ankit and Singh, Rishav},
  doi          = {10.1007/s11554-021-01171-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1851-1863},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Recognizing human violent action using drone surveillance within real-time proximity},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ‘A real-time classification model based on joint
sparse-collaborative representation. <em>JRTIP</em>, <em>18</em>(5),
1837–1849. (<a
href="https://doi.org/10.1007/s11554-021-01167-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the low computational complexity, the two-phase test sample representation method shows outstanding advantages in the field of real-time face recognition. However, its first phase does not fully consider the imbalance of the determined K-nearest training samples. This defect is not well reflected and will seriously affect the recognition accuracy of the second phase. In this paper, we explore the above issues and propose to incorporate the unselected training samples into the modeling process. The proposed method not only allows the unselected samples to perform the final representation but also makes the selected nearest training samples play a more significant role than the unselected ones in classification. It is efficiently solved with an analytical solution. The rationales and probability interpretation of the proposed method are presented to further guarantee its rationality and effectiveness. Extensive experiments on diverse face databases are conducted to demonstrate the superior recognition performance in comparison with other competing classifiers.},
  archive      = {J_JRTIP},
  author       = {Li, Yanting and Jin, Junwei and Chen, C. L. Philip},
  doi          = {10.1007/s11554-021-01167-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1837-1849},
  shortjournal = {J. Real-Time Image Process.},
  title        = {`A real-time classification model based on joint sparse-collaborative representation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real time video summarizing using image semantic
segmentation for CBVR. <em>JRTIP</em>, <em>18</em>(5), 1827–1836. (<a
href="https://doi.org/10.1007/s11554-021-01151-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieving the exact video of choice in real time requires searching in annotated videos. Manual annotation is impossible for the huge data available nowadays. Hence, an effective model is proposed for summarizing the videos frame wise using stacked generalization to ensemble different machine learning algorithms. Also, the ranks are given to videos on the basis of the time a particular building or monument appears in the video. The videos are queried using KD tree. Semantic segmentation corresponds to the content of the video and hence the content based video retrieval.},
  archive      = {J_JRTIP},
  author       = {Jain, Rahul and Jain, Pooja and Kumar, Tapan and Dhiman, Gaurav},
  doi          = {10.1007/s11554-021-01151-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1827-1836},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real time video summarizing using image semantic segmentation for CBVR},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive and stabilized real-time super-resolution control
for UAV-assisted smart harbor surveillance platforms. <em>JRTIP</em>,
<em>18</em>(5), 1815–1825. (<a
href="https://doi.org/10.1007/s11554-021-01163-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, there are active research for deep learning applications to smart cities, e.g., smart factory, smart and micro grids, and smart logistics. Among them, for industrial smart harbor and logistics platforms, this paper proposes a novel two-stage algorithm for large-scale surveillance. For the purpose, this paper utilizes drones for flexible localization, and thus, the algorithm for scheduling between multiple drones and multiple multi-access edge computing (MEC) systems is proposed under the consideration of stability in this first-stage. After the scheduling, each drone transmits its own data to its associated MEC for enhancing the quality and then eventually the data will be used for surveillance. For improving the quality, super-resolution is used. In the second-stage algorithm, the self-adaptive super-resolution control is proposed for time-average performance maximization subject to stability, inspired by Lyapunov optimization. Based on data-intensive simulation results, it has been verified that the proposed algorithm achieves desired performance.},
  archive      = {J_JRTIP},
  author       = {Jung, Soyi and Kim, Joongheon},
  doi          = {10.1007/s11554-021-01163-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1815-1825},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Adaptive and stabilized real-time super-resolution control for UAV-assisted smart harbor surveillance platforms},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time person tracking system based on SiamMask network
for intelligent video surveillance. <em>JRTIP</em>, <em>18</em>(5),
1803–1814. (<a
href="https://doi.org/10.1007/s11554-021-01144-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time video surveillance systems are widely deployed in various environments, including public areas, commercial buildings, and public infrastructures. Person detection is a key and crucial task in different video surveillance applications, such as person detection, segmentation, and tracking. Researchers presented different image processing and artificial intelligence-based approaches (including machine and deep learning) for person detection and tracking, but mainly comprised of frontal view camera perspective. A real-time person tracking and segmentation system is introduced in this work, using an overhead camera perspective. The system applied a deep learning-based algorithm, i.e., SiamMask, a simple, versatile, fast, and surpassing other real-time tracking algorithms. The algorithm also performs segmentation of the target person by combining a mask branch to the fully convolutional twin neural network for target or person tracking. First, the person video sequences are obtained from an overhead perspective, and then additional training is performed with the help of transfer learning. Finally, a comparison is performed with other tracking algorithms. The SiamMask algorithm delivers good results, with a tracking accuracy of 95%.},
  archive      = {J_JRTIP},
  author       = {Ahmed, Imran and Jeon, Gwanggil},
  doi          = {10.1007/s11554-021-01144-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1803-1814},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time person tracking system based on SiamMask network for intelligent video surveillance},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RGB+d and deep learning-based real-time detection of
suspicious event in bank-ATMs. <em>JRTIP</em>, <em>18</em>(5),
1789–1801. (<a
href="https://doi.org/10.1007/s11554-021-01155-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection of human activities has become very important in terms of surveillance and security of Bank-Automated Teller Machines (ATMs), public offices because of the day-to-day increase in criminal activities. The current way of monitoring such constrained environments is done through monocular CCTV cameras which capture only RGB video. The RGB+D sensor provides depth data of the scene in addition to RGB data. To address the problem of online detection of abnormal activities in Bank ATMs, we propose a supervised deep learning framework based on multi-stream CNNs and RGB+D sensor. From the online video stream of RGB+D data, motion templates are created from RGB and depth video segments and then trained on CNNs to detect a suspicious event in ongoing activity. Moreover, due to the unavailability of any dataset for analyzing human activities in ATMs, we also contributed a novel RGB+D dataset in this paper. The proposed deep learning-based framework is evaluated on qualitative and quantitative statistical evaluation parameters and detect suspicious event with the precision of 0.932 and accuracy of 94.2%. Detailed statistical analysis of results shows that the proposed framework can detect the suspicious event in a real-time online manner before the abnormal activity gets completed.},
  archive      = {J_JRTIP},
  author       = {Khaire, Pushpajit A. and Kumar, Praveen},
  doi          = {10.1007/s11554-021-01155-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1789-1801},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RGB+D and deep learning-based real-time detection of suspicious event in bank-ATMs},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time intelligent image processing for security
applications. <em>JRTIP</em>, <em>18</em>(5), 1787–1788. (<a
href="https://doi.org/10.1007/s11554-021-01169-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of machine learning techniques and image processing techniques has led to new research opportunities in this area. Machine learning has enabled automatic extraction and analysis of information from images. The convergence of machine learning with image processing is useful in a variety of security applications. Image processing plays a significant role in physical as well as digital security. Physical security applications include homeland security, surveillance applications, identity authentication, and so on. Digital security implies protecting digital data. Techniques like digital watermarking, network security, and steganography enable digital security.},
  archive      = {J_JRTIP},
  author       = {Singh, Akansha and Li, Ping and Singh, Krishna Kant and Saravana, Vijayalakshmi},
  doi          = {10.1007/s11554-021-01169-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1787-1788},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time intelligent image processing for security applications},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic deep q network for real-time path planning in
censorious robotic procedures using force sensors. <em>JRTIP</em>,
<em>18</em>(5), 1773–1785. (<a
href="https://doi.org/10.1007/s11554-021-01122-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, enormous advancement has taken place in biomedical engineering, which has paved the way for robot-assisted surgery in various complex surgical procedures. In robotic surgery, the reinforcement-based Temporal Difference (TD) based approach through assistive approaches has tremendous potential. Probabilistic Roadmap (PR) can be used for recognition of the path to the region of interest without any obstacles and, Inverse Kinematics (IK) approach can be used for the accurate approximation of the pixel space to the real-time workspace. Our proposed system would be more effective in approximating the path length, depth evaluation, and less invasive contact force sensor. This article presents a robust algorithm that would assist in robotic surgery for censorious surgeries in real-time. For working on such soft tissues, software-driven procedures and algorithms must be more precise in choosing the optimal path for reaching out to the procedural region. The statistical analysis has proven that the proposed approach would be outperforming under favorable learning rate, discount factor, and the exploration factor.},
  archive      = {J_JRTIP},
  author       = {Srinivasu, Parvathaneni Naga and Bhoi, Akash Kumar and Jhaveri, Rutvij H. and Reddy, Gadekallu Thippa and Bilal, Muhammad},
  doi          = {10.1007/s11554-021-01122-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1773-1785},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Probabilistic deep q network for real-time path planning in censorious robotic procedures using force sensors},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient robust method for accurate and real-time
vehicle plate recognition. <em>JRTIP</em>, <em>18</em>(5), 1759–1772.
(<a href="https://doi.org/10.1007/s11554-021-01118-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy and real-timeliness are the top concerns in vehicle plate recognition. Several factors put restrictions on plate recognition system, including illumination, vehicle high speed, camera angle, and bad weather condition. Damaged and pale plates also lead to incorrect recognition in the present approaches. In this regard, this paper proposes an efficient robust method for vehicle plate recognition, which consists of four steps: (i) vehicle detection, (ii) plate detection, (iii) character segmentation, and (iv) character recognition. In the first step, the vehicle image is detected using background emission. Plates are localized by means of character recognition and pattern matching approaches in the second step, where the contours are recognized and extracted using connected component analysis, and then, low-density areas are emitted using density criterion and vehicle plate is extracted. In the third step, statistical feature, filtering methods, and morphology operators are employed for segmentation and extraction of plate characters. After plate segmentation, statistical and global features and local pattern are extracted from each segment image for segment classification in the final step, where features are ranked using F-Score, and then, classification of each section to one of 37 classes is performed using random forest. The proposed method is evaluated using several databases in both left to right and right to left languages; English for the former and Persian for the latter. In the first part of the evaluation, the proposed approach is evaluated in terms of robustness and recognition speed. The proposed method has the accuracy of 99.2% for plate recognition, 100% for plate segmentation, and 98.41% for character recognition. In this part, the dataset of Iranian plates is collected by the authors of this paper. However, character recognition rate is 100% in other Persian databases. Moreover, the experimental evaluations witness that the proposed method can process at least 8 frames per second, that means it is fast enough to be adopted for real-time applications. In the second phase, the proposed method is evaluated on an English plate dataset. In this dataset, the proposed method shows an accuracy of 100% for plate detection and 97.5% for character recognition. The experimental results show that the proposed method outperforms methods proposed in recent years in terms of time and accuracy that is also independent of plate language.},
  archive      = {J_JRTIP},
  author       = {Pirgazi, Jamshid and Sorkhi, Ali Ghanbari and Kallehbasti, Mohammad Mehdi Pourhashem},
  doi          = {10.1007/s11554-021-01118-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1759-1772},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient robust method for accurate and real-time vehicle plate recognition},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time efficient object segmentation system based on
u-net using aerial drone images. <em>JRTIP</em>, <em>18</em>(5),
1745–1758. (<a
href="https://doi.org/10.1007/s11554-021-01166-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object detection and segmentation are considered as one of the fundamental but challenging problems in remote sensing and surveillance applications (including satellite and aerial). Consequently, it performs a crucial role in various management and monitoring applications and has received notable attention in recent years. This paper aims to present a real-time, efficient system in which a deep learning-based model U-Net is explored for multiple object segmentation in aerial drone images. We perform data augmentation and apply transfer learning to enhance the model efficiency. We experimented U-Net segmentation model with different base architectures, including VGG 16, ResNet-50, and MobileNet, and compare their performance. We also compare the results U-Net segmentation model with different base architectures and concludes that the U-Net (MobileNet) achieves good results. The experimental results demonstrate that data augmentation improves the model’s performance by achieving a segmentation accuracy of 92%, 93%, and 95% with base architectures VGG-16, ResNet-50, and MobileNet, respectively.},
  archive      = {J_JRTIP},
  author       = {Ahmed, Imran and Ahmad, Misbah and Jeon, Gwanggil},
  doi          = {10.1007/s11554-021-01166-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1745-1758},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time efficient object segmentation system based on U-net using aerial drone images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SD-net: Understanding overcrowded scenes in real-time via an
efficient dilated convolutional neural network. <em>JRTIP</em>,
<em>18</em>(5), 1729–1743. (<a
href="https://doi.org/10.1007/s11554-020-01020-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in computer vision-related technologies attract many researchers for surveillance applications, particularly involving the automated crowded scenes analysis such as crowd counting in a very congested scene. In crowd counting, the main goal is to count or estimate the number of people in a particular scene. Understanding overcrowded scenes in real-time is important for instant responsive actions. However, it is a very difficult task due to some of the key challenges including clutter background, occlusion, variations in human pose and scale, and limited surveillance training data, that are inadequately covered in the employed literature. To tackle these challenges, we introduce “SD-Net” an end-to-end CNN architecture, which produces real-time high quality density maps and effectively counts people in extremely overcrowded scenes. The proposed architecture consists of depthwise separable, standard, and dilated 2D convolutional layers. Depthwise separable and standard 2D convolutional layers are used to extract 2D features. Instead of using pooling layers, dilated 2D convolutional layers are employed that results in huge receptive fields and reduces the number of parameters. Our CNN architecture is evaluated using four publicly available crowd analysis datasets, demonstrating superiority over state-of-the-art in terms of accuracy and model size.},
  archive      = {J_JRTIP},
  author       = {Khan, Noman and Ullah, Amin and Haq, Ijaz Ul and Menon, Varun G. and Baik, Sung Wook},
  doi          = {10.1007/s11554-020-01020-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1729-1743},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SD-net: Understanding overcrowded scenes in real-time via an efficient dilated convolutional neural network},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel optical image denoising technique using
convolutional neural network and anisotropic diffusion for real-time
surveillance applications. <em>JRTIP</em>, <em>18</em>(5), 1711–1728.
(<a href="https://doi.org/10.1007/s11554-020-01060-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The elimination of noisy content from digital images is one of the major issues during image pre-processing. The process of image acquisition, compression, and image transmission is a major reason for image noise that causes loss of information. This loss of information causes irregularities and error in the working of many real-time applications such as computerized photography, hurdle detection and traffic monitoring (computer vision), automatic character recognition, morphing, and surveillance applications. This paper proposes a new hybrid and multi-level digital image denoising approach (MLAC) using a convolutional neural network (CNN) and anisotropic diffusion (AD). The denoising approach uses a hybrid combination of CNN and AD using multi-level implementation. First of all, CNN is applied to noisy images for noise elimination, which results in a denoised image in the first level of image denoising. After that, denoised image is passed to AD in the second level of image denoising. The AD is applied for edge and corner preservation of objects. This hybrid approach is highly efficient in removing noise while preserving fine details of image. The proposed denoising method is experimented on all standard inbuilt image datasets of Matlab framework. It is tested on SAR images as well. The results are compared with those of some of the latest works in the field of CNN and AD. The quality of the denoised image is tested by using naked eye visual analysis factors and quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), universal image quality index (UIQI), feature similarity index metric (FSIM), equivalent numbers of looks (ENL), noise variance (NV), and mean-squared error (MSE). The denoising results are further critically analyzed using zooming analysis method, plotting histogram, comparative running real-time implementation aspects, and time complexity evaluation. The detailed study of result confirms that the proposed approach gives an excellent result in terms of structure, edge preservation, and noise suppression.},
  archive      = {J_JRTIP},
  author       = {Singh, Prabhishek and Shankar, Achyut},
  doi          = {10.1007/s11554-020-01060-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1711-1728},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel optical image denoising technique using convolutional neural network and anisotropic diffusion for real-time surveillance applications},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image deconvolution for optical small satellite with deep
learning and real-time GPU acceleration. <em>JRTIP</em>, <em>18</em>(5),
1697–1710. (<a
href="https://doi.org/10.1007/s11554-021-01113-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-orbit optical-imaging instruments may suffer from degradations due to space environment impacts or long-time operation. The degradation causes blurring on the image received from the ground. Degradations come from defocus and spherical aberrations cause blurring on the received image. Image deblurring should be done in pre-processing step to compensate the sensor bad impacts. The aberrations are modeled by Zernike polynomials and treated by deep learning in deblurring method. This paper presents a method to deconvolve the acquired data to improve the image quality. A convolution neural network is trained to estimate the point spread function (PSF) parameters using acquired images over satellite calibration site with specific pattern. Image deconvolution is performed to obtain image signal-to-noise (SNR) and modulation transfer function (MTF) improvement. Technical and image data used for modeling and experiment are used from VNREDSat-1 satellite (the first operational Vietnam Earth observation optical small satellite). The experiment is performed on computers accelerated by graphics processing units (GPU) to ensure fast computation.},
  archive      = {J_JRTIP},
  author       = {Ngo, Tan D. and Bui, Tuyen T. and Pham, Tuan M. and Thai, Hong T. B. and Nguyen, Giang L. and Nguyen, Tu N.},
  doi          = {10.1007/s11554-021-01113-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1697-1710},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Image deconvolution for optical small satellite with deep learning and real-time GPU acceleration},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). V3O2: Hybrid deep learning model for hyperspectral image
classification using vanilla-3D and octave-2D convolution.
<em>JRTIP</em>, <em>18</em>(5), 1681–1695. (<a
href="https://doi.org/10.1007/s11554-020-00966-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image analysis is an emerging area of research and is used for various applications such as climate analysis, crop monitoring and change detection. Hyperspectral image (HSI) is one of the dominant remote sensing imaging modalities that captures information beyond the visible spectrum. The evolution of deep learning has made a significant impact on HSI analysis, mainly for its classification. The spatial–spectral feature-based classification model improves the classification accuracy of hyperspectral images (HSIs). However, these models are computationally expensive, and redundancy exists in the spatial dimension of features. This research work proposes a hybrid convolutional neural network (CNN) for HSI classification. The proposed model uses principal component analysis (PCA) as a preprocessing technique for optimal band extraction from HSIs. The hybrid CNN classification technique extracts the spectral and spatial features using three-dimensional CNN (3D CNN). These features are fed into a two-dimensional CNN (2D CNN) for further feature extraction and classification. The redundancy in spatial features of the hybrid CNN model is reduced by octave convolution (OctConv) instead of standard vanilla convolution. OctConv factorizes the spatial features into lower and higher spatial frequencies, and different convolutions are performed on them based on their frequencies. The hybrid model is compared against various state-of-the-art CNN-based techniques and found that the accuracy is boosted with a lesser computational cost.},
  archive      = {J_JRTIP},
  author       = {Mohan, Alkha and Meenakshi Sundaram, Venkatesan},
  doi          = {10.1007/s11554-020-00966-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1681-1695},
  shortjournal = {J. Real-Time Image Process.},
  title        = {V3O2: Hybrid deep learning model for hyperspectral image classification using vanilla-3D and octave-2D convolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time crowd behavior recognition in surveillance videos
based on deep learning methods. <em>JRTIP</em>, <em>18</em>(5),
1669–1679. (<a
href="https://doi.org/10.1007/s11554-021-01116-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic video surveillance in public crowded places has been an active research area for security purposes. Traditional approaches try to solve the crowd behavior recognition task using a sequential two-stage pipeline as low-level feature extraction and classification. Lately, deep learning has shown promising results in comparison to traditional methods by extracting high-level representation and solving the problem in an end-to-end pipeline. In this paper, we investigate a deep architecture for crowd event recognition to detect seven behavior categories in PETS2009 event recognition dataset. More especially, we apply an integrated handcrafted and Conv-LSTM-AE method with optical flow images as input to extract a high-level representation of data and conduct classification. After achieving a latent representation of input optical flow image sequences in the bottleneck of autoencoder(AE), the architecture is split into two separate branches, one as AE decoder and the other as the classifier. The proposed architecture is jointly trained for representation and classification by defining two different losses. The experimental results in comparison to the state-of-the-art methods demonstrate that our algorithm can be promising for real-time event recognition and achieves a better performance in calculated metrics.},
  archive      = {J_JRTIP},
  author       = {Rezaei, Fariba and Yazdi, Mehran},
  doi          = {10.1007/s11554-021-01116-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1669-1679},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time crowd behavior recognition in surveillance videos based on deep learning methods},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Real-time underwater image resolution enhancement using
super-resolution with deep convolutional neural networks.
<em>JRTIP</em>, <em>18</em>(5), 1653–1667. (<a
href="https://doi.org/10.1007/s11554-020-01024-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a two-step image enhancement is presented. In the first step, color correction and underwater image quality enhancement are conducted if there are artifacts such as darkening, hazing and fogging. In the second step, the image resolution optimized in the previous step is enhanced using the convolutional neural network (CNN) with deep learning capability. The main reason behind the adoption of this two-step technique, which includes image quality enhancement and super-resolution, is the need for a robust strategy to visually improve underwater images at different depths and under diverse artifact conditions. The effectiveness and robustness of the real-time algorithm are satisfactory for various underwater images under different conditions, and several experiments have been undertaken for the two datasets of images. In both stages and for each of image datasets, the mean square error (MSE), peak signal to noise ratio (PSNR), and structural similarity (SSIM) evaluation measures were fulfilled. In addition, the low computational complexity and suitable outputs were obtained for different artifacts that represented divergent depths of water to achieve a real-time system. The super-resolution in the proposed structure for medium layers can offer a proper response. For this reason, time is also one of the major factors reported in the research. Applying this model to underwater imagery systems will yield more accurate and detailed information.},
  archive      = {J_JRTIP},
  author       = {Moghimi, Mohammad Kazem and Mohanna, Farahnaz},
  doi          = {10.1007/s11554-020-01024-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1653-1667},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time underwater image resolution enhancement using super-resolution with deep convolutional neural networks},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time and effective pan-sharpening for remote sensing
using multi-scale fusion network. <em>JRTIP</em>, <em>18</em>(5),
1635–1651. (<a
href="https://doi.org/10.1007/s11554-021-01080-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time monitoring and surveillance play an important role in the field of remote sensing, where multi-spectral (MS) images with high spatial resolution are widely desired for better analysis. However, high-resolution MS images cannot be directly obtained due to the limitations of sensors and bandwidth. As an essential way to alleviate this problem, pan-sharpening aims at fusing the complementary information of a low-resolution MS image and a high-resolution panchromatic (PAN) image to reconstruct a high-resolution MS image. Most previous deep-learning based methods can meet the real-time requirements with the help of graphics processing unit (GPU). However, they don’t fully exploit the favorable hierarchical information, sparing huge room for performance improvement. In this paper, to meet the requirement of real-time implementation and achieve more effective performance simultaneously, we propose a multi-scale fusion network (MSFN) to make full use of hierarchical complementary features of PAN and MS images. Specifically, we introduce an encoder–decoder structure and coarse-to-fine strategy to effectively extract multi-scale features of PAN and MS images, separately. Meanwhile, an information pool is adopted to preserve primitive information. Then a multi-scale feature fusion module is applied to fuse multi-scale features from the decoder and information pool. Finally, the fused features are utilized to reconstruct the high-resolution MS image. Extensive experiments demonstrate that our proposed method achieves favorable performance against other methods in terms of quantitative metrics and visual quality. Besides, the results on running time indicate that our method can achieve real-time performance.},
  archive      = {J_JRTIP},
  author       = {Lai, Zhibing and Chen, Lihui and Jeon, Gwanggil and Liu, Zitao and Zhong, Rui and Yang, Xiaomin},
  doi          = {10.1007/s11554-021-01080-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1635-1651},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time and effective pan-sharpening for remote sensing using multi-scale fusion network},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the realization and analysis of circular harmonic
transforms for feature detection. <em>JRTIP</em>, <em>18</em>(5),
1621–1633. (<a
href="https://doi.org/10.1007/s11554-020-01040-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular-harmonic spectra are a compact representation of local image features in two dimensions. It is well known that the computational complexity of such transforms is greatly reduced when polar separability is exploited in steerable filter-banks. Further simplifications are possible when Cartesian separability is incorporated using the radial apodization (i.e. weight, window, or taper) described here, as a consequence of the Laguerre/Hermite correspondence over polar/Cartesian coordinates. The chosen form also mitigates undesirable discretization artefacts due to angular aliasing. The local angular spectrum at each pixel is deployed in a novel test-statistic to detect and characterize corners of arbitrary angle and orientation (i.e. wedges). The test-statistic considers uncertainty due to finite sampling and clutter/noise. The possible utility of this detector, and circular-harmonic spectra for the description of simple features in general, is illustrated using real data from an overhead electro-optic sensor. Monte-Carlo simulations are also performed to quantify performance relative to other simple corner detectors. Possible computer realizations for a small remote-sensing platform are discussed.},
  archive      = {J_JRTIP},
  author       = {Kennedy, Hugh L.},
  doi          = {10.1007/s11554-020-01040-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1621-1633},
  shortjournal = {J. Real-Time Image Process.},
  title        = {On the realization and analysis of circular harmonic transforms for feature detection},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved u-net model for remote sensing image classification
method based on distributed storage. <em>JRTIP</em>, <em>18</em>(5),
1607–1619. (<a
href="https://doi.org/10.1007/s11554-020-01028-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the low efficiency of traditional methods for the management and classification of massive remote sensing image data, a mass remote sensing image classification method based on distributed storage is proposed. The aim is to obtain near real-time image classification in mobile devices or internet applications. In this paper, we designed two levels of an image processing structure. A distributed file system is taken as the underlying storage architecture to efficiently manage and query massive remote sensing images. The upper layer uses a GPU server to train the remote sensing image classification model to improve the classification accuracy. To improve the classification accuracy, we add two parameters to adjust the data of the current layer in U-Net. The experimental results show that the proposed method based on distributed storage has a high degree of scalability, and it has a short processing time while maintaining a high classification accuracy for remote sensing images.},
  archive      = {J_JRTIP},
  author       = {Jing, Weipeng and Zhang, Mingwei and Tian, Dongxue},
  doi          = {10.1007/s11554-020-01028-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1607-1619},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved U-net model for remote sensing image classification method based on distributed storage},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time prediction of spatial raster time series: A
context-aware autonomous learning model. <em>JRTIP</em>, <em>18</em>(5),
1591–1605. (<a
href="https://doi.org/10.1007/s11554-021-01099-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time prediction of spatial raster time series, such as those derived from satellite remote sensing imagery, is important for making emergency decisions on various geo-spatial processes/events. However, because of the scalability issue and large training time requirement, the neural network (NN)-based models often fail to perform real-time prediction, in spite of their tremendous potential. In this paper, we propose ContRast, a variant of recurrent NN-based context-aware raster time series prediction model that attempts to resolve these issues by: (1) eliminating the need for offline adjustment of network structure by employing self-evolving autonomous learning of recurrent neural network, (2) saving training time by adopting single-pass parameter learning mechanism, and (3) reducing redundant learning by skipping sub-regional data associated with similar spatio-temporal context and reusing already learned parameters to predict for the same. Experimental evaluations with respect to predicting normalized difference vegetation index (NDVI)-raster derived from MODIS Terra satellite remote sensing imagery show that ContRast is highly effective for real-time prediction of spatial raster time series, and it significantly outperforms the existing models. In addition, the theoretical analyses of model complexity and computational cost further justify our empirical observations.},
  archive      = {J_JRTIP},
  author       = {Das, Monidipa},
  doi          = {10.1007/s11554-021-01099-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1591-1605},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time prediction of spatial raster time series: A context-aware autonomous learning model},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instant water body variation detection via analysis on
remote sensing imagery. <em>JRTIP</em>, <em>18</em>(5), 1577–1590. (<a
href="https://doi.org/10.1007/s11554-020-01062-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water resource is one of the most valuable natural resources for human beings, which requires to be monitored for careful protection. Inspired by a significant power of machine learning methods, researchers have successfully developed many applications to automatically perform identification on the water body via analyzing remote sensing images. Since a similar category of ground objects could show a large difference in spectral representation, researchers try to propose distinctive and effective features to offer redundant information for category classification. Moreover, large amount of high-resolution remote sensing images require analyzing algorithms to be parallel processed for instant feedback. Based on these requirements, we propose a novel water body variation detection via analysis on remote sensing images. Specifically, the proposed method firstly perform pixel-level classification to locate abnormal changes with thoughts of visual word patterns. Afterwards, the proposed method proposes block division method to construct parallel running version with Mapreduce structure. With high representational and parallel running abilities, the proposed method is capable to accurately detect variation areas on remote sensing images with instant feedback. Experiments on several self-collected datasets show the proposed method has achieved better efficiencies than comparative studies.},
  archive      = {J_JRTIP},
  author       = {Wu, Yirui and Han, Pengfei and Zheng, Zhan},
  doi          = {10.1007/s11554-020-01062-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1577-1590},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Instant water body variation detection via analysis on remote sensing imagery},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time image dehazing by superpixels segmentation and
guidance filter. <em>JRTIP</em>, <em>18</em>(5), 1555–1575. (<a
href="https://doi.org/10.1007/s11554-020-00953-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze and fog had a great influence on the quality of images, and to eliminate this, dehazing and defogging are applied. For this purpose, an effective and automatic dehazing method is proposed. To dehaze a hazy image, we need to estimate two important parameters such as atmospheric light and transmission map. For atmospheric light estimation, the superpixels segmentation method is used to segment the input image. Then each superpixel intensities are summed and further compared with each superpixel individually to extract the maximum intense superpixel. Extracting the maximum intense superpixel from the outdoor hazy image automatically selects the hazy region (atmospheric light). Thus, we considered the individual channel intensities of the extracted maximum intense superpixel as an atmospheric light for our proposed algorithm. Secondly, on the basis of measured atmospheric light, an initial transmission map is estimated. The transmission map is further refined through a rolling guidance filter that preserves much of the image information such as textures, structures and edges in the final dehazed output. Finally, the haze-free image is produced by integrating the atmospheric light and refined transmission with the haze imaging model. Through detailed experimentation on several publicly available datasets, we showed that the proposed model achieved higher accuracy and can restore high-quality dehazed images as compared to the state-of-the-art models. The proposed model could be deployed as a real-time application for real-time image processing, real-time remote sensing images, real-time underwater images enhancement, video-guided transportation, outdoor surveillance, and auto-driver backed systems.},
  archive      = {J_JRTIP},
  author       = {Hassan, Haseeb and Bashir, Ali Kashif and Ahmad, Muhammad and Menon, Varun G. and Afridi, Imran Uddin and Nawaz, Raheel and Luo, Bin},
  doi          = {10.1007/s11554-020-00953-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1555-1575},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time image dehazing by superpixels segmentation and guidance filter},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time segmentation of remote sensing images with a
combination of clustering and bayesian approaches. <em>JRTIP</em>,
<em>18</em>(5), 1541–1554. (<a
href="https://doi.org/10.1007/s11554-020-00990-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of remote sensing image processing, accurate segmentation of high-resolution remote sensing images in real time remains a challenging problem and numerous approaches have been developed for the problem. This paper proposes a new unsupervised approach that can efficiently analyze a remote sensing image and provide accurate segmentation results. The approach performs segmentation in three stages. In the first stage, an image is partitioned into blocks of equal sizes. The mean values of the R, G and B components of the pixels in each block are computed to form a feature vector of the block. A preliminary segmentation result is obtained by clustering the feature vectors with a simple clustering algorithm. In the second stage, a Bayesian approach is applied to refine the preliminary segmentation result. In the final stage, a graph-based method is utilized to recognize regions with complex texture structures. The performance of this approach has been tested on a few benchmark datasets, and its segmentation accuracy is compared with that of many state-of-the-art segmentation tools for remote sensing images. The testing results show that the overall segmentation accuracy of the proposed approach is higher than that of the other tools, and real-time analysis suggests that the approach is promising for real-time applications. An implementation of the approach in MATLAB is freely available upon request.},
  archive      = {J_JRTIP},
  author       = {Song, Yinglei and Qu, Junfeng},
  doi          = {10.1007/s11554-020-00990-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1541-1554},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time segmentation of remote sensing images with a combination of clustering and bayesian approaches},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time registration of remote sensing images with a
markov chain model. <em>JRTIP</em>, <em>18</em>(5), 1527–1540. (<a
href="https://doi.org/10.1007/s11554-020-01043-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic registration of different remote sensing images of the same scene is an important problem in the processing of remote sensing images. Such images usually have different resolutions and computing an accurate registration of them in real time is thus often a challenging task. In this paper, a new statistical approach is developed for the accurate and efficient registration of two remote sensing images with different resolutions. The proposed approach utilizes a statistical model to evaluate the probability for each possible mapping between the two images and computes the one with the maximum probability for registration. Similar to most of the state-of-the-art methods for remote sensing image registration, the proposed approach assumes the existence of an affine transformation between the images to be registered. The registration is performed in three stages. In the first stage, the pixels in each image are efficiently partitioned into two sets with the Otsu’s algorithm and four approximate equations for the parameters in the affine transformation are established. In the second stage, pixels in both images with significant edge geometric features are selected for potential pixel matching. In the third stage, based on the four approximate equations generated in the first stage, an Markov Chain Model-based method is used to efficiently compute the matching that can pair the selected pixels with the maximum likelihood. The parameters of the affine transformation can be determined from the pixel pairs with a least square regression approach. Experimental results on a number of pairs of remote sensing images show that this approach can generate registration results more accurate than those obtained with a few state-of-the-art approaches. In addition, real-time evaluation also shows that the approach is computationally efficient and can be used in real-time applications for remote sensing image processing.},
  archive      = {J_JRTIP},
  author       = {Song, Yinglei and Qu, Junfeng and Liu, Chunmei},
  doi          = {10.1007/s11554-020-01043-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1527-1540},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time registration of remote sensing images with a markov chain model},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Real-time underwater image enhancement: A systematic
review. <em>JRTIP</em>, <em>18</em>(5), 1509–1525. (<a
href="https://doi.org/10.1007/s11554-020-01052-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep sea and ocean explorations have attracted more attention in the marine industry. Most of the marine vehicles, including robots, submarines, and ships, would be equipped with automatic imaging of deep sea layers. There is a reason which the quality of the images taken by the underwater devices is not optimal due to water properties and impurities. Consequently, water absorbs a series of colors, so processing gets more difficult. Scattering and absorption are related to underwater imaging light and are called light attenuation in water. The examination has previously shown that the emergence of some inherent limitations is due to the presence of artifacts and environmental noise in underwater images. As a result, it is hard to distinguish objects from their backgrounds in those images in a real-time system. This paper discusses the effect of the software and hardware parts for the underwater image, surveys the state-of-art different strategies and algorithms in underwater image enhancement, and measures the algorithm performance from various aspects. We also consider the important conducted studies on the field of quality enhancement in underwater images. We have analyzed the methods from five perspectives: (a) hardware and software tools, (b) a variety of underwater imaging techniques, (c) improving real-time image quality, (d) identifying specific objectives in underwater imaging, and (e) assessments. Finally, the advantages and disadvantages of the presented real/non-real-time image processing techniques are addressed to improve the quality of the underwater images. This systematic review provides an overview of the major underwater image algorithms and real/non-real-time processing.},
  archive      = {J_JRTIP},
  author       = {Moghimi, Mohammad Kazem and Mohanna, Farahnaz},
  doi          = {10.1007/s11554-020-01052-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1509-1525},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time underwater image enhancement: A systematic review},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time cheating immune secret sharing for remote sensing
images. <em>JRTIP</em>, <em>18</em>(5), 1493–1508. (<a
href="https://doi.org/10.1007/s11554-020-01005-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To observe the earth surface and its atmospheric interaction, various advanced optical and radar sensors are utilized. This observation returns a huge amount of optical multidimensional remote sensing images which may be used in multidisciplinary fields. The processing of these images in real time is a challenging task because of their high spatial resolution and complex data structure. At the same time, these images are quite confidential in various applications such as in the military and intelligence sectors. For secretly transmitting the remote sensing images in real time, a real-time cheating immune secret sharing approach is introduced in this paper. The proposed approach minimizes the time as well as space complexity for the secret sharing effectively. It also generates meaningful shares without the restriction for any fixed number participants. Generated shares by the proposed approach are cheating immune. That means they can authenticate themselves if tampered with. Experimental results show the effectiveness of the proposed approach.},
  archive      = {J_JRTIP},
  author       = {Shivani, Shivendra and Patel, Subhash Chandra and Arora, Vinay and Sharma, Bhisham and Jolfaei, Alireza and Srivastava, Gautam},
  doi          = {10.1007/s11554-020-01005-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1493-1508},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time cheating immune secret sharing for remote sensing images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion-blurred star image restoration based on multi-frame
superposition under high dynamic and long exposure conditions.
<em>JRTIP</em>, <em>18</em>(5), 1477–1491. (<a
href="https://doi.org/10.1007/s11554-020-00965-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under high dynamic and long exposure conditions, the number of recognized stars on motion-blurred star images decreases, thereby degrading the attitude accuracy of star sensors. To improve the attitude accuracy, a restoration method based on multi-frame superposition, which focuses on the noise removal and quality of restored star images, is proposed for a star sensor. During each short exposure time, the corrected coordinate variation of the same star spot between adjacent star images is determined using a motion recursive model. Subsequently, the corrected star spot region is obtained, and the noise is removed. A restoration algorithm based on multi-frame superposition is proposed, taking the time consumption and quality of restored star image considered simultaneously. Simulation results indicate that the proposed restoration method based on multi-frame superposition is effective in removing noise and improving the quality of restored star images. The star recognition rate in simulation experiments verifies the advantages of the proposed method.},
  archive      = {J_JRTIP},
  author       = {He, Yiyang and Wang, Hongli and Feng, Lei and You, Sihai},
  doi          = {10.1007/s11554-020-00965-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1477-1491},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Motion-blurred star image restoration based on multi-frame superposition under high dynamic and long exposure conditions},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time automated video highlight generation with
dual-stream hierarchical growing self-organizing maps. <em>JRTIP</em>,
<em>18</em>(5), 1457–1475. (<a
href="https://doi.org/10.1007/s11554-020-00957-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video has rapidly become one of the most common sources of visual information transfer. The number of videos uploaded to YouTube in a single day is estimated to take over 82 years to watch. Automated tools and techniques for analyzing and understanding video content, thus, have become an essential requirement. This paper addresses the problem of video highlight generation for large video files. We propose a novel skimming-based unsupervised video highlight generation method utilizing statistical image processing and data clustering, which process frame-level static and dynamic features of input video in two streams. The dynamic feature stream is represented by computing a dense optical flow for each consecutive frame, providing instantaneous velocity information for every pixel, which is then characterized by a per-frame orientation histogram, weighted by the norm, with orientations quantized. To process multi-scene videos, we utilize the divisive hierarchical clustering capability of growing self-organizing map (GSOM) using a dual-step top-down hierarchical approach in which the first level consists of clustering of spatial and temporal features of the video and in the second level, each parent cluster is hierarchically subdivided into child clusters using GSOM. The video highlight generation process is conducted real time by evaluating segments of video snippets based on a pre-defined time interval. We demonstrate the accuracy, robustness and the quality of highlights generated using a qualitative analysis conducted using 1625 human experts on highlights generated from two datasets. Further, we conduct a runtime analysis to demonstrate the efficient processing capability of the proposed method, to be used in real-time settings.},
  archive      = {J_JRTIP},
  author       = {Gunawardena, Pawara and Amila, Oshada and Sudarshana, Heshan and Nawaratne, Rashmika and Luhach, Ashish Kr. and Alahakoon, Damminda and Perera, Amal Shehan and Chitraranjan, Charith and Chilamkurti, Naveen and De Silva, Daswin},
  doi          = {10.1007/s11554-020-00957-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1457-1475},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time automated video highlight generation with dual-stream hierarchical growing self-organizing maps},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Majorization–minimization approach for real-time enhancement
of sparsity-driven SAR imaging. <em>JRTIP</em>, <em>18</em>(5),
1441–1455. (<a
href="https://doi.org/10.1007/s11554-021-01076-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The earlier works in the context of sparsity-driven SAR imaging have shown significant improvement in the reconstruction process due to admitting sparsity as a prior. In spite of the importance of real-time processing requirement in the remote sensing (RS) applications, most of the works have not focused on real-time procedures and reducing the computational burden, but rather enhancing the quality of formed image. To address this weakness, this paper presents a problem-driven algorithm, which relies on Majorization–Minimization (MM) procedure. Using MM in our solutions, a simpler surrogate optimization problem is solved instead of the difficult original form. To show the efficacy of MM algorithm in real-time applications experimental results based on simulated and real data along with a performance analysis are presented. All results validate the superiority of the proposed MM-based method in terms of computational load and processing time as compared with previous works.},
  archive      = {J_JRTIP},
  author       = {Asadipooya, Anahita and Samadi, Sadegh and Moradikia, Majid and Mohseni, Reza},
  doi          = {10.1007/s11554-021-01076-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1441-1455},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Majorization–Minimization approach for real-time enhancement of sparsity-driven SAR imaging},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time statistical image and video processing for remote
sensing and surveillance applications. <em>JRTIP</em>, <em>18</em>(5),
1435–1439. (<a
href="https://doi.org/10.1007/s11554-021-01168-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Khosravi, Mohammad R. and Tavallali, Pooya},
  doi          = {10.1007/s11554-021-01168-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1435-1439},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time statistical image and video processing for remote sensing and surveillance applications},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). <span class="math display">C<sup>3</sup>Net</span>:
End-to-end deep learning for efficient real-time visual active camera
control. <em>JRTIP</em>, <em>18</em>(4), 1421–1433. (<a
href="https://doi.org/10.1007/s11554-021-01077-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for automated real-time visual systems in applications such as smart camera surveillance, smart environments, and drones necessitates the improvement of methods for visual active monitoring and control. Traditionally, the active monitoring task has been handled through a pipeline of modules such as detection, filtering, and control. However, such methods are difficult to jointly optimize and tune their various parameters for real-time processing in resource constraint systems. In this paper a deep Convolutional Camera Controller Neural Network is proposed to go directly from visual information to camera movement to provide an efficient solution to the active vision problem. It is trained end-to-end without bounding box annotations to control a camera and follow multiple targets from raw pixel values. Evaluation through both a simulation framework and real experimental setup, indicate that the proposed solution is robust to varying conditions and able to achieve better monitoring performance than traditional approaches both in terms of number of targets monitored as well as in effective monitoring time. The advantage of the proposed approach is that it is computationally less demanding and can run at over 10 FPS ( $$\sim 4\times $$ speedup) on an embedded smart camera providing a practical and affordable solution to real-time active monitoring.},
  archive      = {J_JRTIP},
  author       = {Kyrkou, Christos},
  doi          = {10.1007/s11554-021-01077-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1421-1433},
  shortjournal = {J. Real-Time Image Process.},
  title        = {$$\text{C}^{3}\text{Net}$$: End-to-end deep learning for efficient real-time visual active camera control},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-branch sharing network for real-time 3D brain tumor
segmentation. <em>JRTIP</em>, <em>18</em>(4), 1409–1419. (<a
href="https://doi.org/10.1007/s11554-020-01049-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are one of the most lethal diseases in the world. The segmentation of brain tumor is of great significance for physician in formulating appropriate diagnostic and treatment plans, not only accurate but also efficient 3D segmentation algorithms are urgently demanded in clinical practice. Nowadays, several 3D convolution neural networks have achieved impressive segmentation performance. However, these architectures come with extremely high computational overheads due to the extra depth dimensionality in 3D convolution, which may make these models prohibitive from practical large-scale clinic application. In this work, we aim at designing a more efficient and lightweight network without accuracy reduction for real-time segmentation of magnetic resonance images. To this end, we propose a multi-branch sharing network which consists of novel multi-branch sharing units. Different from other works, our proposed multi-branch sharing units focus the information sharing and communication between grouped layers by leveraging a Multiplexer operation, which can reduce the computational cost significantly while maintaining decent performance. Extensive experimental results on the BraTS2018 challenge dataset show that the proposed architecture achieve real-time inference while maintaining high accuracy for 3D brain magnetic resonance image segmentation.},
  archive      = {J_JRTIP},
  author       = {Li, Jiangyun and Zheng, Junfeng and Ding, Meng and Yu, Hong},
  doi          = {10.1007/s11554-020-01049-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1409-1419},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-branch sharing network for real-time 3D brain tumor segmentation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multichannel heuristic learning based single layer neural
network filter for mixed noise suppression from color doppler ultrasound
images. <em>JRTIP</em>, <em>18</em>(4), 1397–1408. (<a
href="https://doi.org/10.1007/s11554-020-01061-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed noise suppression from color Doppler ultrasound (CDUS) images is always a challenging task because the noise distribution usually does not have a parametric model and heavy tail. It affects the inherent features of the image awkwardly. Consequently, identifying an internal blockage or hemorrhage of the patient becomes arduous in such conditions. An acquired CDUS image is majorly affected by speckle noise and can be coupled with Gaussian and impulse noises. In this paper, the evolutionary multichannel Jaya based functional link artificial neural network (named as M-Jaya-FLANN) has been proposed to get rid of mixed noise from the CDUS images. The subjective evaluation and the measurement of qualitative metrics, such as structural similarity index, computational time, convergence rate, and Friedman’s test are carried out for the performance analysis of different filters. The research outcomes exhibit the supremacy of the proposed filter over other competitive filters and can handle real-time noise elimination after completion of training. For the experimentation purpose, CDUS image data are collected from Medanta hospital, Ranchi, India.},
  archive      = {J_JRTIP},
  author       = {Kumar, Manish and Mishra, Sudhansu Kumar and Choubey, Dilip Kumar and Jangir, Sunil Kumar and Goyal, Dinesh},
  doi          = {10.1007/s11554-020-01061-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1397-1408},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multichannel heuristic learning based single layer neural network filter for mixed noise suppression from color doppler ultrasound images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel PCA–whale optimization-based deep neural network
model for classification of tomato plant diseases using GPU.
<em>JRTIP</em>, <em>18</em>(4), 1383–1396. (<a
href="https://doi.org/10.1007/s11554-020-00987-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human population is growing at a very rapid scale. With this progressive growth, it is extremely important to ensure that healthy food is available for the survival of the inhabitants of this planet. Also, the economy of developing countries is highly dependent on agricultural production. The overall economic balance gets affected if there is a variance in the demand and supply of food or agricultural products. Diseases in plants are a great threat to the yield of the crops thereby causing famines and economy slow down. Our present study focuses on applying machine learning model for classifying tomato disease image dataset to proactively take necessary steps to combat such agricultural crisis. In this work, the dataset is collected from publicly available plant–village dataset. The significant features are extracted from the dataset using the hybrid-principal component analysis–Whale optimization algorithm. Further the extracted data are fed into a deep neural network for classification of tomato diseases. The proposed model is then evaluated with the classical machine learning techniques to establish the superiority in terms of accuracy and loss rate metrics.},
  archive      = {J_JRTIP},
  author       = {Gadekallu, Thippa Reddy and Rajput, Dharmendra Singh and Reddy, M. Praveen Kumar and Lakshmanna, Kuruva and Bhattacharya, Sweta and Singh, Saurabh and Jolfaei, Alireza and Alazab, Mamoun},
  doi          = {10.1007/s11554-020-00987-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1383-1396},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel PCA–whale optimization-based deep neural network model for classification of tomato plant diseases using GPU},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time monocular depth estimation with adaptive receptive
fields. <em>JRTIP</em>, <em>18</em>(4), 1369–1381. (<a
href="https://doi.org/10.1007/s11554-020-01036-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is a popular research topic in the field of autonomous driving. Nowadays many models are leading in accuracy but performing poorly in a real-time scenario. To effectively increase the depth estimation efficiency, we propose a novel model combining a multi-scale pyramid architecture for depth estimation together with adaptive receptive fields. The pyramid architecture reduces the trainable parameters from dozens of mega to less than 10 mega. Adaptive receptive fields are more sensitive to objects at different depth/distances in images, leading to better accuracy. We have adopted stacked convolution kernels instead of raw kernels to compress the model. Thus, the model that we proposed performs well in both real-time performance and estimation accuracy. We provide a set of experiments where our model performs better in terms of Eigen split than other previously known models. Furthermore, we show that our model is also better in runtime performance in regard to the depth estimation to the rest of models but the Pyd-Net model. Finally, our model is a lightweight depth estimation model with state-of-the-art accuracy.},
  archive      = {J_JRTIP},
  author       = {Ji, Zhenyan and Song, Xiaojun and Guo, Xiaoxuan and Wang, Fangshi and Armendáriz-Iñigo, José Enrique},
  doi          = {10.1007/s11554-020-01036-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1369-1381},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time monocular depth estimation with adaptive receptive fields},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient unsupervised monocular depth estimation using
attention guided generative adversarial network. <em>JRTIP</em>,
<em>18</em>(4), 1357–1368. (<a
href="https://doi.org/10.1007/s11554-021-01092-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based approaches to depth estimation are rapidly advancing, offering better performance over traditional computer vision approaches across many domains. However, for many critical applications, cutting-edge deep-learning based approaches require too much computational overhead to be operationally feasible. This is especially true for depth-estimation methods that leverage adversarial learning, such as Generative Adversarial Networks (GANs). In this paper, we propose a computationally efficient GAN for unsupervised monocular depth estimation using factorized convolutions and an attention mechanism. Specifically, we leverage the Extremely Efficient Spatial Pyramid of Depth-wise Dilated Separable Convolutions (EESP) module of ESPNetv2 inside the network, leading to a total reduction of $$22.8\%$$ , $$35.37\%$$ , and $$31.5\%$$ in the number of model parameters, FLOPs, and inference time respectively, as compared to the previous unsupervised GAN approach. Finally, we propose a context-aware attention architecture to generate detail-oriented depth images. We demonstrate superior performance of our proposed model on two benchmark datasets KITTI and Cityscapes. We have also provided more qualitative examples (Fig. 8) at the end of this paper.},
  archive      = {J_JRTIP},
  author       = {Bhattacharyya, Sumanta and Shen, Ju and Welch, Stephen and Chen, Chen},
  doi          = {10.1007/s11554-021-01092-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1357-1368},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient unsupervised monocular depth estimation using attention guided generative adversarial network},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). AFTD-net: Real-time anchor-free detection network of threat
objects for x-ray baggage screening. <em>JRTIP</em>, <em>18</em>(4),
1343–1356. (<a
href="https://doi.org/10.1007/s11554-021-01136-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray baggage screening is a vitally important task to detect all kinds of threat objects at controlled access positions, which can prevent crime and guard personal safety. It is generally performed by screeners to visually determine whether a bag contains threat objects. However, manual detection shows several limitations, from the detection errors to different detection results produced by different screeners. To address these issues, several automated detection approaches have been proposed; nevertheless, none of the methods can achieve end-to-end detection and the results have only classification information without positional information. In this paper, we propose a real-time anchor-free detector of threat objects that can recognize threat objects without using pre-designed anchor boxes. We employ a lightweight but strong backbone network: MobileNetV2 to extract the multi-level information. The backbone network is followed by a deformation layer which aims at handling the nonrigid deformation of threat objects in X-ray images. To further strengthen the proposed network, we design a context enhancement module to aggregate the multi-scale features and generate the enhanced features. We name the network as anchor-free detection network of threat objects (AFTD-Net). We demonstrate the effectiveness of the proposed method against other object detection algorithms on the GDXray database. Our AFTD-Net is a fully convolutional network which does not need any pre-designed anchors and achieves a real-time computation speed of 44.8 FPS.},
  archive      = {J_JRTIP},
  author       = {Wei, Yiru and Zhu, Zhiliang and Yu, Hai and Zhang, Wei},
  doi          = {10.1007/s11554-021-01136-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1343-1356},
  shortjournal = {J. Real-Time Image Process.},
  title        = {AFTD-net: Real-time anchor-free detection network of threat objects for X-ray baggage screening},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on artificial intelligence and machine
learning for real-time image processing. <em>JRTIP</em>, <em>18</em>(4),
1341. (<a href="https://doi.org/10.1007/s11554-021-01157-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Balamurugan, S.},
  doi          = {10.1007/s11554-021-01157-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1341},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Special issue on artificial intelligence and machine learning for real-time image processing},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Realtime fire detection using CNN and search space
navigation. <em>JRTIP</em>, <em>18</em>(4), 1331–1340. (<a
href="https://doi.org/10.1007/s11554-021-01153-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent search techniques and an intelligent agent for smart search are useful in many application domains. We develop a state space navigational model for intelligent agents aimed at industrial surveillance from fire hazards. Our focus is on fire detection using the convolution neural network then proactively search the area which is more likely to have routes toward the target. This problem can be simulated into an optimization problem over a state space, which can be figure out effectively through a greedy algorithm. We also compare our approach with both uninformed and informed search algorithms. We evaluate our proposed system using various search algorithms for search and rescue agent. The analysis of the results obtained demonstrate the efficiency of the system.},
  archive      = {J_JRTIP},
  author       = {Rahmatov, Nematullo and Paul, Anand and Saeed, Faisal and Seo, Hyuncheol},
  doi          = {10.1007/s11554-021-01153-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1331-1340},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Realtime fire detection using CNN and search space navigation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time high-speed autonomous driving based on a
low-cost RTK-GPS. <em>JRTIP</em>, <em>18</em>(4), 1321–1330. (<a
href="https://doi.org/10.1007/s11554-021-01084-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the feasibility of a low-cost real-time kinematic GPS (RTK-GPS) sensor for localization of an autonomous vehicle to achieve a high-speed driving is studied. For achieving high-speed autonomous driving, although the image-based method combined with a GPS can be a good approach for localization, the RTK-GPS position can be utilized with a low cost. On high-speed driving, it is important to acquire an accurate localization, because a less accurate position may degrade the performance to follow the waypoints on a target path with proper driving stability. Thus, in this study, the RTK-GPS position was applied to reduce position errors in a test vehicle with a low-cost GPS and the RTKLIB. A modified adaptive look-ahead distance pure pursuit algorithm was implemented to control the test vehicle. An autonomous driving experiment using the RTK-GPS position was carried out to verify the performance of the vehicle at a high speed of 130 kph on a track of 2477 m long with various corners and inclinations in a racing circuit. The test vehicle with the proposed real-time autonomous driving system using the RTK-GPS position achieved 111 s to complete a full lap on the racing track without departures from the track and noticeable lateral errors. This result was 32 s slower than the record accomplished by a professional human driver&#39;s 79 s.},
  archive      = {J_JRTIP},
  author       = {Park, Seonghyeon and Ryu, Seokhoon and Lim, Jihea and Lee, Young-Sup},
  doi          = {10.1007/s11554-021-01084-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1321-1330},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time high-speed autonomous driving based on a low-cost RTK-GPS},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convolution neural network with low operation FLOPS and high
accuracy for image recognition. <em>JRTIP</em>, <em>18</em>(4),
1309–1319. (<a
href="https://doi.org/10.1007/s11554-021-01140-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolution neural network makes deeper and wider for better accuracy, but requires higher computations. When the neural network goes deeper, some information loss is more. To improve this drawback, the residual structure was developed to connect the information of the previous layers. This is a good solution to prevent the loss of information, but it requires a huge amount of parameters for deeper layer operations. In this study, the fast computational algorithm is proposed to reduce the parameters and to save the operations with the modification of DenseNet deep layer block. With channel merging procedures, this solution can reduce the dilemma of multiple growth of the parameter quantity for deeper layer. This approach is not only to reduce the parameters and FLOPs, but also to keep high accuracy. Comparisons with the original DenseNet and RetNet-110, the parameters can be efficiency reduced about 30–70%, while the accuracy degrades little. The lightweight network can be implemented on a low-cost embedded system for real-time application.},
  archive      = {J_JRTIP},
  author       = {Hsia, Shih-Chang and Wang, Szu-Hong and Chang, Chuan-Yu},
  doi          = {10.1007/s11554-021-01140-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1309-1319},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Convolution neural network with low operation FLOPS and high accuracy for image recognition},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Focusing highly squinted missile-borne SAR data using
azimuth frequency nonlinear chirp scaling algorithm. <em>JRTIP</em>,
<em>18</em>(4), 1301–1308. (<a
href="https://doi.org/10.1007/s11554-021-01135-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of SAR imaging, the efficiency and rapidity of imaging algorithms have always been a research direction, and real-time imaging is a reflection of the speed of the imaging algorithm. For missile-borne SAR, real-time imaging is particularly important because it provides radar with a sense of the battlefield environment. In the case with a highly squinted angle, the azimuth space variance and the coupling between range and azimuth dimension will become serious in imaging, thus an azimuth frequency nonlinear chirp scaling (AFNCS) algorithm is proposed to solve this problem. Based on linear range walk correction, a novel chirp scaling algorithm is adopted to correct the range migration, and then a high-order phase filtering factor is introduced into the azimuth dimension frequency domain to decrease the azimuth space variance. In addition, combined with the SPECtral Analysis method, the image is focused on the Doppler domain. The AFNCS algorithm does not need complex mathematical calculations such as interpolation and can meet the real-time requirement of missile-borne SAR imaging. Simulation results illustrate the effectiveness of the proposed algorithm. The integration of the research in this paper and the deep learning will further pave the way for real-time SAR imaging applications in disaster monitoring, security and surveillance.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yan and Qu, Tan},
  doi          = {10.1007/s11554-021-01135-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1301-1308},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Focusing highly squinted missile-borne SAR data using azimuth frequency nonlinear chirp scaling algorithm},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E2BNet: MAC-free yet accurate 2-level binarized neural
network accelerator for embedded systems. <em>JRTIP</em>,
<em>18</em>(4), 1285–1299. (<a
href="https://doi.org/10.1007/s11554-021-01148-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are widely used in computer vision, pattern recognition, and speech recognition and achieve high accuracy at the cost of remarkable computation. High computational complexity and memory accesses of such networks create a big challenge for using them in resource-limited and low-power embedded systems. Several binary neural networks have been proposed that exploit only 1-bit values for both weights and activations. Binary neural networks substitute complex multiply-accumulation operations with bitwise logic operations to reduce computations and memory usage. However, these quantized neural networks suffer from accuracy loss, especially in big datasets. In this paper, we introduce a quantized neural network with 2-bit weights and activations that is more accurate compared to the state-of-the-art quantized neural networks, and also the accuracy is close to the full precision neural networks. Moreover, we propose E2BNet, an efficient MAC-free hardware architecture that increases power efficiency and throughput/W about 3.6 × and 1.5 × , respectively, compared to the state-of-the-art quantized neural networks. E2BNet processes more than 500 images/s on the ImageNet dataset that not only meet real-time requirements of images/video processing but also can be deployed on high frame rate video applications.},
  archive      = {J_JRTIP},
  author       = {Mirsalari, Seyed Ahmad and Nazari, Najmeh and Ansarmohammadi, Seyed Ali and Salehi, Mostafa E. and Ghiasi, Soheil},
  doi          = {10.1007/s11554-021-01148-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1285-1299},
  shortjournal = {J. Real-Time Image Process.},
  title        = {E2BNet: MAC-free yet accurate 2-level binarized neural network accelerator for embedded systems},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight network with one-shot aggregation for image
super-resolution. <em>JRTIP</em>, <em>18</em>(4), 1275–1284. (<a
href="https://doi.org/10.1007/s11554-021-01127-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural network-based methods have achieved remarkable performance for the single-image super-resolution task. However, huge computational complexity and memory consumption of these methods limit their applications on the resource-constrained device. In this paper, we propose a lightweight network named one-shot aggregation network (OAN) to address this problem for image super-resolution. Specifically, to take advantage of diversified features with multiple receptive fields and overcome the inefficiency of dense aggregation which aggregates all previous feature maps to the subsequent layer, we propose an one-shot aggregation block as the cascaded block to adopt one-shot aggregation strategy by aggregating the intermediate features with multiple receptive fields only once in the last feature map. Experimental results on benchmark datasets demonstrate that our proposed OAN outperforms the state-of-the-art SR methods in terms of the reconstruction quality, the number of parameters, and multiply-accumulate operations.},
  archive      = {J_JRTIP},
  author       = {Tang, Rui and Chen, Lihui and Zou, Yiye and Lai, Zhibing and Albertini, Marcelo Keese and Yang, Xiaomin},
  doi          = {10.1007/s11554-021-01127-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1275-1284},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight network with one-shot aggregation for image super-resolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast and effective video vehicle detection method
leveraging feature fusion and proposal temporal link. <em>JRTIP</em>,
<em>18</em>(4), 1261–1274. (<a
href="https://doi.org/10.1007/s11554-021-01121-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection in videos is a valuable but challenging technology in traffic monitoring. Due to the advantage of real-time detection, Single Shot MultiBox Detector (SSD) is often used to detect vehicles in images. However, the accuracy degradation caused by SSD is one of the significant problems in video vehicle detection. To address this problem in real time, this paper enhances the detection performance by improving the SSD and employing the relationship of inter-frame detections. We propose a feature-fused SSD detector and a Tracking-guided Detections Optimizing (TDO) strategy for fast and effective video vehicle detection. We introduce a lightweight feature fusion sub-network to the standard SSD network, which aggregate the deeper layer features into the shallower layer features to enhance the semantic information of the shallower layer features. At the post-processing stage of the feature-fused SSD, the non-maximum suppression (NMS) is replaced by the TDO strategy, which link vehicles of inter-frames by fast tracking algorithm. Thus the missed detections can be compensated by the propagated results, and the confidence of the final results can be optimized in the temporal. Our approach significantly improves the temporal consistency of the detection results with lower complexity computations. We evaluate the proposed method on two datasets. The experiments on our labeled highway dataset show that the mean average precision (mAP) of our method is 8.2% higher than that of the base detector. The runtime of our feature-fused SSD is 27.1 frames per second (fps), which is suitable for real-time detection. The experiments on the ImageNet VID dataset prove that the proposed method is comparable with the state-of-the-art detectors as well.},
  archive      = {J_JRTIP},
  author       = {Yang, Yanni and Song, Huansheng and Sun, Shijie and Zhang, Wentao and Chen, Yan and Rakal, Lionel and Fang, Yong},
  doi          = {10.1007/s11554-021-01121-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1261-1274},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A fast and effective video vehicle detection method leveraging feature fusion and proposal temporal link},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mobile augmented reality application for supporting
real-time skin lesion analysis based on deep learning. <em>JRTIP</em>,
<em>18</em>(4), 1247–1259. (<a
href="https://doi.org/10.1007/s11554-021-01109-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma is considered the deadliest skin cancer and when it is in an advanced state it is difficult to treat. Diagnoses are visually performed by dermatologists, by naked-eye observation. This paper proposes an augmented reality smartphone application for supporting the dermatologist in the real-time analysis of a skin lesion. The app augments the camera view with information related to the lesion features generally measured by the dermatologist for formulating the diagnosis. The lesion is also classified by a deep learning approach for identifying melanoma. The real-time process adopted for generating the augmented content is described. The real-time performances are also evaluated and a user study is also conducted. Results revealed that the real-time process may be entirely executed on the Smartphone and that the support provided is well judged by the target users.},
  archive      = {J_JRTIP},
  author       = {Francese, Rita and Frasca, Maria and Risi, Michele and Tortora, Genoveffa},
  doi          = {10.1007/s11554-021-01109-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1247-1259},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A mobile augmented reality application for supporting real-time skin lesion analysis based on deep learning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient local cascading residual network for real-time
single image super-resolution. <em>JRTIP</em>, <em>18</em>(4),
1235–1246. (<a
href="https://doi.org/10.1007/s11554-021-01134-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, single image super-resolution (SISR) based on convolutional neural networks (CNNs) has been represented remarkable performance. Powerful characterization of CNN is important for recent methods to learn an intricate non-linear mapping between high-resolution and corresponding low-resolution images. However, a deeper and wider network structure brings superior performance while increasing the number of network parameters and calculations so that it is difficult to handle the real-time information. Hence, it can be embedded in mobile devices with difficulty. Inspired by the above motivation, a lightweight network for the real-time SISR is proposed by stacking efficient cascading residual blocks, which consist of several concatenate effective modules with wide activation. To further improve the network performance, with the increase of a slight number of parameters, the proposed network cooperates with a lightweight residual efficient channel attention module to capture feature interaction between channels. Extensive experiments provide significant demonstrations that the proposed network obtains the superior trade-off between performance and parameters compared with other current methods. The lightweight trait of our method allows it to implement real-time image processing and can be embedded in mobile devices.},
  archive      = {J_JRTIP},
  author       = {Yang, Haoran and Dou, Qingyu and Liu, Kai and Liu, Zitao and Francese, Rita and Yang, Xiaomin},
  doi          = {10.1007/s11554-021-01134-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1235-1246},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient local cascading residual network for real-time single image super-resolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lightweight multi-scale feature integration network for
real-time single image super-resolution. <em>JRTIP</em>, <em>18</em>(4),
1221–1234. (<a
href="https://doi.org/10.1007/s11554-021-01142-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous methods based on convolutional neural networks (CNNs) have been proposed to attain satisfactory performance in single image super-resolution (SISR). Meanwhile, diverse lightweight CNN-based networks have been proposed to achieve applicability in real-time scenarios. However, the receptive fields in lightweight networks are limited because they do not make good use of multi-scale information. In this paper, we propose a lightweight multi-scale feature integration network (MFIN) to address the above issue. Specifically, to expand the receptive fields for global features, MFIN is constructed by cascading the multi-scale feature integration blocks (MFIBs) in a serial manner. Each MFIB contains a multi-scale feature extraction module (MFEM) and a feature integration unit (FIU). To enlarge the receptive fields at a granular level, the features in MFEM are cascaded in a parallel manner. To capture the full-image dependencies, FIU incorporates the dense and pixel-wise correlations from the outputs of MFEM efficiently. The conducted experiments demonstrate that our method outperforms state-of-the-art methods in quantitative and qualitative evaluation. Notably, the experimental results on running time state that our method can achieve real-time performance.},
  archive      = {J_JRTIP},
  author       = {He, Zheng and Liu, Kai and Liu, Zitao and Dou, Qingyu and Yang, Xiaomin},
  doi          = {10.1007/s11554-021-01142-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1221-1234},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight multi-scale feature integration network for real-time single image super-resolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time classification of brain tumors in MRI images with
a convolutional operator-based hidden markov model. <em>JRTIP</em>,
<em>18</em>(4), 1207–1219. (<a
href="https://doi.org/10.1007/s11554-021-01072-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of brain tumors based on the brain magnetic resonance imaging (MRI) results of patients has become an important problem in medical image processing. A computer program that can efficiently analyze brain MRI images of patients in real time and generate accurate classification results of the tumors in these images can significantly reduce the amount of time needed for diagnosis, which may increase the chances for patients to survive. This paper proposes a new statistical method that can accurately classify three types of brain tumors based on MRI images, the three types of tumors considered include pituitary tumor, glioma, and meningioma. The features for a pixel in an MRI image are obtained by applying a set of convolutional operators to the neighborhood area of the pixel. For training, a hidden Markov model (HMM) is constructed and trained from a training dataset by computing a statistical profile for the feature vectors for pixels in the tumor regions of each type of brain tumors. In addition, a statistical profile is also obtained for pixels that are in the background of a tumor. For classification, the trained HMM is used to assign labels to pixels in an MRI image with a dynamic programming approach and the classification result of the image is obtained from the labels assigned to the tumor region. Both the training and classification processes can be efficiently performed in linear time and does not require the availability of a large amount of computational resources. Experimental results on a large dataset of MRI images show that the proposed method can provide classification results with high accuracy for all three types of brain tumors. A comparison with state-of-the-art methods for brain tumor classification suggests that the proposed method can achieve improved classification accuracy. In addition, real-time analysis also reveals that the proposed approach can probably be used for real-time classification of brain tumors.},
  archive      = {J_JRTIP},
  author       = {Li, Guoliang and Sun, Jinhong and Song, Yinglei and Qu, Junfeng and Zhu, Zhiyu and Khosravi, Mohammad R.},
  doi          = {10.1007/s11554-021-01072-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1207-1219},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time classification of brain tumors in MRI images with a convolutional operator-based hidden markov model},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Background subtraction in videos using LRMF and CWM
algorithm. <em>JRTIP</em>, <em>18</em>(4), 1195–1206. (<a
href="https://doi.org/10.1007/s11554-021-01120-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction is a substantially important video processing task that aims at separating the foreground from a video to make the post-processing tasks efficient and relatively easier. Until now, several different techniques have been proposed for this task, but most of them cannot perform well for the videos having variations in both the foreground and the background. In this paper, a novel background subtraction technique is proposed that aims at progressively fitting a particular subspace for the background that is obtained from $$L_1$$ -low-rank matrix factorization using the cyclic weighted median algorithm and a certain distribution of a mixture of Gaussian of noise for the foreground. The expectation maximization algorithm is applied to optimize the Gaussian mixture model. Furthermore, to eliminate the camera jitter effects, the affine transformation operator is involved to align the successive frames. Finally, the effectiveness of the proposed method is augmented using a subsampling technique that can accelerate the proposed method to execute on an average more than 250 frames per second while maintaining good performance in terms of accuracy. The performance of the proposed method is compared with other state-of-the-art methods and it was concluded that the proposed method performs well in terms of F-measure and computational complexity.},
  archive      = {J_JRTIP},
  author       = {Munir, Wajiha and Siddiqui, Adil Masood and Imran, Muhammad and Tauqir, Imran and Zulfiqar, Nazish and Iqbal, Waseem and Ahmad, Awais},
  doi          = {10.1007/s11554-021-01120-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1195-1206},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Background subtraction in videos using LRMF and CWM algorithm},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A locally-processed light-weight deep neural network for
detecting colorectal polyps in wireless capsule endoscopes.
<em>JRTIP</em>, <em>18</em>(4), 1183–1194. (<a
href="https://doi.org/10.1007/s11554-021-01126-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless capsule endoscopes (WCE) are revolutionary devices for noninvasive inspection of gastrointestinal tract diseases. However, it is tedious and error-prone for physicians to inspect the huge number of captured images. Artificial Intelligence supports computer-aided diagnostic tools to tackle this challenge. Unlike previous research focusing on the application of large deep neural network (DNN) models for processing images that have been saved on the computer, we propose a light-weight DNN model that has the potential of running locally in the WCE. Thus, only images with diseases are transmitted, saving energy on data transmission. Several aspects of the design are presented in detail, including the DNN’s architecture, the loss function, the criterion of true positive, and data augmentation. We explore design parameters of the DNN architecture in several experiments. These experiments use a training dataset of 1222 images and a test dataset with 153 images. The results of our study indicate that our designed DNN has an Average Precision of AP $$_{25} = 91.7\%$$ on our test dataset while the parameter storage size is only $$29.1\,\mathrm {KB}$$ , which is small enough to run locally on a WCE. In addition, the real-time performance of the designed DNN model is tested on an FPGA, completing one image classification in less than $$6.28\,\mathrm {ms}$$ , which is much less than the $$167\,\mathrm {ms}$$ needed to achieve real-time operation on the WCE. We conclude that our DNN model possesses significant advantages over previous models for WCEs, in terms of model size and real-time performance.},
  archive      = {J_JRTIP},
  author       = {Wang, Yunlong and Yoo, Sunyoung and Braun, Jan-Matthias and Nadimi, Esmaeil S.},
  doi          = {10.1007/s11554-021-01126-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1183-1194},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A locally-processed light-weight deep neural network for detecting colorectal polyps in wireless capsule endoscopes},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-pathway attention network for real-time facial
expression recognition. <em>JRTIP</em>, <em>18</em>(4), 1173–1182. (<a
href="https://doi.org/10.1007/s11554-021-01123-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scholars are committed to using deep learning methods to study facial expression recognition (FER). In recent years, FER has gradually been confined to psychology research in the early days to now involves knowledge of many disciplines such as physiology, psychology, cognition and medicine. With the extreme achievement of computer version techniques, various convolutional neural network structures were developed for real-time and accurate FER. There are two main problems in the existing convolutional neural network for handling FER problems: insufficient training data caused over-fitting and expression-unrelated intra-class differences. In this paper, we propose a two-pathway attention network to solve these two problems better. We suppress the intra-class differences efficiently by extracting facial regions based on facial muscle movements driven by facial expressions. We prevent deep networks from insufficient training data by extensively extracting global structures and local facial regions as the training dataset to feed a two-pathway ensemble model. Further more, we weight the whole feature maps from the global image and local regions by introducing an attention mechanism module to reweighs each part according to its contribution to FER. We adopt real-time facial region extraction and multi-layer feature data compression to ensure the real-time performance of the algorithm and reduce the amount of parameters in ensemble model. Experiments on public datasets suggest that our method certifies its effectiveness, reaches human-level performance, and outperforms current state-of-the-art methods with 92.8% on CK+ and 87.0% on FERPLUS.},
  archive      = {J_JRTIP},
  author       = {Wang, Lining and He, Zheng and Meng, Bin and Liu, Kai and Dou, Qingyu and Yang, Xiaomin},
  doi          = {10.1007/s11554-021-01123-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1173-1182},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Two-pathway attention network for real-time facial expression recognition},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on deep learning for emerging embedded
real-time image and video processing systems. <em>JRTIP</em>,
<em>18</em>(4), 1167–1171. (<a
href="https://doi.org/10.1007/s11554-021-01156-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Jeon, Gwanggil and Chehri, Abdellah},
  doi          = {10.1007/s11554-021-01156-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1167-1171},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Special issue on deep learning for emerging embedded real-time image and video processing systems},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The analysis of intelligent real-time image recognition
technology based on mobile edge computing and deep learning.
<em>JRTIP</em>, <em>18</em>(4), 1157–1166. (<a
href="https://doi.org/10.1007/s11554-020-01039-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to improve the accuracy of real-time image recognition in the context of the Internet of Things (IoT), reduce the core network pressure of the IoT and the proportion of IoT broadband, and meet people’s demand for internet image transmission. An intelligent image fusion system based on mobile edge computing (MEC) and deep learning is proposed, which can extract the features of images and optimize the sum of intra-class distance and inter-class distance relying on the hierarchical mode of deep learning, and realize distributed computing with the edge server and base station. Through comparison with other algorithms and strategies on the text and character data sets, the effectiveness of the constructed system is verified in the performance of the algorithm and the IoT. The results reveal that the application of the unsupervised learning hierarchical discriminant analysis (HDA) has better accuracy and recall in various databases compared with conventional image recognition algorithms. When the sum intra-class and inter-class distance K is 2, the accuracy of the algorithm can be as high as 98%. The combination of MEC and layered algorithms effectively reduces the pressures of core network and shortens the response time, greatly reduces the broadband occupancy ratio. The performance of IoT is increased by 37.03% compared with the general extraction and common cloud computing. Image recognition based on the MEC architecture can reduce the amount of network transmission and reduce the response time under the premise of ensuring the recognition rate, which can provide a theoretical basis for the research and application of user image recognition under the IoT.},
  archive      = {J_JRTIP},
  author       = {Shen, Tao and Gao, Chan and Xu, Dawei},
  doi          = {10.1007/s11554-020-01039-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1157-1166},
  shortjournal = {J. Real-Time Image Process.},
  title        = {The analysis of intelligent real-time image recognition technology based on mobile edge computing and deep learning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nighttime object detection system with lightweight deep
network for internet of vehicles. <em>JRTIP</em>, <em>18</em>(4),
1141–1155. (<a
href="https://doi.org/10.1007/s11554-021-01110-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving systems in internet of vehicles (IoV) applications usually adopt a cloud computing mode. In these systems, information got at the edge of the cloud computing center for data analysis and situation response. However, the conventional IoV face enormous challenges to meet the requirements in terms of storage, communication, and computing problems because of the considerable amount of information on the traffic environment. The environment perception during the nighttime is poorer than that during the daytime that this problem also requires addressing. To solve these problems, we propose a nighttime object detection scheme based on a lightweight deep learning model in the edge computing mode. First, the pedestrian detection and the vehicle detection algorithm that using the thermal images based on the YOLO architecture. We can implement the model on edge devices that can achieve real-time detection through the designed lightweight strategy. Next, a spatial prior information and temporal prior information into the detection algorithm and divide the frames into key and non-key frames to increase the performance and speed of the system simultaneously. Finally, we implemented the detection network for performance and feasibility verification on the Jetson TX2 edge device. The experimental results show that the proposed system can achieve real-time and high-accuracy object detection on edge devices.},
  archive      = {J_JRTIP},
  author       = {Jhong, Sin-Ye and Chen, Yung-Yao and Hsia, Chih-Hsien and Lin, Shih-Chang and Hsu, Kuo-Hua and Lai, Chin-Feng},
  doi          = {10.1007/s11554-021-01110-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1141-1155},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Nighttime object detection system with lightweight deep network for internet of vehicles},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An IoT-enabled real-time overhead view person detection
system based on cascade-RCNN and transfer learning. <em>JRTIP</em>,
<em>18</em>(4), 1129–1139. (<a
href="https://doi.org/10.1007/s11554-021-01103-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) is transforming technological evolution in several practical applications. These applications range from smart cities, smart healthcare to intelligent video surveillance, where the primary interest is person monitoring and detection. The amalgamation of Artificial Intelligence (AI) and IoT-based techniques maintain a balance between computational cost and efficiency that is essential for next-generation IoT networks. In this context, a real-time IoT-enabled people detection system is introduced. The developed system performs image processing task over the cloud using an internet connection, thus reduces the computational cost by processing high-resolution images over the cloud. For person detection, a pre-trained Cascade RCNN, a deep learning approach is used. It is an object detection architecture, seeks to address discrediting performance with increased Intersection over Union (IoU) thresholds. As the architecture is pre-trained with COCO data set and the person body’s appearance in overhead perspective is significantly different; thus, additional training is performed to enhance the detection results. Taking advantage of transfer learning architecture is trained for overhead person images, and the newly trained feature layer is added to the existing architecture. Experimental outcomes reveal that additional training increases the detection architecture’s performance with an accuracy rate of 0.96.},
  archive      = {J_JRTIP},
  author       = {Ahmad, Misbah and Ahmed, Imran and Jeon, Gwanggil},
  doi          = {10.1007/s11554-021-01103-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1129-1139},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An IoT-enabled real-time overhead view person detection system based on cascade-RCNN and transfer learning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of film and television big data in real-time image
detection and processing in the internet of things era. <em>JRTIP</em>,
<em>18</em>(4), 1115–1127. (<a
href="https://doi.org/10.1007/s11554-021-01105-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet, images play an increasingly critical role as an important data source in the Internet of Things (IoT). To meet the low-latency and high-efficiency transmission method of the IoT platform, the current problems in the image processing are analyzed from the perspective of real-time image processing in this study. The image features are extracted with the back propagation neural network (BPNN), and the images are classified with the support vector machines (SVM). A real-time image detection and processing platform (RT-IDPP) is constructed using the Adaboost framework based on the IoT, and the real-time image transmission and processing is realized based on different databases. It is found that the RT-IDPP proposed for the IoT realizes the image detection and tracking. The proposed method can not only run effectively on different cloud platforms for use, but also meet the real-time requirements in the image detection and tracking process, ensuring that the image detection rate is higher than 97%. Thus, the detection effect is better. Compared with the traditional image detection methods, the proposed method has higher detection rate and lower false-negative rate (FNR) and false-positive rate (FPR). The experimental detection effect on the film and television big data (FTBD) database is significantly better than that of other databases. This research can provide a theoretical basis for related researches on real-time image processing in the environment of IoT.},
  archive      = {J_JRTIP},
  author       = {Tong, Yangfan and Sun, Wei},
  doi          = {10.1007/s11554-021-01105-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1115-1127},
  shortjournal = {J. Real-Time Image Process.},
  title        = {The role of film and television big data in real-time image detection and processing in the internet of things era},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach for the detection of pneumonia in children
using CXR images based on an real-time IoT system. <em>JRTIP</em>,
<em>18</em>(4), 1099–1114. (<a
href="https://doi.org/10.1007/s11554-021-01086-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumonia is responsible for high infant morbidity and mortality. This disease affects the small air sacs (alveoli) in the lung and requires prompt diagnosis and appropriate treatment. Chest X-rays are one of the most common tests used to detect pneumonia. In this work, we propose a real-time Internet of Things (IoT) system to detect pneumonia in chest X-ray images. The dataset used has 6000 chest X-ray images of children, and three medical specialists performed the validations. In this work, twelve different architectures of Convolutional Neural Networks (CNNs) trained on ImageNet were adapted to operate as the resource extractors. Subsequently, the CNNs were combined with consolidated learning methods, such as k-Nearest Neighbor (kNN), Naive Bayes, Random Forest, Multilayer Perceptron (MLP), and Support Vector Machine (SVM). The results showed that the VGG19 architecture with the SVM classifier using the RBF kernel was the best model to detect pneumonia in these chest radiographs. This combination reached 96.47%, 96.46%, and 96.46% for Accuracy, F1 score, and Precision values, respectively. Compared to other works in the literature, the proposed approach had better results for the metrics used. These results show that this approach for the detection of pneumonia in children using a real-time IoT system is efficient and is, therefore, a potential tool to aid in medical diagnoses. This approach will allow specialists to obtain faster and more accurate results and thus provide the appropriate treatment.},
  archive      = {J_JRTIP},
  author       = {Chagas, João Victor S. das and de A. Rodrigues, Douglas and Ivo, Roberto F. and Hassan, Mohammad Mehedi and de Albuquerque, Victor Hugo C. and Filho, Pedro P. Rebouças},
  doi          = {10.1007/s11554-021-01086-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1099-1114},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A new approach for the detection of pneumonia in children using CXR images based on an real-time IoT system},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized convolutional neural network by firefly algorithm
for magnetic resonance image classification of glioma brain tumor grade.
<em>JRTIP</em>, <em>18</em>(4), 1085–1098. (<a
href="https://doi.org/10.1007/s11554-021-01106-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most frequent brain tumor types are gliomas. The magnetic resonance imaging technique helps to make the diagnosis of brain tumors. It is hard to get the diagnosis in the early stages of the glioma brain tumor, although the specialist has a lot of experience. Therefore, for the magnetic resonance imaging interpretation, a reliable and efficient system is required which helps the doctor to make the diagnosis in early stages. To make classification of the images, to which class the glioma belongs, convolutional neural networks, which proved that they can obtain an excellent performance in the image classification tasks, can be used. Convolutional network hyperparameters’ tuning is a very important issue in this domain for achieving high accuracy on the image classification; however, this task takes a lot of computational time. Approaching this issue, in this manuscript, we propose a metaheuristics method to automatically find the near-optimal values of convolutional neural network hyperparameters based on a modified firefly algorithm and develop a system for automatic image classification of glioma brain tumor grades from magnetic resonance imaging. First, we have tested the proposed modified algorithm on the set of standard unconstrained benchmark functions and the performance is compared to the original algorithm and other modified variants. Upon verifying the efficiency of the proposed approach in general, it is applied for hyperparameters’ optimization of the convolutional neural network. The IXI dataset and the cancer imaging archive with more collections of data are used for evaluation purposes, and additionally, the method is evaluated on the axial brain tumor images. The obtained experimental results and comparative analysis with other state-of-the-art algorithms tested under the same conditions show the robustness and efficiency of the proposed method.},
  archive      = {J_JRTIP},
  author       = {Bacanin, Nebojsa and Bezdan, Timea and Venkatachalam, K. and Al-Turjman, Fadi},
  doi          = {10.1007/s11554-021-01106-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1085-1098},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Optimized convolutional neural network by firefly algorithm for magnetic resonance image classification of glioma brain tumor grade},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Research on role modeling and behavior control of virtual
reality animation interactive system in internet of things.
<em>JRTIP</em>, <em>18</em>(4), 1069–1083. (<a
href="https://doi.org/10.1007/s11554-020-01046-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problems of poor real-time collision accuracy and low efficiency of modeling in the virtual reality environment, we propounded a depth image-based 3D modeling system and a hybrid intelligent collision detection algorithm. With the development of the 3D animation interactive system as an example, this paper uses 3D modeling technology based on depth images to build single role models, then reorganizes and merges the models to form 3D scenes. The hybrid intelligent collision detection algorithm, which combines the quantum behavior particle swarm optimization algorithm and the differential algorithm, improves the collision detection efficiency and accuracy and realizes behavior control of the characters in the interactive system. The experimental result shows that the 3D modeling technology based on depth images has greatly improved the accuracy and quantity of model texture and motion rate. By comparing the hybrid intelligent collision detection algorithm, the QPSO algorithm, and the FDH bounding box for collision detection, we conclude that the algorithm used in this paper has a shorter average collision time, more stable role behavior control, and better robustness.},
  archive      = {J_JRTIP},
  author       = {Gan, Baiqiang and Zhang, Chi and Chen, Yunqiang and Chen, Yeh-Cheng},
  doi          = {10.1007/s11554-020-01046-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1069-1083},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research on role modeling and behavior control of virtual reality animation interactive system in internet of things},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new wavelet-based multi-focus image fusion technique using
method noise and anisotropic diffusion for real-time surveillance
application. <em>JRTIP</em>, <em>18</em>(4), 1051–1068. (<a
href="https://doi.org/10.1007/s11554-021-01125-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new wavelet-based multi-focus image fusion approach using method noise and anisotropic diffusion for two separate cases, i.e., with and without a reference image. It is specifically designed for real-time surveillance applications. It is a multi-step image fusion approach. Firstly, stationary wavelet transform (SWT) is performed to get low and high-frequency coefficients. Secondly, the input images&#39; LL bands are fused using average operation. The rest of the respective bands are fused using a new correlation coefficient (CC) based fusion strategy using the threshold value calculated by structural similarity index metric (SSIM). Then inverse SWT is performed to reconstruct the fused coefficients. Thirdly, anisotropic diffusion-based method noise thresholding is introduced to recover the unprocessed and still damaged input images&#39; components. Finally, the proposed approach&#39;s performance has experimented with various qualitative (visual perception) and quantitative factors (performance metrics). The experimental outcomes show that the proposed approach generates fine edges, high visual quality, high clarity of objects, and less degradation. The proposed multi-step hybrid technique is implemented to generate high-quality fused images. The experimental outcomes verify the achievement of the proposed approach.},
  archive      = {J_JRTIP},
  author       = {Singh, Prabhishek and Diwakar, Manoj and Cheng, Xiaochun and Shankar, Achyut},
  doi          = {10.1007/s11554-021-01125-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1051-1068},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A new wavelet-based multi-focus image fusion technique using method noise and anisotropic diffusion for real-time surveillance application},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EfficientHRNet. <em>JRTIP</em>, <em>18</em>(4), 1037–1049.
(<a href="https://doi.org/10.1007/s11554-021-01132-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing demand for lightweight multi-person pose estimation for many emerging smart IoT applications. However, the existing algorithms tend to have large model sizes and intense computational requirements, making them ill-suited for real-time applications and deployment on resource-constrained hardware. Lightweight and real-time approaches are exceedingly rare and come at the cost of inferior accuracy. In this paper, we present EfficientHRNet, a family of lightweight multi-person human pose estimators that are able to perform in real-time on resource-constrained devices. By unifying recent advances in model scaling with high-resolution feature representations, EfficientHRNet creates highly accurate models while reducing computation enough to achieve real-time performance. The largest model is able to come within 4.4% accuracy of the current state-of-the-art, while having 1/3 the model size and 1/6 the computation, achieving 23 FPS on Nvidia Jetson Xavier. Compared to the top real-time approach, EfficientHRNet increases accuracy by 22% while achieving similar FPS with $$\frac{1}{3}$$ the power. At every level, EfficientHRNet proves to be more computationally efficient than other bottom-up 2D human pose estimation approaches, while achieving highly competitive accuracy.},
  archive      = {J_JRTIP},
  author       = {Neff, Christopher and Sheth, Aneri and Furgurson, Steven and Middleton, John and Tabkhi, Hamed},
  doi          = {10.1007/s11554-021-01132-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1037-1049},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EfficientHRNet},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metric learning with generator for closed loop detection in
VSLAM. <em>JRTIP</em>, <em>18</em>(4), 1025–1036. (<a
href="https://doi.org/10.1007/s11554-020-01067-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of Driverless Car, Unmanned Aerial Vehicle, Human–Computer Interaction and Artificial Intelligence has promoted the Internet of Things (IoT) industry, in which, Visual Simultaneous Localization and Mapping (VSLAM) is an important Localization and Mapping technique. Closed loop detection can alleviate the error accumulation during the operation of VSLAM. The traditional closed loop detection methods mostly rely on manually defined features, subjective and unstable, which are difficult to cope with complex and repetitive scenarios. Thus, triplet loss-based metric learning has been considered as a better solution for closed loop detection. In this paper, first, constructed Generator is applied to generate feature vector of hard negative sample. Second, triplet loss and generative loss have been applied to construct loss function. The keyframes are converted into feature vectors with well-trained model, evaluating the similarity of keyframes by calculating their distance of feature vectors, which is used to determine whether a closed loop is formed. Finally, TUM dataset is introduced to evaluate the Precision and Recall of the proposed metric learning. The well-trained model is applied to establish loop closing thread for VSLAM system. The experimental results illustrate the feasibility and effectiveness of the metric learning-based closed loop detection, which can be further applied to practical VSLAM systems.},
  archive      = {J_JRTIP},
  author       = {Chang, Jianfang and Dong, Na and Li, Donghui and Ip, Wai Hung and Yung, Kai Leung},
  doi          = {10.1007/s11554-020-01067-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1025-1036},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Metric learning with generator for closed loop detection in VSLAM},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Predicting behavioral competencies
automatically from facial expressions in real-time video-recorded
interviews. <em>JRTIP</em>, <em>18</em>(4), 1023. (<a
href="https://doi.org/10.1007/s11554-021-01090-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correction to this paper has been published: https://doi.org/10.1007/s11554-021-01090-2},
  archive      = {J_JRTIP},
  author       = {Su, Yu-Sheng and Suen, Hung-Yue and Hung, Kuo-En},
  doi          = {10.1007/s11554-021-01090-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1023},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: Predicting behavioral competencies automatically from facial expressions in real-time video-recorded interviews},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Predicting behavioral competencies automatically from
facial expressions in real-time video-recorded interviews.
<em>JRTIP</em>, <em>18</em>(4), 1011–1021. (<a
href="https://doi.org/10.1007/s11554-021-01071-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to develop a real-time image and video processor enabled with an artificial intelligence (AI) agent that can predict a job candidate’s behavioral competencies according to his or her facial expressions. This is accomplished using a real-time video-recorded interview with a histogram of oriented gradients and support vector machine (HOG-SVM) plus convolutional neural network (CNN) recognition. Different from the classical view of recognizing emotional states, this prototype system was developed to automatically decode a job candidate’s behaviors by their microexpressions based on the behavioral ecology view of facial displays (BECV) in the context of employment interviews using a real-time video-recorded interview. An experiment was conducted at a Fortune 500 company, and the video records and competency scores were collected from the company’s employees and hiring managers. The results indicated that our proposed system can provide better predictive power than can human-structured interviews, personality inventories, occupation interest testing, and assessment centers. As such, our proposed approach can be utilized as an effective screening method using a personal-value-based competency model.},
  archive      = {J_JRTIP},
  author       = {Su, Yu-Sheng and Suen, Hung-Yue and Hung, Kuo-En},
  doi          = {10.1007/s11554-021-01071-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1011-1021},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Predicting behavioral competencies automatically from facial expressions in real-time video-recorded interviews},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning framework for face verification without
alignment. <em>JRTIP</em>, <em>18</em>(4), 999–1009. (<a
href="https://doi.org/10.1007/s11554-020-01037-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the CNN (convolutional neural networks) methods require alignment, which will affect the efficiency of verification. This paper proposes a deep face verification framework without alignment. First and foremost, the framework consists of two training stages and one testing stage. In the first training stage, the CNN is fully trained on the large face dataset. In the second training stage, embedding triplet is adopted to fine-tune the models. Furthermore, in the testing stage, SIFT (scale invariant feature transform) descriptors are extracted from intermediate pooling results for cascading verification, which effectively improves the accuracy of face verification without alignment. Last but not least, two CNN architectures are designed for different scenarios. The CNN1 (convolutional neural networks 1), with fewer layers and parameters, requires a small amount of memory and computation in training and testing, so it is suitable for real-time system. The CNN2 (convolutional neural networks 2), with more layers and parameters, has excellent face verification. Through the long-term training on WEB-face dataset and experiments on the LFW (labled faces in the wild), YTB (YouTube) datasets, the results show that the proposed method has superior performance compared with some state-of-the-art methods.},
  archive      = {J_JRTIP},
  author       = {Fan, Zhongkui and Guan, Ye-peng},
  doi          = {10.1007/s11554-020-01037-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {999-1009},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A deep learning framework for face verification without alignment},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time intelligent image processing for the internet of
things. <em>JRTIP</em>, <em>18</em>(4), 997–998. (<a
href="https://doi.org/10.1007/s11554-021-01149-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Chen, Mu-Yen and Wu, Hsin-Te},
  doi          = {10.1007/s11554-021-01149-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {997-998},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time intelligent image processing for the internet of things},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Machine learning-based fast CU size decision algorithm for
3D-HEVC inter-coding. <em>JRTIP</em>, <em>18</em>(3), 983–995. (<a
href="https://doi.org/10.1007/s11554-020-01059-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D-high efficiency video coding (3D-HEVC) is an extension of the high efficiency video coding (HEVC) standard for the compression of the texture videos and depth maps. In 3D-HEVC inter-coding, the coding unit (CU) is recursively performed on variable sizes, namely, depth levels. The CU size decision process is conducted using all the possible depth levels to obtain the one with the least rate-distortion (RD) cost using the Lagrange multiplier. These tools achieve the highest coding efficiency but incur a very high computational complexity. In this paper, a fast CU size decision algorithm is proposed to reduce the complexity caused by the CU size splitting process. The proposed algorithm is based on the CU homogeneity classification using machine learning technology. First, the tensor feature is extracted to characterize the homogeneity of CU, which has a strong relationship with CU sizes. Then, a boosted decision stump algorithm is employed to analyze and construct a binary classification model from the extracted features and find suitable thresholds for the proposed method. Finally, an efficient early termination of CU splitting is released based on adaptive thresholds for texture videos and depth maps. The experimental results show that the proposed algorithm reduces a significant encoding time, while the loss in coding efficiency is negligible.},
  archive      = {J_JRTIP},
  author       = {Bakkouri, Siham and Elyousfi, Abderrahmane},
  doi          = {10.1007/s11554-020-01059-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {983-995},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Machine learning-based fast CU size decision algorithm for 3D-HEVC inter-coding},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast background subtraction with adaptive block learning
using expectation value suitable for real-time moving object detection.
<em>JRTIP</em>, <em>18</em>(3), 967–981. (<a
href="https://doi.org/10.1007/s11554-020-01058-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a method of moving object detection through a fast background subtraction technique suitable for real-time performance in wide range of platforms. An intermittent background update using adaptive blocks individually calculates the learning rate through expected difference values. Then, coupled with a fast background subtraction process, the design achieves fast throughput with well-rounded performance. To compensate for the lagging effects of intermittent background update, an adaptation bias is devised to improve precision and recall metrics. Experiments show a versatile performance in varying scenes with overall results better than conventional techniques. The proposed method achieved a fast execution speed of up to 56 fps in PC using Full HD video. It also achieved 655 fps and 83 fps in PC and ARM core-embedded platform, respectively, using the minimum input resolution of 320 × 240. Overall, it is suitable for real-time performance applications.},
  archive      = {J_JRTIP},
  author       = {Montero, Vince Jebryl and Jung, Woo-Young and Jeong, Yong-Jin},
  doi          = {10.1007/s11554-020-01058-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {967-981},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast background subtraction with adaptive block learning using expectation value suitable for real-time moving object detection},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid hardware oriented motion estimation algorithm for
HEVC/h.265. <em>JRTIP</em>, <em>18</em>(3), 953–966. (<a
href="https://doi.org/10.1007/s11554-020-01056-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Efficiency Video Coding (HEVC) is the latest video coding standard that supports high resolution videos by providing approximately twice the compression efficiency as compared to its previous standard H.264. Motion Estimation (ME) in HEVC is the most computation-intensive block as a result it becomes a bottleneck in the design of the encoder while implementing video applications on various computing platforms such as general purpose and embedded processors. So developing computational efficient architectures on Field Programmable Gate Array (FPGA) and Application Specific Integrated Circuit (ASIC) platforms is inevitable. This paper proposes a fast hybrid search pattern algorithm and its hardware architecture for encoding UHD videos. The proposed Integer ME (IME) algorithm requires an average of 11.19% less encoding time than the default Test Zone Search (TZS) algorithm in HM reference software with compromising decrement in PSNR and increment in bit rate. The proposed architecture is implemented in both FPGA and ASIC platform with TSMC 90 nm technology library. It consumed 32-33% of resources in Virtex-7 FPGA and 2784.4 K equivalent gate count (in terms of NAND ) and 18 kB of memory, respectively. The results show that maximum frequency of the proposed architecture is 162 MHz and a total power consumption is 463.4 mW. The architecture provides a maximum throughput of 2.78 Gpixels/sec due to it process $$32\times 32$$ CU comparatively much less clock cycles (59.5) as compared to the state-of-the-art literature . Further, it supports 8K UHD $$(8192\times 4320)$$ @ 78 fps.},
  archive      = {J_JRTIP},
  author       = {Gogoi, Sushanta and Peesapati, Rangababu},
  doi          = {10.1007/s11554-020-01056-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {953-966},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A hybrid hardware oriented motion estimation algorithm for HEVC/H.265},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced TLD-based video object-tracking implementation
tested on embedded platforms. <em>JRTIP</em>, <em>18</em>(3), 937–952.
(<a href="https://doi.org/10.1007/s11554-020-01050-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-tracking algorithms on embedded platforms are very important in many civilian and military applications. The Tracking–Learning–Detection (TLD) algorithm is considered one of the state-of-the-art online long-term object-tracking algorithms. The performance of running such computationally intensive algorithms on embedded platforms with limited computing resources is a challenge. This work proposes an enhanced TLD implementation, specifically designed for, and tested on, embedded platforms. In this new implementation, an extra-stage has been added to the TLD detector cascade, called a Region filter. This filter dynamically identifies the candidate region for the tracked object. Further, the two independent tracker and detector TLD components, and the two independent Forward–Backward (FB) and Normalized Cross Correlation (NCC) error measures in the tracker have been parallelized. Still further, the computations of Image Integral in the detector and the NCC in both the tracker and the detector have been optimized using a single instruction multiple data (SIMD) architecture. We evaluate our proposed implementation on the Apalis T30 embedded platform, using the same video sequences that the original TLD is evaluated on. Our results show that our enhanced implementation outperforms the baseline with an average speedup of 3.7 × in the total number of frames per second (fps), while achieving an average 91% of the Precision and 86.6% of the Recall metrics, across all sequences. Further, our enhanced implementation achieves an average speedup of 4.52 × and 1.86 × in the detector and tracker execution times, respectively. Moreover, it results in an average 66.3% energy saving, as compared to the original implementation.},
  archive      = {J_JRTIP},
  author       = {Otoom, Mwaffaq and Al-Louzi, Malek},
  doi          = {10.1007/s11554-020-01050-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {937-952},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced TLD-based video object-tracking implementation tested on embedded platforms},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). An automated detection model of threat objects for x-ray
baggage inspection based on depthwise separable convolution.
<em>JRTIP</em>, <em>18</em>(3), 923–935. (<a
href="https://doi.org/10.1007/s11554-020-01051-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray baggage inspection is an essential task to detect threat objects at important controlled access places, which can guard personal safety and prevent crime. Generally, it is carried out by screeners to visually determine whether or not a bag contains threat objects. Whereas, manual detection exhibits distinct shortcomings, from high detection errors to different detection results produced by screeners. These limitations can be addressed by introducing automated detection model of threat objects for X-ray baggage inspection. However, existing automated detection methods cannot realize end-to-end detection and the detection results include only classification without location. In this paper, we propose an automated detection model of threat objects based on depthwise separable convolution. Our model is able to not only categorize the threat object but also locate it simultaneously. The network model has the advantage of high detection accuracy, fast computational speed, and a few parameters. Meanwhile, the precision of threat object regions is enhanced with the help of multi-scale prediction. A deformation layer is added in our model, which can provide invariance to affine warping. The experiments on the GDXray database (Mery et al. in J Nondestr Eval 34(4):42, 2015) demonstrate that the overall performance of our proposed model is superior to YOLOv3 (Redmon J and Farhadi A in YOLOv3: an incremental improvement, 2018) model, SSD (Liu et al. in SSD: single shot multibox detector. In: European Conference on Computer Vision (ECCV), pp. 21–37, 2016) model, and Tiny_YOLO (Redmon et al. in You only look once: unified, real-time object detection. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779–788, 2015) model.},
  archive      = {J_JRTIP},
  author       = {Wei, Yiru and Zhu, Zhiliang and Yu, Hai and Zhang, Wei},
  doi          = {10.1007/s11554-020-01051-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {923-935},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An automated detection model of threat objects for X-ray baggage inspection based on depthwise separable convolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SmartSORT: An MLP-based method for tracking multiple objects
in real-time. <em>JRTIP</em>, <em>18</em>(3), 913–921. (<a
href="https://doi.org/10.1007/s11554-020-01054-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent advances in the object detection research field, tracking-by-detection has become the leading paradigm adopted by multi-object tracking algorithms. By extracting different features from detected objects, those algorithms can estimate the similarities and association patterns of objects along with successive frames. However, since similarity functions applied by tracking algorithms are handcrafted, it is difficult to use them in new contexts. In this study, it is investigated the use of artificial neural networks to learning a similarity function that can be used among detections. During training, multilayer perceptron (MLP) neural networks were introduced to correct and incorrect association patterns, sampled from a pedestrian tracking data set. For such, different motion and appearance feature combinations have been explored. Finally, a trained MLP has been inserted into a multiple-object tracking framework, which has been assessed on the MOT Challenge benchmark. Throughout the experiments, the proposed tracker matched the results obtained by state-of-the-art methods by scoring a tracking accuracy of 60.4%, while running 58% faster than DeepSORT, a recent and similar method used as a baseline. After all, this work demonstrates its method can be automatically trained for different tracking contexts and it has highly competitive cost-effectiveness for online real-time tracking applications.},
  archive      = {J_JRTIP},
  author       = {Meneses, Michel and Matos, Leonardo and Prado, Bruno and Carvalho, André and Macedo, Hendrik},
  doi          = {10.1007/s11554-020-01054-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {913-921},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SmartSORT: An MLP-based method for tracking multiple objects in real-time},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA-based architecture for bi-cubic interpolation: The best
trade-off between precision and hardware resource consumption.
<em>JRTIP</em>, <em>18</em>(3), 901–911. (<a
href="https://doi.org/10.1007/s11554-020-01035-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate and low-cost design of the image interpolation unit is a crucial part for many real-time image processing systems. To reach this goal, bi-cubic interpolation is generally selected because it provides the best trade-off between computational complexity and interpolation quality. The aim of this paper is to study the optimal hardware implementation of heterogeneous bi-cubic interpolation. Bi-cubic algorithm is reformulated and improved for FPGA implementation. This improved algorithm avoids twelve redundant calculations and reduces the number of multipliers by $$25\%$$ . Hardware precision versus resource utilization is studied to minimize the quantization error of hardware realization, and to obtain the best trade-off between design cost and accuracy. A compromise that reduces $$33,33\%$$ of  bit-width utilization with a precision higher than $$99.922\%$$ is reached. Besides, the proposed architecture is fully pipelined to reach high operating frequency. Instantiation on Xilinx and Intel targets shows the benefit of our approach, especially in terms of hardware resource consumption.},
  archive      = {J_JRTIP},
  author       = {Boukhtache, S. and Blaysat, B. and Grédiac, M. and Berry, F.},
  doi          = {10.1007/s11554-020-01035-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {901-911},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-based architecture for bi-cubic interpolation: The best trade-off between precision and hardware resource consumption},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Real-time video fire/smoke detection based on CNN in
antifire surveillance systems. <em>JRTIP</em>, <em>18</em>(3), 889–900.
(<a href="https://doi.org/10.1007/s11554-020-01044-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a real-time video-based fire and smoke detection using YOLOv2 Convolutional Neural Network (CNN) in antifire surveillance systems. YOLOv2 is designed with light-weight neural network architecture to account the requirements of embedded platforms. The training stage is processed off-line with indoor and outdoor fire and smoke image sets in different indoor and outdoor scenarios. Ground truth labeler app is used to generate the ground truth data from the training set. The trained model was tested and compared to the other state-of-the-art methods. We used a large scale of fire/smoke and negative videos in different environments, both indoor (e.g., a railway carriage, container, bus wagon, or home/office) or outdoor (e.g., storage or parking area). YOLOv2 is a better option compared to the other approaches for real-time fire/smoke detection. This work has been deployed in a low-cost embedded device (Jetson Nano), which is composed of a single, fixed camera per scene, working in the visible spectral range. There are not specific requirements for the video camera. Hence, when the proposed solution is applied for safety on-board vehicles, or in transport infrastructures, or smart cities, the camera installed in closed-circuit television surveillance systems can be reused. The achieved experimental results show that the proposed solution is suitable for creating a smart and real-time video-surveillance system for fire/smoke detection.},
  archive      = {J_JRTIP},
  author       = {Saponara, Sergio and Elhanashi, Abdussalam and Gagliardi, Alessio},
  doi          = {10.1007/s11554-020-01044-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {889-900},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time video fire/smoke detection based on CNN in antifire surveillance systems},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A de-raining semantic segmentation network for real-time
foreground segmentation. <em>JRTIP</em>, <em>18</em>(3), 873–887. (<a
href="https://doi.org/10.1007/s11554-020-01042-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few researches have been proposed specifically for real-time semantic segmentation in rainy environments. However, the demand in this area is huge and it is challenging for lightweight networks. Therefore, this paper proposes a lightweight network which is specially designed for the foreground segmentation in rainy environments, named De-raining Semantic Segmentation Network (DRSNet). By analyzing the characteristics of raindrops, the MultiScaleSE Block is targetedly designed to encode the input image, it uses multi-scale dilated convolutions to increase the receptive field, and SE attention mechanism to learn the weights of each channels. To combine semantic information between different encoder and decoder layers, it is proposed to use Asymmetric Skip, that is, the higher semantic layer of encoder employs bilinear interpolation and the output passes through pointwise convolution, then added element-wise to the lower semantic layer of the decoder. According to the control experiments, the performances of MultiScaleSE Block and Asymmetric Skip compared with SEResNet18 and Symmetric Skip respectively are improved to a certain degree on the Foreground Accuracy index. The parameters and the floating point of operations (FLOPs) of DRSNet are only 0.54M and 0.20GFLOPs separately. The state-of-the-art results and real-time performances are achieved on both the UESTC all-day Scenery add rain (UAS-add-rain) and the Baidu People Segmentation add rain (BPS-add-rain) benchmarks with the input sizes of 192*128, 384*256 and 768*512. The speed of DRSNet exceeds all the networks within 1GFLOPs, and Foreground Accuracy index is also the best among the similar magnitude networks on both benchmarks.},
  archive      = {J_JRTIP},
  author       = {Wang, Fanyi and Zhang, Yihui},
  doi          = {10.1007/s11554-020-01042-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {873-887},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A de-raining semantic segmentation network for real-time foreground segmentation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Position-aware lightweight object detectors with depthwise
separable convolutions. <em>JRTIP</em>, <em>18</em>(3), 857–871. (<a
href="https://doi.org/10.1007/s11554-020-01027-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, significant improvements have been achieved for object detection algorithm by increasing the size of convolutional neural network (CNN) models, but the resulting increase of computational complexity poses an obstacle to practical applications. And some of the lightweight methods fail to consider the characteristics of object detection into and suffer a huge loss of accuracy. In this paper, we design a multi-scale feature lightweight network structure and specific convolution module for object detection based on depthwise separable convolution, which not only reduces the computational complexity but also improves the accuracy by using the specific position information in object detection. Furthermore, in order to improve the detection accuracy for small objects, we construct a multi-channel position-aware map and propose training based on knowledge distillation for object detection to train the lightweight model effectively. Last, we propose a training strategy based on a key-layer guiding structure to balance performance with training time. The experimental results show that on the COCO dataset that takes the state-of-the-art object detection algorithm, YOLOv3, as the baseline, our model size is compressed to 1/11 while accuracy drops by 7.4 mmAP, and the computational latency on the GPU and ARM platforms are reduced to 43.7% and 0.29%, respectively. Compared with the state-of-the-art lightweight object detection model, MNet V2 + SSDLite, the accuracy of our model increases by 3.5 mmAP while the inferencing time stays nearly the same. On the PASCAL VOC2007 dataset, the accuracy of our model increases by 5.2 mAP compared to the state-of-the-art lightweight algorithm based on knowledge distillation. Therefore, in terms of accuracy, parameter count, and real-time performance, our algorithm has better performance than lightweight algorithms based on knowledge distillation or depthwise separable convolution.},
  archive      = {J_JRTIP},
  author       = {Chang, Libo and Zhang, Shengbing and Du, Huimin and You, Zhonglun and Wang, Shiyu},
  doi          = {10.1007/s11554-020-01027-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {857-871},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Position-aware lightweight object detectors with depthwise separable convolutions},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction mode grouping and coding bits grouping based on
texture complexity for fast HEVC intra-coding. <em>JRTIP</em>,
<em>18</em>(3), 839–856. (<a
href="https://doi.org/10.1007/s11554-020-01034-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HEVC (High Efficiency Video Coding) as one of the newest international video coding standard, can achieve about 50% bit rate reduction compared with H.264/AVC (Advanced Video Coding) with the same perceptual quality by the use of flexible CTU (coding tree unit) structure, but at the same time, it also dramatically adds its computational complexity for HEVC. To reduce the computational complexity, a fast intra-prediction mode and CU (Coding Unit) size decision algorithm based on prediction mode and coding bits grouping is presented for HEVC intra-encoding in this paper. The contribution of this paper lies in the fact that we successfully use the prediction mode grouping and coding bits grouping technologies to rapidly realize the optimal prediction mode and size decision for the current CU, thus saving much computation complexity for HEVC. Specifically, in our scheme, first, we use grouping technology to group 35 intra-prediction modes into 5 subsets of candidate modes list according to the texture complexity of current PU (Prediction Unit), and each subset only contains 11 intra-prediction modes, which can greatly reduce the traversing and calculating number of candidate mode in RMD (Rough Mode Decision); second, we use coding bits grouping technology to quickly judge whether the current CU needs to be further divided on the basis of the studying of texture complexity in the current CU, which can reduce many unnecessary prediction and partition operations for the current CU; at last we use the fast intra-mode prediction and CU size decision algorithm above to quickly realize the optimal encoding for the current CU in HEVC. As a result, the high computational complexity in HEVC intra-encoding can be efficiently reduced by our proposed scheme. And the simulation results of our experiments show that our proposed fast intra-coding algorithm based on prediction mode and coding bit grouping in this paper can reduce about 49.10% computational complexity on average only at a cost of 0.92% bit rate increase and 0.065 db PSNR decline compared with the standard reference HM16.1algorithm under all-intra-configuration.},
  archive      = {J_JRTIP},
  author       = {Wang, Jianhua and Ji, Bang and Wang, Haozhan and Cheng, Lianglun},
  doi          = {10.1007/s11554-020-01034-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {839-856},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Prediction mode grouping and coding bits grouping based on texture complexity for fast HEVC intra-coding},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight densely connected residual network for human
pose estimation. <em>JRTIP</em>, <em>18</em>(3), 825–837. (<a
href="https://doi.org/10.1007/s11554-020-01025-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods pay much attention to how to improve the accuracy of human pose estimation results. They usually ignore what the size of their model is. However, besides accuracy, real-time and speed are also important. In this paper, a new module named Densely Connected Residual Module is presented to effectively decrease the number of parameters in our network. We introduce our module into the backbone of High-Resolution Net. In addition, we change direct addition fusion into pyramid fusion at the end of the network. No need for ImageNet pre-training sharply decreases the total time of our training processes. We do our experiments over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. As a result, we achieve a decrease on number of parameters and calculated amount, respectively by around 72% and 14%, making our network more lightweight than High-Resolution Net. During testing process, our model can predict an image at a speed of 25 ms per image, which also achieves real-time fundamentally. The code has been available at https://github.com/consistent1997/LDCRN .},
  archive      = {J_JRTIP},
  author       = {Yang, Lianping and Qin, Yu and Zhang, Xiangde},
  doi          = {10.1007/s11554-020-01025-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {825-837},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight densely connected residual network for human pose estimation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time implementation of fabric defect detection based on
variational automatic encoder with structure similarity. <em>JRTIP</em>,
<em>18</em>(3), 807–823. (<a
href="https://doi.org/10.1007/s11554-020-01023-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of fabric defects based on machine vision is an important topic in the quality control of cotton textile factories. There are many kinds of defects in fabric production, it is very difficult to classify the defects automatically. In recent years, deep learning image processing technology based on a convolutional neural network (CNN) can train and extract features of the target image automatically. Since a large number of defect samples cannot be collected completely, we compared unsupervised learning algorithms based on CNN, including auto encoder (AE), variational automatic encoder (VAE), and generative adversarial networks (GAN). Because of the large amount of calculation and the difficulty of training in GAN, we chose AE and VAE codec networks and then introduced mean structural similarity (MSSIM) as network training loss function to improve the performance that only used $${L}_{p}$$ -distance loss function for image brightness comparison. After training finished, the authors used the trained model to obtain target defects from SSIM residual maps between input and reconstruct images. According to the evaluation results, we finally implemented a fabric defect detection system based on VAE on Jetson TX2 from Nvidia Corporation, USA. The optimized algorithm can meet the real-time requirements of the project and realize its popularization and application.},
  archive      = {J_JRTIP},
  author       = {Wei, Wei and Deng, Dexiang and Zeng, Lin and Zhang, Chen},
  doi          = {10.1007/s11554-020-01023-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {807-823},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time implementation of fabric defect detection based on variational automatic encoder with structure similarity},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast and fully distributed method for region-based image
segmentation. <em>JRTIP</em>, <em>18</em>(3), 793–806. (<a
href="https://doi.org/10.1007/s11554-020-01021-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed and parallel computing techniques allow fast image processing, namely when these techniques are applied at the low and the medium level of a vision system. In this paper, a collective and distributed method for image segmentation is introduced and evaluated. The method is modeled as a multi-agent system, where the agents aim to collectively produce a region-based segmentation. Each agent starts searching for an acceptable region seed by randomly jumping within the image. Next, it performs a region growing around its position. Thus, several agents find themselves within the same homogeneous region and are organized in a graph where two agents are connected if they are within the same region. So, a unifying of the labels in a same region is collaboratively performed by the agents themselves. The proposed method was experimented on real range images from the ABW dataset and the Object Segmentation Database (OSD) one, and the obtained results were compared to those of some well-referenced methods from the literature. The evaluation results show that the proposed method provides fast and accurate image segmentation, allowing it to be deployed for real-time vision systems.},
  archive      = {J_JRTIP},
  author       = {Mazouzi, Smaine and Guessoum, Zahia},
  doi          = {10.1007/s11554-020-01021-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {793-806},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A fast and fully distributed method for region-based image segmentation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized execution of morphological reconstruction in large
medical images on embedded devices. <em>JRTIP</em>, <em>18</em>(3),
779–791. (<a href="https://doi.org/10.1007/s11554-020-01011-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a hardware/software co-design implementation of the morphological reconstruction targeting a System-on-Chip (SoC) FPGA-based embedded system. Our approach processes large images with fast algorithms. This was achieved by the proposal and use of an execution scheme that partitions the input image into sub-images that are independently processed before a second phase is executed to enable propagation of information among sub-images. The SoC is efficiently used by processing sub-images on hardware (the costly phase), while the software takes care of computations due to discontinuities that are irregular and inefficient for the hardware execution. Several optimizations were proposed, including parallel software and hardware execution and the use of borders to minimize computations in the discontinuities correction. This enables the processing of large images from our use-case brain cancer tissue image analysis application. For an image of $$8192 \times 8192$$ pixels, our co-design solution attains a speedup of 12.7 $$\times$$ vs. the software execution (Dual core ARM A9 Cortex).},
  archive      = {J_JRTIP},
  author       = {Cabral, Felipe and Anacona-Mosquera, Oscar and Sampaio, Renato C. and Teodoro, George and Llanos, Carlos H. and Jacobi, Ricardo P.},
  doi          = {10.1007/s11554-020-01011-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {779-791},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Optimized execution of morphological reconstruction in large medical images on embedded devices},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HipaccVX: Wedding of OpenVX and DSL-based code generation.
<em>JRTIP</em>, <em>18</em>(3), 765–777. (<a
href="https://doi.org/10.1007/s11554-020-01015-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writing programs for heterogeneous platforms optimized for high performance is hard since this requires the code to be tuned at a low level with architecture-specific optimizations that are most times based on fundamentally differing programming paradigms and languages. OpenVX promises to solve this issue for computer vision applications with a royalty-free industry standard that is based on a graph-execution model. Yet, the OpenVX ’ algorithm space is constrained to a small set of vision functions. This hinders accelerating computations that are not included in the standard. In this paper, we analyze OpenVX vision functions to find an orthogonal set of computational abstractions. Based on these abstractions, we couple an existing domain-specific language (DSL) back end to the OpenVX environment and provide language constructs to the programmer for the definition of user-defined nodes. In this way, we enable optimizations that are not possible to detect with OpenVX graph implementations using the standard computer vision functions. These optimizations can double the throughput on an Nvidia GTX GPU and decrease the resource usage of a Xilinx Zynq FPGA by 50% for our benchmarks. Finally, we show that our proposed compiler framework, called HipaccVX, can achieve better results than the state-of-the-art approaches Nvidia VisionWorks and Halide-HLS.},
  archive      = {J_JRTIP},
  author       = {Özkan, M. Akif and Ok, Burak and Qiao, Bo and Teich, Jürgen and Hannig, Frank},
  doi          = {10.1007/s11554-020-01015-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {765-777},
  shortjournal = {J. Real-Time Image Process.},
  title        = {HipaccVX: Wedding of OpenVX and DSL-based code generation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A frame-level MLP-based bit-rate controller for real-time
video transmission using VVC standard. <em>JRTIP</em>, <em>18</em>(3),
751–763. (<a href="https://doi.org/10.1007/s11554-020-01018-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time video transmission is one of the most popular applications that are included in the versatile video coding (VVC) standard. However, real-time applications are encountered with practical limitations, including the buffer size and available bandwidth. In these applications, the buffer overflow and underflow should be strictly prevented and also the bit-rate fluctuation should be suppressed. In this paper, a video bit-rate controller is proposed that completely conforms with the constraints of real-time applications. The proposed controller is based on a multi-layer perceptron (MLP) neural network which estimates the proper quantization parameter (QP) modification at the frame level. The buffer occupancy is directly included in the QP derivation process for robust buffer control. Experimental results show that the proposed bit-rate controller fulfils the buffering constraints and controls the bit-rate accurately. The average bit-rate error of the proposed method is 0.29% while providing a low initial buffering delay of about 0.21 s. Also, the rate-distortion analysis shows that the performance of the proposed method is close to those of the conventional algorithms.},
  archive      = {J_JRTIP},
  author       = {Raufmehr, Farhad and Salehi, Mohammad Reza and Abiri, Ebrahim},
  doi          = {10.1007/s11554-020-01018-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {751-763},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A frame-level MLP-based bit-rate controller for real-time video transmission using VVC standard},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-smooth non-local variational approach to saliency
detection in real time. <em>JRTIP</em>, <em>18</em>(3), 739–750. (<a
href="https://doi.org/10.1007/s11554-020-01016-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose and solve numerically a general non-smooth, non-local variational model to tackle the saliency detection problem in natural images. In order to overcome the typical drawback of the non-local methods in image processing, which mainly is the inherent computational complexity of non-local calculus, as the non-local derivatives are computed w.r.t every point of the domain, we propose a different scenario. We present a novel convex energy minimization problem in the feature space, which is efficiently solved by means of a non-local primal-dual method. Several implementations and discussions are presented taking care of the computing platforms, CPU and GPU, achieving up to 33 fps and 62 fps respectively for 300 $$\times$$ 400 image resolution, making the method eligible for real time applications.},
  archive      = {J_JRTIP},
  author       = {Alcaín, Eduardo and Muñoz, Ana I. and Schiavi, Emanuele and Montemayor, Antonio S.},
  doi          = {10.1007/s11554-020-01016-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {739-750},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A non-smooth non-local variational approach to saliency detection in real time},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and energy-efficient approximate motion estimation
architecture for real-time 4 k UHD processing. <em>JRTIP</em>,
<em>18</em>(3), 723–737. (<a
href="https://doi.org/10.1007/s11554-020-01014-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing techniques exploit the characteristics of error-tolerant applications either to provide faster implementations of their computational structures or to achieve substantial improvements in terms of energy efficiency. In video encoding, the motion estimation (ME) stage, including the Integer ME (IME) and the Fractional ME (FME) steps, is the most computational intensive task and it is highly resilient to controlled losses of accuracy. In accordance, this article proposes the exploitation of approximate computing techniques to implement energy efficient dedicated hardware structures targeting the motion estimation stage of current video encoders. The designed ME architecture supports IME and FME and is able to real-time process 4 K UHD videos (3840 × 2160 pixels) at 30 frames per second, while dissipating 108.92 mW. When running at its maximum operation frequency, the architecture can process 8 K UHD videos (7680 × 4320 pixels) at 120 frames per second. The solution described in this article presents the highest throughput and the highest energy efficiency among all state-of-the-art compared works, showing that the use of approximate computing is a promising solution when implementing video encoders in dedicated hardware.},
  archive      = {J_JRTIP},
  author       = {Porto, Roger and Perleberg, Murilo and Afonso, Vladimir and Zatt, Bruno and Roma, Nuno and Agostini, Luciano and Porto, Marcelo},
  doi          = {10.1007/s11554-020-01014-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {723-737},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast and energy-efficient approximate motion estimation architecture for real-time 4 k UHD processing},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A high-speed feature matching method of high-resolution
aerial images. <em>JRTIP</em>, <em>18</em>(3), 705–722. (<a
href="https://doi.org/10.1007/s11554-020-01012-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel corner detection and scale estimation algorithm for image feature description and matching. Inspired by Adaboost’s weak classifier, a series of sub-detectors is elaborately designed to obtain reliable corner pixels. The new corner detection algorithm is more robust than the FAST and HARRIS algorithm, and it is especially suitable for the implementation in FPGA. The new scale estimation method can be directly implemented in the original image without building Gaussian pyramid and searching max response value in each level, which not only increase computational efficiency but also greatly reduces memory requirement. Based on the proposed algorithm, a CPU-FPGA cooperative parallel processing architecture is presented. The architecture overcomes the memory space limitation of FPGA and achieves high-speed feature matching for massive high-resolution aerial images. The speed of the CPU-FPGA cooperative process is hundred times faster than SIFT algorithm running on CPU, and dozens of times faster than SIFT running in CPU + GPU system.},
  archive      = {J_JRTIP},
  author       = {Peng, Zhiyong and Wu, Jun and Zhang, Yongjun and Lin, Xianhua},
  doi          = {10.1007/s11554-020-01012-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {705-722},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high-speed feature matching method of high-resolution aerial images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: PDCAT: A framework for fast, robust, and
occlusion resilient fiducial marker tracking. <em>JRTIP</em>,
<em>18</em>(3), 703. (<a
href="https://doi.org/10.1007/s11554-020-01017-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the original publication of the article, the family name of the 1st author has been changed to Araar.},
  archive      = {J_JRTIP},
  author       = {Araar, Oualid and Mokhtari, Imad Eddine and Bengherabi, Mohamed},
  doi          = {10.1007/s11554-020-01017-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {703},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Correction to: PDCAT: a framework for fast, robust, and occlusion resilient fiducial marker tracking},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). PDCAT: A framework for fast, robust, and occlusion
resilient fiducial marker tracking. <em>JRTIP</em>, <em>18</em>(3),
691–702. (<a href="https://doi.org/10.1007/s11554-020-01010-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Square binary patterns have become the de facto fiducial marker for most computer vision applications. Existing tracking solutions suffer a number of limitations, such as the low frame-rate and sensitivity to partial occlusions. This work aims at overcoming these limitations, by exploiting temporal information in video-sequences. We propose a parallel detection, compensation and tracking (PDCAT) framework, which can be integrated into any binary marker system. Our solution is capable of recovering markers even when they become mostly occluded. Furthermore, the low processing time of the tracking task makes PDCAT more than an order of magnitude faster than a track-by-detect solution. This is particularly important for embedded computer vision applications, wherein the detection run at a very low frame rate. In the experiments conducted on an embedded computer, the processing frame rate of the track-by-detect solution was merely 11 FPS. Our solution, on the other hand, was capable of processing more than 100 FPS.},
  archive      = {J_JRTIP},
  author       = {Araar, Oualid and Mokhtari, Imad Eddine and Bengherabi, Mohamed},
  doi          = {10.1007/s11554-020-01010-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {691-702},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PDCAT: A framework for fast, robust, and occlusion resilient fiducial marker tracking},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-speed gaze detection using a single FPGA for driver
assistance systems. <em>JRTIP</em>, <em>18</em>(3), 681–690. (<a
href="https://doi.org/10.1007/s11554-020-01004-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of driver gaze detection by video-based eye-tracking often encounters problems in lowcomputing speed, high-power consumption, and installation space constraints inside the vehicle. In this paper, we present an eye-tracking system that uses a single field-programmable-gate-array chip to overcome the aforementioned problems. In the detection system, the image quality is 640 $$\times$$ 480 pixels with an 80 fps frame rate. Eye feature extraction is conducted using the enhanced semantics-based vague image representation approach. A succinct fully-connected neural network is then employed to classify various directions of sightline. Our experimental results exhibited a noticeable recognition speed at 0.52 $$\upmu$$ s using a 100 MHz system clock and had an average detection rate of 92%.},
  archive      = {J_JRTIP},
  author       = {Yu, Ying-Hao and Ting, Yi-Siang and Kwok, Ngaiming and Mayer, Norbert Michael},
  doi          = {10.1007/s11554-020-01004-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {681-690},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High-speed gaze detection using a single FPGA for driver assistance systems},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time chinese traffic warning signs recognition based on
cascade and CNN. <em>JRTIP</em>, <em>18</em>(3), 669–680. (<a
href="https://doi.org/10.1007/s11554-020-01003-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warning signs are of great significance to traffic safety. In this paper, a real-time recognition method for Chinese Traffic Warning Signs (CTWS) is proposed. CTWS are all triangles with yellow background, black border and black pattern. Their similarity is conducive to the localization task of object detection but adverse to the classification task of object detection. After analyzing the characteristics of these signs, real-time recognition for CTWS is carried out by employing Cascade classifier and Convolutional Neural Network (CNN). A Cascade classifier with 9 layers is trained with local binary patterns to locate the CTWS in frames. And a 10-layer CNN model is built to determine the specific category of the signs located by the Cascade classifier. We evaluate the method on CCTSDB-based dataset and GTSDB, and experiments show that the proposed method can perform accurate recognition at an average speed of 81.79fps without GPU. Since the proposed method only needs to call CNN that requires vast computing power in a small number of frames containing CTWS while performing real-time recognition, it can effectively save the valuable on-board computing resources compared with other object detection algorithms that is purely based on CNN such as YOLOv3, YOLOv3-tiny and Faster R-CNN.},
  archive      = {J_JRTIP},
  author       = {Gao, Yining and Xiao, Guangyi},
  doi          = {10.1007/s11554-020-01003-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {669-680},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time chinese traffic warning signs recognition based on cascade and CNN},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). De-ghosted HDR video acquisition for embedded systems.
<em>JRTIP</em>, <em>18</em>(3), 659–668. (<a
href="https://doi.org/10.1007/s11554-020-01001-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel ghost-free High Dynamic Range (HDR) multi-exposure video acquisition suitable for real-time implementation in embedded systems. While the method is limited to stationary cameras, it achieves, with low requirements on resources, results comparable to state-of-the-art de-ghosting methods that are often very computationally expensive and almost impossible to implement in smart cameras and embedded systems. The paper describes the method itself and includes an evaluation of the performance on selected embedded platforms and a comparison of the results to the state of the art using HDR datasets.},
  archive      = {J_JRTIP},
  author       = {Musil, Martin and Nosko, Svetozar and Zemcik, Pavel},
  doi          = {10.1007/s11554-020-01001-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {659-668},
  shortjournal = {J. Real-Time Image Process.},
  title        = {De-ghosted HDR video acquisition for embedded systems},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast contour detection with supervised attention learning.
<em>JRTIP</em>, <em>18</em>(3), 647–657. (<a
href="https://doi.org/10.1007/s11554-020-00980-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep convolutional neural networks have led to significant success in many computer vision tasks, including edge detection. However, the existing edge detectors neglected the structural relationships among pixels, especially those among contour pixels. Inspired by human perception, this work points out the importance of learning structural relationships and proposes a novel real-time attention edge detection (AED) framework. Firstly, an elaborately designed attention mask is employed to capture the structural relationships among pixels at edges. Secondly, in the decoding phase of our encoder–decoder model, a new module called dense upsampling group convolution is designed to tackle the problem of information loss due to stride downsampling. And then, the detailed structural information can be preserved even it is ever destroyed in the encoding phase. The proposed relationship learning module introduces negligible computation overhead, and as a result, the proposed AED meets the requirement of real-time execution with only 0.65M parameters. With the proposed model, an optimal dataset scale F-score of 79.5 is obtained on the BSDS500 dataset with an inference speed of 105 frames per second, which is significantly faster than existing methods with comparable accuracy. In addition, a state-of-the-art performance is achieved on the BSDS500 (81.6) and NYU Depth (77.0) datasets when using a heavier model.},
  archive      = {J_JRTIP},
  author       = {Zhang, Rufeng and You, Mingyu},
  doi          = {10.1007/s11554-020-00980-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {647-657},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast contour detection with supervised attention learning},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ear tracking via siamese hierarchical refinement network for
local active noise control. <em>JRTIP</em>, <em>18</em>(3), 635–646. (<a
href="https://doi.org/10.1007/s11554-020-01000-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active noise control (ANC) technology has been applied to reduce unwanted sound in the vehicle cabin. In this paper, a real-time ear tracking system assists ANC performance as the driver’s head moves around. For long-term robust ear tracking, an offline-trained ear detector initializes target area. With precise pre-cropped image patches, a Siamese hierarchical refinement network (SHRNet) builds high-fidelity feature map based on Siamese pyramid branch. Hierarchical feature extraction with lateral refinement makes most use of all levels of feature representation. The offline matching network is trained in an augmented dataset from the self-collected in-vehicle ear database and the ear-labeled McGill face video database. Further, Q-learning is capable of learning a decision-making policy for refining tracking strategy to improve efficiency. Extensive experiment results in various scenes based on NVIDIA Jetson TX2 show the tracker performs at a real-time speed while maintaining a robust performance. In particular, the method achieves AUC score of 67.6% with 26 fps on self-collected in-vehicle ear database.},
  archive      = {J_JRTIP},
  author       = {Zhang, Weiwei and Zou, Yi and Wang, Yansong},
  doi          = {10.1007/s11554-020-01000-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {635-646},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Ear tracking via siamese hierarchical refinement network for local active noise control},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time critical part detection for the blurred image of
infrared reconnaissance balloon with boundary curvature feature
analysis. <em>JRTIP</em>, <em>18</em>(3), 619–634. (<a
href="https://doi.org/10.1007/s11554-020-00997-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and striking the critical part of blurred balloon image is an important approach to counterattack the reconnaissance balloons. The existing algorithms of the image target detection task are not able to achieve the high precision and real-time performance in the meanwhile since the critical part of the balloon is tiny, weak and not easy to be segmented. In this paper, a real-time algorithm based curvature feature in the polar coordinate system is proposed to detect the critical part of reconnaissance balloons. We divide the proposed method into three steps: the image is firstly subjected to a gray-scale projection by calculating third-order post-difference, then the balloon boundary is extracted in transformed polar coordinates, and finally the boundary curvature identifies the position of the critical part. The core strategy of the proposed method is to adopt the boundary features of the balloon instead of the general time-consuming image operations (e.g. region labeling, matching) to capture the target part. The experimental results show that the proposed method obtains high precision results with a real-time detection. Our proposed method achieves a processing speed of 200 frames per second on DSP (TMS320C6678) while a state-of-the-art detection precision (&gt;93 $$\%$$ ), which overcomes the existing comparison algorithms.},
  archive      = {J_JRTIP},
  author       = {Hong, Hanyu and Shi, JiaoWei and Liu, Ziyuan and Zhang, Yaozong and Wu, Jinmeng},
  doi          = {10.1007/s11554-020-00997-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {619-634},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time critical part detection for the blurred image of infrared reconnaissance balloon with boundary curvature feature analysis},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grain complexity control of HEVC intra prediction in
battery-powered video codecs. <em>JRTIP</em>, <em>18</em>(3), 603–618.
(<a href="https://doi.org/10.1007/s11554-020-00996-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-efficiency video coding (HEVC) standard improves the coding efficiency at the cost of a significantly more complex encoding process. This is an issue for a large number of video-capable devices that operate on batteries, with limited and varying processing power. A complexity controller enables an encoder to provide the best possible quality at any power quota. This paper proposes a complexity control method for HEVC intra coding, based on a Pareto-efficient rate–distortion–complexity (R–D–C) analysis. The proposed method limits the intra prediction for each block (as opposed to existing methods which limit the block partitioning), on a frame-level basis. This method consists of three steps, namely rate-complexity modeling, complexity allocation, and configuration selection. In the first step, a rate-complexity model is presented which estimates the encoding complexity according to the compression intensity. Then, according to the estimated complexity and target complexity, a complexity budget is allocated to each frame. Finally, an encoding configuration from a set of Pareto-efficient configurations is selected according to the allocated complexity and the video content, which offers the best compression performance. Experimental results indicate that the proposed method can adjust the complexity from 100 to 50%, with a mean error rate of less than 0.1%. The proposed method outperforms many state-of-the-art approaches, in terms of both control accuracy and compression efficiency. The encoding performance loss in terms of BD-rate varies from 0.06 to 3.69%, on average, for 90–60% computational complexity, respectively. The method can also be used for lower than 50% complexity if need be, with a higher BD-rate.},
  archive      = {J_JRTIP},
  author       = {Hosseini, Elahe and Pakdaman, Farhad and Hashemi, Mahmoud Reza and Ghanbari, Mohammad},
  doi          = {10.1007/s11554-020-00996-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {603-618},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fine-grain complexity control of HEVC intra prediction in battery-powered video codecs},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithm optimisation and hardware implementation of
interprediction mode decision. <em>JRTIP</em>, <em>18</em>(3), 593–601.
(<a href="https://doi.org/10.1007/s11554-020-00985-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High efficiency video coding is the most widely used video coding standard. It has higher coding performance compared with its predecessor, H.264, but it also has higher computational complexity. Interprediction is the most computationally intensive part of the entire video encoding process. Selecting the optimal interprediction mode by the rate–distortion cost calculation function requires substantial complex calculation and memory access, thus greatly increasing the difficulty of real-time hardware encoding. This study proposes to replace the traditional complex error square sum calculation with an estimation method for distortion and rate. The estimation of distortion uses the Hadamard-transformed sum of absolute transformation difference instead of the complex calculation of the sum of squared difference, whereas the estimation of rate is obtained by weighting the number of prediction units (PUs). The experiment proves that the proposed interprediction rate–distortion cost calculation model can greatly reduce computational complexity when BD-rate is increased by 3.02%. In hardware implementation, the value of rate can be obtained by indexing the number of PUs, and the resource expenditure is small.},
  archive      = {J_JRTIP},
  author       = {Shi, Long-zhao and Yan, Danyu and Hong, Xiaojian and Huang, Bo and Yang, Xiuzhi},
  doi          = {10.1007/s11554-020-00985-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {593-601},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Algorithm optimisation and hardware implementation of interprediction mode decision},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time wavelet transform for infinite image strips.
<em>JRTIP</em>, <em>18</em>(3), 585–591. (<a
href="https://doi.org/10.1007/s11554-020-00995-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a single-loop approach to a 2-D discrete wavelet transform that allows processing infinitely high-image strip-maps. The paper gradually compares several computational strategies to finally show how to deal with a multi-scale wavelet transform of infinite image streams. Besides, the transform is followed by a bit-plane encoder which also processes data in a single loop. The whole machinery is part of a CCSDS 122.0 image codec which manages to process a single pixel in about 33 ns on a contemporary desktop computer, without the contribution of any parallel computing or SIMD vectorization.},
  archive      = {J_JRTIP},
  author       = {Barina, David},
  doi          = {10.1007/s11554-020-00995-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {585-591},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time wavelet transform for infinite image strips},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CGMBE: A model-based tool for the design and implementation
of real-time image processing applications on CPU–GPU platforms.
<em>JRTIP</em>, <em>18</em>(3), 561–583. (<a
href="https://doi.org/10.1007/s11554-020-00994-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing large images in real time requires effective image processing algorithms as well as efficient software design and implementation to take full advantage of all CPU cores and GPU resources on state of the art CPU/GPU platforms. Efficiently coordinating computations on both the host (CPU) and devices (GPUs), along with host–device data transfers is critical to achieving real-time performance. However, such coordination is challenging for system designers given the complexity of modern image processing applications and the targeted processing platforms. In this paper, we present a novel model-based design tool that automates and optimizes these critical design decisions for real-time image processing implementation. The proposed tool consists of a compile-time static analyzer and a run-time dynamic scheduler. The tool automates the process of scheduling dataflow tasks (actors) and coordinating CPU–GPU data transfers in an integrated manner. The approach uses an unfolded dataflow graph representation of the application along with thread-pool-based executors, which are optimized for efficient operation on the targeted CPU–GPU platform. This approach automates the most complicated aspects of the design and implementation process for image processing system designers, while maximizing the utilization of computational power, reducing the memory footprint for both the CPU and GPU, and facilitating experimentation for tuning performance-oriented designs.},
  archive      = {J_JRTIP},
  author       = {Wu, Jiahao and Xie, Jing and Bardakoff, Alexandre and Blattner, Timothy and Keyrouz, Walid and Bhattacharyya, Shuvra S.},
  doi          = {10.1007/s11554-020-00994-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {561-583},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CGMBE: A model-based tool for the design and implementation of real-time image processing applications on CPU–GPU platforms},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The genetic algorithm census transform: Evaluation of census
windows of different size and level of sparseness through hardware
in-the-loop training. <em>JRTIP</em>, <em>18</em>(3), 539–559. (<a
href="https://doi.org/10.1007/s11554-020-00993-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo correspondence is a well-established research topic and has spawned categories of algorithms combining several processing steps and strategies. One core part to stereo correspondence is to determine matching cost between the two images, or patches from the two images. Over the years several different cost metrics have been proposed, one being the Census Transform (CT). The CT is well proven for its robust matching, especially along object boundaries, with respect to outliers and radiometric differences. The CT also comes at a low computational cost and is suitable for hardware implementation. Two key developments to the CT are non-centric and sparse comparison schemas, to increase matching performance and/or save computational resources. Recent CT algorithms share both traits but are handcrafted, bounded with respect to symmetry, edge lengths and defined for a specific window size. To overcome this, a Genetic Algorithm (GA) was applied to the CT, proposing the Genetic Algorithm Census Transform (GACT), to automatically derive comparison schemas from example data. In this paper, FPGA-based hardware acceleration of GACT, has enabled evaluation of census windows of different size and shape, by significantly reducing processing time associated with training. The experiments show that lateral GACT windows produce better matching accuracy and require less resources when compared to square windows.},
  archive      = {J_JRTIP},
  author       = {Ahlberg, Carl and León, Miguel and Ekstrand, Fredrik and Ekström, Mikael},
  doi          = {10.1007/s11554-020-00993-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {539-559},
  shortjournal = {J. Real-Time Image Process.},
  title        = {The genetic algorithm census transform: Evaluation of census windows of different size and level of sparseness through hardware in-the-loop training},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA implementation of HOOFR bucketing extractor-based
real-time embedded SLAM applications. <em>JRTIP</em>, <em>18</em>(3),
525–538. (<a href="https://doi.org/10.1007/s11554-020-00986-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction is an important vision task in many applications like simultaneous localization and mapping (SLAM). In the recent computing systems, FPGA-based acceleration have presented a strong competition to GPU-based acceleration due to its high computation capabilities and lower energy consumption. In this paper, we present a high-level synthesis implementation on a SoC-FPGA of a feature extraction algorithm dedicated for SLAM applications. We choose HOOFR extraction algorithm which provides a robust performance but requires a significant computation on embedded CPU. Our system is dedicated for SLAM applications so that we also integrated bucketing detection method in order to have a homogeneous distribution of keypoints in the image. Moreover, instead of optimizing performance by simplifying the original algorithm as in many other researches, we respected the complexity of HOOFR extractor and have parallelized the processing operations. The design has been validated on an Intel Arria 10 SoC-FPGA with a throughput of 54 fps at $$1226 \times 370$$ pixels (handling 1750 features) or 14 fps at $$1920 \times 1080$$ pixels (handling 6929 features).},
  archive      = {J_JRTIP},
  author       = {Nguyen, Dai Duong and El Ouardi, Abdelhafid and Rodriguez, Sergio and Bouaziz, Samir},
  doi          = {10.1007/s11554-020-00986-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {525-538},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of HOOFR bucketing extractor-based real-time embedded SLAM applications},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complexity reduction of test zonal search for fast motion
estimation in uni-prediction of high efficiency video coding.
<em>JRTIP</em>, <em>18</em>(3), 511–524. (<a
href="https://doi.org/10.1007/s11554-020-00983-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of motion estimation in the High Efficiency Video Coding (HEVC) standard is very high as it uses a number of prediction block sizes. The test zonal search (TZS) mechanism is used as motion estimation algorithm in the fast search mode of HM encoder, the reference software for HEVC. In this paper, we present schemes for reducing the complexity of the TZS algorithm for uni-prediction in HEVC. The proposed mechanisms help in reducing complexity of grid, raster and refinement search stages of TZS. The performance of the proposed mechanisms is tested independently and also combined in the fast search mode of HM-16.18. The motion estimation time and the number of search points in the combined algorithm are reduced by 68.14% and 77.10% with a BD-rate of 0.30% and a BD-PSNR of $$-0.007$$ dB in comparison with the original fast search mode in the low-delay P main profile. The proposed complexity reduction schemes are compared with those adopted in other recently proposed fast motion estimation algorithms and the performance of the proposed scheme is found to be superior in terms of the reduction in motion estimation time.},
  archive      = {J_JRTIP},
  author       = {Varma, K. C. Ravi Chandra and Mahapatra, Sudipta},
  doi          = {10.1007/s11554-020-00983-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {511-524},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Complexity reduction of test zonal search for fast motion estimation in uni-prediction of high efficiency video coding},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate and robust tracking of rigid objects in real time.
<em>JRTIP</em>, <em>18</em>(3), 493–510. (<a
href="https://doi.org/10.1007/s11554-020-00978-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the shape model object tracker, which is accurate, robust, and real-time capable on a standard CPU. The tracker has a failure mode detection, is robust to nonlinear illumination changes, and can cope with occlusions. It uses subpixel-precise image edges to track roughly rigid objects with high accuracy and is virtually drift-free even for long sequences. Furthermore, it is inherently capable of object re-detection when tracking fails. To evaluate the accuracy, robustness, and efficiency of the tracker precisely, we present a challenging new tracking dataset with pixel-precise ground truth. The precise ground-truth labels are created automatically from the photo-realistic synthetic VIPER dataset. The tracker is thoroughly evaluated against the state of the art through a number of qualitative and quantitative experiments. It is able to perform on par with the current state-of-the-art deep-learning trackers, but is at least 45 times faster, even without using a GPU. The efficiency and low memory consumption of the tracker are validated in further experiments that are conducted on an embedded device.},
  archive      = {J_JRTIP},
  author       = {Böttger, Tobias and Steger, Carsten},
  doi          = {10.1007/s11554-020-00978-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {493-510},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accurate and robust tracking of rigid objects in real time},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dedicated hardware accelerator for real-time acceleration
of YOLOv2. <em>JRTIP</em>, <em>18</em>(3), 481–492. (<a
href="https://doi.org/10.1007/s11554-020-00977-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, dedicated hardware accelerators for the acceleration of the convolutional neural network (CNN) have been extensively studied. Although many studies have presented efficient designs on FPGAs for image classification neural network models such as AlexNet and VGG, there are still little implementations for CNN-based object detection applications. This paper presents an OpenCL-based high-throughput FPGA accelerator for the YOLOv2 object detection algorithm on Arria-10 GX1150 FPGA. The proposed hardware architecture adopts a scalable pipeline design to support multi-resolution input image and full 8-bit fixed-point datapath to improve hardware resource utilization. Layer fusion technology that merges the convolution, batch normalization and Leaky-ReLU is also developed to avoid transmission of intermediate data between FPGA and external memory. Experimental results show that the final design achieves a peak throughput of 566 GOP/s under the working frequency of 190 MHz. The accelerator can execute YOLOv2 inference computation ( $$288\times 288$$ resolution) and tiny YOLOv2 ( $$416\times 416$$ resolution) at the speed of 35 and 71 FPS, respectively.},
  archive      = {J_JRTIP},
  author       = {Xu, Ke and Wang, Xiaoyun and Liu, Xinyang and Cao, Changfeng and Li, Huolin and Peng, Haiyong and Wang, Dong},
  doi          = {10.1007/s11554-020-00977-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {481-492},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A dedicated hardware accelerator for real-time acceleration of YOLOv2},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High precision and fast disparity estimation via parallel
phase correlation hierarchical framework. <em>JRTIP</em>,
<em>18</em>(3), 463–479. (<a
href="https://doi.org/10.1007/s11554-020-00972-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When estimating the disparity of remote sensing images, known phase correlation (PC)-based disparity estimation methods are not fast and robust, such as hierarchical structure PC method and fixed window PC method. To tackle this problem, a parallel PC-based hierarchical framework is proposed, which includes two ideas: first, a weighted PC peak fitting algorithm is introduced for estimating the high precise disparity matrix efficiently and stably; second, a graphics processing unit-based parallel PC algorithm is integrated into the hierarchical framework for fast and robustly estimating high precise disparity map. Additionally, many stages of hierarchical framework, such as padding and reliable evaluation stages, are improved for improving the computational efficiency of disparity estimation system. In a large number of experiments, the results have shown that the efficiency of the proposed algorithm is on average 24 times faster than the compared state-of-the-art methods. Meanwhile, the precision of the proposed algorithm is also superior to or very close to the compared algorithms. The proposed algorithm has been successfully used in a unmanned aerial vehicle three-dimensional retrieval system, and the practice effect has also been verified.},
  archive      = {J_JRTIP},
  author       = {Li, Jie and Liu, Yiguang},
  doi          = {10.1007/s11554-020-00972-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {463-479},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High precision and fast disparity estimation via parallel phase correlation hierarchical framework},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU-accelerated uncapacitated facility location and
semi-dense SymStereo pipelines for piecewise-planar-based 3D
reconstruction. <em>JRTIP</em>, <em>18</em>(3), 445–461. (<a
href="https://doi.org/10.1007/s11554-020-00974-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planar 3D reconstruction presents advantages over point cloud representations. This work focuses on the acceleration of piecewise-planar-based 3D reconstruction, a StereoScan method. We identify the SymStereo (logN) and uncapacitated facility location (UFL) algorithms as the most computationally expensive tasks, consuming nearly 80 × of total runtime, when detecting planes in a single stereo pair on a sequential CPU pipeline. Consequently, these algorithms have been parallelized using single- and multi-GPU architectures to perform significantly faster than previous sequential approaches. Experimental results show that accelerated parallel implementations of SymStereo (logN) can process up to 56 frames per second, achieving a speedup of 38 × against the sequential C implementation (Intel Core i7-4790k). The parallel version of the message-passing algorithm (max-sum) for the UFL problem processes up to five matrices per second and outperforms the sequential C baseline for computing UFL by 38 ×.},
  archive      = {J_JRTIP},
  author       = {Graca, Carlos and Raposo, Carolina and Barreto, Joao P. and Nunes, Urbano and Falcao, Gabriel},
  doi          = {10.1007/s11554-020-00974-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {445-461},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU-accelerated uncapacitated facility location and semi-dense SymStereo pipelines for piecewise-planar-based 3D reconstruction},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combined kernel for fast GPU computation of zernike moments.
<em>JRTIP</em>, <em>18</em>(3), 431–444. (<a
href="https://doi.org/10.1007/s11554-020-00979-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zernike moments, as a representative orthogonal moment, have been widely applied in the fields of image processing and pattern recognition. The calculations are time-consuming due to the complexity of definition. Based on the GPU octant symmetry algorithm in our previous work, this paper presents a novel algorithm to increase the resource utilization by the combined kernel. Also, it optimizes radial polynomials of Zernike moments to reduce amount of calculations. The experimental results demonstrated that the proposed algorithm achieved overall computational performance improvement for any sized images. Moreover, there is no compromise in terms of precision compared to the typical accurate algorithm.},
  archive      = {J_JRTIP},
  author       = {Zhao, Zengjun and Kuang, Xinkai and Zhu, Yukuan and Liang, Yecheng and Xuan, Yubo},
  doi          = {10.1007/s11554-020-00979-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {431-444},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Combined kernel for fast GPU computation of zernike moments},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate real-time visual SLAM combining building models and
GPS for mobile robot. <em>JRTIP</em>, <em>18</em>(2), 419–429. (<a
href="https://doi.org/10.1007/s11554-020-00989-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel 7 DOF (i.e., orientation, translation, and scale) visual simultaneous localization and mapping (vSLAM) system for mobile robots in outdoor environments. In the front end of this vSLAM system, a fast initialization method is designed for different vSLAM backbones, which upgrades the accuracy of trajectory and reconstruction of vSLAM with an absolute scale computed from depth maps generated by building blocks. In the back end of this vSLAM, we propose a nonlinear optimization mechanism throughout which multimodal data are combined for more robust optimization. The modality of building blocks in optimization can improve the tracking accuracy and the scale estimation. By integrating the pose estimated from visual information and the position received through GPS, the optimization further alleviates the drift. The experimental results prove that the proposed method is extremely suitable for outer AR application for outdoor environments, because our method has superior initialization performance, runs in real time, and achieves real scale, higher accuracy, and robustness.},
  archive      = {J_JRTIP},
  author       = {Liu, Ruyu and Zhang, Jianhua and Chen, Shengyong and Yang, Thomas and Arth, Clemens},
  doi          = {10.1007/s11554-020-00989-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {419-429},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accurate real-time visual SLAM combining building models and GPS for mobile robot},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Study on computer vision target tracking algorithm based on
sparse representation. <em>JRTIP</em>, <em>18</em>(2), 407–418. (<a
href="https://doi.org/10.1007/s11554-020-00999-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video target tracking covers a variety of interdisciplinary subjects such as pattern recognition, image processing, computer graphics and artificial intelligence. In recent years, visual tracking research methods have made significant progress, and scholars have proposed many excellent algorithms. Based on this, this paper uses the basic tracking algorithm and block orthogonal matching pursuit (BOMP) algorithm of image reconstruction, respectively, from the run time, the quality of reconstruction, reconstruction error of the two algorithms to do simulation experiments, and compare their performance, the results show that the BOMP algorithm running time is short, has extensive application, therefore, to determine the BOMP algorithm as a sparse representation model is the core of the method. Then establish the target observation model, introduce the sparse display into the particle filter framework, and update the sparse representation coefficients so that the norm reaches the optimal solution and ensure the accuracy of the tracking target. Finally, through the simulation experiment, the success rate of the target coverage is calculated. The results show that the BOMP algorithm can maintain high tracking accuracy and strong stability in the case of appearance changes caused by illumination changes, partial occlusion and attitude changes.},
  archive      = {J_JRTIP},
  author       = {Ma, Wenjuan and Xu, Feng},
  doi          = {10.1007/s11554-020-00999-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {407-418},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Study on computer vision target tracking algorithm based on sparse representation},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MARS: Parallelism-based metrically accurate 3D
reconstruction system in real-time. <em>JRTIP</em>, <em>18</em>(2),
393–405. (<a href="https://doi.org/10.1007/s11554-020-01031-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing application demands, lightweight device-based 3D recovery draws many attentions from a wide group of researchers in both academic and industrial fields. The current 3D reconstruction solutions are commonly achieved either using depth data or RGB data. The depth data usually come from a deliberately designed hardware for specific tasks, while the RGB data-based solutions only employ a single RGB camera with vision-based computing algorithms. Limitations are expected from both. Depth sensors are commonly either bulky or relatively expensive compared to RGB cameras, thus of less flexibility. Normal RGB cameras usually have better mobility but less accuracy in 3D sensing than depth sensors. Recently, machine learning based depth estimation has also been presented. However, its accuracy is still limited. To improve the flexibility of the 3D reconstruction system without loss in accuracy, this paper presents a solution of unconstrained Metrically Accurate 3D Reconstruction System (MARS) for 3D sensing based on a consumer-grade camera. With a simple initialization from a depth map, the system can achieve incremental 3D reconstruction with a stable metric scale. Experiments are conducted using both real-world data and public datasets. Competitive results are obtained using the proposed system compared with several existing methods.},
  archive      = {J_JRTIP},
  author       = {Zhang, Shu and Wang, Ting and Li, Gongfa and Dong, Junyu and Yu, Hui},
  doi          = {10.1007/s11554-020-01031-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {393-405},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MARS: Parallelism-based metrically accurate 3D reconstruction system in real-time},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast incremental structure from motion based on parallel
bundle adjustment. <em>JRTIP</em>, <em>18</em>(2), 379–392. (<a
href="https://doi.org/10.1007/s11554-020-00970-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structure from motion has attracted a lot of research in recent years, with new state-of-the-art approaches coming almost every year. One of its advantages over 3D reconstruction is that it can be used for any cameras (UAVs, depth sensor, light field) and produces relatively accurate point clouds and camera parameters. One of its disadvantages compared to other approaches is that it is computationally expensive. In this paper, we design a novel structure-from-motion framework to reduce the computational cost and implement a parallel bundle adjustment on GPU device for large-scale optimization. In our framework, the local bundle adjustment is added into the architecture of the incremental structure from motion; namely, the point clouds and camera’s parameters are optimized when an additional number of images was added. Then, the purpose is not only to improve the quality of the produced point clouds but also to reduce computation time via parallel bundle adjustment. We conduct extensively experiments on several challenging datasets and make comparison with the state-of-the-art methods. Experimental results show that the proposed method has the best performance in terms of accuracy and efficiency.},
  archive      = {J_JRTIP},
  author       = {Cao, Mingwei and Zheng, Liping and Jia, Wei and Liu, Xiaoping},
  doi          = {10.1007/s11554-020-00970-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {379-392},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast incremental structure from motion based on parallel bundle adjustment},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image real-time augmented reality technology based on
spatial color and depth consistency. <em>JRTIP</em>, <em>18</em>(2),
369–377. (<a href="https://doi.org/10.1007/s11554-020-00988-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality can enhance people’s perception of the environment by embedding virtual objects or other information in real-time images. In this paper, the color image is used as a reference to calculate the confidence of the original depth map, and stereo matching is performed according to the feature points. The depth map is mainly enhanced by the color, edge, and segmentation results of the color image. A deep computing system based on augmented reality is designed. The system can use a binocular camera to collect object images in real time and obtain better parallax images by correcting the calibrated image. The semi-global block matching algorithm and depth calculation are used to realize the tracking registration of virtual objects. Experiments in different environments show that the system has good real-time performance, light invariance, and depth consistency.},
  archive      = {J_JRTIP},
  author       = {Zhai, Lijie and Chen, Donghui},
  doi          = {10.1007/s11554-020-00988-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {369-377},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Image real-time augmented reality technology based on spatial color and depth consistency},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human motion tracking and positioning for augmented reality.
<em>JRTIP</em>, <em>18</em>(2), 357–368. (<a
href="https://doi.org/10.1007/s11554-020-01030-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AR (Augmented reality) is a research hotspot in the current computer application field. AR technology enhances people’s understanding and experience of the real environment by adding virtual objects to real scenes to integrate virtual objects with the real environment. Aiming at the weak processing power of intelligent terminals and the characteristics of limited hardware resources, this paper proposes a more effective human motion feature extraction and descriptor algorithm. The feature point detection and positioning method suitable for intelligent terminals is proposed in a targeted manner, which solves the problem of mismatching of similar structures. In addition, this paper proposes an AR-oriented recursive tracking algorithm for human motion. The positional relationship of the current frame is calculated from the position of the previous frame. A combination of ORB (Oriented fast and Rotated Brief) feature descriptors and KLT (Kanade-Lucas-Tomasi) algorithm is adopted. The ORB feature descriptor matched by the first frame image and the reference image is tracked by the KLT tracking algorithm, and the feature descriptor of the previous frame is tracked in the current frame, thereby eliminating the phenomenon of virtual object jitter. The experimental results show that the recursive tracking scheme has better performance in time and precision than the detection tracking scheme.},
  archive      = {J_JRTIP},
  author       = {Yue, Shaojun},
  doi          = {10.1007/s11554-020-01030-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {357-368},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Human motion tracking and positioning for augmented reality},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view data capture for dynamic object reconstruction
using handheld augmented reality mobiles. <em>JRTIP</em>,
<em>18</em>(2), 345–355. (<a
href="https://doi.org/10.1007/s11554-021-01095-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a system to capture nearly synchronous frame streams from multiple and moving handheld mobiles that is suitable for dynamic object 3D reconstruction. Each mobile executes Simultaneous Localisation and Mapping on-board to estimate its pose, and uses a wireless communication channel to send or receive synchronisation triggers. Our system can harvest frames and mobile poses in real time using a decentralised triggering strategy and a data-relay architecture that can be deployed either at the Edge or in the Cloud. We show the effectiveness of our system by employing it for 3D skeleton and volumetric reconstructions. Our triggering strategy achieves equal performance to that of an NTP-based synchronisation approach, but offers higher flexibility, as it can be adjusted online based on application needs. We created a challenging new dataset, namely 4DM, that involves six handheld augmented reality mobiles recording an actor performing sports actions outdoors. We validate our system on 4DM, analyse its strengths and limitations, and compare its modules with alternative ones.},
  archive      = {J_JRTIP},
  author       = {Bortolon, Matteo and Bazzanella, Luca and Poiesi, Fabio},
  doi          = {10.1007/s11554-021-01095-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {345-355},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-view data capture for dynamic object reconstruction using handheld augmented reality mobiles},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient information distillation network for real-time
single-image super-resolution. <em>JRTIP</em>, <em>18</em>(2), 333–344.
(<a href="https://doi.org/10.1007/s11554-021-01083-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep convolutional neural networks have played an increasingly important role in single-image super-resolution (SR). However, with the increase of the depth and width of networks, the super-resolution methods based on convolution neural networks are facing training difficulties, memory consumption, running slowness and other problems. Furthermore, most of the methods do not make full use of the image gradient information which leads to the loss of geometric structure information of the image. To solve these problems, we propose a gradient information distillation network in this paper. On the one hand, the advantages of fast and lightweight are maintained through information distillation. On the other hand, the SR performance is improved by gradient information. Our network has two branches named gradient information distillation branch (GIDB) and image information distillation branch. To combine features in both branches, we also introduce a residual feature transfer mechanism (RFT). Under the function of GIDB and RFT, our network can retain the rich geometric structure information which can make the edge details of the reconstructed image sharper. The experimental results show that our method is superior to the existing methods while well limits the parameters, computation and running time of the model. It provides the possibility for real-time image processing and mobile applications.},
  archive      = {J_JRTIP},
  author       = {Meng, Bin and Wang, Lining and He, Zheng and Jeon, Gwanggil and Dou, Qingyu and Yang, Xiaomin},
  doi          = {10.1007/s11554-021-01083-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {333-344},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Gradient information distillation network for real-time single-image super-resolution},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using augmented reality and deep learning to enhance taxila
museum experience. <em>JRTIP</em>, <em>18</em>(2), 321–332. (<a
href="https://doi.org/10.1007/s11554-020-01038-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Museums have adapted their traditional ways of providing services with the advent of novel digital technologies to match up with the pace and growing needs of current industry revolution. Mixed Reality has revitalized interpretation of numerous domains by offering immersive experiences in digital and real world. In the proposed study, an attempt was made to enrich user’s museum experience with relevant multimedia information and for building a better connection with the artifacts with in Taxila Museum in Pakistan, which has beautifully preserved the Gandhara civilization. The proposed solution is an Augmented Reality (AR)-based smartphone application which recognizes artifacts using Deep Learning in real time and retrieve supportive multimedia information for the visitors. To provide user with exact content, convolutional neural networks (CNN) will be applied to correctly recognize artifacts. The significance of proposed application is compared with traditional human guided or free user tours through user-centric questionnaire-based survey. The evaluation is carefully performed using relevant evaluation models including Museum Experience Scale (MES) and triptych model of interactivity.  The findings of the study are discussed and assessed comprehensively using statistical methods to highlight its significance.},
  archive      = {J_JRTIP},
  author       = {Khan, Mudassar Ali and Israr, Sabahat and S Almogren, Abeer and Din, Ikram Ud and Almogren, Ahmad and Rodrigues, Joel J. P. C.},
  doi          = {10.1007/s11554-020-01038-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {321-332},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Using augmented reality and deep learning to enhance taxila museum experience},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network algorithm real-time depth image 3D human recognition
for augmented reality. <em>JRTIP</em>, <em>18</em>(2), 307–319. (<a
href="https://doi.org/10.1007/s11554-020-01045-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the application of augmented reality real-time depth image technology to 3D human motion recognition technology. The accuracy and real-time performance of sensor-based 3D human reconstruction are affected by visual characteristics and illumination changes. Features are not easily extracted and cannot be tracked, leading to failures in the 3D reconstruction of the human body. Based on this system, the sensor-based visual inertial initialization algorithm is studied, which is integrated in the two-frame image time interval to provide accurate initial values for vision-based motion estimation, improve the accuracy of the calculated posture, and finally improve the accuracy of the 3D reconstruction system. Based on the relationship between the depth image and the distance and reflectivity, a model for correcting the distance error and reflectivity error of the depth image is established to improve the accuracy of the depth image, and finally the accuracy of the three-dimensional reconstruction of the human body.},
  archive      = {J_JRTIP},
  author       = {Huang, Renyong and Sun, Mingyi},
  doi          = {10.1007/s11554-020-01045-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {307-319},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Network algorithm real-time depth image 3D human recognition for augmented reality},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error elimination method in moving target tracking in
real-time augmented reality. <em>JRTIP</em>, <em>18</em>(2), 295–305.
(<a href="https://doi.org/10.1007/s11554-020-01047-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an augmented reality interactive method based on improved Kinect sensor commands to eliminate the tracking error of moving targets. Designed posture tracking architecture. Experiments show that the above two types of algorithms can meet the needs of the project and have certain light resistance. Among them, the algorithm based on feature points has certain advantages in speed and requires fewer manual processing steps. In this case, it is necessary to combine the previous algorithm to solve the posture, use the optical flow method to track the feature points, avoid the operation of extracting, matching, and removing the mismatch of each frame of feature points, and improve the speed of the algorithm.},
  archive      = {J_JRTIP},
  author       = {Shi, Yingjie and Zhao, Zijian},
  doi          = {10.1007/s11554-020-01047-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {295-305},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Error elimination method in moving target tracking in real-time augmented reality},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile augmented reality technology for design and
implementation of library document push system. <em>JRTIP</em>,
<em>18</em>(2), 283–293. (<a
href="https://doi.org/10.1007/s11554-020-01048-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through mobile augmented reality interaction, the library can keep abreast of the user’s current information orientation and lay the foundation for the further establishment of a personal information database. At the same time, on the basis of network interaction, it plays a good guiding role for the construction of library document resources, ensuring the effective use of document resources. This paper analyzes the key technologies of the mobile augmented reality system, including virtual graphics rendering technology, camera tracking and positioning technology, display technology and so on. To solve the problem of human error caused by design deviation in the current mobile augmented reality library document push, a new data visualization and interactive design process is proposed. On the basis of analyzing user needs and comparing augmented reality library document push with traditional library document push, the demand is transformed into tasks. We use a combination of overview and detail to lay out the visual elements. The progressive interactive method is used to organize discrete data views and generate interactive interfaces. The system test results show that the library document pushing system designed in this paper can effectively reduce the memory consumption of the system, improve the refresh frame rate of the view and improve the user’s interactive fluency.},
  archive      = {J_JRTIP},
  author       = {Lu, Jing},
  doi          = {10.1007/s11554-020-01048-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {283-293},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Mobile augmented reality technology for design and implementation of library document push system},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of real-time augmented reality scene light sources
and construction of photorealis tic rendering framework. <em>JRTIP</em>,
<em>18</em>(2), 271–281. (<a
href="https://doi.org/10.1007/s11554-020-01022-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the main network of multi-channel light sources is improved, so that multi-channel pictures can be fused for joint training. Secondly, for high-resolution detection pictures, the huge memory consumption leads to a reduction in batches and then affects the model distribution. Group regularization is adopted. We can still train the model normally in small batches; then, combined with the method of the regional candidate network, the final detection accuracy and the accuracy of the candidate frame regression are improved. Finally, through in-depth analysis, based on image lighting technology and physical-based rendering theory, the requirements for lighting effects and performance limitations, combined with a variety of image enhancement technologies, such as gamma correction, HDR, and these technologies used in Java. Real-time lighting algorithms that currently run efficiently on mainstream PCs. The algorithm can be well integrated into the existing rasterization rendering pipeline, while into account better lighting effects and higher operating efficiency. Finally, the lighting effects achieved by the algorithm are tested and compared through experiments. This algorithm not only achieves a very good light and shadow effect when rendering virtual objects with a real scene as the background but also can meet the realistic rendering of picture frames in more complex scenes. Rate requirements. The experimental results show that the virtual light source automatically generated by this algorithm can approximate the lighting of the real scene, and the virtual object and the real object can produce approximately consistent lighting effects in an augmented reality environment with one or more real light sources.},
  archive      = {J_JRTIP},
  author       = {Ni, Taile and Chen, Yingshuang and Liu, Shoupeng and Wu, Jinglong},
  doi          = {10.1007/s11554-020-01022-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {271-281},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Detection of real-time augmented reality scene light sources and construction of photorealis tic rendering framework},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A cylindrical shape descriptor for registration of
unstructured point clouds from real-time 3D sensors. <em>JRTIP</em>,
<em>18</em>(2), 261–269. (<a
href="https://doi.org/10.1007/s11554-020-01033-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with data sets from real-time 3D sensors of RGB-D or TOF cameras, this paper presents a method for registration of unstructured point clouds. We firstly derive intrinsic shape context descriptors for 3D data organization. To replace the Fast-Marching method, a vertex-oriented triangle propagation method is applied to calculate the ’angle’ and ’radius’ in descriptor charting, so that the matching accuracy at the twisting and folding area is significantly improved. Then, a 3D cylindrical shape descriptor is proposed for registration of unstructured point clouds. The chosen points are projected into the cylindrical coordinate system to construct the descriptors. The projection parameters are respectively determined by the distances from the chosen points to the reference normal vector, and the distances from the chosen points to the reference tangent plane and the projection angle. Furthermore, Fourier transform is adopted to deal with orientation ambiguity in descriptor matching. Practical experiments demonstrate a satisfactory result in point cloud registration and notable improvement on standard benchmarks.},
  archive      = {J_JRTIP},
  author       = {He, Yu and Chen, Shengyong and Yu, Hongchuan and Yang, Thomas},
  doi          = {10.1007/s11554-020-01033-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {261-269},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A cylindrical shape descriptor for registration of unstructured point clouds from real-time 3D sensors},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-speed real-time augmented reality tracking algorithm
model of camera based on mixed feature points. <em>JRTIP</em>,
<em>18</em>(2), 249–259. (<a
href="https://doi.org/10.1007/s11554-020-01032-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At this stage of augmented reality, simple feature descriptions are mainly used in camera real-time motion tracking, but this is prone to the problem of unstable camera motion tracking. Aiming at the balance between real-time performance and stability, a new method model of real-time camera motion tracking based on mixed features was proposed. By comprehensively using feature points and feature lines as scene features, feature extraction, optimization, and fusion are used to construct hybrid features, and the hybrid features are unified for real-time camera parameter estimation. An image feature optimization method based on scene structure analysis is proposed to meet the computing constraints of mobile terminals. An iterative feature line-screening method is proposed to calculate a stable feature line set, and based on the scene feature composition and feature geometry, a hybrid feature is adaptively constructed to improve the tracking stability of the camera. Based on improved SIFT feature matching target detection and tracking algorithm, a hybrid feature point detection operator detection algorithm is used to achieve rapid feature point extraction, and the speed of descriptor generation is reduced by reducing the feature descriptor vector dimension. The experimental results prove that the proposed target detection and tracking algorithm has good real-time and robustness, and improves the success rate of target detection and tracking.},
  archive      = {J_JRTIP},
  author       = {Sun, Wei and Mo, Chengcheng},
  doi          = {10.1007/s11554-020-01032-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {249-259},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High-speed real-time augmented reality tracking algorithm model of camera based on mixed feature points},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time image processing for augmented reality on mobile
devices. <em>JRTIP</em>, <em>18</em>(2), 245–248. (<a
href="https://doi.org/10.1007/s11554-021-01097-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, remarkable progress has been made in mobile consumer devices. Modern smartphones and tablet computers offer multi-core processors and graphics processing units, which have opened up new application possibilities such as augmented reality, virtual reality, and 3D reconstruction. Augmented Reality (AR) is a key technology that is going to facilitate a paradigm shift in the way users interact with data and has only just recently been recognized as a viable solution for solving many critical needs.},
  archive      = {J_JRTIP},
  author       = {Lv, Zhihan and Lloret, Jaime and Song, Houbing},
  doi          = {10.1007/s11554-021-01097-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {245-248},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time image processing for augmented reality on mobile devices},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Channel-independent spatially regularized discriminative
correlation filter for visual object tracking. <em>JRTIP</em>,
<em>18</em>(1), 233–243. (<a
href="https://doi.org/10.1007/s11554-020-00967-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study proposes the improvements for visual object trackers based on discriminative correlation filters. These improvements consist in the development of the channel-independent spatially regularized method for filter calculation, which is based on the alternating direction method of multipliers as well as in the use of additional features that are the result of the backprojection of normalized weighted object histogram. The VOT Challenge 2018 benchmark has confirmed that the proposed approaches allow to increase the tracking robustness. Particularly, by the value of expected average overlap (EAO = 0.1828), the tracker that uses these approaches (CISRDCF) can reach the level of more computationally complex competitors that utilize convolutional neural features. At the same time, the software-optimized version of the CISRDCF tracker, which implements the suggested improvements has moderate computational complexity and can operate in the real-time both on the PC and on the mid-range ARM-based processors, making the CISRDCF tracker promising for embedded applications.},
  archive      = {J_JRTIP},
  author       = {Varfolomieiev, A.},
  doi          = {10.1007/s11554-020-00967-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {233-243},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Channel-independent spatially regularized discriminative correlation filter for visual object tracking},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple, robust and secure data hiding based on CRT feature
extraction and closed-loop chaotic encryption system. <em>JRTIP</em>,
<em>18</em>(1), 221–232. (<a
href="https://doi.org/10.1007/s11554-020-00971-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust data hiding methods are basically used when rightful ownership is intended and/or transmitting channel is prone to noise. In this paper, a robust and secure data hiding method is proposed in Tchebichef domain. For feature extraction, remainders of Tchebichef coefficients are calculated according to Chinese remainder theorem (CRT). Distances of these remainders are utilized as the extracted features. These distances are then modified according to the hidden bits, to complete the embedding process. To reduce the chance of illegal access and increase the security of the proposed method, a closed-loop chaotic encryption system (CLCES) has been introduced. The proposed CLCES requires very little side information and produces extremely sensitive noise-like sequences with high security. Robustness and security of the proposed method have been analyzed. Comparison results with other related methods in the current literature confirm the superiority of the proposed method. Due to its simplicity, the proposed method is a good candidate for real-time data hiding applications.},
  archive      = {J_JRTIP},
  author       = {Ghaemi, Alireza and Danyali, Habibollah and Kazemi, Kamran},
  doi          = {10.1007/s11554-020-00971-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {221-232},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Simple, robust and secure data hiding based on CRT feature extraction and closed-loop chaotic encryption system},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time accurate eye center localization for
low-resolution grayscale images. <em>JRTIP</em>, <em>18</em>(1),
193–220. (<a href="https://doi.org/10.1007/s11554-020-00955-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye center localization is considered a crucial step for many human–computer interaction (HCI) real-time applications. Detecting the center of eye (COE), accurately and in real time, is very challenging due to the wide variation of poses, eye appearance and specular reflection, especially in low-resolution images. In this paper, an accurate real-time detection algorithm of the COE is proposed. The proposed approach depends on the image gradient to detect the COE. The computational complexity is minimized and the accuracy is improved by down sampling the face resolution and applying a rough-to-fine algorithms, to reduce the search area, in accordance with the Eye Region Of Interest (EROI) and the number of COE candidates, tested by the proposed algorithm. Also, the detection algorithm is applied on a limited number of pixels that represent the iris boundary of the COE candidates. The Look Up Tables (LUTs) are implemented to, initially, store the invariant elements of the proposed image gradient-based algorithm, to reduce the detection time. Before applying the proposed COE detection approach, a modified specular reflection method is used to improve the detection accuracy. The performance of the proposed algorithm has been evaluated by applying it to three benchmark databases: the BIOID, GI4E and Talking Face video datasets, at different face resolutions. Experimental results revealed that the accuracy of the proposed algorithm is up to 91.68% and 96.7% for BIOID and GI4E datasets, respectively, while the minimum achieved average detection time is 2.7 ms. The promising results highlight the potential of the proposed algorithm to be used in some eye gaze-based real-time applications. Comparing the proposed method with the most state-of-the-art approaches showed that the system outperforms most of them and has a comparable performance with the others, in terms of the COE localization accuracy and detection speed.},
  archive      = {J_JRTIP},
  author       = {Ahmed, Noha Younis},
  doi          = {10.1007/s11554-020-00955-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {193-220},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time accurate eye center localization for low-resolution grayscale images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A light tracker for online multiple pedestrian tracking.
<em>JRTIP</em>, <em>18</em>(1), 175–191. (<a
href="https://doi.org/10.1007/s11554-020-00962-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel real-time multiple pedestrian tracker for videos acquired from both static and moving cameras in unconstrained real-world environment. In such scenes, trackers always suffer from noisy detections and frequent occlusions. Existing methods usually use complex learning approaches and a large number of training samples to get discriminative appearance features. However, this leads to high computational cost and hardly works in occlusions (missing detections) and undistinguishable appearance. Addressing this, we design a light two-stage tracker. Firstly, a shallow net with two layers of full convolution is proposed to encode appearance. Compared with other deep architectures and sophisticated learning approaches, our shallow net is efficient and robust enough without any online updating. Secondly, we design a motion model to deal with noisy detections and missing objects caused by motion blur or occlusion. By mining the motion pattern, our tracker can reliably predict the object location under challenging scenes. Furthermore, we propose a speedup version to verify our robustness and the possibility of using in online applications. Extensive experiments are implemented on multiple object tracking benchmarks, MOT15 and MOT17. The performance is competitive over a number of state-of-the-art trackers and demonstrates that our tracker is very promising for real-time applications.},
  archive      = {J_JRTIP},
  author       = {Wang, Nan and Zou, Qi and Ma, Qiulin and Huang, Yaping and Luan, Di},
  doi          = {10.1007/s11554-020-00962-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {175-191},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A light tracker for online multiple pedestrian tracking},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bio-inspired smart vision sensor: Toward a reconfigurable
hardware modeling of the hierarchical processing in the brain.
<em>JRTIP</em>, <em>18</em>(1), 157–174. (<a
href="https://doi.org/10.1007/s11554-020-00960-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological vision systems inspire processing methods in computer vision applications. This paper employs the insights of vision systems in hardware and presents a pixel-parallel, reconfigurable, and layer-based hierarchical architecture for smart image sensors. The architecture aims to bring computation close to the sensor to achieve high acceleration for different machine vision applications while consuming low power. We logically divide the image into multiple regions and perform pixel-level and region-level processing after removing spatiotemporal redundancy. Those processors use bio-inspired algorithms to activate the regions with region of interest of a scene. The hierarchical processing breaks the traditional sequential image processing and introduces parallelism for machine vision applications. Also, we make the hardware design reconfigurable even after fabrication to make the hardware reusable for different applications. Simulation results show that the area overhead and power penalty for adding reconfigurable features stay in an acceptable range. We emphasize to maximize the operating speed and obtain 800 MHz. Besides, the design saves 84.01% and 96.91% dynamic power at the first and second stages of the hierarchy by removing redundant information. Furthermore, the sequential deployment of high-level reasoning only on the selected regions of the image becomes computationally inexpensive to execute a complex task in real time.},
  archive      = {J_JRTIP},
  author       = {Bhowmik, Pankaj and Pantho, Md Jubaer Hossain and Bobda, Christophe},
  doi          = {10.1007/s11554-020-00960-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {157-174},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Bio-inspired smart vision sensor: Toward a reconfigurable hardware modeling of the hierarchical processing in the brain},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel hashing-based matching for real-time aerial image
mosaicing. <em>JRTIP</em>, <em>18</em>(1), 143–156. (<a
href="https://doi.org/10.1007/s11554-020-00959-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a GPU-based real-time approach for generating high-definition (HD) aerial image mosaics. The cumbersome process of registering HD images is addressed by a parallel scheme that rapidly matches binary features. The proposed feature matcher takes advantage of the fast ORB (oriented FAST and rotated BRIEF) descriptor and its attainable arrangement into hash tables. By exploiting the best functionalities of binary descriptors and hashing-based data structures, the process of creating HD mosaics is accelerated. On average, real-time performance of 14.5 ms is achieved in a frame-to-frame process, for input images of 2.7 K resolution (2704 × 1521). For evaluation purposes in terms of robustness and speed, we selected two image registration methods for comparison. The first method uses the feature extractor and matcher modules of the well-known ORB-SLAM. The second comparison is carried out against the standard KNN-based matcher of OpenCV. The experiments were conducted under different conditions and scenarios, and the proposed approach exhibits a speed-up of 10.5 times compared to ORB-SLAM-based approach and 36.5 times compared to the OpenCV matcher. Therefore, this research widens the range of applications for aerial mosaicing, since the proposed system is capable of creating high-detail panoramas of large sites while acquiring data.},
  archive      = {J_JRTIP},
  author       = {de Lima, Roberto and Cabrera-Ponce, Aldrich A. and Martinez-Carranza, Jose},
  doi          = {10.1007/s11554-020-00959-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {143-156},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel hashing-based matching for real-time aerial image mosaicing},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time filtering on parallel SIMD architectures for
automated quality inspection. <em>JRTIP</em>, <em>18</em>(1), 127–141.
(<a href="https://doi.org/10.1007/s11554-020-00954-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface metrology in automated quality inspection is a field, among many others, affected by noise and thus requiring filtering. In surface metrology, filtering is required to remove undesired information from data in order to extract surface features and relevant properties necessary for quality control. Moreover, filtering requires immediate results, while the product is being manufactured. This way, quick correcting actions can be directly applied to solve possible manufacturing issues. This work proposes different strategies to filter height maps in real-time acquired using laser profilers, the most widely used inspection method in industrial applications. Different models to apply the filtering operations are considered, particularly assessing different alternatives to store previous samples in memory, which are required for data filtering. FIFO, double FIFO, circular and double circular buffers are evaluated. Furthermore, CPU parallelism, SIMD instructions and cache-line friendly data structures are analyzed. The proposed methods are extremely efficient, capable of filtering laser profiles at extremely high acquisition rates. The proposed methods are designed for real-time surface metrology, but they are very likely to find potential applications in different areas. The filters are compared in terms of accuracy and speed, including other well-known filters such as the spline filter. Tests analyze execution time, including cache efficiency and filtering accuracy. Results with synthetic data and real data obtained from steel strips show excellent performance, providing accurate results at very high speeds.},
  archive      = {J_JRTIP},
  author       = {Usamentiaga, R.},
  doi          = {10.1007/s11554-020-00954-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {127-141},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time filtering on parallel SIMD architectures for automated quality inspection},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPGA-accelerated adaptive projection-based image
registration. <em>JRTIP</em>, <em>18</em>(1), 113–125. (<a
href="https://doi.org/10.1007/s11554-020-00952-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a high-speed hardware-based image registration is proposed exploiting parallelism and adaptive sampling technique to fulfill the requirement of high-speed portable multimedia devices. This technique computes radial and angular projections in parallel way without converting the image into polar domain, but by adjusting the number of samples along angular direction according to radius length which decreases computational load. Further, a complete image registration algorithm without using any geometric transformation is proposed. The software-based implementation of the proposed algorithm is 1.33 times faster than its latest available method in the literature. The proposed algorithm is mapped in field-programmable gate array (FPGA, Virtex6-xc6vlx760-2-ff1760) and it utilizes $$2.03\%$$ Slice LUTs, $$35.14\%$$ LUT-FF pair and $$1.27\%$$ DSP48E1s; and maximum clock frequency is 266 Mz. The hardware-based implementation of the proposed algorithm is $$10^4$$ times faster than software counterpart.},
  archive      = {J_JRTIP},
  author       = {Mondal, Pulak and Banerjee, Swapna},
  doi          = {10.1007/s11554-020-00952-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {113-125},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-accelerated adaptive projection-based image registration},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time maximally stable homogeneous regions.
<em>JRTIP</em>, <em>18</em>(1), 99–112. (<a
href="https://doi.org/10.1007/s11554-020-00951-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present the concept of maximally stable homogeneous regions (MSHR). MSHR are conceptually very similar to maximally stable extremal regions but can be segmented in images with an arbitrary number of channels. The computation of the presented MSHR relies on the construction of a quasi-flat zone hierarchy. We present a fast algorithm for computing the hierarchy that overcomes the runtime restrictions of existing approaches. The proposed algorithm can construct the quasi-flat zone hierarchy efficiently in real time, scales linearly in the number of pixels and, in practice, sub-linearly in the number of channels. In the experiments, we display how MSHR can be used to improve the results of optical character recognition systems and to perform 3D object segmentation. We further demonstrate the universality and speed of the proposed algorithm for three example applications: image segmentation, object tracking, and image filtering.},
  archive      = {J_JRTIP},
  author       = {Böttger, Tobias},
  doi          = {10.1007/s11554-020-00951-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {99-112},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time maximally stable homogeneous regions},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast image encryption algorithm with high security level
using the bülban chaotic map. <em>JRTIP</em>, <em>18</em>(1), 85–98. (<a
href="https://doi.org/10.1007/s11554-020-00948-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, a big number of image encryption schemes have been proposed. Most of these schemes reach a high-security level, however, their slow speeds due to their complex process make them unusable in real-time applications. Motivated by this, we propose a new efficient and high-speed image encryption scheme based on the Bülban chaotic map. Unlike most of the existing schemes, we make a wisely use of this simple chaotic map to generate only a few numbers of random rows and columns. Moreover, to further increase the speed, we raise the processing unit from the pixel level to the row/column level. Security of the new scheme is achieved through a substitution-permutation network, where we apply a circular shift of rows and columns to break the strong correlation of adjacent pixels. Then, we combine the XOR operation with the Modulo function to mask the pixels values and prevent any leak of information. High-security tests and simulation analysis have been carried out to demonstrate that the scheme is extremely secure and highly fast for real-time image processing at 80 fps (frames per second).},
  archive      = {J_JRTIP},
  author       = {Talhaoui, Mohamed Zakariya and Wang, Xingyuan and Midoun, Mohamed Amine},
  doi          = {10.1007/s11554-020-00948-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {85-98},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast image encryption algorithm with high security level using the bülban chaotic map},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU fast restoration of non-uniform illumination images.
<em>JRTIP</em>, <em>18</em>(1), 75–83. (<a
href="https://doi.org/10.1007/s11554-020-00950-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a GPU based parallel implementation for the non-uniform illumination image restoration method, which uses a retinex based algorithm to decompose the original image into brightness and reflectance components, and adjusts the brightness value through an adaptive gamma correction and nonparametric mapping to achieve the restoration. Specifically, we parallelize the improved retinex algorithm on GPU to extract the brightness value of each pixel. After that, the probability of different brightness range is counted through each block to the entire image to reduce the competition of memory access. Finally, we use two different parallel reduce methods to calculate the probability density and cumulative density of brightness value and generate the mapping curve. The experiment conducted on three different GPUs and two CPUs with different resolution images shows that our method can process a 1024 × 2048 image in 1.024 ms on RTX2080Ti, indicates a great potential for real-time application.},
  archive      = {J_JRTIP},
  author       = {Cheng, Kuanhong and Yu, Yue and Zhou, Huixin and Zhao, Dong and Qian, Kun},
  doi          = {10.1007/s11554-020-00950-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {75-83},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU fast restoration of non-uniform illumination images},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU acceleration of NL-means, BM3D and VBM3D.
<em>JRTIP</em>, <em>18</em>(1), 57–74. (<a
href="https://doi.org/10.1007/s11554-020-00945-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising is an essential part of any image- or video-processing pipeline. Unfortunately, due to time-processing constraints, many pipelines do not consider the use of modern denoisers. These algorithms have only CPU implementations or suboptimal GPU implementations. We propose a new efficient GPU implementation of NL-means and BM3D, and, to our knowledge, the first GPU implementation of the video-denoising algorithm VBM3D. The performance of these implementations enable their use in real-time scenarios.},
  archive      = {J_JRTIP},
  author       = {Davy, Axel and Ehret, Thibaud},
  doi          = {10.1007/s11554-020-00945-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {57-74},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU acceleration of NL-means, BM3D and VBM3D},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flooding region growing: A new parallel image segmentation
model based on membrane computing. <em>JRTIP</em>, <em>18</em>(1),
37–55. (<a href="https://doi.org/10.1007/s11554-020-00949-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region-growing (RG) algorithm is one of the most common image segmentation methods used for different image processing and machine vision applications. However, this algorithm has two main problems: (1) high computational complexity and the difficulty of its parallel implementation caused by sequential process of adding pixels to regions; (2) low performance of RG in region with weak edges, due to the use of location and the number of seed points. In this paper, a new model of RG algorithm based on tissue-like P system is proposed to resolve these limitations. In this model, each pixel is modeled by a membrane, and in one step, the similarity of each membrane with its neighbors is computed. Then, all membranes are used as seed points to grow simultaneously in a parallel and flood-like manner. To realize the parallel implementation of the proposed model, Graphic Processing Unit (GPU) and CUDA programming language are used. The evaluation of execution time indicates that the proposed model has better performance than the conventional RG algorithm, its speed-up is about 12.5×. Qualitative and quantitative evaluations of segmentation performance also demonstrate that the proposed method not only does not damage the overall segmentation accuracy, but also it has better results on images with complicated background compared to the state-of-the-art methods.},
  archive      = {J_JRTIP},
  author       = {Dalvand, Mehran and Fathi, Abdolhossein and Kamran, Arezoo},
  doi          = {10.1007/s11554-020-00949-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {37-55},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Flooding region growing: A new parallel image segmentation model based on membrane computing},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast RT-LoG operator for scene text detection.
<em>JRTIP</em>, <em>18</em>(1), 19–36. (<a
href="https://doi.org/10.1007/s11554-020-00942-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new real-time Laplacian of Gaussian (RT-LoG) operator for scene text detection. This method takes advantage of the Gaussian kernel distribution in the spatial/scale-space domains and kernel decomposition with the box filtering method. Two levels of optimization are given. The first level of optimization within the spatial domain is obtained by box mutualization. The second level of optimization within the spatial/scale-space domains is performed using a mixed method for box selection. The proposed RT-LoG operator is evaluated on the ICDAR2017 RRC-MLT dataset in terms of robustness and time processing. The results are compared with the state-of-the-art real-time operators for scene text detection. The proposed operator appears as the top performance with the best trade-off between robustness and time processing. The proposed operator can support approximately 30 frames per second (FPS) up to the Quad-HD resolution on a regular CPU architecture with a low-level latency. In addition, the proposed operator can support the full pipeline for scene text detection. Our system is competitive with the top accurate systems of the literature while processing with a difference of two orders of magnitude in term of processing resources.},
  archive      = {J_JRTIP},
  author       = {Nguyen Dinh, Cong and Delalandre, Mathieu and Conte, Donatello and Pham, The Anh},
  doi          = {10.1007/s11554-020-00942-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {19-36},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast RT-LoG operator for scene text detection},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A secure image encryption scheme based on a novel 2D
sine–cosine cross-chaotic (SC3) map. <em>JRTIP</em>, <em>18</em>(1),
1–18. (<a href="https://doi.org/10.1007/s11554-019-00940-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new 2D sine–cosine cross-chaotic (SC3) map to design an image encryption scheme with high confusion and diffusion capability. We evaluate the maximum Lyapunov exponent (MLE) of the proposed SC3 map to measure its degree of sensitivity to initial conditions and perform bifurcation analysis to find the chaotic region. The proposed chaotic map generates two pseudo-random sequence $$R_1$$ and $$R_2$$ , which are used in confusion (permutation) and diffusion phase, respectively. The confusion layer is designed by shuffling the image pixels, and the diffusion layer is designed by bitwise XOR operation. The strength of the proposed image encryption scheme is evaluated against resistance to the statistical attack (information entropy, correlation coefficient, and histogram analysis), differential attack (NPCR and UACI), and sensitivity to the secret key. The experimental results of both security and performance analysis show that the proposed image encryption scheme is secure enough to resist all the existing cryptanalytic attack and efficient in terms of encryption time.},
  archive      = {J_JRTIP},
  author       = {Mondal, Bhaskar and Behera, Pratap Kumar and Gangopadhyay, Sugata},
  doi          = {10.1007/s11554-019-00940-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A secure image encryption scheme based on a novel 2D sine–cosine cross-chaotic (SC3) map},
  volume       = {18},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
