<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---90">COAP - 90</h2>
<ul>
<li><details>
<summary>
(2021). Nonsmooth exact penalization second-order methods for
incompressible bi-viscous fluids. <em>COAP</em>, <em>80</em>(3),
979–1025. (<a href="https://doi.org/10.1007/s10589-021-00314-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the exact penalization of the incompressibility condition $$\text {div}(\mathbf {u})=0$$ for the velocity field of a bi-viscous fluid in terms of the $$L^1$$ –norm. This penalization procedure results in a nonsmooth optimization problem for which we propose an algorithm using generalized second-order information. Our method solves the resulting nonsmooth problem by considering the steepest descent direction and extra generalized second-order information associated to the nonsmooth term. This method has the advantage that the divergence-free property is enforced by the descent direction proposed by the method without the need of build-in divergence-free approximation schemes. The inexact penalization approach, given by the $$L^2$$ -norm, is also considered in our discussion and comparison.},
  archive      = {J_COAP},
  author       = {González-Andrade, Sergio and López-Ordóñez, Sofía and Merino, Pedro},
  doi          = {10.1007/s10589-021-00314-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {979-1025},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Nonsmooth exact penalization second-order methods for incompressible bi-viscous fluids},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating convergence of a globalized sequential
quadratic programming method to critical lagrange multipliers.
<em>COAP</em>, <em>80</em>(3), 943–978. (<a
href="https://doi.org/10.1007/s10589-021-00317-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the issue of asymptotic acceptance of the true Hessian and the full step by the sequential quadratic programming algorithm for equality-constrained optimization problems. In order to enforce global convergence, the algorithm is equipped with a standard Armijo linesearch procedure for a nonsmooth exact penalty function. The specificity of considerations here is that the standard assumptions for local superlinear convergence of the method may be violated. The analysis focuses on the case when there exist critical Lagrange multipliers, and does not require regularity assumptions on the constraints or satisfaction of second-order sufficient optimality conditions. The results provide a basis for application of known acceleration techniques, such as extrapolation, and allow the formulation of algorithms that can outperform the standard SQP with BFGS approximations of the Hessian on problems with degenerate constraints. This claim is confirmed by some numerical experiments.},
  archive      = {J_COAP},
  author       = {Izmailov, A. F.},
  doi          = {10.1007/s10589-021-00317-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {943-978},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerating convergence of a globalized sequential quadratic programming method to critical lagrange multipliers},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). <span class="math display">B</span> -subdifferentials of the
projection onto the matrix simplex. <em>COAP</em>, <em>80</em>(3),
915–941. (<a href="https://doi.org/10.1007/s10589-021-00316-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important tool in matrix optimization problems is the strong semismoothness of the projection mapping onto the cone of real symmetric positive semidefinite matrices, and the explicit formula for its $${\text {B}}$$ (ouligand)-subdifferentials. In this paper, we examine the corresponding results for the so-called matrix simplex, that is, the set of real symmetric positive semidefinite matrices whose traces are equal to one. This result complements the current literature and enlarges the toolbox of matrix spectral operators whose $${\text {B}}$$ -subdifferentials are explicitly formulated. Since the matrix simplex frequently arises in subproblems for solving matrix optimization problems, the derived results can potentially serve as a useful tool for efficiently solving these problems. As an illustration, we present a numerical example to demonstrate that the proposed approach can outperform the existing approaches which used projection mapping onto positive semidefinite matrix cone directly.},
  archive      = {J_COAP},
  author       = {Hu, Shenglong and Li, Guoyin},
  doi          = {10.1007/s10589-021-00316-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {915-941},
  shortjournal = {Comput. Optim. Appl.},
  title        = {$${\text {B}}$$ -subdifferentials of the projection onto the matrix simplex},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). T-product factorization method for internet traffic data
completion with spatio-temporal regularization. <em>COAP</em>,
<em>80</em>(3), 883–913. (<a
href="https://doi.org/10.1007/s10589-021-00315-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovery of network traffic data from incomplete observed data is an important issue in internet engineering and management. In this paper, by fully combining the temporal stability and periodicity features in internet traffic data, a new separable optimization model for internet data recovery is proposed, which is based upon the T-product factorization and the rapid discrete Fourier transform of tensors. The separable structural features presented in the model provide the possibility to design more efficient parallel algorithms. Moreover, by using generalized inverse matrices, an easy-to-operate and effective algorithm is proposed. In theory, we prove that under suitable conditions, every accumulation point of the sequence generated by the proposed algorithm is a stationary point of the established model. Numerical simulation results carried on the widely used real-world internet network datasets, show that the proposed method outperforms state-of-the-art competitions.},
  archive      = {J_COAP},
  author       = {Ling, Chen and Yu, Gaohang and Qi, Liqun and Xu, Yanwei},
  doi          = {10.1007/s10589-021-00315-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {883-913},
  shortjournal = {Comput. Optim. Appl.},
  title        = {T-product factorization method for internet traffic data completion with spatio-temporal regularization},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal portfolio selections via <span
class="math display"><em>ℓ</em><sub>1, 2</sub></span> -norm
regularization. <em>COAP</em>, <em>80</em>(3), 853–881. (<a
href="https://doi.org/10.1007/s10589-021-00312-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been much research about regularizing optimal portfolio selections through $$\ell _1$$ norm and/or $$\ell _2$$ -norm squared. The common consensuses are (i) $$\ell _1$$ leads to sparse portfolios and there exists a theoretical bound that limits extreme shorting of assets; (ii) $$\ell _2$$ (norm-squared) stabilizes the computation by improving the condition number of the problem resulting in strong out-of-sample performance; and (iii) there exist efficient numerical algorithms for those regularized portfolios with closed-form solutions each step. When combined such as in the well-known elastic net regularization, theoretical bounds are difficult to derive so as to limit extreme shorting of assets. In this paper, we propose a minimum variance portfolio with the regularization of $$\ell _1$$ and $$\ell _2$$ norm combined (namely $$\ell _{1, 2}$$ -norm). The new regularization enjoys the best of the two regularizations of $$\ell _1$$ norm and $$\ell _2$$ -norm squared. In particular, we derive a theoretical bound that limits short-sells and develop a closed-form formula for the proximal term of the $$\ell _{1,2}$$ norm. A fast proximal augmented Lagrange method is applied to solve the $$\ell _{1,2}$$ -norm regularized problem. Extensive numerical experiments confirm that the new model often results in high Sharpe ratio, low turnover and small amount of short sells when compared with several existing models on six datasets.},
  archive      = {J_COAP},
  author       = {Zhao, Hongxin and Kong, Lingchen and Qi, Hou-Duo},
  doi          = {10.1007/s10589-021-00312-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {853-881},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimal portfolio selections via $$\ell _{1, 2}$$ -norm regularization},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parallel splitting ALM-based algorithm for separable
convex programming. <em>COAP</em>, <em>80</em>(3), 831–851. (<a
href="https://doi.org/10.1007/s10589-021-00321-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The augmented Lagrangian method (ALM) provides a benchmark for solving the canonical convex optimization problem with linear constraints. The direct extension of ALM for solving the multiple-block separable convex minimization problem, however, is proved to be not necessarily convergent in the literature. It has thus inspired a number of ALM-variant algorithms with provable convergence. This paper presents a novel parallel splitting method for the multiple-block separable convex optimization problem with linear equality constraints, which enjoys a larger step size compared with the existing parallel splitting methods. We first show that a fully Jacobian decomposition of the regularized ALM can contribute a descent direction yielding the contraction of proximity to the solution set; then, the new iterate is generated via a simple correction step with an ignorable computational cost. We establish the convergence analysis for the proposed method, and then demonstrate its numerical efficiency by solving an application problem arising in statistical learning.},
  archive      = {J_COAP},
  author       = {Xu, Shengjie and He, Bingsheng},
  doi          = {10.1007/s10589-021-00321-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {831-851},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A parallel splitting ALM-based algorithm for separable convex programming},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distributed algorithm for high-dimension convex
quadratically constrained quadratic programs. <em>COAP</em>,
<em>80</em>(3), 781–830. (<a
href="https://doi.org/10.1007/s10589-021-00319-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Jacobi-style distributed algorithm to solve convex, quadratically constrained quadratic programs (QCQPs), which arise from a broad range of applications. While small to medium-sized convex QCQPs can be solved efficiently by interior-point algorithms, high-dimension problems pose significant challenges to traditional algorithms that are mainly designed to be implemented on a single computing unit. The exploding volume of data (and hence, the problem size), however, may overwhelm any such units. In this paper, we propose a distributed algorithm for general, non-separable, high-dimension convex QCQPs, using a novel idea of predictor–corrector primal–dual update with an adaptive step size. The algorithm enables distributed storage of data as well as parallel, distributed computing. We establish the conditions for the proposed algorithm to converge to a global optimum, and implement our algorithm on a computer cluster with multiple nodes using message passing interface. The numerical experiments are conducted on data sets of various scales from different applications, and the results show that our algorithm exhibits favorable scalability for solving high-dimension problems.},
  archive      = {J_COAP},
  author       = {Chen, Run and Liu, Andrew L.},
  doi          = {10.1007/s10589-021-00319-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {781-830},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A distributed algorithm for high-dimension convex quadratically constrained quadratic programs},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two limited-memory optimization methods with minimum
violation of the previous secant conditions. <em>COAP</em>,
<em>80</em>(3), 755–780. (<a
href="https://doi.org/10.1007/s10589-021-00318-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited-memory variable metric methods based on the well-known Broyden-Fletcher-Goldfarb-Shanno (BFGS) update are widely used for large scale optimization. The block version of this update, derived for general objective functions in Vlček and Lukšan (Numerical Algorithms 2019), satisfies the secant conditions with all used difference vectors and for quadratic objective functions gives the best improvement of convergence in some sense, but the corresponding direction vectors are not descent directions generally. To guarantee the descent property of direction vectors and simultaneously violate the secant conditions as little as possible in some sense, two methods based on the block BFGS update are proposed. They can be advantageously used together with methods based on vector corrections for conjugacy. Here we combine two types of these corrections to satisfy the secant conditions with both the corrected and uncorrected (original) latest difference vectors. Global convergence of the proposed algorithm is established for convex and sufficiently smooth functions. Numerical experiments demonstrate the efficiency of the new methods.},
  archive      = {J_COAP},
  author       = {Vlček, Jan and Lukšan, Ladislav},
  doi          = {10.1007/s10589-021-00318-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {755-780},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Two limited-memory optimization methods with minimum violation of the previous secant conditions},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A zeroth order method for stochastic weakly convex
optimization. <em>COAP</em>, <em>80</em>(3), 731–753. (<a
href="https://doi.org/10.1007/s10589-021-00313-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider stochastic weakly convex optimization problems, however without the existence of a stochastic subgradient oracle. We present a derivative free algorithm that uses a two point approximation for computing a gradient estimate of the smoothed function. We prove convergence at a similar rate as state of the art methods, however with a larger constant, and report some numerical results showing the effectiveness of the approach.},
  archive      = {J_COAP},
  author       = {Kungurtsev, V. and Rinaldi, F.},
  doi          = {10.1007/s10589-021-00313-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {731-753},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A zeroth order method for stochastic weakly convex optimization},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A data-driven approach for a class of stochastic dynamic
optimization problems. <em>COAP</em>, <em>80</em>(3), 687–729. (<a
href="https://doi.org/10.1007/s10589-021-00320-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic stochastic optimization models provide a powerful tool to represent sequential decision-making processes. Typically, these models use statistical predictive methods to capture the structure of the underlying stochastic process without taking into consideration estimation errors and model misspecification. In this context, we propose a data-driven prescriptive analytics framework aiming to integrate the machine learning and dynamic optimization machinery in a consistent and efficient way to build a bridge from data to decisions. The proposed framework tackles a relevant class of dynamic decision problems comprising many important practical applications. The basic building blocks of our proposed framework are: (1) a Hidden Markov Model as a predictive (machine learning) method to represent uncertainty; and (2) a distributionally robust dynamic optimization model as a prescriptive method that takes into account estimation errors associated with the predictive model and allows for control of the risk associated with decisions. Moreover, we present an evaluation framework to assess out-of-sample performance in rolling horizon schemes. A complete case study on dynamic asset allocation illustrates the proposed framework showing superior out-of-sample performance against selected benchmarks. The numerical results show the practical importance and applicability of the proposed framework since it extracts valuable information from data to obtain robustified decisions with an empirical certificate of out-of-sample performance evaluation.},
  archive      = {J_COAP},
  author       = {Silva, Thuener and Valladão, Davi and Homem-de-Mello, Tito},
  doi          = {10.1007/s10589-021-00320-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {687-729},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A data-driven approach for a class of stochastic dynamic optimization problems},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COAP 2020 best paper prize. <em>COAP</em>, <em>80</em>(3),
681–685. (<a href="https://doi.org/10.1007/s10589-021-00327-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  doi          = {10.1007/s10589-021-00327-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {681-685},
  shortjournal = {Comput. Optim. Appl.},
  title        = {COAP 2020 best paper prize},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Globalized inexact proximal newton-type
methods for nonconvex composite functions. <em>COAP</em>,
<em>80</em>(2), 679–680. (<a
href="https://doi.org/10.1007/s10589-021-00302-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Kanzow, Christian and Lechner, Theresa},
  doi          = {10.1007/s10589-021-00302-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {679-680},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction to: Globalized inexact proximal newton-type methods for nonconvex composite functions},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A proximal gradient method for control problems with
non-smooth and non-convex control cost. <em>COAP</em>, <em>80</em>(2),
639–677. (<a href="https://doi.org/10.1007/s10589-021-00308-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the convergence of the proximal gradient method applied to control problems with non-smooth and non-convex control cost. Here, we focus on control cost functionals that promote sparsity, which includes functionals of $$L^p$$ -type for $$p\in [0,1)$$ . We prove stationarity properties of weak limit points of the method. These properties are weaker than those provided by Pontryagin’s maximum principle and weaker than L-stationarity.},
  archive      = {J_COAP},
  author       = {Natemeyer, Carolin and Wachsmuth, Daniel},
  doi          = {10.1007/s10589-021-00308-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {639-677},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A proximal gradient method for control problems with non-smooth and non-convex control cost},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The selection of the optimal parameter in the modulus-based
matrix splitting algorithm for linear complementarity problems.
<em>COAP</em>, <em>80</em>(2), 617–638. (<a
href="https://doi.org/10.1007/s10589-021-00309-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modulus-based matrix splitting (MMS) algorithm is effective to solve linear complementarity problems (Bai in Numer Linear Algebra Appl 17: 917–933, 2010). This algorithm is parameter dependent, and previous studies mainly focus on giving the convergence interval of the iteration parameter. Yet the specific selection approach of the optimal parameter has not been systematically studied due to the nonlinearity of the algorithm. In this work, we first propose a novel and simple strategy for obtaining the optimal parameter of the MMS algorithm by merely solving two quadratic equations in each iteration. Further, we figure out the interval of optimal parameter which is iteration independent and give a practical choice of optimal parameter to avoid iteration-based computations. Compared with the experimental optimal parameter, the numerical results from three problems, including the Signorini problem of the Laplacian, show the feasibility, effectiveness and efficiency of the proposed strategy.},
  archive      = {J_COAP},
  author       = {Li, Zhizhi and Zhang, Huai and Ou-Yang, Le},
  doi          = {10.1007/s10589-021-00309-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {617-638},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The selection of the optimal parameter in the modulus-based matrix splitting algorithm for linear complementarity problems},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Newton-type methods near critical solutions of piecewise
smooth nonlinear equations. <em>COAP</em>, <em>80</em>(2), 587–615. (<a
href="https://doi.org/10.1007/s10589-021-00306-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-recognized that in the presence of singular (and in particular nonisolated) solutions of unconstrained or constrained smooth nonlinear equations, the existence of critical solutions has a crucial impact on the behavior of various Newton-type methods. On the one hand, it has been demonstrated that such solutions turn out to be attractors for sequences generated by these methods, for wide domains of starting points, and with a linear convergence rate estimate. On the other hand, the pattern of convergence to such solutions is quite special, and allows for a sharp characterization which serves, in particular, as a basis for some known acceleration techniques, and for the proof of an asymptotic acceptance of the unit stepsize. The latter is an essential property for the success of these techniques when combined with a linesearch strategy for globalization of convergence. This paper aims at extensions of these results to piecewise smooth equations, with applications to corresponding reformulations of nonlinear complementarity problems.},
  archive      = {J_COAP},
  author       = {Fischer, A. and Izmailov, A. F. and Jelitte, M.},
  doi          = {10.1007/s10589-021-00306-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {587-615},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Newton-type methods near critical solutions of piecewise smooth nonlinear equations},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strengthened splitting methods for computing resolvents.
<em>COAP</em>, <em>80</em>(2), 549–585. (<a
href="https://doi.org/10.1007/s10589-021-00291-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop a systematic framework for computing the resolvent of the sum of two or more monotone operators which only activates each operator in the sum individually. The key tool in the development of this framework is the notion of the “strengthening” of a set-valued operator, which can be viewed as a type of regularisation that preserves computational tractability. After deriving a number of iterative schemes through this framework, we demonstrate their application to best approximation problems, image denoising and elliptic PDEs.},
  archive      = {J_COAP},
  author       = {Aragón Artacho, Francisco J. and Campoy, Rubén and Tam, Matthew K.},
  doi          = {10.1007/s10589-021-00291-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {549-585},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Strengthened splitting methods for computing resolvents},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two methods for the maximization of homogeneous polynomials
over the simplex. <em>COAP</em>, <em>80</em>(2), 523–548. (<a
href="https://doi.org/10.1007/s10589-021-00307-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the numerical solution of the problem P to maximize a homogeneous polynomial over the unit simplex. We discuss the convergence properties of the so-called replicator dynamics for solving P. We further examine an ascent method, which also makes use of the replicator transformation. Numerical experiments with polynomials of different degrees illustrate the theoretical convergence results.},
  archive      = {J_COAP},
  author       = {Ahmed, Faizan and Still, Georg},
  doi          = {10.1007/s10589-021-00307-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {523-548},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Two methods for the maximization of homogeneous polynomials over the simplex},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting term sparsity in noncommutative polynomial
optimization. <em>COAP</em>, <em>80</em>(2), 483–521. (<a
href="https://doi.org/10.1007/s10589-021-00301-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a new hierarchy of semidefinite programming relaxations, called NCTSSOS, to solve large-scale sparse noncommutative polynomial optimization problems. This hierarchy features the exploitation of term sparsity hidden in the input data for eigenvalue and trace optimization problems. NCTSSOS complements the recent work that exploits correlative sparsity for noncommutative optimization problems by Klep et al. (MP, 2021), and is the noncommutative analogue of the TSSOS framework by Wang et al. (SIAMJO 31: 114–141, 2021, SIAMJO 31: 30–58, 2021). We also propose an extension exploiting simultaneously correlative and term sparsity, as done previously in the commutative case (Wang in CS-TSSOS: Correlative and term sparsity for large-scale polynomial optimization, 2020). Under certain conditions, we prove that the optima of the NCTSSOS hierarchy converge to the optimum of the corresponding dense semidefinite programming relaxation. We illustrate the efficiency and scalability of NCTSSOS by solving eigenvalue/trace optimization problems from the literature as well as randomly generated examples involving up to several thousand variables.},
  archive      = {J_COAP},
  author       = {Wang, Jie and Magron, Victor},
  doi          = {10.1007/s10589-021-00301-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {483-521},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Exploiting term sparsity in noncommutative polynomial optimization},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On solving a class of fractional semi-infinite polynomial
programming problems. <em>COAP</em>, <em>80</em>(2), 439–481. (<a
href="https://doi.org/10.1007/s10589-021-00311-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a class of fractional semi-infinite polynomial programming (FSIPP) problems, in which the objective is a fraction of a convex polynomial and a concave polynomial, and the constraints consist of infinitely many convex polynomial inequalities. To solve such a problem, we first reformulate it to a pair of primal and dual conic optimization problems, which reduce to semidefinite programming (SDP) problems if we can bring sum-of-squares structures into the conic constraints. To this end, we provide a characteristic cone constraint qualification for convex semi-infinite programming problems to guarantee strong duality and also the attainment of the solution in the dual problem, which is of its own interest. In this framework, we first present a hierarchy of SDP relaxations with asymptotic convergence for the FSIPP problem whose index set is defined by finitely many polynomial inequalities. Next, we study four cases of the FSIPP problems which can be reduced to either a single SDP problem or a finite sequence of SDP problems, where at least one minimizer can be extracted. Then, we apply this approach to the four corresponding multi-objective cases to find efficient solutions.},
  archive      = {J_COAP},
  author       = {Guo, Feng and Jiao, Liguo},
  doi          = {10.1007/s10589-021-00311-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {439-481},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On solving a class of fractional semi-infinite polynomial programming problems},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An augmented subgradient method for minimizing nonsmooth DC
functions. <em>COAP</em>, <em>80</em>(2), 411–438. (<a
href="https://doi.org/10.1007/s10589-021-00304-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method, called an augmented subgradient method, is developed to solve unconstrained nonsmooth difference of convex (DC) optimization problems. At each iteration of this method search directions are found by using several subgradients of the first DC component and one subgradient of the second DC component of the objective function. The developed method applies an Armijo-type line search procedure to find the next iteration point. It is proved that the sequence of points generated by the method converges to a critical point of the unconstrained DC optimization problem. The performance of the method is demonstrated using academic test problems with nonsmooth DC objective functions and its performance is compared with that of two general nonsmooth optimization solvers and five solvers specifically designed for unconstrained DC optimization. Computational results show that the developed method is efficient and robust for solving nonsmooth DC optimization problems.},
  archive      = {J_COAP},
  author       = {Bagirov, A. M. and Hoseini Monjezi, N. and Taheri, S.},
  doi          = {10.1007/s10589-021-00304-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {411-438},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An augmented subgradient method for minimizing nonsmooth DC functions},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two new bidirectional search algorithms. <em>COAP</em>,
<em>80</em>(2), 377–409. (<a
href="https://doi.org/10.1007/s10589-021-00303-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two new bidirectional heuristic search algorithms for solving the shortest path problem on graphs: consistent-heuristic bucket-based bidirectional search (CBBS) and front-to-front GPU bidirectional search (FFGBS). CBBS uses a consistent heuristic and groups nodes into buckets that organize nodes based on estimated path cost and known heuristic errors. FFGBS splits the work between the CPU and GPU, with the GPU solving a front-to-front heuristic and the CPU choosing nodes to expand. This paper also includes a new front-to-front version of the GAP heuristic for the pancake problem that is efficient to solve on a GPU. Computational experiments for CBBS are performed on the pancake problem. CBBS is faster and requires less node expansions with the GAP-1 heuristic, compared to bidirectional state of the algorithms like DIBBS and DVCBS. Computational experiments for FFGBS are performed on the pancake problem and DIMACS road network, showing that FFGBS is consistently the fastest algorithm on all but the smallest pancake stacks when using the GAP-2 heuristic and is also the fastest algorithm on the largest road networks.},
  archive      = {J_COAP},
  author       = {Pavlik, John A. and Sewell, Edward C. and Jacobson, Sheldon H.},
  doi          = {10.1007/s10589-021-00303-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {377-409},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Two new bidirectional search algorithms},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MADAM: A parallel exact solver for max-cut based on
semidefinite programming and ADMM. <em>COAP</em>, <em>80</em>(2),
347–375. (<a href="https://doi.org/10.1007/s10589-021-00310-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MADAM, a parallel semidefinite-based exact solver for Max-Cut, a problem of finding the cut with the maximum weight in a given graph. The algorithm uses the branch and bound paradigm that applies the alternating direction method of multipliers as the bounding routine to solve the basic semidefinite relaxation strengthened by a subset of hypermetric inequalities. The benefit of the new approach is a less computationally expensive update rule for the dual variable with respect to the inequality constraints. We provide a theoretical convergence of the algorithm as well as extensive computational experiments with this method, to show that our algorithm outperforms state-of-the-art approaches. Furthermore, by combining algorithmic ingredients from the serial algorithm, we develop an efficient distributed parallel solver based on MPI.},
  archive      = {J_COAP},
  author       = {Hrga, Timotej and Povh, Janez},
  doi          = {10.1007/s10589-021-00310-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {347-375},
  shortjournal = {Comput. Optim. Appl.},
  title        = {MADAM: A parallel exact solver for max-cut based on semidefinite programming and ADMM},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Forward-reflected-backward method with variance reduction.
<em>COAP</em>, <em>80</em>(2), 321–346. (<a
href="https://doi.org/10.1007/s10589-021-00305-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a variance reduced algorithm for solving monotone variational inequalities. Without assuming strong monotonicity, cocoercivity, or boundedness of the domain, we prove almost sure convergence of the iterates generated by the algorithm to a solution. In the monotone case, the ergodic average converges with the optimal O(1/k) rate of convergence. When strong monotonicity is assumed, the algorithm converges linearly, without requiring the knowledge of strong monotonicity constant. We finalize with extensions and applications of our results to monotone inclusions, a class of non-monotone variational inequalities and Bregman projections.},
  archive      = {J_COAP},
  author       = {Alacaoglu, Ahmet and Malitsky, Yura and Cevher, Volkan},
  doi          = {10.1007/s10589-021-00305-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {321-346},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Forward-reflected-backward method with variance reduction},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A third-order weighted essentially non-oscillatory scheme in
optimal control problems governed by nonlinear hyperbolic conservation
laws. <em>COAP</em>, <em>80</em>(1), 301–320. (<a
href="https://doi.org/10.1007/s10589-021-00295-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weighted essentially non-oscillatory (WENO) methods are popular and effective spatial discretization methods for nonlinear hyperbolic partial differential equations. Although these methods are formally first-order accurate when a shock is present, they still have uniform high-order accuracy right up to the shock location. In this paper, we propose a novel third-order numerical method for solving optimal control problems subject to scalar nonlinear hyperbolic conservation laws. It is based on the first-disretize-then-optimize approach and combines a discrete adjoint WENO scheme of third order with the classical strong stability preserving three-stage third-order Runge–Kutta method SSPRK3. We analyze its approximation properties and apply it to optimal control problems of tracking-type with non-smooth target states. Comparisons to common first-order methods such as the Lax–Friedrichs and Engquist–Osher method show its great potential to achieve a higher accuracy along with good resolution around discontinuities.},
  archive      = {J_COAP},
  author       = {Frenzel, David and Lang, Jens},
  doi          = {10.1007/s10589-021-00295-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {301-320},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A third-order weighted essentially non-oscillatory scheme in optimal control problems governed by nonlinear hyperbolic conservation laws},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse dirichlet optimal control problems. <em>COAP</em>,
<em>80</em>(1), 271–300. (<a
href="https://doi.org/10.1007/s10589-021-00290-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyze optimal control problems governed by an elliptic partial differential equation, in which the control acts as the Dirichlet data. Box constraints for the controls are imposed and the cost functional involves the state and possibly a sparsity-promoting term, but not a Tikhonov regularization term. Two different discretizations are investigated: the variational approach and a full discrete approach. For the latter, we use continuous piecewise linear elements to discretize the control space and numerical integration of the sparsity-promoting term. It turns out that the best way to discretize the state equation is to use the Carstensen quasi-interpolant of the boundary data, and a new discrete normal derivative of the adjoint state must be introduced to deal with this. Error estimates, optimization procedures and examples are provided.},
  archive      = {J_COAP},
  author       = {Mateos, Mariano},
  doi          = {10.1007/s10589-021-00290-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {271-300},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Sparse dirichlet optimal control problems},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Alternating conditional gradient method for convex
feasibility problems. <em>COAP</em>, <em>80</em>(1), 245–269. (<a
href="https://doi.org/10.1007/s10589-021-00293-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical convex feasibility problem in a finite dimensional Euclidean space consists of finding a point in the intersection of two convex sets. In the present paper we are interested in two particular instances of this problem. First, we assume to know how to compute an exact projection onto one of the sets involved and the other set is compact such that the conditional gradient (CondG) method can be used for computing efficiently an inexact projection on it. Second, we assume that both sets involved are compact such that the CondG method can be used for computing efficiently inexact projections on them. We combine alternating projection method with CondG method to design a new method, which can be seen as an inexact feasible version of alternate projection method. The proposed method generates two different sequences belonging to each involved set, which converge to a point in the intersection of them whenever it is not empty. If the intersection is empty, then the sequences converge to points in the respective sets whose distance between them is equal to the distance between the sets in consideration. Numerical experiments are provided to illustrate the practical behavior of the method.},
  archive      = {J_COAP},
  author       = {Díaz Millán, R. and Ferreira, O. P. and Prudente, L. F.},
  doi          = {10.1007/s10589-021-00293-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {245-269},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Alternating conditional gradient method for convex feasibility problems},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quadratic convergence analysis of a nonmonotone
levenberg–marquardt type method for the weighted nonlinear
complementarity problem. <em>COAP</em>, <em>80</em>(1), 213–244. (<a
href="https://doi.org/10.1007/s10589-021-00300-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the weighted nonlinear complementarity problem (denoted by wNCP) which contains a wide class of optimization problems. We introduce a family of new weighted complementarity functions and show that it is continuously differentiable everywhere and has several favorable properties. Based on this function, we reformulate the wNCP as a smooth nonlinear equation and propose a nonmonotone Levenberg–Marquardt type method to solve it. We show that the proposed method is well-defined and it is globally convergent without any additional condition. Moreover, we prove that the whole iteration sequence converges to a solution of the wNCP locally superlinearly or quadratically under the nonsingularity condition. In addition, we establish the local quadratic convergence of the proposed method under the local error bound condition. Some numerical results are also reported.},
  archive      = {J_COAP},
  author       = {Tang, Jingyong and Zhou, Jinchuan},
  doi          = {10.1007/s10589-021-00300-8},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {213-244},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quadratic convergence analysis of a nonmonotone Levenberg–Marquardt type method for the weighted nonlinear complementarity problem},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential optimality conditions for cardinality-constrained
optimization problems with applications. <em>COAP</em>, <em>80</em>(1),
185–211. (<a href="https://doi.org/10.1007/s10589-021-00298-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a new approach to tackle cardinality-constrained optimization problems based on a continuous reformulation of the problem was proposed. Following this approach, we derive a problem-tailored sequential optimality condition, which is satisfied at every local minimizer without requiring any constraint qualification. We relate this condition to an existing M-type stationary concept by introducing a weak sequential constraint qualification based on a cone-continuity property. Finally, we present two algorithmic applications: We improve existing results for a known regularization method by proving that it generates limit points satisfying the aforementioned optimality conditions even if the subproblems are only solved inexactly. And we show that, under a suitable Kurdyka–Łojasiewicz-type assumption, any limit point of a standard (safeguarded) multiplier penalty method applied directly to the reformulated problem also satisfies the optimality condition. These results are stronger than corresponding ones known for the related class of mathematical programs with complementarity constraints.},
  archive      = {J_COAP},
  author       = {Kanzow, Christian and Raharja, Andreas B. and Schwartz, Alexandra},
  doi          = {10.1007/s10589-021-00298-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {185-211},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Sequential optimality conditions for cardinality-constrained optimization problems with applications},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic mathematical programs with probabilistic
complementarity constraints: SAA and distributionally robust approaches.
<em>COAP</em>, <em>80</em>(1), 153–184. (<a
href="https://doi.org/10.1007/s10589-021-00292-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a class of stochastic mathematical programs with probabilistic complementarity constraints is considered. We first investigate convergence properties of sample average approximation (SAA) approach to the corresponding chance constrained relaxed complementarity problem. Our discussion can be not only applied to the specific model in this paper, but also viewed as a supplementary for the SAA approach to general joint chance constrained problems. Furthermore, considering the uncertainty of the underlying probability distribution, a distributionally robust counterpart with a moment ambiguity set is proposed. The numerically tractable reformulation is derived. Finally, we use a production planing model to report some preliminary numerical results.},
  archive      = {J_COAP},
  author       = {Peng, Shen and Jiang, Jie},
  doi          = {10.1007/s10589-021-00292-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {153-184},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic mathematical programs with probabilistic complementarity constraints: SAA and distributionally robust approaches},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minibatch stochastic subgradient-based projection algorithms
for feasibility problems with convex inequalities. <em>COAP</em>,
<em>80</em>(1), 121–152. (<a
href="https://doi.org/10.1007/s10589-021-00294-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider convex feasibility problems where the feasible set is given as the intersection of a collection of closed convex sets. We assume that each set is specified algebraically as a convex inequality, where the associated convex function is general (possibly non-differentiable). For finding a point satisfying all the convex inequalities we design and analyze random projection algorithms using special subgradient iterations and extrapolated stepsizes. Moreover, the iterate updates are performed based on parallel random observations of several constraint components. For these minibatch stochastic subgradient-based projection methods we prove sublinear convergence results and, under some linear regularity condition for the functional constraints, we prove linear convergence rates. We also derive sufficient conditions under which these rates depend explicitly on the minibatch size. To the best of our knowledge, this work is the first deriving conditions that show theoretically when minibatch stochastic subgradient-based projection updates have a better complexity than their single-sample variants when parallel computing is used to implement the minibatch. Numerical results also show a better performance of our minibatch scheme over its non-minibatch counterpart.},
  archive      = {J_COAP},
  author       = {Necoara, Ion and Nedić, Angelia},
  doi          = {10.1007/s10589-021-00294-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {121-152},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Minibatch stochastic subgradient-based projection algorithms for feasibility problems with convex inequalities},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient global algorithm for worst-case linear
optimization under uncertainties based on nonlinear semidefinite
relaxation. <em>COAP</em>, <em>80</em>(1), 89–120. (<a
href="https://doi.org/10.1007/s10589-021-00289-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The worst-case linear optimization (WCLO) with uncertainties in the right-hand-side of the constraints often arises from numerous applications such as systemic risk estimate in finance and stochastic optimization, which is known to be NP-hard. In this paper, we investigate the efficient global algorithm for WCLO based on its nonlinear semidefinite relaxation (SDR). We first derive an enhanced nonlinear SDR for WCLO via secant cuts and RLT approaches. A secant search algorithm is then proposed to solve the nonlinear SDR and its global convergence is established. Second, we propose a new global algorithm for WCLO, which integrates the nonlinear SDR with successive convex optimization method, initialization and branch-and-bound, to find a globally optimal solution to the underlying WCLO within a pre-specified $$\epsilon$$ -tolerance. We establish the global convergence of the algorithm and estimate its complexity. Preliminary numerical results demonstrate that the proposed algorithm can effectively find a globally optimal solution to the WCLO instances.},
  archive      = {J_COAP},
  author       = {Ding, Xiaodong and Luo, Hezhi and Wu, Huixian and Liu, Jianzhen},
  doi          = {10.1007/s10589-021-00289-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {89-120},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An efficient global algorithm for worst-case linear optimization under uncertainties based on nonlinear semidefinite relaxation},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compact representations of structured BFGS matrices.
<em>COAP</em>, <em>80</em>(1), 55–88. (<a
href="https://doi.org/10.1007/s10589-021-00297-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For general large-scale optimization problems compact representations exist in which recursive quasi-Newton update formulas are represented as compact matrix factorizations. For problems in which the objective function contains additional structure, recent structured quasi-Newton methods exploit available second-derivative information and approximate unavailable second derivatives. This article develops the compact representations of two structured Broyden-Fletcher-Goldfarb-Shanno update formulas. The compact representations enable efficient limited memory and initialization strategies. Two limited memory line search algorithms are described for which extensive numerical results demonstrate the efficacy of the algorithms, including comparisons to IPOPT on large machine learning problems, and to L-BFGS on a real world large scale ptychographic imaging application.},
  archive      = {J_COAP},
  author       = {Brust, Johannes J. and Di, Zichao (Wendy) and Leyffer, Sven and Petra, Cosmin G.},
  doi          = {10.1007/s10589-021-00297-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {55-88},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Compact representations of structured BFGS matrices},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parameter-free unconstrained reformulation for nonsmooth
problems with convex constraints. <em>COAP</em>, <em>80</em>(1), 33–53.
(<a href="https://doi.org/10.1007/s10589-021-00296-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper we propose to rewrite a nonsmooth problem subjected to convex constraints as an unconstrained problem. We show that this novel formulation shares the same global and local minima with the original constrained problem. Moreover, the reformulation can be solved with standard nonsmooth optimization methods if we are able to make projections onto the feasible sets. Numerical evidence shows that the proposed formulation compares favorably against state-of-art approaches. Code can be found at https://github.com/jth3galv/dfppm .},
  archive      = {J_COAP},
  author       = {Galvan, Giulio and Sciandrone, Marco and Lucidi, Stefano},
  doi          = {10.1007/s10589-021-00296-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {33-53},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A parameter-free unconstrained reformulation for nonsmooth problems with convex constraints},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective procedure for feature subset selection in
logistic regression based on information criteria. <em>COAP</em>,
<em>80</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s10589-021-00288-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the problem of best subset selection in logistic regression is addressed. In particular, we take into account formulations of the problem resulting from the adoption of information criteria, such as AIC or BIC, as goodness-of-fit measures. There exist various methods to tackle this problem. Heuristic methods are computationally cheap, but are usually only able to find low quality solutions. Methods based on local optimization suffer from similar limitations as heuristic ones. On the other hand, methods based on mixed integer reformulations of the problem are much more effective, at the cost of higher computational requirements, that become unsustainable when the problem size grows. We thus propose a new approach, which combines mixed-integer programming and decomposition techniques in order to overcome the aforementioned scalability issues. We provide a theoretical characterization of the proposed algorithm properties. The results of a vast numerical experiment, performed on widely available datasets, show that the proposed method achieves the goal of outperforming state-of-the-art techniques.},
  archive      = {J_COAP},
  author       = {Civitelli, Enrico and Lapucci, Matteo and Schoen, Fabio and Sortino, Alessio},
  doi          = {10.1007/s10589-021-00288-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An effective procedure for feature subset selection in logistic regression based on information criteria},
  volume       = {80},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Exact linesearch limited-memory quasi-newton methods for
minimizing a quadratic function. <em>COAP</em>, <em>79</em>(3), 789–816.
(<a href="https://doi.org/10.1007/s10589-021-00277-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main focus in this paper is exact linesearch methods for minimizing a quadratic function whose Hessian is positive definite. We give a class of limited-memory quasi-Newton Hessian approximations which generate search directions parallel to those of the BFGS method, or equivalently, to those of the method of preconditioned conjugate gradients. In the setting of reduced Hessians, the class provides a dynamical framework for the construction of limited-memory quasi-Newton methods. These methods attain finite termination on quadratic optimization problems in exact arithmetic. We show performance of the methods within this framework in finite precision arithmetic by numerical simulations on sequences of related systems of linear equations, which originate from the CUTEst test collection. In addition, we give a compact representation of the Hessian approximations in the full Broyden class for the general unconstrained optimization problem. This representation consists of explicit matrices and gradients only as vector components.},
  archive      = {J_COAP},
  author       = {Ek, David and Forsgren, Anders},
  doi          = {10.1007/s10589-021-00277-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {789-816},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Exact linesearch limited-memory quasi-newton methods for minimizing a quadratic function},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual simplex-type algorithm for the smallest enclosing
ball of balls. <em>COAP</em>, <em>79</em>(3), 767–787. (<a
href="https://doi.org/10.1007/s10589-021-00283-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a dual simplex-type algorithm for computing the smallest enclosing ball of a set of balls and other closely related problems. Our algorithm employs a pivoting scheme resembling the simplex method for linear programming, in which a sequence of exact curve searches is performed until a new dual feasible solution with a strictly smaller objective function value is found. We utilize the Cholesky factorization and procedures for updating it, yielding a numerically stable implementation of the algorithm. We show that our algorithm can efficiently solve instances of dimension 5000 with 100000 points, often within minutes.},
  archive      = {J_COAP},
  author       = {Cavaleiro, Marta and Alizadeh, Farid},
  doi          = {10.1007/s10589-021-00283-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {767-787},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A dual simplex-type algorithm for the smallest enclosing ball of balls},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fastest rates for stochastic mirror descent methods.
<em>COAP</em>, <em>79</em>(3), 717–766. (<a
href="https://doi.org/10.1007/s10589-021-00284-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relative smoothness—a notion introduced in Birnbaum et al. (Proceedings of the 12th ACM conference on electronic commerce, ACM, pp 127–136, 2011) and recently rediscovered in Bauschke et al. (Math Oper Res 330–348, 2016) and Lu et al. (Relatively-smooth convex optimization by first-order methods, and applications, arXiv:1610.05708 , 2016)—generalizes the standard notion of smoothness typically used in the analysis of gradient type methods. In this work we are taking ideas from well studied field of stochastic convex optimization and using them in order to obtain faster algorithms for minimizing relatively smooth functions. We propose and analyze two new algorithms: Relative Randomized Coordinate Descent (relRCD) and Relative Stochastic Gradient Descent (relSGD), both generalizing famous algorithms in the standard smooth setting. The methods we propose can be in fact seen as particular instances of stochastic mirror descent algorithms, which has been usually analyzed under stronger assumptions: Lipschitzness of the objective and strong convexity of the reference function. As a consequence, one of the proposed methods, relRCD corresponds to the first stochastic variant of mirror descent algorithm with linear convergence rate.},
  archive      = {J_COAP},
  author       = {Hanzely, Filip and Richtárik, Peter},
  doi          = {10.1007/s10589-021-00284-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {717-766},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Fastest rates for stochastic mirror descent methods},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-block bregman proximal alternating linearized
minimization and its application to orthogonal nonnegative matrix
factorization. <em>COAP</em>, <em>79</em>(3), 681–715. (<a
href="https://doi.org/10.1007/s10589-021-00286-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and analyze BPALM and A-BPALM, two multi-block proximal alternating linearized minimization algorithms using Bregman distances for solving structured nonconvex problems. The objective function is the sum of a multi-block relatively smooth function (i.e., relatively smooth by fixing all the blocks except one) and block separable (nonsmooth) nonconvex functions. The sequences generated by our algorithms are subsequentially convergent to critical points of the objective function, while they are globally convergent under the KL inequality assumption. Moreover, the rate of convergence is further analyzed for functions satisfying the Łojasiewicz’s gradient inequality. We apply this framework to orthogonal nonnegative matrix factorization (ONMF) that satisfies all of our assumptions and the related subproblems are solved in closed forms, where some preliminary numerical results are reported.},
  archive      = {J_COAP},
  author       = {Ahookhosh, Masoud and Hien, Le Thi Khanh and Gillis, Nicolas and Patrinos, Panagiotis},
  doi          = {10.1007/s10589-021-00286-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {681-715},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Multi-block bregman proximal alternating linearized minimization and its application to orthogonal nonnegative matrix factorization},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A FISTA-type accelerated gradient algorithm for solving
smooth nonconvex composite optimization problems. <em>COAP</em>,
<em>79</em>(3), 649–679. (<a
href="https://doi.org/10.1007/s10589-021-00280-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe and establish iteration-complexity of two accelerated composite gradient (ACG) variants to solve a smooth nonconvex composite optimization problem whose objective function is the sum of a nonconvex differentiable function f with a Lipschitz continuous gradient and a simple nonsmooth closed convex function h. When f is convex, the first ACG variant reduces to the well-known FISTA for a specific choice of the input, and hence the first one can be viewed as a natural extension of the latter one to the nonconvex setting. The first variant requires an input pair (M, m) such that f is m-weakly convex, $$\nabla f$$ is M-Lipschitz continuous, and $$m \le M$$ (possibly $$m&lt;M$$ ), which is usually hard to obtain or poorly estimated. The second variant on the other hand can start from an arbitrary input pair (M, m) of positive scalars and its complexity is shown to be not worse, and better in some cases, than that of the first variant for a large range of the input pairs. Finally, numerical results are provided to illustrate the efficiency of the two ACG variants.},
  archive      = {J_COAP},
  author       = {Liang, Jiaming and Monteiro, Renato D. C. and Sim, Chee-Khian},
  doi          = {10.1007/s10589-021-00280-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {649-679},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A FISTA-type accelerated gradient algorithm for solving smooth nonconvex composite optimization problems},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the use of jordan algebras for improving global
convergence of an augmented lagrangian method in nonlinear semidefinite
programming. <em>COAP</em>, <em>79</em>(3), 633–648. (<a
href="https://doi.org/10.1007/s10589-021-00281-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jordan Algebras are an important tool for dealing with semidefinite programming and optimization over symmetric cones in general. In this paper, a judicious use of Jordan Algebras in the context of sequential optimality conditions is done in order to generalize the global convergence theory of an Augmented Lagrangian method for nonlinear semidefinite programming. An approximate complementarity measure in this context is typically defined in terms of the eigenvalues of the constraint matrix and the eigenvalues of an approximate Lagrange multiplier. By exploiting the Jordan Algebra structure of the problem, we show that a simpler complementarity measure, defined in terms of the Jordan product, is stronger than the one defined in terms of eigenvalues. Thus, besides avoiding a tricky analysis of eigenvalues, a stronger necessary optimality condition is presented. We then prove the global convergence of an Augmented Lagrangian algorithm to this improved necessary optimality condition. The results are also extended to an interior point method. The optimality conditions we present are sequential ones, and no constraint qualification is employed; in particular, a global convergence result is available even when Lagrange multipliers are unbounded.},
  archive      = {J_COAP},
  author       = {Andreani, R. and Fukuda, E. H. and Haeser, G. and Santos, D. O. and Secchin, L. D.},
  doi          = {10.1007/s10589-021-00281-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {633-648},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the use of jordan algebras for improving global convergence of an augmented lagrangian method in nonlinear semidefinite programming},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An effective logarithmic formulation for piecewise
linearization requiring no inequality constraint. <em>COAP</em>,
<em>79</em>(3), 601–631. (<a
href="https://doi.org/10.1007/s10589-021-00285-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the commonly used techniques for tackling the nonconvex optimization problems in which all the nonlinear terms are univariate is the piecewise linear approximation by which the nonlinear terms are reformulated. The performance of the linearization technique primarily depends on the quantities of variables and constraints required in the formulation of a piecewise linear function. The state-of-the-art linearization method introduces $$2\lceil \log _2 m\rceil$$ inequality constraints, where m is the number of line segments in the constructed piecewise linear function. This study proposes an effective alternative logarithmic scheme by which no inequality constraint is incurred. The price that more continuous variables are needed in the proposed scheme than in the state-of-the-art method is less than offset by the simultaneous inclusion of a system of equality constraints satisfying the canonical form and the absence of any inequality constraint. Our numerical experiments demonstrate that the developed scheme has the computational superiority, the degree of which increases with m.},
  archive      = {J_COAP},
  author       = {Hwang, F. J. and Huang, Yao-Huei},
  doi          = {10.1007/s10589-021-00285-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {601-631},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An effective logarithmic formulation for piecewise linearization requiring no inequality constraint},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing mixed strategies equilibria in presence of
switching costs by the solution of nonconvex QP problems. <em>COAP</em>,
<em>79</em>(3), 561–599. (<a
href="https://doi.org/10.1007/s10589-021-00282-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address game theory problems arising in the context of network security. In traditional game theory problems, given a defender and an attacker, one searches for mixed strategies which minimize a linear payoff functional. In the problems addressed in this paper an additional quadratic term is added to the minimization problem. Such term represents switching costs, i.e., the costs for the defender of switching from a given strategy to another one at successive rounds of a Nash game. The resulting problems are nonconvex QP ones with linear constraints and turn out to be very challenging. We will show that the most recent approaches for the minimization of nonconvex QP functions over polytopes, including commercial solvers such as CPLEX and GUROBI, are unable to solve to optimality even test instances with $$n=50$$ variables. For this reason, we propose to extend with them the current benchmark set of test instances for QP problems. We also present a spatial branch-and-bound approach for the solution of these problems, where a predominant role is played by an optimality-based domain reduction, with multiple solutions of LP problems at each node of the branch-and-bound tree. Of course, domain reductions are standard tools in spatial branch-and-bound approaches. However, our contribution lies in the observation that, from the computational point of view, a rather aggressive application of these tools appears to be the best way to tackle the proposed instances. Indeed, according to our experiments, while they make the computational cost per node high, this is largely compensated by the rather slow growth of the number of nodes in the branch-and-bound tree, so that the proposed approach strongly outperforms the existing solvers for QP problems.},
  archive      = {J_COAP},
  author       = {Liuzzi, G. and Locatelli, M. and Piccialli, V. and Rass, S.},
  doi          = {10.1007/s10589-021-00282-7},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {561-599},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Computing mixed strategies equilibria in presence of switching costs by the solution of nonconvex QP problems},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subspace quadratic regularization method for group sparse
multinomial logistic regression. <em>COAP</em>, <em>79</em>(3), 531–559.
(<a href="https://doi.org/10.1007/s10589-021-00287-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse multinomial logistic regression has recently received widespread attention. It provides a useful tool for solving multi-classification problems in various fields, such as signal and image processing, machine learning and disease diagnosis. In this paper, we first study the group sparse multinomial logistic regression model and establish its optimality conditions. Based on the theoretical results of this model, we hence propose an efficient algorithm called the subspace quadratic regularization algorithm to compute a stationary point of a given problem. This algorithm enjoys excellent convergence properties, including the global convergence and locally quadratic convergence. Finally, our numerical results on standard benchmark data clearly demonstrate the superior performance of our proposed algorithm in terms of logistic loss value, sparsity recovery and computational time.},
  archive      = {J_COAP},
  author       = {Wang, Rui and Xiu, Naihua and Toh, Kim-Chuan},
  doi          = {10.1007/s10589-021-00287-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {531-559},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Subspace quadratic regularization method for group sparse multinomial logistic regression},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The circumcentered-reflection method achieves better rates
than alternating projections. <em>COAP</em>, <em>79</em>(2), 507–530.
(<a href="https://doi.org/10.1007/s10589-021-00275-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the convergence rate of the Circumcentered-Reflection Method (CRM) for solving the convex feasibility problem and compare it with the Method of Alternating Projections (MAP). Under an error bound assumption, we prove that both methods converge linearly, with asymptotic constants depending on a parameter of the error bound, and that the one derived for CRM is strictly better than the one for MAP. Next, we analyze two classes of fairly generic examples. In the first one, the angle between the convex sets approaches zero near the intersection, so that the MAP sequence converges sublinearly, but CRM still enjoys linear convergence. In the second class of examples, the angle between the sets does not vanish and MAP exhibits its standard behavior, i.e., it converges linearly, yet, perhaps surprisingly, CRM attains superlinear convergence.},
  archive      = {J_COAP},
  author       = {Arefidamghani, Reza and Behling, Roger and Bello-Cruz, Yunier and Iusem, Alfredo N. and Santos, Luiz-Rafael},
  doi          = {10.1007/s10589-021-00275-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {507-530},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The circumcentered-reflection method achieves better rates than alternating projections},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An accelerated first-order method with complexity analysis
for solving cubic regularization subproblems. <em>COAP</em>,
<em>79</em>(2), 471–506. (<a
href="https://doi.org/10.1007/s10589-021-00274-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a first-order method to solve the cubic regularization subproblem (CRS) based on a novel reformulation. The reformulation is a constrained convex optimization problem whose feasible region admits an easily computable projection. Our reformulation requires computing the minimum eigenvalue of the Hessian. To avoid the expensive computation of the exact minimum eigenvalue, we develop a surrogate problem to the reformulation where the exact minimum eigenvalue is replaced with an approximate one. We then apply first-order methods such as the Nesterov’s accelerated projected gradient method (APG) and projected Barzilai-Borwein method to solve the surrogate problem. As our main theoretical contribution, we show that when an $$\epsilon$$ -approximate minimum eigenvalue is computed by the Lanczos method and the surrogate problem is approximately solved by APG, our approach returns an $$\epsilon$$ -approximate solution to CRS in $${\tilde{O}}(\epsilon ^{-1/2})$$ matrix-vector multiplications (where $${\tilde{O}}(\cdot )$$ hides the logarithmic factors). Numerical experiments show that our methods are comparable to and outperform the Krylov subspace method in the easy and hard cases, respectively. We further implement our methods as subproblem solvers of adaptive cubic regularization methods, and numerical results show that our algorithms are comparable to the state-of-the-art algorithms.},
  archive      = {J_COAP},
  author       = {Jiang, Rujun and Yue, Man-Chung and Zhou, Zhishuo},
  doi          = {10.1007/s10589-021-00274-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {471-506},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An accelerated first-order method with complexity analysis for solving cubic regularization subproblems},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A laplacian approach to <span
class="math display"><em>ℓ</em><sub>1</sub></span> -norm minimization.
<em>COAP</em>, <em>79</em>(2), 441–469. (<a
href="https://doi.org/10.1007/s10589-021-00270-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel differentiable reformulation of the linearly-constrained $$\ell _1$$ minimization problem, also known as the basis pursuit problem. The reformulation is inspired by the Laplacian paradigm of network theory and leads to a new family of gradient-based methods for the solution of $$\ell _1$$ minimization problems. We analyze the iteration complexity of a natural solution approach to the reformulation, based on a multiplicative weights update scheme, as well as the iteration complexity of an accelerated gradient scheme. The results can be seen as bounds on the complexity of iteratively reweighted least squares (IRLS) type methods of basis pursuit.},
  archive      = {J_COAP},
  author       = {Bonifaci, Vincenzo},
  doi          = {10.1007/s10589-021-00270-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {441-469},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A laplacian approach to $$\ell _1$$ -norm minimization},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerated bregman proximal gradient methods for relatively
smooth convex optimization. <em>COAP</em>, <em>79</em>(2), 405–440. (<a
href="https://doi.org/10.1007/s10589-021-00273-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of minimizing the sum of two convex functions: one is differentiable and relatively smooth with respect to a reference convex function, and the other can be nondifferentiable but simple to optimize. We investigate a triangle scaling property of the Bregman distance generated by the reference convex function and present accelerated Bregman proximal gradient (ABPG) methods that attain an $$O(k^{-\gamma })$$ convergence rate, where $$\gamma \in (0,2]$$ is the triangle scaling exponent (TSE) of the Bregman distance. For the Euclidean distance, we have $$\gamma =2$$ and recover the convergence rate of Nesterov’s accelerated gradient methods. For non-Euclidean Bregman distances, the TSE can be much smaller (say $$\gamma \le 1$$ ), but we show that a relaxed definition of intrinsic TSE is always equal to 2. We exploit the intrinsic TSE to develop adaptive ABPG methods that converge much faster in practice. Although theoretical guarantees on a fast convergence rate seem to be out of reach in general, our methods obtain empirical $$O(k^{-2})$$ rates in numerical experiments on several applications and provide posterior numerical certificates for the fast rates.},
  archive      = {J_COAP},
  author       = {Hanzely, Filip and Richtárik, Peter and Xiao, Lin},
  doi          = {10.1007/s10589-021-00273-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {405-440},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerated bregman proximal gradient methods for relatively smooth convex optimization},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and safe: Accelerated gradient methods with optimality
certificates and underestimate sequences. <em>COAP</em>, <em>79</em>(2),
369–404. (<a href="https://doi.org/10.1007/s10589-021-00269-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we introduce the concept of an Underestimate Sequence (UES), which is motivated by Nesterov’s estimate sequence. Our definition of a UES utilizes three sequences, one of which is a lower bound (or under-estimator) of the objective function. The question of how to construct an appropriate sequence of lower bounds is addressed, and we present lower bounds for strongly convex smooth functions and for strongly convex composite functions, which adhere to the UES framework. Further, we propose several first order methods for minimizing strongly convex functions in both the smooth and composite cases. The algorithms, based on efficiently updating lower bounds on the objective functions, have natural stopping conditions that provide the user with a certificate of optimality. Convergence of all algorithms is guaranteed through the UES framework, and we show that all presented algorithms converge linearly, with the accelerated variants enjoying the optimal linear rate of convergence.},
  archive      = {J_COAP},
  author       = {Jahani, Majid and Gudapati, Naga Venkata C. and Ma, Chenxin and Tappenden, Rachael and Takáč, Martin},
  doi          = {10.1007/s10589-021-00269-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {369-404},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Fast and safe: Accelerated gradient methods with optimality certificates and underestimate sequences},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stochastic subspace approach to gradient-free optimization
in high dimensions. <em>COAP</em>, <em>79</em>(2), 339–368. (<a
href="https://doi.org/10.1007/s10589-021-00271-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a stochastic descent algorithm for unconstrained optimization that is particularly efficient when the objective function is slow to evaluate and gradients are not easily obtained, as in some PDE-constrained optimization and machine learning problems. The algorithm maps the gradient onto a low-dimensional random subspace of dimension $$\ell$$ at each iteration, similar to coordinate descent but without restricting directional derivatives to be along the axes. Without requiring a full gradient, this mapping can be performed by computing $$\ell$$ directional derivatives (e.g., via forward-mode automatic differentiation). We give proofs for convergence in expectation under various convexity assumptions as well as probabilistic convergence results under strong-convexity. Our method provides a novel extension to the well-known Gaussian smoothing technique to descent in subspaces of dimension greater than one, opening the doors to new analysis of Gaussian smoothing when more than one directional derivative is used at each iteration. We also provide a finite-dimensional variant of a special case of the Johnson–Lindenstrauss lemma. Experimentally, we show that our method compares favorably to coordinate descent, Gaussian smoothing, gradient descent and BFGS (when gradients are calculated via forward-mode automatic differentiation) on problems from the machine learning and shape optimization literature.},
  archive      = {J_COAP},
  author       = {Kozak, David and Becker, Stephen and Doostan, Alireza and Tenorio, Luis},
  doi          = {10.1007/s10589-021-00271-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {339-368},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A stochastic subspace approach to gradient-free optimization in high dimensions},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DMulti-MADS: Mesh adaptive direct multisearch for
bound-constrained blackbox multiobjective optimization. <em>COAP</em>,
<em>79</em>(2), 301–338. (<a
href="https://doi.org/10.1007/s10589-021-00272-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The context of this research is multiobjective optimization where conflicting objectives are present. In this work, these objectives are only available as the outputs of a blackbox for which no derivative information is available. This work proposes a new extension of the mesh adaptive direct search (MADS) algorithm to multiobjective derivative-free optimization with bound constraints. This method does not aggregate objectives and keeps a list of non dominated points which converges to a (local) Pareto set as long as the algorithm unfolds. As in the single-objective optimization MADS algorithm, this method is built around a search step and a poll step. Under classical direct search assumptions, it is proved that the so-called DMulti-MADS algorithm generates multiple subsequences of iterates which converge to a set of local Pareto stationary points. Finally, computational experiments suggest that this approach is competitive compared to the state-of-the-art algorithms for multiobjective blackbox optimization.},
  archive      = {J_COAP},
  author       = {Bigeon, Jean and Le Digabel, Sébastien and Salomon, Ludovic},
  doi          = {10.1007/s10589-021-00272-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {301-338},
  shortjournal = {Comput. Optim. Appl.},
  title        = {DMulti-MADS: Mesh adaptive direct multisearch for bound-constrained blackbox multiobjective optimization},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank factorization for rank minimization with nonconvex
regularizers. <em>COAP</em>, <em>79</em>(2), 273–300. (<a
href="https://doi.org/10.1007/s10589-021-00276-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rank minimization is of interest in machine learning applications such as recommender systems and robust principal component analysis. Minimizing the convex relaxation to the rank minimization problem, the nuclear norm, is an effective technique to solve the problem with strong performance guarantees. However, nonconvex relaxations have less estimation bias than the nuclear norm and can more accurately reduce the effect of noise on the measurements. We develop efficient algorithms based on iteratively reweighted nuclear norm schemes, while also utilizing the low rank factorization for semidefinite programs put forth by Burer and Monteiro. We prove convergence and computationally show the advantages over convex relaxations and alternating minimization methods. Additionally, the computational complexity of each iteration of our algorithm is on par with other state of the art algorithms, allowing us to quickly find solutions to the rank minimization problem for large matrices.},
  archive      = {J_COAP},
  author       = {Sagan, April and Mitchell, John E.},
  doi          = {10.1007/s10589-021-00276-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {273-300},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Low-rank factorization for rank minimization with nonconvex regularizers},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matrix optimization based euclidean embedding with outliers.
<em>COAP</em>, <em>79</em>(2), 235–271. (<a
href="https://doi.org/10.1007/s10589-021-00279-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Euclidean embedding from noisy observations containing outlier errors is an important and challenging problem in statistics and machine learning. Many existing methods would struggle with outliers due to a lack of detection ability. In this paper, we propose a matrix optimization based embedding model that can produce reliable embeddings and identify the outliers jointly. We show that the estimators obtained by the proposed method satisfy a non-asymptotic risk bound, implying that the model provides a high accuracy estimator with high probability when the order of the sample size is roughly the degree of freedom up to a logarithmic factor. Moreover, we show that under some mild conditions, the proposed model also can identify the outliers without any prior information with high probability. Finally, numerical experiments demonstrate that the matrix optimization-based model can produce configurations of high quality and successfully identify outliers even for large networks.},
  archive      = {J_COAP},
  author       = {Zhang, Qian and Zhao, Xinyuan and Ding, Chao},
  doi          = {10.1007/s10589-021-00279-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {235-271},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Matrix optimization based euclidean embedding with outliers},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MINLP formulations for continuous piecewise linear function
fitting. <em>COAP</em>, <em>79</em>(1), 223–233. (<a
href="https://doi.org/10.1007/s10589-021-00268-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a nonconvex mixed-integer nonlinear programming (MINLP) model proposed by Goldberg et al. (Comput Optim Appl 58:523–541, 2014. https://doi.org/10.1007/s10589-014-9647-y ) for piecewise linear function fitting. We show that this MINLP model is incomplete and can result in a piecewise linear curve that is not the graph of a function, because it misses a set of necessary constraints. We provide two counterexamples to illustrate this effect, and propose three alternative models that correct this behavior. We investigate the theoretical relationship between these models and evaluate their computational performance.},
  archive      = {J_COAP},
  author       = {Goldberg, Noam and Rebennack, Steffen and Kim, Youngdae and Krasko, Vitaliy and Leyffer, Sven},
  doi          = {10.1007/s10589-021-00268-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {223-233},
  shortjournal = {Comput. Optim. Appl.},
  title        = {MINLP formulations for continuous piecewise linear function fitting},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence of sum-up rounding schemes for cloaking problems
governed by the helmholtz equation. <em>COAP</em>, <em>79</em>(1),
193–221. (<a href="https://doi.org/10.1007/s10589-020-00262-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of designing a cloak for waves described by the Helmholtz equation from an integer programming point of view. The problem can be modeled as a PDE-constrained optimization problem with integer-valued control inputs that are distributed in the computational domain. A first-discretize-then-optimize approach results in a large-scale mixed-integer nonlinear program that is in general intractable because of the large number of integer variables that arise from the discretization of the domain. Instead, we propose an efficient algorithm that is able to approximate the local infima of the underlying nonconvex infinite-dimensional problem arbitrarily close without the need to solve the discretized finite-dimensional integer programs to optimality. We optimize only the continuous relaxations of the approximations for local minima and then apply the sum-up rounding methodology to obtain integer-valued controls. If the solutions of the discretized continuous relaxations converge to a local minimizer of the continuous relaxation, then the resulting discrete-valued control sequence converges weakly $$^*$$ in $$L^\infty$$ to the same local minimizer. These approximation properties follow under suitable refinements of the involved discretization grids. Our results use familiar concepts arising from the analytical properties of the underlying PDE and complement previous results, derived from a topology optimization point of view.},
  archive      = {J_COAP},
  author       = {Leyffer, Sven and Manns, Paul and Winckler, Malte},
  doi          = {10.1007/s10589-020-00262-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {193-221},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of sum-up rounding schemes for cloaking problems governed by the helmholtz equation},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Approximate solution of system of equations arising in
interior-point methods for bound-constrained optimization.
<em>COAP</em>, <em>79</em>(1), 155–191. (<a
href="https://doi.org/10.1007/s10589-021-00265-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus in this paper is interior-point methods for bound-constrained nonlinear optimization, where the system of nonlinear equations that arise are solved with Newton’s method. There is a trade-off between solving Newton systems directly, which give high quality solutions, and solving many approximate Newton systems which are computationally less expensive but give lower quality solutions. We propose partial and full approximate solutions to the Newton systems. The specific approximate solution depends on estimates of the active and inactive constraints at the solution. These sets are at each iteration estimated by basic heuristics. The partial approximate solutions are computationally inexpensive, whereas a system of linear equations needs to be solved for the full approximate solution. The size of the system is determined by the estimate of the inactive constraints at the solution. In addition, we motivate and suggest two Newton-like approaches which are based on an intermediate step that consists of the partial approximate solutions. The theoretical setting is introduced and asymptotic error bounds are given. We also give numerical results to investigate the performance of the approximate solutions within and beyond the theoretical framework.},
  archive      = {J_COAP},
  author       = {Ek, David and Forsgren, Anders},
  doi          = {10.1007/s10589-021-00265-8},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {155-191},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Approximate solution of system of equations arising in interior-point methods for bound-constrained optimization},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inexact proximal memoryless quasi-newton methods based on
the broyden family for minimizing composite functions. <em>COAP</em>,
<em>79</em>(1), 127–154. (<a
href="https://doi.org/10.1007/s10589-021-00264-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers a proximal Newton-type method to solve the minimization of a composite function that is the sum of a smooth nonconvex function and a nonsmooth convex function. In general, the method uses the Hessian matrix of the smooth portion of the objective function or its approximation. The uniformly positive definiteness of the matrix plays an important role in establishing the global convergence of the method. In this study, an inexact proximal memoryless quasi-Newton method is proposed based on the memoryless Broyden family with the modified spectral scaling secant condition. The proposed method inexactly solves the subproblems to calculate scaled proximal mappings. The approximation matrix is shown to retain the uniformly positive definiteness and the search direction is a descent direction. Using these properties, the proposed method is shown to have global convergence for nonconvex objective functions. Furthermore, the R-linear convergence for strongly convex objective functions is proved. Finally, some numerical results are provided.},
  archive      = {J_COAP},
  author       = {Nakayama, Shummin and Narushima, Yasushi and Yabe, Hiroshi},
  doi          = {10.1007/s10589-021-00264-9},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {127-154},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact proximal memoryless quasi-newton methods based on the broyden family for minimizing composite functions},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantitative results on a halpern-type proximal point
algorithm. <em>COAP</em>, <em>79</em>(1), 101–125. (<a
href="https://doi.org/10.1007/s10589-021-00263-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We apply proof mining methods to analyse a result of Boikanyo and Moroşanu on the strong convergence of a Halpern-type proximal point algorithm. As a consequence, we obtain quantitative versions of this result, providing uniform effective rates of asymptotic regularity and metastability.},
  archive      = {J_COAP},
  author       = {Leuştean, Laurenţiu and Pinto, Pedro},
  doi          = {10.1007/s10589-021-00263-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {101-125},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quantitative results on a halpern-type proximal point algorithm},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A time-consistent benders decomposition method for
multistage distributionally robust stochastic optimization with a
scenario tree structure. <em>COAP</em>, <em>79</em>(1), 67–99. (<a
href="https://doi.org/10.1007/s10589-021-00266-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computational method is developed for solving time consistent distributionally robust multistage stochastic linear programs with discrete distribution. The stochastic structure of the uncertain parameters is described by a scenario tree. At each node of this tree, an ambiguity set is defined by conditional moment constraints to guarantee time consistency. This method employs the idea of nested Benders decomposition that incorporates forward and backward steps. The backward steps solve some conic programming problems to approximate the cost-to-go function at each node, while the forward steps are used to generate additional trial points. A new framework of convergence analysis is developed to establish the global convergence of the approximation procedure, which does not depend on the assumption of polyhedral structure of the original problem. Numerical results of a practical inventory model are reported to demonstrate the effectiveness of the proposed method.},
  archive      = {J_COAP},
  author       = {Yu, Haodong and Sun, Jie and Wang, Yanjun},
  doi          = {10.1007/s10589-021-00266-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {67-99},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A time-consistent benders decomposition method for multistage distributionally robust stochastic optimization with a scenario tree structure},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating set search using simplex gradients for
bound-constrained black-box optimization. <em>COAP</em>, <em>79</em>(1),
35–65. (<a href="https://doi.org/10.1007/s10589-021-00267-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization problems arising in modern engineering practice are increasingly simulation-based, characterized by extreme types of nonsmoothness, the inaccessibility of derivatives, and high computational expense. While generating set searches (GSS) generally offer a satisfying level of robustness and converge to stationary points, the convergence rates may be slow. In order to accelerate the solution process without sacrificing robustness, we introduce (simplex) gradient-informed generating set search (GIGS) methods for solving bound-constrained minimization problems. These algorithms use simplex gradients, acquired over several iterations, as guidance for adapting the search stencil to the local topography of the objective function. GIGS is shown to inherit first-order convergence properties of GSS and to possess a natural tendency for avoiding saddle points. Numerical experiments are performed on an academic set of smooth, nonsmooth and noisy test problems, as well as a realistic engineering case study. The results demonstrate that including simplex gradient information enables computational cost savings over non-adaptive GSS methods.},
  archive      = {J_COAP},
  author       = {Dedoncker, Sander and Desmet, Wim and Naets, Frank},
  doi          = {10.1007/s10589-021-00267-6},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {35-65},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Generating set search using simplex gradients for bound-constrained black-box optimization},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic mesh adaptive direct search for blackbox
optimization using probabilistic estimates. <em>COAP</em>,
<em>79</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10589-020-00249-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a stochastic extension of the mesh adaptive direct search (MADS) algorithm originally developed for deterministic blackbox optimization. The algorithm, called StoMADS, considers the unconstrained optimization of an objective function f whose values can be computed only through a blackbox corrupted by some random noise following an unknown distribution. The proposed method is based on an algorithmic framework similar to that of MADS and uses random estimates of function values obtained from stochastic observations since the exact deterministic computable version of f is not available. Such estimates are required to be accurate with a sufficiently large but fixed probability and to satisfy a variance condition. The ability of the proposed algorithm to generate an asymptotically dense set of search directions is then exploited using martingale theory to prove convergence to a Clarke stationary point of f with probability one.},
  archive      = {J_COAP},
  author       = {Audet, Charles and Dzahini, Kwassi Joseph and Kokkolaras, Michael and Le Digabel, Sébastien},
  doi          = {10.1007/s10589-020-00249-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic mesh adaptive direct search for blackbox optimization using probabilistic estimates},
  volume       = {79},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Secant update generalized version of PSB: A new approach.
<em>COAP</em>, <em>78</em>(3), 953–982. (<a
href="https://doi.org/10.1007/s10589-020-00256-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optimization, one of the main challenges of the widely used family of Quasi-Newton methods is to find an estimate of the Hessian matrix as close as possible to the real matrix. In this paper, we develop a new update formula for the estimate of the Hessian starting from the Powell-Symetric-Broyden (PSB) formula and adding pieces of information from the previous steps of the optimization path. This lead to a multisecant version of PSB, which we call generalised PSB (gPSB), but which does not exist in general as was proven before. We provide a novel interpretation of this non-existence. In addition, we provide a formula that satisfies the multisecant condition and is as close to symmetric as possible and vice versa for a second formula. Subsequently, we add enforcement of the last secant equation and present a comparison between the different methods.},
  archive      = {J_COAP},
  author       = {Boutet, Nicolas and Haelterman, Rob and Degroote, Joris},
  doi          = {10.1007/s10589-020-00256-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {953-982},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Secant update generalized version of PSB: A new approach},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the properties of the cosine measure and the uniform
angle subspace. <em>COAP</em>, <em>78</em>(3), 915–952. (<a
href="https://doi.org/10.1007/s10589-020-00253-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a nonempty finite set of nonzero vectors $$S \subset \mathbb {R}^n$$ . The angle between a nonzero vector $$v \in \mathbb {R}^n$$ and S is the smallest angle between v and an element of S. The cosine measure of S is the cosine of the largest possible angle between a nonzero vector $$v \in \mathbb {R}^n$$ and S. The cosine measure provides a way of quantifying the positive spanning property of a set of vectors, which is important in the area of derivative-free optimization. This paper proves some of the properties of the cosine measure for a nonempty finite set of nonzero vectors. It also introduces the notion of the uniform angle subspace and some cones associated with it and proves some of their properties. Moreover, this paper proves some results that characterize the Karush–Kuhn–Tucker (KKT) points for the optimization problem of calculating the cosine measure. These characterizations of the KKT points involve the uniform angle subspace and its associated cones. Finally, this paper provides an outline for calculating the cosine measure of any nonempty finite set of nonzero vectors.},
  archive      = {J_COAP},
  author       = {Regis, Rommel G.},
  doi          = {10.1007/s10589-020-00253-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {915-952},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the properties of the cosine measure and the uniform angle subspace},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polyhedral approximations of the semidefinite cone and their
application. <em>COAP</em>, <em>78</em>(3), 893–913. (<a
href="https://doi.org/10.1007/s10589-020-00255-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop techniques to construct a series of sparse polyhedral approximations of the semidefinite cone. Motivated by the semidefinite (SD) bases proposed by Tanaka and Yoshise (Ann Oper Res 265:155–182, 2018), we propose a simple expansion of SD bases so as to keep the sparsity of the matrices composing it. We prove that the polyhedral approximation using our expanded SD bases contains the set of all diagonally dominant matrices and is contained in the set of all scaled diagonally dominant matrices. We also prove that the set of all scaled diagonally dominant matrices can be expressed using an infinite number of expanded SD bases. We use our approximations as the initial approximation in cutting plane methods for solving a semidefinite relaxation of the maximum stable set problem. It is found that the proposed methods with expanded SD bases are significantly more efficient than methods using other existing approximations or solving semidefinite relaxation problems directly.},
  archive      = {J_COAP},
  author       = {Wang, Yuzhu and Tanaka, Akihiro and Yoshise, Akiko},
  doi          = {10.1007/s10589-020-00255-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {893-913},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Polyhedral approximations of the semidefinite cone and their application},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A strictly contractive peaceman-rachford splitting method
for the doubly nonnegative relaxation of the minimum cut problem.
<em>COAP</em>, <em>78</em>(3), 853–891. (<a
href="https://doi.org/10.1007/s10589-020-00261-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum cut problem, MC, and the special case of the vertex separator problem, consists in partitioning the set of nodes of a graph G into k subsets of given sizes in order to minimize the number of edges cut after removing the k-th set. Previous work on approximate solutions uses, in increasing strength and expense: eigenvalue, semidefinite programming, SDP, and doubly nonnegative, DNN, bounding techniques. In this paper, we derive strengthened SDP and DNN relaxations, and we propose a scalable algorithmic approach for efficiently evaluating, theoretically verifiable, both upper and lower bounds. Our stronger relaxations are based on a new gangster set, and we demonstrate how facial reduction, FR, fits in well to allow for regularized relaxations. Moreover, the FR appears to be perfectly well suited for a natural splitting of variables, and thus for the application of splitting methods. Here, we adopt the strictly contractive Peaceman-Rachford splitting method, sPRSM. Further, we bring useful redundant constraints back into the subproblems, and show empirically that this accelerates sPRSM.In addition, we employ new strategies for obtaining lower bounds and upper bounds of the optimal value of MC from approximate iterates of the sPRSM thus aiding in early termination of the algorithm. We compare our approach with others in the literature on random datasets and vertex separator problems. This illustrates the efficiency and robustness of our proposed method.},
  archive      = {J_COAP},
  author       = {Li, Xinxin and Pong, Ting Kei and Sun, Hao and Wolkowicz, Henry},
  doi          = {10.1007/s10589-020-00261-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {853-891},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A strictly contractive peaceman-rachford splitting method for the doubly nonnegative relaxation of the minimum cut problem},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A proximal DC approach for quadratic assignment problem.
<em>COAP</em>, <em>78</em>(3), 825–851. (<a
href="https://doi.org/10.1007/s10589-020-00252-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we show that the quadratic assignment problem (QAP) can be reformulated to an equivalent rank constrained doubly nonnegative (DNN) problem. Under the framework of the difference of convex functions (DC) approach, a semi-proximal DC algorithm is proposed for solving the relaxation of the rank constrained DNN problem whose subproblems can be solved by the semi-proximal augmented Lagrangian method. We show that the generated sequence converges to a stationary point of the corresponding DC problem, which is feasible to the rank constrained DNN problem under some suitable assumptions. Moreover, numerical experiments demonstrate that for most QAP instances, the proposed approach can find the global optimal solutions efficiently, and for others, the proposed algorithm is able to provide good feasible solutions in a reasonable time.},
  archive      = {J_COAP},
  author       = {Jiang, Zhuoxuan and Zhao, Xinyuan and Ding, Chao},
  doi          = {10.1007/s10589-020-00252-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {825-851},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A proximal DC approach for quadratic assignment problem},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gauss–newton-type methods for bilevel optimization.
<em>COAP</em>, <em>78</em>(3), 793–824. (<a
href="https://doi.org/10.1007/s10589-020-00254-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies Gauss–Newton-type methods for over-determined systems to find solutions to bilevel programming problems. To proceed, we use the lower-level value function reformulation of bilevel programs and consider necessary optimality conditions under appropriate assumptions. First, under strict complementarity for upper- and lower-level feasibility constraints, we prove the convergence of a Gauss–Newton-type method in computing points satisfying these optimality conditions under additional tractable qualification conditions. Potential approaches to address the shortcomings of the method are then proposed, leading to alternatives such as the pseudo or smoothing Gauss–Newton-type methods for bilevel optimization. Our numerical experiments conducted on 124 examples from the recently released Bilevel Optimization LIBrary (BOLIB) compare the performance of our method under different scenarios and show that it is a tractable approach to solve bilevel optimization problems with continuous variables.},
  archive      = {J_COAP},
  author       = {Fliege, Jörg and Tin, Andrey and Zemkoho, Alain},
  doi          = {10.1007/s10589-020-00254-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {793-824},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Gauss–Newton-type methods for bilevel optimization},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The tikhonov regularization for vector equilibrium problems.
<em>COAP</em>, <em>78</em>(3), 769–792. (<a
href="https://doi.org/10.1007/s10589-020-00258-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider vector equilibrium problems in real Banach spaces and study their regularized problems. Based on cone continuity and generalized convexity properties of vector-valued mappings, we propose general conditions that guarantee existence of solutions to such problems in cases of monotonicity and nonmonotonicity. First, our study indicates that every Tikhonov trajectory converges to a solution to the original problem. Then, we establish the equivalence between the problem solvability and the boundedness of any Tikhonov trajectory. Finally, the convergence of the Tikhonov trajectory to the least-norm solution of the original problem is discussed.},
  archive      = {J_COAP},
  author       = {Anh, Lam Quoc and Duy, Tran Quoc and Muu, Le Dung and Tri, Truong Van},
  doi          = {10.1007/s10589-020-00258-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {769-792},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The tikhonov regularization for vector equilibrium problems},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional gradient method for multiobjective optimization.
<em>COAP</em>, <em>78</em>(3), 741–768. (<a
href="https://doi.org/10.1007/s10589-020-00260-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the conditional gradient method, also known as Frank–Wolfe method, for constrained multiobjective optimization. The constraint set is assumed to be convex and compact, and the objectives functions are assumed to be continuously differentiable. The method is considered with different strategies for obtaining the step sizes. Asymptotic convergence properties and iteration-complexity bounds with and without convexity assumptions on the objective functions are stablished. Numerical experiments are provided to illustrate the effectiveness of the method and certify the obtained theoretical results.},
  archive      = {J_COAP},
  author       = {Assunção, P. B. and Ferreira, O. P. and Prudente, L. F.},
  doi          = {10.1007/s10589-020-00260-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {741-768},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Conditional gradient method for multiobjective optimization},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic proximal gradient methods for nonconvex problems
in hilbert spaces. <em>COAP</em>, <em>78</em>(3), 705–740. (<a
href="https://doi.org/10.1007/s10589-020-00259-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For finite-dimensional problems, stochastic approximation methods have long been used to solve stochastic optimization problems. Their application to infinite-dimensional problems is less understood, particularly for nonconvex objectives. This paper presents convergence results for the stochastic proximal gradient method applied to Hilbert spaces, motivated by optimization problems with partial differential equation (PDE) constraints with random inputs and coefficients. We study stochastic algorithms for nonconvex and nonsmooth problems, where the nonsmooth part is convex and the nonconvex part is the expectation, which is assumed to have a Lipschitz continuous gradient. The optimization variable is an element of a Hilbert space. We show almost sure convergence of strong limit points of the random sequence generated by the algorithm to stationary points. We demonstrate the stochastic proximal gradient algorithm on a tracking-type functional with a $$L^1$$ -penalty term constrained by a semilinear PDE and box constraints, where input terms and coefficients are subject to uncertainty. We verify conditions for ensuring convergence of the algorithm and show a simulation.},
  archive      = {J_COAP},
  author       = {Geiersbach, Caroline and Scarinci, Teresa},
  doi          = {10.1007/s10589-020-00259-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {705-740},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic proximal gradient methods for nonconvex problems in hilbert spaces},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decomposition algorithms for some deterministic and
two-stage stochastic single-leader multi-follower games. <em>COAP</em>,
<em>78</em>(3), 675–704. (<a
href="https://doi.org/10.1007/s10589-020-00257-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a certain class of hierarchical decision problems that can be viewed as single-leader multi-follower games, and be represented by a virtual market coordinator trying to set a price system for traded goods, according to some criterion that balances supply and demand. The objective function of the market coordinator involves the decisions of many agents, which are taken independently by solving convex optimization problems that depend on the price configuration and on realizations of future states of the economy. One traditional way of solving this problem is via a mixed complementarity formulation. However, this approach can become impractical when the numbers of agents and/or scenarios become large. This work concerns agent-wise and scenario-wise decomposition algorithms to solve the equilibrium problems in question, assuming that the solutions of the agents’ problems are unique, which is natural in many applications (when solutions are not unique, the approximating problems are still well-defined, but the convergence properties of the algorithm are not established). The algorithm is based on a previous work of the authors, where a suitable regularization of solution mappings of fully parameterized convex problems is developed. Here, we show one specific strategy to manage the regularization parameter, extend some theoretical results to the current setting, and prove that the smooth approximations of the market coordinator’s problem converge epigraphically to the original problem. Numerical experiments and some comparisons with the complementarity solver PATH are shown for the two-stage stochastic Walrasian equilibrium problem.},
  archive      = {J_COAP},
  author       = {Borges, Pedro and Sagastizábal, Claudia and Solodov, Mikhail},
  doi          = {10.1007/s10589-020-00257-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {675-704},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Decomposition algorithms for some deterministic and two-stage stochastic single-leader multi-follower games},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Theoretical and numerical comparison of the
karush–kuhn–tucker and value function reformulations in bilevel
optimization. <em>COAP</em>, <em>78</em>(2), 625–674. (<a
href="https://doi.org/10.1007/s10589-020-00250-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Karush–Kuhn–Tucker and value function (lower-level value function, to be precise) reformulations are the most common single-level transformations of the bilevel optimization problem. So far, these reformulations have either been studied independently or as a joint optimization problem in an attempt to take advantage of the best properties from each model. To the best of our knowledge, these reformulations have not yet been compared in the existing literature. This paper is a first attempt towards establishing whether one of these reformulations is best at solving a given class of the optimistic bilevel optimization problem. We design a comparison framework, which seems fair, considering the theoretical properties of these reformulations. This work reveals that although none of the models seems to particularly dominate the other from the theoretical point of view, the value function reformulation seems to numerically outperform the Karush–Kuhn–Tucker reformulation on a Newton-type algorithm. The computational experiments here are mostly based on test problems from the Bilevel Optimization LIBrary (BOLIB).},
  archive      = {J_COAP},
  author       = {Zemkoho, Alain B. and Zhou, Shenglong},
  doi          = {10.1007/s10589-020-00250-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {625-674},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Theoretical and numerical comparison of the Karush–Kuhn–Tucker and value function reformulations in bilevel optimization},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On mixed-integer optimal control with constrained total
variation of the integer control. <em>COAP</em>, <em>78</em>(2),
575–623. (<a href="https://doi.org/10.1007/s10589-020-00244-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combinatorial integral approximation (CIA) decomposition suggests solving mixed-integer optimal control problems by solving one continuous nonlinear control problem and one mixed-integer linear program (MILP). Unrealistic frequent switching can be avoided by adding a constraint on the total variation to the MILP. Within this work, we present a fast heuristic way to solve this CIA problem and investigate in which situations optimality of the constructed feasible solution is guaranteed. In the second part of this article, we show tight bounds on the integrality gap between a relaxed continuous control trajectory and an integer feasible one in the case of two controls. Finally, we present numerical experiments to highlight the proposed algorithm’s advantages in terms of run time and solution quality.},
  archive      = {J_COAP},
  author       = {Sager, Sebastian and Zeile, Clemens},
  doi          = {10.1007/s10589-020-00244-5},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {575-623},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On mixed-integer optimal control with constrained total variation of the integer control},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor z-eigenvalue complementarity problems. <em>COAP</em>,
<em>78</em>(2), 559–573. (<a
href="https://doi.org/10.1007/s10589-020-00248-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies tensor Z-eigenvalue complementarity problems. We formulate the tensor Z-eigenvalue complementarity problem as constrained polynomial optimization, and propose a semidefinite relaxation algorithm for solving the complementarity Z-eigenvalues of tensors. For every tensor that has finitely many complementarity Z-eigenvalues, we can compute all of them and show that our algorithm has the asymptotic and finite convergence. Numerical experiments indicate the efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Zeng, Meilan},
  doi          = {10.1007/s10589-020-00248-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {559-573},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Tensor Z-eigenvalue complementarity problems},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The gauss–seidel method for generalized nash equilibrium
problems of polynomials. <em>COAP</em>, <em>78</em>(2), 529–557. (<a
href="https://doi.org/10.1007/s10589-020-00242-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the generalized Nash equilibrium problem of polynomials (GNEPP). We apply the Gauss–Seidel method and Moment-SOS relaxations to solve GNEPPs. The convergence of the Gauss–Seidel method is known for some special GNEPPs, such as generalized potential games (GPGs). We give a sufficient condition for GPGs and propose a numerical certificate, based on Putinar’s Positivstellensatz. Numerical examples for both convex and nonconvex GNEPPs are given for demonstrating the efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Nie, Jiawang and Tang, Xindong and Xu, Lingling},
  doi          = {10.1007/s10589-020-00242-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {529-557},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The Gauss–Seidel method for generalized nash equilibrium problems of polynomials},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding multi-objective supported efficient spanning trees.
<em>COAP</em>, <em>78</em>(2), 491–528. (<a
href="https://doi.org/10.1007/s10589-020-00251-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new algorithm for computing the set of supported non-dominated points in the objective space and all the corresponding efficient solutions in the decision space for the multi-objective spanning tree (MOST) problem. This algorithm is based on the connectedness property of the set of efficient supported solutions and uses a decomposition of the weight set in the weighting space defined for a parametric version of the MOST problem. This decomposition is performed through a space reduction approach until an indifference region for each supported non-dominated point is obtained. An adjacency relation defined in the decision space is used to compute all the supported efficient spanning trees associated to the same non-dominated supported point as well as to define the indifference region of the next points. An in-depth computational analysis of this approach for different types of networks with three objectives is also presented.},
  archive      = {J_COAP},
  author       = {Correia, Pedro and Paquete, Luís and Figueira, José Rui},
  doi          = {10.1007/s10589-020-00251-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {491-528},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finding multi-objective supported efficient spanning trees},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bundle method for nonsmooth DC programming with
application to chance-constrained problems. <em>COAP</em>,
<em>78</em>(2), 451–490. (<a
href="https://doi.org/10.1007/s10589-020-00241-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers nonsmooth and nonconvex optimization problems whose objective and constraint functions are defined by difference-of-convex (DC) functions. We consider an infeasible bundle method based on the so-called improvement functions to compute critical points for problems of this class. Our algorithm neither employs penalization techniques nor solves subproblems with linearized constraints. The approach, which encompasses bundle methods for nonlinearly-constrained convex programs, defines trial points as solutions of strongly convex quadratic programs. Different stationarity definitions are investigated, depending on the functions’ structures. The approach is assessed in a class of nonsmooth DC-constrained optimization problems modeling chance-constrained programs.},
  archive      = {J_COAP},
  author       = {van Ackooij, W. and Demassey, S. and Javal, P. and Morais, H. and de Oliveira, W. and Swaminathan, B.},
  doi          = {10.1007/s10589-020-00241-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {451-490},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A bundle method for nonsmooth DC programming with application to chance-constrained problems},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonconvex robust programming via value-function
optimization. <em>COAP</em>, <em>78</em>(2), 411–450. (<a
href="https://doi.org/10.1007/s10589-020-00245-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex programming based robust optimization is an active research topic in the past two decades, partially because of its computational tractability for many classes of optimization problems and uncertainty sets. However, many problems arising from modern operations research and statistical learning applications are nonconvex even in the nominal case, let alone their robust counterpart. In this paper, we introduce a systematic approach for tackling the nonconvexity of the robust optimization problems that is usually coupled with the nonsmoothness of the objective function brought by the worst-case value function. A majorization-minimization algorithm is presented to solve the penalized min-max formulation of the robustified problem that deterministically generates a “better” solution compared with the starting point (that is usually chosen as an unrobustfied optimal solution). A generalized saddle-point theorem regarding the directional stationarity is established and a game-theoretic interpretation of the computed solutions is provided. Numerical experiments show that the computed solutions of the nonconvex robust optimization problems are less sensitive to the data perturbation compared with the unrobustfied ones.},
  archive      = {J_COAP},
  author       = {Cui, Ying and He, Ziyu and Pang, Jong-Shi},
  doi          = {10.1007/s10589-020-00245-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {411-450},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Nonconvex robust programming via value-function optimization},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Globalized inexact proximal newton-type methods for
nonconvex composite functions. <em>COAP</em>, <em>78</em>(2), 377–410.
(<a href="https://doi.org/10.1007/s10589-020-00243-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization problems with composite functions consist of an objective function which is the sum of a smooth and a (convex) nonsmooth term. This particular structure is exploited by the class of proximal gradient methods and some of their generalizations like proximal Newton and quasi-Newton methods. The current literature on these classes of methods almost exclusively considers the case where also the smooth term is convex. Here we present a globalized proximal Newton-type method which allows the smooth term to be nonconvex. The method is shown to have nice global and local convergence properties, and some numerical results indicate that this method is very promising also from a practical point of view.},
  archive      = {J_COAP},
  author       = {Kanzow, Christian and Lechner, Theresa},
  doi          = {10.1007/s10589-020-00243-6},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {377-410},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Globalized inexact proximal newton-type methods for nonconvex composite functions},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using partial spectral information for block diagonal
preconditioning of saddle-point systems. <em>COAP</em>, <em>78</em>(2),
353–375. (<a href="https://doi.org/10.1007/s10589-020-00246-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering saddle-point systems of the Karush–Kuhn–Tucker (KKT) form, we propose approximations of the “ideal” block diagonal preconditioner based on the exact Schur complement proposed by Murphy et al. (SIAM J Sci Comput 21(6):1969–1972, 2000). We focus on the case where the (1,1) block is symmetric and positive definite, but with a few very small eigenvalues that possibly affect the convergence of Krylov subspace methods like Minres. Assuming that these eigenvalues and their associated eigenvectors are available, we first propose a Schur complement preconditioner based on this knowledge and establish lower and upper bounds on the preconditioned Schur complement. We next analyse theoretically the spectral properties of the preconditioned KKT systems using this Schur complement approximation in two spectral preconditioners of block diagonal forms. In addition, we derive a condensed “two in one” formulation of the proposed preconditioners in combination with a preliminary level of preconditioning on the KKT system. Finally, we illustrate on a PDE test case how, in the context of a geometric multigrid framework, it is possible to construct practical block preconditioners that help to improve on the convergence of Minres.},
  archive      = {J_COAP},
  author       = {Ramage, Alison and Ruiz, Daniel and Sartenaer, Annick and Tannier, Charlotte},
  doi          = {10.1007/s10589-020-00246-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {353-375},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Using partial spectral information for block diagonal preconditioning of saddle-point systems},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An interior point-proximal method of multipliers for convex
quadratic programming. <em>COAP</em>, <em>78</em>(2), 307–351. (<a
href="https://doi.org/10.1007/s10589-020-00240-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we combine an infeasible Interior Point Method (IPM) with the Proximal Method of Multipliers (PMM). The resulting algorithm (IP-PMM) is interpreted as a primal-dual regularized IPM, suitable for solving linearly constrained convex quadratic programming problems. We apply few iterations of the interior point method to each sub-problem of the proximal method of multipliers. Once a satisfactory solution of the PMM sub-problem is found, we update the PMM parameters, form a new IPM neighbourhood and repeat this process. Given this framework, we prove polynomial complexity of the algorithm, under standard assumptions. To our knowledge, this is the first polynomial complexity result for a primal-dual regularized IPM. The algorithm is guided by the use of a single penalty parameter; that of the logarithmic barrier. In other words, we show that IP-PMM inherits the polynomial complexity of IPMs, as well as the strict convexity of the PMM sub-problems. The updates of the penalty parameter are controlled by IPM, and hence are well-tuned, and do not depend on the problem solved. Furthermore, we study the behavior of the method when it is applied to an infeasible problem, and identify a necessary condition for infeasibility. The latter is used to construct an infeasibility detection mechanism. Subsequently, we provide a robust implementation of the presented algorithm and test it over a set of small to large scale linear and convex quadratic programming problems. The numerical results demonstrate the benefits of using regularization in IPMs as well as the reliability of the method.},
  archive      = {J_COAP},
  author       = {Pougkakiotis, Spyridon and Gondzio, Jacek},
  doi          = {10.1007/s10589-020-00240-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {307-351},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An interior point-proximal method of multipliers for convex quadratic programming},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient algorithm for nonconvex-linear minimax
optimization problem and its application in solving weighted maximin
dispersion problem. <em>COAP</em>, <em>78</em>(1), 287–306. (<a
href="https://doi.org/10.1007/s10589-020-00237-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the minimax optimization problem that is nonconvex in one variable and linear in the other variable, which is a special case of nonconvex-concave minimax problem, which has attracted significant attention lately due to their applications in modern machine learning tasks, signal processing and many other fields. We propose a new alternating gradient projection algorithm and prove that it can find an $$\varepsilon$$ -first-order stationary solution within $${\mathcal {O}}\left( \varepsilon ^{-3}\right)$$ projected gradient step evaluations. Moreover, we apply it to solve the weighted maximin dispersion problem and the numerical results show that the proposed algorithm outperforms the state-of-the-art algorithms.},
  archive      = {J_COAP},
  author       = {Pan, Weiwei and Shen, Jingjing and Xu, Zi},
  doi          = {10.1007/s10589-020-00237-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {287-306},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An efficient algorithm for nonconvex-linear minimax optimization problem and its application in solving weighted maximin dispersion problem},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating convergence of the globalized newton method to
critical solutions of nonlinear equations. <em>COAP</em>,
<em>78</em>(1), 273–286. (<a
href="https://doi.org/10.1007/s10589-020-00230-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the case of singular (and possibly even nonisolated) solutions of nonlinear equations, while superlinear convergence of the Newton method cannot be guaranteed, local linear convergence from large domains of starting points still holds under certain reasonable assumptions. We consider a linesearch globalization of the Newton method, combined with extrapolation and over-relaxation accelerating techniques, aiming at a speed up of convergence to critical solutions (a certain class of singular solutions). Numerical results indicate that an acceleration is observed indeed.},
  archive      = {J_COAP},
  author       = {Fischer, A. and Izmailov, A. F. and Solodov, M. V.},
  doi          = {10.1007/s10589-020-00230-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {273-286},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerating convergence of the globalized newton method to critical solutions of nonlinear equations},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). T-positive semidefiniteness of third-order symmetric tensors
and t-semidefinite programming. <em>COAP</em>, <em>78</em>(1), 239–272.
(<a href="https://doi.org/10.1007/s10589-020-00231-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The T-product for third-order tensors has been used extensively in the literature. In this paper, we first introduce first-order and second-order T-derivatives for the multi-variable real-valued function with the tensor T-product. Inspired by an equivalent characterization of a twice continuously T-differentiable multi-variable real-valued function being convex, we present a definition of the T-positive semidefiniteness of third-order symmetric tensors. After that, we extend many properties of positive semidefinite matrices to the case of third-order symmetric tensors. In particular, analogue to the widely used semidefinite programming (SDP for short), we introduce the semidefinite programming over the space of third-order symmetric tensors (T-semidefinite programming or TSDP for short), and provide a way to solve the TSDP problem by converting it into an SDP problem in the complex domain. Furthermore, we give several TSDP examples and especially some preliminary numerical results for two unconstrained polynomial optimization problems. Experiments show that finding the global minimums of polynomials via the TSDP relaxation outperforms the traditional SDP relaxation for the test examples.},
  archive      = {J_COAP},
  author       = {Zheng, Meng-Meng and Huang, Zheng-Hai and Wang, Yong},
  doi          = {10.1007/s10589-020-00231-w},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {239-272},
  shortjournal = {Comput. Optim. Appl.},
  title        = {T-positive semidefiniteness of third-order symmetric tensors and T-semidefinite programming},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decomposition and discrete approximation methods for solving
two-stage distributionally robust optimization problems. <em>COAP</em>,
<em>78</em>(1), 205–238. (<a
href="https://doi.org/10.1007/s10589-020-00234-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition methods have been well studied for solving two-stage and multi-stage stochastic programming problems, see Rockafellar and Wets (Math. Oper. Res. 16:119–147, 1991), Ruszczyński and Shapiro (Stochastic Programming, Handbook in OR &amp; MS, North-Holland Publishing Company, Amsterdam, 2003) and Ruszczyński (Math. Program. 79:333–353, 1997). In this paper, we propose an algorithmic framework based on the fundamental ideas of the methods for solving two-stage minimax distributionally robust optimization (DRO) problems where the underlying random variables take a finite number of distinct values. This is achieved by introducing nonanticipativity constraints for the first stage decision variables, rearranging the minimax problem through Lagrange decomposition and applying the well-known primal-dual hybrid gradient (PDHG) method to the new minimax problem. The algorithmic framework does not depend on specific structure of the ambiguity set. To extend the algorithm to the case that the underlying random variables are continuously distributed, we propose a discretization scheme and quantify the error arising from the discretization in terms of the optimal value and the optimal solutions when the ambiguity set is constructed through generalized prior moment conditions, the Kantorovich ball and $$\phi$$ -divergence centred at an empirical probability distribution. Some preliminary numerical tests show the proposed decomposition algorithm featured with parallel computing performs well.},
  archive      = {J_COAP},
  author       = {Chen, Yannan and Sun, Hailin and Xu, Huifu},
  doi          = {10.1007/s10589-020-00234-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {205-238},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Decomposition and discrete approximation methods for solving two-stage distributionally robust optimization problems},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implementing and modifying broyden class updates for large
scale optimization. <em>COAP</em>, <em>78</em>(1), 181–203. (<a
href="https://doi.org/10.1007/s10589-020-00239-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Broyden class updates for large scale optimization problems in n dimensions, restricting attention to the case when the initial second derivative approximation is the identity matrix. Under this assumption we present an implementation of the Broyden class based on a coordinate transformation on each iteration. It requires only $$2nk + O(k^{2}) + O(n)$$ multiplications on the kth iteration and stores $$nK+ O(K^2) + O(n)$$ numbers, where K is the total number of iterations. We investigate a modification of this algorithm by a scaling approach and show a substantial improvement in performance over the BFGS method. We also study several adaptations of the new implementation to the limited memory situation, presenting algorithms that work with a fixed amount of storage independent of the number of iterations. We show that one such algorithm retains the property of quadratic termination. The practical performance of the new methods is compared with the performance of Nocedal’s (Math Comput 35:773--782, 1980) method, which is considered the benchmark in limited memory algorithms. The tests show that the new algorithms can be significantly more efficient than Nocedal’s method. Finally, we show how a scaling technique can significantly improve both Nocedal’s method and the new generalized conjugate gradient algorithm.},
  archive      = {J_COAP},
  author       = {Buhmann, Martin and Siegel, Dirk},
  doi          = {10.1007/s10589-020-00239-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {181-203},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Implementing and modifying broyden class updates for large scale optimization},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Properties of the delayed weighted gradient method.
<em>COAP</em>, <em>78</em>(1), 167–180. (<a
href="https://doi.org/10.1007/s10589-020-00232-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The delayed weighted gradient method, recently introduced in Oviedo-Leon (Comput Optim Appl 74:729–746, 2019), is a low-cost gradient-type method that exhibits a surprisingly and perhaps unexpected fast convergence behavior that competes favorably with the well-known conjugate gradient method for the minimization of convex quadratic functions. In this work, we establish several orthogonality properties that add understanding to the practical behavior of the method, including its finite termination. We show that if the $$n\times n$$ real Hessian matrix of the quadratic function has only $$p&lt;n$$ distinct eigenvalues, then the method terminates in p iterations. We also establish an optimality condition, concerning the gradient norm, that motivates the future use of this novel scheme when low precision is required for the minimization of non-quadratic functions.},
  archive      = {J_COAP},
  author       = {Andreani, Roberto and Raydan, Marcos},
  doi          = {10.1007/s10589-020-00232-9},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {167-180},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Properties of the delayed weighted gradient method},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-forward-step projective splitting: Exploiting
cocoercivity. <em>COAP</em>, <em>78</em>(1), 125–166. (<a
href="https://doi.org/10.1007/s10589-020-00238-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes a new variant of projective splitting for solving maximal monotone inclusions and complicated convex optimization problems. In the new version, cocoercive operators can be processed with a single forward step per iteration. In the convex optimization context, cocoercivity is equivalent to Lipschitz differentiability. Prior forward-step versions of projective splitting did not fully exploit cocoercivity and required two forward steps per iteration for such operators. Our new single-forward-step method establishes a symmetry between projective splitting algorithms, the classical forward–backward splitting method (FB), and Tseng’s forward-backward-forward method. The new procedure allows for larger stepsizes for cocoercive operators: the stepsize bound is $$2\beta$$ for a $$\beta$$ -cocoercive operator, the same bound as has been established for FB. We show that FB corresponds to an unattainable boundary case of the parameters in the new procedure. Unlike FB, the new method allows for a backtracking procedure when the cocoercivity constant is unknown. Proving convergence of the algorithm requires some departures from the prior proof framework for projective splitting. We close with some computational tests establishing competitive performance for the method.},
  archive      = {J_COAP},
  author       = {Johnstone, Patrick R. and Eckstein, Jonathan},
  doi          = {10.1007/s10589-020-00238-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {125-166},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Single-forward-step projective splitting: Exploiting cocoercivity},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence study on strictly contractive peaceman–rachford
splitting method for nonseparable convex minimization models with
quadratic coupling terms. <em>COAP</em>, <em>78</em>(1), 87–124. (<a
href="https://doi.org/10.1007/s10589-020-00229-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alternating direction method of multipliers (ADMM) and Peaceman Rachford splitting method (PRSM) are two popular splitting algorithms for solving large-scale separable convex optimization problems. Though problems with nonseparable structure appear frequently in practice, researches on splitting methods for these problems remain to be scarce. Very recently, Chen et al. (Math Program 173(1–2):37–77, 2019) extended the 2-block ADMM to linearly constrained nonseparable models with quadratic coupling terms and established its convergence. However, theoretical researches about nonseparable PRSM or its variants are still lacking. To fill the gap, in this paper we focus on the strictly contractive PRSM (SC-PRSM) applied to 2-block linearly constrained convex minimization problems with quadratic coupling objective functions. Under mild conditions, we prove the convergence of our proposed SC-PRSM and establish its o(1/k) convergence rate. Moreover, we implement the SC-PRSM to solve a problem of calculating the Euclidian distance between two ellipsoids, and compare its performance with three ADMM type algorithms. The results show the nonseparable SC-PRSM outperforms the other three algorithms in terms of both the iteration numbers and CPU time.},
  archive      = {J_COAP},
  author       = {Li, Peixuan and Shen, Yuan and Jiang, Suhong and Liu, Zehua and Chen, Caihua},
  doi          = {10.1007/s10589-020-00229-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {87-124},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence study on strictly contractive Peaceman–Rachford splitting method for nonseparable convex minimization models with quadratic coupling terms},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tractable ADMM schemes for computing KKT points and local
minimizers for <span class="math display"><em>ℓ</em><sub>0</sub></span>
-minimization problems. <em>COAP</em>, <em>78</em>(1), 43–85. (<a
href="https://doi.org/10.1007/s10589-020-00227-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an $$\ell _0$$ -minimization problem where $$f(x) + \gamma \Vert x\Vert _0$$ is minimized over a polyhedral set and the $$\ell _0$$ -norm regularizer implicitly emphasizes the sparsity of the solution. Such a setting captures a range of problems in image processing and statistical learning. Given the nonconvex and discontinuous nature of this norm, convex regularizers as substitutes are often employed and studied, but less is known about directly solving the $$\ell _0$$ -minimization problem. Inspired by Feng et al. (Pac J Optim 14:273–305, 2018), we consider resolving an equivalent formulation of the $$\ell _0$$ -minimization problem as a mathematical program with complementarity constraints (MPCC) and make the following contributions towards the characterization and computation of its KKT points: (i) First, we show that feasible points of this formulation satisfy the relatively weak Guignard constraint qualification. Furthermore, if f is convex, an equivalence is derived between first-order KKT points and local minimizers of the MPCC formulation. (ii) Next, we apply two alternating direction method of multiplier (ADMM) algorithms, named (ADMM $$_{\mathrm{cf}}^{\mu , \alpha , \rho }$$ ) and (ADMM $$_{\mathrm{cf}}$$ ), to exploit the special structure of the MPCC formulation. Both schemes feature tractable subproblems. Specifically, in spite of the overall nonconvexity, it is shown that the first update can be effectively reduced to a closed-form expression by recognizing a hidden convexity property while the second necessitates solving a tractable convex program. In (ADMM $$_{\mathrm{cf}}^{\mu , \alpha , \rho }$$ ), subsequential convergence to a perturbed KKT point under mild assumptions is proved. Preliminary numerical experiments suggest that the proposed tractable ADMM schemes are more scalable than their standard counterpart while (ADMM $$_{\mathrm{cf}}$$ ) compares well with its competitors in solving the $$\ell _0$$ -minimization problem.},
  archive      = {J_COAP},
  author       = {Xie, Yue and Shanbhag, Uday V.},
  doi          = {10.1007/s10589-020-00227-6},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {43-85},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Tractable ADMM schemes for computing KKT points and local minimizers for $$\ell _0$$ -minimization problems},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An accelerated active-set algorithm for a quadratic
semidefinite program with general constraints. <em>COAP</em>,
<em>78</em>(1), 1–42. (<a
href="https://doi.org/10.1007/s10589-020-00228-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we are concerned with efficient algorithms for solving the least squares semidefinite programming which contains many equalities and inequalities constraints. Our proposed method is built upon its dual formulation and is a type of active-set approach. In particular, by exploiting the nonnegative constraints in the dual form, our method first uses the information from the Barzlai–Borwein step to estimate the active/inactive sets, and within an adaptive framework, it then accelerates the convergence by switching the L-BFGS iteration and the semi-smooth Newton iteration dynamically. We show the global convergence under mild conditions, and furthermore, the local quadratic convergence under the additional nondegeneracy condition. Various types of synthetic as well as real-world examples are tested, and preliminary but promising numerical experiments are reported.},
  archive      = {J_COAP},
  author       = {Shen, Chungen and Wang, Yunlong and Xue, Wenjuan and Zhang, Lei-Hong},
  doi          = {10.1007/s10589-020-00228-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-42},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An accelerated active-set algorithm for a quadratic semidefinite program with general constraints},
  volume       = {78},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
