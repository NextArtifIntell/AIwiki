<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---126">MVA - 126</h2>
<ul>
<li><details>
<summary>
(2021). LPI: Learn postures for interactions. <em>MVA</em>,
<em>32</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01235-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To a great extent, immersion of a virtual environment (VE) depends on the naturalness of the interface it provides for interaction. As people commonly exploit gestures during communication, therefore interaction based on hand-postures enhances the degree of realism of a VE. However, the choice of selecting hand postures for interaction varies from person to person. Generalizing the use of a specific posture with a particular interaction requires considerable computation which in turns depletes intuition of a 3D interface. By investigating machine learning in the domain of virtual reality (VR), this paper presents an open posture-based approach for 3D interaction. The technique is user-independent and relies neither on the size and color of hand nor on the distance between camera and posing-position. The system works in two phases—in the first phase, hand-postures are learnt, whereas in the second phase the known postures are used to perform interaction. With an ordinary camera, a scanned image is partitioned into equal size non-overlapping tiles. Four light-weight features, based on binary histogram and invariant moments, are calculated for each part and portion of a posture-image. The support vector machine classifier is trained by posture-specific knowledge carried accumulatively in each tile. By posing any known posture, the system extracts the tiles information to detect a particular hand-posture. At the successful recognition, appropriate interaction is activated in the designed VE. The proposed system is implemented in a case-study application; vision-based open posture interaction using the libraries of OpenCV and OpenGL. The system is assessed in three separate evaluation sessions. Results of the evaluations testify efficacy of the approach in various VR applications.},
  archive      = {J_MVA},
  author       = {Raees, Muhammad and Ullah, Sehat},
  doi          = {10.1007/s00138-021-01235-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {LPI: Learn postures for interactions},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate IoU computation for rotated bounding boxes in <span
class="math display">ℝ<sup>2</sup></span> and <span
class="math display">ℝ<sup>3</sup></span>. <em>MVA</em>, <em>32</em>(6),
1–23. (<a href="https://doi.org/10.1007/s00138-021-01238-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In object detection, the Intersection over Union ( $${\mathrm{IoU}}$$ ) is the most popular criterion used to validate the performance of an object detector on the testing object dataset, or to compare the performances of various object detectors on a common object dataset. The calculation of this criterion requires the determination of the overlapping area between two bounding boxes. If these latter are axis-aligned (or horizontal), then the exact calculation of their overlapping area is simple. But if these bounding boxes are rotated (or oriented), then the exact calculation of their overlapping area is laborious. Many rotated objects detectors have been developed using heuristics to approximate $${\mathrm{IoU}}$$ between two rotated bounding boxes. We have shown, through counterexamples, that these heuristics are not efficient in the sense that they can lead to false positive or false negative detection, which can bias the performance of comparative studies between object detectors. In this paper, we develop a method to calculate exact value of $${{\mathrm{IoU}}}$$ between two rotated bounding boxes. Moreover, we present an $$(\epsilon ,\alpha )$$ -estimator $$\widehat{{\mathrm{IoU}}}$$ of $${{\mathrm{IoU}}}$$ that satisfies $${\mathbf {Pr}} (|\widehat{{\mathrm{IoU}}} -{\mathrm{IoU}}| \le {\mathrm{IoU}}\epsilon )\ge 1-\alpha $$ . We also generalize the exact computing method and the $$(\epsilon ,\alpha )$$ -estimator of $${{\mathrm{IoU}}}$$ , to three-dimensional bounding boxes. Finally, we carry out many numerical experiments in $${\mathbb {R}}^2$$ and $${\mathbb {R}}^3$$ , in order to test the exact method of calculating the $${{\mathrm{IoU}}}$$ , and to compare the efficiency of the $$(\epsilon ,\alpha )$$ -estimator with respect to heuristic estimates of $${{\mathrm{IoU}}}$$ . Numerical study shows that the $$(\epsilon ,\alpha )$$ -estimator is distinguished by both precision and simplicity of implementation, while the exact calculation method is distinguished by both precision and speed.},
  archive      = {J_MVA},
  author       = {Zaïdi, Abdelhamid},
  doi          = {10.1007/s00138-021-01238-x},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Accurate IoU computation for rotated bounding boxes in $${\mathbb {R}}^2$$ and $${\mathbb {R}}^3$$},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised learning for person re-identification based
on style-transfer-generated data by CycleGANs. <em>MVA</em>,
<em>32</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01239-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) is an exceedingly significant branch in the field of computer vision, especially for video surveillance. It is still a challenge to obtain more labeled training data and use them reasonably for more precise matching, though the person re-ID performance has been improved significantly. In order to solve this challenge, this study proposes a semi-supervised learning algorithm for data augmentation, the style-transfer-generated data as an extra class (STGDEC), which is aided by the Cycle-Consistent Adversarial Networks (CycleGANs) in generating extra unlabeled training data. Specifically, the algorithm firstly trains the CycleGANs and Deep Convolutional Generative Adversarial Networks so as to generate large amounts of unlabeled data. Secondly, we propose an adaptive receptive field module to expand the size of receptive fields and select the appropriate receptive field features dynamically in order to learn more contextual information and discriminative feature representation and embed the module in the backbone network easily. Thirdly, we use the combination of label smoothing regularization for outliers and an extra class loss to regularize the generated data and encourage the network not to be too confident to the ground-truth. Finally, this paper proposes three training strategies for the combination of standard dataset and generated samples. Comprehensive experiments based on the STGDEC are conducted, and these results show that the proposed algorithm gains a significant improvement over the baseline, the Basel. + LSRO and state-of-the-art approaches of person re-ID in many cases.},
  archive      = {J_MVA},
  author       = {Zhu, Shangdong and Zhang, Yunzhou and Coleman, Sonya and Wang, Song and Li, Ruilong and Liu, Shuangwei},
  doi          = {10.1007/s00138-021-01239-w},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semi-supervised learning for person re-identification based on style-transfer-generated data by CycleGANs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integration of 2D iteration and a 3D CNN-based model for
multi-type artifact suppression in c-arm cone-beam CT. <em>MVA</em>,
<em>32</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-021-01240-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limiting the potential risks associated with radiation exposure is critically important when obtaining a diagnostic image. However, lowering the level of radiation may cause excessive noise and artifacts in computed tomography (CT) scans. In this study, we implemented and tested the performance of patch-based and block-based REDCNN models and revealed that a 3D kernel is efficient in removing 3D noise and artifacts. Additionally, we applied a 3D bilateral filter and a 2D-based Landweber iteration method to remove any remaining noise and to prevent the edges from blurring, which are limitations of a deep learning-based noise reduction system. For the 2D-based Landweber iteration, we examined the requisite step size and the number of iterations. The representative CT noise and artifacts, which were Gaussian noise and view aliasing artifacts, respectively, were simulated on XCAT and reproduced in vivo to verify that the proposed method could be used in an analogous clinical setting. Lastly, the performance of the proposed algorithm was evaluated on in vivo data with real low-dose noise. Our proposed method effectively suppressed complex noise without losing diagnostic features in both the simulation study and experimental evaluation. Furthermore, for the simulation study, we adopted a numerical observer model to evaluate the structural fidelity of the image quality more appropriately than existing image quality assessment methods.},
  archive      = {J_MVA},
  author       = {Choi, Dahim and Kim, Wonjin and Lee, Jiyeon and Han, Mina and Baek, Jongduk and Choi, Jang-Hwan},
  doi          = {10.1007/s00138-021-01240-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Integration of 2D iteration and a 3D CNN-based model for multi-type artifact suppression in C-arm cone-beam CT},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of inclusion by using 3D laser scanner in
composite prepreg manufacturing technique using convolutional neural
networks. <em>MVA</em>, <em>32</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s00138-021-01241-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among different manufacturing techniques available for composite aircraft structures, prepreg-based manual layup is widely used. During the fabrication process, the protective films of the prepregs or other materials used in the process could get inside as a foreign object between the layers. The present method of finding the inclusions during the prepreg layup is by visual inspection in the cleanroom. Carrying out visual inspection is challenging as the layup is usually carried out on large surfaces and reflective by nature. This paper proposes a 3D laser scanner-based approach for the detection of inclusion on flat and curved surfaces. Using the portable laser scanner, the surfaces of each layer are scanned and compared the resulting point clouds using with a reference layer data. Thicknesses between two surfaces are computed with Cloud to Cloud, Mesh to Cloud and Hausdorff distance to enhance the visibility of inclusions. It was found that this approach could enhance the visibility of inclusions over 50 micron and above. These enhanced features are used to train a multiview convolutional neural network to mark the inclusion regions, which can aid the inspector to identify the inclusion regions in a fast and efficient way.},
  archive      = {J_MVA},
  author       = {Augustin, M. J. and Ramesh, Vandana and Prasad, R. Krishna and Gupta, Nitesh and Kumar, M. Ramesh},
  doi          = {10.1007/s00138-021-01241-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Detection of inclusion by using 3D laser scanner in composite prepreg manufacturing technique using convolutional neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Squeezed fire binary segmentation model using convolutional
neural network for outdoor images on embedded device. <em>MVA</em>,
<em>32</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01242-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though image-based prediction of fire events is widely used, the current predictive methods are difficult to implement due to low performance and high specifications. In this work designed to overcome such problems, we propose binary semantic segmentation for fire images by employing deep learning that can be applied to embedded devices such as Jetson TX2. To reduce the parameters and consequently the model size while maintaining the performance, we replaced regular convolution with depthwise separable convolution and $$1 \times 1 $$ convolution. Moreover, the addition operation in the long skip connection was replaced with the concatenation operation to properly convey the information in the encoding phase. Besides, we propose the confusion block that can execute the model to proceed training more actively. From these approaches, we achieved a significantly small-sized network for fire segmentation with the highest performance. We compared the performance of the proposed method with various deep learning-based binary segmentation networks and image processing algorithm. Extensive experimental results on the FiSmo Dataset and Corsican Fire Database demonstrated that the proposed network outperforms other models with fewer parameters and is suitable for application in embedded devices.},
  archive      = {J_MVA},
  author       = {Song, Kyungmin and Choi, Han-Soo and Kang, Myungjoo},
  doi          = {10.1007/s00138-021-01242-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Squeezed fire binary segmentation model using convolutional neural network for outdoor images on embedded device},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depthwise grouped convolution for object detection.
<em>MVA</em>, <em>32</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01243-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection usually adopts two-stage end-to-end networks, which use backbone network (such as VGG and ResNet) for feature extraction and are combined with the region proposal network (RPN) for object localization and classification. In this paper, we explore a novel depthwise grouped convolution (DGC) in the backbone network by integrating channels grouping and depthwise separable convolution, which is able to share the convolution parameters in different channels to reduce the amounts of parameters for speeding up training. In particular, split and shuffle strategies of channels are introduced to enhance information exchange between different groups of channels in DGC block, which can prevent the decrease of performance caused by insufficient object samples. Furthermore, non-local block is adopted in RPN to focus on small objects that are hard to identify. Consequently, we introduce margin-based loss to guide the model training together with the loss of classification and regression. Experiments conducted on the VOC2007, VOC2012 and COCO2017 datasets demonstrate the efficiency and effectiveness of our method for object detection.},
  archive      = {J_MVA},
  author       = {Liao, Yongwei and Lu, Siwei and Yang, Zhenguo and Liu, Wenyin},
  doi          = {10.1007/s00138-021-01243-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Depthwise grouped convolution for object detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defect segmentation for multi-illumination quality control
systems. <em>MVA</em>, <em>32</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01244-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to recent advancements in image processing and deep learning techniques, visual surface inspection in production lines has become an automated process as long as all the defects are visible in a single or a few images. However, it is often necessary to inspect parts under many different illumination conditions to capture all the defects. Training deep networks to perform this task requires large quantities of annotated data, which are rarely available and cumbersome to obtain. To alleviate this problem, we devised an original augmentation approach that, given a small image collection, generates rotated versions of the images while preserving illumination effects, something that random rotations cannot do. We introduce three real multi-illumination datasets, on which we demonstrate the effectiveness of our illumination preserving rotation approach. Training deep neural architectures with our approach delivers a performance increase of up to 51% in terms of AuPRC score over using standard rotations to perform data augmentation.},
  archive      = {J_MVA},
  author       = {Honzátko, David and Türetken, Engin and Bigdeli, Siavash A. and Dunbar, L. Andrea and Fua, Pascal},
  doi          = {10.1007/s00138-021-01244-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Defect segmentation for multi-illumination quality control systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FPANet: Feature-enhanced position attention network for
semantic segmentation. <em>MVA</em>, <em>32</em>(6), 1–9. (<a
href="https://doi.org/10.1007/s00138-021-01246-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanism is beneficial to capture the contextual information in visual task. This paper proposes a feature-enhanced position attention network (FPANet) for semantic segmentation based on framework of FCN. On the top of dilated FCN, we design a feature integration module, which aggregates the context over local features by expanding the receptive field and multiscale representation, to promote a position attention module, which models spatial interdependencies over features, so as to form a feature-enhanced position attention module to enhance the discrimination of features for better semantic segmentation. Experimental comparisons show that our proposed FPANet is superior to other state-of-the-art models in the performance of segmentation accuracy on datasets PASCAL VOC 2012 and Cityscapes.},
  archive      = {J_MVA},
  author       = {Xu, Haixia and Wang, Shuailong and Huang, Yunjia and Zhou, Wei and Chen, Qi and Zhang, Dongbo},
  doi          = {10.1007/s00138-021-01246-x},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FPANet: Feature-enhanced position attention network for semantic segmentation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view subspace clustering with
kronecker-basis-representation-based tensor sparsity measure.
<em>MVA</em>, <em>32</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01247-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data are popular in many machine learning and computer vision applications. For example, in computer vision fields, one object can be described with images, text or videos. Recently, multi-view subspace clustering approaches, which can make use of the complementary information among different views to improve the performance of clustering, have attracted much attention. In this paper, we propose a novel multi-view subspace clustering method with Kronecker-basis-representation-based tensor sparsity measure (MSC-KBR) to address multi-view subspace clustering problem. In the MSC-KBR model, we first construct a tensor based on the subspace representation matrices of different views, and, then the high-order correlations underlying different views can be explored. We also adopt a novel Kronecker-basis-representation-based tensor sparsity measure (KBR) to the constructed tensor to reduce the redundancy of the learned subspace representations and improve the accuracy of clustering. Different from the traditional unfolding-based tensor norm, KBR can encode both sparsity insights delivered by Tucker and CANDECOMP/PARAFAC decompositions for a general tensor. By using the augmented Lagrangian method, an efficient algorithm is presented to solve the optimization problem of the MSC-KBR model. The experimental results on some datasets show that the proposed MSC-KBR model outperforms many state-of-the-art multi-view clustering approaches.},
  archive      = {J_MVA},
  author       = {Lu, Gui-Fu and Li, Hua and Wang, Yong and Tang, Ganyi},
  doi          = {10.1007/s00138-021-01247-w},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-view subspace clustering with kronecker-basis-representation-based tensor sparsity measure},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BPFD-net: Enhanced dehazing model based on pix2pix framework
for single image. <em>MVA</em>, <em>32</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01248-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an image dehazing model based on the generative adversarial networks (GAN). The pix2pix framework is taken as the starting point in the proposed model. First, a UNet-like network is employed as the dehazing network in view of the high consistency of the image dehazing problem. In the proposed model, a shortcut module is proposed to effectively increase the nonlinear characteristics of the network, which is beneficial for subsequent processes of image generation and stabilizing the training process of the GAN network. Also, inspired by the face illumination processing model and the perceptual loss model, the quality vision loss strategy is designed to obtain a better visual quality of the dehazed image, based on peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and perceptual losses. The experimental results on public datasets show that our network demonstrates the superiority over the compared models on indoor images. Also, the dehazed image by the proposed model shows better chromaticity and qualitative quality.},
  archive      = {J_MVA},
  author       = {Li, Shaoyi and Lin, Jian and Yang, Xi and Ma, Jun and Chen, Yifeng},
  doi          = {10.1007/s00138-021-01248-9},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {BPFD-net: Enhanced dehazing model based on pix2pix framework for single image},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Early, intermediate and late fusion strategies for robust
deep learning-based multimodal action recognition. <em>MVA</em>,
<em>32</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s00138-021-01249-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal action recognition techniques combine several image modalities (RGB, Depth, Skeleton, and InfraRed) for a more robust recognition. According to the fusion level in the action recognition pipeline, we can distinguish three families of approaches: early fusion, where the raw modalities are combined ahead of feature extraction; intermediate fusion, the features, respective to each modality, are concatenated before classification; and late fusion, where the modality-wise classification results are combined. After reviewing the literature, we identified the principal defects of each category, which we try to address by first investigating more deeply the early-stage fusion that has been poorly explored in the literature. Second, intermediate fusion protocols operate on the feature map, irrespective of the particularity of human action, we propose a new scheme where we optimally combine modality-wise features. Third, as most of the late fusion solutions use handcrafted rules, prone to human bias, and far from real-world peculiarities, we adopt a neural learning strategy to extract significant features from data rather than assuming that artificial rules are correct. We validated our findings on two challenging datasets. Our obtained results were as good or better than their literature counterparts.},
  archive      = {J_MVA},
  author       = {Boulahia, Said Yacine and Amamra, Abdenour and Madi, Mohamed Ridha and Daikh, Said},
  doi          = {10.1007/s00138-021-01249-8},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Early, intermediate and late fusion strategies for robust deep learning-based multimodal action recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency detection based on color descriptor and high-level
prior. <em>MVA</em>, <em>32</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01250-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing saliency detection methods calculate the Euclidean distance in CIElab color space as similarity degrees between image pixels or patches, although CIElab color owns a better perceptually uniform color difference, closing to human color perception. However, it may fail if salient objects consist of diverse color regions and are surrounded by cluttered backgrounds. Aiming at tackling the problem, we propose a background-based saliency detection method by exploring the new color descriptor and high-level prior. Specifically, here a novelty color space is produced to remedy the shortages of CIElab color space. Based on the global and local descriptors, two boundary-based saliency detection algorithms are individually performed, achieving the corresponding coarse saliency results. After that, we embed the center prior and objectness prior together into the two saliency results, respectively. To this end, L2 norm is applied to select the best saliency result. The experimental results on three benchmark datasets demonstrate the proposed method achieves competitive performance against several state-of-the-art methods under the comparison evaluation.},
  archive      = {J_MVA},
  author       = {Wang, Fan and Peng, Guohua},
  doi          = {10.1007/s00138-021-01250-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Saliency detection based on color descriptor and high-level prior},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lesion-aware attention with neural support vector machine
for retinopathy diagnosis. <em>MVA</em>, <em>32</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01253-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is a severe eye disease which can lead to permanent blindness. Identifying DR in early stages by using computer-aided diagnosis (CAD) systems can help the ophthalmologists to give proper treatment rationally, there by preventing many people from going blind. Due to intra-class variations and imbalanced data distribution, it is highly difficult to design a CAD system for DR severity diagnosis with greater generalizability. In this article, we propose a multi-stage deep learning pipeline, lesion-aware attention with neural support vector machine, for diabetic retinopathy diagnosis. Proposed pipeline consists of a pre-trained convolution base for learning retinal image spatial representations, lesion-aware attention for weighting lesion specific features, convolution autoencoder for learning latent attention representations and a neural support vector machine for discrimination. Convolutional autoencoder and neural support vector machine are jointly trained in end-to-end fashion to obtain category based lesion specific latent attention features by complementing each other in re-constructor and discriminator paths. Proposed approach is validated using two benchmark retinal scan image datasets, Kaggle APTOS 2019 and ISBI 2018 IDRiD, for DR type and severity grade classification tasks. Our experimental studies expose that using lesion-aware attention along with the joint training of autoencoder and neural support vector machine boosted the performance of models used for DR diagnosis, thereby outperforming existing works presented in the literature for DR severity grading. Proposed model achieved the highest accuracy of 90.45%, 84.31% on APTOS dataset and an accuracy of 79.85%, 63.24% on IDRiD dataset for DR type and severity grade classification tasks, respectively.},
  archive      = {J_MVA},
  author       = {Shaik, Nagur Shareef and Cherukuri, Teja Krishna},
  doi          = {10.1007/s00138-021-01253-y},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Lesion-aware attention with neural support vector machine for retinopathy diagnosis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metaheuristics for the positioning of 3D objects based on
image analysis of complementary 2D photographs. <em>MVA</em>,
<em>32</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s00138-021-01229-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, advances in 3D modeling make it possible to identically reproduce objects, animals, humans and even entire scenes. The broad applications concern video games, virtual reality or augmented reality and cinema, for example. In this article, we propose a new method to build a 3D scene directly from several complementary photographs. The positions of the objects for which we already have a 3D model will be determined by triangulation, thanks to the information extracted from the photographs, such as the outline of the objects on the images. Each pixel of the images is converted into a value that gives its distance to the nearest outline. The 3D model of the objects is then projected on the converted images, and the triangulation is done using a cost function that gives the distance of each projection of the objects to their respective outlines. A projection is considered perfect when its distance to its outlines is null, which means that the cost function gives a score of zero as well. We propose to solve this optimization problem by means of two algorithms, namely Simulated Annealing (SA) and quantum particle swarm optimization (QUAPSO).},
  archive      = {J_MVA},
  author       = {Flori, Arnaud and Oulhadj, Hamouche and Siarry, Patrick},
  doi          = {10.1007/s00138-021-01229-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Metaheuristics for the positioning of 3D objects based on image analysis of complementary 2D photographs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-purpose method for de-hazing and enhancement of
underwater and low-light images. <em>MVA</em>, <em>32</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01230-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many similarities between underwater images and low-light images, such as image blur and color distortion, but for such problems, there are few unified methods that can solve these problems well. This paper proposes a method based on multi-scale retinex color recovery (MSRCR) and color correction. First, color channel transfer (CCT) is used to preprocess the image. Then, a method of MSRCR and guided filtering is proposed to remove image fog. Finally, the statistical colorless slant correction fusion smoothing filter method is proposed to enhance the image, which improves the color contrast and sharpness of the image. Experiments have proved that the method proposed in this paper is effective in image de-hazing and enhancement.},
  archive      = {J_MVA},
  author       = {Liu, Ke and Liang, Yongquan},
  doi          = {10.1007/s00138-021-01230-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Dual-purpose method for de-hazing and enhancement of underwater and low-light images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the safety of vulnerable road users by cyclist detection
and tracking. <em>MVA</em>, <em>32</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s00138-021-01231-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection of vulnerable road users is of great relevance to avoid accidents in the context of intelligent transportation systems. In this work, detection and tracking is acknowledged for a particularly vulnerable class of road users, the cyclists. We present a performance comparison between the main deep learning-based algorithms reported in the literature for object detection, such as SSD, Faster R-CNN and R-FCN along with InceptionV2, ResNet50, ResNet101, Mobilenet V2 feature extractors. In order to identify the cyclist heading and predict its intentions, we propose a multi-class detection with eight classes according to orientations. To do so, we introduce a new dataset called “CIMAT-Cyclist”, containing 20,229 cyclist instances over 11,103 images, labeled based on the cyclist’s orientation. To improve the performance in cyclists’ detection, the Kalman filter is used for tracking, coupled together with the Kuhn–Munkres algorithm for multi-target association. Finally, the vulnerability of the cyclists is evaluated for each instance in the field of view, taking into account their proximity and predicted intentions according to their heading angle, and a risk level is assigned to each cyclist. Experimental results validate the proposed strategy in real scenarios, showing good performance.},
  archive      = {J_MVA},
  author       = {García-Venegas, M. and Mercado-Ravell, D. A. and Pinedo-Sánchez, L. A. and Carballo-Monsivais, C. A.},
  doi          = {10.1007/s00138-021-01231-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {On the safety of vulnerable road users by cyclist detection and tracking},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse representation with enhanced nonlocal self-similarity
for image denoising. <em>MVA</em>, <em>32</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01232-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, the sparsity prior of image is investigated and utilized widely as the development of compressed sensing theory. The dictionary learning combined with the convex optimization methods promotes the sparse representation to be one of the state-of-the-art techniques in image processing, such as denoising, super-resolution, deblurring, and inpainting. Empirically, the sparser of image representation, the better of image restoration. In this work, the non-local clustering sparse representation is applied with optimized matching strategies of self-similar patches, which break through the bottleneck of search window (localization) and improve the estimation effect of the sparse coefficient. The experimental results show that the proposed method provides an effective suppression on noise, preserves more details of image and presents more comfortable visual experience.},
  archive      = {J_MVA},
  author       = {Zhou, Tao and Li, Chen and Zeng, Xuan and Zhao, Yuhang},
  doi          = {10.1007/s00138-021-01232-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Sparse representation with enhanced nonlocal self-similarity for image denoising},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ST-CSNN: A novel method for vehicle counting. <em>MVA</em>,
<em>32</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01233-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle counting using computer vision techniques has potential to alleviate traffic congestion in intelligent transportation system. In this paper, we propose a novel method to count vehicles in a human-like manner. This paper has two main contributions. Firstly, we propose ST-CSNN, which is an efficient, lightweight vehicle counting method. The method counts based on vehicle identity comparison to omit duplicate instances. Combined with the spatio-temporal information between frames, it is able to accelerate speed and improve accuracy of counting. Secondly, we strengthen the method’s performance by proposing an improved loss function on the basis of Siamese neural network. Besides, we conduct experiments on several datasets to evaluate the performance of the proposed loss function for verification and the whole method for counting. The experimental results show the practicability of this method for real counting scenes.},
  archive      = {J_MVA},
  author       = {Yin, Kang and Wang, Liantao and Zhang, Jinxia},
  doi          = {10.1007/s00138-021-01233-2},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ST-CSNN: A novel method for vehicle counting},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partitioned iterated function systems by regression models
for head pose estimation. <em>MVA</em>, <em>32</em>(5), 1–8. (<a
href="https://doi.org/10.1007/s00138-021-01234-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation represents an important computer vision technique in different contexts where image acquisition cannot be controlled by an operator, making face recognition of unknown subjects more accurate and efficient. In this work, starting from partitioned iterated function systems to identify the pose, different regression models are adopted to predict the angular value errors (yaw, pitch and roll axes, respectively). This method combines the fractal image compression characteristics, such as self-similar structures in order to identify similar head rotation, with regression analysis prediction. The experimental evaluation is performed on widely used benchmark datasets, i.e., Biwi and AFLW2000, and the results are compared with many existing state-of-the-art methods, demonstrating the robustness of the proposed fusion approach and excellent performance.},
  archive      = {J_MVA},
  author       = {Abate, Andrea F. and Barra, Paola and Pero, Chiara and Tucci, Maurizio},
  doi          = {10.1007/s00138-021-01234-1},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-8},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Partitioned iterated function systems by regression models for head pose estimation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic high fidelity foot contact location and timing for
elite sprinting. <em>MVA</em>, <em>32</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s00138-021-01236-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making accurate measurements of human body motions using only passive, non-interfering sensors such as video is a difficult task with a wide range of applications throughout biomechanics, health, sports and entertainment. The rise of machine learning-based human pose estimation has allowed for impressive performance gains, but machine learning-based systems require large datasets which might not be practical for niche applications. As such, it may be necessary to adapt systems trained for more general-purpose goals, but this might require a sacrifice in accuracy when compared with systems specifically developed for the application. This paper proposes two approaches to measuring a sprinter’s foot-ground contact locations and timing (step length and step frequency), a task which requires high accuracy. The first approach is a learning-free system based on occupancy maps. The second approach is a multi-camera 3D fusion of a state-of-the-art machine learning-based human pose estimation model. Both systems use the same underlying multi-camera system. The experiments show the learning-free computer vision algorithm to provide foot timing to better than 1 frame at 180 fps, and step length accurate to 7 mm, while the system based on pose estimation achieves timing better than 1.5 frames at 180 fps, and step length estimates accurate to 20 mm.},
  archive      = {J_MVA},
  author       = {Evans, Murray and Colyer, Steffi and Salo, Aki and Cosker, Darren},
  doi          = {10.1007/s00138-021-01236-z},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic high fidelity foot contact location and timing for elite sprinting},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image dataset creation and networks improvement method based
on CAD model and edge operator for object detection in the manufacturing
industry. <em>MVA</em>, <em>32</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s00138-021-01237-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating image dataset for object detection is a time-consuming and laborious work, seriously hindering the rapid application of object detection in the industrial manufacturing field. To reduce time and cost of object detection application, a method of image dataset creation and networks improvement based on CAD model and edge extraction operators is proposed. It can quickly generate effective training dataset without any tedious work and make the object detection networks obtain good detection performance. The method consists of three steps: capture-images-automatically, cut-and-paste and networks-improvement. To improve the performance of the detection networks, edge extraction operators are used to obtain the common features of the synthetic images and the real images. These edge extraction operators include Sobel edge, Laplacian edge, Canny edge and adaptive threshold edge, and the experimental results show that the adaptive threshold edge achieves the best effect. In addition, a class-weights is adopted to improve the AP of hard-to-detect parts. Ten mechanical parts of a 3D-printed aero-engine are used to evaluate this method. The results show that the improved networks (yolov5s) trained with the synthetic images can achieve 99.08%, 93.83% and 98.91% of the average recall, average precision and mAP, respectively. Taking into account the time, cost and detection performance, the proposed method is much better than the traditional method and current advanced method. The proposed method is feasible for object detection in many industrial scenarios where CAD models of products can be easily obtained.},
  archive      = {J_MVA},
  author       = {Tang, Pengzhou and Guo, Yu and Li, Han and Wei, Zhen and Zheng, Guanguan and Pu, Jun},
  doi          = {10.1007/s00138-021-01237-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Image dataset creation and networks improvement method based on CAD model and edge operator for object detection in the manufacturing industry},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmentation of photovoltaic module cells in uncalibrated
electroluminescence images. <em>MVA</em>, <em>32</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s00138-021-01191-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High resolution electroluminescence (EL) images captured in the infrared spectrum allow to visually and non-destructively inspect the quality of photovoltaic (PV) modules. Currently, however, such a visual inspection requires trained experts to discern different kinds of defects, which is time-consuming and expensive. Automated segmentation of cells is therefore a key step in automating the visual inspection workflow. In this work, we propose a robust automated segmentation method for extraction of individual solar cells from EL images of PV modules. This enables controlled studies on large amounts of data to understanding the effects of module degradation over time—a process not yet fully understood. The proposed method infers in several steps a high-level solar module representation from low-level ridge edge features. An important step in the algorithm is to formulate the segmentation problem in terms of lens calibration by exploiting the plumbline constraint. We evaluate our method on a dataset of various solar modules types containing a total of 408 solar cells with various defects. Our method robustly solves this task with a median weighted Jaccard index of $$94.47\%$$ and an $$F_1$$ score of $$97.62\%$$ , both indicating a high sensitivity and a high similarity between automatically segmented and ground truth solar cell masks.},
  archive      = {J_MVA},
  author       = {Deitsch, Sergiu and Buerhop-Lutz, Claudia and Sovetkin, Evgenii and Steland, Ansgar and Maier, Andreas and Gallwitz, Florian and Riess, Christian},
  doi          = {10.1007/s00138-021-01191-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Segmentation of photovoltaic module cells in uncalibrated electroluminescence images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic visual estimation of tomato cluster maturity in
plant rows. <em>MVA</em>, <em>32</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s00138-021-01202-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper aims to study image processing algorithms to accelerate and facilitate the evaluation of the harvest condition in tomato farms. In order to achieve this, two different deep learning models are trained and combined with counting methods to produce a harvest monitoring system for embedded applications using an Intel® MovidiusTM and an affordable RGB camera. The first model detects the location of cherry tomato clusters, while the second estimates the fruit’s maturity. The results are compared to a baseline implementation based on segmentation. Next, a multiple counting method based on regions of interest is applied to the detected clusters in videos to count the tomatoes at different maturity stages. In order to produce a more robust counting, a tracking system is implemented which uses temporal information to find the unique tomato clusters in videos. In the evaluation stage, the obtained location results indicate an intersection over union ( $$ IoU $$ ) of about $$89\%$$ when using the MobileNetV1 as a feature extractor and choosing the appropriate location anchors. The maturity estimation results indicate better performance for the trained algorithm as compared to the baseline, providing a root mean square error of $$7.7\%$$ . The best results were obtained when combining the fully learned solution with the tracking system, correctly counting the majority of the tomato clusters at multiple maturity stages.},
  archive      = {J_MVA},
  author       = {Tenorio, Gabriel Lins and Caarls, Wouter},
  doi          = {10.1007/s00138-021-01202-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic visual estimation of tomato cluster maturity in plant rows},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rock segmentation visual system for assisting driving in TBM
construction. <em>MVA</em>, <em>32</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01203-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tunnel boring machine (TBM) is a key equipment for excavating long-range tunnels. It is a complex system and hard to be controlled well in practice. In this paper, we propose the rock segmentation visual system to assist TBM driving. Through the system, online size distribution of excavated rocks is automatically analysed and sent back to TBM driver, from which many statistical information can be gathered. The system’s core algorithm is based on semantic segmentation, and the rock detection task is viewed as a rock/background pixel-wise classification problem. Accordingly, the Rock Segmentation Dataset is made with specific annotation strategies, and the goal of the dataset is to pick out large rocks in the images. Many networks are evaluated quantitatively on it, and we select the best suited one. We design two parallel networks to extract rock object and contour mask, such that the connected rock areas in object mask can be split with a mask fusion algorithm. Further network modification is made to boost inference speed that meets the requirement of system design. Experimental results show that the system can effectively detect large rock particles in the images and make necessary statistical analysis. Specifically, the segmentation accuracy achieves 68.3% mIoU, and the inference speed achieves 19.4 FPS under image resolution of $$1600\times 1200$$ on one NVIDIA Titan XP GPU. From the viewpoint of statistical analysis, 43.5% rock size IoU and 14.7% error rate of mean rock size are obtained, which is acceptable from the viewpoint of real applications.},
  archive      = {J_MVA},
  author       = {Xue, Zhenfeng and Chen, Liang and Liu, Zhitao and Lin, Fulong and Mao, Weijie},
  doi          = {10.1007/s00138-021-01203-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Rock segmentation visual system for assisting driving in TBM construction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer-aided automatic detection of acrylamide in
deep-fried carbohydrate-rich food items using deep learning.
<em>MVA</em>, <em>32</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01204-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-fried carbohydrate-rich foods items such as potato chips and French fries are one of the most popular snack foods consumed across the globe. In the production of these carbohydrate-rich foods items, a compound known as acrylamide is formed which is carcinogen and mutagen as well. The conventional chemical-based methods for detection of the presence of acrylamide in the deep-fried carbohydrate-rich food items are a time-consuming, destructive process that requires skilled manpower. The present work proposes a deep learning-based computer vision framework for automatic detection of the presence of acrylamide in potato chip samples with and without transfer learning. The performance of proposed six-layer CNN (without transfer learning) has been compared with the performance of the other transfer learned-models, for the present classification task using fivefold cross-validation. Experimental results show that the proposed six-layer CNN classifies the acrylamide-positive and negative samples with an average f1 score of 0.9251, whereas with the transfer learning-based approach, best average f1 score of 0.9644 was achieved. In conclusion, the proposed methodology in the current work is well suited for the acrylamide detection problem and the proposed work also analyses the effectiveness of the transfer learning-based approach when compared with the approach without utilizing the concept of transfer learning.},
  archive      = {J_MVA},
  author       = {Maurya, Ritesh and Singh, Suman and Pathak, Vinay Kumar and Dutta, Malay Kishore},
  doi          = {10.1007/s00138-021-01204-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Computer-aided automatic detection of acrylamide in deep-fried carbohydrate-rich food items using deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Abnormal event detection by variation matching.
<em>MVA</em>, <em>32</em>(4), 1–8. (<a
href="https://doi.org/10.1007/s00138-021-01205-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, surveillance systems have been widely used to analyze video recordings captured by surveillance cameras and to detect abnormal or irregular events in real-world scenes. In this study, we present a novel system that detects abnormal events. Unlike conventional methods, we consider abnormal event detection as variation matching problems. In approaching this problem, we transform from a single video to multiple ones by imposing variations on the video. Using a fully connected cross-entropy Monte Carlo method, we match multiple videos in a fully connected manner and detect abnormal events in all the videos concurrently. The experimental results show that our method can accurately detect abnormal events in multiple videos. Our proposed method can be used to automatically recognize abnormal events included in multi-view CCTV videos, which are available at airport terminals and underground stations.},
  archive      = {J_MVA},
  author       = {Cho, Sungmin and Kwon, Junseok},
  doi          = {10.1007/s00138-021-01205-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-8},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Abnormal event detection by variation matching},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing effective power law-based loss function for faster
and better bounding box regression. <em>MVA</em>, <em>32</em>(4), 1–10.
(<a href="https://doi.org/10.1007/s00138-021-01206-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective bounding box regression is essential for running any real-time object detection algorithm with acceptable accuracy. The currently available loss functions have issues like high computations, and sometimes they suffer from a subtle problem of plateau for non-overlapping bounding boxes, as the resultant bounding boxes are found to be far from the ground truth. In the present investigation, we have proposed a loss function with a new power-law term introduced in it for the normalized distance, which converges as fast as the Complete Intersection over Union (CIoU), but turns out to be computationally much faster than the Intersection over Union (IoU) and Generalised IoU (GIoU). The proposed function is simpler than CIoU. The incorporated power term has been optimized based on the corresponding computational time and on the sum of errors simulated for about multi-million cases, the details of which have been elaborated in the paper. The proposed Absolute IoU (AIoU) loss function has been successfully implemented and tested using the state-of-the-art object detection algorithms, such as You Only Look Once (YOLO) and Single Shot Multibox Detector (SSD) and is found to achieve significant performance improvement, using well-known metric Average Precision (AP), indicating the effectiveness of our approach.},
  archive      = {J_MVA},
  author       = {Aswal, Diksha and Shukla, Priya and Nandi, G. C.},
  doi          = {10.1007/s00138-021-01206-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Designing effective power law-based loss function for faster and better bounding box regression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face aging using global and pyramid generative adversarial
networks. <em>MVA</em>, <em>32</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s00138-021-01207-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach that addresses face aging as an unsupervised image-to-image translation problem. The proposed approach achieves age progression (i.e., future looks) and regression (i.e., previous looks) of face images that belong to a specific age class by translating them to other (subsequent or precedent) age classes. It learns pairwise translations between all age classes. Two variants are presented. The first one learns a global transformation, while the second one incorporates a pyramid encoding and decoding scheme to more effectively diffuse age class information. The proposed variants are thoroughly evaluated with respect to both qualitative and quantitative criteria. They yield appealing face age progression and regression results when compared to ground truth images and outperform state-of-the-art approaches for face aging based on quantitative evaluation metrics. Notably, the incorporation of pyramid encoding and decoding is proven to be beneficial to the quality of the generated images.},
  archive      = {J_MVA},
  author       = {Pantraki, Evangelia and Kotropoulos, Constantine},
  doi          = {10.1007/s00138-021-01207-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Face aging using global and pyramid generative adversarial networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FAE-GAN: Facial attribute editing with multi-scale attention
normalization. <em>MVA</em>, <em>32</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01208-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial attribute editing has gained increasing attention recently. Previous methods tackle this challenge by incorporating encoder–decoder and generative adversarial networks. However, the bottleneck layer in encoder–decoder of these methods often leads to blurry and low-quality editing results. And skip connections are used between deep and shallow layers to improve image quality but suffer from a limited ability to manipulate attribute. To address these issues, we propose a novel Facial Attribute Editing Generative Adversarial Networks from a selective refinement perspective, which is capable of focusing on editing the image attributes to be changed while preserving its unique details. Specifically, our method first learns a spatially varying function that maps a high-level feature map to an appropriate parameter map of the normalization layer. Then, by utilizing the residual block, the low-level feature map is added to the feature map after modulation, making the attribute refinement task easier. Experimental results show the superiority of our method in both performances of the attribute manipulation accuracy and perception quality.},
  archive      = {J_MVA},
  author       = {Zhu, Jiaqi and Ouyang, Pengxiang and Tao, Ran and Chen, Xin and Wang, Jing and Zhan, Shu},
  doi          = {10.1007/s00138-021-01208-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FAE-GAN: Facial attribute editing with multi-scale attention normalization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning-based object recognition in multispectral
satellite imagery for real-time applications. <em>MVA</em>,
<em>32</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-021-01209-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite imagery is changing the way we understand and predict economic activity in the world. Advancements in satellite hardware and low-cost rocket launches have enabled near-real-time, high-resolution images covering the entire Earth. It is too labour-intensive, time-consuming and expensive for human annotators to analyse petabytes of satellite imagery manually. Current computer vision research exploring this problem still lack accuracy and prediction speed, both significantly important metrics for latency-sensitive automatized industrial applications. Here we address both of these challenges by proposing a set of improvements to the object recognition model design, training and complexity regularisation, applicable to a range of neural networks. Furthermore, we propose a fully convolutional neural network (FCN) architecture optimised for accurate and accelerated object recognition in multispectral satellite imagery. We show that our FCN exceeds human-level performance with state-of-the-art 97.67% accuracy over multiple sensors, it is able to generalize across dispersed scenery and outperforms other proposed methods to date. Its computationally light architecture delivers a fivefold improvement in training time and a rapid prediction, essential to real-time applications. To illustrate practical model effectiveness, we analyse it in algorithmic trading environment. Additionally, we publish a proprietary annotated satellite imagery dataset for further development in this research field. Our findings can be readily implemented for other real-time applications too.},
  archive      = {J_MVA},
  author       = {Gudžius, Povilas and Kurasova, Olga and Darulis, Vytenis and Filatovas, Ernestas},
  doi          = {10.1007/s00138-021-01209-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep learning-based object recognition in multispectral satellite imagery for real-time applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced machine perception by a scalable fusion of RGB–NIR
image pairs in diverse exposure environments. <em>MVA</em>,
<em>32</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s00138-021-01210-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-spectral imaging technique for the swift fusion of red–green–blue (RGB) and near infrared (NIR) image pairs with a deep learning based resolution enhancement technique is proposed, mpirically investigated and compared to some state-of-the-art techniques in the current work. The results of the proposed multi-spectral image fusion demonstrate good chrominance preservation, improved sharpness and optimised lighting in low-light dawn and dusk scenes. The fused image shows the culmination of the edges that are inherent to both the RGB and NIR spectrum images. Some examples include increased visibility between vegetation and the sky, shadowed and non-shaded areas, and increased optical depth in tree branches and vehicles. A hue, saturation, value (HSV)–NIR fusion is also evaluated by simply converting the RGB image to the HSV colour space. HSV, due to its high colour strength, illuminates high-colour contrast artefacts such as road signs and the rear of vehicles better than their RGB-based fused image equivalent. Empirical research shows that RGB–NIR fusion outperforms other strategies in contrast restoration metric (r), two image quality assessment metrics, and a peak-to-noise-ratio metric. The two image fusion models are implemented in a deep learning semantic segmentation network to investigate their perceived consistency in real-world scenarios. The proposed coarse-grained semantic segmentation network is trained to auto-annotate pixels as belonging to one of the 10 classes. The per-class performance of the RGB–NIR and HSV–NIR-based semantic segmentation in comparison with other methods is discussed in detail in the current work.},
  archive      = {J_MVA},
  author       = {Kumar, Wahengbam Kanan and Singh, Ningthoujam Johny and Singh, Aheibam Dinamani and Nongmeikapam, Kishorjit},
  doi          = {10.1007/s00138-021-01210-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced machine perception by a scalable fusion of RGB–NIR image pairs in diverse exposure environments},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble learning with advanced fast image filtering
features for semi-global matching. <em>MVA</em>, <em>32</em>(4), 1–15.
(<a href="https://doi.org/10.1007/s00138-021-01211-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the past several years, a variety of algorithms have been focused on how to exploit two-dimensional scanline optimization to augment one-dimensional ones for semi-global matching. Different from the former contributions, an ensemble learning with advanced fast image filtering features for semi-global matching is proposed in this paper. Firstly, fewer categories of features (confidence measures) are extracted through various advanced fast image filters on the original scale of 8 directions’ semi-global matching disparity maps. Then, all the features are weaved together and divided into positive and negative samples for ensemble learning after comparing with ground truth. After that, the initial disparity map is obtained by leveraging the confidence probability of ensemble learning prediction. Finally, an efficient two-step single view disparity refinement strategy is employed, which no longer requires the right view’s disparity map for attaining the final refined results. Performance evaluations on Middlebury v.2 and v.3 stereo data sets demonstrate that the proposed algorithm outperforms other four most recent stereo matching algorithms. In addition, the presented algorithm shows relative high implementation efficiency compared with others.},
  archive      = {J_MVA},
  author       = {Yao, Peng and Feng, Jieqing},
  doi          = {10.1007/s00138-021-01211-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ensemble learning with advanced fast image filtering features for semi-global matching},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A compressed matrix sequence method for solving normal
equations of bundle adjustment. <em>MVA</em>, <em>32</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s00138-021-01212-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle adjustment is a least squares method-based algorithm for minimizing the global reprojection error and has provided an effective solution for structure from motion (SfM). The Levenberg–Marquardt algorithm provides a feasible and convenient way for bundle adjustment and creates a system of linear equations which are normal equations. For the special sparsity structure of an augmented Hessian matrix, a Schur complement trick is introduced to reduce computation complexity. However, the general sparse matrix storage formats are not optimized for the augmented Hessian matrix and consume too much computation time. According to the Schur complement trick, this paper divides the arrow-like augmented Hessian matrix into a structure matrix, a camera matrix and an observation matrix, and then proposes a new compressed matrix sequence (CMS) method to reduce time complexity for matrix operations. Under the definition of CMS, all of the matrices are stored in a dense form to accelerate the matrix operations, of which the matrix operations are redefined as well. CMS costs little computation time to build sparse matrices or access sparse matrices. The experimental results show that CMS achieves a significant speedup over general sparse matrix storage formats. Also, CMS being insensitive to the data input is more stable.},
  archive      = {J_MVA},
  author       = {Peng, Jiaxin and Liu, Jie and Wei, Hua},
  doi          = {10.1007/s00138-021-01212-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A compressed matrix sequence method for solving normal equations of bundle adjustment},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-shot person re-identification based on appearance and
spatial-temporal cues in a large camera network. <em>MVA</em>,
<em>32</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01213-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is an important video analysis problem that aims to track people over non-overlapping views in a large camera network. The purpose is to find the same person from disjoint camera views at different times and locations. Most of the existing re-identification approaches assume pre-selected people bounding box captured by two non-overlapping cameras to perform the re-identification task while ignoring the inter-camera relationships in the network. In this paper, we propose a multi-shot person re-identification approach in a large camera network that relies on the appearance and spatial-temporal features. Firstly, we introduce a novel algorithm that selects a set of key appearance images depicting the different body postures from the target’s trajectory. Secondly, we model the appearance characteristics by extracting the multi-level semantic appearance representation descriptor that encodes both low- and mid-level appearance characteristics. Then, a dynamic gallery set is constructed containing the candidate people to whom the probe person can correspond based on his spatial-temporal characteristics. A novel camera network spatial-temporal graph is introduced to model the inter-camera relationships of the network in terms of entry/exit points, transition time, etc. The proposed approach has been experimentally validated on HDA+ and VIPeR datasets. The outcomes of this evaluation show promising results and demonstrate the effectiveness of our approach.},
  archive      = {J_MVA},
  author       = {Frikha, Mayssa and Fendri, Emna and Hammami, Mohamed},
  doi          = {10.1007/s00138-021-01213-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-shot person re-identification based on appearance and spatial-temporal cues in a large camera network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiplicative noise removal and blind inpainting of
ultrasound images based on a new variational framework. <em>MVA</em>,
<em>32</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01214-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting and denoising are two important preprocessing steps widely used in image and visual analysis. In this paper, by the maximum a posterior estimation, we present a new framework to remove multiplicative noise and artifacts simultaneously, when the locations of the artifacts/damaged pixels are unknown. By taking into account the statistical distribution of multiplicative noise as Gamma or Rayleigh noise, we give the special data fidelity term. To suppress the noise and repair the missing intensities, the proposed method applies spatial regularization to the desirable image, and $$\ell _{0}$$ norm regularization to the artifacts. We introduce three typical spatial regularization: total variation, second-order total generalized variation (TGV) and fractional-order total variation (FOTV) for smoothing images. Due to the non-convexity and non-differentiability of the proposed minimization problem, we introduce additional auxiliary variables to simplify the original problem, and then use the alternating direction method of multipliers to solve it. A set of experiments on synthetic images and real medical ultrasound images show that the proposed method can efficiently remove the multiplicative noise, and more importantly fill in the missing pixels very well. Compared to other similar method, the TGV and FOTV regularization can not only preserve edges and texture details of the image but also avoid the staircase effect.},
  archive      = {J_MVA},
  author       = {Dong, Fangfang and Li, Nannan},
  doi          = {10.1007/s00138-021-01214-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multiplicative noise removal and blind inpainting of ultrasound images based on a new variational framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effect of camera settings on image noise and accuracy of
subpixel image registration. <em>MVA</em>, <em>32</em>(4), 1–10. (<a
href="https://doi.org/10.1007/s00138-021-01215-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the accuracy of subpixel image registration algorithms in practical experiments is important, yet challenging in many applications. The accuracy of these algorithms in camera-based systems is influenced not only by image content, but also camera noise. In this study, five experiments were designed to quantify the effect of changing camera settings on the image noise level, and consequently on the accuracy of subpixel displacement measurements. The systematic errors of the algorithms were measured separately to provide estimates of their contributions to the total measurement error. Experiments were conducted to measure the effects of random noise on the accuracy of displacement measurements. The results showed that the camera parameters directly influenced the image noise level, but the accuracy of algorithms in measuring displacements was not proportional to the image noise level. These results provide information on the quantitative relationship between camera settings and the registration error for a camera-based measurement system.},
  archive      = {J_MVA},
  author       = {HajiRassouliha, Amir and Richardson, Samuel P. and Taberner, Andrew J. and Nash, Martyn P. and Nielsen, Poul M. F.},
  doi          = {10.1007/s00138-021-01215-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {The effect of camera settings on image noise and accuracy of subpixel image registration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive fast scale estimation, with accurate online model
update based on kernelized correlation filter. <em>MVA</em>,
<em>32</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01216-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the considerable advances that are emerged in correlation filter-based tracking, in fact, they may achieve excellent performance in robustness, speed, and accuracy; they still fail when dealing with large-scale alteration and show the inability to handle long-term tracking in complex scenarios, where the object undergoes partial occlusion, out-of-view, and deformation. In this paper, we propose a robust approach to address two important problems: the first one is scale estimation in kernelized correlation filter (KCF), and the second one is how to update the model in the process of tracking. We aim in this work to overcome the scale fixed size limitation of kernelized correlation filter-based tracking algorithms and improve the mechanism of model online training. Our approach learns a separate correlation filter to estimate the accurate target scale by finding the scale&#39;s candidate that maximizes the output response of the correlation filter mentioned above. Besides, we define a minimum rate of similarity for the online model update to avoid training with failure detections. Our approach is evaluated in terms of precision and accuracy, on a commonly used tracking benchmark with 100 challenging videos; the experimental results show that our proposed tracker outperforms the KCF algorithm and shows promising performance compared to state-of-the-art tracking methods.},
  archive      = {J_MVA},
  author       = {Chabane, Mohammed and Khelifa, Benahmed and Tariq, Benahmed},
  doi          = {10.1007/s00138-021-01216-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adaptive fast scale estimation, with accurate online model update based on kernelized correlation filter},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting vehicle collisions using data collected from
video games. <em>MVA</em>, <em>32</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01217-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a deep learning model for identifying dangerous vehicles requires a large amount of labeled accident data. However, it is difficult to collect a sufficient amount of accident data in the real world. To address this challenge, we introduce a driving-simulator-based data generator that can arbitrarily produce a wide variety of accident scenarios. Furthermore, in order to reduce the gap between synthetic data and real data, we propose a new domain adaptation algorithm that refines both features and labels. We conduct extensive real-data experiments to demonstrate that our dangerous vehicle classifier can reduce the missed detection rate by at least $$23.2\%$$ , as compared to those trained only with scarce real data, for an interested scenario in which time-to-collision is 1.6–1.8 s. We also find that our algorithm can identify various accident-related factors (such as wheel angles, vehicle orientations, and velocities of nearby vehicles) to enable high prediction accuracy for complex accident scenes.},
  archive      = {J_MVA},
  author       = {Kim, Hoon and Lee, Kangwook and Hwang, Gyeongjo and Suh, Changho},
  doi          = {10.1007/s00138-021-01217-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Predicting vehicle collisions using data collected from video games},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IC solder joint inspection via generator-adversarial-network
based template. <em>MVA</em>, <em>32</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01218-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic optical inspection is a vital part of the production process for solder joints appearance inspection in surface mounted technology assembling lines. However, IC solder joint inspection is a challenging problem because IC solder joints have extremely small sizes and no distinct appearance differences between qualified and unqualified ones. In this paper, we propose an IC solder joint inspection method via generator-adversarial-network based template. We are the first to introduce the GAN strategy into IC solder joint inspection. The method consists of GAN template generator training, offline statistical modelling and online real-time inspection. At the training stage, the GAN template generator is trained based on a designed GAN, which involves the feature maps in both of high-dimension and low-dimension spaces. Then, the binary difference image can be achieved by the input IC solder joint image and the corresponding GAN-based template. At the offline statistical modelling stage, to reduce the interferences, a pixel probability image is statistically modelled by the binary difference images corresponding to qualified IC solder joints. At the online real-time inspection stage, the potential defect pixels for the inspected IC solder joint can be shown in a defect salient image achieved by the multiplication of its corresponding binary difference image and the pixel probability image. Finally, we can accumulate the pixels in the defect salient image to distinguish the quality of the inspected IC solder joint. Experimental results show that the proposed method is superior to the state-of-the-art inspection methods with 0% omission rate and 0.15% error rate at a reasonable inspection speed of 4.32 ms per sample.},
  archive      = {J_MVA},
  author       = {Li, Jiaming and Cai, Nian and Mo, Zhuokun and Zhou, Guang and Wang, Han},
  doi          = {10.1007/s00138-021-01218-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {IC solder joint inspection via generator-adversarial-network based template},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Camera and inertial sensor fusion for the PnP problem:
Algorithms and experimental results. <em>MVA</em>, <em>32</em>(4), 1–11.
(<a href="https://doi.org/10.1007/s00138-021-01219-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we face the problem of estimating the relative position and orientation of a camera and an object, when they are both equipped with inertial measurement units (IMUs), and the object exhibits a set of n landmark points with known coordinates (the so-called Pose estimation or PnP Problem). We present two algorithms that, fusing the information provided by the camera and the IMUs, solve the PnP problem with good accuracy. These algorithms only use the measurements given by IMUs’ inclinometers, as the magnetometers usually give inaccurate estimates of the Earth magnetic vector. The effectiveness of the proposed methods is assessed by numerical simulations and experimental tests. The results of the tests are compared with the most recent methods proposed in the literature.},
  archive      = {J_MVA},
  author       = {D’Alfonso, Luigi and Garone, Emanuele and Muraca, Pietro and Pugliese, Paolo},
  doi          = {10.1007/s00138-021-01219-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Camera and inertial sensor fusion for the PnP problem: Algorithms and experimental results},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Research on 3D model reconstruction based on a sequence of
cross-sectional images. <em>MVA</em>, <em>32</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01220-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often difficult to obtain the high-precision inner cavity contour size and 3D model of parts and components in reverse engineering. This paper proposes a method that uses a sequence of section images of a part to reconstruct their 3D models. This method cuts the part layer by layer to obtain the sectional images and extracts the 3D information of the sectional image contours to generate point clouds. These point clouds are then used to reconstruct a 3D model of the part. High contrast material is used to embed the target part for pre-processing. A machining centre was used to mill the part layer by layer vertically to acquire high precision section profile images. The improved Canny edge detection operator was combined with the spatial moment sub-pixel subdivision algorithm to improve the edge detection accuracy. The camera imaging model algorithm transforms the coordinates of the image edge position to obtain a high-precision 3D point cloud of the part. The 3D solid model of the target part was obtained using NURBS surface reconstruction. The results show that the 3D model reconstruction method using the profile sequence of the cross-sectional images is independent of the complexity of the part’s structure and the complete internal structure of the part can be obtained. The proposed edge detection algorithm significantly refines the edge position of the contours in the cross-sectional image and the measurement accuracy was improved. This method improves the minimum deviation to 50 μm. The shape accuracy of roundness, cylindricity and perpendicularity of the structure is high. The proposed method can meet the reverse precision requirements in general precision machining.},
  archive      = {J_MVA},
  author       = {Dong, Zhiguo and Wu, Xiaobo and Ma, Zhipeng},
  doi          = {10.1007/s00138-021-01220-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Research on 3D model reconstruction based on a sequence of cross-sectional images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gabor capsule network with preprocessing blocks for the
recognition of complex images. <em>MVA</em>, <em>32</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s00138-021-01221-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule network (CapsNet) is a novel concept demonstrating the importance of learning spatial hierarchical relationship between features for the effective recognition of images. However, the baseline capsule network is not suitable for the recognition of complex images leading to its poor performance on such images. This limitation can partially be attributed to the inability of CapsNets to extract important features from the input images as well as the attempt to account for every object in the image including background objects. To address these problems, we propose a variant of a capsule network that is less complex yet robust with strong feature extraction capabilities. The model uses the advantages of Gabor filter and custom preprocessing block to learn the structure and semantic information in the image. This enhances the extraction of only important features, resulting in improved activation diagrams that enable meaningful hierarchical information to be learned. Experimental results show that the proposed model can achieve 85.24%, 68.17%, 94.78% and 91.50% test accuracies on complex images such as CIFAR 10, CIFAR 100, fashion-MNIST and kvasir-dataset-v2 datasets, respectively. The performance of the proposed model is comparable to that of the state-of-the-art models on the five datasets with a relatively small number of parameters.},
  archive      = {J_MVA},
  author       = {Abra Ayidzoe, Mighty and Yu, Yongbin and Mensah, Patrick Kwabena and Cai, Jingye and Adu, Kwabena and Tang, Yifan},
  doi          = {10.1007/s00138-021-01221-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Gabor capsule network with preprocessing blocks for the recognition of complex images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction to: A compressed matrix sequence method for
solving normal equations of bundle adjustment. <em>MVA</em>,
<em>32</em>(4), 1. (<a
href="https://doi.org/10.1007/s00138-021-01222-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correction to this paper has been published: https://doi.org/10.1007/s00138-021-01222-5},
  archive      = {J_MVA},
  author       = {Peng, Jiaxin and Liu, Jie and Wei, Hua},
  doi          = {10.1007/s00138-021-01222-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction to: A compressed matrix sequence method for solving normal equations of bundle adjustment},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised approach for thermal to visible image
translation using autoencoder and generative adversarial network.
<em>MVA</em>, <em>32</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s00138-021-01223-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thermal to visible image translation is essential for night-vision applications since images acquired during night-time using visible camera are relying on the amount of illumination present around the objects being observed. The poor lighting and/or illumination during night-time results into inadequate details in the acquired scene using the visible camera, and hence, they are no longer useful for high-end applications. The current research on image-to-image translation for day-time has achieved remarkable performance using deep learning methods. However, it is very challenging to obtain same performance for night-time images, especially for the situations when low/no sources of light are available. The existing state-of-the-art image-to-image methods suffer from lack of preservation of fine details and also with incorrect mapping for night-time images due to unavailability of better corresponding visible images. Therefore, a novel architecture is proposed here to provide better visual information in night-time scenarios using unsupervised training. It consists of generative adversarial networks (GANs) and Autoencoders with a newly proposed Residual Block to extract versatile features from thermal and visible images. In order to learn better visualization of night-time images, we also introduce the gradient-based loss function along with standard GAN and cycle consistency losses in the proposed method. A weight sharing concept is implied further to relate features of thermal and visible domains. The experimental validation of the proposed method implies committed qualitative improvement and quantitative performance in terms of no-reference quality metrics such as NIQE, BRISQUE, BIQAA and BLIINDS over the other existing methods. Such work could be useful to the many vision-based applications specifically for night-time situations including the surveillance systems at border.},
  archive      = {J_MVA},
  author       = {Patel, Heena and Upla, Kishor P.},
  doi          = {10.1007/s00138-021-01223-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An unsupervised approach for thermal to visible image translation using autoencoder and generative adversarial network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Images denoising for COVID-19 chest x-ray based on
multi-resolution parallel residual CNN. <em>MVA</em>, <em>32</em>(4),
1–15. (<a href="https://doi.org/10.1007/s00138-021-01224-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray (CXR) is a medical imaging technology that is common and economical to use in clinical. Recently, coronavirus (COVID-19) has spread worldwide, and the second wave is rebounding strongly now with the coming winter that has a detrimental effect on the global economy and health. To make pre-diagnosis of COVID-19 as soon as possible, and reduce the work pressure of medical staff, making use of deep learning networks to detect positive CXR images of infected patients is a critical step. However, there are complex edge structures and rich texture details in the CXR images susceptible to noise that can interfere with the diagnosis of the machines and the doctors. Therefore, in this paper, we proposed a novel multi-resolution parallel residual CNN (named MPR-CNN) for CXR images denoising and special application for COVID-19 which can improve the image quality. The core of MPR-CNN consists of several essential modules. (a) Multi-resolution parallel convolution streams are utilized for extracting more reliable spatial and semantic information in multi-scale features. (b) Efficient channel and spatial attention can let the network focus more on texture details in CXR images with fewer parameters. (c) The adaptive multi-resolution feature fusion method based on attention is utilized to improve the expression of the network. On the whole, MPR-CNN can simultaneously retain spatial information in the shallow layers with high resolution and semantic information in the deep layers with low resolution. Comprehensive experiments demonstrate that our MPR-CNN can better retain the texture structure details in CXR images. Additionally, extensive experiments show that our MPR-CNN has a positive impact on CXR images classification and detection of COVID-19 cases from denoised CXR images.},
  archive      = {J_MVA},
  author       = {Jiang, Xiaoben and Zhu, Yu and Zheng, Bingbing and Yang, Dawei},
  doi          = {10.1007/s00138-021-01224-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Images denoising for COVID-19 chest X-ray based on multi-resolution parallel residual CNN},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cultural behaviors analysis in video sequences.
<em>MVA</em>, <em>32</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s00138-021-01225-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the cultural aspects of different populations from video sequences. For that, we proposed a model that considers a series of characteristics of the pedestrians and the crowd, such as distances and speeds and performs the mapping of these characteristics in personalities, emotions, and cultural aspects. The model called Big4GD consists of four dimensions of geometric characteristics and seeks to describe the behavior of pedestrians and groups in the crowd. We performed a study of group behavior in a controlled experiment and focused on differences in two attributes that vary across cultures: (walking speed and personal distance) in three countries (India, Brazil, and Germany). We use the Fundamental Diagram theory that determines the relationship between the density and speed of individuals. We use Computer Vision methods to detect and track individuals through video sequences by generating their positions and speeds as a function of time. With these data, we analyze emergent walking speeds and densities while considering the personal distance of each individual and the neighbor in front of him/her. Our results show that human behavior is more similar in highly dense populations, i.e., individuals behave like a mass when presented with limited free personal space. The opposite result is also relevant: cultural differences can be observed at low and moderate densities, and such assumptions can be applied to computational interfaces and simulations, games, and movies. Besides, we present GeoMind, a software we developed to detect a series of characteristics from pedestrians. We also performed a practical case-study using GeoMind focusing on event detection in video sequences.},
  archive      = {J_MVA},
  author       = {Migon Favaretto, Rodolfo and de Andrade Araujo, Victor Flavio and Vilanova, Felipe and Brandelli Costa, Angelo and Raupp Musse, Soraia},
  doi          = {10.1007/s00138-021-01225-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cultural behaviors analysis in video sequences},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anomaly detection of core failures in die casting x-ray
inspection images using a convolutional autoencoder. <em>MVA</em>,
<em>32</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-021-01226-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Core failure inspection is an important issue in die casting. The inspection process is often carried out by manually examining X-ray images. However, human visual inspection suffers from individual biases and eye fatigues. Computer-vision-based automatic inspection, if it can achieve equal to or better than human performance, is favored to assist the inspectors to achieve better quality control. Most existing works are heavily relied on the supervised methods, which require enormous labeling and cannot be deployed quickly and economically. This is particularly difficult for a die casting plant that has many different types of products. Labeling each type of product before applying automated inspection may not be feasible in practice. It is therefore necessary to investigate unsupervised methods for die casting products. In this research, an inspection framework built on top of convolutional autoencoder (CAE) is designed and developed to inspect core failures from real-world die casting X-ray images in an unsupervised manner. Identification of good and scrap product, and localization of the defect are achieved in a single network. The framework is designed to be easily generalized to other image inspection scenarios. The area of interest for inspection is first extracted automatically through the Hough transformation. Then the preprocessed image is inspected by CAE. The noises of the model are removed using edge detection. It achieved an impressive 97.45% classification accuracy on average, and precisely pinpointed the defect regions with a small training set of 30 images.},
  archive      = {J_MVA},
  author       = {Tang, Weitao and Vian, Corey M. and Tang, Ziyang and Yang, Baijian},
  doi          = {10.1007/s00138-021-01226-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Anomaly detection of core failures in die casting X-ray inspection images using a convolutional autoencoder},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust information hiding algorithm based on lossless
encryption and NSCT-HD-SVD. <em>MVA</em>, <em>32</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01227-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of the security of secret information in various potential applications, we introduce a robust information hiding algorithm based on lossless encryption, non-subsampled contourlet transform (NSCT), Hessen-berg decomposition (HD) and singular value decomposition (SVD). Firstly, the carrier and secret mark information is transformed by NSCT-HD-SVD. Secondly, the singular score of secret media information is concealed in the carrier image. Thirdly, the text document is further concealed in the carrier marked image via pseudo magic cubes to achieve the final carrier marked image. Finally, the lossless encryption scheme is utilized to encrypt the final marked image. The simulation results of the proposed algorithm indicate good invisibility and robustness effect compared to existing schemes with high security and hiding efficiency. It indicates a considerable improvement in robustness of up to 96.36% over other schemes. Overall, the proposed algorithm for various images, achieved peak signal-to-noise ratio (PSNR), normalized correlation (NC), structural similarity index (SSIM), number of changing pixel rate (NPCR) and unified averaged changed intensity (UACI) of up to 67.36 dB, 0.9996, 1.0000, 0.9964 and 0.4005, respectively, indicating its effectiveness for secure media applications.},
  archive      = {J_MVA},
  author       = {Singh, O. P. and Singh, A. K.},
  doi          = {10.1007/s00138-021-01227-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A robust information hiding algorithm based on lossless encryption and NSCT-HD-SVD},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SA-SinGAN: Self-attention for single-image generation
adversarial networks. <em>MVA</em>, <em>32</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s00138-021-01228-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image training is a research hotspot task of generating adversarial networks, especially in tasks such as image editing and image coordination. However, the existing network has a series of problems such as a long training time, poor image quality, and an unstable training model. Based on the research hot issues, we propose a single-image generation adversarial network of the self-attention mechanism and discuss the changes of the model when the self-attention mechanism is placed in different positions of the generator. We introduced the spectral normalization in the generator and discriminator networks to stabilize the training process and compared the influence of the learning rate on the network. We used artificial vision and model evaluation methods to test the performance of the model on three representative datasets and compared with the current more advanced models. Experiments show that our proposed model has better performance than single-sample generative adversarial networks, reducing Single Image Fréchet Inception Distance (SIFID) from 4.80 to 2.057 on the challenging Generation datasets, reducing SIFID from 0.06 to 0.02 on the Places datasets, and reducing SIFID from 0.23 to 0.04 on the LSUN datasets. The training time of our model is one-ninth of the single-sample generation adversarial network, which can obtain the overall structure of the single training sample, which has great research significance.},
  archive      = {J_MVA},
  author       = {Chen, Xi and Zhao, Hongdong and Yang, Dongxu and Li, Yueyuan and Kang, Qing and Lu, Haiyan},
  doi          = {10.1007/s00138-021-01228-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SA-SinGAN: Self-attention for single-image generation adversarial networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DCNet: Dark channel network for single-image dehazing.
<em>MVA</em>, <em>32</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01173-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image dehazing is an extensively studied field and an ill-posed problem faced by vision-based systems in an outdoor environment. This paper proposes a dark channel network to estimate the transmission map of an input hazy scene for single-image dehazing. The architecture constitutes two major components—feature extraction layer and convolutional neural network layer. The former extracts the haze relevant features, while latter convolve these features with filter kernels to estimate the true scene transmission. Finally, the estimated transmission map is used to obtain the dehazed image using atmospheric scattering model. The experiments have been performed on synthetic hazy images and benchmark hazy dataset available in the literature. The performance of the proposed architecture outperforms the existing models in terms of standard quantitative metrics—mean square error, structural similarity index, and peak signal-to-noise ratio.},
  archive      = {J_MVA},
  author       = {Bhola, Akshay and Sharma, Teena and Verma, Nishchal K.},
  doi          = {10.1007/s00138-021-01173-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {DCNet: Dark channel network for single-image dehazing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). 3DTDesc: Learning local features using 2D and 3D cues.
<em>MVA</em>, <em>32</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01176-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise frame registration with sparse geometric local features on real-world depth images is not particularly robust due to the low resolution and incomplete nature of the 3D scan data. Moreover, there might be many regions with similar geometric information. In this paper, we present 3DTDesc, a data-driven descriptor which closely combines both 2D texture and 3D geometric information for frame registration. The proposed descriptor is learned directly from color point clouds, which is time-efficient and provides robust and accurate geometric feature matching in a variety of settings. The texture information and the geometric information closely interact in the fusing network, which are complements of each other in situations of textureless regions or regions with similar geometric information and different texture information. We also propose a multi-scale 3DTDesc to further improve the performance of the feature matching. The effectiveness and efficiency of our proposed 3DTDesc are demonstrated by extensive experimental results on challenging RGB-D datasets and various ablation studies.},
  archive      = {J_MVA},
  author       = {Xing, Xiaoxia and Cai, Yinghao and Lu, Tao and Yang, Yiping and Wen, Dayong},
  doi          = {10.1007/s00138-021-01176-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3DTDesc: Learning local features using 2D and 3D cues},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finger knuckle print recognition for personal authentication
based on relaxed local ternary pattern in an effective learning
framework. <em>MVA</em>, <em>32</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01178-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finger knuckle print (FKP) as a physiological trait with a small image dimension, also a highly distinctive pattern, can be used as a reliable biometric identifier. In this paper, a new effective biometric authentication system using FKP texture based on relaxed local ternary pattern (RLTP) is presented. To further improve performance, cascading, overlapped patching and uniform rotation invariant pattern selection are used. Also to obtain more discriminative dominant patterns, an efficient learning framework is integrated with RLTP feature vectors. Identification and verification experiments conducted on the standard PolyU FKP dataset show the effectiveness of the proposed scheme.},
  archive      = {J_MVA},
  author       = {Anbari, Mohammad and Fotouhi, Ali M.},
  doi          = {10.1007/s00138-021-01178-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Finger knuckle print recognition for personal authentication based on relaxed local ternary pattern in an effective learning framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local instance and context dictionary-based detection and
localization of abnormalities. <em>MVA</em>, <em>32</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s00138-021-01179-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies on contextual abnormality detection and localization for images and videos are presented in this work. The task of detecting abnormalities becomes challenging while considering the context in the scene. Some object which is normal in one scenario may be considered as abnormal in another. We present conceptually simple, flexible and a general framework, by incorporating instance segmentation, skip-gram with negative sampling and isolation forest for detecting and localizing contextual abnormality in images and videos. The skip-gram-based model is generally used for word2vec in natural language processing for finding the similarity between words. In this work, we extended them to detect the object-based abnormality in the images and video. Then we introduce the voting technique, which overcomes the variable-length feature vector issues; the decision of normal or abnormal object is based on this technique by considering the output from the isolation forest. We consider the anomalous events as scenarios having a different distribution from the normal settings such as a less frequently seen object in a given combination, the increase in the number of specific objects category, the object’s presence at unseen distance and occupancy of the out-of-vocabulary object. We observed that the proposed framework works in the proximity of multiple object categories and camera motion in the natural capture videos.},
  archive      = {J_MVA},
  author       = {Sharma, M. K. and Sheet, D. and Biswas, P. K.},
  doi          = {10.1007/s00138-021-01179-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Local instance and context dictionary-based detection and localization of abnormalities},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion-aware ensemble of three-mode trackers for unmanned
aerial vehicles. <em>MVA</em>, <em>32</em>(3), 1–12. (<a
href="https://doi.org/10.1007/s00138-021-01181-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle problems arising from unexpected camera motions in unmanned aerial vehicles (UAVs), we propose a three-mode ensemble tracker where each mode specializes in distinctive situations. The proposed ensemble tracker is composed of appearance-based tracking mode, homography-based tracking mode, and momentum-based tracking mode. The appearance-based tracking mode tracks a moving object well when the UAV is nearly stopped, whereas the homography-based tracking mode shows good tracking performance under smooth UAV or object motion. The momentum-based tracking mode copes with large or abrupt motion of either the UAV or the object. We evaluate the proposed tracking scheme on a widely-used UAV123 benchmark dataset. The proposed motion-aware ensemble shows a 5.3% improvement in average precision compared to the baseline correlation filter tracker, which effectively employs deep features while achieving a tracking speed of at least 80fps in our experimental settings. In addition, the proposed method outperforms existing real-time correlation filter trackers.},
  archive      = {J_MVA},
  author       = {Lee, Kyuewang and Chang, Hyung Jin and Choi, Jongwon and Heo, Byeongho and Leonardis, Aleš and Choi, Jin Young},
  doi          = {10.1007/s00138-021-01181-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Motion-aware ensemble of three-mode trackers for unmanned aerial vehicles},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepROD: A deep learning approach for real-time and online
detection of a panic behavior in human crowds. <em>MVA</em>,
<em>32</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01182-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting a panic behavior within a human crowd is of a high importance as it allows to prevent disasters. Online analysis of video streams, real-time processing and accurate detection are required to ensure effective surveillance of the crowded places. However, these requirements are not simultaneously fulfilled by the existing techniques. Rapid advances in artificial intelligence are investing the power for automatic public surveillance and timely detection of a possible abnormal behavior. Thus, the aim of the present work is to propose an online, real-time and effective technique for panic behavior detection. It relies on a handcrafted feature that accounts for the characteristics of the crowd to understand people behaviors and a long short-term memory neural network to predict future feature values. Experiments are performed on well-known datasets of panic situations to evaluate the performance and accuracy of the proposed algorithm. Results show the system yields excellent performances in terms of accuracy and processing time with respect to the state of the art techniques.},
  archive      = {J_MVA},
  author       = {Ammar, Heyfa and Cherif, Asma},
  doi          = {10.1007/s00138-021-01182-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {DeepROD: A deep learning approach for real-time and online detection of a panic behavior in human crowds},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic detection of multi-crossing crack defects in
multi-crystalline solar cells based on machine vision. <em>MVA</em>,
<em>32</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s00138-021-01183-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of defects in solar cells based on machine vision has become the main direction of current development, but the graphical feature extraction of micro-cracks, especially cracks with complex shapes, still faces formidable challenges due to the difficulties associated with the complex background, non-uniform texture, and poor contrast between crack defects and background. In this paper, a novel detection scheme based on machine vision to detect multi-crossing cracks for multi-crystalline solar cells was proposed. First, faced with periodic noise, we improved the filter method in the frequency domain and eliminated the background interference of fingers by filtering out the periodic noise while retaining the integrity of the crack signal. To address the anisotropy of multi-crossing cracks, we designed a special grid-shaped, convolution kernel filter to accurately extract crack features at low contrast and in the presence of a complex textured background. Finally, to address the missing features from the central region of multi-crossing cracks, we designed a method based on the orientation information of mask pattern to implement feature reconstruction for the central region of the crack. The experimental results showed that, compared to other crack detection methods, the strategy designed herein exhibited a better detection performance and stronger robustness.},
  archive      = {J_MVA},
  author       = {Fu, Yongzhong and Ma, Xiaolong and Zhou, Hang},
  doi          = {10.1007/s00138-021-01183-9},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic detection of multi-crossing crack defects in multi-crystalline solar cells based on machine vision},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of a hybrid deep learning system for discriminating
between low- and high-grade colorectal cancer lesions, using microscopy
images of IHC stained for AIB1 expression biopsy material. <em>MVA</em>,
<em>32</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s00138-021-01184-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To design a hybrid deep learning system (hDL-system) for discriminating low-grade from high-grade colorectal cancer (CRC) lesions, using immunohistochemically stained biopsy specimens for AIB1 expression. AIB1 has oncogenic function in tumour genesis, and it is an important prognostic factor regarding various types of cancers, including CRC. Clinical material consisted of biopsy specimens of sixty-seven patients with verified CRC (26 low-grade, 41 high-grade cases). From each patient, we digitized images, at × 50 and × 200 lens magnifications. We designed the hDL-system, employing the VGG16 pre-trained convolution neural network for generating DL-features, the SVM classifier, and the bootstrap evaluation method for assessing the discrimination accuracy between low-grade and high-grade CRC lesions. Furthermore, we compared the hDL-system’s discrimination accuracy with that of a supervised machine learning system (sML-system). We designed the sML-system by (i) generating sixty-nine (69) textural and colour features from each image, (ii) employing the probabilistic neural network (PNN) classifier, and (iii) using the bootstrapping method for evaluating sML-system performance. The system design was enabled by employing the CUDA platform for programming in parallel the multiprocessors of the Nvidia graphics processing unit card. The hDL-system provided the highest discrimination accuracy of 99.1% using the × 200 lens magnification images as compared to the 92.5.% best accuracy achieved by the sML-system, employing both the × 50 and × 200 lens magnification images. Our results showed that the hDL-system was superior to the sML-system (i) in discriminating low-grade from high-grade CRC-lesions and (ii) by requiring fewer images for its best design, only those at the × 200 lens magnification. The sML-system by employing textural and colour features in its design revealed that high-grade CRC lesions are characterized by (i) loss in the definition of structures, (ii) coarser texture in larger structures, (iii) hazy formless texture, (iv) lower AIB1 uptake, (v) lower local correlation and (vi) slower varying image contrast.},
  archive      = {J_MVA},
  author       = {Theodosi, Angeliki and Ouzounis, Sotiris and Kostopoulos, Spiros and Glotsos, Dimitris and Kalatzis, Ioannis and Tzelepi, Vassiliki and Ravazoula, Panagiota and Asvestas, Pantelis and Cavouras, Dionisis and Sakellaropoulos, George},
  doi          = {10.1007/s00138-021-01184-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Design of a hybrid deep learning system for discriminating between low- and high-grade colorectal cancer lesions, using microscopy images of IHC stained for AIB1 expression biopsy material},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple convolutional features in siamese networks for
object tracking. <em>MVA</em>, <em>32</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01185-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese trackers demonstrated high performance in object tracking due to their balance between accuracy and speed. Unlike classification-based CNNs, deep similarity networks are specifically designed to address the image similarity problem and thus are inherently more appropriate for the tracking task. However, Siamese trackers mainly use the last convolutional layers for similarity analysis and target search, which restricts their performance. In this paper, we argue that using a single convolutional layer as feature representation is not an optimal choice in a deep similarity framework. We present a Multiple Features-Siamese Tracker (MFST), a novel tracking algorithm exploiting several hierarchical feature maps for robust tracking. Since convolutional layers provide several abstraction levels in characterizing an object, fusing hierarchical features allows to obtain a richer and more efficient representation of the target. Moreover, we handle the target appearance variations by calibrating the deep features extracted from two different CNN models. Based on this advanced feature representation, our method achieves high tracking accuracy, while outperforming the standard siamese tracker on object tracking benchmarks.The source code and trained models are available at https://github.com/zhenxili96/MFST .},
  archive      = {J_MVA},
  author       = {Li, Zhenxi and Bilodeau, Guillaume-Alexandre and Bouachir, Wassim},
  doi          = {10.1007/s00138-021-01185-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multiple convolutional features in siamese networks for object tracking},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end annotation-free machine vision system for
detection of products on the rack. <em>MVA</em>, <em>32</em>(3), 1–13.
(<a href="https://doi.org/10.1007/s00138-021-01186-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given single instance (or template image) per product, our objective is to detect merchandise displayed in the images of racks available in a supermarket. Our end-to-end solution consists of three consecutive modules: exemplar-driven region proposal, classification followed by non-maximal suppression of the region proposals. The two-stage exemplar-driven region proposal works with the example or template of the product. The first stage estimates the scale between the template images of products and the rack image. The second stage generates proposals of potential regions using the estimated scale. Subsequently, the potential regions are classified using convolutional neural network. The generation and classification of region proposal do not need annotation of rack image in which products are recognized. In the end, the products are identified removing ambiguous overlapped region proposals using greedy non-maximal suppression. Extensive experiments are performed on one in-house dataset and three publicly available datasets: Grocery Products, WebMarket and GroZi-120. The proposed solution outperforms the competing approaches improving up to around $$4\%$$ detection accuracy. Moreover, in the repeatability test, our solution is found to be better compared to state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Santra, Bikash and Shaw, Avishek Kumar and Mukherjee, Dipti Prasad},
  doi          = {10.1007/s00138-021-01186-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An end-to-end annotation-free machine vision system for detection of products on the rack},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual metric learning for video anomaly detection.
<em>MVA</em>, <em>32</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s00138-021-01187-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a new approach to localize anomalies in surveillance video. The main novelty is the idea of using a Siamese convolutional neural network to learn a metric between a pair of video patches (spatiotemporal regions of video). The learned metric, which is not specific to the target video, is used to measure the perceptual distance between each video patch in the testing video and the video patches found in normal training video. If a testing video patch is far from all normal video patches, then it must be anomalous. We further generalize the approach from operating on video patches from a fixed grid to arbitrary-sized region proposals. We compare our approaches to previously published algorithms using four evaluation measures and three challenging target benchmark datasets. Experiments show that our approaches either surpass or perform comparably to current state-of-the-art methods while enjoying other favorable properties.},
  archive      = {J_MVA},
  author       = {Ramachandra, Bharathkumar and Jones, Michael and Vatsavai, Ranga Raju},
  doi          = {10.1007/s00138-021-01187-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Perceptual metric learning for video anomaly detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Water–air imaging: Distorted image reconstruction based on a
twice registration algorithm. <em>MVA</em>, <em>32</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01188-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image will be seriously distorted when camera captures the scene through water surface. This will cause the captured image to lose its readability. Although several previous methods have been proposed to resolve this problem, the methods still need improvement. In this paper, we propose a reconstruction method to restore the distorted images based on a twice registration algorithm. Several calculation methods will be applied to update reference image in different registration phase. Firstly, the method uses the mean image of the image sequence as the reference image for the first registration. Due to the reference image may be blurred, a deblurring step has been applied in this paper. To register the distorted image sequence, a non-rigid registration method has been employed based on B-spline method. In addition, a patch search method also is proposed to update the reference image in the second registration because reference image is the key to improve the accuracy of image registration. It is designed to obtain the most similar patches from the image sequence. Compared with the previous methods using the existing database and generated database, the results show that our method performs better than the state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Cai, Chengtao and Meng, Haiyang and Qiao, Renjie and Wang, Feng},
  doi          = {10.1007/s00138-021-01188-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Water–air imaging: Distorted image reconstruction based on a twice registration algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-resolution depth image restoration. <em>MVA</em>,
<em>32</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-021-01189-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth degradation caused by the conditions and environment of depth sensor hardware restricts its application potential, and this limitation cannot be avoided simply by improving the design of sensor. To overcome this limitation, we propose a multi-resolution depth image restoration method. Firstly, the sub-images of depth image and color image at different scales are obtained by multi-resolution analysis based on two-dimensional discrete wavelet transform. The multi-resolution joint bilateral filtering is then applied to the approximation low-frequency sub-image of the decomposed image. At the same time, using color-guided filtering method to restore high-frequency sub-images can effectively suppress edge artifacts without adding extra time burden. The high-quality output image is finally reconstructed using two-dimensional inverse discrete wavelet transform. A color guide image with rich edge information is introduced into the depth sub-image restoration to improve the depth image edge detail. Extensive experiments with synthetic and real datasets demonstrate that the proposed algorithm can effectively reduce additive Gaussian noise without losing sharp details in the noisy images and reduce the time consumption of depth image restoration.},
  archive      = {J_MVA},
  author       = {Zhang, Yue and Liu, Zhenfang and Huang, Min and Zhu, Qibing and Yang, Bao},
  doi          = {10.1007/s00138-021-01189-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-resolution depth image restoration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In color constancy: Data mattered more than network.
<em>MVA</em>, <em>32</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s00138-021-01190-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to argue that data mattered more than network in terms of color constancy. Computational color constancy is a linear operation device-dependent, which is part of the camera imaging pipeline. We extend the dataset based on this pipeline and prove that the scene illumination can be predicted using a very simple network as long as the dataset is large enough and evenly distributed. In the process of expanding the dataset, firstly, we remove illumination color casts in images which is ground-truth illumination color and then casts randomly generated evenly distributed illumination colors in images. We randomly generate five labels for each image and then work on the image to obtain this dataset. Using this dataset, we introduce a very simple network that is able to compute the color mapping function to correct the image’s colors. Experiments on our new datasets demonstrate that the method of this paper significantly outperforms the state-of-the-art color constancy methods.},
  archive      = {J_MVA},
  author       = {Du, Zhuo-Ming and Li, Hong-An and Fan, Xin-Yi},
  doi          = {10.1007/s00138-021-01190-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {In color constancy: Data mattered more than network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TsanKit: Artificial intelligence for solder ball
head-in-pillow defect inspection. <em>MVA</em>, <em>32</em>(3), 1–17.
(<a href="https://doi.org/10.1007/s00138-021-01192-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an AI (Artificial Intelligence) solution for solder ball HIP (Head-In-Pillow) defect inspection. The HIP defect will affect the conductivity of the solder balls leading to intermittent failures. Due to the variable location and shape of the HIP defect, traditional machine vision algorithms cannot solve the problem completely. In recent years, Convolutional Neural Network (CNN) has an outstanding performance in image recognition and classification, but it is easy to cause overfitting problems due to insufficient data. Therefore, we combine CNN and the machine learning algorithm Support Vector Machine (SVM) to design our inspection process. Referring to the advantages of several state-of-the-art models, we propose our 3D CNN model and adopt focal loss as well as triplet loss to solve the data imbalance problem caused by rare defective data. Our inspection method has the best performance and fast testing speed compared with several classic CNN models and the deep learning inspection software SuaKIT.},
  archive      = {J_MVA},
  author       = {Tsan, Ting-Chen and Shih, Teng-Fu and Fuh, Chiou-Shann},
  doi          = {10.1007/s00138-021-01192-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {TsanKit: Artificial intelligence for solder ball head-in-pillow defect inspection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and efficient object reconstructions from closed loop
sequences. <em>MVA</em>, <em>32</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s00138-021-01193-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new hierarchical structure from motion system for close loop sequences. Our system includes a novel approach for clustering cameras into multiple sets, whose camera poses are initially reconstructed separately and later globally registered w.r.t a single coordinate frame. Each of the multiple sets is robustly reconstructed by a novel guarded least median square protocol. Our method is accelerated by reducing the parameter space of bundle adjustment in the local reconstruction optimizations. We also propose a new synthetic dataset that could be useful in 3D object reconstruction problems. Extensive experiments with both synthetic and real data were carried out to validate our method. Our system presented better results than ACTS and GPE in terms of rotation and translation errors in the camera pose estimations, and the accuracy is quite close to COLMAP while our method is much faster.},
  archive      = {J_MVA},
  author       = {Han, Kyung Min and Rueda, Antonio J.},
  doi          = {10.1007/s00138-021-01193-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Robust and efficient object reconstructions from closed loop sequences},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A data independent approach to generate adversarial patches.
<em>MVA</em>, <em>32</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s00138-021-01194-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are vulnerable to adversarial examples, i.e., carefully perturbed inputs designed to mislead the network at inference time. Recently, adversarial patch, with perturbations confined to a small and localized patch, emerged for its easy accessibility in real-world attack. However, existing attack strategies require training data on which the deep neural networks were trained, which makes them unsuitable for practical attacks since it is unreasonable for an attacker to obtain the training data. In this paper, we propose a data independent approach to generate adversarial patches (DiAP). The goal is to craft adversarial patches that can fool the target model on most of the images without any knowledge about the training data distribution. In the absence of data, we carry out non-targeted attacks by fooling the features learned at multiple layers of the deep neural network, and then employ the potential information of non-targeted adversarial patches to craft targeted adversarial patches. Extensive experiments demonstrate impressive attack success rates for DiAP. Particularly in the blackbox setting, DiAP outperforms state-of-the-art adversarial patch attack methods. The patches generated by DiAP also function well in real physical scenarios, and could be created offline and then broadly shared.},
  archive      = {J_MVA},
  author       = {Zhou, Xingyu and Pan, Zhisong and Duan, Yexin and Zhang, Jin and Wang, Shuaihui},
  doi          = {10.1007/s00138-021-01194-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A data independent approach to generate adversarial patches},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aluminum casting inspection using deep object detection
methods and simulated ellipsoidal defects. <em>MVA</em>, <em>32</em>(3),
1–16. (<a href="https://doi.org/10.1007/s00138-021-01195-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the automotive industry, light-alloy aluminum castings are an important element for determining roadworthiness. X-ray testing with computer vision is used during automated inspections of aluminum castings to identify defects inside of the test object that are not visible to the naked eye. In this article, we evaluate eight state-of-the-art deep object detection methods (based on YOLO, RetinaNet, and EfficientDet) that are used to detect aluminum casting defects. We propose a training strategy that uses a low number of defect-free X-ray images of castings with superimposition of simulated defects (avoiding manual annotations). The proposed solution is simple, effective, and fast. In our experiments, the YOLOv5s object detector was trained in just 2.5 h, and the performance achieved on the testing dataset (with only real defects) was very high (average precision was 0.90 and the $$F_1$$ factor was 0.91). This method can process 90 X-ray images per second, i.e. ,this solution can be used to help human operators conduct real-time inspections. The code and datasets used in this paper have been uploaded to a public repository for future studies. It is clear that deep learning-based methods will be used more by the aluminum castings industry in the coming years due to their high level of effectiveness. This paper offers an academic contribution to such efforts.},
  archive      = {J_MVA},
  author       = {Mery, Domingo},
  doi          = {10.1007/s00138-021-01195-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Aluminum casting inspection using deep object detection methods and simulated ellipsoidal defects},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A stacked dense denoising–segmentation network for
undersampled tomograms and knowledge transfer using synthetic tomograms.
<em>MVA</em>, <em>32</em>(3), 1–22. (<a
href="https://doi.org/10.1007/s00138-021-01196-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent years, many approaches have been proposed for the denoising or semantic segmentation of X-ray computed tomography (CT) scans. In most cases, high-quality CT reconstructions are used; however, such reconstructions are not always available. When the X-ray exposure time has to be limited, undersampled tomograms (in terms of their component projections) are attained. This low number of projections offers low-quality reconstructions that are difficult to segment. Here, we consider CT time-series (i.e. 4D data), where the limited time for capturing fast-occurring temporal events results in the time-series tomograms being necessarily undersampled. Fortunately, in these collections, it is common practice to obtain representative highly sampled tomograms before or after the time-critical portion of the experiment. In this paper, we propose an end-to-end network that can learn to denoise and segment the time-series’ undersampled CTs, by training with the earlier highly sampled representative CTs. Our single network can offer two desired outputs while only training once, with the denoised output improving the accuracy of the final segmentation. Our method is able to outperform state-of-the-art methods in the task of semantic segmentation and offer comparable results in regard to denoising. Additionally, we propose a knowledge transfer scheme using synthetic tomograms. This not only allows accurate segmentation and denoising using less real-world data, but also increases segmentation accuracy. Finally, we make our datasets, as well as the code, publicly available.},
  archive      = {J_MVA},
  author       = {Bellos, Dimitrios and Basham, Mark and Pridmore, Tony and French, Andrew P.},
  doi          = {10.1007/s00138-021-01196-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A stacked dense denoising–segmentation network for undersampled tomograms and knowledge transfer using synthetic tomograms},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic image co-segmentation: A survey. <em>MVA</em>,
<em>32</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s00138-021-01197-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image co-segmentation is important for its advantage of alleviating the ill-pose nature of image segmentation through exploring the correlation between related images. Many automatic image co-segmentation algorithms have been developed in the last decade, which are investigated comprehensively in this paper. We firstly analyze visual/semantic cues for guiding image co-segmentation, including object cues and correlation cues. Then, we describe the traditional methods in three categories of object elements based, object regions/contours based, common object model based. In the next part, deep learning-based methods are reviewed. Furthermore, widely used test datasets and evaluation criteria are introduced and the reported performances of the surveyed algorithms are compared with each other. Finally, we discuss the current challenges and possible future directions and conclude the paper. Hopefully, this comprehensive investigation will be helpful for the development of image co-segmentation technique.},
  archive      = {J_MVA},
  author       = {Liu, Xiabi and Duan, Xin},
  doi          = {10.1007/s00138-021-01197-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic image co-segmentation: A survey},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel ship classification network with cascade deep
features for line-of-sight sea data. <em>MVA</em>, <em>32</em>(3), 1–15.
(<a href="https://doi.org/10.1007/s00138-021-01198-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ship classification, selecting distinctive features and designing a proper classifier are two key points of the process. As a lack of most of the studies, these two essential points are considered separately. In this study, our proposal includes joint feature extraction, selection, and classifier design framework to build a novel deep cascade network for ship classification. We propose a transfer learning-based deep feature extraction using cascade Convolutional Neural Network architecture to convert the input image to multi-dimensional feature maps. The distributions of the MUTual Information (MUTInf) based feature selection algorithm compose a distinctive feature set originated for a public ship imagery dataset. The dataset consists of five specific classes of ships most existed in the maritime domain. A quadratic kernel-based non-linear Support Vector Machine is the designed classifier. Extensive experiments on the benchmark dataset indicate that the proposed framework can integrate the optimal feature set and a well-designed classifier to increase the performance of the classification process in ship imagery. In the experiments, the proposed method achieves an overall accuracy of 95.06%. The ship classes are also performed high classification performances into cargo, military, carrier, cruise, and tanker with an accuracy of 88.26%, 98.38%, 98.38%, 98.78%, and 91.50%, respectively. In addition, MUTInf feature selection reduces the features at a rate of 50.04%. These results show that the proposed method provides the highest performance value with less number of elements and outperforms state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Ucar, Ferhat and Korkmaz, Deniz},
  doi          = {10.1007/s00138-021-01198-2},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel ship classification network with cascade deep features for line-of-sight sea data},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maritime vessel re-identification: Novel VR-VCA dataset and
a multi-branch architecture MVR-net. <em>MVA</em>, <em>32</em>(3), 1–14.
(<a href="https://doi.org/10.1007/s00138-021-01199-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maritime vessel re-identification (re-ID) is a computer vision task of vessel identity matching across disjoint camera views. Prominent applications of vessel re-ID exist in the fields of surveillance and maritime traffic flow analysis. However, the field suffers from the absence of a large-scale dataset that enables training of deep learning models. In this study, we present a new dataset that includes 4614 images of 729 vessels along with 5-bin orientation and 8-class vessel-type annotations to promote further research. A second contribution of this study is the baseline re-ID analysis of our new dataset. Performances of 10 recent deep learning architectures are quantitatively compared to reveal the best practices. Lastly, we propose a novel multi-branch deep learning architecture, Maritime Vessel Re-ID network (MVR-net), to address the challenging problem of vessel re-ID. Evaluation of our approach on the new dataset yields 74.5% mAP and 77.9% Rank-1 score, providing a performance increase of 5.7% mAP and 5.0% Rank-1 over the best-performing baseline. MVR-net also outperforms the PRN (a pioneering vehicle re-ID network), by 2.9% and 4.3% higher mAP and Rank-1, respectively.},
  archive      = {J_MVA},
  author       = {Ghahremani, Amir and Alkanat, Tunc and Bondarev, Egor and de With, Peter H. N.},
  doi          = {10.1007/s00138-021-01199-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Maritime vessel re-identification: Novel VR-VCA dataset and a multi-branch architecture MVR-net},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction to: 3DTDesc: Learning local features using 2D
and 3D cues. <em>MVA</em>, <em>32</em>(3), 1. (<a
href="https://doi.org/10.1007/s00138-021-01200-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correction to this paper has been published: https://doi.org/10.1007/s00138-021-01200-x},
  archive      = {J_MVA},
  author       = {Xing, Xiaoxia and Cai, Yinghao and Lu, Tao and Yang, Yiping and Wen, Dayong},
  doi          = {10.1007/s00138-021-01200-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction to: 3DTDesc: learning local features using 2D and 3D cues},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Plug-and-play video reconstruction using sparse 3D
transform-domain block matching. <em>MVA</em>, <em>32</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01201-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel video reconstruction methodology built based on a generalization of alternating direction method of multipliers (ADMM) named Plug-and-Play. The motivation of the proposed technique is the improvement in visual quality performance of the video frames and decreasing the reconstruction error in comparison with the former video reconstruction methods. The proposed algorithm is an end-to-end embedding tool to integrate video reconstruction techniques with denoiser methods. Correspondingly, we use compressive sensing (CS)-based Gaussian mixture models (GMM) as a sub-problem regarding the proposed framework which is used as a method to model spatiotemporal video patches for video reconstruction. On the other hand, sparse 3D transform-domain block matching is applied as the denoiser of the proposed methodology to remove the remaining artifacts and noise in the reconstructed video frames. Consequently, by considering both online and offline CS-based GMM frameworks, we are able to make two forms of GMM-based embedding video reconstruction algorithms. The outcome has been compared with the result of CS-based GMM algorithm, GAP, TwIST and KSVD-OMP on the same datasets considering PSNR, SSIM, VSNR, WSNR, NQM, UQI, VIF and IFC as the evaluation metrics. It has been experimentally proved that the proposed online and offline GMM-based Plug-and-Play algorithms have more suitable results in comparison with their conventional CS-based online and offline GMM counterparts as well as other state-of-the-art techniques. The general quantitative results (considering all the datasets) for online proposed method regarding PSNR and SSIM metrics are 29.84 and 0.891, respectively, which is higher than the results of other techniques.},
  archive      = {J_MVA},
  author       = {Khorasani Ghassab, Vahid and Bouguila, Nizar},
  doi          = {10.1007/s00138-021-01201-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Plug-and-play video reconstruction using sparse 3D transform-domain block matching},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Next-best-view regression using a 3D convolutional neural
network. <em>MVA</em>, <em>32</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01166-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated three-dimensional (3D) object reconstruction is the task of building a geometric representation of a physical object by means of sensing its surface. Even though new single-view reconstruction techniques can predict the surface, they lead to incomplete models, specially, for non-commons objects such as antique objects or art sculptures. Therefore, to achieve the task’s goals, it is essential to automatically determine the locations where the sensor will be placed so that the surface will be completely observed. This problem is known as the next-best-view problem. In this paper, we propose a data-driven approach to address the problem. The proposed approach trains a 3D convolutional neural network (3D CNN) with previous reconstructions in order to regress the position of the next-best-view. To the best of our knowledge, this is one of the first works that directly infers the next-best-view in a continuous space using a data-driven approach for the 3D object reconstruction task. We have validated the proposed approach making use of two groups of experiments. In the first group, several variants of the proposed architecture are analyzed. Predicted next-best-views were observed to be closely positioned to the ground truth. In the second group of experiments, the proposed approach is requested to reconstruct several unseen objects, namely, objects not considered by the 3D CNN during training nor validation. Coverage percentages of up to 90 % were observed. With respect to current state-of-the-art methods, the proposed approach improves the performance of previous next-best-view classification approaches and it is quite fast in running time (3 frames per second), given that it does not compute the expensive ray tracing required by previous information metrics.},
  archive      = {J_MVA},
  author       = {Vasquez-Gomez, J. Irving and Troncoso, David and Becerra, Israel and Sucar, Enrique and Murrieta-Cid, Rafael},
  doi          = {10.1007/s00138-020-01166-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Next-best-view regression using a 3D convolutional neural network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd density classification method based on pixels and
texture features. <em>MVA</em>, <em>32</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s00138-021-01167-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd density classification has been a challenging task in the field of computer vision, which has various applications in public and commercial domains. Many researches on the classification and recognition method of the crowd density have been introduced in the past, while there still exists the problems of inaccuracy, poor robustness and inefficiency. An adaptive crowd density classification method based on pixels and texture features is proposed in this paper. Core part of the method is to adopt different processing methods according to the corresponding crowd density. The method based on pixel regression method is used for the sparse crowd condition, while the texture features are applied in the dense crowd. Variety of texture features like local binary pattern (LBP), gray-level co-occurrence matrix (GLCM), Gabor, Haar-like and Wavelet group are used on the WorldExpo’10 dataset to obtain an optimum combination of these features, which is proposed to extract the texture features of the crowd images. Then the SVM classifier model based on Bayesian estimation is adopted to train the model which can filter the abnormal sample data to improve the accuracy and generalization performance of the algorithm. Meanwhile, a K-means clustering iterative training method based on optimized sorting samples is designed to improve the training speed in the training process. Extensive experiments from various aspects including parameter optimization, feature selection and model evaluation were conducted. The performance of the model is tested based on mean absolute error (MAE), mean squared error (MSE) and classification rate (CR) in dataset UCSD, Shanghai Tech_A and UCF_CC_50. The experimental results show that CR of the proposed method can reach to 98.2%, whose indexes of MAE and MSE also outperform the most existing methods. In general, the proposed approach in this paper has obvious advantages and great application value.},
  archive      = {J_MVA},
  author       = {Jia, Dongyao and Zhang, Chuanwang and Zhang, Bing},
  doi          = {10.1007/s00138-021-01167-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Crowd density classification method based on pixels and texture features},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recognition of varying size scene images using semantic
analysis of deep activation maps. <em>MVA</em>, <em>32</em>(2), 1–19.
(<a href="https://doi.org/10.1007/s00138-021-01168-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the complex semantic structure of scene images requires mapping the image from pixel space to high-level semantic space. In semantic space, a scene image is represented by the posterior probabilities of concepts (e.g., ‘car,’ ‘chair,’ ‘window,’ etc.) present in it and such representation is known as semantic multinomial (SMN) representation. SMN generation requires a concept annotated dataset for concept modeling which is infeasible to generate manually due to the large size of databases. To tackle this issue, we propose a novel approach of building the concept model via pseudo-concepts. Pseudo-concept acts as a proxy for the actual concept and gives the cue for its presence instead of actual identity. We propose to use filter responses from deeper convolutional layers of convolutional neural networks (CNNs) as pseudo-concepts, as filters in deeper convolutional layers are trained for different semantic concepts. Most of the prior work considers fixed-size ( $$\approx $$ 227 $$\times $$ 227) images for semantic analysis which suppresses many concepts present in the images. In this work, we preserve the true-concept structure in images by passing in their original resolution to convolutional layers of CNNs. We further propose to prune the non-prominent pseudo-concepts, group the similar one using kernel clustering and later model them using a dynamic-based support vector machine. We demonstrate that resulting SMN representation indeed captures the semantic concepts better and results in state-of-the-art classification accuracy on varying size scene image datasets such as MIT67 and SUN397.},
  archive      = {J_MVA},
  author       = {Gupta, Shikha and Dileep, A. D. and Thenkanidiyoor, Veena},
  doi          = {10.1007/s00138-021-01168-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Recognition of varying size scene images using semantic analysis of deep activation maps},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight convolutional neural network-based pedestrian
detection and re-identification in multiple scenarios. <em>MVA</em>,
<em>32</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s00138-021-01169-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection and re-identification technology is a research hotspot in the field of computer vision. This technology currently has issues such as insufficient pedestrian expression ability, occlusion, diverse pedestrian attitude, and difficulty of small-scale pedestrian detection. In this paper, we proposed an end-to-end pedestrian detection and re-identification model in real scenes, which can effectively solve these problems. In our model, the original images are processed with a non-overlapped image blocking data augmentation method, and then input them into the YOLOv3 detector to obtain the object position information. LCNN-based pedestrian re-identification model is used to extract the features of the object. Furthermore, the eigenvectors of the object and the detected pedestrians are calculated, and the similarity between them are used to determine whether they can be marked as target pedestrians. Our method is lightweight and end-to-end, which can be applied to the real scenes.},
  archive      = {J_MVA},
  author       = {Ke, Xiao and Lin, Xinru and Qin, Liyun},
  doi          = {10.1007/s00138-021-01169-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Lightweight convolutional neural network-based pedestrian detection and re-identification in multiple scenarios},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). List-wise learning-to-rank with convolutional neural
networks for person re-identification. <em>MVA</em>, <em>32</em>(2),
1–14. (<a href="https://doi.org/10.1007/s00138-021-01170-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel machine learning-based image ranking approach using Convolutional Neural Networks (CNN). Our proposed method relies on a similarity metric learning algorithm operating on lists of image examples and a loss function taking into account the ranking in these lists with respect to different query images. This comprises two major contributions: (1) Rank lists instead of image pairs or triplets are used for training, thus integrating more explicitly the order of similarity and relations between sets of images. (2) A weighting is introduced in the loss function based on two evaluation measures: the mean average precision and the rank 1 score. We evaluated our approach on two different computer vision applications that are commonly formulated as ranking problems: person re-identification and image retrieval with several public benchmarks and showed that our new loss function outperforms other common functions and that our method achieves state-of-the-art performance compared to existing approaches from the literature.},
  archive      = {J_MVA},
  author       = {Chen, Yiqiang and Duffner, Stefan and Stoian, Andrei and Dufour, Jean-Yves and Baskurt, Atilla},
  doi          = {10.1007/s00138-021-01170-0},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {List-wise learning-to-rank with convolutional neural networks for person re-identification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated detection and classification of spilled loads on
freeways based on improved YOLO network. <em>MVA</em>, <em>32</em>(2),
1–12. (<a href="https://doi.org/10.1007/s00138-021-01171-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to utilize a modified you only look once (YOLO) network to address the detection and classification of spilled loads on freeways. YOLO architecture was augmented in two ways. Firstly, a kernel size of 1 × 1 for the conv layers was used. Secondly, the use of connections between the convolution layers was proposed. For training the network, a synthetic dataset was constructed where ImageNet was used to choose ten types of spilled load objects and KITTI dataset as the background. The objects were blended in the KITTI images&#39; road region, where the road area is segmented through an already trained network previously available. The testing dataset was constructed with manually taken photographs. Experiment results showed that the training model can arrive at an accuracy rate of 74%. The trained model was also demonstrated on the test set generated by taking the background images with camera mounted on a station wagon and on a field test.},
  archive      = {J_MVA},
  author       = {Zhou, Siqi and Bi, Yufeng and Wei, Xu and Liu, Jiachen and Ye, Zixin and Li, Feng and Du, Yuchuan},
  doi          = {10.1007/s00138-021-01171-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automated detection and classification of spilled loads on freeways based on improved YOLO network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3MNet: Multi-task, multi-level and multi-channel feature
aggregation network for salient object detection. <em>MVA</em>,
<em>32</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01172-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection is a hot spot of current computer vision. The emergence of the convolutional neural network (CNN) greatly improves the existing detection methods. In this paper, we present 3MNet, which is based on the CNN, to make the utmost of various features of the image and utilize the contour detection task of the salient object to explicitly model the features of multi-level structures, multiple tasks and multiple channels, so as to obtain the final saliency map of the fusion of these features. Specifically, we first utilize contour detection task for auxiliary detection and then utilize use multi-layer network structure to extract multi-scale image information. Finally, we introduce a unique module into the network to model the channel information of the image. Our network has produced good results on five widely used datasets. In addition, we also conducted a series of ablation experiments to verify the effectiveness of some components in the network.},
  archive      = {J_MVA},
  author       = {Yan, Xinghe and Chen, Zhenxue and Wu, Q. M. Jonathan and Lu, Mengxu and Sun, Luna},
  doi          = {10.1007/s00138-021-01172-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3MNet: Multi-task, multi-level and multi-channel feature aggregation network for salient object detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-FAN: Multi-spectral mosaic super-resolution via
multi-scale feature aggregation network. <em>MVA</em>, <em>32</em>(2),
1–10. (<a href="https://doi.org/10.1007/s00138-021-01174-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel method to super-resolve multi-spectral images captured by modern real-time single-shot mosaic image sensors, also known as multi-spectral cameras. Our contribution is twofold. Firstly, we super-resolve multi-spectral images from mosaic images rather than image cubes, which helps to take into account the spatial offset of each wavelength. Secondly, we introduce an external multi-scale feature aggregation network (Multi-FAN) which concatenates the feature maps with different levels of semantic information throughout a super-resolution (SR) network. A cascade of convolutional layers then implicitly selects the most valuable feature maps to generate a mosaic image. This mosaic image is then merged with the mosaic image generated by the SR network to produce a quantitatively superior image. We apply our Multi-FAN to RCAN (residual channel attention network), which is the state-of-the-art SR algorithm. We show that Multi-FAN improves both quantitative results (PSNR and SSIM) and inference time.},
  archive      = {J_MVA},
  author       = {Sheoiby, Mehrdad and Aliakbarian, Sadegh and Anwar, Saeed and Petersson, Lars},
  doi          = {10.1007/s00138-021-01174-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-FAN: Multi-spectral mosaic super-resolution via multi-scale feature aggregation network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Two-layer pyramid-based blending method for exposure fusion.
<em>MVA</em>, <em>32</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s00138-021-01175-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-exposure fusion is a technique used to generate a high-dynamic-range image without calculating the camera response function and without compressing its ranges with the tone mapping process. There are many schemes for fusing multi-exposure images. One of the famous schemes is the pyramid-based blending, which fuses multi-exposure images together based on the concept of multi-resolution blending. The computational time of this method is fast, and the resulting image is satisfying. However, in some cases, the pyramid-based blending method has a trade-off between preserving local areas and the quality in terms of smoothness boundaries. To solve this trade-off, we propose a new pyramid-based blending scheme, called the two-layer pyramid-based blending method for multi-exposure fusion. We found that, for some sets of input images, we have to generate a virtual photograph and add it into those sets. Thus, we first propose a criterion for adding the virtual photograph. Then, we construct a concatenation of two pyramid-based blending methods, in which the resulting images from different levels of the first-layer pyramid-based blending method are used as inputs of the second-layer one. The test results showed that the resulting images were satisfying, and their objective evaluation scores regarding the perceptual image quality and detail-preserving assessment were high, even though they are not highest, compared with those of methods used in our experiments. Also, there is one advantage of the proposed method compared with the others. That is, the proposed method could preserve details in some areas, whereas the other methods could not.},
  archive      = {J_MVA},
  author       = {Keerativittayanun, Suthum and Kondo, Toshiaki and Kotani, Kazunori and Phatrapornnant, Teera and Karnjana, Jessada},
  doi          = {10.1007/s00138-021-01175-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Two-layer pyramid-based blending method for exposure fusion},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Research on defect detection method of powder metallurgy
gear based on machine vision. <em>MVA</em>, <em>32</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-021-01177-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Powder metallurgy gears are often accompanied by broken teeth, abrasion, scratches and crack defects. In order to eliminate the defective gears in gear production and improve the yield of gears, this paper presents an improved GA–PSO algorithm, called the SHGA–PSO algorithm. Firstly, the gear images were preprocessed by bilateral filtering, and the images were segmented by the Sobel operator. Then, the geometrical shape, texture feature and color features of the sample were extracted. Next, the BP neural network was reconstructed and SHGA–PSO algorithm was used optimize its structure and weights. Finally, four different gear defect samples were brought into the neural network for calculation, and the performance of the SHGA–PSO algorithm was compared with the GA, PSO and GA–PSO algorithms. Compared with GA–BP algorithm, PSO–BP algorithm, and GA–PSO–BP algorithm, the defect diagnosis of SHGA–PSO–BP algorithm not only enhanced generalization ability, but also improved recognition accuracy.},
  archive      = {J_MVA},
  author       = {Xiao, Maohua and Wang, Weichen and Shen, Xiaojie and Zhu, Yue and Bartos, Petr and Yiliyasi, Yilidaer},
  doi          = {10.1007/s00138-021-01177-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Research on defect detection method of powder metallurgy gear based on machine vision},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse2Fine: A two-stage training method for fine-grained
visual classification. <em>MVA</em>, <em>32</em>(2), 1–9. (<a
href="https://doi.org/10.1007/s00138-021-01180-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small inter-class and large intra-class variations are the key challenges in fine-grained visual classification. Objects from different classes share visually similar structures, and objects in the same class can have different poses and viewpoints. Therefore, the proper extraction of discriminative local features (e.g., bird’s beak or car’s headlight) is crucial. Most of the recent successes on this problem are based upon the attention models which can localize and attend the local discriminative objects parts. In this work, we propose a training method for visual attention networks, Coarse2Fine, which creates a differentiable path from the attended feature maps to the input space. Coarse2Fine learns an inverse mapping function from the attended feature maps to the informative regions in the raw image, which will guide the attention maps to better attend the fine-grained features. Besides, we propose an initialization method for the attention weights. Our experiments show that Coarse2Fine reduces the classification error by up to 5.1% on common fine-grained datasets.},
  archive      = {J_MVA},
  author       = {Eshratifar, Amir Erfan and Eigen, David and Gormish, Michael and Pedram, Massoud},
  doi          = {10.1007/s00138-021-01180-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Coarse2Fine: A two-stage training method for fine-grained visual classification},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature-driven viewpoint placement for model-based surface
inspection. <em>MVA</em>, <em>32</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s00138-020-01116-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of visual surface inspection is to analyze an object’s surface and detect defects by looking at it from different angles. Developments over the past years have made it possible to partially automate this process. Inspection systems use robots to move cameras and obtain pictures that are evaluated by image processing algorithms. Setting up these systems or adapting them to new models is primarily done manually. A key challenge is to define camera viewpoints from which the images are taken. The number of viewpoints should be as low as possible while still guaranteeing an inspection of the desired quality. System engineers define and evaluate configurations that are improved based on a time-consuming trial-and-error process leading to a sufficient, but not necessarily optimal, configuration. With the availability of 3D surface models defined by triangular meshes, this step can be done virtually. This paper presents a new scalable approach to determine a small number of well-placed camera viewpoints for optical surface inspection planning. The initial model is approximated by B-spline surfaces. A set of geometric feature functionals is defined and used for an adaptive, non-uniform surface sampling that is sparse in geometrically low-complexity areas and dense in regions of higher complexity. The presented approach is applicable to solid objects with a given 3D surface model. It makes camera viewpoint generation independent of the resolution of the triangle mesh, and it improves previous results considering number of viewpoints and their relevance.},
  archive      = {J_MVA},
  author       = {Mosbach, Dennis and Gospodnetić, Petra and Rauhut, Markus and Hamann, Bernd and Hagen, Hans},
  doi          = {10.1007/s00138-020-01116-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Feature-driven viewpoint placement for model-based surface inspection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature-transfer network and local background suppression
for microaneurysm detection. <em>MVA</em>, <em>32</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01119-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microaneurysm (MA) is the earliest lesion of diabetic retinopathy (DR). Accurate detection of MA is helpful for the early diagnosis of DR. In this paper, an efficient approach is proposed to detect MA, based on feature-transfer network and local background suppression. In order to reduce noise, a feature-distance-based algorithm is proposed to suppress local background. The similarity matrix of feature distances is calculated to measure the difference between background noise and retinal objects. Moreover, a feature-transfer network is proposed to detect MAs with imbalanced data. For each training process, the optimized weights and bias are transferred to the next training, until the optimal network is generated. Experimental results demonstrate that the proposed approach can accurately detect subtle MAs surrounded by complex background. Furthermore, the sensitivity values on the public datasets are up to 98.3%, 100%, 99.3%, 100%, 96.5%, respectively. The proposed approach outperforms the state-of-the-arts, in terms of the competition performance measure score.},
  archive      = {J_MVA},
  author       = {Zhang, Xinpeng and Wu, Jigang and Meng, Min and Sun, Yifei and Sun, Weijun},
  doi          = {10.1007/s00138-020-01119-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Feature-transfer network and local background suppression for microaneurysm detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generalizable approach for multi-view 3D human pose
regression. <em>MVA</em>, <em>32</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01120-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant improvement in the performance of monocular pose estimation approaches and their ability to generalize to unseen environments, multi-view approaches are often lagging behind in terms of accuracy and are specific to certain datasets. This is mainly due to the fact that (1) contrary to real-world single-view datasets, multi-view datasets are often captured in controlled environments to collect precise 3D annotations, which do not cover all real-world challenges, and (2) the model parameters are learned for specific camera setups. To alleviate these problems, we propose a two-stage approach to detect and estimate 3D human poses, which separates single-view pose detection from multi-view 3D pose estimation. This separation enables us to utilize each dataset for the right task, i.e. single-view datasets for constructing robust pose detection models and multi-view datasets for constructing precise multi-view 3D regression models. In addition, our 3D regression approach only requires 3D pose data and its projections to the views for building the model, hence removing the need for collecting annotated data from the test setup. Our approach can therefore be easily generalized to a new environment by simply projecting 3D poses into 2D during training according to the camera setup used at test time. As 2D poses are collected at test time using a single-view pose detector, which might generate inaccurate detections, we model its characteristics and incorporate this information during training. We demonstrate that incorporating the detector’s characteristics is important to build a robust 3D regression model and that the resulting regression model generalizes well to new multi-view environments. Our evaluation results show that our approach achieves competitive results on the Human3.6M dataset and significantly improves results on a multi-view clinical dataset that is the first multi-view dataset generated from live surgery recordings.},
  archive      = {J_MVA},
  author       = {Kadkhodamohammadi, Abdolrahim and Padoy, Nicolas},
  doi          = {10.1007/s00138-020-01120-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A generalizable approach for multi-view 3D human pose regression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic camera calibration by landmarks on rigid objects.
<em>MVA</em>, <em>32</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01125-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new method for automatic calibration of surveillance cameras. We are dealing with traffic surveillance, and therefore, the camera is calibrated by observing vehicles; however, other rigid objects can be used instead. The proposed method is using keypoints or landmarks automatically detected on the observed objects by a convolutional neural network. By using fine-grained recognition of the vehicles (calibration objects), and by knowing the 3D positions of the landmarks for the (very limited) set of known objects, the extracted keypoints are used for calibration of the camera, resulting in internal (focal length) and external (rotation, translation) parameters and scene scale of the surveillance camera. We collected a dataset in two parking lots and equipped it with a calibration ground truth by measuring multiple distances in the ground plane. This dataset seems to be more accurate than the existing comparable data (GT calibration error reduced from 4.62 % to 0.99 %). Also, the experiments show that our method overcomes the best existing alternative in terms of accuracy (error reduced from 6.56 % to $$4.03\,\%$$ ) and our solution is also more flexible in terms of viewpoint change and other.},
  archive      = {J_MVA},
  author       = {Bartl, Vojtěch and Špaňhel, Jakub and Dobeš, Petr and Juránek, Roman and Herout, Adam},
  doi          = {10.1007/s00138-020-01125-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic camera calibration by landmarks on rigid objects},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning an end-to-end spatial grasp generation and
refinement algorithm from simulation. <em>MVA</em>, <em>32</em>(1),
1–12. (<a href="https://doi.org/10.1007/s00138-020-01127-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel object grasping is an important technology for robot manipulation in unstructured environments. For most of current works, a grasp sampling process is required to obtain grasp candidates, combined with a local feature extractor using deep learning. However, this pipeline is time–cost, especially when grasp points are sparse such as at the edge of a bowl. To tackle this problem, our algorithm takes the whole sparse point clouds as the input and requires no sampling or search process. Our work is combined with two steps. The first step is to predict poses, categories and scores (qualities) based on a SPH3D-GCN network. The second step is an iterative grasp pose refinement, which is to refine the best grasp generated in the first step. The whole weight sizes for these two steps are only about 0.81M and 0.52M, which takes about 73 ms for a whole prediction process including an iterative grasp pose refinement using a GeForce 840M GPU. Moreover, to generate training data of multi-object scene, a single-object dataset (79 objects from YCB object set, 23.7k grasps) and a multi-object dataset (20k point clouds with annotations and masks) combined with thin structures grasp planning are generated. Our experiment shows our work gets 76.67% success rate and 94.44% completion rate, which performs better than current state-of-the-art works.},
  archive      = {J_MVA},
  author       = {Ni, Peiyuan and Zhang, Wenguang and Zhu, Xiaoxiao and Cao, Qixin},
  doi          = {10.1007/s00138-020-01127-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Learning an end-to-end spatial grasp generation and refinement algorithm from simulation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A five-layer deep convolutional neural network with
stochastic pooling for chest CT-based COVID-19 diagnosis. <em>MVA</em>,
<em>32</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01128-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Till August 17, 2020, COVID-19 has caused 21.59 million confirmed cases in more than 227 countries and territories, and 26 naval ships. Chest CT is an effective way to detect COVID-19. This study proposed a novel deep learning model that can diagnose COVID-19 on chest CT more accurately and swiftly. Based on traditional deep convolutional neural network (DCNN) model, we proposed three improvements: (i) We introduced stochastic pooling to replace average pooling and max pooling; (ii) We combined conv layer with batch normalization layer and obtained the conv block (CB); (iii) We combined dropout layer with fully connected layer and obtained the fully connected block (FCB). Our algorithm achieved a sensitivity of 93.28% ± 1.50%, a specificity of 94.00% ± 1.56%, and an accuracy of 93.64% ± 1.42%, in identifying COVID-19 from normal subjects. We proved using stochastic pooling yields better performance than average pooling and max pooling. We compared different structure configurations and proved our 3CB + 2FCB yields the best performance. The proposed model is effective in detecting COVID-19 based on chest CT images.},
  archive      = {J_MVA},
  author       = {Zhang, Yu-Dong and Satapathy, Suresh Chandra and Liu, Shuaiqi and Li, Guang-Run},
  doi          = {10.1007/s00138-020-01128-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A five-layer deep convolutional neural network with stochastic pooling for chest CT-based COVID-19 diagnosis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel approach for unsupervised image segmentation fusion
of plant leaves based on g-mutual information. <em>MVA</em>,
<em>32</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01130-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant leaf segmentation has a very important role in most plant identification methods. Tree leaves segmentation in images with complex background is very difficult when there is no prior information about the leaves and backgrounds. In practice, the parameters of unsupervised image segmentation algorithms must be set for each image to get the best results. In this paper, to overcome this problem, fusion of the results of five leaf segmentation algorithms (fuzzy c-means, SOM and k-means in various color spaces or different parameters) is applied. To fuse the results of these segmentations, new equations for mutual information (g-mutual information equations) based on the g-calculus are introduced to find the best consensus segmentation. The results of the mentioned primary clustering algorithms are considered as a new feature vector for each pixel. To reduce the time complexity, a fast method is employed using truth table containing different feature vectors. To evaluate this new approach, a leaf image database with natural scenes, taken from Pl@ntLeaves database, is generated to have different positions and orientations. In addition, a widely used database is used to compare the proposed method with other methods. The experimental results presented in this paper show that the use of g-calculus in fusion of image segmentations improves the evaluation parameters.},
  archive      = {J_MVA},
  author       = {Nikbakhsh, Navid and Baleghi, Yasser and Agahi, Hamzeh},
  doi          = {10.1007/s00138-020-01130-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel approach for unsupervised image segmentation fusion of plant leaves based on G-mutual information},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A system for the generation of in-car human body pose
datasets. <em>MVA</em>, <em>32</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01131-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of autonomous vehicles, detection of the occupants’ posture is crucial to tackle the needs of infotainment interaction or passive safety systems. Generative approaches have been recently proposed for human body pose in-car detection, but this type of approaches requires a large training dataset for a feasible accuracy. This requirement poses a difficulty, given the substantial time required to annotate such large amount of data. In the in-car scenario, this requirement risk increases even further, since a robust human body pose ground-truth system capable of working in it is needed but inexistent. Currently, the gold standard for human body pose capture is based on optical systems, requiring up to 39 visible markers for a plug-in gait model, which in this case are not feasible given the occlusions inside the car. Other solutions, such as inertial suits, also have limitations linked to magnetic sensitivity and global positioning drift. In this paper, a system for the generation of images for human body pose detection in an in-car environment is proposed. To this end, we propose to smartly combine inertial and optical systems to suppress their individual limitations: By combining the global positioning of 3 visible head markers provided by the optical system with the inertial suit’s relative human body pose, we obtain an occlusion-ready, drift-free full-body global positioning system. This system is then spatially and temporally calibrated with a time-of-flight sensor, automatically obtaining in-car image data with (multi-person) pose annotations. Besides quantifying the inertial suit inherent sensitivity and accuracy, the feasibility of the overall system for human body pose capture in the in-car scenario was demonstrated. Our results quantify the errors associated with the inertial suit, pinpoint some sources of the system’s uncertainty and propose how to minimize some of them. Finally, we demonstrate the feasibility of using system generated data (which was made publicly available), independently or mixed with two publicly available generic datasets (not in-car), to train 2 machine learning algorithms, demonstrating the improvement in their algorithmic accuracy for the in-car scenario.},
  archive      = {J_MVA},
  author       = {Borges, João and Queirós, Sandro and Oliveira, Bruno and Torres, Helena and Rodrigues, Nelson and Coelho, Victor and Pallauf, Johannes and Brito, José Henrique and Mendes, José and Fonseca, Jaime C.},
  doi          = {10.1007/s00138-020-01131-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A system for the generation of in-car human body pose datasets},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd flow estimation from calibrated cameras. <em>MVA</em>,
<em>32</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01132-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many crowd analysis methods rely on optical flow techniques to estimate the main moving directions. In this work, we propose a crowd flow filtering approach for calibrated cameras that can be coupled to any generic optical flow method. It projects the input optical flow to the world coordinate system, performs a local motion analysis exploring a Social Forces Model and then projects the filtered flow back onto the image plane. The method was tested on publicly available datasets involving crowded scenarios used in conjunction with different optical flow methods, and results indicate that the proposed filtering method provides coherent crowd flows when coupled to the tested methods.},
  archive      = {J_MVA},
  author       = {Almeida, Igor and Jung, Claudio},
  doi          = {10.1007/s00138-020-01132-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Crowd flow estimation from calibrated cameras},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive superpixel-based multi-object pedestrian
recognition. <em>MVA</em>, <em>32</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s00138-020-01133-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed an adaptive multi-object pedestrian recognition algorithm based on SLIC. First, we used SLIC to superpixel the pre-segmentation processing on the image. Then, the hash distance is added as the superpixel point aggregation parameter based on the traditional superpixel measurement unit of LAB color space distance and position distance. Finally, we identified the clustering subject by using the extreme learning machine neural network. The proposed method can adaptively determine the number of superpixels to achieve high recognition performance. This method simply needs to preset the number of pre-segments, which can reduce the number of detection targets, improve the segmentation efficiency, and shorten the image identification time.},
  archive      = {J_MVA},
  author       = {Yu, Tianhe and Wang, Chengdong and Liu, Xiao and Zhu, Ming},
  doi          = {10.1007/s00138-020-01133-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adaptive superpixel-based multi-object pedestrian recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Numerical joint invariant level set formulation with unique
image segmentation result. <em>MVA</em>, <em>32</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01134-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The level set method is one of the most widely used and powerful techniques in image science such as image/motion segmentation, object tracking, etc. This paper brings up an unstudied issue with discretized level set algorithms about ‘the non-uniqueness’ of segmentation results which is different from the problem of ‘the existence’ of a result. Our solution is to numerically approximate the level set formulation based on suitable combination of some visual joint invariants, leading to the unique segmentation results, therefore unique visual joint invariant numerical signatures—independent of contour initialization and what visual group is applied. To figure out ‘the cause’ of resulting unique segmentations in this scheme, we utilize the level set algorithm to introduce three energy features—called fingerprints, flows, and stem charts. Our experimental results indicate that curvature-based energies can be classified in terms of these characteristics—depending merely on the nature of each energy. Besides, the energies generated by the current discretization are ‘positive,’ while the visual joint invariant curvature-based energies sketch charts with ‘negative’ values.},
  archive      = {J_MVA},
  author       = {Aghayan, Reza},
  doi          = {10.1007/s00138-020-01134-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Numerical joint invariant level set formulation with unique image segmentation result},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RSRGAN: Computationally efficient real-world single image
super-resolution using generative adversarial network. <em>MVA</em>,
<em>32</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01135-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural network has been employed to obtain better performance in single image super-resolution task. Most of these models are trained and evaluated on synthetic datasets in which low-resolution images are synthesized with known bicubic degradation and hence they perform poorly on real-world images. However, by stacking more convolution layers, the super-resolution (SR) performance can be improved. But, such idea increases the number of training parameters and it offers a heavy computational burden on resources which makes them unsuitable for real-world applications. To solve this problem, we propose a computationally efficient real-world image SR network referred as RSRN. The RSRN model is optimized using pixel-wise $$L_1$$ loss function which produces overly-smooth blurry images. Hence, to recover the perceptual quality of SR image, a real-world image SR using generative adversarial network called RSRGAN is proposed. Generative adversarial network has an ability to generate perceptual plausible solutions. Several experiments have been conducted to validate the effectiveness of the proposed RSRGAN model, and it shows that the proposed RSRGAN generates SR samples with more high-frequency details and better perception quality than that of recently proposed SRGAN and $$\hbox {SRFeat}_{\textit{IF}}$$ models, while it sets comparable performance with the ESRGAN model with significant less number of training parameters.},
  archive      = {J_MVA},
  author       = {Chudasama, Vishal and Upla, Kishor},
  doi          = {10.1007/s00138-020-01135-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RSRGAN: Computationally efficient real-world single image super-resolution using generative adversarial network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Snapshot hyperspectral imaging using wide dilation networks.
<em>MVA</em>, <em>32</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01136-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) cameras record the spectrum at multiple wavelengths for each pixel in an image, and are used, e.g., for quality control and agricultural remote sensing. We introduce a fast, cost-efficient and mobile method of taking HS images using a regular digital camera equipped with a passive diffraction grating filter, using machine learning for constructing the HS image. The grating distorts the image by effectively mapping the spectral information into spatial dislocations, which we convert into a HS image by a convolutional neural network utilizing novel wide dilation convolutions that accurately model optical properties of diffraction. We demonstrate high-quality HS reconstruction using a model trained on only 271 pairs of diffraction grating and ground truth HS images.},
  archive      = {J_MVA},
  author       = {Toivonen, Mikko E. and Rajani, Chang and Klami, Arto},
  doi          = {10.1007/s00138-020-01136-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Snapshot hyperspectral imaging using wide dilation networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-based relative pose determination of cooperative
spacecraft in neutral buoyancy environment. <em>MVA</em>,
<em>32</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s00138-020-01137-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neutral buoyancy systems simulate the microgravity environment by taking advantage of buoyancy forces of water to offset the gravity of test bodies. Functional verification of space robots in neutral buoyancy system is of great importance for ground tests. The relative pose determination of a spacecraft plays an essential role in the on-orbit operation of space robots. In order to meet the requirement of on-orbit operation ground verification for space robots, this paper develops a vision-based system for determining the relative pose of cooperative spacecraft in neutral buoyancy environment. Cooperative markers and underwater binocular vision system are designed for the pose determination, and a cooperative spacecraft model is built. A detection and recognition method based on the topological characteristic is proposed for the cooperative marker. An underwater imaging model of binocular camera is established, and its refraction parameters are calibrated. The marker points are measured with an underwater binocular 3D measurement algorithm. Furthermore, the pose of cooperative spacecraft is determined using axisymmetric plane feature points. Additionally, the stable and reliable pose and velocity are obtained after the data are further processed with a Kalman filter. Finally, the experiments are carried out and the experimental results show that the proposed system can achieve a stable and reliable high-precision relative pose determination for cooperative spacecraft in neutral buoyancy environment.},
  archive      = {J_MVA},
  author       = {Jia, Guohua and Min, Chaoqing and Wang, Kedian and Zhu, Zhanxia},
  doi          = {10.1007/s00138-020-01137-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Vision-based relative pose determination of cooperative spacecraft in neutral buoyancy environment},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LS-net: Fast single-shot line-segment detector.
<em>MVA</em>, <em>32</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-020-01138-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unmanned aerial vehicle (UAV) flights, power lines are considered as one of the most threatening hazards and one of the most difficult obstacles to avoid. In recent years, many vision-based techniques have been proposed to detect power lines to facilitate self-driving UAVs and automatic obstacle avoidance. However, most of the proposed methods are typically based on a common three-step approach: (i) edge detection, (ii) the Hough transform, and (iii) spurious line elimination based on power line constrains. These approaches not only are slow and inaccurate but also require a huge amount of effort in post-processing to distinguish between power lines and spurious lines. In this paper, we introduce LS-Net, a fast single-shot line-segment detector, and apply it to power line detection. The LS-Net is by design fully convolutional, and it consists of three modules: (i) a fully convolutional feature extractor, (ii) a classifier, and (iii) a line segment regressor. Due to the unavailability of large datasets with annotations of power lines, we render synthetic images of power lines using the physically based rendering approach and propose a series of effective data augmentation techniques to generate more training data. With a customized version of the VGG-16 network as the backbone, the proposed approach outperforms existing state-of-the-art approaches. In addition, the LS-Net can detect power lines in near real time. This suggests that our proposed approach has a promising role in automatic obstacle avoidance and as a valuable component of self-driving UAVs, especially for automatic autonomous power line inspection.},
  archive      = {J_MVA},
  author       = {Nguyen, Van Nhan and Jenssen, Robert and Roverso, Davide},
  doi          = {10.1007/s00138-020-01138-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {LS-net: Fast single-shot line-segment detector},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FSD: Feature skyscraper detector for stem end and blossom
end of navel orange. <em>MVA</em>, <em>32</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01139-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately and efficiently distinguish the stem end and the blossom end of a navel orange from its black spots, we propose a feature skyscraper detector (FSD) with low computational cost, compact architecture and high detection accuracy. The main part of the detector is inspired from small object that the stem (blossom) end is complex and the black spot is densely distributed, so we design the feature skyscraper networks (FSN) based on dense connectivity. In particular, FSN is distinguished from regular feature pyramids, and which provides more intensive detection of high-level features. Then we design the backbone of the FSD based on attention mechanism and dense block for better feature extraction to the FSN. In addition, the architecture of the detector is also added Swish to further improve the accuracy. And we create a dataset in Pascal VOC format annotated three types of detection targets the stem end, the blossom end and the black spot. Experimental results on our orange dataset confirm that the FSD has competitive results to the state-of-the-art one-stage detectors like SSD, DSOD, YOLOv2, YOLOv3, RFB and FSSD, and it achieves 87.479% mAP at 131 FPS with only 5.812M parameters.},
  archive      = {J_MVA},
  author       = {Sun, Xiaoye and Li, Gongyan and Xu, Shaoyun},
  doi          = {10.1007/s00138-020-01139-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FSD: Feature skyscraper detector for stem end and blossom end of navel orange},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical fusion network for periocular and iris by
neural network approximation and sparse autoencoder. <em>MVA</em>,
<em>32</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01140-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The eye region is one of the most attractive sources for identification and verification due to the representative availability of such biometric modalities as periocular and iris. Many score-level fusion approaches have been proposed to combine these two modalities targeting to improve the robustness. The score-level approaches can be grouped into three categories: transformation-based, classification-based and density-based. Each category has its own benefits, if combined can lead to a robust fusion mechanism. In this paper, we propose a hierarchical fusion network to fuse multiple fusion approaches from transformation-based and classification-based categories into a unified framework for classification. The proposed hierarchical approach relies on the universal approximation theorem for neural networks to approximate each fusion approach as one child neural network and then ensemble them into a unified parent network. This mechanism takes advantage of both categories to improve the fusion performance, illustrated by an improved equal error rate of the multimodal biometric system. We subsequently force the parent network to learn the representation and interaction strategy between the child networks from the training data through a sparse autoencoder layer, leading to further improvements. Experiments on two public datasets (MBGC version 2 and CASIA-Iris-Thousand) and our own dataset validate the effectiveness of the proposed hierarchical fusion approach for periocular and iris modalities.},
  archive      = {J_MVA},
  author       = {Algashaam, Faisal and Nguyen, Kien and Banks, Jasmine and Chandran, Vinod and Do, Tuan-Anh and Alkanhal, Mohamed},
  doi          = {10.1007/s00138-020-01140-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hierarchical fusion network for periocular and iris by neural network approximation and sparse autoencoder},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel and intelligent vision-based tutor for yogāsana:
E-YogaGuru. <em>MVA</em>, <em>32</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s00138-020-01141-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent days have stamped enormous upsurge about health awareness in society. Self tutoring systems for supervising the performed exercises offer numerous advantages and are therefore emerging as an entity of dire necessity in health-sector. Considering the significantly increasing global acceptance of ‘ $${{Yog\bar{a}sana}}$$ ’ as one of the most preferred exercise, this paper proposes a novel and an intelligent vision-based self-tutoring system for $${{Yog\bar{a}sana}}$$ . The proposed system, ’e-YogaGuru’ analyzes the body movements while performing $${{Yog\bar{a}sana}}$$ , provides feedback about its correctness and further, suggests amendment, if required. Incorporation of angle features in the novel state transition-based approach addresses the earlier reported issues raised due to human anthropometry and variance in the execution speed. Consideration of hold time and suggestion of amendment at two levels, abstract level and detailed amendment (sequences of pre-posture, main-posture and post-posture), make the proposed e-YogaGuru unique and efficient. System is trained for 21 postures derived from the skeleton stream of 8 experts exhibiting variations in anthropometry and execution speed (Knowledge base). A dataset composed of 1750 video sequences (7 $${{Yog\bar{a}sana}}$$ performed by 25 practitioners) is used to validate the efficacy of the devised approach. The proposed e-YogaGuru achieved 98.29 % accuracy in correctly identifying the $${{Yog\bar{a}sana}}$$ and has been able to suggest required amendment in the incorrectly performed $${{Yog\bar{a}sana}}$$ with an accuracy of 96.34 %. Proposed ‘e-YogaGuru’ incorporates significant parameters (hold time and amendment) and achieves appreciable accuracies, thus it not only out-performs the earlier reported systems but also marks a long bounce towards practical deployment.},
  archive      = {J_MVA},
  author       = {Kale, Geetanjali and Patil, Varsha and Munot, Mousami},
  doi          = {10.1007/s00138-020-01141-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel and intelligent vision-based tutor for yogāsana: E-YogaGuru},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measurement and inspection of electrical discharge machined
steel surfaces using deep neural networks. <em>MVA</em>, <em>32</em>(1),
1–15. (<a href="https://doi.org/10.1007/s00138-020-01142-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an industrial measurement and inspection system for steel workpieces eroded by electrical discharge machining, which uses deep neural networks for surface roughness estimation and defect detection. Specifically, a convolutional neural network (CNN) is used as a regressor in order to obtain steel surface roughness and a CNN based on spatial pooling pyramid is applied for defect classification. In addition, a new method for the region of interest selection based on morphological reconstruction and mean shift filtering is proposed for defect detection and localization. The regressor and classifier based on deep neural networks proposed here outperform state-of-the-art methods using handcrafted feature extraction. We achieve a mean absolute percentage error of 7.32% on roughness estimation; on defect detection, our approach yields an accuracy of 97.26% and an area under the ROC curve metric of 99.09%.},
  archive      = {J_MVA},
  author       = {Saeedi, Jamal and Dotta, Matteo and Galli, Andrea and Nasciuti, Adriano and Maradia, Umang and Boccadoro, Marco and Gambardella, Luca Maria and Giusti, Alessandro},
  doi          = {10.1007/s00138-020-01142-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Measurement and inspection of electrical discharge machined steel surfaces using deep neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing collaborative road scene reconstruction with
unsupervised domain alignment. <em>MVA</em>, <em>32</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-020-01144-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene reconstruction and visual localization in dynamic environments such as street scenes are a challenge due to the lack of distinctive, stable keypoints. While learned convolutional features have proven to be robust to changes in viewing conditions, handcrafted features still have advantages in distinctiveness and accuracy when applied to structure from motion. For collaborative reconstruction of road sections by a car fleet, we propose to use multimodal domain adaptation as a preprocessing step to align images in their appearance and enhance keypoint matching across viewing conditions while preserving the advantages of handcrafted features. Training a generative adversarial network for translations between different illumination and weather conditions, we evaluate qualitative and quantitative aspects of domain adaptation and its impact on feature correspondences. Combined with a multi-feature discriminator, the model is optimized for synthesis of images which do not only improve feature matching but also exhibit a high visual quality. Experiments with a challenging multi-domain dataset recorded in various road scenes on multiple test drives show that our approach outperforms other traditional and learning-based methods by improving completeness or accuracy of structure from motion with multimodal two-domain image collections in eight out of ten test scenes.},
  archive      = {J_MVA},
  author       = {Venator, Moritz and Aklanoglu, Selcuk and Bruns, Erich and Maier, Andreas},
  doi          = {10.1007/s00138-020-01144-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhancing collaborative road scene reconstruction with unsupervised domain alignment},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learnable spatiotemporal feature pyramid for prediction of
future optical flow in videos. <em>MVA</em>, <em>32</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01145-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep learning-based techniques in solving various computer vision problems motivated the researchers to apply deep learning to predict the optical flow of a video in the next frame. However, the problem of predicting the motion of an object in the next few frames remains an unsolved and less explored problem. Given a sequence of frames, predicting the motion in the next few frames of the video becomes difficult in cases where the displacement of optical flow vector across frames is large. Traditional CNNs often fail to learn the dynamics of the objects across frames in case of large displacements of objects in consecutive frames. In this paper, we present an efficient CNN based on the concept of feature pyramid for extracting the spatial features from a few consecutive frames. The spatial features extracted from consecutive frames by a modified PWC-Net architecture are fed into a bidirectional LSTM for obtaining the temporal features. The proposed spatiotemporal feature pyramid is able to capture the abrupt motion of the moving objects in video, especially when displacement of the object is large across the consecutive frames. Further, the proposed spatiotemporal pyramidal feature can effectively predict the optical flow in next few frames, instead of predicting only the next frame. The proposed method of predicting optical flow outperforms the state of the art when applied on challenging datasets such as “MPI Sintel Final Pass,” “Monkaa” and “Flying Chairs” where abrupt and large displacement of the moving objects in consecutive frames is the main challenge.},
  archive      = {J_MVA},
  author       = {Wadhwa, Laisha and Mukherjee, Snehasis},
  doi          = {10.1007/s00138-020-01145-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Learnable spatiotemporal feature pyramid for prediction of future optical flow in videos},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal feature level fusion for secured human
authentication in multimodal biometric system. <em>MVA</em>,
<em>32</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01146-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising demand for high security and reliable authentication schemes, led to the development of the unimodal biometric system so that the multimodal biometric system has emerged. The multimodal biometric system will use more than one biometric trait of an individual for identification and security purpose. Fusion plays a major role in the multimodal biometric system. Several fusion techniques are used in biometric systems. Feature level fusion is a very much popular method as compared to the other fusion techniques. In this fusion, features are extracted from all biometric traits. After that extracted features are combined into a final feature vector of high dimension. In this paper, we introduce a new technique to perform fusion at the feature level by optimal feature level fusion; here the relevant features are selected using an optimization technique. Here, we proposed OGWO for selecting optimal features. Moreover, we suggested the recognition technique. For recognition, we use the multi-kernel support vector machine algorithm. Finally, the performance of our proposed method is evaluated by some evaluation measures. Our recommended method is implemented in the MATLAB platform.},
  archive      = {J_MVA},
  author       = {Purohit, Himanshu and Ajmera, Pawan K.},
  doi          = {10.1007/s00138-020-01146-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Optimal feature level fusion for secured human authentication in multimodal biometric system},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibrating a profile measurement system for dimensional
inspection in rail rolling mills. <em>MVA</em>, <em>32</em>(1), 1–16.
(<a href="https://doi.org/10.1007/s00138-020-01147-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern, high-speed, railway transportation requires rails to conform strictly to requirements specified in various standards. One key requirement is the conformance of the dimensions of the rail cross section to those of the corresponding rail model, within tight tolerances. This paper deals with a system for dimensional quality inspection during the manufacture of railway rails. Optical triangulation is used to build a profile from laser lines projected on the rails from four different locations. Then, the profile is compared to that of the corresponding rail model. The differences between certain numerical values (the dimensions) for the profile and the model are compared to standard tolerances for each dimension in order to detect dimensional defects. As a prerequisite for this, the cameras used to capture the laser lines must be calibrated. Standard calibration plates are unsuitable for sheet-of-light calibration in a production environment, as determining the location of the laser emitters relative to the plates would be an issue. For this reason, a cylinder-based calibration target is used instead. Different calibration algorithms are discussed and compared to said standard calibration. The results of accuracy and repeatability tests in the production environment are also shown. The accuracy of the system is found to be appropriate for the purpose of quality inspection under the requirements of applicable rail standards.},
  archive      = {J_MVA},
  author       = {Millara, Álvaro F. and Molleda, Julio and Usamentiaga, Rubén and García, Daniel F.},
  doi          = {10.1007/s00138-020-01147-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Calibrating a profile measurement system for dimensional inspection in rail rolling mills},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pruning method based on the measurement of feature
extraction ability. <em>MVA</em>, <em>32</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01148-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the network structure of convolutional neural network (CNN) becomes deeper and wider, network optimization, such as pruning, has received ever-increasing research focus. This paper propose a new pruning strategy based on Feature Extraction Ability Measurement (FEAM), which is a novel index of the feature extraction ability from both theoretical analysis and practical operation. Firstly, FEAM is computed as the product of the the kernel sparsity and feature dispersion. Kernel sparsity describes the ability of feature extraction in theory, and feature dispersion represents the feature extraction ability in practical operation. Secondly, FEAMs of all filters in the network are normalized so that the pruning operation can be applied to cross-layer filters. Finally, filters with weak FEAM are pruned to obtain a compact CNN model. In addition, fine-tuning is adopted to restore the generalization ability. Experiments on CAFAR-10 and CUB-200-2011 demonstrate the effectiveness of our method.},
  archive      = {J_MVA},
  author       = {Wu, Honggang and Tang, Yi and Zhang, Xiang},
  doi          = {10.1007/s00138-020-01148-4},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A pruning method based on the measurement of feature extraction ability},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular 3D reconstruction of sail flying shape using
passive markers. <em>MVA</em>, <em>32</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s00138-020-01149-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to recover the 3D flying shape of a sail using passive markers. In the navigation and naval architecture domain, retrieving the sail shape may be of immense value to confirm or contest simulation results, and to aid the design of new optimal sails. Our acquisition setup is very simple and low-cost, as it is only necessary to fix a series of printable markers on the sail and register the flying shape in real sailing conditions from a side vessel with a single camera. We reconstruct the average sail shape during an interval where the sailor maintains the sail as stable as possible. The average is further improved by a Bundle Adjustment algorithm. We tested our method in a real sailing scenario and present promising results. Quantitatively, we show the precision in regards to the reconstructed markers area and the reprojected points. Qualitatively, we present feedback from domain experts who evaluated our results and confirmed the usefulness and quality of the reconstructed shape.},
  archive      = {J_MVA},
  author       = {Maciel, Luiz and Marroquim, Ricardo and Vieira, Marcelo and Ribeiro, Kevyn and Alho, Alexandre},
  doi          = {10.1007/s00138-020-01149-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Monocular 3D reconstruction of sail flying shape using passive markers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A cognitive vision method for the detection of plant disease
images. <em>MVA</em>, <em>32</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01150-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food security, which has currently attracted much attention, requires minimizing crop damage by timely detection of plant diseases. Therefore, the automatic identification and diagnosis of plant diseases are highly desired in agricultural information. In this paper, we propose a novel approach to identify plant diseases. The method is divided into two parts: starting with the enhancement of the artificial neural network, the extracted pixel values and feature values are input to the enhanced artificial neural network for the image segmentation; then, following the establishment of a CNN based model, the segmented images are input to the proposed CNN model for the image classification. The proposed approach shows an impressive performance in the experimental analyses. It achieved an average accuracy of 93.75% to identify the crop diseases under the complex background conditions, and the validation accuracy was, on average, 10% higher than that of the conventional method. Additionally, almost all the plant disease samples were correctly detected by the proposed approach, and thus the recall rate achieved 100%. The experimental finding presents a substantial performance relative to other state-of-the-art methods and demonstrates the efficiency and extensibility of the proposed approach.},
  archive      = {J_MVA},
  author       = {Chen, Junde and Chen, Jinxiu and Zhang, Defu and Nanehkaran, Y. A. and Sun, Yuandong},
  doi          = {10.1007/s00138-020-01150-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A cognitive vision method for the detection of plant disease images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge missing image inpainting with compression–decompression
network in low similarity images. <em>MVA</em>, <em>32</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s00138-020-01151-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting technology can patch images with missing pixels. Existing methods propose convolutional neural networks to repair corrupted images. The network extracts effective pixels around the missing pixels and uses the encoding–decoding structure to extract valuable information to repair the vacancy. However, if the missing part is too large to provide useful information, the result will be fuzzy, color mixing, and object confusion. In order to patch the large hole image, we propose a new algorithm, the compression–decompression network, based on the research of existing methods. The compression network takes responsibility for inpainting and generating a down-sample image. The decompression network takes responsibility for extending the down-sample image into the original resolution. We use the residual network to construct the compression network and propose a similar pixel selection algorithm to expand the image, which is better than using the super-resolution network. We evaluate our model over Places2 and CelebA data set and use the similarity ratio as the metric. The result shows that our model has better performance when the inpainting task has many conflicts.},
  archive      = {J_MVA},
  author       = {Wu, Zhenghang and Cui, Yidong},
  doi          = {10.1007/s00138-020-01151-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Edge missing image inpainting with compression–decompression network in low similarity images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weighted feature transfer gan for medical image synthesis.
<em>MVA</em>, <em>32</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01152-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that CycleGAN is a highly influential medical image synthesis model. However, the lack of sufficient constraints and the bottleneck layer in auto-encoder network usually lead to blurry image and meaningless features, which may affect medical judgment. In order to synthesize accurate and meaningful medical images, weighted feature transfer GAN (WFT-GAN) is proposed to improve the quality of generated medical image, which is applied to the synthesis of unpaired multi-modal data. WFT-GAN adopts weighted feature transfer (WFT) instead of traditional skip connection to reduce the interference of encoding information on image decoding, while retaining the advantage of skip connection to the information transmission of the generated image. Moreover, the local perceptual adversarial loss combines the VGG feature map and adversarial model to make the local features of the image more meaningful. Experiments in three data sets show that the method in this paper can synthesize higher-quality medical images.},
  archive      = {J_MVA},
  author       = {Yao, Shuaizhen and Tan, Jianhua and Chen, Yi and Gu, Yanhui},
  doi          = {10.1007/s00138-020-01152-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A weighted feature transfer gan for medical image synthesis},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multiple instance learning for airplane detection in
high-resolution imagery. <em>MVA</em>, <em>32</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01153-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic airplane detection in aerial imagery has a variety of applications. Two of the significant challenges in this task are variations in the scale and direction of the airplanes. To solve these challenges, we present a rotation-and-scale-invariant airplane proposal generator. We call this generator symmetric line segments (SLS) that is developed based on the symmetric and regular boundaries of airplanes from the top view. Then, the generated proposals are used to train a deep convolutional neural network for removing non-airplane proposals. Since each airplane can have multiple SLS proposals, where some of them are not in the direction of the fuselage, we collect all proposals corresponding to one ground truth as a positive bag and the others as the negative instances. To have multiple instance deep learning, we modify the loss function of the network to learn from each positive bag at least one instance as well as all negative instances. Finally, we employ non-maximum suppression to remove duplicate detections. Our experiments on NWPU VHR-10 and DOTA datasets show that our method is a promising approach for automatic airplane detection in very high-resolution images. Moreover, we estimate the direction of the airplanes using box-level annotations as an extra achievement.},
  archive      = {J_MVA},
  author       = {Mohammadi, Mohammad Reza},
  doi          = {10.1007/s00138-020-01153-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep multiple instance learning for airplane detection in high-resolution imagery},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hessian-polar context: A descriptor for microfilaria
recognition. <em>MVA</em>, <em>32</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01154-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new effective descriptor for microfilaria. Since microfilaria is a thin elastic object, the proposed descriptor handles it locally. At each medial point of the microfilaria, the local structure of the microfilaria votes for a given shape. Accumulating these votes in the polar domain yields a rich descriptor. Experimental results show the effectiveness of the proposed approach when compared to a set of different well-established methods.},
  archive      = {J_MVA},
  author       = {AL-Tam, Faroq and dos Anjos, António and Shahbazkia, Hamid Reza},
  doi          = {10.1007/s00138-020-01154-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hessian-polar context: A descriptor for microfilaria recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel approach for ear recognition: Learning mahalanobis
distance features from deep CNNs. <em>MVA</em>, <em>32</em>(1), 1–14.
(<a href="https://doi.org/10.1007/s00138-020-01155-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks (CNNs) have been used for ear recognition with the increasing and available ear image databases. However, most known ear recognition methods may be affected by selecting and weighting features; this is always a challenging issue in ear recognition and other pattern recognition applications. Metric learning can address this issue by using an accurate and efficient metric distance called Mahalanobis distance. Therefore, this paper presents a novel approach for ear recognition problems based on a learning Mahalanobis distance metric on deep CNN features. In detail, firstly, various deep features are extracted by adopting VGG and ResNet pre-trained models. Secondly, the discriminant correlation analysis is exploited to eliminate the dimensionality problem. Thirdly, the Mahalanobis distance is learned based on LogDet divergence metric learning. Finally, K-nearest neighbor is used for ear recognition. The experiments are performed on four public ear databases: AWE, USTB II, AMI, and WPUT, and experimental results prove that the proposed approach outperforms the existing state-of-the-art ear recognition methods.},
  archive      = {J_MVA},
  author       = {Omara, Ibrahim and Hagag, Ahmed and Ma, Guangzhi and Abd El-Samie, Fathi E. and Song, Enmin},
  doi          = {10.1007/s00138-020-01155-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel approach for ear recognition: Learning mahalanobis distance features from deep CNNs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel partition selection method for modular face
recognition approaches on occlusion problem. <em>MVA</em>,
<em>32</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01156-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing the face with partial occlusion is an important problem for many face recognition applications. Since the occluded parts have no contribution to recognize the face, these parts should be excluded when performing the classification. In this paper, we propose a new method to detect and to use the non-occluded parts of face image for modular face recognition approaches. The occlusion of a partition is decided using the combination of three coefficients which can be easily derived: (i) image entropy, (ii) image correlation, (iii) root-mean-square error. The performance of the proposed partition selection method is tested using the modular extensions of three subspace-based approaches, namely linear regression classification (LRC), common vector approach (CVA), and discriminative common vector approach (DCVA). Modular DCVA is also proposed for the first time in this paper. After the selection of the non-occluded partitions of the face image, LRC, CVA, and DCVA are applied to each of the partitions independently. Then the classifier supports acquired from each of the partitions are combined using three well-known (product, sum, and Borda count) methods to get the final decision. The experiments implemented on the AR and the Extended Yale B face databases show that selection of the face partitions using the proposed strategy improves the recognition accuracy and outperforms state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Koc, Mehmet},
  doi          = {10.1007/s00138-020-01156-4},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel partition selection method for modular face recognition approaches on occlusion problem},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inception recurrent convolutional neural network for object
recognition. <em>MVA</em>, <em>32</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01157-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network (DCNN) is an influential tool for solving various problems in machine learning and computer vision. Recurrent connectivity is a very important component of visual information processing within the human brain. The idea of recurrent connectivity is rarely applied within convolutional layers, the exceptions being a couple of DCNN architectures including recurrent convolutional neural network (RCNN) in Liang and Hu (in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2015) and Pinheiro and Collobert (in: ICML, 2014). On the other hand, the Inception network architecture has become popular among the computer vision community (Szegedy et al. in Inception-v4, Inception-ResNet and the impact of Residual connections on learning, 2016. arXiv:1602.07261 ). In this paper, we introduce a deep learning architecture called the Inception Recurrent Convolutional Neural Network (IRCNN), which utilizes the power of an Inception network combined with recurrent convolutional layers. Although the inputs are static, the recurrent property plays a huge role in modeling the contextual information for object recognition tasks and thus improves overall training and testing accuracy. In addition, this proposed architecture generalizes both Inception and RCNN models. We have empirically evaluated the recognition performance of the proposed IRCNN model using different benchmark datasets such as MNIST, CIFAR-10, CIFAR-100, and SVHN. The experimental results show higher recognition accuracy when compared to most of the popular DCNNs including the RCNN. Furthermore, we have investigated IRCNN performance against equivalent Inception networks (EIN) and equivalent Inception–Residual networks (EIRN) using the CIFAR-100 dataset. When using the augmented CIFAR-100 dataset, we achieved about 3.5%, 3.47% and 2.54% improvement in classification accuracy compared to the RCNN, EIN, and EIRN respectively. We have also conducted experiment on Tiny ImageNet-200 dataset with IRCNN, EIN, EIRN, RCNN, DenseNet in Huang et al. (Densely connected convolutional networks, 2016. arXiv:1608.06993 ), and DenseNet with Recurrent Convolution Layer, where the proposed model shows significantly better performance against baseline models.},
  archive      = {J_MVA},
  author       = {Alom, Md Zahangir and Hasan, Mahmudul and Yakopcic, Chris and Taha, Tarek M. and Asari, Vijayan K.},
  doi          = {10.1007/s00138-020-01157-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Inception recurrent convolutional neural network for object recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online inspection of narrow overlap weld quality using
two-stage convolution neural network image recognition. <em>MVA</em>,
<em>32</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01158-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In narrow overlap welding, serious defects in the weld will lead to band breakage accident, and the whole hot dip galvanizing unit will be shut down. Laser vision inspection hardware is used to collect real-time image of weld surface, and image defect recognition and evaluation system is developed to real-time detect quality. Firstly, region division is implemented. In view of the characteristics of gray image such as large information, low contrast and blurred edge, an improved image segmentation algorithm is proposed by using image convolution to enhance edge features and combining with integral image, which can quickly and accurately extract the weld edge and divide the region, and the processing time can meet the real-time requirements. Then comparing the effect of traditional method and convolution neural network in identifying defects, VGG16 is used to identify weld defects. In order to ensure real-time performance, a two-stage weld defect recognition is proposed. First, the large defective area is identified, and then, the defect is accurately identified in the area. This method can quickly extract defect areas and complete weld defect classification. Experiments show that the accuracy can reach 97% and average running time takes 3.2 s, meeting the online detection requirements.},
  archive      = {J_MVA},
  author       = {Miao, Rui and Jiang, Zihang and Zhou, Qinye and Wu, Yizhou and Gao, Yuntian and Zhang, Jie and Jiang, Zhibin},
  doi          = {10.1007/s00138-020-01158-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Online inspection of narrow overlap weld quality using two-stage convolution neural network image recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Circular coded target system for industrial applications.
<em>MVA</em>, <em>32</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01159-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coded targets are used as reference targets with a known location during camera calibration, for robust searching of corresponding features between images during various applications of machine vision like object tracking, robot navigation or 3D measurement. In this paper, a target system for industrial photogrammetric applications is outlined. The methods which have been chosen emphasize maximum robustness, along with accuracy at the expense of computational efficiency, since photogrammetric measurements are mainly evaluated offline. The outlined system combines widely used photogrammetric circular coded target design with an automatic library generator. It also utilizes robust methods of target detection and recognition with error correction (in case of 60 15-bit targets, up to 1 bit confused or 2 bits occluded) along with preserving the low false-positive or confusion rate. Any error correction method for this type of targets was not introduced before. The solution also allows for the creation of versatile target libraries or to work with the existing target libraries of a number of commercial photogrammetric systems. The properties of the target system were tested under challenging conditions (including heavy noise, blur, occlusion and geometrical image transformations) and compared to state-of-the-art systems, e.g. TRITOP (GOM), or ArUco, which it outperforms. The target system is already used for the camera calibration of a specialized photogrammetric system utilized in the heavy industry environment.},
  archive      = {J_MVA},
  author       = {Hurník, Jakub and Zatočilová, Aneta and Paloušek, David},
  doi          = {10.1007/s00138-020-01159-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Circular coded target system for industrial applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of a character CAPTCHA recognition system for
the visually impaired community using deep learning. <em>MVA</em>,
<em>32</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01160-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposed an assistive system to recognize the special character CAPTCHAs for the visually impaired community in the Chinese region. To improve the recognition precision, a convolutional neural network (CNN), which is named Captchanet for recognition, was proposed. Firstly, a ten-layer network architecture was designed and three improved training strategies were proposed for the deep learning model. Secondly, a customized Chinese character training set was designed using a novel and fast method, with the view of overcoming the limitation in labeled data collection and uneven data distribution. Finally, the experiments were conducted on the test set gathered from public websites to test the effectiveness of the proposed Captchanet. The statistical results demonstrated that the Captchanet has better classification performance and has obtained higher success rates of recognition than the well-known machine learning approaches and CNN-based approaches.},
  archive      = {J_MVA},
  author       = {Zhang, Xiaohui and Liu, Xinhua and Sarkodie-Gyan, Thompson and Li, Zhixiong},
  doi          = {10.1007/s00138-020-01160-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Development of a character CAPTCHA recognition system for the visually impaired community using deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual localization and servoing for drone use in indoor
remote laboratory environment. <em>MVA</em>, <em>32</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01161-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a localization system for the use of drone in a remote laboratory. The objective is to allow a drone to inspect remote electronic instruments autonomously, as well as to return to its base and land on a platform for the recharge of its batteries. In addition, the drone should be able to detect the presence of a teacher in the laboratory and to center the human face in the image in order to enable remote student–teacher communication. To achieve the first objective, the localization approach is composed of a monocular simultaneous localization and mapping algorithm, parallel tracking and mapping and an estimation based on the homography transform. For the face-drone servoing, the approach is based on the 3D Candide model. Both approaches work in real time. Quantitative and qualitative experiments are presented that show the robustness of both methods.},
  archive      = {J_MVA},
  author       = {Khattar, Fawzi and Luthon, Franck and Larroque, Benoit and Dornaika, Fadi},
  doi          = {10.1007/s00138-020-01161-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Visual localization and servoing for drone use in indoor remote laboratory environment},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deblur and deep depth from single defocus image.
<em>MVA</em>, <em>32</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01162-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle depth estimation and blur removal from a single out-of-focus image. Previously, depth is estimated, and blurred is removed using multiple images; for example, from multiview or stereo scenes, but doing so with a single image is challenging. Earlier works of monocular images for depth estimated and deblurring either exploited geometric characteristics or priors using hand-crafted features. Lately, there is enough evidence that deep convolutional neural networks (CNN) significantly improved numerous vision applications; hence, in this article, we present a depth estimation method that leverages rich representations learned from cascaded convolutional and fully connected neural networks operating on a patch-pooled set of feature maps. Furthermore, from this depth, we computationally reconstruct an all-focus image, i.e., removing the blur and achieve synthetic re-focusing, all from a single image. Our method is fast, and it substantially improves depth accuracy over the state-of-the-art alternatives. Our proposed depth estimation approach can be utilized for everyday scenes without any geometric priors or extra information. Furthermore, our experiments on two benchmark datasets consist images of indoor and outdoor scenes, i.e., Make3D and NYU-v2 demonstrate superior performance in comparison with other available depth estimation state-of-the-art methods by reducing the root-mean-squared error by 57% and 46%, and state-of-the-art blur removal methods by 0.36 dB and 0.72 dB in PSNR, respectively. This improvement in-depth estimation and deblurring is further demonstrated by the superior performance using real defocus images against images captured with a prototype lens.},
  archive      = {J_MVA},
  author       = {Anwar, Saeed and Hayder, Zeeshan and Porikli, Fatih},
  doi          = {10.1007/s00138-020-01162-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deblur and deep depth from single defocus image},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classification of materials using a pulsed time-of-flight
camera. <em>MVA</em>, <em>32</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01163-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an innovative method of material classification based on the imaging model of pulsed time-of-flight (ToF) camera integrated with the unique signature that describes physical properties of each material named reflection point spread function (RPSF). First, the optimization method reduces the effect of material surface interreflection, which would affect RPSF and lead to decreased accuracy in classification, by alternating direction method of multipliers (ADMM). A method named feature vector normalization is proposed to extract material RPSF features. Second, according to the nonlinearity of the feature vectors, the structure of hidden layer neurons of radial basis function (RBF) neural network is optimized based on singular value decomposition (SVD) to improve generalization. Finally, the similar appearance of plastics and metals are classified on turntable-based measurement system by own design. The average classification accuracy reaches 93.3%, and the highest classification accuracy reaches 94.6%.},
  archive      = {J_MVA},
  author       = {Lang, Shinan and Zhang, Jizhong and Cai, Yiheng and Zhu, Xiaoqing and Wu, Qiang},
  doi          = {10.1007/s00138-020-01163-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Classification of materials using a pulsed time-of-flight camera},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TriGAN: Image-to-image translation for multi-source domain
adaptation. <em>MVA</em>, <em>32</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01164-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most domain adaptation methods consider the problem of transferring knowledge to the target domain from a single-source dataset. However, in practical applications, we typically have access to multiple sources. In this paper we propose the first approach for multi-source domain adaptation (MSDA) based on generative adversarial networks. Our method is inspired by the observation that the appearance of a given image depends on three factors: the domain, the style (characterized in terms of low-level features variations) and the content. For this reason, we propose to project the source image features onto a space where only the dependence from the content is kept, and then re-project this invariant representation onto the pixel space using the target domain and style. In this way, new labeled images can be generated which are used to train a final target classifier. We test our approach using common MSDA benchmarks, showing that it outperforms state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Roy, Subhankar and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu and Ricci, Elisa},
  doi          = {10.1007/s00138-020-01164-4},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {TriGAN: Image-to-image translation for multi-source domain adaptation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reciprocal kernel-based weighted collaborative–competitive
representation for robust face recognition. <em>MVA</em>,
<em>32</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01165-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian kernel function is widely used to encode the nonlinear correlations of the face images. However, some issues greatly limit its superiority, for example, it is sensitive to the parameter setting because of its definition based on the exponential operation, on the other hand, the Gaussian kernel needs costly computational time. Besides, the hidden information such as the distance information of the samples is conducive to improving the performance of face recognition. To overcome the above problems, we propose a reciprocal kernel-based weighted collaborative–competitive representation for face recognition. Different from other methods, a new reciprocal kernel is designed to realize the nonlinear representation of the samples. Moreover, a new weight based on the reciprocal kernel is imposed on coding coefficients to disclose the hidden information of the samples in the nonlinear space. With the help of the collaborative–competitive method, the proposed method can well achieve the trade-off between collaborative and competitive representation to promote the performance of face recognition. These factors explicitly encourage the proposed method to be a better representation-type classifier. Finally, extensive experiments are conducted on five benchmark datasets, and the experimental results show that the proposed approach outperforms many state-of-the-art approaches.},
  archive      = {J_MVA},
  author       = {Wang, Shuangxi and Ge, Hongwei and Yang, Jinlong and Tong, Yubing and Su, Shuzhi},
  doi          = {10.1007/s00138-020-01165-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Reciprocal kernel-based weighted collaborative–competitive representation for robust face recognition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
