<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis---117">KIS - 117</h2>
<ul>
<li><details>
<summary>
(2021). Discovering fortress-like cohesive subgraphs. <em>KIS</em>,
<em>63</em>(12), 3217–3250. (<a
href="https://doi.org/10.1007/s10115-021-01624-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morris (Rev Econ Stud 67:57–78, 2000) defines the $$p$$ -cohesion by a connected subgraph in which every vertex has at least a fraction p of its neighbors in the subgraph, i.e., at most a fraction ( $$1-p$$ ) of its neighbors outside. We can find that a $$p$$ -cohesion ensures not only inner-cohesiveness but also outer-sparseness. The textbook on networks by Easley and Kleinberg  (Networks, Crowds, and Markets - Reasoning About a Highly Connected World, Cambridge University Press, 2010) shows that $$p$$ -cohesions are fortress-like cohesive subgraphs which can hamper the entry of the cascade, following the contagion model. Despite the elegant definition and promising properties, to our best knowledge, there is no existing study on $$p$$ -cohesion regarding problem complexity and efficient computing algorithms. In this paper, we fill this gap by conducting a comprehensive theoretical analysis on the complexity of the problem and developing efficient computing algorithms. We focus on the minimal $$p$$ -cohesion because they are elementary units of $$p$$ -cohesions and the combination of multiple minimal $$p$$ -cohesions is a larger $$p$$ -cohesion. We demonstrate that the discovered minimal $$p$$ -cohesions can be utilized to solve the MinSeed problem: finding a smallest set of initial adopters (seeds) such that all the network users are eventually influenced. Extensive experiments on 8 real-life social networks verify the effectiveness of this model and the efficiency of our algorithms.},
  archive      = {J_KIS},
  author       = {Li, Conggai and Zhang, Fan and Zhang, Ying and Qin, Lu and Zhang, Wenjie and Lin, Xuemin},
  doi          = {10.1007/s10115-021-01624-x},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3217-3250},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Discovering fortress-like cohesive subgraphs},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pruning strategies for the efficient traversal of the search
space in PILP environments. <em>KIS</em>, <em>63</em>(12), 3183–3215.
(<a href="https://doi.org/10.1007/s10115-021-01620-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic inductive logic programming (PILP) is a statistical relational learning technique which extends inductive logic programming by considering probabilistic data. The ability to use probabilities to represent uncertainty comes at the cost of an exponential evaluation time when composing theories to model the given problem. For this reason, PILP systems rely on various pruning strategies in order to reduce the search space. However, to the best of the authors’ knowledge, there has been no systematic analysis of the different pruning strategies, how they impact the search space and how they interact with one another. This work presents a unified representation for PILP pruning strategies which enables end-users to understand how these strategies work both individually and combined and to make an informed decision on which pruning strategies to select so as to best achieve their goals. The performance of pruning strategies is evaluated both time and quality-wise in two state-of-the-art PILP systems with datasets from three different domains. Besides analysing the performance of the pruning strategies, we also illustrate the utility of PILP in one of the application domains, which is a real-world application.},
  archive      = {J_KIS},
  author       = {Côrte-Real, Joana and Dutra, Inês and Rocha, Ricardo},
  doi          = {10.1007/s10115-021-01620-1},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3183-3215},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Pruning strategies for the efficient traversal of the search space in PILP environments},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaboratively weighted naive bayes. <em>KIS</em>,
<em>63</em>(12), 3159–3182. (<a
href="https://doi.org/10.1007/s10115-021-01622-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naive Bayes (NB) was once awarded as one of the top 10 data mining algorithms, but the unreliable probability estimation and the unrealistic attribute conditional independence assumption limit its performance. To alleviate these two primary weaknesses simultaneously, instance and attribute weighting has been recently proposed. However, the existing approach learns instance and attribute weights separately, without considering their interactions at all, which restricts the performance of the learned model. Therefore, in this study, we propose a novel approach to learning instance and attribute weights collaboratively and call the resulting model collaboratively weighted naive Bayes (CWNB). In CWNB, we first learn the weight of each training instance iteratively based on its estimated posterior probability loss to make the prior and conditional probabilities more accurate, then we incorporate these two probabilities into the conditional log-likelihood (CLL) formula, and at last we search the optimal weight of each attribute by maximizing the CLL. Extensive experimental results show that CWNB significantly outperforms the standard NB and all the other existing state-of-the-art competitors.},
  archive      = {J_KIS},
  author       = {Zhang, Huan and Jiang, Liangxiao and Li, Chaoqun},
  doi          = {10.1007/s10115-021-01622-z},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3159-3182},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Collaboratively weighted naive bayes},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PGT: News recommendation coalescing personal and global
temporal preferences. <em>KIS</em>, <em>63</em>(12), 3139–3158. (<a
href="https://doi.org/10.1007/s10115-021-01618-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given sequential news watch logs of users, how can we accurately recommend news articles? Compared to other items (e.g., movies and e-commerce products) for a recommendation, the worth of news articles decays quickly and massive news articles are published every second. Moreover, people frequently watch popular news articles regardless of their personal tastes to browse remarkable events at a specific time. Current state-of-the-art methods, designed for other target item domains, show low performance when they are used for news recommendation because of these peculiarities of news articles. In this paper, we propose PGT (News Recommendation Coalescing Personal and Global Temporal Preferences), an accurate news recommendation method designed with consideration of the above properties of news articles. PGT sufficiently reflects users’ behaviors by utilizing latent features extracted from both personal and global temporal preferences. Furthermore, we propose an attention-based architecture to extract adequate coalesced features from both of the preferences. We carefully tune each component of PGT to find optimal architecture. Experimental results show that PGT provides the most accurate news recommendation, giving the state-of-the-art accuracy.},
  archive      = {J_KIS},
  author       = {Koo, Bonhun and Jeon, Hyunsik and Kang, U.},
  doi          = {10.1007/s10115-021-01618-9},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3139-3158},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PGT: News recommendation coalescing personal and global temporal preferences},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural networks for model-free and scale-free automated
planning. <em>KIS</em>, <em>63</em>(12), 3103–3138. (<a
href="https://doi.org/10.1007/s10115-021-01619-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated planning for problems without an explicit model is an elusive research challenge. However, if tackled, it could provide a general approach to problems in real-world unstructured environments. There are currently two strong research directions in the area of artificial intelligence (AI), namely machine learning and symbolic AI. The former provides techniques to learn models of unstructured data but does not provide further problem solving capabilities on such models. The latter provides efficient algorithms for general problem solving, but requires a model to work with. Creating the model can itself be a bottleneck of many problem domains. Complicated problems require an explicit description that can be very costly or even impossible to create. In this paper, we propose a combination of the two areas, namely deep learning and classical planning, to form a planning system that works without a human-encoded model for variably scaled problems. The deep learning part extracts the model in the form of a transition system and a goal-distance heuristic estimator; the classical planning part uses such a model to efficiently solve the planning problem. Both networks in the planning system, we introduced, work with a problem in its graphic form and there is no need for any additional information to create the state transition system or to estimate a heuristic value. We proposed three different architectures for the heuristic estimator to compare different characteristics of well- known deep learning techniques. Besides the design of such planning systems, we provide experimental evaluation comparing the implemented techniques to classical model-based methods.},
  archive      = {J_KIS},
  author       = {Urbanovská, Michaela and Komenda, Antonín},
  doi          = {10.1007/s10115-021-01619-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3103-3138},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Neural networks for model-free and scale-free automated planning},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ProAID: Path-based reasoning for self-attentional disease
prediction. <em>KIS</em>, <em>63</em>(12), 3087–3101. (<a
href="https://doi.org/10.1007/s10115-021-01617-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to introduction and increased availability of Electronic Health Records (EHRs), disease prediction has recently gained immense research attention and achieved impressive progress. Existing methods are based on RNN-like architectures, which treat every disease equally, and learn the representations from medical knowledge. However, strong structural information among diseases is ignored in these methods. In this paper, we introduce a novel Path-based reasoning model for self-AttentIonal Disease prediction (ProAID), which utilizes medical paths extracted from patient EHR and external medical knowledge bases to augment the latent interaction between diseases and learn highly representative patient embeddings. By explicitly incorporating medical paths, ProAID effectively generates embeddings that capture the hierarchical information of diseases and learn effective representations of a patient based on the historical patient admission sequences in her/his EHRs to allow accurate disease prediction for the next hospital admission. Extensive experiments on public medical datasets show that ProAID achieves better performance than the compared methods, which indicates the effectiveness of the proposed model.},
  archive      = {J_KIS},
  author       = {Lu, Xudong and Cui, Lizhen and Sun, Zhenchao and Zhu, Yuening},
  doi          = {10.1007/s10115-021-01617-w},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3087-3101},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {ProAID: Path-based reasoning for self-attentional disease prediction},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal inference for time series analysis: Problems, methods
and evaluation. <em>KIS</em>, <em>63</em>(12), 3041–3085. (<a
href="https://doi.org/10.1007/s10115-021-01621-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data are a collection of chronological observations which are generated by several domains such as medical and financial fields. Over the years, different tasks such as classification, forecasting and clustering have been proposed to analyze this type of data. Time series data have been also used to study the effect of interventions overtime. Moreover, in many fields of science, learning the causal structure of dynamic systems and time series data is considered an interesting task which plays an important role in scientific discoveries. Estimating the effect of an intervention and identifying the causal relations from the data can be performed via causal inference. Existing surveys on time series discuss traditional tasks such as classification and forecasting or explain the details of the approaches proposed to solve a specific task. In this paper, we focus on two causal inference tasks, i.e., treatment effect estimation and causal discovery for time series data and provide a comprehensive review of the approaches in each task. Furthermore, we curate a list of commonly used evaluation metrics and datasets for each task and provide an in-depth insight. These metrics and datasets can serve as benchmark for research in the field.},
  archive      = {J_KIS},
  author       = {Moraffah, Raha and Sheth, Paras and Karami, Mansooreh and Bhattacharya, Anchit and Wang, Qianru and Tahir, Anique and Raglin, Adrienne and Liu, Huan},
  doi          = {10.1007/s10115-021-01621-0},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {3041-3085},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Causal inference for time series analysis: Problems, methods and evaluation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic enrichment of documents: A classification
perspective for ontology-based imbalanced semantic descriptions.
<em>KIS</em>, <em>63</em>(11), 3001–3039. (<a
href="https://doi.org/10.1007/s10115-021-01615-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel framework for the semantic enrichment of documents, exploiting the hierarchical ontological knowledge of a domain in conjunction with classification techniques. The main contributions of this work are fourfold: (a) a well-defined theoretical model for the semantic representation and enrichment of documents is defined, (b) a method for dealing with the problem of class imbalance is outlined, based on the transformation of the document representations into more balanced ones, (c) a methodology is proposed for assigning semantic labels in those cases where it is hard to decide which label fits best and (d) a set of novel metrics for evaluating the performance of the suggested framework are introduced. The extensive experimental procedure that follows, conducted on two popular datasets, exhibits promising results and constitutes a proof of the robustness of the overall approach.},
  archive      = {J_KIS},
  author       = {Stratogiannis, Georgios and Kouris, Panagiotis and Alexandridis, Georgios and Siolas, Georgios and Stamou, Giorgos and Stafylopatis, Andreas},
  doi          = {10.1007/s10115-021-01615-y},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {3001-3039},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semantic enrichment of documents: A classification perspective for ontology-based imbalanced semantic descriptions},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3-3FS: Ensemble method for semi-supervised multi-label
feature selection. <em>KIS</em>, <em>63</em>(11), 2969–2999. (<a
href="https://doi.org/10.1007/s10115-021-01616-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has received considerable attention over the past decade. However, it is continuously challenged by new emerging issues. Semi-supervised multi-label learning is one of these promising novel approaches. In this work, we refer to it as an approach that combines data consisting of a huge amount of unlabeled instances with a small number of multi-labeled instances. Semi-supervised multi-label feature selection, like conventional feature selection algorithms, has a rather poor record as regards stability (i.e. robustness with respect to changes in data). To address this weakness and improve the robustness of the feature selection process in high-dimensional data, this document develops an ensemble methodology based on a 3-way resampling of data: (1) Bagging, (2) a random subspace method (RSM) and (3) an additional random sub-labeling strategy (RSL). The proposed framework contributes to enhancing the stability of feature selection algorithms and to improving their performance. Our research findings illustrate that bagging and RSM help improve the stability of the feature selection process and increase learning accuracy, while RSL addresses label correlation, which is a major concern with multi-label data. The paper presents the key findings of a series of experiments, which we conducted on selected benchmark data sets in the classification task. Results are promising, highlighting that the proposed method either outperforms state-of-the-art algorithms or produces at least comparable results.},
  archive      = {J_KIS},
  author       = {Alalga, Abdelouahid and Benabdeslem, Khalid and Mansouri, Dou El Kefel},
  doi          = {10.1007/s10115-021-01616-x},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2969-2999},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {3-3FS: Ensemble method for semi-supervised multi-label feature selection},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized maximal utility for mining high average-utility
itemsets. <em>KIS</em>, <em>63</em>(11), 2947–2967. (<a
href="https://doi.org/10.1007/s10115-021-01614-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high average-utility itemsets (HAUIs) is a promising research topic in data mining because, in contrast to high utility itemsets, they are not biased toward long itemsets. Regardless of what upper bounds and pruning strategies are used, most existing HAUI mining algorithms are founded on the concept of maximal utility, namely the highest utility of a single item in each transaction. In this paper, we study this problem by generalizing the typical maximal utility and average-utility upper bound from a single item to an itemset, and propose an efficient HAIU mining algorithm based on generalized maximal utility (HAUIM-GMU). For this algorithm, we first propose the concepts of generalized maximal utility and the generalized average-utility upper bound, and discuss how the proposed upper bound can be made tighter to generate fewer candidates. A new pruning strategy is then proposed based on the concept of support, and this is shown to be effective for filtering out unpromising itemsets. The final algorithm is described in detail. Extensive experimental results show that the HAUIM-GMU algorithm outperforms existing state-of-the-art algorithms.},
  archive      = {J_KIS},
  author       = {Song, Wei and Liu, Lu and Huang, Chaomin},
  doi          = {10.1007/s10115-021-01614-z},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2947-2967},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Generalized maximal utility for mining high average-utility itemsets},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An experimental analysis on evolutionary ontology
meta-matching. <em>KIS</em>, <em>63</em>(11), 2919–2946. (<a
href="https://doi.org/10.1007/s10115-021-01613-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every year, new ontology matching approaches have been published to address the heterogeneity problem in ontologies. It is well known that no one is able to stand out from others in all aspects. An ontology meta-matcher combines different alignment techniques to explore various aspects of heterogeneity to avoid the alignment performance being restricted to some ontology characteristics. The meta-matching process consists of several stages of execution, and sometimes the contribution/cost of each algorithm is not clear when evaluating an approach. This article presents the evaluation of solutions commonly used in the literature in order to provide more knowledge about the ontology meta-matching problem. Results showed that the more characteristics of the entities that can be captured by similarity measures set, the greater the accuracy of the model. It was also possible to observe the good performance and accuracy of local search-based meta-heuristics when compared to global optimization meta-heuristics. Experiments with different objective functions have shown that semi-supervised methods can shorten the execution time of the experiment but, on the other hand, bring more instability to the result.},
  archive      = {J_KIS},
  author       = {Ferranti, Nicolas and de Souza, Jairo Francisco and Sã Rosário Furtado Soares, Stênio},
  doi          = {10.1007/s10115-021-01613-0},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2919-2946},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An experimental analysis on evolutionary ontology meta-matching},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven valued dominance relation in incomplete ordered
decision system. <em>KIS</em>, <em>63</em>(11), 2901–2917. (<a
href="https://doi.org/10.1007/s10115-021-01607-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dominance-based rough set approach is successfully applied to analyze multicriteria decision problems. For the incomplete ordered decision system, its various extensions have been proposed. The valued dominance relation is such an extension. However, the general calculation of dominance degree between objects depends on a prior distribution of incomplete ordered decision system, and how to choose a suitable threshold is also difficult. To solve these problems, a data-driven valued dominance relation is proposed in this paper. First of all, an objective calculation method of dominance degree between objects is designed, which is based on probability statistics. Moreover, this method is more effective for big data sets with a large quantity of objects. Secondly, an automatic threshold calculation method is presented, which does not depend on any prior knowledge except data sets. Finally, some properties of this method are investigated. Experimental results show that this method is superior to other generalized dominance relations in dealing with incomplete information.},
  archive      = {J_KIS},
  author       = {Guan, Lihe},
  doi          = {10.1007/s10115-021-01607-y},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2901-2917},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Data-driven valued dominance relation in incomplete ordered decision system},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive weighted least squares regression for subspace
clustering. <em>KIS</em>, <em>63</em>(11), 2883–2900. (<a
href="https://doi.org/10.1007/s10115-021-01612-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research paper, we consider the subspace clustering problem which aims at finding a low-dimensional representation of a high-dimensional data set. In particular, our central focus is upon the least squares regression based on which we elaborate an adaptive weighted least squares regression for subspace clustering. Compared to the least squares regression, we consider the data locality to adaptively select relevant and close samples and discard irrelevant and faraway ones. Additionally, we impose a weight matrix on the representation errors to adaptively highlight the meaningful features and minimize the effect of redundant/noisy ones. Finally, we also add a non-negativity constraint on the representation coefficients to enhance the graph interpretability. These interesting properties allow to build up a more informative and quality graph, thereby yielding very promising clustering results. Extensive experiments on synthetic and real databases demonstrated that our clustering method achieves consistently optimal results, compared to multiple clustering methods.},
  archive      = {J_KIS},
  author       = {Bouhlel, Noura and Feki, Ghada and Ben Amar, Chokri},
  doi          = {10.1007/s10115-021-01612-1},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2883-2900},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adaptive weighted least squares regression for subspace clustering},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer alignment network for blind unsupervised domain
adaptation. <em>KIS</em>, <em>63</em>(11), 2861–2881. (<a
href="https://doi.org/10.1007/s10115-021-01608-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we transfer the knowledge from a source domain to a target domain when each side cannot observe the data in the other side? Recent transfer learning methods show significant performance in classification tasks by leveraging both source and target data simultaneously at training time. However, leveraging both source and target data simultaneously is often impossible due to privacy reasons. In this paper, we define the problem of unsupervised domain adaptation under blind constraint, where each of the source and the target domains cannot observe the data in the other domain, but data from both domains are used for training. We propose TAN (Transfer Alignment Network for Blind Domain Adaptation), an effective method for the problem by aligning source and target domain features in the blind setting. TAN maps the target feature into source feature space so that the classifier learned from the labeled data in the source domain is readily used in the target domain. Extensive experiments show that TAN (1) provides the state-of-the-art accuracy for blind domain adaptation outperforming the standard supervised learning by up to 9.0% and (2) performs well regardless of the proportion of target domain data in the training data.},
  archive      = {J_KIS},
  author       = {Xu, Huiwen and Kang, U},
  doi          = {10.1007/s10115-021-01608-x},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2861-2881},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Transfer alignment network for blind unsupervised domain adaptation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Word and graph attention networks for semi-supervised
classification. <em>KIS</em>, <em>63</em>(11), 2841–2859. (<a
href="https://doi.org/10.1007/s10115-021-01610-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph attention networks are effective graph neural networks that perform graph embedding for semi-supervised learning, which considers the neighbors of a node when learning its features. This paper presents a novel attention-based graph neural network that introduces an attention mechanism in the word-represented features of a node together incorporating the neighbors’ attention in the embedding process. Instead of using a vector as the feature of a node in the traditional graph attention networks, the proposed method uses a 2D matrix to represent a node, where each row in the matrix stands for a different attention distribution against the original word-represented features of a node. Then, the compressed features are fed into a graph attention layer that aggregates the matrix representation of the node and its neighbor nodes with different attention weights as a new representation. By stacking several graph attention layers, it obtains the final representation of nodes as matrices, which considers both that the neighbors of a node have different importance and that the words also have different importance in their original features. Experimental results on three citation network datasets show that the proposed method significantly outperforms eight state-of-the-art methods in semi-supervised classification tasks.},
  archive      = {J_KIS},
  author       = {Zhang, Jing and Li, Mengxi and Gao, Kaisheng and Meng, Shunmei and Zhou, Cangqi},
  doi          = {10.1007/s10115-021-01610-3},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2841-2859},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Word and graph attention networks for semi-supervised classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DyFT: A dynamic similarity search method on integer
sketches. <em>KIS</em>, <em>63</em>(11), 2815–2840. (<a
href="https://doi.org/10.1007/s10115-021-01611-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity-preserving hashing is a core technique for fast similarity searches, and it randomly maps data points in a metric space to strings of discrete symbols (i.e., sketches) in the Hamming space. While traditional hashing techniques produce binary sketches, recent ones produce integer sketches for preserving various similarity measures. However, most similarity search methods are designed for binary sketches and inefficient for integer sketches. Moreover, most methods are either inapplicable or inefficient for dynamic datasets, although modern real-world datasets are updated over time. We propose dynamic filter trie (DyFT), a dynamic similarity search method for both binary and integer sketches. An extensive experimental analysis using large real-world datasets shows that DyFT performs superiorly with respect to scalability, time performance, and memory efficiency. For example, on a huge dataset of 216 million data points, DyFT performs a similarity search 6000 times faster than a state-of-the-art method while reducing to one-thirteenth in memory.},
  archive      = {J_KIS},
  author       = {Kanda, Shunsuke and Tabei, Yasuo},
  doi          = {10.1007/s10115-021-01611-2},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {2815-2840},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DyFT: A dynamic similarity search method on integer sketches},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HAR-sEMG: A dataset for human activity recognition on
lower-limb sEMG. <em>KIS</em>, <em>63</em>(10), 2791–2814. (<a
href="https://doi.org/10.1007/s10115-021-01598-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, human activity recognition (HAR) has grown in popularity due to its applications in security and entertainment. As recent years have witnessed the emergence of health care and exoskeleton robotics which make use of wearable suits, human–machine interaction based on action recognition performs an important role in multimedia applications. Considering the limitations of the application scenario, the surface electromyography (sEMG) signal stands out in many wearable data collection devices for HAR. That is because: (1) timely feedback; (2) no damage to the human body; and (3) the wide range of recognizable actions. However, existing public datasets of sEMG contained relatively few activities, and several large-scale datasets only collected the action of the hand. In addition, the processing of sEMG signals is a new field with no effective evaluation system for it. To tackle these problems, we establish a novel dataset for HAR on lower-limb sEMG named “HAR-sEMG,” using 6 sEMG signal sensors attached to the left leg. A benchmark summarizing experiments with many combinations of existing high-dimensional signal processing algorithms-based manifold learning on our dataset is also provided for a performance analysis.},
  archive      = {J_KIS},
  author       = {Luan, Yu and Shi, Yuhang and Wu, Wanyin and Liu, Zhiyao and Chang, Hai and Cheng, Jun},
  doi          = {10.1007/s10115-021-01598-w},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2791-2814},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HAR-sEMG: A dataset for human activity recognition on lower-limb sEMG},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extensible ontology-based views for business process models.
<em>KIS</em>, <em>63</em>(10), 2763–2789. (<a
href="https://doi.org/10.1007/s10115-021-01604-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A business process model may be used as both a communication artefact for gathering and sharing knowledge of a business practice among stakeholders and as a specification for the automation of the process by a Business Process Management System (BPMS). For each of these uses, it is desirable to have an ability to visualise the process model from a range of different perspectives and at various levels of granularity. Such views are a common feature of enterprise architecture frameworks, but process modelling and management systems and tools generally have a limited number of available views to offer. This paper presents a taxonomy of process views that are presented in the literature and then proposes the definition and use of a common process model ontology, from which an extensible range of process views may be derived. The approach is illustrated through the realisation of a plug-in component for the YAWL BPMS, although it is by no means limited to that environment. The component illustrates that the process views frequently mentioned in the literature as desirable can be effectively implemented and extended using an ontology-based approach. It is envisaged that the accessibility of a repertoire of views that support business process development will lead to greater efficiencies through more accurate process definitions and improved change management support.},
  archive      = {J_KIS},
  author       = {Adams, Michael and Hense, Andreas V. and Hofstede, Arthur H. M. ter},
  doi          = {10.1007/s10115-021-01604-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2763-2789},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Extensible ontology-based views for business process models},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simple question answering over knowledge graph enhanced by
question pattern classification. <em>KIS</em>, <em>63</em>(10),
2741–2761. (<a
href="https://doi.org/10.1007/s10115-021-01609-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering over knowledge graph (KGQA), which automatically answers natural language questions by querying the facts in knowledge graph (KG), has drawn significant attention in recent years. In this paper, we focus on single-relation questions, which can be answered through a single fact in KG. This task is a non-trivial problem since capturing the meaning of questions and selecting the golden fact from billions of facts in KG are both challengeable. We propose a pipeline framework for KGQA, which consists of three cascaded components: (1) an entity detection model, which can label the entity mention in the question; (2) a novel entity linking model, which considers the contextual information of candidate entities in KG and builds a question pattern classifier according to the correlations between question patterns and relation types to mitigate entity ambiguity problem; and (3) a simple yet effective relation detection model, which is used to match the semantic similarity between the question and relation candidates. Substantial experiments on the SimpleQuestions benchmark dataset show that our proposed method could achieve better performance than many existing state-of-the-art methods on accuracy, top-N recall and other evaluation metrics.},
  archive      = {J_KIS},
  author       = {Cui, Hai and Peng, Tao and Feng, Lizhou and Bao, Tie and Liu, Lu},
  doi          = {10.1007/s10115-021-01609-w},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2741-2761},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Simple question answering over knowledge graph enhanced by question pattern classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improvements on approximation algorithms for clustering
probabilistic data. <em>KIS</em>, <em>63</em>(10), 2719–2740. (<a
href="https://doi.org/10.1007/s10115-021-01601-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty about data appears in many real-world applications and an important issue is how to manage, analyze and solve optimization problems over such data. An important tool for data analysis is clustering. When the data set is uncertain, we can model them as a set of probabilistic points each formalized as a probability distribution function which describes the possible locations of the points. In this paper, we study k-center problem for probabilistic points in a general metric space. First, we present a fast greedy approximation algorithm that builds k centers using a farthest-first traversal in k iterations. This algorithm improves the previous approximation factor of the unrestricted assigned k-center problem from 10 (see [1]) to 6. Next, we restrict the centers to be selected from all the probabilistic locations of the given points and we show that an optimal solution for this restricted setting is a 2-approximation factor solution for an optimal solution of the assigned k-center problem with expected distance assignment. Using this idea, we improve the approximation factor of the unrestricted assigned k-center problem to 4 by increasing the running time. The algorithm also runs in polynomial time when k is a constant. Additionally, we implement our algorithms on three real data sets. The experimental results show that in practice the approximation factors of our algorithms are better than in theory for these data sets. Also we compare the results of our algorithm with the previous works and discuss about the achieved results. At the end, we present our theoretical results for probabilistic k-median clustering.},
  archive      = {J_KIS},
  author       = {Alipour, Sharareh},
  doi          = {10.1007/s10115-021-01601-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2719-2740},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improvements on approximation algorithms for clustering probabilistic data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A turning point prediction method of stock price based on
RVFL-GMDH and chaotic time series analysis. <em>KIS</em>,
<em>63</em>(10), 2693–2718. (<a
href="https://doi.org/10.1007/s10115-021-01602-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock market prediction is extremely important for investors because knowing the future trend of stock prices will reduce the risk of investing capital for profit. Therefore, seeking an accurate, fast, and effective approach to identify the stock market movement is of great practical significance. This study proposes a novel turning point prediction method for the time series analysis of stock price. Through the chaos theory analysis and application, we put forward a new modeling approach for the nonlinear dynamic system. The turning indicator of time series is computed firstly; then, by applying the RVFL-GMDH model, we perform the turning point prediction of the stock price, which is based on the fractal characteristic of a strange attractor with an infinite self-similar structure. The experimental findings confirm the efficacy of the proposed procedure and have become successful for the intelligent decision support of the stock trading strategy.},
  archive      = {J_KIS},
  author       = {Chen, Junde and Yang, Shuangyuan and Zhang, Defu and Nanehkaran, Y. A.},
  doi          = {10.1007/s10115-021-01602-3},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2693-2718},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A turning point prediction method of stock price based on RVFL-GMDH and chaotic time series analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo support vector domain description to train large-size
and continuously growing datasets. <em>KIS</em>, <em>63</em>(10),
2671–2692. (<a
href="https://doi.org/10.1007/s10115-021-01606-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector domain description (SVDD) is a data description method inspired by support vector machine (SVM). This classifier describes a set of data points with a sphere that encloses the majority of them and has a minimal volume. The boundary of this sphere is used to classify new samples. SVDD has been successfully applied to many challenging classification problems and has shown a good generalization capability. However, this classifier still has some major weaknesses. This paper focuses on two of them: The first regards the large amount of memory and computational time required by SVDD in the training step. This problem manifests most strongly when dealing with large-size datasets and can hinder or prevent its use. This paper presents an approximate solution to this problem that permits to apply SVDD to large-scale datasets. This new version is based on divide-and-conquer strategy and it processes in two steps: It begins by dividing the whole large-size dataset into random subsets that each can be described efficiently with a small sphere using SVDD. Then, it applies our new algorithm that can find the smallest sphere that encloses the minimal spheres built in the previous step. The second weak point of standard SVDD concerns its static learning process. This classifier must be re-trained with the whole dataset each time when new training samples are available. This paper proposes a new dynamic approach that only trains the new samples with SVDD and incorporates the resulting minimal sphere with the previous one (s) to construct the smallest sphere that encloses all the samples. Like Support Vector Domain Description, the proposed approach can be extended to non-linear classification cases by using kernel functions. Experimental results on artificial and real datasets have successfully validated the performance of our approach.},
  archive      = {J_KIS},
  author       = {El Boujnouni, Mohamed},
  doi          = {10.1007/s10115-021-01606-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2671-2692},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Pseudo support vector domain description to train large-size and continuously growing datasets},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DORIC: Discovering topological relations based on spatial
link composition. <em>KIS</em>, <em>63</em>(10), 2645–2669. (<a
href="https://doi.org/10.1007/s10115-021-01603-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of the Semantic Web technologies, more and more spatial knowledge bases are being published on the Web. Discovering spatial links among spatial knowledge bases is crucial in achieving real-time applications such as reasoning and question answering over spatial linked data. However, existing approaches rely on numerous high-cost Dimensionally Extended Nine-Intersection Model (DE-9IM) computations which lead to inefficient spatial link discovery. To address this problem, we propose a novel approach for discovering topological relations based on the spatial link composition, namely DORIC. Different from conventional spatial link discovery methods, DORIC further reduces the required number of DE-9IM computations by composing existing spatial links. Specifically, we first propose a spatial link composition (SLC) model to infer new spatial links of topological relations from existing or intermediate links. We replace part of high-cost DE-9IM computations with relatively low-cost SLC, and it leads to reduced spatial link discovery time. Then to maximize the utility of SLC during the process of DORIC, we design two effective strategies for deciding the discovery and access orders. Experiments on three real-world datasets show that the proposed DORIC outperforms the state-of-the-art approaches in terms of the spatial link discovery time.},
  archive      = {J_KIS},
  author       = {Jin, Xiongnan and Eom, Sungkwang and Shin, Sangjin and Lee, Kyong-Ho and Hong, Chaoqun},
  doi          = {10.1007/s10115-021-01603-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2645-2669},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DORIC: Discovering topological relations based on spatial link composition},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bypassing combinatorial explosions in equivalence structure
extraction. <em>KIS</em>, <em>63</em>(10), 2621–2644. (<a
href="https://doi.org/10.1007/s10115-021-01599-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equivalence structure (ES) extraction enables us to determine correspondence relations within a dataset or between multiple datasets. Applications of ES extraction include the analysis of time series data, preprocessing of imitation learning, and preprocessing of transfer learning. Currently, pairwise incremental search (PIS) is the fastest method to extract ESs; however, a combinatorial explosion can occur when employing this method. In this paper, we show that combinatorial explosion is a problem that occurs in the PIS, and we propose a new method where this problem does not occur. We evaluate the proposed method via experiments; the results show that our proposed method is 39 times faster than the PIS for synthetic datasets where a 20-dimensional ES exists. For the experiment using video datasets, the proposed method enabled us to obtain a 29-dimensional ES, whereas the PIS did not because the memory usage reached its limit when the number of dimensions was 9. In this experiment, the total processing time for our proposed method up to 29 dimensions was 6.3 times shorter than that for PIS up to even 8 dimensions.},
  archive      = {J_KIS},
  author       = {Satoh, Seiya and Yamakawa, Hiroshi},
  doi          = {10.1007/s10115-021-01599-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2621-2644},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Bypassing combinatorial explosions in equivalence structure extraction},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model complexity of deep learning: A survey. <em>KIS</em>,
<em>63</em>(10), 2585–2619. (<a
href="https://doi.org/10.1007/s10115-021-01605-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model complexity is a fundamental problem in deep learning. In this paper, we conduct a systematic overview of the latest studies on model complexity in deep learning. Model complexity of deep learning can be categorized into expressive capacity and effective model complexity. We review the existing studies on those two categories along four important factors, including model framework, model size, optimization process, and data complexity. We also discuss the applications of deep learning model complexity including understanding model generalization, model optimization, and model selection and design. We conclude by proposing several interesting future directions.},
  archive      = {J_KIS},
  author       = {Hu, Xia and Chu, Lingyang and Pei, Jian and Liu, Weiqing and Bian, Jiang},
  doi          = {10.1007/s10115-021-01605-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {2585-2619},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Model complexity of deep learning: A survey},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imputing sentiment intensity for SaaS service quality
aspects using t-nearest neighbors with correlation-weighted euclidean
distance. <em>KIS</em>, <em>63</em>(9), 2541–2584. (<a
href="https://doi.org/10.1007/s10115-021-01591-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid, increasing adoption of businesses to deliver their services in Software as a Service (SaaS) products in the marketplace presents selection challenges to users. Recently, major cloud service providers such as Amazon Web Services and Microsoft Azure have introduced well-architected frameworks that assess SaaS products in different pillars (also referred to herein as features). Furthermore, customers leave feedback on these features after using SaaS products. However, they do not provide feedback on all the features of a product, which renders the reviews unusable to prospective users needing to assess a product’s quality before committing to it. Our study addresses this drawback by imputing or inferring the intensity of the customer’s feedback on features that they do not mention in their reviews. Specifically, we propose threshold-based nearest neighbors (T-NN) as an extension of the conventional k-nearest neighbor approach to determine the missing sentiment intensity score of a feature from the values of its other features. We evaluate the proposed approach in two different systems and compare our results with seven other data imputation techniques. The results show that the proposed T-NN approach performs better than the other imputation approaches on the SaaS sentiment dataset.},
  archive      = {J_KIS},
  author       = {Raza, Muhammad and Hussain, Farookh Khadeer and Hussain, Omar K. and Rehman, Zia ur and Zhao, Ming},
  doi          = {10.1007/s10115-021-01591-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2541-2584},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Imputing sentiment intensity for SaaS service quality aspects using T-nearest neighbors with correlation-weighted euclidean distance},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sharp characterization of optimal minibatch size for
stochastic finite sum convex optimization. <em>KIS</em>, <em>63</em>(9),
2513–2539. (<a
href="https://doi.org/10.1007/s10115-021-01593-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minibatching technique has been extensively adopted to facilitate stochastic first-order methods because of their computational efficiency in parallel computing for large-scale machine learning and data mining. Indeed, increasing the minibatch size decreases the iteration complexity (number of minibatch queries) to converge, resulting in the decrease of the running time by processing a minibatch in parallel. However, this gain is usually saturated for too large minibatch sizes and the total computational complexity (number of access to an example) is deteriorated. Hence, the determination of an appropriate minibatch size which controls the trade-off between the iteration and total computational complexities is important to maximize performance of the method with as few computational resources as possible. In this study, we define the optimal minibatch size as the minimum minibatch size with which there exists a stochastic first-order method that achieves the optimal iteration complexity and we call such a method the optimal minibatch method. Moreover, we show that Katyusha (in: Proceedings of annual ACM SIGACT symposium on theory of computing vol 49, pp 1200–1205, ACM, 2017), DASVRDA (Murata and Suzuki, in: Advances in neural information processing systems vol 30, pp 608–617, 2017), and the proposed method which is a combination of Acc-SVRG (Nitanda, in: Advances in neural information processing systems vol 27, pp 1574–1582, 2014) with APPA (Cotter et al. in: Advances in neural information processing systems vol 27, pp 3059–3067, 2014) are optimal minibatch methods. In experiments, we compare optimal minibatch methods with several competitors on $$L_1$$ -and $$L_2$$ -regularized logistic regression problems and observe that iteration complexities of optimal minibatch methods linearly decrease as minibatch sizes increase up to reasonable minibatch sizes and finally attain the best iteration complexities. This confirms the computational efficiency of optimal minibatch methods suggested by the theory.},
  archive      = {J_KIS},
  author       = {Nitanda, Atsushi and Murata, Tomoya and Suzuki, Taiji},
  doi          = {10.1007/s10115-021-01593-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2513-2539},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sharp characterization of optimal minibatch size for stochastic finite sum convex optimization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep reinforcement learning-based resource allocation and
seamless handover in multi-access edge computing based on SDN.
<em>KIS</em>, <em>63</em>(9), 2479–2511. (<a
href="https://doi.org/10.1007/s10115-021-01590-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the access devices that are densely deployed in multi-access edge computing environments, users frequently switch access devices when moving, which causes the imbalance of network load and the decline of service quality. To solve the problems above, a seamless handover scheme for wireless access points based on perception is proposed. First, a seamless handover model based on load perception is proposed to solve the unbalanced network load, in which a seamless handover algorithm for wireless access points is used to calculate the access point with the highest weight, and a software-defined network controller controls the switching process. A joint allocation method of communication and computing resources based on deep reinforcement learning is proposed to minimize the terminal energy consumption and the system delay. A resource allocation model is based on minimizing terminal energy consumption, and system delay is built. The optimal value of task offloading decision and resource allocation vector are calculated with deep reinforcement learning. Experimental results show that the proposed method can reduce the network load and the task execution cost.},
  archive      = {J_KIS},
  author       = {Li, Chunlin and Zhang, Yong and Luo, Youlong},
  doi          = {10.1007/s10115-021-01590-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2479-2511},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep reinforcement learning-based resource allocation and seamless handover in multi-access edge computing based on SDN},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy protection of user profiles in online search via
semantic randomization. <em>KIS</em>, <em>63</em>(9), 2455–2477. (<a
href="https://doi.org/10.1007/s10115-021-01597-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Querying a search engine is one of the most frequent activities performed by Internet users. As queries are submitted, the server collects and aggregates them to build detailed user profiles. While user profiles are used to offer personalized search services, they may also be employed in behavioral targeting or, even worse, be transferred to third parties. Proactive protection of users&#39; privacy in front of search engines has been tackled by submitting fake queries that aim at distorting the users&#39; real profile. However, most approaches submit either random queries (which do not allow controlling the profile distortion) or queries constructed by following deterministic algorithms (which may be detected by aware search engines). In this paper, we propose a semantically grounded method to generate fake queries that (i) is driven by the privacy requirements of the user, (ii) submits the least number of fake queries needed to fulfill the requirements and (iii) creates queries in a non-deterministic way. Unlike related works, we accurately analyze and exploit the semantics underlying to user queries and their influence in the resulting profile. As a result, our approach offers more control—because users can tailor how their profile should be protected—and greater efficiency—because the desired protection is achieved with fewer fake queries. The experimental results on real query logs illustrate the benefits of our approach.},
  archive      = {J_KIS},
  author       = {Rodriguez-Garcia, Mercedes and Batet, Montserrat and Sánchez, David and Viejo, Alexandre},
  doi          = {10.1007/s10115-021-01597-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2455-2477},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Privacy protection of user profiles in online search via semantic randomization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward data-driven solutions to interactive dynamic
influence diagrams. <em>KIS</em>, <em>63</em>(9), 2431–2453. (<a
href="https://doi.org/10.1007/s10115-021-01600-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the availability of significant amount of data, data-driven decision making becomes an alternative way for solving complex multiagent decision problems. Instead of using domain knowledge to explicitly build decision models, the data-driven approach learns decisions (probably optimal ones) from available data. This removes the knowledge bottleneck in the traditional knowledge-driven decision making, which requires a strong support from domain experts. In this paper, we study data-driven decision making in the context of interactive dynamic influence diagrams (I-DIDs)—a general framework for multiagent sequential decision making under uncertainty. We propose a data-driven framework to solve the I-DIDs model and focus on learning the behavior of other agents in problem domains. The challenge is on learning a complete policy tree that will be embedded in the I-DIDs models due to limited data. We propose two new methods to develop complete policy trees for the other agents in the I-DIDs. The first method uses a simple clustering process, while the second one employs sophisticated statistical checks. We analyze the proposed algorithms in a theoretical way and experiment them over two problem domains.},
  archive      = {J_KIS},
  author       = {Pan, Yinghui and Tang, Jing and Ma, Biyang and Zeng, Yifeng and Ming, Zhong},
  doi          = {10.1007/s10115-021-01600-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2431-2453},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Toward data-driven solutions to interactive dynamic influence diagrams},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OpenWGL: Open-world graph learning for unseen class node
classification. <em>KIS</em>, <em>63</em>(9), 2405–2430. (<a
href="https://doi.org/10.1007/s10115-021-01594-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph learning, such as node classification, is typically carried out in a closed-world setting. A number of nodes are labeled, and the learning goal is to correctly classify remaining (unlabeled) nodes into classes, represented by the labeled nodes. In reality, due to limited labeling capability or dynamic evolving nature of networks, some nodes in the networks may not belong to any existing/seen classes and therefore cannot be correctly classified by closed-world learning algorithms. In this paper, we propose a new open-world graph learning paradigm, where the learning goal is to correctly classify nodes belonging to labeled classes into correct categories and also classify nodes not belonging to labeled classes to an unseen class. Open-world graph learning has three major challenges: (1) Graphs do not have features to represent nodes for learning; (2) unseen class nodes do not have labels and may exist in an arbitrary form different from labeled classes; and (3) graph learning should differentiate whether a node belongs to an existing/seen class or an unseen class. To tackle the challenges, we propose an uncertain node representation learning principle to use multiple versions of node feature representation to test a classifier’s response on a node, through which we can differentiate whether a node belongs to the unseen class. Technical wise, we propose constrained variational graph autoencoder, using label loss and class uncertainty loss constraints, to ensure that node representation learning is sensitive to the unseen class. As a result, node embedding features are denoted by distributions, instead of deterministic feature vectors. In order to test the certainty of a node belonging to seen classes, a sampling process is proposed to generate multiple versions of feature vectors to represent each node, using automatic thresholding to reject nodes not belonging to seen classes as unseen class nodes. Experiments, using graph convolutional networks and graph attention networks on four real-world networks, demonstrate the algorithm performance. Case studies and ablation analysis also show the advantage of the uncertain representation learning and automatic threshold selection for open-world graph learning.},
  archive      = {J_KIS},
  author       = {Wu, Man and Pan, Shirui and Zhu, Xingquan},
  doi          = {10.1007/s10115-021-01594-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2405-2430},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {OpenWGL: Open-world graph learning for unseen class node classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tweet-scan-post: A system for analysis of sensitive private
data disclosure in online social media. <em>KIS</em>, <em>63</em>(9),
2365–2404. (<a
href="https://doi.org/10.1007/s10115-021-01592-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social media technologies are open to users who are intended in creating a community and publishing their opinions of recent incidents. The participants of the online social networking sites remain ignorant of the criticality of disclosing personal data to the public audience. The private data of users are at high risk leading to many adverse effects like cyberbullying, identity theft, and job loss. This research work aims to define the user entities or data like phone number, email address, family details, health-related information as user’s sensitive private data (SPD) in a social media platform. The proposed system, Tweet-Scan-Post (TSP), is mainly focused on identifying the presence of SPD in user’s posts under personal, professional, and health domains. The TSP framework is built based on the standards and privacy regulations established by social networking sites and organizations like NIST, DHS, GDPR. The proposed approach of TSP addresses the prevailing challenges in determining the presence of sensitive PII, user privacy within the bounds of confidentiality and trustworthiness. A novel layered classification approach with various state-of-art machine learning models is used by the TSP framework to classify tweets as sensitive and insensitive. The findings of TSP systems include 201 Sensitive Privacy Keywords using a boosting strategy, sensitivity scaling that measures the degree of sensitivity allied with a tweet. The experimental results revealed that personal tweets were highly related to mother and children, professional tweets with apology, and health tweets with concern over the father’s health condition.},
  archive      = {J_KIS},
  author       = {Geetha, R. and Karthika, S. and Kumaraguru, Ponnurangam},
  doi          = {10.1007/s10115-021-01592-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2365-2404},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tweet-scan-post: A system for analysis of sensitive private data disclosure in online social media},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generative model for time evolving networks. <em>KIS</em>,
<em>63</em>(9), 2347–2363. (<a
href="https://doi.org/10.1007/s10115-021-01596-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time evolving networks have some properties in common with complex networks, while some characteristics are specific to their time evolving nature. A number of interesting properties have been observed in time-varying complex networks such as densification power-law, shrinking diameter, scale-free degree distribution, big clustering coefficient and the emergence of community structure. Existing generative models either fail to simulate all the properties or undermine the social interactions between the existing nodes over time. In this paper, we propose a generative model called socializing graph model (SGM) for those networks that evolve over time. It is an iterative procedure consisting of two steps. In the first step, we add one new node to the network at every timestamp and connect it to an existing node using a preferential attachment rule. In the second step, we add a number of edges between the existing nodes in order to reflect the emergence of social interactions between nodes over time and mimic the evolution of real networks. We present empirical results to show that SGM generates realistic prototypes of evolving networks.},
  archive      = {J_KIS},
  author       = {Yousuf, Muhammad Irfan and Kim, Suhyun},
  doi          = {10.1007/s10115-021-01596-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2347-2363},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A generative model for time evolving networks},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On entropy-based term weighting schemes for text
categorization. <em>KIS</em>, <em>63</em>(9), 2313–2346. (<a
href="https://doi.org/10.1007/s10115-021-01581-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In text categorization, Vector Space Model (VSM) has been widely used for representing documents, in which a document is represented by a vector of terms. Since different terms contribute to a document’s semantics in various degrees, a number of term weighting schemes have been proposed for VSM to improve text categorization performance. Much evidence shows that the performance of a term weighting scheme often varies across different text categorization tasks, while the mechanism underlying variability in a scheme’s performance remains unclear. Moreover, existing schemes often weight a term with respect to a category locally, without considering the global distribution of a term’s occurrences across all categories in a corpus. In this paper, we first systematically examine pros and cons of existing term weighting schemes in text categorization and explore the reasons why some schemes with sound theoretical bases, such as chi-square test and information gain, perform poorly in empirical evaluations. By measuring the concentration that a term distributes across all categories in a corpus, we then propose a series of entropy-based term weighting schemes to measure the distinguishing power of a term in text categorization. Through extensive experiments on five different datasets, the proposed term weighting schemes consistently outperform the state-of-the-art schemes. Moreover, our findings shed new light on how to choose and develop an effective term weighting scheme for a specific text categorization task.},
  archive      = {J_KIS},
  author       = {Wang, Tao and Cai, Yi and Leung, Ho-fung and Lau, Raymond Y. K. and Xie, Haoran and Li, Qing},
  doi          = {10.1007/s10115-021-01581-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2313-2346},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On entropy-based term weighting schemes for text categorization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On augmenting database schemas by latent visual attributes.
<em>KIS</em>, <em>63</em>(9), 2277–2312. (<a
href="https://doi.org/10.1007/s10115-021-01595-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making in our everyday lives is surrounded by visually important information. Fashion, housing, dating, food or travel are just a few examples. At the same time, most commonly used tools for information retrieval operate on relational and text-based search models which are well understood by end users, but unable to directly cover visual information contained in images or videos. Researcher communities have been trying to reveal the semantics of multimedia in the last decades with ever-improving results, dominated by the success of deep learning. However, this does not close the gap to relational retrieval model on its own and often rather solves a very specialized task like assigning one of pre-defined classes to each object within a closed application ecosystem. Retrieval models based on these novel techniques are difficult to integrate in existing application-agnostic environments built around relational databases, and therefore, they are not so widely used in the industry. In this paper, we address the problem of closing the gap between visual information retrieval and relational database model. We propose and formalize a model for discovering candidates for new relational attributes by analysis of available visual content. We design and implement a system architecture supporting the attribute extraction, suggestion and acceptance processes. We apply the solution in the context of e-commerce and show how it can be seamlessly integrated with SQL environments widely used in the industry. At last, we evaluate the system in a user study and discuss the obtained results.},
  archive      = {J_KIS},
  author       = {Grošup, Tomáš and Peška, Ladislav and Skopal, Tomáš},
  doi          = {10.1007/s10115-021-01595-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2277-2312},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On augmenting database schemas by latent visual attributes},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data stream classification with novel class detection: A
review, comparison and challenges. <em>KIS</em>, <em>63</em>(9),
2231–2276. (<a
href="https://doi.org/10.1007/s10115-021-01582-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing effective and efficient data stream classifiers is challenging for the machine learning community because of the dynamic nature of data streams. As a result, many data stream learning algorithms have been proposed during the past decades and achieve great success in various fields. This paper aims to explore a specific type of challenge in learning evolving data streams, called concept evolution (emergence of novel classes). Concept evolution indicates that the underlying patterns evolve over time, and new patterns (classes) may emerge at any time in streaming data. Therefore, data stream classifiers with emerging class detection have received increasing attention in recent years due to the practical values in many real-world applications. In this article, we provide a comprehensive overview of the existing works in this line of research. We discuss and analyze various aspects of the proposed algorithms for data stream classification with concept evolution detection and adaptation. Additionally, we discuss the potential application areas in which these techniques can be used. We also provide a detailed overview of evaluation measures and datasets used in these studies. Finally, we describe the current research challenges and future directions for data stream classification with novel class detection.},
  archive      = {J_KIS},
  author       = {Din, Salah Ud and Shao, Junming and Kumar, Jay and Mawuli, Cobbinah Bernard and Mahmud, S. M. Hasan and Zhang, Wei and Yang, Qinli},
  doi          = {10.1007/s10115-021-01582-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {2231-2276},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Data stream classification with novel class detection: A review, comparison and challenges},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving security in NoSQL document databases through
model-driven modernization. <em>KIS</em>, <em>63</em>(8), 2209–2230. (<a
href="https://doi.org/10.1007/s10115-021-01589-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NoSQL technologies have become a common component in many information systems and software applications. These technologies are focused on performance, enabling scalable processing of large volumes of structured and unstructured data. Unfortunately, most developments over NoSQL technologies consider security as an afterthought, putting at risk personal data of individuals and potentially causing severe economic loses as well as reputation crisis. In order to avoid these situations, companies require an approach that introduces security mechanisms into their systems without scrapping already in-place solutions to restart all over again the design process. Therefore, in this paper we propose the first modernization approach for introducing security in NoSQL databases, focusing on access control and thereby improving the security of their associated information systems and applications. Our approach analyzes the existing NoSQL solution of the organization, using a domain ontology to detect sensitive information and creating a conceptual model of the database. Together with this model, a series of security issues related to access control are listed, allowing database designers to identify the security mechanisms that must be incorporated into their existing solution. For each security issue, our approach automatically generates a proposed solution, consisting of a combination of privilege modifications, new roles and views to improve access control. In order to test our approach, we apply our process to a medical database implemented using the popular document-oriented NoSQL database, MongoDB. The great advantages of our approach are that: (1) it takes into account the context of the system thanks to the introduction of domain ontologies, (2) it helps to avoid missing critical access control issues since the analysis is performed automatically, (3) it reduces the effort and costs of the modernization process thanks to the automated steps in the process, (4) it can be used with different NoSQL document-based technologies in a successful way by adjusting the metamodel, and (5) it is lined up with known standards, hence allowing the application of guidelines and best practices.},
  archive      = {J_KIS},
  author       = {Maté, Alejandro and Peral, Jesús and Trujillo, Juan and Blanco, Carlos and García-Saiz, Diego and Fernández-Medina, Eduardo},
  doi          = {10.1007/s10115-021-01589-x},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2209-2230},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving security in NoSQL document databases through model-driven modernization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalising combinatorial discriminant analysis through
conditioning truncated rayleigh flow. <em>KIS</em>, <em>63</em>(8),
2189–2208. (<a
href="https://doi.org/10.1007/s10115-021-01587-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher’s Linear Discriminant Analysis (LDA) has been widely used for linear classification, feature selection, and metrics learning in multivariate data analytics. To ensure high classification accuracy while optimally discovering predictive features from the data, this paper studied $$\mathbf {CDA}$$ , namely Combinatorial Discriminant Analysis that intends to combinatorially select a subset of features and assign weights to them optimally. $$\mathbf {CDA}$$ extents the Truncated Rayleigh Flow algorithm (Tan et al. in J R Stat Soc: Ser B (Stat Methodol) 80(5):1057–1086, 2018) and improves LDA estimation under k-sparsity constraint. The experimental results based on the synthesized and real-world datasets demonstrate that our algorithm outperforms other LDA baselines and downstream classifiers. The empirical analysis shows that our algorithm can recover the combinatorial structure of optimal LDA with empirical consistency.},
  archive      = {J_KIS},
  author       = {Yang, Sijia and Xiong, Haoyi and Hu, Di and Xu, Kaibo and Wang, Licheng and Zhu, Peizhen and Sun, Zeyi},
  doi          = {10.1007/s10115-021-01587-z},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2189-2208},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Generalising combinatorial discriminant analysis through conditioning truncated rayleigh flow},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross- and multiple-domains visual transfer learning via
iterative fischer linear discriminant analysis. <em>KIS</em>,
<em>63</em>(8), 2157–2188. (<a
href="https://doi.org/10.1007/s10115-021-01586-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard machine learning tasks often assume that the training (source domain) and test (target domain) data follow the same distribution and feature space. However, many real-world applications suffer from the limited number of training labeled data and benefit from the related available labeled datasets to train the model. In this way, since there is the distribution difference across the source and target domains (i.e., domain shift problem), the learned classifier on the training set might perform poorly on the test set. To address the shift problem, domain adaptation provides variety of solutions to learn robust classifiers to deal with distribution mismatch across the source and target domains. In this paper, we put forward a novel domain adaptation approach, referred to as cross- and multiple-domains visual transfer learning via iterative Fischer linear discriminant analysis (CIDA) to tackle shift problem across domains. CIDA transfers the source and target domains into a shared low-dimensional Fischer linear discriminant analysis (FLDA)-based subspace in an unsupervised manner. CIDA benefits joint FLDA and domain adaptation criterions to reduce the distribution mismatch across the training and test sets. Moreover, CIDA employs an adaptive classifier to build a robust model against data drift across different domains. Also, CIDA generates the intermediate pseudotarget labels to utilize the target data in training process. In this way, CIDA refines the pseudolabels using an iterative manner to converge the model. Our extensive experiments illustrate that CIDA significantly outperforms the baseline machine learning and other state-of-the-art transfer learning methods on nine visual benchmark datasets under different difficulties.},
  archive      = {J_KIS},
  author       = {Mardani, Mehri and Tahmoresnezhad, Jafar},
  doi          = {10.1007/s10115-021-01586-0},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2157-2188},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cross- and multiple-domains visual transfer learning via iterative fischer linear discriminant analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved incremental local outlier detection for data
streams based on the landmark window model. <em>KIS</em>,
<em>63</em>(8), 2129–2155. (<a
href="https://doi.org/10.1007/s10115-021-01585-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing algorithms of anomaly detection are suitable for static data where all data are available during detection but are incapable of handling dynamic data streams. In this study, we proposed an improved iLOF (incremental local outlier factor) algorithm based on the landmark window model, which provides an efficient method for anomaly detection in data streams and outperforms conventional methods. What is more, data windows as updating units are introduced to reduce the false alarm rate, and multiple tests are taken here to identify candidate anomalies and real anomalies. The improved iLOF shows its obvious advantage with its false positive rate. Furthermore, the proposed algorithm instantly deletes data points of identified real anomalies. We analyzed the performance of the improved algorithm and the sensitivity of certain parameters via empirical experiments using synthetic and real data sets. The experimental results demonstrate that the proposed improved algorithm achieved better performance on the higher detection rate and the lower false alarm rate compared with the original iLOF algorithm and its improvements.},
  archive      = {J_KIS},
  author       = {Li, Aihua and Xu, Weijia and Liu, Zhidong and Shi, Yong},
  doi          = {10.1007/s10115-021-01585-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2129-2155},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved incremental local outlier detection for data streams based on the landmark window model},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generic MOT boosting framework by combining cues from SOT,
tracklet and re-identification. <em>KIS</em>, <em>63</em>(8), 2109–2127.
(<a href="https://doi.org/10.1007/s10115-021-01576-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a generic boosting framework for multiple object tracking (MOT). Unlike other works tracking objects from zero, our framework uses their results (tracklets) and makes further optimizations. The motivation of us derives from the observation that most modern MOT trackers have been acceptable performance and can yield relatively reliable tracklets; accordingly, we straight focus on the tracklet-level re-identification, which is the most challenging issue in this case. To achieve that goal, we simultaneously utilize the techniques of single object tracking, tracking fragment (tracklets) and re-identification mechanism through casting them into a multi-label energy optimization and then innovatively solving it using the $$\alpha -$$ expansion with label costs algorithm. All these techniques inspire recent MOT a lot to mitigate the occlusion problem, but to our knowledge, by far few works explore to reasonably combine them all like us. Furthermore, we introduce a spatial attention to improve the appearance model and a hierarchical clustering as post-process to progressively improve the tracking consistency. Finally, testing results on the most used benchmarks demonstrate the significant effectiveness and generality of our framework, and the importance of each contribution is also verified through ablative studies.},
  archive      = {J_KIS},
  author       = {Liang, Tianyi and Lan, Long and Zhang, Xiang and Luo, Zhigang},
  doi          = {10.1007/s10115-021-01576-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2109-2127},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A generic MOT boosting framework by combining cues from SOT, tracklet and re-identification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recipe analysis for knowledge discovery of gastronomic
dishes. <em>KIS</em>, <em>63</em>(8), 2075–2108. (<a
href="https://doi.org/10.1007/s10115-021-01584-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet plays an important role in society as a whole. One of the services that arose from the Internet was collaborative systems, in which several users create the content of the systems by means of personal experiences. One of the many collaborative systems existing today is those of sharing gastronomic recipes. The area of information retrieval on the Web has increased interest in regard to recovering the information contained in this environment and studying it in order to identify relationships such as the ingredients used in the preparation of a dish, which can be identified through the use of textual data mining techniques. In this scope, the present work proposes a methodology of knowledge discovery in gastronomic recipes collected from various sources of data. For this, information such as the ingredients, quantities, units of measure, preparation directions and other characteristics associated with the recipes is used. With the results found in the experiments presented in this article, this work represents a first step for the development of a service that, besides aggregating recipes from various sources, explores the collective knowledge that can be discovered when analyzing hundreds of thousands of recipes available on the Internet.},
  archive      = {J_KIS},
  author       = {Rodrigues, Edwaldo Soares and Paiva, Débora Maria Barroso and Júnior, Álvaro Rodrigues Pereira},
  doi          = {10.1007/s10115-021-01584-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2075-2108},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recipe analysis for knowledge discovery of gastronomic dishes},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaboration prediction in heterogeneous academic network
with dynamic structure and topic. <em>KIS</em>, <em>63</em>(8),
2053–2074. (<a
href="https://doi.org/10.1007/s10115-021-01580-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Academic collaborations improve research efficiency and spur scientific innovation. However, scholarly big data has hindered scholars from finding suitable collaborators. Although some studies have involved the prediction problem of academic collaborations, they neglect the rich dynamic information of the heterogeneous academic network. In this paper, we propose a prediction model for academic collaborations, which considers both the dynamic structure and content information. We first formally define the dynamic academic network and the collaboration prediction problem. Then, a scholar representation model is designed by capturing both the dynamic structure and content features, together with the macro-impact of overall academic trends. Finally, we build the prediction model based on the representation result of scholars. Extensive experiments for predicting new collaborations are conducted on the DBLP dataset. The experimental results on the accuracy, F1, and AUC metrics demonstrate that our method outperforms the baseline methods and can predict academic collaborations efficiently.},
  archive      = {J_KIS},
  author       = {Zhao, Weidong and Pu, Shi},
  doi          = {10.1007/s10115-021-01580-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2053-2074},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Collaboration prediction in heterogeneous academic network with dynamic structure and topic},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Short isometric shapelet transform for binary time series
classification. <em>KIS</em>, <em>63</em>(8), 2023–2051. (<a
href="https://doi.org/10.1007/s10115-021-01583-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the research area of time series classification, the ensemble shapelet transform algorithm is one of the state-of-the-art algorithms for classification. However, its high time complexity is an issue to hinder its application since its base classifier shapelet transform includes a high time complexity of a distance calculation and shapelet selection. Therefore, in this paper we introduce a novel algorithm, i.e., short isometric shapelet transform (SIST), which contains two strategies to reduce the time complexity. The first strategy of SIST fixes the length of shapelet based on a simplified distance calculation, which largely reduces the number of shapelet candidates as well as speeds up the distance calculation in the ensemble shapelet transform algorithm. The second strategy is to train a single linear classifier in the feature space instead of an ensemble classifier. The theoretical evidence of these two strategies is presented to guarantee a near-lossless accuracy under some preconditions while reducing the time complexity. Furthermore, empirical experiments demonstrate the superior performance of the proposed algorithm.},
  archive      = {J_KIS},
  author       = {Shu, Weibo and Yao, Yaqiang and Lyu, Shengfei and Li, Jinlong and Chen, Huanhuan},
  doi          = {10.1007/s10115-021-01583-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2023-2051},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Short isometric shapelet transform for binary time series classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Location selection by multi-criteria decision-making methods
based on objective and subjective weightings. <em>KIS</em>,
<em>63</em>(8), 1991–2021. (<a
href="https://doi.org/10.1007/s10115-021-01588-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location selection is a strategic decision that significantly influences revenue, level of competition, and success of companies and countries. This study aims to propose a hybrid approach for the location selection, to evaluate the potential location for the automotive manufacturing plant of Turkey, and to reveal a comprehensive analysis of weighting and multiple criteria decision-making (MCDM) methods. The proposed approach integrates different objective and subjective weighting, MCDM, and Copeland methods. Turkey has recently introduced its first automobile prototypes and has announced that the manufacturing plant will be located in Bursa. This decision is thoroughly examined via four objective weighting methods—entropy, criteria importance through inter-criteria correlation, standard deviation, and mean weight and a subjective method—analytic hierarchy process. Besides, the alternatives are evaluated based on six MCDM methods—technique for order preference by similarity to ideal solution, preference ranking organization method for enrichment evaluations, vise kriterijumska optimizacija i kompromisno resenje, organization, rangement et synthese de donnes relationnelles, elimination and choice translating reality, and the weighted sum method. The outcomes of the weighting methods and MCDM methods, the impact of the attribute weights provided by each method on rankings, the outcome of each method pair, and selection of the best location (Bursa) are thoroughly evaluated considering a real-world case with a potential outcome that makes evaluations more realistic and tangible unlike most of the other studies in the literature. In this regard, Spearman&#39;s rank correlation coefficients are considered. Also, sensitivity analysis is conducted to reveal the robustness of the methods and the impact of each weight on outcomes. Some considerable results, including the most robust method and optimal method pairs for the case, are presented.},
  archive      = {J_KIS},
  author       = {Şahin, Mehmet},
  doi          = {10.1007/s10115-021-01588-y},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {1991-2021},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Location selection by multi-criteria decision-making methods based on objective and subjective weightings},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting data complexity metrics based on morphology for
overlap and imbalance: Snapshot, new overlap number of balls metrics and
singular problems prospect. <em>KIS</em>, <em>63</em>(7), 1961–1989. (<a
href="https://doi.org/10.1007/s10115-021-01577-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Science and Machine Learning have become fundamental assets for companies and research institutions alike. As one of its fields, supervised classification allows for class prediction of new samples, learning from given training data. However, some properties can cause datasets to be problematic to classify. In order to evaluate a dataset a priori, data complexity metrics have been used extensively. They provide information regarding different intrinsic characteristics of the data, which serve to evaluate classifier compatibility and a course of action that improves performance. However, most complexity metrics focus on just one characteristic of the data, which can be insufficient to properly evaluate the dataset towards the classifiers’ performance. In fact, class overlap, a very detrimental feature for the classification process (especially when imbalance among class labels is also present) is hard to assess. This research work focuses on revisiting complexity metrics based on data morphology. In accordance to their nature, the premise is that they provide both good estimates for class overlap, and great correlations with the classification performance. For that purpose, a novel family of metrics has been developed. Being based on ball coverage by classes, they are named after Overlap Number of Balls. Finally, some prospects for the adaptation of the former family of metrics to singular (more complex) problems are discussed.},
  archive      = {J_KIS},
  author       = {Pascual-Triana, José Daniel and Charte, David and Andrés Arroyo, Marta and Fernández, Alberto and Herrera, Francisco},
  doi          = {10.1007/s10115-021-01577-1},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1961-1989},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Revisiting data complexity metrics based on morphology for overlap and imbalance: Snapshot, new overlap number of balls metrics and singular problems prospect},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Application of genetic algorithm-based intuitionistic fuzzy
weighted c-ordered-means algorithm to cluster analysis. <em>KIS</em>,
<em>63</em>(7), 1935–1959. (<a
href="https://doi.org/10.1007/s10115-021-01574-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of information technology, many fields have begun using data clustering to reveal data structures and obtain useful information. Most of the existing clustering algorithms are susceptible to outliers and noises as well as the initial solution. The fuzzy c-ordered-means (FCOM) method can handle outlier and noise problems by using Huber’s M-estimators and Yager’s OWA operator to enhance its robustness. However, the result of the FCOM algorithm is still unstable because its initial centroids are randomly generated. Besides, the attributes’ weight also affect the clustering performance. Thus, this study first proposed an intuitionistic fuzzy weighted c-ordered-means (IFWCOM) algorithm that combines intuitionistic fuzzy sets (IFSs), the feature-weighted and FCOM together to improve the clustering result. Moreover, this study proposed a real-coded genetic algorithm-based IFWCOM (GA-IFWCOM) that employs the genetic algorithm to exploit the global optimal solution of the IFWCOM algorithm. Twelve benchmark datasets were used for verification in the experiment. According to the experimental results, the GA-IFWCOM algorithm achieved better clustering accuracy than the other clustering algorithms for most of the datasets.},
  archive      = {J_KIS},
  author       = {Kuo, R. J. and Chang, C. K. and Nguyen, Thi Phuong Quyen and Liao, T. W.},
  doi          = {10.1007/s10115-021-01574-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1935-1959},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Application of genetic algorithm-based intuitionistic fuzzy weighted c-ordered-means algorithm to cluster analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated approach using IF-TOPSIS, fuzzy DEMATEL, and
enhanced CSA optimized ANFIS for software risk prediction. <em>KIS</em>,
<em>63</em>(7), 1909–1934. (<a
href="https://doi.org/10.1007/s10115-021-01573-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successful project is determined based on its effective performance and prioritization of all unavoidable software project risks. In this paper, the risk evaluation in software projects is done through developing a new hybridized fuzzy-based risk evaluation framework. During decision making process, this proposed scheme has determined and ranked all the significant project risks. Software project risks are better assessed with the incorporation of Intuitionistic fuzzy-based TOPSIS, adaptive neuro-fuzzy inference system-based multi-criteria decision making (ANFIS MCDM), and fuzzy decision making trial and evaluation laboratory methods. In order to attain accurate software risk estimation, the ANFIS parameters are adjusted with the help of enhanced crow search algorithm (ECSA). To the ANFIS approach, the ECSA is combined to make free the solutions sticking inside the local optimum and adopting only small changes for the adjustment of ANFIS parameters. NASA 93 dataset with 93 software project values was used to conduct the experimental validation. Experimental outcomes have proved evidently that the software project risks evaluation were done accurately and effectively using proposed integrated fuzzy-based framework.},
  archive      = {J_KIS},
  author       = {Suresh, K. and Dillibabu, R.},
  doi          = {10.1007/s10115-021-01573-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1909-1934},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An integrated approach using IF-TOPSIS, fuzzy DEMATEL, and enhanced CSA optimized ANFIS for software risk prediction},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GrAFCI+ a fast generator-based algorithm for mining frequent
closed itemsets. <em>KIS</em>, <em>63</em>(7), 1873–1908. (<a
href="https://doi.org/10.1007/s10115-021-01575-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining itemsets for association rule generation is a fundamental data mining task originally stemming from the traditional market basket analysis problem. However, enumerating all frequent itemsets, especially in a dense dataset, or with low support thresholds, remains costly. In this paper, a novel theorem builds the relationship between frequent closed itemsets (FCIs) and frequent generator itemsets (FGIs) and proves that the process of mining FCIs is equivalent to mining FGIs, unified with their full-support and extension items. On the basis of this theorem, a generator-based algorithm for mining FCIs, called GrAFCI+, is proposed and explained in details including its correctness. The comparative effectiveness of the algorithm in terms of scalability is first investigated, along with the compression rate—a measure of the interestingness of a given FIs representation. Extensive experiments are further undertaken on eight datasets and four state-of-the-art algorithms, namely DCI_CLOSED*, DCI_PLUS, FPClose, and NAFCP. The results show that the proposed algorithm is more efficient regarding the execution time in most cases as compared to these algorithms. Because GrAFCI+ main goal is to address the runtime issue, it paid a memory cost, especially when the support is too small. However, this cost is not high since GrAFCI+ is seconded by only one competitor out of four in memory utilization and for large support values. As an overall assessment, GrAFCI+ gives better results than most of its competitors.},
  archive      = {J_KIS},
  author       = {Ledmi, Makhlouf and Zidat, Samir and Hamdi-Cherif, Aboubekeur},
  doi          = {10.1007/s10115-021-01575-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1873-1908},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GrAFCI+ a fast generator-based algorithm for mining frequent closed itemsets},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anonymous location sharing in urban area mobility.
<em>KIS</em>, <em>63</em>(7), 1849–1871. (<a
href="https://doi.org/10.1007/s10115-021-01566-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies the location-privacy preserving location update in the context of data-centric people mobility applications. The mobility model involves an urban area annotated city network (ACN) over which the users move and record/report their locations at non-regular intervals. The ACN is modeled as a directed weighted graph. Since the data receiver (e.g., an LBS provider) is curious in our privacy model, the users share their locations after anonymization which requires k-member partitioning of the ACN. Our framework, in the offline stage, requires a prototype vertex selection for each of the partitions. To this end, we develop a heuristic to obtain more representative prototype vertices. The temporal dimension of the location anonymity is achieved by two notions of the anonymity models, called weak location k-anonymity (to provide snapshot location anonymity) and strong location k-anonymity (to provide historical location anonymity). The attack scenario models the belief of the attacker (the LBS provider) on the whereabouts of the users at each location update. In the online stage, our algorithms make anonymity violation tests at every location update request and selectively block the anonymity violating ones. The online stage algorithms providing weak/strong location k-anonymity are shown to run in constant time per location update. An extensive experimental evaluation, mainly addressing the issue of privacy/utility trade-off, on three real ACNs with a simulated mobility is presented.},
  archive      = {J_KIS},
  author       = {Abul, Osman and Bitirgen, Ozan Berk},
  doi          = {10.1007/s10115-021-01566-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1849-1871},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anonymous location sharing in urban area mobility},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature extraction for chart pattern classification in
financial time series. <em>KIS</em>, <em>63</em>(7), 1807–1848. (<a
href="https://doi.org/10.1007/s10115-021-01569-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting shape-related features from a given query subsequence is a crucial preprocessing step for chart pattern matching in rule-based, template-based and hybrid pattern classification methods. The extracted features can significantly influence the accuracy of pattern recognition tasks during the data mining process. Although shape-related features are widely used for chart pattern matching in financial time series, the intrinsic properties of these features and their relationships to the patterns are rarely investigated in research community. This paper aims to formally identify shape-related features used in chart patterns and investigates their impact on chart pattern classifications in financial time series. In this paper, we describe a comprehensive analysis of 14 shape-related features which can be used to classify 41 known chart patterns in technical analysis domain. In order to evaluate their effectiveness, shape-related features are then translated into rules for chart pattern classification. We perform extensive experiments on real datasets containing historical price data of 24 stocks/indices to analyze the effectiveness of the rules. Experimental results reveal that the features put forward in this paper can be effectively used for recognizing chart patterns in financial time series. Our analysis also reveals that high-level features can be hierarchically composed from low-level features. Hierarchical composition allows construction of complex chart patterns from features identified in this paper. We hope that the features identified in this paper can be used as a reference model for the future research in chart pattern analysis.},
  archive      = {J_KIS},
  author       = {Zheng, Yuechu and Si, Yain-Whar and Wong, Raymond},
  doi          = {10.1007/s10115-021-01569-1},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1807-1848},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature extraction for chart pattern classification in financial time series},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A study on using data clustering for feature extraction to
improve the quality of classification. <em>KIS</em>, <em>63</em>(7),
1771–1805. (<a
href="https://doi.org/10.1007/s10115-021-01572-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a certain belief among data science researchers and enthusiasts alike that clustering can be used to improve classification quality. Insofar as this belief is fairly uncontroversial, it is also very general and therefore produces a lot of confusion around the subject. There are many ways of using clustering in classification and it obviously cannot always improve the quality of predictions, so a question arises, in which scenarios exactly does it help? Since we were unable to find a rigorous study addressing this question, in this paper, we try to shed some light on the concept of using clustering for classification. To do so, we first put forward a framework for incorporating clustering as a method of feature extraction for classification. The framework is generic w.r.t. similarity measures, clustering algorithms, classifiers, and datasets and serves as a platform to answer ten essential questions regarding the studied subject. Each answer is formulated based on a separate experiment on 16 publicly available datasets, followed by an appropriate statistical analysis. After performing the experiments and analyzing the results separately, we discuss them from a global perspective and form general conclusions regarding using clustering as feature extraction for classification.},
  archive      = {J_KIS},
  author       = {Piernik, Maciej and Morzy, Tadeusz},
  doi          = {10.1007/s10115-021-01572-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1771-1805},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A study on using data clustering for feature extraction to improve the quality of classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A meta-algorithm for finding large k-plexes. <em>KIS</em>,
<em>63</em>(7), 1745–1769. (<a
href="https://doi.org/10.1007/s10115-021-01570-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the automatic detection of communities in large networks, a challenging problem in many disciplines (such as sociology, biology, and computer science). Humans tend to associate to form families, villages, and nations. Similarly, the elements of real-world networks naturally tend to form highly connected groups. A popular model to represent such structures is the clique, that is, a set of fully interconnected nodes. However, it has been observed that cliques are too strict to represent communities in practice. The k-plex relaxes the notion of clique, by allowing each node to miss up to k connections. Although k-plexes are more flexible than cliques, finding them is more challenging as their number is greater. In addition, most of them are small and not significant. In this paper we tackle the problem of finding only large k-plexes (i.e., comparable in size to the largest clique) and design a meta-algorithm that can be used on top of known enumeration algorithms to return only significant k-plexes in a fraction of the time. Our approach relies on: (1) methods for strongly reducing the search space and (2) decomposition techniques based on the efficient computation of maximal cliques. We demonstrate experimentally that known enumeration algorithms equipped with our approach can run orders of magnitude faster than full enumeration.},
  archive      = {J_KIS},
  author       = {Conte, Alessio and Firmani, Donatella and Patrignani, Maurizio and Torlone, Riccardo},
  doi          = {10.1007/s10115-021-01570-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1745-1769},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A meta-algorithm for finding large k-plexes},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A streaming edge sampling method for network visualization.
<em>KIS</em>, <em>63</em>(7), 1717–1743. (<a
href="https://doi.org/10.1007/s10115-021-01571-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization strategies facilitate streaming network analysis by allowing its exploration through graphical and interactive layouts. Depending on the strategy and the network density, such layouts may suffer from a high level of visual clutter that hides meaningful temporal patterns, highly active groups of nodes, bursts of activity, and other important network properties. Edge sampling improves layout readability, highlighting important properties and leading to easier and faster pattern identification and decision making. This paper presents Streaming Edge Sampling for Network Visualization–SEVis, a streaming edge sampling method that discards edges of low-active nodes while preserving a distribution of edge counts that is similar to the original network. It can be applied to a variety of layouts to enhance streaming network analyses. We evaluated SEVis performance using synthetic and real-world networks through quantitative and visual analyses. The results indicate a higher performance of SEVis for clutter reduction and pattern identification when compared with other sampling methods.},
  archive      = {J_KIS},
  author       = {Ponciano, Jean R. and Linhares, Claudio D. G. and Rocha, Luis E. C. and Faria, Elaine R. and Travençolo, Bruno A. N.},
  doi          = {10.1007/s10115-021-01571-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1717-1743},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A streaming edge sampling method for network visualization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive multi-task learning for group itinerary
recommendation. <em>KIS</em>, <em>63</em>(7), 1687–1716. (<a
href="https://doi.org/10.1007/s10115-021-01567-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tourism is one of the largest service industries and a popular leisure activity participated by people with friends or family. A significant problem faced by the tourists is how to plan sequences of points of interest (POIs) that maintain a balance between the group preferences and the given temporal and spatial constraints. Most traditional group itinerary recommendation methods adopt predefined preference aggregate strategies without considering the group members’ distinctive characteristics and inner relations. Besides, POI textual information is beneficial to capture overall group preferences but is rarely considered. With these concerns in mind, this paper proposes an AMT-IRE (short for Attentive Multi-Task learning-based group Itinerary REcommendation) framework, which can dynamically learn the inner relations between group members and obtain consensus group preferences via the attention mechanism. Meanwhile, AMT-IRE integrates POI categories and POI textual information via another attention network. Finally, the group preferences are used in a variant of the orienteering problem to recommend group itineraries. Extensive experiments on six datasets validate the effectiveness of AMT-IRE.},
  archive      = {J_KIS},
  author       = {Chen, Lei and Cao, Jie and Chen, Huanhuan and Liang, Weichao and Tao, Haicheng and Zhu, Guixiang},
  doi          = {10.1007/s10115-021-01567-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1687-1716},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Attentive multi-task learning for group itinerary recommendation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel cluster-based approach for keyphrase extraction from
MOOC video lectures. <em>KIS</em>, <em>63</em>(7), 1663–1686. (<a
href="https://doi.org/10.1007/s10115-021-01568-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive open online courses (MOOCs) have emerged as a great resource for learners. Numerous challenges remain to be addressed in order to make MOOCs more useful and convenient for learners. One such challenge is how to automatically extract a set of keyphrases from MOOC video lectures that can help students quickly identify the right knowledge they want to learn and thus expedite their learning process. In this paper, we propose SemKeyphrase, an unsupervised cluster-based approach for keyphrase extraction from MOOC video lectures. SemKeyphrase incorporates a new semantic relatedness metric and a ranking algorithm, called PhraseRank, that involves two phases on ranking candidates. We conducted experiments on a real-world dataset of MOOC video lectures, and the results show that our proposed approach outperforms the state-of-the-art keyphrase extraction methods.},
  archive      = {J_KIS},
  author       = {Albahr, Abdulaziz and Che, Dunren and Albahar, Marwan},
  doi          = {10.1007/s10115-021-01568-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1663-1686},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel cluster-based approach for keyphrase extraction from MOOC video lectures},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A nested two-stage clustering method for structured temporal
sequence data. <em>KIS</em>, <em>63</em>(7), 1627–1662. (<a
href="https://doi.org/10.1007/s10115-021-01578-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining patterns of temporal sequence data is an important problem across many disciplines. Under appropriate preprocessing procedures, a structured temporal sequence can be organized into a probability measure or a time series representation, which grants a potential to reveal distinctive temporal pattern characteristics. In this paper, we propose a nested two-stage clustering method that integrates optimal transport and the dynamic time warping distances to learn the distributional and dynamic shape-based dissimilarity at the respective stage. The proposed clustering algorithm preserves both the distribution and shape patterns present in the data, which are critical for the datasets composed of structured temporal sequences. The effectiveness of the method is tested against existing agglomerative and K-shape-based clustering algorithms on Monte Carlo simulated synthetic datasets, and the performance is compared through various cluster validation metrics. Furthermore, we apply the developed method to real-world datasets from three domains: temporal dietary records, online retail sales, and smart meter energy profiles. The expressiveness of the cluster and subcluster centroid patterns shows significant promise of our method for structured temporal sequence data mining.},
  archive      = {J_KIS},
  author       = {Wang, Liang and Narayanan, Vignesh and Yu, Yao-Chi and Park, Yikyung and Li, Jr-Shin},
  doi          = {10.1007/s10115-021-01578-0},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1627-1662},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A nested two-stage clustering method for structured temporal sequence data},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from evolving data streams through ensembles of
random patches. <em>KIS</em>, <em>63</em>(7), 1597–1625. (<a
href="https://doi.org/10.1007/s10115-021-01579-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble methods represent an effective way to solve supervised learning problems. Such methods are prevalent for learning from evolving data streams. One of the main reasons for such popularity is the possibility of incorporating concept drift detection and recovery strategies in conjunction with the ensemble algorithm. On top of that, successful ensemble strategies, such as bagging and random forest, can be easily adapted to a streaming setting. In this work, we analyse a novel ensemble method designed specially to cope with evolving data streams, namely the streaming random patches (SRP) algorithm. SRP combines random subspaces and online bagging to achieve competitive predictive performance in comparison with other methods. We significantly extend previous theoretical insights and empirical results illustrating different aspects of SRP. In particular, we explain how the widely adopted incremental Hoeffding trees are not, in fact, unstable learners, unlike their batch counterparts, and how this fact significantly influences ensemble methods design and performance. We compare SRP against state-of-the-art ensemble variants for streaming data in a multitude of datasets. The results show how SRP produces a high predictive performance for both real and synthetic datasets. We also show how ensembles of random subspaces can be an efficient and accurate option to SRP and leveraging bagging as we increase the number of base learners. Besides, we analyse the diversity over time and the average tree depth, which provides insights on the differences between local subspace randomization (as in random forest) and global subspace randomization (as in random subspaces). Finally, we analyse the behaviour of SRP when using Naive Bayes as its base learner instead of Hoeffding trees.},
  archive      = {J_KIS},
  author       = {Gomes, Heitor Murilo and Read, Jesse and Bifet, Albert and Durrant, Robert J.},
  doi          = {10.1007/s10115-021-01579-z},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {1597-1625},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning from evolving data streams through ensembles of random patches},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advancing synthesis of decision tree-based multiple
classifier systems: An approximate computing case study. <em>KIS</em>,
<em>63</em>(6), 1577–1596. (<a
href="https://doi.org/10.1007/s10115-021-01565-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {So far, multiple classifier systems have been increasingly designed to take advantage of hardware features, such as high parallelism and computational power. Indeed, compared to software implementations, hardware accelerators guarantee higher throughput and lower latency. Although the combination of multiple classifiers leads to high classification accuracy, the required area overhead makes the design of a hardware accelerator unfeasible, hindering the adoption of commercial configurable devices. For this reason, in this paper, we exploit approximate computing design paradigm to trade hardware area overhead off for classification accuracy. In particular, starting from trained DT models and employing precision-scaling technique, we explore approximate decision tree variants by means of multiple objective optimization problem, demonstrating a significant performance improvement targeting field-programmable gate array devices.},
  archive      = {J_KIS},
  author       = {Barbareschi, Mario and Barone, Salvatore and Mazzocca, Nicola},
  doi          = {10.1007/s10115-021-01565-5},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1577-1596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Advancing synthesis of decision tree-based multiple classifier systems: An approximate computing case study},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metro passengers counting and density estimation via
dilated-transposed fully convolutional neural network. <em>KIS</em>,
<em>63</em>(6), 1557–1575. (<a
href="https://doi.org/10.1007/s10115-021-01563-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metro passenger counting and density estimation are crucial for traffic scheduling and risk prevention. Although deep learning has achieved great success in passenger counting, most existing methods ignore fundamental appearance information, leading to density maps of low quality. To address this problem, we propose a novel counting method called “dilated-transposed fully convolution neural network” (DT-CNN), which combines a feature extraction module (FEM) and a feature recovery module (FRM) to generate high-quality density maps and accurately estimate passenger counts in highly congested metro scenes. Specifically, the FEM is composed of a CNN, and a set of dilated convolutional layers extract 2D features relevant to scenes containing crowded human objects. Then, the resulting density map produced by the FEM is processed by the FRM to learn potential features, which is used to restore feature map pixels. The DT-CNN is end-to-end trainable and independent of the backbone fully convolutional network architecture. In addition, we introduce a new metro passenger counting dataset (Zhengzhou_MT++) that contains 396 images with 3,978 annotations. Extensive experiments conducted on self-built datasets and three representative crowd-counting datasets show the proposed method achieves superior performance relative to other state-of-the-art methods in terms of counting accuracy and density map quality. The Zhengzhou MT++ dataset is available at https://github.com/YellowChampagne/Zhengzhou_MT .},
  archive      = {J_KIS},
  author       = {Zhu, Gaoyi and Zeng, Xin and Jin, Xiangjie and Zhang, Jun},
  doi          = {10.1007/s10115-021-01563-7},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1557-1575},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Metro passengers counting and density estimation via dilated-transposed fully convolutional neural network},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A word embedding-based approach to cross-lingual topic
modeling. <em>KIS</em>, <em>63</em>(6), 1529–1555. (<a
href="https://doi.org/10.1007/s10115-021-01555-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cross-lingual topic analysis aims at extracting latent topics from corpora of different languages. Early approaches rely on high-cost multilingual resources (e.g., a parallel corpus), which is hard to come by in many real cases. Some works only require a translation dictionary as a linkage between languages; however, when given an inappropriate dictionary (e.g., small coverage of dictionary), the cross-lingual topic model would shrink to a monolingual topic model and generate less diversified topics. Therefore, it is imperative to investigate a cross-lingual topic model requiring fewer bilingual resources. Recently, some space-mapping techniques have been proposed to help align multiple word embedding of different languages into a quality cross-lingual word embedding by referring to a small number of translation pairs. This work proposes a cross-lingual topic model, called Cb-CLTM, which incorporates with cross-lingual word embedding. To leverage the power of word semantics and the linkage between languages from the cross-lingual word embedding, the Cb-CLTM considers each word as a continuous embedding vector rather than a discrete word type. The experiments demonstrate that, when cross-lingual word space exhibits strong isomorphism, Cb-CLTM can generate more coherent topics with higher diversity and induce better representations of documents across languages for further tasks such as cross-lingual document clustering and classification. When the cross-lingual word space is less isomorphic, Cb-CLTM generates less coherent topics yet still prevails in topic diversity and document classification.},
  archive      = {J_KIS},
  author       = {Chang, Chia-Hsuan and Hwang, San-Yih},
  doi          = {10.1007/s10115-021-01555-7},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1529-1555},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A word embedding-based approach to cross-lingual topic modeling},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient unsupervised drift detector for fast and
high-dimensional data streams. <em>KIS</em>, <em>63</em>(6), 1497–1527.
(<a href="https://doi.org/10.1007/s10115-021-01564-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream mining considers the online arrival of examples at high speed and the possibility of changes in its descriptive features or class definitions compared with past knowledge (i.e., concept drifts). The fast detection of drifts is essential to keep the predictive model updated and stable in changing environments. For many applications, such as those related to smart sensors, the high number of features is an additional challenge in terms of memory and time for stream processing. This paper presents an unsupervised and model-independent concept drift detector suitable for high-speed and high-dimensional data streams. We propose a straightforward two-dimensional data representation that allows the faster processing of datasets with a large number of examples and dimensions. We developed an adaptive drift detector on this visual representation that is efficient for fast streams with thousands of features and is accurate as existing costly methods that perform various statistical tests considering each feature individually. Our method achieves better performance measured by execution time and accuracy in classification problems for different types of drifts. The experimental evaluation considering synthetic and real data demonstrates the method’s versatility in several domains, including entomology, medicine, and transportation systems.},
  archive      = {J_KIS},
  author       = {Souza, Vinicius M. A. and Parmezan, Antonio R. S. and Chowdhury, Farhan A. and Mueen, Abdullah},
  doi          = {10.1007/s10115-021-01564-6},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1497-1527},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient unsupervised drift detector for fast and high-dimensional data streams},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive approaches to flexible group skyline queries.
<em>KIS</em>, <em>63</em>(6), 1471–1496. (<a
href="https://doi.org/10.1007/s10115-021-01562-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The G-Skyline (GSky) query is formulated to report optimal groups that are not dominated by any other group of the same size. Particularly, a given group $$G_1$$ dominates another group $$G_2$$ if for any point $$p\in G_1$$ , p dominates or equals to points $$p{&#39;}\in G_2$$ ; at the same time, there is at least one point p dominating $$p{&#39;}$$ . Most existing group skyline queries need to calculate an aggregate point for each group. Compared to these queries, the GSky query is more practical because it avoids specifying an aggregate function which leads to miss important results containing non-skyline points. This means the GSky query can get much more comprehensive query results which not only contain the G-Skylines consisting of skyline points but also the G-Skylines including non-skyline points. Here, a non-skyline point is dominated by another point in a given data set. However, the GSky query usually returns too many results, making it a big burden for users to pick out their expected results. To address these issues, we investigate a flexible group skyline query, namely Flexible G-Skyline (FGSky) query, which is flexible and practical for directly computing the optimal groups on the basis of user preferences. In this paper, we formulate the FGSky query, identify its properties, and present effective pruning strategies. Besides, we propose progressive algorithms for the FGSky query where a grouping strategy and a layered strategy are utilized to get better query performance. Through extensive experiments on both synthetic and real data sets, we demonstrate the efficiency, effectiveness, and progressiveness of the proposed algorithms.},
  archive      = {J_KIS},
  author       = {Yang, Zhibang and Zhou, Xu and Li, Kenli and Gao, Yunjun and Li, Keqin},
  doi          = {10.1007/s10115-021-01562-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1471-1496},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Progressive approaches to flexible group skyline queries},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The impact of data difficulty factors on classification of
imbalanced and concept drifting data streams. <em>KIS</em>,
<em>63</em>(6), 1429–1469. (<a
href="https://doi.org/10.1007/s10115-021-01560-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance introduces additional challenges when learning classifiers from concept drifting data streams. Most existing work focuses on designing new algorithms for dealing with the global imbalance ratio and does not consider other data complexities. Independent research on static imbalanced data has highlighted the influential role of local data difficulty factors such as minority class decomposition and presence of unsafe types of examples. Despite often being present in real-world data, the interactions between concept drifts and local data difficulty factors have not been investigated in concept drifting data streams yet. We thoroughly study the impact of such interactions on drifting imbalanced streams. For this purpose, we put forward a new categorization of concept drifts for class imbalanced problems. Through comprehensive experiments with synthetic and real data streams, we study the influence of concept drifts, global class imbalance, local data difficulty factors, and their combinations, on predictions of representative online classifiers. Experimental results reveal the high influence of new considered factors and their local drifts, as well as differences in existing classifiers’ reactions to such factors. Combinations of multiple factors are the most challenging for classifiers. Although existing classifiers are partially capable of coping with global class imbalance, new approaches are needed to address challenges posed by imbalanced data streams.},
  archive      = {J_KIS},
  author       = {Brzezinski, Dariusz and Minku, Leandro L. and Pewinski, Tomasz and Stefanowski, Jerzy and Szumaczuk, Artur},
  doi          = {10.1007/s10115-021-01560-w},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1429-1469},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The impact of data difficulty factors on classification of imbalanced and concept drifting data streams},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering cluster evolution patterns with the cluster
association-aware matrix factorization. <em>KIS</em>, <em>63</em>(6),
1397–1428. (<a
href="https://doi.org/10.1007/s10115-021-01561-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking of document collections over time (or across domains) is helpful in several applications such as finding dynamics of terminologies, identifying emerging and evolving trends, and concept drift detection. We propose a novel ‘Cluster Association-aware’ Non-negative Matrix Factorization (NMF)-based method with graph-based visualization to identify the changing dynamics of text clusters over time/domains. NMF is utilized to find similar clusters in the set of clustering solutions. Based on the similarities, four major lifecycle states of clusters, namely birth, split, merge and death, are tracked to discover their emergence, growth, persistence and decay. The novel concepts of ‘cluster associations’ and term frequency-based ‘cluster density’ have been used to improve the quality of evolution patterns. The cluster evolution is visualized using a k-partite graph. Empirical analysis with the text data shows that the proposed method is able to produce accurate and efficient solution as compared to the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Mohotti, Wathsala Anupama and Nayak, Richi},
  doi          = {10.1007/s10115-021-01561-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1397-1428},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Discovering cluster evolution patterns with the cluster association-aware matrix factorization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient discovery of co-location patterns from massive
spatial datasets with or without rare features. <em>KIS</em>,
<em>63</em>(6), 1365–1395. (<a
href="https://doi.org/10.1007/s10115-021-01559-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A co-location pattern indicates a group of spatial features whose instances are frequently located together in proximate geographic area. Spatial co-location pattern mining (SCPM) is valuable for many practical applications. Numerous previous SCPM studies emphasize the equal participation per feature. As a result, the interesting co-locations with rare features cannot be captured. In this paper, we propose a novel interest measure, i.e., the weighted participation index (WPI), to identify co-locations with or without rare features. The WPI measure possesses a conditional anti-monotone property which can be utilized to prune the search space. In addition, a fast row instance identification mechanism based on the ordered NR-tree is proposed to enhance efficiency. Subsequently, the ordered NR-tree-based algorithm is developed. To further improve efficiency and process massive spatial data, we break the ordered NR-tree into multiple independent subtrees, and parallelize the ordered NR-tree-based algorithm on MapReduce framework. Extensive experiments are conducted on both real and synthetic datasets to verify the effectiveness, efficiency and scalability of our techniques.},
  archive      = {J_KIS},
  author       = {Yang, Peizhong and Wang, Lizhen and Wang, Xiaoxuan and Zhou, Lihua},
  doi          = {10.1007/s10115-021-01559-3},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1365-1395},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient discovery of co-location patterns from massive spatial datasets with or without rare features},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental communication patterns in online social groups.
<em>KIS</em>, <em>63</em>(6), 1339–1364. (<a
href="https://doi.org/10.1007/s10115-021-01552-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, temporal networks played a key role in modelling, understanding, and analysing the properties of dynamic systems where individuals and events vary in time. Of paramount importance is the representation and the analysis of Social Media, in particular Social Networks and Online Communities, through temporal networks, due to their intrinsic dynamism (social ties, online/offline status, users’ interactions, etc..). The identification of recurrent patterns in Online Communities, and in detail in Online Social Groups, is an important challenge which can reveal information concerning the structure of the social network, but also patterns of interactions, trending topics, and so on. Different works have already investigated the pattern detection in several scenarios by focusing mainly on identifying the occurrences of fixed and well known motifs (mostly, triads) or more flexible subgraphs. In this paper, we present the concept on the Incremental Communication Patterns, which is something in-between motifs, from which they inherit the meaningfulness of the identified structure, and subgraph, from which they inherit the possibility to be extended as needed. We formally define the Incremental Communication Patterns and exploit them to investigate the interaction patterns occurring in a real dataset consisting of 17 Online Social Groups taken from the list of Facebook groups. The results regarding our experimental analysis uncover interesting aspects of interactions patterns occurring in social groups and reveal that Incremental Communication Patterns are able to capture roles of the users within the groups.},
  archive      = {J_KIS},
  author       = {Michienzi, Andrea and Guidi, Barbara and Ricci, Laura and De Salve, Andrea},
  doi          = {10.1007/s10115-021-01552-w},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1339-1364},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Incremental communication patterns in online social groups},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep graph transformation for attributed, directed, and
signed networks. <em>KIS</em>, <em>63</em>(6), 1305–1337. (<a
href="https://doi.org/10.1007/s10115-021-01553-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized from image and language translation, the goal of graph translation or transformation is to generate a graph of the target domain on the condition of an input graph of the source domain. Existing works are limited to either merely generating the node attributes of graphs with fixed topology or only generating the graph topology without allowing the node attributes to change. They are prevented from simultaneously generating both node and edge attributes due to: (1) difficulty in modeling the iterative, interactive, and asynchronous process of both node and edge translation and (2) difficulty in learning and preserving the inherent consistency between the nodes and edges in generated graphs. A general, end-to-end framework for jointly generating node and edge attributes is needed for real-world problems. In this paper, this generic problem of multi-attributed graph translation is named and a novel framework coherently accommodating both node and edge translations is proposed. The proposed generic edge translation path is also proven to be a generalization of existing topology translation models. Then, in order to discover and preserve the consistency of the generated nodes and edges, a spectral graph regularization based on our nonparametric graph Laplacian is designed. In addition, two extensions of the proposed model are developed for signed and directed graph translation. Lastly, comprehensive experiments on both synthetic and real-world practical datasets demonstrate the power and efficiency of the proposed method.},
  archive      = {J_KIS},
  author       = {Guo, Xiaojie and Zhao, Liang and Homayoun, Houman and Dinakarrao, Sai Manoj Pudukotai},
  doi          = {10.1007/s10115-021-01553-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1305-1337},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep graph transformation for attributed, directed, and signed networks},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing ontologies and databases: A critical review of
lifecycle engineering models in manufacturing. <em>KIS</em>,
<em>63</em>(6), 1271–1304. (<a
href="https://doi.org/10.1007/s10115-021-01558-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature on the modeling and management of data generated through the lifecycle of a manufacturing system is split into two main paradigms: product lifecycle management (PLM) and product, process, resource (PPR) modeling. These paradigms are complementary, and the latter could be considered a more neutral version of the former. There are two main technologies associated with these paradigms: ontologies and databases. Database technology is widespread in industry and is well established. Ontologies remain largely a plaything of the academic community which, despite numerous projects and publications, have seen limited implementations in industrial manufacturing applications. The main objective of this paper is to provide a comparison between ontologies and databases, offering both qualitative and quantitative analyses in the context of PLM and PPR. To achieve this, the article presents (1) a literature review within the context of manufacturing systems that use databases and ontologies, identifying their respective strengths and weaknesses, and (2) an implementation in a real industrial scenario that demonstrates how different modeling approaches can be used for the same purpose. This experiment is used to enable discussion and comparative analysis of both modeling strategies.},
  archive      = {J_KIS},
  author       = {Ramis Ferrer, Borja and Mohammed, Wael M. and Ahmad, Mussawar and Iarovyi, Sergii and Zhang, Jiayi and Harrison, Robert and Martinez Lastra, Jose Luis},
  doi          = {10.1007/s10115-021-01558-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {1271-1304},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Comparing ontologies and databases: A critical review of lifecycle engineering models in manufacturing},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining discriminative itemsets in data streams using the
tilted-time window model. <em>KIS</em>, <em>63</em>(5), 1241–1270. (<a
href="https://doi.org/10.1007/s10115-021-01550-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A discriminative itemset is a frequent itemset in the target data stream with much higher frequency than that of the same itemset in the rest of the data streams in the dataset. The discriminative itemsets describe the distinguishing features between data streams. Mining discriminative itemsets in data streams is very important, where continuously arriving transactions can be inserted in fast speed and large volume. Compared with frequent itemset mining in single data stream, there are additional challenges in the discriminative itemset mining process as the Apriori property of subset is not applicable. We propose an efficient and high accurate method for mining discriminative itemsets in data streams using a tilted-time window model. The proposed single-pass H-DISSparse algorithm is designed particularly based on several well-defined characteristics aiming to improve the approximate frequencies of the itemsets in the tilted-time window model. The data structures are dynamically adjusted in offline time intervals to reflect the discriminative itemset frequencies in different time periods in unsynchronized data streams. Empirical analysis shows the efficient time and space complexity of the proposed method in the fast-growing big data streams.},
  archive      = {J_KIS},
  author       = {Seyfi, Majid and Nayak, Richi and Xu, Yue and Geva, Shlomo},
  doi          = {10.1007/s10115-021-01550-y},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1241-1270},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining discriminative itemsets in data streams using the tilted-time window model},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unifying community detection and network embedding in
attributed networks. <em>KIS</em>, <em>63</em>(5), 1221–1239. (<a
href="https://doi.org/10.1007/s10115-021-01557-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, community detection and network embedding are two separate tasks. Network embedding aims to output a vector representation for each node in the network, and community detection aims to find all densely connected groups of nodes and well separate them from others. Most of the existing approaches do community detection and network embedding in a separate manner, and ignore node attributes information, which leads to poor results. In this paper, we propose a novel model that jointly solves the network embedding and community detection problems together. The model can make use of the network local information, the global information and node attributes information collaboratively. We empirically show that by jointly solving these two problems together, the model can greatly improve the ability of community detection, but also learn better network embedding than the advanced baseline methods. We evaluate the proposed model on several datasets, and the experimental results have shown the effectiveness and advancement of our model.},
  archive      = {J_KIS},
  author       = {Ding, Yu and Wei, Hao and Hu, Guyu and Pan, Zhisong and Wang, Shuaihui},
  doi          = {10.1007/s10115-021-01557-5},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1221-1239},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unifying community detection and network embedding in attributed networks},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expert-driven trace clustering with instance-level
constraints. <em>KIS</em>, <em>63</em>(5), 1197–1220. (<a
href="https://doi.org/10.1007/s10115-021-01548-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the field of process mining, several different trace clustering approaches exist for partitioning traces or process instances into similar groups. Typically, this partitioning is based on certain patterns or similarity between the traces, or driven by the discovery of a process model for each cluster. The main drawback of these techniques, however, is that their solutions are usually hard to evaluate or justify by domain experts. In this paper, we present two constrained trace clustering techniques that are capable to leverage expert knowledge in the form of instance-level constraints. In an extensive experimental evaluation using two real-life datasets, we show that our novel techniques are indeed capable of producing clustering solutions that are more justifiable without a substantial negative impact on their quality.},
  archive      = {J_KIS},
  author       = {De Koninck, Pieter and Nelissen, Klaas and vanden Broucke, Seppe and Baesens, Bart and Snoeck, Monique and De Weerdt, Jochen},
  doi          = {10.1007/s10115-021-01548-6},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1197-1220},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Expert-driven trace clustering with instance-level constraints},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning diffusion model-free and efficient influence
function for influence maximization from information cascades.
<em>KIS</em>, <em>63</em>(5), 1173–1196. (<a
href="https://doi.org/10.1007/s10115-021-01556-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When considering the problem of influence maximization from information cascades, one essential component is influence estimation. Traditional approaches for influence estimation generally follow a two-stage framework, i.e., learn a hypothetical diffusion model from information cascades and then calculate the influence spread according to the learned diffusion model via Monte Carlo simulation or heuristic approximation. The effectiveness of these approaches heavily relies on the correctness of the diffusion model, suffering from the problem of model misspecification. Meanwhile, these approaches are inefficient when influence estimation is conducted via lots of Monte Carlo simulations. In this paper, without assuming a diffusion model a priori, we directly learn a monotone and submodular influence function from information cascades. Once the influence function is obtained, greedy algorithm is applied to efficiently solve influence maximization. Experimental results on both synthetic and real-world datasets show the effectiveness and efficiency of the learned influence function for both influence estimation and influence maximization tasks.},
  archive      = {J_KIS},
  author       = {Cao, Qi and Shen, Huawei and Gao, Jinhua and Cheng, Xueqi},
  doi          = {10.1007/s10115-021-01556-6},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1173-1196},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning diffusion model-free and efficient influence function for influence maximization from information cascades},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel-based regression via a novel robust loss function and
iteratively reweighted least squares. <em>KIS</em>, <em>63</em>(5),
1149–1172. (<a
href="https://doi.org/10.1007/s10115-021-01554-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Least squares kernel-based methods have been widely used in regression problems due to the simple implementation and good generalization performance. Among them, least squares support vector regression (LS-SVR) and extreme learning machine (ELM) are popular techniques. However, the noise sensitivity is a major bottleneck. To address this issue, a generalized loss function, called $$\ell _s$$ -loss, is proposed in this paper. With the support of novel loss function, two kernel-based regressors are constructed by replacing the $$\ell _2$$ -loss in LS-SVR and ELM with the proposed $$\ell _s$$ -loss for better noise robustness. Important properties of $$\ell _s$$ -loss, including robustness, asymmetry and asymptotic approximation behaviors, are verified theoretically. Moreover, iteratively reweighted least squares are utilized to optimize and interpret the proposed methods from a weighted viewpoint. The convergence of the proposal is proved, and detailed analyses of robustness are given. Experiments on both artificial and benchmark datasets confirm the validity of the proposed methods.},
  archive      = {J_KIS},
  author       = {Dong, Hongwei and Yang, Liming},
  doi          = {10.1007/s10115-021-01554-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1149-1172},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Kernel-based regression via a novel robust loss function and iteratively reweighted least squares},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PPNW: Personalized pairwise novelty loss weighting for novel
recommendation. <em>KIS</em>, <em>63</em>(5), 1117–1148. (<a
href="https://doi.org/10.1007/s10115-021-01546-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most works of recommender systems focus on providing users with highly accurate item predictions based on the assumption that accurate suggestions can best satisfy users. However, accuracy-focused models also create great system bias towards popular items and, as a result, unpopular items rarely get recommended and will stay as “cold items” forever. Both users and item providers will suffer in such scenario. To promote item novelty, which plays a crucial role in system robustness and diversity, previous studies focus mainly on re-ranking a top-N list generated by an accuracy-focused base model. The re-ranking algorithm is thus completely independent of the base model. Eventually, these frameworks are essentially limited by the base model and the separated 2 stages cause greater complication and inefficiency in providing novel suggestions. In this work, we propose a personalized pairwise novelty weighting framework for BPR loss function, which covers the limitations of BPR and effectively improves novelty with negligible decrease in accuracy. Base model will be guided by the novelty-aware loss weights to learn user preference and to generate novel top-N list in only 1 stage. Comprehensive experiments on 3 public datasets show that our approach effectively promotes novelty with almost no decrease in accuracy.},
  archive      = {J_KIS},
  author       = {Lo, Kachun and Ishigaki, Tsukasa},
  doi          = {10.1007/s10115-021-01546-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1117-1148},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PPNW: Personalized pairwise novelty loss weighting for novel recommendation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable framework for large time series prediction.
<em>KIS</em>, <em>63</em>(5), 1093–1116. (<a
href="https://doi.org/10.1007/s10115-021-01544-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge discovery systems are nowadays supposed to store and process very large data. When working with big time series, multivariate prediction becomes more and more complicated because the use of all the variables does not allow to have the most accurate predictions and poses certain problems for classical prediction models. In this article, we present a scalable prediction process for large time series prediction, including a new algorithm for identifying time series predictors, which analyses the dependencies between time series using the mutual reinforcement principle between Hubs and Authorities of the Hits (Hyperlink-Induced Topic Search) algorithm. The proposed framework is evaluated on 3 real datasets. The results show that the best predictions are obtained using a very small number of predictors compared to the initial number of variables. The proposed feature selection algorithm shows promising results compared to widely known algorithms, such as the classic and the kernel principle component analysis, factor analysis, and the fast correlation-based filter method, and improves the prediction accuracy of many time series of the used datasets.},
  archive      = {J_KIS},
  author       = {Hmamouche, Youssef and Lakhal, Lotfi and Casali, Alain},
  doi          = {10.1007/s10115-021-01544-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1093-1116},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A scalable framework for large time series prediction},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cost-sensitive selection of variables by ensemble of model
sequences. <em>KIS</em>, <em>63</em>(5), 1069–1092. (<a
href="https://doi.org/10.1007/s10115-021-01551-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications require the collection of data on different variables or measurements over many system performance metrics. We term those broadly as measures or variables. Often data collection along each measure incurs a cost, thus it is desirable to consider the cost of measures in modeling. This is a fairly new class of problems in the area of cost-sensitive learning. A few attempts have been made to incorporate costs in combining and selecting measures. However, existing studies either do not strictly enforce a budget constraint, or are not the ‘most’ cost effective. With a focus on classification problems, we propose a computationally efficient approach that could find a near optimal model under a given budget by exploring the most ‘promising’ part of the solution space. Instead of outputting a single model, we produce a model schedule—a list of models, sorted by model costs and expected predictive accuracy. This could be used to choose the model with the best predictive accuracy under a given budget, or to trade off between the budget and the predictive accuracy. Experiments on some benchmark datasets show that our approach compares favorably to competing methods.},
  archive      = {J_KIS},
  author       = {Yan, Donghui and Qin, Zhiwei and Gu, Songxiang and Xu, Haiping and Shao, Ming},
  doi          = {10.1007/s10115-021-01551-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1069-1092},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cost-sensitive selection of variables by ensemble of model sequences},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PE-MSC: Partial entailment-based minimum set cover for text
summarization. <em>KIS</em>, <em>63</em>(5), 1045–1068. (<a
href="https://doi.org/10.1007/s10115-020-01537-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of Textual Entailment (TE) is an established indicator of text connectedness. It captures semantic relationships between texts. Recently, it has been used successfully for determining sentence salience in many text summarization methods. However, it has been reported in previous works that the standard textual entailment is not ideal for measuring sentence salience. This is because textual entailment relationships between sentences are quite rare in real-world texts. Therefore, we suggest using partial TE to accomplish the task of recognizing standard TE. We present the single document summarization problem as an optimization problem which is solved using a weighted Minimum Set Cover (wMSC) algorithm. In this method, sentences are broken into fragments and Partial TE is used to form sets of fragments. Finally, wMSC is applied to the sets to obtain the minimum set cover, which corresponds to the summary of the document. The results achieved on the DUC 2002 dataset using ROUGE and other quality metrics show that the proposed method outperforms the state of the art.},
  archive      = {J_KIS},
  author       = {Gupta, Anand and Kaur, Manpreet and Mittal, Sonaali and Garg, Swati},
  doi          = {10.1007/s10115-020-01537-1},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1045-1068},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PE-MSC: Partial entailment-based minimum set cover for text summarization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-layer linear embedding with feature subset selection.
<em>KIS</em>, <em>63</em>(4), 1029–1043. (<a
href="https://doi.org/10.1007/s10115-020-01535-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fundamental problems in machine learning require some form of dimensionality reduction. To this end, two different strategies were used: manifold learning and feature selection. Manifold learning (or data embedding) attempts to compute a subspace from original data by feature recombination/transformation. Feature selection aims to select the most relevant features in the original space. In this paper, we propose a novel cooperative manifold learning-feature selection that goes beyond the simple concatenation of these two modules. Our basic idea is to learn an embedding (or the subspace) by computing a cascade of embeddings in which each embedding undergoes feature selection and elimination. We use filter approaches in order to efficiently select irrelevant features at any stage of the process. For a case study, our proposed framework was used with two typical linear embedding algorithms: local discriminant embedding (a supervised technique) and locality preserving projections (unsupervised technique) on four challenging face databases and it has been conveniently compared with other cooperative schemes. Moreover, a comparison with several state-of-the-art manifold learning methods is provided. As it is exhibited by our experimental study, the proposed framework can achieve superior learning performance with respect to classic cooperative schemes and to many competing manifold learning methods.},
  archive      = {J_KIS},
  author       = {Dornaika, F.},
  doi          = {10.1007/s10115-020-01535-3},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1029-1043},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-layer linear embedding with feature subset selection},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed processing of regular path queries in RDF
graphs. <em>KIS</em>, <em>63</em>(4), 993–1027. (<a
href="https://doi.org/10.1007/s10115-020-01536-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SPARQL 1.1 offers a type of navigational query for RDF systems, called regular path query (RPQ). A regular path query allows for retrieving node pairs with the paths between them satisfying regular expressions. Regular path queries are always difficult to be evaluated efficiently because of the possible large search space. Thus there has been no scalable and practical solution so far. In this paper, we present Leon+, an in-memory distributed framework, to address the RPQ problem in the context of the knowledge graph. To reduce search space and mitigate mounting communication costs, Leon+ takes advantage of join-ahead pruning via a novel RDF summarization technique together with a path partitioning strategy. We also develop a subtle cost model to devise query plans to achieve high efficiency for complex RPQs. As there has been no available RPQ benchmark, we create micro-benchmarks on both synthetic and real-world datasets. A thorough experimental evaluation is presented between our approach and the state-of-the-art RDF stores. The results show that our approach outperforms 5x faster than the competitors on single RPQ. For query workload, it saves up to 1/2 time and 2/3 communication overheads over the baseline method.},
  archive      = {J_KIS},
  author       = {Guo, Xintong and Gao, Hong and Zou, Zhaonian},
  doi          = {10.1007/s10115-020-01536-2},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {993-1027},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Distributed processing of regular path queries in RDF graphs},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selectivity estimation with density-model-based
multidimensional histogram. <em>KIS</em>, <em>63</em>(4), 971–992. (<a
href="https://doi.org/10.1007/s10115-021-01547-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histograms are widely used in selectivity estimation for one-dimensional data. Using the one-dimensional histograms to estimate the selectivity of the multidimensional queries will result in a high estimation error, unless the assumption of attribute independence is true. Constructing a multidimensional histogram also brings great challenges. The storage of a multidimensional histogram exponentially increases with the number of dimensions. In this paper, we propose a density-model-based multidimensional histogram. It uses a lightweight density model to predict the densities of a large number of regions instead of storing too many buckets. The experimental results indicate that our method can provide highly accurate selectivity estimations while occupying little space. In addition, the superiority of our method is more evident in high-dimensional data.},
  archive      = {J_KIS},
  author       = {Zhang, Meifan and Wang, Hongzhi},
  doi          = {10.1007/s10115-021-01547-7},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {971-992},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Selectivity estimation with density-model-based multidimensional histogram},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tracking triadic cardinality distributions for burst
detection in high-speed graph streams. <em>KIS</em>, <em>63</em>(4),
939–969. (<a href="https://doi.org/10.1007/s10115-021-01543-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In everyday life, we often observe unusually frequent interactions among people before or during important events, e.g., people send/receive more greetings to/from their friends on holidays than regular days. We also observe that some videos or hashtags suddenly go viral through people’s sharing on online social networks (OSNs). Do these seemingly different phenomena share a common structure? All these phenomena are associated with the sudden surges of node interactions in networks, which we call “bursts” in this work. We uncover that, in many scenarios, the emergence of a burst is accompanied with the formation of triangles in networks. This finding motivates us to propose a new and robust method for burst detection on an OSN. We first introduce a new measure, i.e., “triadic cardinality distribution,” corresponding to the fractions of nodes with different numbers of triangles, i.e., triadic cardinalities, in a network. We show that this distribution not only changes when a burst occurs, but it also has a robustness property that it is immunized against common spamming social-bot attacks. Hence, by tracking triadic cardinality distributions, we can more reliably detect bursts than simply counting node interactions on an OSN. To avoid handling massive activity data generated by OSN users during the triadic tracking, we design an efficient “sample-estimate” framework to provide maximum likelihood estimate of the triadic cardinality distribution. We propose several sampling methods and provide insights into their performance difference through both theoretical analysis and empirical experiments on real-world networks.},
  archive      = {J_KIS},
  author       = {Zhao, Junzhou and Wang, Pinghui and Chen, Zhouguo and Ding, Jianwei and Lui, John C. S. and Towsley, Don and Guan, Xiaohong},
  doi          = {10.1007/s10115-021-01543-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {939-969},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tracking triadic cardinality distributions for burst detection in high-speed graph streams},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AQUA+: Query optimization for hybrid database-MapReduce
system. <em>KIS</em>, <em>63</em>(4), 905–938. (<a
href="https://doi.org/10.1007/s10115-020-01542-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MapReduce has been widely recognized as an efficient tool for large-scale data analysis. It achieves high performance by exploiting parallelism among processing nodes while providing a simple interface for upper-layer applications. However, there are many existing applications maintaining their data in a distributed database. It is costly to export those data into the storage system of MapReduce (normally a distributed file system). Moreover, compared to MapReduce, database is equipped with many state-of-the-art techniques, such as index and optimizer. Therefore, a hybrid Database-MapReduce system inheriting the advantages of both systems is preferred. In this paper, we propose AQUA+, a query optimizer tailored for the hybrid system. AQUA+ is an extension work of our previous system AQUA. It generates a plan that adaptively assigns the operators to the database engine and MapReduce engine to optimize the performance. The intuition is to exploit the index, co-partition and other features provided by the database as much as possible and reduce the data volume processed by the MapReduce. Due to the complexity of query optimization, in AQUA+, we introduce a novel tuning technique, learning to optimize. In particular, two neural networks are trained to predict cost and refine query plan, respectively. We train them based on our log of real query processing. Experiments carried out on our in-house cluster confirm the effectiveness of our query optimizer.},
  archive      = {J_KIS},
  author       = {Pang, Zhifei and Wu, Sai and Huang, Haichao and Hong, Zhouzhenyan and Xie, Yuqing},
  doi          = {10.1007/s10115-020-01542-4},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {905-938},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {AQUA+: Query optimization for hybrid database-MapReduce system},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards metrics-driven ontology engineering. <em>KIS</em>,
<em>63</em>(4), 867–903. (<a
href="https://doi.org/10.1007/s10115-021-01545-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software engineering field is continuously making an effort to improve the effectiveness of the software development process. This improvement is performed by developing quantitative measures that can be used to enhance the quality of software products and to more accurately describe, better understand and manage the software development life cycle. Even if the ontology engineering field is constantly adopting practices from software engineering, it has not yet reached a state in which metrics are an integral part of ontology engineering processes and support making evidence-based decisions over the process and its outputs. Up to now, ontology metrics are mainly focused on the ontology implementation and do not take into account the development process or other artefacts that can help assessing the quality of the ontology, e.g. its requirements. This work envisions the need for a metrics-driven ontology engineering process and, as a first step, presents a set of metrics for ontology engineering which are obtained from artefacts generated during the ontology development process and from the process itself. The approach is validated by measuring the ontology engineering process carried out in a research project and by showing how the proposed metrics can be used to improve the efficiency of the process by making predictions, such as the effort needed to implement an ontology, or assessments, such as the coverage of the ontology according to its requirements.},
  archive      = {J_KIS},
  author       = {Fernández-Izquierdo, Alba and Poveda-Villalón, María and Gómez-Pérez, Asunción and García-Castro, Raúl},
  doi          = {10.1007/s10115-021-01545-9},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {867-903},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Towards metrics-driven ontology engineering},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfer learning for fine-grained entity typing.
<em>KIS</em>, <em>63</em>(4), 845–866. (<a
href="https://doi.org/10.1007/s10115-021-01549-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained entity typing (FGET) is to classify the mentions of entities into hierarchical fine-grained semantic types. There are two main issues with existing FGET approaches. Firstly, the process of training corpora for FGET is normally to label the data automatically, which inevitably induces noises. Existing approaches either directly tweak noisy labels in corpora by heuristics or algorithmically retreat to parental types, both leading to coarse-grained type labels instead of fine-grained ones. Secondly, existing approaches usually use recurrent neural networks to generate feature representations of mention phrases and their contexts, which, however, perform relatively poor on long contexts and out-of-vocabulary (OOV) words. In this paper, we propose a transfer learning-based approach to extract more efficient feature representations and offset label noises. More precisely, we adopt three transfer learning schemes: (i) transferring sub-word embeddings to generate more efficient OOV embeddings; (ii) using a pre-trained language model to generate more efficient context features; (iii) using a pre-trained topic model to transfer the topic-type relatedness through topic anchors and select confusing fine-grained types at inference time. The pre-trained topic model can offset the label noises without retreating to coarse-grained types. The experimental results demonstrate the effectiveness of our transfer learning approach for FGET.},
  archive      = {J_KIS},
  author       = {Hou, Feng and Wang, Ruili and Zhou, Yi},
  doi          = {10.1007/s10115-021-01549-5},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {845-866},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Transfer learning for fine-grained entity typing},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Natural language question answering over knowledge graph:
The marriage of SPARQL query and keyword search. <em>KIS</em>,
<em>63</em>(4), 819–844. (<a
href="https://doi.org/10.1007/s10115-020-01534-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language question answering over knowledge graph has received widespread attention. However, the existing methods always aim to improve every phase of natural language question answering and neglect the defects; namely, not all query intentions can be identified and mapped to the correct SPARQL statement. In contrast, keyword search relies on the links among multiple keywords regardless of the exact logic relations in question. Therefore, we propose a framework (abbreviated as NLQSK for title of this paper) that introduces keyword search into natural language question answering to compensate for the defects mentioned above. First, we translate a natural language question into top-k SPARQL statements by using the existing methods. Second, we transform the valuable information that cannot be identified and mapped into keywords, and then, return the neighboring information in a knowledge graph by keyword index. Third, we combine the SPARQL block (i.e., the SPARQL statement and its result) and keyword search to produce the answer to the natural language question. Finally, the experiments on the benchmark dataset confirm that keyword search can compensate for the defects of natural language question answering and that NLQSK can answer more questions than the existing state-of-the-art question answering systems.},
  archive      = {J_KIS},
  author       = {Hu, Xin and Duan, Jiangli and Dang, Depeng},
  doi          = {10.1007/s10115-020-01534-4},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {819-844},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Natural language question answering over knowledge graph: The marriage of SPARQL query and keyword search},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RELINE: Point-of-interest recommendations using multiple
network embeddings. <em>KIS</em>, <em>63</em>(4), 791–817. (<a
href="https://doi.org/10.1007/s10115-020-01541-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of users’ involvement in Location-Based Social Networks has led to the expeditious growth of the data on a global scale. The need of accessing and retrieving relevant information close to users’ preferences is an open problem which continuously raises new challenges for recommendation systems. The exploitation of points-of-interest (POIs) recommendation by existing models is inadequate due to the sparsity and the cold start problems. To overcome these problems many models were proposed in the literature; however, most of them ignore important factors, such as: geographical proximity, social influence, or temporal and preference dynamics, which tackle their accuracy while personalize their recommendations. In this work, we investigate these problems and present a unified model that jointly learns user’s and POI dynamics. Our proposal is termed RELINE (REcommendations with muLtIple Network Embeddings). More specifically, RELINE captures: (i) the social, (ii) the geographical, (iii) the temporal influence, and (iv) the users’ preference dynamics, by embedding eight relational graphs into one shared latent space. We have evaluated our approach against state-of-the-art methods with three large real-world datasets in terms of accuracy. Additionally, we have examined the effectiveness of our approach against the cold-start problem. Performance evaluation results demonstrate that significant performance improvement is achieved in comparison to existing state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Christoforidis, Giannis and Kefalas, Pavlos and Papadopoulos, Apostolos N. and Manolopoulos, Yannis},
  doi          = {10.1007/s10115-020-01541-5},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {791-817},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {RELINE: Point-of-interest recommendations using multiple network embeddings},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient computation of deletion-robust k-coverage queries.
<em>KIS</em>, <em>63</em>(3), 759–789. (<a
href="https://doi.org/10.1007/s10115-020-01540-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting a controllable subset from a large-scale dataset so that users can fully understand the entire dataset is a significant topic for multicriteria decision making. In recent years, this problem has been widely studied, and various query models have been proposed, such as top-k, skyline, k-regret and k-coverage queries. Among these models, the k-coverage query is an ideal query method; this model has stability, scale invariance and high traversal efficiency. However, current methods including k-coverage queries cannot deal with deleting some points from the dataset while providing an effective solution set efficiently. In this paper, we study the robustness of k-coverage queries in two cases involving the dynamic deletion of data points. The first case is when it is assumed that the whole dataset can be obtained in advance, while the second is when the data points arrive in a stream. For a centralized dataset, we introduce a sieving mechanism and use a precalculated threshold to filter a coreset from the entire dataset. Then, the k-coverage query can be carried out on this small coreset instead of the entire dataset, and we propose a threshold-based k-coverage query algorithm, which greatly accelerates query processing. For a streaming dataset, a special chain structure is adopted. Furthermore, a single-pass streaming algorithm named Robust-Sieving is proposed. Moreover, the coreset-based method is extended to answer the problem. In addition, sampling techniques are adopted to accelerate query processing under these two circumstances. Extensive experiments verify the effectiveness of our proposed Robust-Sieving algorithm and the coreset-based algorithms with or without sampling.},
  archive      = {J_KIS},
  author       = {Zheng, Jiping and Huang, Xingnan and Ma, Yuan},
  doi          = {10.1007/s10115-020-01540-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {759-789},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient computation of deletion-robust k-coverage queries},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep dynamic neural networks for temporal language modeling
in author communities. <em>KIS</em>, <em>63</em>(3), 733–757. (<a
href="https://doi.org/10.1007/s10115-020-01539-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language models are at the heart of numerous works, notably in the text mining and information retrieval communities. These statistical models aim at extracting word distributions, from simple unigram models to recurrent approaches with latent variables that capture subtle dependencies in texts. However, those models are learned from word sequences only, and authors’ identities, as well as publication dates, are seldom considered. We propose a neural model, based on recurrent language modeling (e.g., LSTM), which aims at capturing language diffusion tendencies in author communities through time. By conditioning language models with author and dynamic temporal vector states, we are able to leverage the latent dependencies between the text contexts. The model captures language evolution of authors via a shared temporal prediction function in a latent space, which allows to handle a variety of modeling tasks, including completion and prediction of language models through time. Experiments show the performances of the approach, compared to several temporal and non-temporal language baselines on two real-world corpora.},
  archive      = {J_KIS},
  author       = {Delasalles, Edouard and Lamprier, Sylvain and Denoyer, Ludovic},
  doi          = {10.1007/s10115-020-01539-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {733-757},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep dynamic neural networks for temporal language modeling in author communities},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency-based YOLO for single target detection.
<em>KIS</em>, <em>63</em>(3), 717–732. (<a
href="https://doi.org/10.1007/s10115-020-01538-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, You only look once (YOLO) is the fastest real-time object detection system based on a unified deep neural network. During training, YOLO divides the input image to $$S \times S $$ gird cells and the only one grid cell that contains the center of an object, takes charge of detecting that object. It is not sure that the cell corresponding to the center of the object is the best choice to detect the object. In this paper, inspired by the visual saliency mechanism we introduce the saliency map to YOLO to develop YOLO3-SM method, where saliency map selects the grid cell containing the most salient part of the object to detect the object. The experimental results on two data sets show that the prediction box of YOLO3-SM obtains the lager IOU value, which demonstrates that compared with YOLO3 , the YOLO3-SM selects the cell that is more suitable to detect the object . In addition, YOLO3-SM gets the highest mAP that the other three state-of-the-art object detection methods on the two data sets, which shows that introducing the saliency map to YOLO can improve the detection performance.},
  archive      = {J_KIS},
  author       = {Hu, Jun-ying and Shi, C.-J. Richard and Zhang, Jiang-she},
  doi          = {10.1007/s10115-020-01538-0},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {717-732},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Saliency-based YOLO for single target detection},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auto-labelling entities in low-resource text: A geological
case study. <em>KIS</em>, <em>63</em>(3), 695–715. (<a
href="https://doi.org/10.1007/s10115-020-01532-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies on named entity recognition (NER) often require a substantial amount of human-annotated training data. This makes technical domain-specific NER from industry data especially challenging as labelled data are scarce. Despite English as the surface language, technical jargon and writing conventions used in technical documents render the low-resource language challenges where techniques such as transfer learning hardly work. Relieving labour intensive annotations using automatic labelling is thus an important research topic, seeking ways to obtain labelled data quickly and consistently. In this work, we propose an iterative deep learning NER framework using distant supervision for automatic labelling of domain-specific datasets. The framework is applied to mineral exploration reports and produced a large BIO-annotated dataset with six geological categories. This quality-labelled dataset, OzROCK, is made publicly available to support future research on technical domain NER. Experimental results demonstrated the effectiveness of this approach, further confirmed by domain experts. The generalisation ability is verified by applying the framework to two other datasets: one for disease names and the other for chemical names. Overall, our approach can effectively reduce annotation efforts by identifying a much smaller subset, that is challenging for automatic labelling thus requires attention from human experts.},
  archive      = {J_KIS},
  author       = {Enkhsaikhan, Majigsuren and Liu, Wei and Holden, Eun-Jung and Duuring, Paul},
  doi          = {10.1007/s10115-020-01532-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {695-715},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Auto-labelling entities in low-resource text: A geological case study},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving spectral clustering with deep embedding, cluster
estimation and metric learning. <em>KIS</em>, <em>63</em>(3), 675–694.
(<a href="https://doi.org/10.1007/s10115-020-01530-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is one of the most popular modern clustering algorithms. It is easy to implement, can be solved efficiently, and very often outperforms other traditional clustering algorithms such as k-means. However, spectral clustering could be insufficient when dealing with most datasets having complex statistical properties, and it requires users to specify the number k of clusters and a good distance metric to construct the similarity graph. To address these problems, in this article, we propose an approach to extending spectral clustering with deep embedding, cluster estimation, and metric learning. First, we generate the deep embedding via learning a deep autoencoder, which transforms the raw data into their lower dimensional representations suitable for clustering. Second, we provide an effective method to estimate the number of clusters by learning a softmax autoencoder from the deep embedding. Third, we construct a more powerful similarity graph by learning a distance metric from the embedding using a Siamese network. Finally, we conduct an extensive experimental study on image and text datasets, which verifies the effectiveness and efficiency of our approach.},
  archive      = {J_KIS},
  author       = {Duan, Liang and Ma, Shuai and Aggarwal, Charu and Sathe, Saket},
  doi          = {10.1007/s10115-020-01530-8},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {675-694},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving spectral clustering with deep embedding, cluster estimation and metric learning},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). L-ideals and rough sets based on l-ideals. <em>KIS</em>,
<em>63</em>(3), 647–673. (<a
href="https://doi.org/10.1007/s10115-020-01529-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let D be a distributive lattice, and let L be a frame. In this article, we introduce the notion of L-ideals of D. We show that the set of all L-ideals of D is a distributive lattice, and some essential properties of this lattice are studied. Also, we discuss some special elements of this lattice. Moreover, we define a novel congruence relation for the concept of L-ideal of D. Finally, we study some properties of rough sets inherited from the congruence relation.},
  archive      = {J_KIS},
  author       = {Estaji, Ali Akbar and Haghdadi, Toktam and Farokhi Ostad, Javad},
  doi          = {10.1007/s10115-020-01529-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {647-673},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {L-ideals and rough sets based on L-ideals},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid neural network approach to combine textual
information and rating information for item recommendation.
<em>KIS</em>, <em>63</em>(3), 621–646. (<a
href="https://doi.org/10.1007/s10115-020-01528-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) is a common method used by many recommender systems. Traditional CF algorithms exploit users’ ratings as the sole information source to learn user preferences. However, ratings usually sparse cause a serious impact on the recommendation results. Most existing CF algorithms use ratings and textual information to alleviate the sparsity of data and then utilize matrix factorization to achieve the latent feature interactions for rating prediction. Nevertheless, the following shortcomings remain in these studies: (1) The word orders and surrounding words of the textual information are ignored. (2) The nonlinearity of feature interactions is seldom exploited. Therefore, we propose a novel hybrid neural network to combine textual information and rating (NCTR) information for item recommendation. The proposed NCTR model is built upon a hybrid neural network framework with fine-grained modeling of latent representation and nonlinearity feature interactions for rating prediction. Specifically, convolution neural network is applied to extract effectively contextual features from textual information. Meanwhile, a fusion layer is exploited to combine features, and the multilayer perceptions are used to model the nonlinear interactions between the merged item latent features and user latent features. Experimental results over five real-world datasets show that NCTR significantly outperforms several state-of-the-art recommendation methods. Source codes are available in https://github.com/luojia527/NCTR_master .},
  archive      = {J_KIS},
  author       = {Liu, Donghua and Li, Jing and Du, Bo and Chang, Jun and Gao, Rong and Wu, Yujia},
  doi          = {10.1007/s10115-020-01528-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {621-646},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid neural network approach to combine textual information and rating information for item recommendation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Warped softmax regression for time series classification.
<em>KIS</em>, <em>63</em>(3), 589–619. (<a
href="https://doi.org/10.1007/s10115-020-01533-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear models are a mainstay in statistical pattern recognition but do not play a role in time series classification, because they fail to account for temporal variations. To overcome this limitation, we combine linear models with dynamic time warping (dtw). We analyze the resulting warped-linear models theoretically and empirically. The three main theoretical results are (i) the Representation Theorem, (ii) the Matrix Complexity Lemma, and (iii) local Lipschitz continuity of the warped softmax function. The Representation Theorem roughly states that warped-linear models correspond to polytope classifiers in Euclidean spaces. This key result is useful because it simplifies analysis of warped-linear models. For example, it provides a geometric interpretation, points to the label dependency problem, and justifies application of warped-linear models not only on temporal but also on multivariate data. The Representation Theorem together with the Matrix Complexity Lemma reveals that warped-linear models implement a weight trick by weight selection and massive weight sharing. Local Lipschitz continuity of warped softmax functions admits a principled training of warped-linear models by stochastic subgradient methods. Empirical results show that replacing the inner product of linear models with a dtw-score substantially improves its predictive performance. The theoretical and empirical contributions of this article provide a simple and efficient first-trial alternative to nearest-neighbor methods and open up new perspectives for more sophisticated classifiers such as warped deep learning.},
  archive      = {J_KIS},
  author       = {Jain, Brijnesh},
  doi          = {10.1007/s10115-020-01533-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {589-619},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Warped softmax regression for time series classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Closed form word embedding alignment. <em>KIS</em>,
<em>63</em>(3), 565–588. (<a
href="https://doi.org/10.1007/s10115-020-01531-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a family of techniques to align word embeddings which are derived from different source datasets or created using different mechanisms (e.g., GloVe or word2vec). Our methods are simple and have a closed form to optimally rotate, translate, and scale to minimize root mean squared errors or maximize the average cosine similarity between two embeddings of the same vocabulary into the same dimensional space. Our methods extend approaches known as absolute orientation, which are popular for aligning objects in three dimensions, and generalize an approach by Smith et al. (ICLR 2017). We prove new results for optimal scaling and for maximizing cosine similarity. Then, we demonstrate how to evaluate the similarity of embeddings from different sources or mechanisms, and that certain properties like synonyms and analogies are preserved across the embeddings and can be enhanced by simply aligning and averaging ensembles of embeddings.},
  archive      = {J_KIS},
  author       = {Dev, Sunipa and Hassan, Safia and Phillips, Jeff M.},
  doi          = {10.1007/s10115-020-01531-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {565-588},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Closed form word embedding alignment},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial multi-label learning with noisy side information.
<em>KIS</em>, <em>63</em>(2), 541–564. (<a
href="https://doi.org/10.1007/s10115-020-01527-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning (PML) aims to learn from the training data where each training example is annotated with a candidate label set, among which only a subset is relevant. Despite the success of existing PML approaches, a major drawback of them lies in lacking of robustness to noisy side information. To tackle this problem, we introduce a novel partial multi-label learning with noisy side information approach, which simultaneously removes noisy outliers from the training instances and trains robust partial multi-label classifier for unlabeled instances prediction. Specifically, we first represent the observed sample set as a feature matrix and then decompose it into an ideal feature matrix and an outlier feature matrix by using the low-rank and sparse decomposition scheme, where the former is constrained to be low rank by considering that the noise-free feature information always lies in a low-dimensional subspace and the latter is assumed to be sparse by considering that the outliers are usually sparse among the observed feature matrix. Secondly, we refine an ideal label confidence matrix from the observed label matrix and use the graph Laplacian regularization to constrain the confidence matrix to keep the intrinsic structure among feature vectors. Thirdly, we constrain the feature mapping matrix to be low rank by utilizing the label correlations. Finally, we obtain both the ideal features and ground-truth labels via minimizing the loss function, where the augmented Lagrange multiplier algorithm and quadratic programming are incorporated to solve the optimization problem. Extensive experiments conducted on ten different data sets demonstrate the effectiveness of our proposed method.},
  archive      = {J_KIS},
  author       = {Sun, Lijuan and Feng, Songhe and Lyu, Gengyu and Zhang, Hua and Dai, Guojun},
  doi          = {10.1007/s10115-020-01527-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {541-564},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Partial multi-label learning with noisy side information},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A graph grammar and <span
class="math display"><em>K</em><sub>4</sub></span> -type
tournament-based approach to detect conflicts of interest in a social
network. <em>KIS</em>, <em>63</em>(2), 497–539. (<a
href="https://doi.org/10.1007/s10115-020-01525-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new approach based on properties of graph grammars to detect conflicts of interest (COIs) in a field represented in the form of a social network. The approach consists of specializing the adaptive star graph grammar (ASGG) of Drewes et al. (Theor Comput Sci 411:3090–3109, 2010) to express kind of subgraphs that we call $$K_4$$ -type tournament graphs, corresponding to COIs, that cannot be generated by the node replacement graph grammar. This approach, called graph grammar and $$K_{4}$$ -type tournament-based approach to detect conflicts of interest $$(GGK_{4}T-COIs)$$ , is applied to detect COIs in the review process of papers accepted in an international conference which is represented through a social network. In this contribution, the principle of the used graph grammar is not to consider all the generated language but only subgraphs with some properties (corresponding to special graph queries), which identify parts of the social network representing COIs. For evaluating the performances and the efficiency of our proposition, experimentations have been done by comparing it with concurrent methods in the literature. The obtained results have shown that the approach GG $$K_{4}$$ T-COIs performs better than the investigated state-of-the-art approaches in terms of type and number of detected COIs.},
  archive      = {J_KIS},
  author       = {Albane, Saadia and Slimani, Hachem and Kheddouci, Hamamache},
  doi          = {10.1007/s10115-020-01525-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {497-539},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A graph grammar and $$K_{4}$$ -type tournament-based approach to detect conflicts of interest in a social network},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A repairing missing activities approach with succession
relation for event logs. <em>KIS</em>, <em>63</em>(2), 477–495. (<a
href="https://doi.org/10.1007/s10115-020-01524-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of process mining, it is worth noting that process mining techniques assume that the resulting event logs can not only continuously record the occurrence of events but also contain all event data. However, like in IoT systems, data transmission may fail due to weak signal or resource competition, which causes the company’s information system to be unable to keep a complete event log. Based on a incomplete event log, the process model obtained by using existing process mining technologies is deviated from actual business process to a certain degree. In this paper, we propose a method for repairing missing activities based on succession relation of activities from event logs. We use an activity relation matrix to represent the event log and cluster it. The number of traces in the cluster is used as a measure of similarity calculation between incomplete traces and cluster results. Parallel activities in selecting pre-occurrence and post-occurrence activities of missing activities from incomplete traces are considered. Experimental results on real-life event logs show that our approach performs better than previous method in repairing missing activities.},
  archive      = {J_KIS},
  author       = {Liu, Jie and Xu, Jiuyun and Zhang, Ruru and Reiff-Marganiec, Stephan},
  doi          = {10.1007/s10115-020-01524-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {477-495},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A repairing missing activities approach with succession relation for event logs},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anytime mining of sequential discriminative patterns in
labeled sequences. <em>KIS</em>, <em>63</em>(2), 439–476. (<a
href="https://doi.org/10.1007/s10115-020-01523-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is extremely useful to exploit labeled datasets not only to learn models and perform predictive analytics but also to improve our understanding of a domain and its available targeted classes. The subgroup discovery task has been considered for more than two decades. It concerns the discovery of patterns covering sets of objects having interesting properties, e.g., they characterize or discriminate a given target class. Though many subgroup discovery algorithms have been proposed for both transactional and numerical data, discovering subgroups within labeled sequential data has been much less studied. First, we propose an anytime algorithm SeqScout that discovers interesting subgroups w.r.t. a chosen quality measure. This is a sampling algorithm that mines discriminant sequential patterns using a multi-armed bandit model. For a given budget, it finds a collection of local optima in the search space of descriptions and thus, subgroups. It requires a light configuration and is independent from the quality measure used for pattern scoring. We also introduce a second anytime algorithm MCTSExtent that pushes further the idea of a better trade-off between exploration and exploitation of a sampling strategy over the search space. To the best of our knowledge, this is the first time that the Monte Carlo Tree Search framework is exploited in a sequential data mining setting. We have conducted a thorough and comprehensive evaluation of our algorithms on several datasets to illustrate their added value, and we discuss their qualitative and quantitative results.},
  archive      = {J_KIS},
  author       = {Mathonat, Romain and Nurbakova, Diana and Boulicaut, Jean-François and Kaytoue, Mehdi},
  doi          = {10.1007/s10115-020-01523-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {439-476},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anytime mining of sequential discriminative patterns in labeled sequences},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CANE: Community-aware network embedding via adversarial
training. <em>KIS</em>, <em>63</em>(2), 411–438. (<a
href="https://doi.org/10.1007/s10115-020-01521-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding aims to learn a low-dimensional representation vector for each node while preserving the inherent structural properties of the network, which could benefit various downstream mining tasks such as link prediction and node classification. Most existing works can be considered as generative models that approximate the underlying node connectivity distribution in the network, or as discriminate models that predict edge existence under a specific discriminative task. Although several recent works try to unify the two types of models with adversarial learning to improve the performance, they only consider the local pairwise connectivity between nodes. Higher-order structural information such as communities, which essentially reflects the global topology structure of the network, is largely ignored. To this end, we propose a novel framework called CANE to simultaneously learn the node representations and identify the network communities. The two tasks are integrated and mutually reinforce each other under a novel adversarial learning framework. Specifically, with the detected communities, CANE jointly minimizes the pairwise connectivity loss and the community assignment error to improve node representation learning. In turn, the learned node representations provide high-quality features to facilitate community detection. Experimental results on multiple real datasets demonstrate that CANE achieves substantial performance gains over state-of-the-art baselines in various applications including link prediction, node classification, recommendation, network visualization, and community detection.},
  archive      = {J_KIS},
  author       = {Wang, Jia and Cao, Jiannong and Li, Wei and Wang, Senzhang},
  doi          = {10.1007/s10115-020-01521-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {411-438},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CANE: Community-aware network embedding via adversarial training},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical model for reproducibility in ranking-based
feature selection. <em>KIS</em>, <em>63</em>(2), 379–410. (<a
href="https://doi.org/10.1007/s10115-020-01519-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability of feature subset selection algorithms has become crucial in real-world problems due to the need for consistent experimental results across different replicates. Specifically, in this paper, we analyze the reproducibility of ranking-based feature subset selection algorithms. When applied to data, this family of algorithms builds an ordering of variables in terms of a measure of relevance. In order to quantify the reproducibility of ranking-based feature subset selection algorithms, we propose a model that takes into account all the different sized subsets of top-ranked features. The model is fitted to data through the minimization of an error function related to the expected values of Kuncheva’s consistency index for those subsets. Once it is fitted, the model provides practical information about the feature subset selection algorithm analyzed, such as a measure of its expected reproducibility or its estimated area under the receiver operating characteristic curve regarding the identification of relevant features. We test our model empirically using both synthetic and a wide range of real data. The results show that our proposal can be used to analyze feature subset selection algorithms based on rankings in terms of their reproducibility and their performance.},
  archive      = {J_KIS},
  author       = {Urkullu, Ari and Pérez, Aritz and Calvo, Borja},
  doi          = {10.1007/s10115-020-01519-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {379-410},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Statistical model for reproducibility in ranking-based feature selection},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BestNeighbor: Efficient evaluation of kNN queries on large
time series databases. <em>KIS</em>, <em>63</em>(2), 349–378. (<a
href="https://doi.org/10.1007/s10115-020-01518-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents parallel solutions (developed based on two state-of-the-art algorithms iSAX and sketch) for evaluating k nearest neighbor queries on large databases of time series, compares them based on various measures of quality and time performance, and offers a tool that uses the characteristics of application data to determine which algorithm to choose for that application and how to set the parameters for that algorithm. Specifically, our experiments show that: (i) iSAX and its derivatives perform best in both time and quality when the time series can be characterized by a few low-frequency Fourier Coefficients, a regime where the iSAX pruning approach works well. (ii) iSAX performs significantly less well when high-frequency Fourier Coefficients have much of the energy of the time series. (iii) A random projection approach based on sketches by contrast is more or less independent of the frequency power spectrum. The experiments show the close relationship between pruning ratio and time for exact iSAX as well as between pruning ratio and the quality of approximate iSAX. Our toolkit analyzes typical time series of an application (i) to determine optimal segment sizes for iSAX and (ii) when to use Parallel Sketches instead of iSAX. Our algorithms have been implemented using Spark, evaluated over a cluster of nodes, and have been applied to both real and synthetic data. The results apply to any databases of numerical sequences, whether or not they relate to time.},
  archive      = {J_KIS},
  author       = {Levchenko, Oleksandra and Kolev, Boyan and Yagoubi, Djamel-Edine and Akbarinia, Reza and Masseglia, Florent and Palpanas, Themis and Shasha, Dennis and Valduriez, Patrick},
  doi          = {10.1007/s10115-020-01518-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {349-378},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {BestNeighbor: Efficient evaluation of kNN queries on large time series databases},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A relative position attention network for aspect-based
sentiment analysis. <em>KIS</em>, <em>63</em>(2), 333–347. (<a
href="https://doi.org/10.1007/s10115-020-01512-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis can predict the sentiment polarity of specific aspect terms in the text. Compared to general sentiment analysis, it extracts more useful information and analyzes the sentiment more accurately in the comment text. Many previous approaches use long short-term memory networks with attention mechanisms to directly learn aspect-specific representations and model comment text. However, these methods always ignore the importance of the aspect terms position and interactive information between the aspect terms and other words. To address these issues, we propose an improved model based on convolutional neural networks. First, a novel relative position encode layer can integrate the relative position information of specific aspect terms validly in a text. Second, by using the aspect attention mechanism, the semantic relationship between aspect terms and words in the text is fully considered. To verify the effectiveness of the proposed models, we conduct a large number of experiments and comparisons on seven public datasets. The experimental results show that this model outperforms to other state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Wu, Chao and Xiong, Qingyu and Gao, Min and Li, Qiude and Yu, Yang and Wang, Kaige},
  doi          = {10.1007/s10115-020-01512-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {333-347},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A relative position attention network for aspect-based sentiment analysis},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning credible DNNs via incorporating prior knowledge and
model local explanation. <em>KIS</em>, <em>63</em>(2), 305–332. (<a
href="https://doi.org/10.1007/s10115-020-01517-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that state-of-the-art DNNs are not always credible, despite their impressive performance on the hold-out test set of a variety of tasks. These models tend to exploit dataset shortcuts to make predictions, rather than learn the underlying task. The non-credibility could lead to low generalization, adversarial vulnerability, as well as algorithmic discrimination of the DNN models. In this paper, we propose CREX in order to develop more credible DNNs. The high-level idea of CREX is to encourage DNN models to focus more on evidences that actually matter for the task at hand and to avoid overfitting to data-dependent shortcuts. Specifically, in the DNN training process, CREX directly regularizes the local explanation with expert rationales, i.e., a subset of features highlighted by domain experts as justifications for predictions, to enforce the alignment between local explanations and rationales. Even when rationales are not available, CREX still could be useful by requiring the generated explanations to be sparse. In addition, CREX is widely applicable to different network architectures, including CNN, LSTM and attention model. Experimental results on several text classification datasets demonstrate that CREX could increase the credibility of DNNs. Comprehensive analysis further shows three meaningful improvements of CREX: (1) it significantly increases DNN accuracy on new and previously unseen data beyond test set, (2) it enhances fairness of DNNs in terms of equality of opportunity metric and reduce models’ discrimination toward certain demographic group, and (3) it promotes the robustness of DNN models with respect to adversarial attack. These experimental results highlight the advantages of the increased credibility by CREX.},
  archive      = {J_KIS},
  author       = {Du, Mengnan and Liu, Ninghao and Yang, Fan and Hu, Xia},
  doi          = {10.1007/s10115-020-01517-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {305-332},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning credible DNNs via incorporating prior knowledge and model local explanation},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A differentially private algorithm for range queries on
trajectories. <em>KIS</em>, <em>63</em>(2), 277–303. (<a
href="https://doi.org/10.1007/s10115-020-01520-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel algorithm to ensure $$\epsilon $$ -differential privacy for answering range queries on trajectory data. In order to guarantee privacy, differential privacy mechanisms add noise to either data or query, thus introducing errors to queries made and potentially decreasing the utility of information. In contrast to the state of the art, our method achieves significantly lower error as it is the first data- and query-aware approach for such queries. The key challenge for answering range queries on trajectory data privately is to ensure an accurate count. Simply representing a trajectory as a set instead of sequence of points will generally lead to highly inaccurate query answers as it ignores the sequential dependency of location points in trajectories, i.e., will violate the consistency of trajectory data. Furthermore, trajectories are generally unevenly distributed across a city and adding noise uniformly will generally lead to a poor utility. To achieve differential privacy, our algorithm adaptively adds noise to the input data according to the given query set. It first privately partitions the data space into uniform regions and computes the traffic density of each region. The regions and their densities, in addition to the given query set, are then used to estimate the distribution of trajectories over the queried space, which ensures high accuracy for the given query set. We show the accuracy and efficiency of our algorithm using extensive empirical evaluations on real and synthetic data sets.},
  archive      = {J_KIS},
  author       = {Ghane, Soheila and Kulik, Lars and Ramamoharao, Kotagiri},
  doi          = {10.1007/s10115-020-01520-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {277-303},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A differentially private algorithm for range queries on trajectories},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dealing with heterogeneity in the context of distributed
feature selection for classification. <em>KIS</em>, <em>63</em>(1),
233–276. (<a href="https://doi.org/10.1007/s10115-020-01526-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in the information technologies have greatly contributed to the advent of larger datasets. These datasets often come from distributed sites, but even so, their large size usually means they cannot be handled in a centralized manner. A possible solution to this problem is to distribute the data over several processors and combine the different results. We propose a methodology to distribute feature selection processes based on selecting relevant and discarding irrelevant features. This preprocessing step is essential for current high-dimensional sets, since it allows the input dimension to be reduced. We pay particular attention to the problem of data imbalance, which occurs because the original dataset is unbalanced or because the dataset becomes unbalanced after data partitioning. Most works approach unbalanced scenarios by oversampling, while our proposal tests both over- and undersampling strategies. Experimental results demonstrate that our distributed approach to classification obtains comparable accuracy results to a centralized approach, while reducing computational time and efficiently dealing with data imbalance.},
  archive      = {J_KIS},
  author       = {Morillo-Salas, José Luis and Bolón-Canedo, Verónica and Alonso-Betanzos, Amparo},
  doi          = {10.1007/s10115-020-01526-4},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {233-276},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dealing with heterogeneity in the context of distributed feature selection for classification},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Specifying and computing causes for query answers in
databases via database repairs and repair-programs. <em>KIS</em>,
<em>63</em>(1), 199–231. (<a
href="https://doi.org/10.1007/s10115-020-01516-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a recently established correspondence between database tuples as causes for query answers in databases and tuple-based repairs of inconsistent databases with respect to denial constraints. In this work, answer-set programs that specify database repairs are used as a basis for solving computational and reasoning problems around causality in databases, including causal responsibility. Furthermore, causes are introduced also at the attribute level by appealing to an attribute-based repair semantics that uses null values. Corresponding repair-programs are introduced, and used as a basis for computation and reasoning about attribute-level causes. The answer-set programs are extended in order to capture causality under integrity constraints.},
  archive      = {J_KIS},
  author       = {Bertossi, Leopoldo},
  doi          = {10.1007/s10115-020-01516-6},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {199-231},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Specifying and computing causes for query answers in databases via database repairs and repair-programs},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hashtag recommendation for short social media texts using
word-embeddings and external knowledge. <em>KIS</em>, <em>63</em>(1),
175–198. (<a href="https://doi.org/10.1007/s10115-020-01515-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of Twitter in recent years, there has been a tremendous increase in the number of tweets generated by users. Twitter allows users to make use of hashtags to facilitate effective categorization and retrieval of tweets. Despite the usefulness of hashtags, a major fraction of tweets do not contain hashtags. Several methods have been proposed to recommend hashtags based on lexical and topical features of tweets. However, semantic features and data sparsity in tweet representation have rarely been addressed by existing methods. In this paper, we propose a novel method for hashtag recommendation that resolves the data sparseness problem by exploiting the most relevant tweet information from external knowledge sources. In addition to lexical features and topical features, the proposed method incorporates the semantic features based on word-embeddings and user influence feature based on users’ influential position. To gain the advantage of various hashtag recommendation methods based on different features, our proposed method aggregates these methods using learning-to-rank and generates top-ranked hashtags. Experimental results show that the proposed method significantly outperforms the current state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Kumar, Nagendra and Baskaran, Eshwanth and Konjengbam, Anand and Singh, Manish},
  doi          = {10.1007/s10115-020-01515-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {175-198},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hashtag recommendation for short social media texts using word-embeddings and external knowledge},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale laplacian graph kernel combined with
lexico-syntactic patterns for biomedical event extraction from
literature. <em>KIS</em>, <em>63</em>(1), 143–173. (<a
href="https://doi.org/10.1007/s10115-020-01514-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bio-event extraction is an extensive research area in the field of biomedical text mining, this focuses on elaborating relationships between biomolecules and can provide various aspects of their nature. Bio-event extraction plays a vital role in biomedical literature mining applications such as biological network construction, pathway curation, and drug repurposing. Extracting biological events automatically is a difficult task because of the uncertainty and assortment of natural language processing such as negations and speculations, which provides further room for the development of feasible methodologies. This paper presents a hybrid approach that integrates an ensemble-learning framework by combining a Multiscale Laplacian Graph kernel and a feature-based linear kernel, using a pattern-matching engine to identify biomedical events with arguments. This graph-based kernel not only captures the topological relationships between the individual event nodes but also identifies the associations among the subgraphs for complex events. In addition, the lexico-syntactic patterns were used to automatically discover the semantic role of each word in the sentence. For performance evaluation, we used the gold standard corpora, namely BioNLP-ST (2009, 2011, and 2013) and GENIA-MK. Experimental results show that our approach achieved better performance than other state-of-the-art systems.},
  archive      = {J_KIS},
  author       = {Abdulkadhar, Sabenabanu and Bhasuran, Balu and Natarajan, Jeyakumar},
  doi          = {10.1007/s10115-020-01514-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {143-173},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multiscale laplacian graph kernel combined with lexico-syntactic patterns for biomedical event extraction from literature},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarially regularized medication recommendation model
with multi-hop memory network. <em>KIS</em>, <em>63</em>(1), 125–142.
(<a href="https://doi.org/10.1007/s10115-020-01513-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medication recommendation is attracting enormous attention due to its promise in effectively prescribing medicines and improving the survival rate of patients. Among all challenges, drug–drug interactions (DDI) related to undesired duplication, antagonism, or alternation between drugs could lead to fatal side effects. Previous researches usually provide models with DDI knowledge to achieve DDI reduction. However, the mixed use of patients with different DDI rates places stringent requirements on the generalization performance of models. In pursuit of a more effective method, we propose the adversarially regularized model for medication recommendation (ARMR). Specifically, ARMR firstly models temporal information from medical records to obtain patient representations and builds a key-value memory network based on information from historical admissions. Then, ARMR carries out multi-hop reading on the memory network to recommend medications. Meanwhile, ARMR uses a GAN model to adversarially regulate the distribution of patient representations by matching the distribution to a desired Gaussian distribution to achieve DDI reduction. Comparative evaluations between ARMR and baselines show that ARMR outperforms all baselines in terms of medication recommendation, achieving DDI reduction regardless of numbers of DDI types being considered.},
  archive      = {J_KIS},
  author       = {Wang, Yanda and Chen, Weitong and Pi, Dechang and Yue, Lin},
  doi          = {10.1007/s10115-020-01513-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {125-142},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adversarially regularized medication recommendation model with multi-hop memory network},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incremental one-class collaborative filtering with
co-evolving side networks. <em>KIS</em>, <em>63</em>(1), 105–124. (<a
href="https://doi.org/10.1007/s10115-020-01511-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class collaborative filtering (OCCF) is a fundamental research problem in a myriad of applications where the preferences of users can only be implicitly inferred from their one-class feedback (e.g., click an ad or purchase a product). The main challenges of OCCF lie in the sparsity of user feedback and the ambiguity of unobserved preferences. To effectively address the above two challenges, side networks from users and items are extensively exploited by state-of-the-art methods, which are predominantly focused on static settings. However, as real-world recommender systems evolve over time (where both the user–item ratings and user–user/item–item side networks will change), it is necessary to update OCCF results (e.g., the latent features of users and items) accordingly. The main obstacle for OCCF online update with co-evolving side networks lies in the fact that the coupled system is highly sensitive to local changes, which may cause massive perturbation on the latent features of a large number of users and items. In this paper, we propose a novel incremental one-class collaborative filtering (OCCF) method that can cope with co-evolving side networks efficiently. In particular, we model the evolution of latent features as a linear transformation process, which enables fast update of the latent features on the fly. Empirical experiments demonstrate that our method can provide high-quality recommendation results on real-world datasets.},
  archive      = {J_KIS},
  author       = {Chen, Chen and Xia, Yinglong and Zang, Hui and Li, Jundong and Liu, Huan and Tong, Hanghang},
  doi          = {10.1007/s10115-020-01511-x},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {105-124},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Incremental one-class collaborative filtering with co-evolving side networks},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transferring trading strategy knowledge to deep learning
models. <em>KIS</em>, <em>63</em>(1), 87–104. (<a
href="https://doi.org/10.1007/s10115-020-01510-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trading strategies are constantly being employed in the financial markets in order to increase consistency, reduce human errors of judgment and boost the probability of taking profitable market positions. In this work, we attempt to transfer the knowledge of several different types of trading strategies to deep learning models. The trading strategies are applied on price data of foreign exchange trading pairs and are actual strategies used in production trading environments. Along with our approach to transfer the strategy knowledge, we introduce a preprocessing method of the original price candles making it suitable for use with Neural Networks. Our results suggest that the deep models that are tested perform better than simpler models and they can accurately learn a variety of trading strategies.},
  archive      = {J_KIS},
  author       = {Tsantekidis, Avraam and Tefas, Anastasios},
  doi          = {10.1007/s10115-020-01510-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {87-104},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Transferring trading strategy knowledge to deep learning models},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A POI recommendation approach integrating social
spatio-temporal information into probabilistic matrix factorization.
<em>KIS</em>, <em>63</em>(1), 65–85. (<a
href="https://doi.org/10.1007/s10115-020-01509-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, point of interest (POI) recommendation has gained increasing attention all over the world. POI recommendation plays an indispensable role in assisting people to find places they are likely to enjoy. The exploitation of POIs recommendation by existing models is inadequate due to implicit correlations among users and POIs and cold start problem. To overcome these problems, this work proposed a social spatio-temporal probabilistic matrix factorization (SSTPMF) model that exploits POI similarity and user similarity, which integrates different spaces including the social space, geographical space and POI category space in similarity modelling. In other words, this model proposes a multivariable inference approach for POI recommendation using latent similarity factors. The results obtained from two real data sets, Foursquare and Gowalla, show that taking POI correlation and user similarity into account can further improve recommendation performance. In addition, the experimental results show that the SSTPMF model performs better in alleviating the cold start problem than state-of-the-art methods in terms of normalized discount cumulative gain on both data sets.},
  archive      = {J_KIS},
  author       = {Davtalab, Mehri and Alesheikh, Ali Asghar},
  doi          = {10.1007/s10115-020-01509-5},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {65-85},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A POI recommendation approach integrating social spatio-temporal information into probabilistic matrix factorization},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning cell embeddings for understanding table layouts.
<em>KIS</em>, <em>63</em>(1), 39–64. (<a
href="https://doi.org/10.1007/s10115-020-01508-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a large amount of data on the web in tabular form, such as Excel sheets, CSV files, and web tables. Often, tabular data is meant for human consumption, using data layouts that are difficult for machines to interpret automatically. Previous work uses the stylistic features of tabular cells (such as font size, border type, and background color) to classify tabular cells by their role in the data layout of the document (top attribute, data, metadata, etc.). In this paper, we propose a deep neural network model which can embed semantic and contextual information about tabular cells in a low-dimensional cell embedding space. We pre-train this cell embedding model on a large corpus of tabular documents from various domains. We then propose a classification technique based on recurrent neural networks (RNNs) to use our pre-trained cell embeddings, combining them with stylistic features introduced in previous work, in order to improve the performance of cell type classification in complex documents. We evaluate the performance of our system on three datasets containing documents with various data layouts, in two settings: in-domain and cross-domain training. Our evaluation result shows that our proposed cell vector representations in combination with our RNN-based classification technique significantly improve cell type classification performance.},
  archive      = {J_KIS},
  author       = {Ghasemi-Gol, Majid and Pujara, Jay and Szekely, Pedro},
  doi          = {10.1007/s10115-020-01508-6},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {39-64},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning cell embeddings for understanding table layouts},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometric consistency of triangular fuzzy multiplicative
preference relation and its application to group decision making.
<em>KIS</em>, <em>63</em>(1), 21–38. (<a
href="https://doi.org/10.1007/s10115-020-01507-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The triangular fuzzy multiplicative preference relation (TFMPR) has attracted the attention of many scholars. This paper investigates the geometric consistency of TFMPR and applies it to group decision making (GDM). Firstly, by introducing two parameters, a triangular fuzzy number is transformed into an interval. According to the geometric consistency of interval multiplicative preference relation (IMPR), the geometric consistency of TFMPR is defined. Then, two corresponding IMPRs are extracted from the TFMPR by programming models in the majority case and minority case, respectively. Using the constructed linear programming models, two interval priority weight vectors are obtained from the two extracted IMPRs, respectively. Combining two interval priority weight vectors, a linear programming model is established to derive the triangular fuzzy priority weights. Subsequently, the closeness degrees of alternatives by experts are defined to obtain the group utility indices and individual regret indices of alternatives. Then, the compromise indices of alternatives are calculated considering experts’ compromise attitude. By minimizing the compromise indices of alternatives, a multi-objective programming model is constructed to obtain experts’ weights. By aggregating the individual TFMPRs, the collective TFMPR is obtained to derive the triangular fuzzy priority weights. Using the arithmetic mean values, the ranking order of alternatives is generated. Therefore, a method is proposed to solve GDM with TFMPRs. Finally, a performance evaluation example of precise poverty alleviation is provided to illustrate the advantage of the proposed method.},
  archive      = {J_KIS},
  author       = {Wang, Feng},
  doi          = {10.1007/s10115-020-01507-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {21-38},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Geometric consistency of triangular fuzzy multiplicative preference relation and its application to group decision making},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E-recruitment recommender systems: A systematic review.
<em>KIS</em>, <em>63</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10115-020-01522-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender Systems (RS) are a subclass of information filtering systems that seek to predict the rating or preference a user would give to an item. e-Recruitment is one of the domains in which RS can contribute due to presenting a list of interesting jobs to a candidate or a list of candidates to a recruiter. This study presents an up-to-date systematic review of recommender systems applied to e-Recruitment considering only papers published from 2012 up to 2020. We searched three databases for published journal articles, conference papers and book chapters. We then evaluated these works in terms of which kinds of RS were applied for e-Recruitment, what kind of information was used in the e-Recruitment RS, and how they were assessed. A total of 896 papers were collected, out of which sixty three research works were included in the survey based on the inclusion and exclusion criteria adopted. We divided the recommender types into five categories (Content-Based Recommendation 26.98%; Collaborative Filtering 6.35%; Knowledge-Based Recommendation 12.7%; Hybrid approaches 20.63%; and Other Types 33.33%); the types of information used were divided into four categories (Social Network 38.1%; Resumés and Job Posts 42.85%; Behavior or Feedback 12.7%; and Others 6.35%), and the assessment types were categorized into four types (Expert Validation 20.83%; Machine Learning Metrics 41.67%; Challenge-specific Metrics 22.92%; and Utility measures 14.58%). Although in many cases a paper may belong to more than one category for each evaluation axis, we chose the most predominant one for our categorization. In addition, there is a clear trend for hybrid and non-traditional techniques to overcome the challenges of e-Recruitment domain.},
  archive      = {J_KIS},
  author       = {Freire, Mauricio Noris and de Castro, Leandro Nunes},
  doi          = {10.1007/s10115-020-01522-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {E-recruitment recommender systems: A systematic review},
  volume       = {63},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
