<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam---33">MAM - 33</h2>
<ul>
<li><details>
<summary>
(2021b). Correction to: AI, explainability and public reason: The
argument from the limitations of the human mind. <em>MAM</em>,
<em>31</em>(4), 637. (<a
href="https://doi.org/10.1007/s11023-021-09576-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Maclure, Jocelyn},
  doi          = {10.1007/s11023-021-09576-5},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {637},
  shortjournal = {Minds Mach.},
  title        = {Correction to: AI, explainability and public reason: the argument from the limitations of the human mind},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). (What) can deep learning contribute to theoretical
linguistics? <em>MAM</em>, <em>31</em>(4), 617–635. (<a
href="https://doi.org/10.1007/s11023-021-09571-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) techniques have revolutionised artificial systems’ performance on myriad tasks, from playing Go to medical diagnosis. Recent developments have extended such successes to natural language processing, an area once deemed beyond such systems’ reach. Despite their different goals (technological development vs. theoretical insight), these successes have suggested that such systems may be pertinent to theoretical linguistics. The competence/performance distinction presents a fundamental barrier to such inferences. While DL systems are trained on linguistic performance, linguistic theories are aimed at competence. Such a barrier has traditionally been sidestepped by assuming a fairly close correspondence: performance as competence plus noise. I argue this assumption is unmotivated. Competence and performance can differ arbitrarily. Thus, we should not expect DL models to illuminate linguistic theory.},
  archive      = {J_MAM},
  author       = {Dupre, Gabe},
  doi          = {10.1007/s11023-021-09571-w},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {617-635},
  shortjournal = {Minds Mach.},
  title        = {(What) can deep learning contribute to theoretical linguistics?},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot autonomy vs. Human autonomy: Social robots, artificial
intelligence (AI), and the nature of autonomy. <em>MAM</em>,
<em>31</em>(4), 595–616. (<a
href="https://doi.org/10.1007/s11023-021-09579-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social robots are robots that can interact socially with humans. As social robots and the artificial intelligence (AI) that powers them becomes more advanced, they will likely take on more social and work roles. This has many important ethical implications. In this paper, we focus on one of the most central of these, the impacts that social robots can have on human autonomy. We argue that, due to their physical presence and social capacities, there is a strong potential for social robots to enhance human autonomy as well as several ways they can inhibit and disrespect it. We argue that social robots could improve human autonomy by helping us to achieve more valuable ends, make more authentic choices, and improve our autonomy competencies. We also argue that social robots have the potential to harm human autonomy by instead leading us to achieve fewer valuable ends ourselves, make less authentic choices, decrease our autonomy competencies, make our autonomy more vulnerable, and disrespect our autonomy. Whether the impacts of social robots on human autonomy are positive or negative overall will depend on the design, regulation, and use we make of social robots in the future.},
  archive      = {J_MAM},
  author       = {Formosa, Paul},
  doi          = {10.1007/s11023-021-09579-2},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {595-616},
  shortjournal = {Minds Mach.},
  title        = {Robot autonomy vs. human autonomy: Social robots, artificial intelligence (AI), and the nature of autonomy},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linking human and machine behavior: A new approach to
evaluate training data quality for beneficial machine learning.
<em>MAM</em>, <em>31</em>(4), 563–593. (<a
href="https://doi.org/10.1007/s11023-021-09573-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine behavior that is based on learning algorithms can be significantly influenced by the exposure to data of different qualities. Up to now, those qualities are solely measured in technical terms, but not in ethical ones, despite the significant role of training and annotation data in supervised machine learning. This is the first study to fill this gap by describing new dimensions of data quality for supervised machine learning applications. Based on the rationale that different social and psychological backgrounds of individuals correlate in practice with different modes of human–computer-interaction, the paper describes from an ethical perspective how varying qualities of behavioral data that individuals leave behind while using digital technologies have socially relevant ramification for the development of machine learning applications. The specific objective of this study is to describe how training data can be selected according to ethical assessments of the behavior it originates from, establishing an innovative filter regime to transition from the big data rationale n = all to a more selective way of processing data for training sets in machine learning. The overarching aim of this research is to promote methods for achieving beneficial machine learning applications that could be widely useful for industry as well as academia.},
  archive      = {J_MAM},
  author       = {Hagendorff, Thilo},
  doi          = {10.1007/s11023-021-09573-8},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {563-593},
  shortjournal = {Minds Mach.},
  title        = {Linking human and machine behavior: A new approach to evaluate training data quality for beneficial machine learning},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trust me on this one: Conforming to conversational
assistants. <em>MAM</em>, <em>31</em>(4), 535–562. (<a
href="https://doi.org/10.1007/s11023-021-09581-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational artificial agents and artificially intelligent (AI) voice assistants are becoming increasingly popular. Digital virtual assistants such as Siri, or conversational devices such as Amazon Echo or Google Home are permeating everyday life, and are designed to be more and more humanlike in their speech. This study investigates the effect this can have on one’s conformity with an AI assistant. In the 1950s, Solomon Asch’s already demonstrated the power and danger of conformity amongst people. In these classical experiments test persons were asked to answer relatively simple questions, whilst others pretending to be participants tried to convince the test person to give wrong answers. These studies were later replicated with embodied robots, but these physical robots are still rare. In light of our increasing reliance on AI assistants, this study investigates to what extent an individual will conform to a disembodied virtual assistant. We also investigate if there is a difference between a group that interacts with an assistant that communicates through text, one that has a robotic voice and one that has a humanlike voice. The assistant attempts to subtly influence participants’ final responses in a general knowledge quiz, and we measure how often participants change their answer after having been given advice. Results show that participants conformed significantly more often to the assistant with a human voice than the one that communicated through text.},
  archive      = {J_MAM},
  author       = {Schreuter, Donna and van der Putten, Peter and Lamers, Maarten H.},
  doi          = {10.1007/s11023-021-09581-8},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {535-562},
  shortjournal = {Minds Mach.},
  title        = {Trust me on this one: Conforming to conversational assistants},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A pragmatic approach to the intentional stance semantic,
empirical and ethical considerations for the design of artificial
agents. <em>MAM</em>, <em>31</em>(4), 505–534. (<a
href="https://doi.org/10.1007/s11023-021-09567-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial agents are progressively becoming more present in everyday-life situations and more sophisticated in their interaction affordances. In some specific cases, like Google Duplex, GPT-3 bots or Deep Mind’s AlphaGo Zero, their capabilities reach or exceed human levels. The use contexts of everyday life necessitate making such agents understandable by laypeople. At the same time, displaying human levels of social behavior has kindled the debate over the adoption of Dennett’s ‘intentional stance’. By means of a comparative analysis of the literature on robots and virtual agents, we defend the thesis that approaching these artificial agents ‘as if’ they had intentions and forms of social, goal-oriented rationality is the only way to deal with their complexity on a daily base. Specifically, we claim that this is the only viable strategy for non-expert users to understand, predict and perhaps learn from artificial agents’ behavior in everyday social contexts. Furthermore, we argue that as long as agents are transparent about their design principles and functionality, attributing intentions to their actions is not only essential, but also ethical. Additionally, we propose design guidelines inspired by the debate over the adoption of the intentional stance.},
  archive      = {J_MAM},
  author       = {Papagni, Guglielmo and Koeszegi, Sabine},
  doi          = {10.1007/s11023-021-09567-6},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {505-534},
  shortjournal = {Minds Mach.},
  title        = {A pragmatic approach to the intentional stance semantic, empirical and ethical considerations for the design of artificial agents},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How robots’ unintentional metacommunication affects
human–robot interactions. A systemic approach. <em>MAM</em>,
<em>31</em>(4), 487–504. (<a
href="https://doi.org/10.1007/s11023-021-09584-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we theoretically address the relevance of unintentional and inconsistent interactional elements in human–robot interactions. We argue that elements failing, or poorly succeeding, to reproduce a humanlike interaction create significant consequences in human–robot relational patterns and may affect human–human relations. When considering social interactions as systems, the absence of a precise interactional element produces a general reshaping of the interactional pattern, eventually generating new types of interactional settings. As an instance of this dynamic, we study the absence of metacommunicative abilities in social artifacts. Then, we analyze the pragmatic consequences of the aforementioned absence through the lens of Paul Watzlawick’s interactionist theory. We suggest that a fixed complementary interactional setting may be produced because of the asymmetric understanding, between robots and humans, of metacommunication. We highlight the psychological implications of this interactional asymmetry within Jessica Benjamin’s concept of “mutual recognition”. Finally, we point out the possible shift of dysfunctional interactional patterns from human–robot interactions to human–human ones.},
  archive      = {J_MAM},
  author       = {Bisconti, Piercosma},
  doi          = {10.1007/s11023-021-09584-5},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {487-504},
  shortjournal = {Minds Mach.},
  title        = {How robots’ unintentional metacommunication affects Human–Robot interactions. a systemic approach},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Language and intelligence. <em>MAM</em>, <em>31</em>(4),
471–486. (<a href="https://doi.org/10.1007/s11023-021-09568-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores aspects of GPT-3 that have been discussed as harbingers of artificial general intelligence and, in particular, linguistic intelligence. After introducing key features of GPT-3 and assessing its performance in the light of the conversational standards set by Alan Turing in his seminal paper from 1950, the paper elucidates the difference between clever automation and genuine linguistic intelligence. A central theme of this discussion on genuine conversational intelligence is that members of a linguistic community never merely respond “algorithmically” to queries through a selective kind of pattern recognition, because they must also jointly attend and act with other speakers in order to count as genuinely intelligent and trustworthy. This presents a challenge for systems like GPT-3, because representing the world in a way that makes conversational common ground salient is an essentially collective task that we can only achieve jointly with other speakers. Thus, the main difficulty for any artificially intelligent model of conversation is to account for the communicational intentions and motivations of a speaker through joint attention. These joint motivations and intentions seem to be completely absent from the standard way in which systems like GPT-3 and other artificial intelligent systems work. This is not merely a theoretical issue. Since GPT-3 and future iterations of similar systems will likely be available for commercial use through application programming interfaces, caution is needed regarding the risks created by these systems, which pass for “intelligent” but have no genuine communicational intentions, and can thereby produce fake and unreliable linguistic exchanges.},
  archive      = {J_MAM},
  author       = {Montemayor, Carlos},
  doi          = {10.1007/s11023-021-09568-5},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {471-486},
  shortjournal = {Minds Mach.},
  title        = {Language and intelligence},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the special issue on “artificial speakers -
philosophical questions and implications.” <em>MAM</em>, <em>31</em>(4),
465–470. (<a href="https://doi.org/10.1007/s11023-021-09585-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Kempt, Hendrik and Bellon, Jacqueline and Nähr-Wagener, Sebastian},
  doi          = {10.1007/s11023-021-09585-4},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {465-470},
  shortjournal = {Minds Mach.},
  title        = {Introduction to the special issue on “Artificial speakers - philosophical questions and implications”},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to: Analysing the combined health, social and
economic impacts of the corona virus pandemic using agent‑based social
simulation. <em>MAM</em>, <em>31</em>(3), 463. (<a
href="https://doi.org/10.1007/s11023-021-09565-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Dignum, Frank and Dignum, Virginia and Davidsson, Paul and Ghorbani, Amineh and van der Hurk, Mijke and Jensen, Maarten and Kammler, Christian and Lorig, Fabian and Ludescher, Luis Gustavo and Melchior, Alexander and Mellema, René and Pastrav, Cezara and Vanhee, Loïs and Verhagen, Harko},
  doi          = {10.1007/s11023-021-09565-8},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {463},
  shortjournal = {Minds Mach.},
  title        = {Correction to: Analysing the combined health, social and economic impacts of the corona virus pandemic using Agent‑Based social simulation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empathy imitation game: Empathy turing test for care- and
chat-bots. <em>MAM</em>, <em>31</em>(3), 457–461. (<a
href="https://doi.org/10.1007/s11023-021-09555-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Howick, Jeremy and Morley, Jessica and Floridi, Luciano},
  doi          = {10.1007/s11023-021-09555-w},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {457-461},
  shortjournal = {Minds Mach.},
  title        = {An empathy imitation game: Empathy turing test for care- and chat-bots},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The treachery of images in the digital sovereignty debate.
<em>MAM</em>, <em>31</em>(3), 439–456. (<a
href="https://doi.org/10.1007/s11023-021-09566-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This short theoretical and argumentative essay contributes to the ongoing deliberation about the so-called digitalfug sovereignty, as pursued particularly in the European Union (EU). Drawing from classical political science literature, the essay approaches the debate through paradoxes that arise from applying classical notions of sovereignty to the digital domain. With these paradoxes and a focus on the Peace of Westphalia in 1648, the essay develops a viewpoint distinct from the conventional territorial notion of sovereignty. Accordingly, the lesson from Westphalia has more to do with the capacity of a state to govern. It is also this capacity that is argued to enable the sovereignty of individuals within the digital realm. With this viewpoint, the essay further advances another, broader, and more pressing debate on politics and democracy in the digital era.},
  archive      = {J_MAM},
  author       = {Ruohonen, Jukka},
  doi          = {10.1007/s11023-021-09566-7},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {439-456},
  shortjournal = {Minds Mach.},
  title        = {The treachery of images in the digital sovereignty debate},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). AI, explainability and public reason: The argument from the
limitations of the human mind. <em>MAM</em>, <em>31</em>(3), 421–438.
(<a href="https://doi.org/10.1007/s11023-021-09570-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based AI algorithms lack transparency. In this article, I offer an interpretation of AI’s explainability problem and highlight its ethical saliency. I try to make the case for the legal enforcement of a strong explainability requirement: human organizations which decide to automate decision-making should be legally obliged to demonstrate the capacity to explain and justify the algorithmic decisions that have an impact on the wellbeing, rights, and opportunities of those affected by the decisions. This legal duty can be derived from the demands of Rawlsian public reason. In the second part of the paper, I try to show that the argument from the limitations of human cognition fails to get AI off the hook of public reason. Against a growing trend in AI ethics, my main argument is that the analogy between human minds and artificial neural networks fails because it suffers from an atomistic bias which makes it blind to the social and institutional dimension of human reasoning processes. I suggest that developing interpretive AI algorithms is not the only possible answer to the explainability problem; social and institutional answers are also available and in many cases more trustworthy than techno-scientific ones.},
  archive      = {J_MAM},
  author       = {Maclure, Jocelyn},
  doi          = {10.1007/s11023-021-09570-x},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {421-438},
  shortjournal = {Minds Mach.},
  title        = {AI, explainability and public reason: The argument from the limitations of the human mind},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Value sensitive design to achieve the UN SDGs with AI: A
case of elderly care robots. <em>MAM</em>, <em>31</em>(3), 395–419. (<a
href="https://doi.org/10.1007/s11023-021-09561-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare is becoming increasingly automated with the development and deployment of care robots. There are many benefits to care robots but they also pose many challenging ethical issues. This paper takes care robots for the elderly as the subject of analysis, building on previous literature in the domain of the ethics and design of care robots. Using the value sensitive design (VSD) approach to technology design, this paper extends its application to care robots by integrating the values of care, values that are specific to AI, and higher-scale values such as the United Nations Sustainable Development Goals (SDGs). The ethical issues specific to care robots for the elderly are discussed at length alongside examples of specific design requirements that work to ameliorate these ethical concerns.},
  archive      = {J_MAM},
  author       = {Umbrello, Steven and Capasso, Marianna and Balistreri, Maurizio and Pirni, Alberto and Merenda, Federica},
  doi          = {10.1007/s11023-021-09561-y},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {395-419},
  shortjournal = {Minds Mach.},
  title        = {Value sensitive design to achieve the UN SDGs with AI: A case of elderly care robots},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). It’s friendship, jim, but not as we know it: A
degrees-of-friendship view of human–robot friendships. <em>MAM</em>,
<em>31</em>(3), 377–393. (<a
href="https://doi.org/10.1007/s11023-021-09560-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article argues in defence of human–robot friendship. I begin by outlining the standard Aristotelian view of friendship, according to which there are certain necessary conditions which x must meet in order to ‘be a friend’. I explain how the current literature typically uses this Aristotelian view to object to human–robot friendships on theoretical and ethical grounds. Theoretically, a robot cannot be our friend because it cannot meet the requisite necessary conditions for friendship. Ethically, human–robot friendships are wrong because they are deceptive (the robot does not actually meet the conditions for being a friend), and could also make it more likely that we will favour ‘perfect’ robots, and disrespect, exploit, or exclude other human beings. To argue against the above position, I begin by outlining and assessing current attempts to reject the theoretical argument—that we cannot befriend robots. I argue that the current attempts are problematic, and do little to support the claim that we can be friends with robots now (rather than in some future time). I then use the standard Aristotelian view as a touchstone to develop a new degrees-of-friendship view. On my view, it is theoretically possible for humans to have some degree of friendship with social robots now. I explain how my view avoids ethical concerns about human–robot friendships being deceptive, and/or leading to the disrespect, exploitation, or exclusion of other human beings.},
  archive      = {J_MAM},
  author       = {Ryland, Helen},
  doi          = {10.1007/s11023-021-09560-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {377-393},
  shortjournal = {Minds Mach.},
  title        = {It’s friendship, jim, but not as we know it: A degrees-of-friendship view of Human–Robot friendships},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What does it mean to empathise with a robot? <em>MAM</em>,
<em>31</em>(3), 361–376. (<a
href="https://doi.org/10.1007/s11023-021-09558-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given that empathy allows people to form and maintain satisfying social relationships with other subjects, it is no surprise that this is one of the most studied phenomena in the area of human–robot interaction (HRI). But the fact that the term ‘empathy’ has strong social connotations raises a question: can it be applied to robots? Can we actually use social terms and explanations in relation to these inanimate machines? In this article, I analyse the range of uses of the term empathy in the field of HRI studies and social robotics, and consider the substantial, functional and relational positions on this issue. I focus on the relational (cooperational) perspective presented by Luisa Damiano and Paul Dumouchel, who interpret emotions (together with empathy) as being the result of affective coordination. I also reflect on the criteria that should be used to determine when, in such relations, we are dealing with actual empathy.},
  archive      = {J_MAM},
  author       = {Malinowska, Joanna K.},
  doi          = {10.1007/s11023-021-09558-7},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {361-376},
  shortjournal = {Minds Mach.},
  title        = {What does it mean to empathise with a robot?},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Should we treat teddy bear 2.0 as a kantian dog? Four
arguments for the indirect moral standing of personal social robots,
with implications for thinking about animals and humans. <em>MAM</em>,
<em>31</em>(3), 337–360. (<a
href="https://doi.org/10.1007/s11023-020-09554-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of autonomous and intelligent personal social robots raises questions concerning their moral standing. Moving away from the discussion about direct moral standing and exploring the normative implications of a relational approach to moral standing, this paper offers four arguments that justify giving indirect moral standing to robots under specific conditions based on some of the ways humans—as social, feeling, playing, and doubting beings—relate to them. The analogy of “the Kantian dog” is used to assist reasoning about this. The paper also discusses the implications of this approach for thinking about the moral standing of animals and humans, showing why, when, and how an indirect approach can also be helpful in these fields, and using Levinas and Dewey as sources of inspiration to discuss some challenges raised by this approach.},
  archive      = {J_MAM},
  author       = {Coeckelbergh, Mark},
  doi          = {10.1007/s11023-020-09554-3},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {337-360},
  shortjournal = {Minds Mach.},
  title        = {Should we treat teddy bear 2.0 as a kantian dog? four arguments for the indirect moral standing of personal social robots, with implications for thinking about animals and humans},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The dawn of social robots: Anthropological and ethical
issues. <em>MAM</em>, <em>31</em>(3), 329–336. (<a
href="https://doi.org/10.1007/s11023-021-09572-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Gasser, Georg},
  doi          = {10.1007/s11023-021-09572-9},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {329-336},
  shortjournal = {Minds Mach.},
  title        = {The dawn of social robots: Anthropological and ethical issues},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ethics-based auditing to develop trustworthy AI.
<em>MAM</em>, <em>31</em>(2), 323–327. (<a
href="https://doi.org/10.1007/s11023-021-09557-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A series of recent developments points towards auditing as a promising mechanism to bridge the gap between principles and practice in AI ethics. Building on ongoing discussions concerning ethics-based auditing, we offer three contributions. First, we argue that ethics-based auditing can improve the quality of decision making, increase user satisfaction, unlock growth potential, enable law-making, and relieve human suffering. Second, we highlight current best practices to support the design and implementation of ethics-based auditing: To be feasible and effective, ethics-based auditing should take the form of a continuous and constructive process, approach ethical alignment from a system perspective, and be aligned with public policies and incentives for ethically desirable behaviour. Third, we identify and discuss the constraints associated with ethics-based auditing. Only by understanding and accounting for these constraints can ethics-based auditing facilitate ethical alignment of AI, while enabling society to reap the full economic and social benefits of automation.},
  archive      = {J_MAM},
  author       = {Mökander, Jakob and Floridi, Luciano},
  doi          = {10.1007/s11023-021-09557-8},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {323-327},
  shortjournal = {Minds Mach.},
  title        = {Ethics-based auditing to develop trustworthy AI},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do computers. <em>MAM</em>, <em>31</em>(2), 305–321. (<a
href="https://doi.org/10.1007/s11023-021-09564-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heyday of discussions initiated by Searle&#39;s claim that computers have syntax, but no semantics has now past, yet philosophers and scientists still tend to frame their views on artificial intelligence in terms of syntax and semantics. In this paper I do not intend to take part in these discussions; my aim is more fundamental, viz. to ask what claims about syntax and semantics in this context can mean in the first place. And I argue that their sense is so unclear that that their ability to act as markers within any disputes on artificial intelligence is severely compromised; and hence that their employment brings us nothing more than an illusion of explanation.},
  archive      = {J_MAM},
  author       = {Peregrin, Jaroslav},
  doi          = {10.1007/s11023-021-09564-9},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {305-321},
  shortjournal = {Minds Mach.},
  title        = {Do computers},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Are generative models structural representations?
<em>MAM</em>, <em>31</em>(2), 277–303. (<a
href="https://doi.org/10.1007/s11023-021-09559-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Philosophers interested in the theoretical consequences of predictive processing often assume that predictive processing is an inferentialist and representationalist theory of cognition. More specifically, they assume that predictive processing revolves around approximated Bayesian inferences drawn by inverting a generative model. Generative models, in turn, are said to be structural representations: representational vehicles that represent their targets by being structurally similar to them. Here, I challenge this assumption, claiming that, at present, it lacks an adequate justification. I examine the only argument offered to establish that generative models are structural representations, and argue that it does not substantiate the desired conclusion. Having so done, I consider a number of alternative arguments aimed at showing that the relevant structural similarity obtains, and argue that all these arguments are unconvincing for a variety of reasons. I then conclude the paper by briefly highlighting three themes that might be relevant for further investigation on the matter.},
  archive      = {J_MAM},
  author       = {Facchin, Marco},
  doi          = {10.1007/s11023-021-09559-6},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {277-303},
  shortjournal = {Minds Mach.},
  title        = {Are generative models structural representations?},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The missing ingredient in the case for regulating big tech.
<em>MAM</em>, <em>31</em>(2), 257–275. (<a
href="https://doi.org/10.1007/s11023-021-09562-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Having been involved in a slew of recent scandals, many of the world’s largest technology companies (“Big Tech,” “Digital Titans”) embarked on devising numerous codes of ethics, intended to promote improved standards in the conduct of their business. These efforts have attracted largely critical interdisciplinary academic attention. The critics have identified the voluntary character of the industry ethics codes as among the main obstacles to their efficacy. This is because individual industry leaders and employees, flawed human beings that they are, cannot be relied on voluntarily to conform with what justice demands, especially when faced with powerful incentives to pursue their own self-interest instead. Consequently, the critics have recommended a suite of laws and regulations to force the tech companies to better comply with the requirements of justice. At the same time, they have paid little attention to the possibility that individuals acting within the political context, e.g. as lawmakers and regulators, are also imperfect and need not be wholly compliant with what justice demands. This paper argues that such an omission is far from trivial. It creates a heavy argumentative burden on the part of the critics that they by and large fail to discharge. As a result, the case for Big Tech regulation that emerges from the recent literature has substantial lacunae and more work needs to be done before we can accept the critics’ calls for greater state involvement in the industry.},
  archive      = {J_MAM},
  author       = {Chomanski, Bartlomiej},
  doi          = {10.1007/s11023-021-09562-x},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {257-275},
  shortjournal = {Minds Mach.},
  title        = {The missing ingredient in the case for regulating big tech},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ethics as a service: A pragmatic operationalisation of AI
ethics. <em>MAM</em>, <em>31</em>(2), 239–256. (<a
href="https://doi.org/10.1007/s11023-021-09563-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the range of potential uses for Artificial Intelligence (AI), in particular machine learning (ML), has increased, so has awareness of the associated ethical issues. This increased awareness has led to the realisation that existing legislation and regulation provides insufficient protection to individuals, groups, society, and the environment from AI harms. In response to this realisation, there has been a proliferation of principle-based ethics codes, guidelines and frameworks. However, it has become increasingly clear that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems. In previous work, we analysed whether it is possible to close this gap between the ‘what’ and the ‘how’ of AI ethics through the use of tools and methods designed to help AI developers, engineers, and designers translate principles into practice. We concluded that this method of closure is currently ineffective as almost all existing translational tools and methods are either too flexible (and thus vulnerable to ethics washing) or too strict (unresponsive to context). This raised the question: if, even with technical guidance, AI ethics is challenging to embed in the process of algorithmic design, is the entire pro-ethical design endeavour rendered futile? And, if no, then how can AI ethics be made useful for AI practitioners? This is the question we seek to address here by exploring why principles and technical translational tools are still needed even if they are limited, and how these limitations can be potentially overcome by providing theoretical grounding of a concept that has been termed ‘Ethics as a Service.’},
  archive      = {J_MAM},
  author       = {Morley, Jessica and Elhalal, Anat and Garcia, Francesca and Kinsey, Libby and Mökander, Jakob and Floridi, Luciano},
  doi          = {10.1007/s11023-021-09563-w},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {239-256},
  shortjournal = {Minds Mach.},
  title        = {Ethics as a service: A pragmatic operationalisation of AI ethics},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computer says i don’t know: An empirical approach to capture
moral uncertainty in artificial intelligence. <em>MAM</em>,
<em>31</em>(2), 215–237. (<a
href="https://doi.org/10.1007/s11023-021-09556-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI Systems become increasingly autonomous, they are expected to engage in decision-making processes that have moral implications. In this research we integrate theoretical and empirical lines of thought to address the matters of moral reasoning and moral uncertainty in AI Systems. We reconceptualize the metanormative framework for decision-making under moral uncertainty and we operationalize it through a latent class choice model. The core idea being that moral heterogeneity in society can be codified in terms of a small number of classes with distinct moral preferences and that this codification can be used to express moral uncertainty of an AI. Choice analysis allows for the identification of classes and their moral preferences based on observed choice data. Our reformulation of the metanormative framework is theory-rooted and practical in the sense that it avoids runtime issues in real time applications. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. While one of the systems uses a baseline morally certain model, the other uses a morally uncertain model. We highlight cases in which the AI Systems disagree about the policy to be chosen, thus illustrating the need to capture moral uncertainty in AI systems.},
  archive      = {J_MAM},
  author       = {Martinho, Andreia and Kroesen, Maarten and Chorus, Caspar},
  doi          = {10.1007/s11023-021-09556-9},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {215-237},
  shortjournal = {Minds Mach.},
  title        = {Computer says i don’t know: An empirical approach to capture moral uncertainty in artificial intelligence},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correction to: Online identity crisis: Identity issues in
online communities. <em>MAM</em>, <em>31</em>(1), 213. (<a
href="https://doi.org/10.1007/s11023-020-09546-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An amendment to this paper has been published and can be accessed via the original article.},
  archive      = {J_MAM},
  author       = {Arfni, Selene and Parandera, Lorenzo Botta and Gazzaniga, Camilla and Maggioni, Nicolò and Tacchino, Alessandro},
  doi          = {10.1007/s11023-020-09546-3},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {213},
  shortjournal = {Minds Mach.},
  title        = {Correction to: online identity crisis: identity issues in online communities},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online identity crisis identity issues in online
communities. <em>MAM</em>, <em>31</em>(1), 193–212. (<a
href="https://doi.org/10.1007/s11023-020-09542-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How have online communities affected the ways their users construct, view, and define their identity? In this paper, we will approach this issue by considering two philosophical sets of problems related to personal identity: the “Characterization Question” and the “Self-Other Relations Question.” Since these queries have traditionally brought out different problems around the concept of identity, here we aim at rethinking them in the framework of online communities. To do so, we will adopt an externalist and cognitive point of view on online communities, describing them as virtual cognitive niches. We will evaluate and agree with the Attachment Theory of Identity, arguing that there is continuity between offline and online identity and that usually the latter contributes to the alteration of the former. Finally, we will discuss ways users can enact self-reflection on online frameworks, considering the impact of the Filter Bubble and the condition of Bad Faith.},
  archive      = {J_MAM},
  author       = {Arfini, Selene and Botta Parandera, Lorenzo and Gazzaniga, Camilla and Maggioni, Nicolò and Tacchino, Alessandro},
  doi          = {10.1007/s11023-020-09542-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {193-212},
  shortjournal = {Minds Mach.},
  title        = {Online identity crisis identity issues in online communities},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithmic fairness in mortgage lending: From absolute
conditions to relational trade-offs. <em>MAM</em>, <em>31</em>(1),
165–191. (<a href="https://doi.org/10.1007/s11023-020-09529-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the rising concern that algorithmic decision-making may reinforce discriminatory biases, researchers have proposed many notions of fairness and corresponding mathematical formalizations. Each of these notions is often presented as a one-size-fits-all, absolute condition; however, in reality, the practical and ethical trade-offs are unavoidable and more complex. We introduce a new approach that considers fairness—not as a binary, absolute mathematical condition—but rather, as a relational notion in comparison to alternative decisionmaking processes. Using US mortgage lending as an example use case, we discuss the ethical foundations of each definition of fairness and demonstrate that our proposed methodology more closely captures the ethical trade-offs of the decision-maker, as well as forcing a more explicit representation of which values and objectives are prioritised.},
  archive      = {J_MAM},
  author       = {Lee, Michelle Seng Ah and Floridi, Luciano},
  doi          = {10.1007/s11023-020-09529-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {165-191},
  shortjournal = {Minds Mach.},
  title        = {Algorithmic fairness in mortgage lending: From absolute conditions to relational trade-offs},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accountability and control over autonomous weapon systems: A
framework for comprehensive human oversight. <em>MAM</em>,
<em>31</em>(1), 137–163. (<a
href="https://doi.org/10.1007/s11023-020-09532-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accountability and responsibility are key concepts in the academic and societal debate on Autonomous Weapon Systems, but these notions are often used as high-level overarching constructs and are not operationalised to be useful in practice. “Meaningful Human Control” is often mentioned as a requirement for the deployment of Autonomous Weapon Systems, but a common definition of what this notion means in practice, and a clear understanding of its relation with responsibility and accountability is also lacking. In this paper, we present a definition of these concepts and describe the relations between accountability, responsibility, control and oversight in order to show how these notions are distinct but also connected. We focus on accountability as a particular form of responsibility—the obligation to explain one’s action to a forum—and we present three ways in which the introduction of Autonomous Weapon Systems may create “accountability gaps”. We propose a Framework for Comprehensive Human Oversight based on an engineering, socio-technical and governance perspective on control. Our main claim is that combining the control mechanisms at technical, socio-technical and governance levels will lead to comprehensive human oversight over Autonomous Weapon Systems which may ensure solid controllability and accountability for the behaviour of Autonomous Weapon Systems. Finally, we give an overview of the military control instruments that are currently used in the Netherlands and show the applicability of the comprehensive human oversight Framework to Autonomous Weapon Systems. Our analysis reveals two main gaps in the current control mechanisms as applied to Autonomous Weapon Systems. We have identified three first options as future work for the design of a control mechanism, one in the technological layer, one in the socio-technical layer and one the governance layer, in order to achieve comprehensive human oversight and ensure accountability over Autonomous Weapon Systems.},
  archive      = {J_MAM},
  author       = {Verdiesen, Ilse and Santoni de Sio, Filippo and Dignum, Virginia},
  doi          = {10.1007/s11023-020-09532-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {137-163},
  shortjournal = {Minds Mach.},
  title        = {Accountability and control over autonomous weapon systems: A framework for comprehensive human oversight},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthetic deliberation: Can emulated imagination enhance
machine ethics? <em>MAM</em>, <em>31</em>(1), 121–136. (<a
href="https://doi.org/10.1007/s11023-020-09531-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is becoming increasingly entwined with our daily lives: AIs work as assistants through our phones, control our vehicles, and navigate our vacuums. As these objects become more complex and work within our societies in ways that affect our well-being, there is a growing demand for machine ethics—we want a guarantee that the various automata in our lives will behave in a way that minimizes the amount of harm they create. Though many technologies exist as moral artifacts (and perhaps moral agents), the development of a truly ethical AI system is highly contentious; theorists have proposed and critiqued countless possibilities for programming these agents to become ethical. Many of these arguments, however, presuppose the possibility that an artificially intelligent system can actually be ethical. In this essay, I will explore a potential path to AI ethics by considering the role of imagination in the deliberative process via the work of John Dewey and his interpreters, showcasing one form of reinforcement learning that mimics imaginative deliberation. With these components in place, I contend that such an artificial agent is capable of something very near ethical behavior—close enough that we may consider it so.},
  archive      = {J_MAM},
  author       = {Pinka, Robert},
  doi          = {10.1007/s11023-020-09531-w},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {121-136},
  shortjournal = {Minds Mach.},
  title        = {Synthetic deliberation: Can emulated imagination enhance machine ethics?},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information and diagrammatic reasoning: An inferentialist
reading. <em>MAM</em>, <em>31</em>(1), 99–120. (<a
href="https://doi.org/10.1007/s11023-020-09547-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current philosophy of information, different authors have been supporting the veridicality thesis (VT). According to this thesis, an epistemically-oriented concept of information must have truth as one of its necessary conditions. Two challenges can be raised against VT. First, some philosophers object that veridicalists erroneously ignore the informativeness of false messages. Secondly, it is not clear whether VT can adequately explain the information considered in hypothetical reasoning. In this sense, logical diagrams offer an interesting case of analysis: by manipulating a logical diagram we can verify that a certain conclusion follows from a set of premises, but it cannot help us to determine the actual truth-value of a given set of propositions. Focusing on the latter challenge, in this paper I claim that logical diagrams set out potential counterexamples to VT and, consequently, pose a real challenge to this thesis. First, a veridicalist analysis of logical diagrams requires the assumption of metatheoretical properties which are not satisfied by some logical systems (and, consequently, are not satisfied by some systems of logical diagrams). So, VT does not fit well as a general framework for a theory of logical diagrammatic information. Secondly, based on semantic inferentialism, one can propose a normative interpretation of the inferential content of logical diagrams not exposed to the problems faced by VT. Moreover, there are several reasons to believe that veridicalism cannot accommodate such a normative interpretation. In other words, normativism represents a real (though still underexplored) alternative to veridicality. Due to these reasons, I conclude that, until further research, we should adopt a more parsimonious standpoint and say that logical diagrams provide inferential information simply.},
  archive      = {J_MAM},
  author       = {Mendonça, Bruno Ramos},
  doi          = {10.1007/s11023-020-09547-2},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {99-120},
  shortjournal = {Minds Mach.},
  title        = {Information and diagrammatic reasoning: An inferentialist reading},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Descriptive complexity, computational tractability, and the
logical and cognitive foundations of mathematics. <em>MAM</em>,
<em>31</em>(1), 75–98. (<a
href="https://doi.org/10.1007/s11023-020-09545-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computational complexity theory, decision problems are divided into complexity classes based on the amount of computational resources it takes for algorithms to solve them. In theoretical computer science, it is commonly accepted that only functions for solving problems in the complexity class P, solvable by a deterministic Turing machine in polynomial time, are considered to be tractable. In cognitive science and philosophy, this tractability result has been used to argue that only functions in P can feasibly work as computational models of human cognitive capacities. One interesting area of computational complexity theory is descriptive complexity, which connects the expressive strength of systems of logic with the computational complexity classes. In descriptive complexity theory, it is established that only first-order (classical) systems are connected to P, or one of its subclasses. Consequently, second-order systems of logic are considered to be computationally intractable, and may therefore seem to be unfit to model human cognitive capacities. This would be problematic when we think of the role of logic as the foundations of mathematics. In order to express many important mathematical concepts and systematically prove theorems involving them, we need to have a system of logic stronger than classical first-order logic. But if such a system is considered to be intractable, it means that the logical foundation of mathematics can be prohibitively complex for human cognition. In this paper I will argue, however, that this problem is the result of an unjustified direct use of computational complexity classes in cognitive modelling. Placing my account in the recent literature on the topic, I argue that the problem can be solved by considering computational complexity for humanly relevant problem solving algorithms and input sizes.},
  archive      = {J_MAM},
  author       = {Pantsar, Markus},
  doi          = {10.1007/s11023-020-09545-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {75-98},
  shortjournal = {Minds Mach.},
  title        = {Descriptive complexity, computational tractability, and the logical and cognitive foundations of mathematics},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The machine scenario: A computational perspective on
alternative representations of indeterminism. <em>MAM</em>,
<em>31</em>(1), 59–74. (<a
href="https://doi.org/10.1007/s11023-020-09530-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In philosophical logic and metaphysics there is a long-standing debate around the most appropriate structures to represent indeterministic scenarios concerning the future. We reconstruct here such a debate in a computational setting, focusing on the fundamental difference between moment-based and history-based structures. Our presentation is centered around two versions of an indeterministic scenario in which a programmer wants a machine to perform a given task at some point after a specified time. One of the two versions includes an assumption about the future behaviour of the machine that cannot be encoded in any programming instruction; such version has models over history-based structures but no model over a moment-based structure. Therefore, our work adds a new stance to the debate: moment-based structures can be said to rule out certain indeterministic scenarios that are computationally unfeasible.},
  archive      = {J_MAM},
  author       = {Grandjean, Vincent and Pascucci, Matteo},
  doi          = {10.1007/s11023-020-09530-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {59-74},
  shortjournal = {Minds Mach.},
  title        = {The machine scenario: A computational perspective on alternative representations of indeterminism},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The computational origin of representation. <em>MAM</em>,
<em>31</em>(1), 1–58. (<a
href="https://doi.org/10.1007/s11023-020-09540-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations—whatever they are—must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantification, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an “assembly language” for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems.},
  archive      = {J_MAM},
  author       = {Piantadosi, Steven T.},
  doi          = {10.1007/s11023-020-09540-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-58},
  shortjournal = {Minds Mach.},
  title        = {The computational origin of representation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
