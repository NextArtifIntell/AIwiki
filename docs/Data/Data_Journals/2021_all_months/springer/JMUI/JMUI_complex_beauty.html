<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui---33">JMUI - 33</h2>
<ul>
<li><details>
<summary>
(2021). Facial expression and action unit recognition augmented by
their dependencies on graph convolutional networks. <em>JMUI</em>,
<em>15</em>(4), 429–440. (<a
href="https://doi.org/10.1007/s12193-020-00363-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human facial expressions is one of the key steps towards achieving human–computer interaction. Owing to the anatomic mechanism that governs facial muscular interactions, there exist powerful dependencies between expressions and action units (AUs) that are useful for exploiting such rules of knowledge to guide the model learning process. However, they have not yet been represented directly and integrated into a network. In this study, we propose a novel method for facial expressions and AUs recognition based on their dependencies on graph convolutional network. First, we train the conditional generative adversarial network to filter out identity information and extract expression information through a de-expression learning procedure. Thereafter, we apply graph convolutional network to represent dependency laying among AU nodes and embed the nodes by dividing the expression component into multi patches, corresponding to the AU-related regions. Finally, we use prior knowledge matrices to represent the dependencies between expressions and AUs and subsequently integrate them into a loss function to constrain the model. The results of our experiments indicate that such representation is effective for improving the recognition rate. They also reveal that our work achieves better performance than several popular approaches.},
  archive      = {J_JMUI},
  author       = {He, Jun and Yu, Xiaocui and Sun, Bo and Yu, Lejun},
  doi          = {10.1007/s12193-020-00363-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {429-440},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Facial expression and action unit recognition augmented by their dependencies on graph convolutional networks},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An audiovisual interface-based drumming system for
multimodal human–robot interaction. <em>JMUI</em>, <em>15</em>(4),
413–428. (<a href="https://doi.org/10.1007/s12193-020-00352-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a study of an audiovisual interface-based drumming system for multimodal human–robot interaction. The interactive multimodal drumming game is used in conjunction with humanoid robots to establish an audiovisual interactive interface. This study is part of a project to design robot and avatar assistants for education and therapy, especially for children with special needs. It specifically focuses on evaluating robot/virtual avatar tutors, tangible interaction devices, and mobile multimedia devices within a simple drumming-based interactive music tutoring game scenario. Several parameters, including the effect of the embodiment of the tutor/interface and the presence of feedback and training mechanisms, were the focus of interest. For that purpose, we created an interactive drumming game relying on turn-taking and imitation principles, in which a human user is able to play drums with a humanoid robot (Robotic Drum Mate). Three interactive scenarios with different experimental setups for humanoid robots and mobile devices were developed and tested. As a part of those scenarios, a system that enables drum strokes to be automatically detected and recognized using auditory cues was implemented and incorporated into the experimental framework. We verified the applicability and effectiveness of the proposed system in a drum-playing game with adult human test subjects by evaluating it both objectively and subjectively. The results showed that the physical robot tutor, the feedback and training mechanisms had a positive effect on the subjects’ performance and, as expected, although the physical medium is preferred, the virtual medium for drumming caused less failure.},
  archive      = {J_JMUI},
  author       = {Ince, Gökhan and Yorganci, Rabia and Ozkul, Ahmet and Duman, Taha Berkay and Köse, Hatice},
  doi          = {10.1007/s12193-020-00352-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {413-428},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {An audiovisual interface-based drumming system for multimodal human–robot interaction},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developing a scenario-based video game generation framework
for computer and virtual reality environments: A comparative usability
study. <em>JMUI</em>, <em>15</em>(4), 393–411. (<a
href="https://doi.org/10.1007/s12193-020-00348-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serious games—games that have additional purposes rather than only entertainment—aim to educate people, solve, and plan several real-life tasks and circumstances in an interactive, efficient, and user-friendly way. Emergency training and planning provide structured curricula, rule-based action items, and interdisciplinary collaborative entities to imitate and teach real-life tasks. This rule-based structure enables the curricula to be transferred into other systematic learning platforms. Although emergency training includes these highly structured and repetitive action responses, a general framework to map the training scenarios’ actions, roles, and collaborative structures to serious games’ game mechanics and game dialogues, is still not available. To address this issue, in this study, a scenario-based game generator, which maps domain-oriented tasks to game rules and game mechanics, was developed. Also, two serious games (i.e., Hospital game and BioGarden game) addressing the training mechanisms of Chemical, Biological, Radiological, Nuclear, and Explosives (CBRNe) domain, were developed by both the game developers and the scenario-based game generator for comparative analysis. Finally, the outcomes of these games were mapped to the virtual reality environment to provide a thorough training program. To test the usability, immersion, presence, and technology acceptance aspects of the proposed game generator’s outcomes, 15 game developer participants tested a complete set of games and answered the questionnaires of the corresponding phenomenon. The results show that although the game generator has higher CPU time and memory usage, it highly outperforms the game development pipeline performance of the game developers and provides usable and immersive games. Thus, this study provides a promising game generator which bridges the CBRNe practitioners and game developers to transform real-life training scenarios into video games efficiently and quickly.},
  archive      = {J_JMUI},
  author       = {Surer, Elif and Erkayaoğlu, Mustafa and Öztürk, Zeynep Nur and Yücel, Furkan and Bıyık, Emin Alp and Altan, Burak and Şenderin, Büşra and Oğuz, Zeliha and Gürer, Servet and Düzgün, H. Şebnem},
  doi          = {10.1007/s12193-020-00348-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {393-411},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Developing a scenario-based video game generation framework for computer and virtual reality environments: A comparative usability study},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MUMBAI: Multi-person, multimodal board game affect and
interaction analysis dataset. <em>JMUI</em>, <em>15</em>(4), 373–391.
(<a href="https://doi.org/10.1007/s12193-021-00364-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Board games are fertile grounds for the display of social signals, and they provide insights into psychological indicators in multi-person interactions. In this work, we introduce a new dataset collected from four-player board game sessions, recorded via multiple cameras, and containing over 46 hours of visual material. The new MUMBAI dataset is extensively annotated with emotional moments for all game sessions. Additional data comes from personality and game experience questionnaires. Our four-person setup allows the investigation of non-verbal interactions beyond dyadic settings. We present three benchmarks for expression detection and emotion classification and discuss potential research questions for the analysis of social interactions and group dynamics during board games.},
  archive      = {J_JMUI},
  author       = {Doyran, Metehan and Schimmel, Arjan and Baki, Pınar and Ergin, Kübra and Türkmen, Batıkan and Salah, Almıla Akdağ and Bakkes, Sander C. J. and Kaya, Heysem and Poppe, Ronald and Salah, Albert Ali},
  doi          = {10.1007/s12193-021-00364-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {373-391},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {MUMBAI: Multi-person, multimodal board game affect and interaction analysis dataset},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PLAAN: Pain level assessment with anomaly-detection based
network. <em>JMUI</em>, <em>15</em>(4), 359–372. (<a
href="https://doi.org/10.1007/s12193-020-00362-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic chronic pain assessment and pain intensity estimation has been attracting growing attention due to its widespread applications. One of the prevalent issues in automatic pain analysis is inadequate balanced expert-labelled data for pain estimation. This work proposes an anomaly detection based network addressing one of the existing limitations of automatic pain assessment. The evaluation of the network is performed on pain intensity estimation and protective behaviour estimation tasks from body movements in the EmoPain Challenge dataset. The EmoPain dataset consists of body part based sensor data for both the tasks. The proposed network, PLAAN (Pain Level Assessment with Anomaly-detection based Network), is a lightweight LSTM-DNN network which considers features based on sensor data as the input and predicts intensity level of pain and presence or absence of protective behaviour in chronic low back pain patients. Joint training considering body movement patterns, such as exercise type, corresponding to pain exhibition as a label improves the performance of the network. However, contrary to perception, protective behaviour rather exists sporadically alongside pain in the EmoPain dataset. This induces yet another complication in accurate estimation of protective behaviour. This problem is resolved by incorporating anomaly detection in the network. A detailed comparison of different networks with varied features is outlined in the paper, presenting a significant improvement with the final proposed anomaly detection based network.},
  archive      = {J_JMUI},
  author       = {Li, Yi and Ghosh, Shreya and Joshi, Jyoti},
  doi          = {10.1007/s12193-020-00362-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {359-372},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {PLAAN: Pain level assessment with anomaly-detection based network},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal analysis of personality traits on videos of
self-presentation and induced behavior. <em>JMUI</em>, <em>15</em>(4),
337–358. (<a href="https://doi.org/10.1007/s12193-020-00347-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality analysis is an important area of research in several fields, including psychology, psychiatry, and neuroscience. With the recent dramatic improvements in machine learning, it has also become a popular research area in computer science. While the current computational methods are able to interpret behavioral cues (e.g., facial expressions, gesture, and voice) to estimate the level of (apparent) personality traits, accessible assessment tools are still substandard for practical use, not to mention the need for fast and accurate methods for such analyses. In this study, we present multimodal deep architectures to estimate the Big Five personality traits from (temporal) audio-visual cues and transcribed speech. Furthermore, for a detailed analysis of personality traits, we have collected a new audio-visual dataset, namely: Self-presentation and Induced Behavior Archive for Personality Analysis (SIAP). In contrast to the available datasets, SIAP introduces recordings of induced behavior in addition to self-presentation (speech) videos. With thorough experiments on SIAP and ChaLearn LAP First Impressions datasets, we systematically assess the reliability of different behavioral modalities and their combined use. Furthermore, we investigate the characteristics and discriminative power of induced behavior for personality analysis, showing that the induced behavior indeed includes signs of personality traits.},
  archive      = {J_JMUI},
  author       = {Giritlioğlu, Dersu and Mandira, Burak and Yilmaz, Selim Firat and Ertenli, Can Ufuk and Akgür, Berhan Faruk and Kınıklıoğlu, Merve and Kurt, Aslı Gül and Mutlu, Emre and Gürel, Şeref Can and Dibeklioğlu, Hamdi},
  doi          = {10.1007/s12193-020-00347-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {337-358},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Multimodal analysis of personality traits on videos of self-presentation and induced behavior},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Behavior and usability analysis for multimodal user
interfaces. <em>JMUI</em>, <em>15</em>(4), 335–336. (<a
href="https://doi.org/10.1007/s12193-021-00372-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal interfaces offer ever-changing tasks and challenges for designers to accommodate newer technologies, and as these technologies become more accessible, newer application scenarios emerge. Prototype development and user evaluation are important steps in the creation of solutions to these challenges. Furthermore, playful interactions and games are shown to be important settings to study social signals of interaction people. Research in multimodal analysis brings together people with diverse skills and specializations on the integration of tools in different modalities, to collect and annotate data, and to exchange ideas and skills, and this special issue is a reflection of that collective effort.},
  archive      = {J_JMUI},
  author       = {Dibeklioğlu, Hamdi and Surer, Elif and Salah, Albert Ali and Dutoit, Thierry},
  doi          = {10.1007/s12193-021-00372-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {12},
  number       = {4},
  pages        = {335-336},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Behavior and usability analysis for multimodal user interfaces},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying and evaluating conceptual representations for
auditory-enhanced interactive physics simulations. <em>JMUI</em>,
<em>15</em>(3), 323–334. (<a
href="https://doi.org/10.1007/s12193-021-00365-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive simulations are tools that can help students understand and learn about complex relationships. While most simulations are primarily visual due to mostly historical reasons, sounds can be used to add to the experience. In this work, we evaluated sets of audio designs for two different, but contextually- and visually-similar simulations. We identified key aspects of the audio representations and the simulation content which needed to be evaluated, and compared designs across two simulations to understand which auditory designs could generalize to other simulations. To compare the designs and explore how audio affected a user’s experience, we measured preference (through usability, user experience, and open-ended questions) and interpretation accuracy for different aspects of the simulation (including the main relationships and control feedback). We suggest important characteristics to represent through audio for future simulations, provide sound design suggestions, and address how overlap between visual and audio representations can support learning opportunities.},
  archive      = {J_JMUI},
  author       = {Tomlinson, Brianna J. and Walker, Bruce N. and Moore, Emily B.},
  doi          = {10.1007/s12193-021-00365-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {323-334},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Identifying and evaluating conceptual representations for auditory-enhanced interactive physics simulations},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neighborhood based decision theoretic rough set under
dynamic granulation for BCI motor imagery classification. <em>JMUI</em>,
<em>15</em>(3), 301–321. (<a
href="https://doi.org/10.1007/s12193-020-00358-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain Computer Interface is an interesting and important research field that has contributed widespread application systems. In the medical field, it is important for physically challenged persons to aid in rehabilitation and restoration. In Brain Computer Interface, computer acts as interface between brain signals and external device. The computer processes the brain signals and sends necessary instructions to external device. The external device helps in restoring the movement ability of patient. Motor imagery is the imagination of motor movements like hand, foot and tongue. There is an associated brain signal when the normal person moves their hand, foot and tongue. Similarly, there is an associated brain signal when the physically challenged person imagines moving their hand, foot and tongue. When this brain signal is analyzed by brain computer interface, it can facilitate motor movements through external device. The aim of this work is to analyze and classify the brain signals for motor movements to aid in rehabilitation and restoration. In this paper BCI Competition IV Dataset I, Dataset IIa, BCI Competition III Dataset IIIa and Neuroprosthetic EEG Dataset are analyzed A novel optimization technique with Neighborhood Decision Theoretic Rough Set under Dynamic Granulation is proposed for motor imagery classification. Neighborhood based Decision Theoretic Rough Set under Dynamic Granulation (NDTRS under DG) is hybrid approach combining two algorithms Neighborhood Rough Set and Decision Theoretic Rough Set under Dynamic Granulation ((DTRS under DG). Neighborhood Rough Set overcomes the drawback of discretization step in Rough Set. Decision Theoretic Rough Set under Dynamic Granulation algorithm has loss function for classification. The effectiveness of classification is improved since the loss function is involved in the construction of algorithm. The proposed method Neighborhood based Decision Theoretic Rough Set under Dynamic Granulation gives higher classification accuracy compared to existing approaches.},
  archive      = {J_JMUI},
  author       = {Renuga Devi, K. and Hannah Inbarani, H.},
  doi          = {10.1007/s12193-020-00358-4},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {301-321},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Neighborhood based decision theoretic rough set under dynamic granulation for BCI motor imagery classification},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Circus in motion: A multimodal exergame supporting
vestibular therapy for children with autism. <em>JMUI</em>,
<em>15</em>(3), 283–299. (<a
href="https://doi.org/10.1007/s12193-020-00345-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exergames are serious games that involve physical exertion and are thought of as a form of exercise by using novel input models. Exergames are promising in improving the vestibular differences of children with autism but often lack of adaptation mechanisms that adjust the difficulty level of the exergame. In this paper, we present the design and development of Circus in Motion, a multimodal exergame supporting children with autism with the practice of non-locomotor movements. We describe how the data from a 3D depth camera enables the tracking of non-locomotor movements allowing children to naturally interact with the exergame . A controlled experiment with 12 children with autism shows Circus in Motion excels traditional vestibular therapies in increasing physical activation and the number of movements repetitions. We show how data from real-time usage of Circus in Motion could be used to feed a fuzzy logic model that can adjust the difficulty level of the exergame according to each childs motor performance. We close discussing open challenges and opportunities of multimodal exergames to support motor therapeutic interventions for children with autism in the long-term.},
  archive      = {J_JMUI},
  author       = {Peña, Oscar and Cibrian, Franceli L. and Tentori, Monica},
  doi          = {10.1007/s12193-020-00345-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {283-299},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Circus in motion: A multimodal exergame supporting vestibular therapy for children with autism},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A BCI video game using neurofeedback improves the attention
of children with autism. <em>JMUI</em>, <em>15</em>(3), 273–281. (<a
href="https://doi.org/10.1007/s12193-020-00339-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major usability and technical challenges have created mistrust of the potential of brain computer interfaces used to control video games in challenging environments like healthcare. Despite several studies showing low cost commercial headsets can read the brainwave patterns of its users with great potential for long term adoption; there are limited studies showing its efficacy in concrete healthcare scenarios. In our past work, we developed FarmerKeeper, a BCI using users’ attention to control a runner videogame to support neurofeedback therapies with great usability and user experience. In this paper, beyond usability, we describe the results of a 10-week deployment study with 26 children with severe autism using FarmerKeeper as a tool to support the neurofeedback therapies of children with autism. Pre- and post-assessment evaluation indicate all children with autism improve their attention, attentional control and sustained attention. Two children with autism no longer showed attention impairments in the post-assessment evaluation. We closed discussing directions for future work and the potential benefits of this new generation of BCI videogames in healthcare scenarios.},
  archive      = {J_JMUI},
  author       = {Mercado, Jose and Escobedo, Lizbeth and Tentori, Monica},
  doi          = {10.1007/s12193-020-00339-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {273-281},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A BCI video game using neurofeedback improves the attention of children with autism},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FNIRS-based classification of mind-wandering with
personalized window selection for multimodal learning interfaces.
<em>JMUI</em>, <em>15</em>(3), 257–272. (<a
href="https://doi.org/10.1007/s12193-020-00325-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of an individual’s mind-wandering state has implications for designing and evaluating engaging and effective learning interfaces. While it is difficult to differentiate whether an individual is mind-wandering or focusing on the task only based on externally observable behavior, brain-based sensing offers unique insights to internal states. To explore the feasibility, we conducted a study using functional near-infrared spectroscopy (fNIRS) and investigated machine learning classifiers to detect mind-wandering episodes based on fNIRS data, both on an individual level and a group level, specifically focusing on automated window selection to improve classification results. For individual-level classification, by using a moving window method combined with a linear discriminant classifier, we found the best windows for classification and achieved a mean F1-score of 74.8%. For group-level classification, we proposed an individual-based time window selection (ITWS) algorithm to incorporate individual differences in window selection. The algorithm first finds the best window for each individual by using embedded individual-level classifiers and then uses these windows from all participants to build the final classifier. The performance of the ITWS algorithm is evaluated when used with eXtreme gradient boosting, convolutional neural networks, and deep neural networks. Our results show that the proposed algorithm achieved significant improvement compared to the previous state of the art in terms of brain-based classification of mind-wandering, with an average F1-score of 73.2%. This builds a foundation for mind-wandering detection for both the evaluation of multimodal learning interfaces and for future attention-aware systems.},
  archive      = {J_JMUI},
  author       = {Liu, Ruixue and Walker, Erin and Friedman, Leah and Arrington, Catherine M. and Solovey, Erin T.},
  doi          = {10.1007/s12193-020-00325-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {257-272},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {FNIRS-based classification of mind-wandering with personalized window selection for multimodal learning interfaces},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advanced multimodal interaction techniques and user
interfaces for serious games and virtual environments. <em>JMUI</em>,
<em>15</em>(3), 255–256. (<a
href="https://doi.org/10.1007/s12193-021-00380-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JMUI},
  author       = {Liarokapis, Fotis and von Mammen, Sebastian and Vourvopoulos, Athanasios},
  doi          = {10.1007/s12193-021-00380-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {255-256},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Advanced multimodal interaction techniques and user interfaces for serious games and virtual environments},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grounding behaviours with conversational interfaces: Effects
of embodiment and failures. <em>JMUI</em>, <em>15</em>(2), 239–254. (<a
href="https://doi.org/10.1007/s12193-021-00366-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational interfaces that interact with humans need to continuously establish, maintain and repair common ground in task-oriented dialogues. Uncertainty, repairs and acknowledgements are expressed in user behaviour in the continuous efforts of the conversational partners to maintain mutual understanding. Users change their behaviour when interacting with systems in different forms of embodiment, which affects the abilities of these interfaces to observe users’ recurrent social signals. Additionally, humans are intellectually biased towards social activity when facing anthropomorphic agents or when presented with subtle social cues. Two studies are presented in this paper examining how humans interact in a referential communication task with wizarded interfaces in different forms of embodiment. In study 1 (N = 30), we test whether humans respond the same way to agents, in different forms of embodiment and social behaviour. In study 2 (N = 44), we replicate the same task and agents but introduce conversational failures disrupting the process of grounding. Findings indicate that it is not always favourable for agents to be anthropomorphised or to communicate with non-verbal cues, as human grounding behaviours change when embodiment and failures are manipulated.},
  archive      = {J_JMUI},
  author       = {Kontogiorgos, Dimosthenis and Pereira, Andre and Gustafson, Joakim},
  doi          = {10.1007/s12193-021-00366-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {239-254},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Grounding behaviours with conversational interfaces: Effects of embodiment and failures},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-native speaker perception of intelligent virtual agents
in two languages: The impact of amount and type of grammatical mistakes.
<em>JMUI</em>, <em>15</em>(2), 229–238. (<a
href="https://doi.org/10.1007/s12193-021-00369-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Having a mixed-cultural membership becomes increasingly common in our modern society. It is thus beneficial in several ways to create Intelligent Virtual Agents (IVAs) that reflect a mixed-cultural background as well, e.g., for educational settings. For research with such IVAs, it is essential that they are classified as non-native by members of a target culture. In this paper, we focus on variations of IVAs’ speech to create the impression of non-native speakers that are identified as such by speakers of two different mother tongues. In particular, we investigate grammatical mistakes and identify thresholds beyond which the agents is clearly categorised as a non-native speaker. Therefore, we conducted two experiments: one for native speakers of German, and one for native speakers of English. Results of the German study indicate that beyond 10% of word order mistakes and 25% of infinitive mistakes German-speaking IVAs are perceived as non-native speakers. Results of the English study indicate that beyond 50% of omission mistakes and 50% of infinitive mistakes English-speaking IVAs are perceived as non-native speakers. We believe these thresholds constitute helpful guidelines for computational approaches of non-native speaker generation, simplifying research with IVAs in mixed-cultural settings.},
  archive      = {J_JMUI},
  author       = {Obremski, David and Lugrin, Jean-Luc and Schaper, Philipp and Lugrin, Birgit},
  doi          = {10.1007/s12193-021-00369-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {229-238},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Non-native speaker perception of intelligent virtual agents in two languages: The impact of amount and type of grammatical mistakes},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An expert-model and machine learning hybrid approach to
predicting human-agent negotiation outcomes in varied data.
<em>JMUI</em>, <em>15</em>(2), 215–227. (<a
href="https://doi.org/10.1007/s12193-021-00368-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the results of a machine-learning approach to the analysis of several human-agent negotiation studies. By combining expert knowledge of negotiating behavior compiled over a series of empirical studies with neural networks, we show that a hybrid approach to parameter selection yields promise for designing more effective and socially intelligent agents. Specifically, we show that a deep feedforward neural network using a theory-driven three-parameter model can be effective in predicting negotiation outcomes. Furthermore, it outperforms other expert-designed models that use more parameters, as well as those using other techniques (such as linear regression models or boosted decision trees). In a follow-up study, we show that the most successful models change as the dataset size increases and the prediction targets change, and show that boosted decision trees may not be suitable for the negotiation domain. We anticipate these results will have impact for those seeking to combine extensive domain knowledge with more automated approaches in human-computer negotiation. Further, we show that this approach can be a stepping stone from purely exploratory research to targeted human-behavioral experimentation. Through our approach, areas of social artificial intelligence that have historically benefited from expert knowledge and traditional AI approaches can be combined with more recent proven-effective machine learning algorithms.},
  archive      = {J_JMUI},
  author       = {Mell, Johnathan and Beissinger, Markus and Gratch, Jonathan},
  doi          = {10.1007/s12193-021-00368-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {215-227},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {An expert-model and machine learning hybrid approach to predicting human-agent negotiation outcomes in varied data},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing mind perception in strategic exchanges:
Human-agent negotiation, dictator and ultimatum games. <em>JMUI</em>,
<em>15</em>(2), 201–214. (<a
href="https://doi.org/10.1007/s12193-020-00356-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research shows that how we respond to other social actors depends on what sort of mind we ascribe to them. In a comparative manner, we observed how perceived minds of agents shape people’s behavior in the dictator game, ultimatum game, and negotiation against artificial agents. To do so, we varied agents’ minds on two dimensions of the mind perception theory: agency (cognitive aptitude) and patiency (affective aptitude) via descriptions and dialogs. In our first study, agents with emotional capacity garnered more allocations in the dictator game, but in the ultimatum game, agents’ described agency and affective capacity, both led to greater offers. In the second study on negotiation, agents ascribed with low-agency traits earned more points than those with high-agency traits, though the negotiation tactic was the same for all agents. Although patiency did not impact game points, participants sent more happy and surprise emojis and emotionally valenced messages to agents that demonstrated emotional capacity during negotiations. Further, our exploratory analyses indicate that people related only to agents with perceived affective aptitude across all games. Both perceived agency and affective capacity contributed to moral standing after dictator and ultimatum games. But after negotiations, only agents with perceived affective capacity were granted moral standing. Manipulating mind dimensions of machines has differing effects on how people react to them in dictator and ultimatum games, compared to a more complex economic exchange like negotiation. We discuss these results, which show that agents are perceived not only as social actors, but as intentional actors through negotiations, in contrast with simple economic games.},
  archive      = {J_JMUI},
  author       = {Lee, Minha and Lucas, Gale and Gratch, Jonathan},
  doi          = {10.1007/s12193-020-00356-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {201-214},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Comparing mind perception in strategic exchanges: Human-agent negotiation, dictator and ultimatum games},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Verbal empathy and explanation to encourage behaviour change
intention. <em>JMUI</em>, <em>15</em>(2), 189–199. (<a
href="https://doi.org/10.1007/s12193-020-00359-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the role of therapist-patient relationship in fostering behaviour change, agent-human relationship has been an active research area. This trusted relationship could be a result of the agent’s behavioural cues or the content it delivers that shows its knowledge. However, the impact of the resulting relationship using the various strategies on behaviour change is understudied. In this paper, we investigate the role of two strategies (empathic and social dialogue and explanation) in building agent-user rapport and whether the level of behaviour change intentions are due to the use of empathy or to trusting the agent’s understanding and recommendations through explanation. Hence, we designed two versions of a virtual advisor, empathic and neutral, to reduce study stress among university students and measured students’ rapport levels and intentions to change their behaviour. Some recommended behaviours had explanations based on the user’s beliefs. Our results showed that the agent could build a trusting relationship with the user with the help of the explanation regardless of the level of rapport. The results further showed that nearly all of the recommendations provided by the agent highly significantly increased the intention of the user to change their behavior related to these recommendations. However, we also found that it is important for the agent to obtain and reason about the user’s intentions concerning the specific behaviour before recommending a certain behavior change.},
  archive      = {J_JMUI},
  author       = {Abdulrahman, Amal and Richards, Deborah and Ranjbartabar, Hedieh and Mascarenhas, Samuel},
  doi          = {10.1007/s12193-020-00359-3},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {189-199},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Verbal empathy and explanation to encourage behaviour change intention},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel focus encoding scheme for addressee detection in
multiparty interaction using machine learning algorithms. <em>JMUI</em>,
<em>15</em>(2), 175–188. (<a
href="https://doi.org/10.1007/s12193-020-00361-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressee detection is a fundamental task for seamless dialogue management and turn taking in human-agent interaction. Though addressee detection is implicit in dyadic interaction, it becomes a challenging task when more than two participants are involved. This article proposes multiple addressee detection models based on smart feature selection and focus encoding schemes. The models are trained using different machine learning and deep learning algorithms. This research work improves existing baseline accuracies for addressee prediction on two datasets. In addition, the article explores the impact of different focus encoding schemes in several addressee detection cases. Finally, an implementation strategy for addressee detection model in real-time is discussed.},
  archive      = {J_JMUI},
  author       = {Malik, Usman and Barange, Mukesh and Saunier, Julien and Pauchet, Alexandre},
  doi          = {10.1007/s12193-020-00361-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {175-188},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {A novel focus encoding scheme for addressee detection in multiparty interaction using machine learning algorithms},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Does an agent’s touch always matter? Study on virtual midas
touch, masculinity, social status, and compliance in polish men.
<em>JMUI</em>, <em>15</em>(2), 163–174. (<a
href="https://doi.org/10.1007/s12193-020-00351-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional gender roles that define what is feminine and masculine also imply that men have higher social status than women. These stereotypes still influence how people interact with each other and with computers. Touch behaviour, essential in social interactions, is an interesting example of such social behaviours. The Midas touch effect describes a situation when a brief touch is used to influence one’s behaviour. Our study aimed to analyse the influence of virtual touch on compliance in men in a decision-making game called Ultimatum. In a series of three studies, we investigated whether social cues such as gender, stereotypical masculine/feminine appearance, and high/low social status modify compliance to offers from embodied agents. We built an immersive version of a repeated Ultimatum game in which a proposer offers how to split ten coins, and a responder accepts or rejects the offer. In study 1, men and women played with a female and a male agent. In study 2 and 3, men played with four agents each, differing in gender and levels of stereotypically seen masculinity and social status. There was no significant touch effect. Compliance was secured mostly by the value of the offer: the more generous the offer, the higher the compliance rate. We also found evidence for the perceived masculinity and social status influence. We also describe relationships between agents’ characteristics and the perception of their touch. The results are discussed in the context of social characteristics that are important in agent design and the effectiveness of social influence techniques in virtual reality.},
  archive      = {J_JMUI},
  author       = {Świdrak, Justyna and Pochwatko, Grzegorz and Insabato, Andrea},
  doi          = {10.1007/s12193-020-00351-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {163-174},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Does an agent’s touch always matter? study on virtual midas touch, masculinity, social status, and compliance in polish men},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Internet-based tailored virtual human health intervention to
promote colorectal cancer screening: Design guidelines from two user
studies. <em>JMUI</em>, <em>15</em>(2), 147–162. (<a
href="https://doi.org/10.1007/s12193-020-00357-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To influence user behaviors, Internet-based virtual humans (VH) have been used to deliver health interventions. When developing Internet-based VH health interventions, the developers have to make several design decisions on VH’s appearance, role, language, or medium. The design decisions can affect the outcomes of the Internet-based VH health intervention. To help make design decisions, the current paper presents design guidelines drawn from two studies. The two studies used Internet-based VH health intervention to promote colorectal cancer (CRC) screening. The two studies examined the influence of visual design and the influence of the information medium on user intentions to pursue more health information. In the first study, the qualitative analysis of the focus group (n = 73 users in 13 focus groups) transcripts shows that the VH’s visual realism, the VH’s healthcare role, and the presence of a local healthcare provider’s logo influenced the user perceptions of the intervention. The findings from the focus groups were used to iterate the intervention and derive design guidelines. In the second study (n = 1400), the analysis of online surveys from users after the VH-based intervention showed that to positively influence the user intentions to pursue the health topic further, the results recommend the use of an animated VH to deliver health information compared to other mediums of information delivery, such as text. The analysis also shows that very few user comments were related to the VH’s appearance after visual design iterations in the second study. The design guidelines from the two studies can be used by developers when using VH-based interventions to positively influence users’ intention to change behaviors.},
  archive      = {J_JMUI},
  author       = {Zalake, Mohan and Tavassoli, Fatemeh and Duke, Kyle and George, Thomas and Modave, Francois and Neil, Jordan and Krieger, Janice and Lok, Benjamin},
  doi          = {10.1007/s12193-020-00357-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {147-162},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Internet-based tailored virtual human health intervention to promote colorectal cancer screening: Design guidelines from two user studies},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual agents as supporting media for scientific
presentations. <em>JMUI</em>, <em>15</em>(2), 131–146. (<a
href="https://doi.org/10.1007/s12193-020-00350-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of scientific oral presentations is often poor, owing to a number of factors, including public speaking anxiety. We present DynamicDuo, a system that uses an automated, life-sized, animated agent to help inexperienced scientists deliver their presentations in front of an audience. The design of the system was informed by an analysis of TED talks given by pairs of human presenters to identify the most common dual-presentation formats and transition behaviors used. We explore the usability and acceptability of DynamicDuo in both controlled laboratory-based studies and real-world environments, and its ability to decrease public speaking anxiety and improve presentation quality. In a within-subjects study (N = 12) comparing co-presenting with DynamicDuo against solo-presenting with conventional presentation software, we demonstrated that our system led to significant improvements in public speaking anxiety and speaking confidence for non-native English speakers. Judges who viewed videotapes of these presentations rated those with DynamicDuo significantly higher on speech quality and overall presentation quality for all presenters. We also explore the affordances of the virtual co-presenter through empirical evaluation of novel roles the agent can play in scientific presentations and novel ways it can interact with the speaker in front of the audience.},
  archive      = {J_JMUI},
  author       = {Bickmore, Timothy and Kimani, Everlyne and Shamekhi, Ameneh and Murali, Prasanth and Parmar, Dhaval and Trinh, Ha},
  doi          = {10.1007/s12193-020-00350-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {131-146},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Virtual agents as supporting media for scientific presentations},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of avatar and voice transform in programming
e-learning lectures. <em>JMUI</em>, <em>15</em>(2), 121–129. (<a
href="https://doi.org/10.1007/s12193-020-00349-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reports the effectiveness of high frame rate facial animated avatar and voice transformer in eLearning. Three avatars: (real male professor, male avatar, female avatar) were combined with male professor’s voice or VT-4 vocoder transformed voice to create six distinguished videos which were then viewed by university freshmen students. A total of 186 students divided into 15 groups participated in this experiment. Female avatar was the most appealing avatar visually, but its combination with voice transform severely hinders its overall score. This research can be extended to real time live evaluation measuring preferences of students and draw more connections between student perception of avatar and actual lecturers.},
  archive      = {J_JMUI},
  author       = {Hsieh, Rex and Sato, Hisashi},
  doi          = {10.1007/s12193-020-00349-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {121-129},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Evaluation of avatar and voice transform in programming e-learning lectures},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Empirical evaluation and pathway modeling of visual
attention to virtual humans in an appearance fidelity continuum.
<em>JMUI</em>, <em>15</em>(2), 109–119. (<a
href="https://doi.org/10.1007/s12193-020-00341-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this contribution we studied how different rendering styles of a virtual human impacted users’ visual attention in an interactive medical training simulator. In a mixed design experiment, 78 participants interacted with a virtual human representing a sample from the non-photorealistic (NPR) to the photorealistic (PR) rendering continuity. We presented five rendering style samples scenarios, namely All Pencil Shaded (APS), Pencil Shaded (PS), All Cartoon Shaded (ACT), Cartoon Shaded (CT), and Human-Like (HL), and compared how visual attention differed between groups of users. For this study, we employed an eye tracking system for collecting and analyzing users’ gaze during interaction with the virtual human in a failure to rescue medical training simulation. Results shows that users spent more total time in the APS and ACT conditions but users visually attended more to virtual humans in the PS, CT and HL appearance conditions.},
  archive      = {J_JMUI},
  author       = {Volonte, Matias and Anaraky, Reza Ghaiumy and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Knijnenburg, Bart P. and Duchowski, Andrew T. and Babu, Sabarish V.},
  doi          = {10.1007/s12193-020-00341-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {109-119},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guidelines for the design of a virtual patient for
psychiatric interview training. <em>JMUI</em>, <em>15</em>(2), 99–107.
(<a href="https://doi.org/10.1007/s12193-020-00338-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A psychiatric diagnosis involves the physician’s ability to create an empathic interaction with the patient in order to accurately extract symptomatology (i.e., clinical manifestations). Virtual patients (VPs) can be used to train these skills but need to propose a structured and multimodal interaction situation, in order to simulate a realistic psychiatric interview. In this study we present a simulated psychiatric interview with a virtual patient suffering from major depressive disorders. We suggested some design guidelines based on psychiatry theories and medicine education standards. We evaluated our VP with user testing with 35 4th year medical students, and probed their opinion during debriefing interviews. All students showed good abilities to communicate empathetically with the VP, and managed to extract symptomatology from VP’s simulation. Students provided positive feedbacks regarding pedagogic usefulness, realism and enjoyment in the interaction, which suggests that our design guidelines are consistent and that such technologies are acceptable to medical students. To conclude this study is the first to simulate a realistic psychiatric interview and to measure both skills needed by future psychiatrists: symptomatology extraction and empathic communication. Results provide evidence for the use of VPs to complement existing tools and to train and evaluate healthcare professionals in the future.},
  archive      = {J_JMUI},
  author       = {Dupuy, Lucile and de Sevin, Etienne and Cassoudesalle, Hélène and Ballot, Orlane and Dehail, Patrick and Aouizerate, Bruno and Cuny, Emmanuel and Micoulaud-Franchi, Jean-Arthur and Philip, Pierre},
  doi          = {10.1007/s12193-020-00338-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {99-107},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Guidelines for the design of a virtual patient for psychiatric interview training},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). “Let me explain!”: Exploring the potential of virtual agents
in explainable AI interaction design. <em>JMUI</em>, <em>15</em>(2),
87–98. (<a href="https://doi.org/10.1007/s12193-020-00332-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In this paper, we explore the effects of incorporating virtual agents into explainable artificial intelligence (XAI) designs on the perceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for keyword classification. As a result of this experiment, we found that the integration of virtual agents leads to increased user trust in the XAI system. Furthermore, we found that the user’s trust significantly depends on the modalities that are used within the user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined with a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the participants’ feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction with the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design. Based on these results, we discuss current limitations and interesting topics for further research in the field of XAI. Moreover, we present design recommendations for virtual agents in XAI systems for future projects.},
  archive      = {J_JMUI},
  author       = {Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and André, Elisabeth},
  doi          = {10.1007/s12193-020-00332-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {87-98},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {“Let me explain!”: Exploring the potential of virtual agents in explainable AI interaction design},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: The augmented movement platform for embodied
learning (AMPEL): Development and reliability. <em>JMUI</em>,
<em>15</em>(1), 85. (<a
href="https://doi.org/10.1007/s12193-020-00360-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There was an error in the affiliations of the co-authors Dr. Thomas Vervust and Prof. Peter Feys. Their correct affiliations are given in this correction},
  archive      = {J_JMUI},
  author       = {Moumdjian, Lousin and Vervust, Thomas and Six, Joren and Schepers, Ivan and Lesaffre, Micheline and Feys, Peter and Leman, Marc},
  doi          = {10.1007/s12193-020-00360-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {85},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Correction to: the augmented movement platform for embodied learning (AMPEL): development and reliability},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). The augmented movement platform for embodied learning
(AMPEL): Development and reliability. <em>JMUI</em>, <em>15</em>(1),
77–83. (<a href="https://doi.org/10.1007/s12193-020-00354-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balance and gait impairments are highly prevalent in the neurological population. Although current rehabilitation strategies focus on motor learning principles, it is of interest to expand into embodied sensori-motor learning; that is learning through a continuous interaction between cognitive and motor systems, within an enriched sensory environment. Current developments in engineering allow for the development of enriched sensory environments through interactive feedback. The Augmented Movement Platform for Embodied Learning (AMPEL) was developed, both in terms of hardware and software by an inter-disciplinary circular participatory design strategy. The developed device was then tested for in-between session reliability for the outcome measures inter-step interval and total onset time. Ten healthy participants walked in four experimental paths on the device in two different sessions, and between session correlations were calculated. AMPEL was developed both in terms of software and hardware, with three Plug-In systems (auditory, visual, auditory + visual). The auditory Plug-In allows for flexible application of augmented feedback. The in-between session reliability of the outcomes measured by the system were between high and very high on all 4 walked paths, tested on ten healthy participants [mean age 41.8 ± 18.5; BMI 24.8 ± 6.1]. AMPEL shows full functionality, and has shown between session reliability for the measures of inter-step-intervals and total-onset-time in healthy controls during walking on different paths.},
  archive      = {J_JMUI},
  author       = {Moumdjian, Lousin and Vervust, Thomas and Six, Joren and Schepers, Ivan and Lesaffre, Micheline and Feys, Peter and Leman, Marc},
  doi          = {10.1007/s12193-020-00354-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {77-83},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The augmented movement platform for embodied learning (AMPEL): Development and reliability},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Words of encouragement: How praise delivered by a social
robot changes children’s mindset for learning. <em>JMUI</em>,
<em>15</em>(1), 61–76. (<a
href="https://doi.org/10.1007/s12193-020-00353-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a longitudinal study in which children could interact unsupervised and at their own initiative with a fully autonomous computer aided learning (CAL) system situated in their classroom. The focus of this study was to investigate how the mindset of children is affected when delivering effort-related praise through a social robot. We deployed two versions: a CAL system that delivered praise through headphones only, and an otherwise identical CAL system that was extended with a social robot to deliver the praise. A total of 44 children interacted repeatedly with the CAL system in two consecutive learning tasks over the course of approximately four months. Overall, the results show that the participating children experienced a significant change in mindset. The effort-related praise that was delivered by a social robot seemed to have had a positive effect on children’s mindset, compared to the regular CAL system where we did not see a significant effect.},
  archive      = {J_JMUI},
  author       = {Davison, Daniel P. and Wijnen, Frances M. and Charisi, Vicky and van der Meij, Jan and Reidsma, Dennis and Evers, Vanessa},
  doi          = {10.1007/s12193-020-00353-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {61-76},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Words of encouragement: How praise delivered by a social robot changes children’s mindset for learning},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring crossmodal perceptual enhancement and integration
in a sequence-reproducing task with cognitive priming. <em>JMUI</em>,
<em>15</em>(1), 45–59. (<a
href="https://doi.org/10.1007/s12193-020-00326-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crossmodal correspondence, a perceptual phenomenon which has been extensively studied in cognitive science, has been shown to play a critical role in people’s information processing performance. However, the evidence has been collected mostly based on strictly-controlled stimuli and displayed in a noise-free environment. In real-world interaction scenarios, background noise may blur crossmodal effects that designers intend to leverage. More seriously, it may induce additional crossmodal effects, which can be mutually exclusive to the intended one, leading to unexpected distractions from the task at hand. In this paper, we report two experiments designed to tackle these problems with cognitive priming techniques. The first experiment examined how to enhance the perception of specific crossmodal stimuli, namely pitch–brightness and pitch–elevation stimuli. The second experiment investigated how people perceive and respond to crossmodal stimuli that were mutually exclusive. Results showed that first, people’s crossmodal perception was affected by cognitive priming, though the effect varies according to the combination of crossmodal stimuli and the types of priming material. Second, when two crossmodal stimuli are mutually exclusive, priming on only the dominant one (Pitch–elevation) lead to improved performance. These results can help inform future design of multisensory systems by presenting details of how to enhance crossmodal information with cognitive priming.},
  archive      = {J_JMUI},
  author       = {Feng, Feng and Li, Puhong and Stockman, Tony},
  doi          = {10.1007/s12193-020-00326-y},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {45-59},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Exploring crossmodal perceptual enhancement and integration in a sequence-reproducing task with cognitive priming},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual intimacy in human-embodied conversational agent
interactions: The influence of multimodality on its perception.
<em>JMUI</em>, <em>15</em>(1), 25–43. (<a
href="https://doi.org/10.1007/s12193-020-00337-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interacting with an embodied conversational agent (ECA) in a professional context addresses social considerations to satisfy customer-relationship. This paper presents an experimental study about the perception of virtual intimacy in human-ECA interactions. We explore how an ECA’s multimodal communication affects our perception of virtual intimacy. To this end, we developed a virtual Tourism Information counselor capable of exhibiting verbal and nonverbal intimate behaviors according to several modalities (voice, chatbox, both media), and we built a corpus of videos showing interactions between the agent and a human tourist. We interrogated observers about their perception of the agent’s level of intimacy. Our results confirm the human ability to perceive intimacy in an ECA displaying multimodal behaviors, although the contribution of nonverbal communication remains unclear. Our study suggests that using voice channel increases the perception of virtual intimacy and offers further evidence that human-inspired design of ECAs is needed. Finally, we demonstrate that intimate cues do not disturb the comprehension of task-related information and are valuable for an attentional focus on the agent’s animation. We discuss the concept of virtual intimacy in relation to interpersonal intimacy, and we question its perception in terms of attentional mechanisms.},
  archive      = {J_JMUI},
  author       = {Potdevin, Delphine and Clavel, Céline and Sabouret, Nicolas},
  doi          = {10.1007/s12193-020-00337-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {25-43},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Virtual intimacy in human-embodied conversational agent interactions: The influence of multimodality on its perception},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving robot’s perception of uncertain spatial
descriptors in navigational instructions by evaluating influential
gesture notions. <em>JMUI</em>, <em>15</em>(1), 11–24. (<a
href="https://doi.org/10.1007/s12193-020-00328-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-friendly interactive features are preferred for service robots used in emerging areas of robotic applications such as caretaking, health care, assistance, education and entertainment since they are intended to be operated by non-expert users. Humans prefer to use voice instructions, responses, and suggestions in their daily interactions. Such voice instructions and responses often include uncertain spatial descriptors such as “little” and “far”, which have no definitive quantitative meaning. Service robots involve direct interactions with human users through voice communication. Therefore, the ability to effectively quantify the meaning of such uncertain spatial descriptors is necessary for human-friendly service robots. This paper proposes a novel method to quantify the uncertain spatial descriptors in navigational instructions based on the current environmental setting and the influential notions conveyed by the pointing gestures that accompany voice instructions. The uncertain spatial descriptors are quantified by a fuzzy inference system that evaluates the spatial parameters of the current environment and the influential notions conveyed by pointing gestures, if available. According to the obtained experimental results, the proposed method is capable of improving the quantification ability of uncertain spatial descriptors by robots.},
  archive      = {J_JMUI},
  author       = {Muthugala, M. A. Viraj J. and Srimal, P. H. D. Arjuna S. and Jayasekara, A. G. Buddhika P.},
  doi          = {10.1007/s12193-020-00328-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {11-24},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Improving robot’s perception of uncertain spatial descriptors in navigational instructions by evaluating influential gesture notions},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal, visuo-haptic games for abstract theory
instruction: Grabbing charged particles. <em>JMUI</em>, <em>15</em>(1),
1–10. (<a href="https://doi.org/10.1007/s12193-020-00327-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extensive metamorphosis is currently taking place in the education industry due to the rapid adoption of different technologies and the proliferation of new student-instructor and student–student interaction models. While traditional face-to-face interaction is still the norm, mobile, online and virtual augmentations are increasingly adopted worldwide. Moreover, with the advent of gaming technology besides the 3D visual paradigm, the “touch” and “feel” paradigm is slowly taking its place in the user interface design through gamification. While haptic (force feedback) devices were barely available a decade ago outside research laboratories, the rapid rise in gaming technology has driven the cost significantly lower enabling the spread of these devices in many households and the wide public. This article presents a novel haptic-based training tool implemented as a gaming scenario to assist students in learning of abstract concepts in Physics. The focus is on electromagnetism as one of the fundamental forces in nature and specifically the abstractions used as building blocks around the Lorentz force. Experimental results suggest that by introducing well designed visual-haptic interfaces in presenting abstract concepts, students become better engaged in the classrooms and superior learning outcomes can be achieved.},
  archive      = {J_JMUI},
  author       = {Hamza-Lup, Felix G. and Goldbach, Ioana R.},
  doi          = {10.1007/s12193-020-00327-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Multimodal, visuo-haptic games for abstract theory instruction: Grabbing charged particles},
  volume       = {15},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
