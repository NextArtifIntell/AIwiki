<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml---102">ML - 102</h2>
<ul>
<li><details>
<summary>
(2021). Loss aware post-training quantization. <em>ML</em>,
<em>110</em>(11), 3245–3262. (<a
href="https://doi.org/10.1007/s10994-021-06053-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network quantization enables the deployment of large models on resource-constrained devices. Current post-training quantization methods fall short in terms of accuracy for INT4 (or lower) but provide reasonable accuracy for INT8 (or above). In this work, we study the effect of quantization on the structure of the loss landscape. We show that the structure is flat and separable for mild quantization, enabling straightforward post-training quantization methods to achieve good results. We show that with more aggressive quantization, the loss landscape becomes highly non-separable with steep curvature, making the selection of quantization parameters more challenging. Armed with this understanding, we design a method that quantizes the layer parameters jointly, enabling significant accuracy improvement over current post-training quantization methods. Reference implementation is available at https://github.com/ynahshan/nn-quantization-pytorch/tree/master/lapq .},
  archive      = {J_ML},
  author       = {Nahshan, Yury and Chmiel, Brian and Baskin, Chaim and Zheltonozhskii, Evgenii and Banner, Ron and Bronstein, Alex M. and Mendelson, Avi},
  doi          = {10.1007/s10994-021-06053-z},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3245-3262},
  shortjournal = {Mach. Learn.},
  title        = {Loss aware post-training quantization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HIVE-COTE 2.0: A new meta ensemble for time series
classification. <em>ML</em>, <em>110</em>(11), 3211–3243. (<a
href="https://doi.org/10.1007/s10994-021-06057-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. HIVE-COTE forms its ensemble from classifiers of multiple domains, including phase-independent shapelets, bag-of-words based dictionaries and phase-dependent intervals. Since it was first proposed in 2016, the algorithm has remained state of the art for accuracy on the UCR time series classification archive. Over time it has been incrementally updated, culminating in its current state, HIVE-COTE 1.0. During this time a number of algorithms have been proposed which match the accuracy of HIVE-COTE. We propose comprehensive changes to the HIVE-COTE algorithm which significantly improve its accuracy and usability, presenting this upgrade as HIVE-COTE 2.0. We introduce two novel classifiers, the Temporal Dictionary Ensemble and Diverse Representation Canonical Interval Forest, which replace existing ensemble members. Additionally, we introduce the Arsenal, an ensemble of ROCKET classifiers as a new HIVE-COTE 2.0 constituent. We demonstrate that HIVE-COTE 2.0 is significantly more accurate on average than the current state of the art on 112 univariate UCR archive datasets and 26 multivariate UEA archive datasets.},
  archive      = {J_ML},
  author       = {Middlehurst, Matthew and Large, James and Flynn, Michael and Lines, Jason and Bostrom, Aaron and Bagnall, Anthony},
  doi          = {10.1007/s10994-021-06057-9},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3211-3243},
  shortjournal = {Mach. Learn.},
  title        = {HIVE-COTE 2.0: A new meta ensemble for time series classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse classification: A scalable discrete optimization
perspective. <em>ML</em>, <em>110</em>(11), 3177–3209. (<a
href="https://doi.org/10.1007/s10994-021-06085-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate the sparse classification problem of n samples with p features as a binary convex optimization problem and propose a outer-approximation algorithm to solve it exactly. For sparse logistic regression and sparse SVM, our algorithm finds optimal solutions for n and p in the 10,000 s within minutes. On synthetic data our algorithm achieves perfect support recovery in the large sample regime. Namely, there exists an $$n_0$$ such that the algorithm takes a long time to find an optimal solution and does not recover the correct support for $$n0$$ .},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Pauphilet, Jean and Van Parys, Bart},
  doi          = {10.1007/s10994-021-06085-5},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3177-3209},
  shortjournal = {Mach. Learn.},
  title        = {Sparse classification: A scalable discrete optimization perspective},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Misalignment problem in matrix decomposition with missing
values. <em>ML</em>, <em>110</em>(11), 3157–3175. (<a
href="https://doi.org/10.1007/s10994-021-05985-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collection within a real-world environment may be compromised by several factors such as data-logger malfunctions and communication errors, during which no data is collected. As a consequence, appropriate tools are required to handle the missing values when analysing and processing such data. This problem is often tackled via matrix decomposition. While it has been successfully applied in a wide range of applications, in this work we report an issue that has been neglected in literature and “degenerates” the quality of the imputations obtained by matrix decomposition in multivariate time-series (with smooth evolution). Briefly, the problem consists of the misalignment of the matrix decomposition result: the missing values imputations fall within an incorrect range of values and the transitions between observed and imputed values are not smooth. We address this problem by proposing a post-processing alignment strategy. According to our experiments, the post-processing adjustment substantially improves the accuracy of the imputations (when the misalignment occurs). Moreover, the results also suggest that the misalignment occurs mostly when dealing with a small number of time-series due to lack of generalization ability.},
  archive      = {J_ML},
  author       = {Fernandes, Sofia and Antunes, Mário and Gomes, Diogo and Aguiar, Rui L.},
  doi          = {10.1007/s10994-021-05985-w},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3157-3175},
  shortjournal = {Mach. Learn.},
  title        = {Misalignment problem in matrix decomposition with missing values},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data driven conditional optimal transport. <em>ML</em>,
<em>110</em>(11), 3135–3155. (<a
href="https://doi.org/10.1007/s10994-021-06060-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-driven procedure is developed to compute the optimal map between two conditional probabilities $$\rho (x|z_{1},\ldots ,z_{L})$$ and $$\mu (y|z_{1},\ldots ,z_{L})$$ , known only through samples and depending on a set of covariates $$z_{l}$$ . The procedure is tested on synthetic data from the ACIC Data Analysis Challenge 2017 and it is applied to non-uniform lightness transfer between images. Exactly solvable examples and simulations are performed to highlight the differences with ordinary optimal transport.},
  archive      = {J_ML},
  author       = {Tabak, Esteban G. and Trigila, Giulio and Zhao, Wenjun},
  doi          = {10.1007/s10994-021-06060-0},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3135-3155},
  shortjournal = {Mach. Learn.},
  title        = {Data driven conditional optimal transport},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified framework for closed-form nonparametric
regression, classification, preference and mixed problems with skew
gaussian processes. <em>ML</em>, <em>110</em>(11), 3095–3133. (<a
href="https://doi.org/10.1007/s10994-021-06039-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skew-Gaussian Processes (SkewGPs) extend the multivariate Unified Skew-Normal distributions over finite dimensional vectors to distribution over functions. SkewGPs are more general and flexible than Gaussian processes, as SkewGPs may also represent asymmetric distributions. In a recent contribution, we showed that SkewGP and probit likelihood are conjugate, which allows us to compute the exact posterior for non-parametric binary classification and preference learning. In this paper, we generalize previous results and we prove that SkewGP is conjugate with both the normal and affine probit likelihood, and more in general, with their product. This allows us to (i) handle classification, preference, numeric and ordinal regression, and mixed problems in a unified framework; (ii) derive closed-form expression for the corresponding posterior distributions. We show empirically that the proposed framework based on SkewGP provides better performance than Gaussian processes in active learning and Bayesian (constrained) optimization. These two tasks are fundamental for design of experiments and in Data Science.},
  archive      = {J_ML},
  author       = {Benavoli, Alessio and Azzimonti, Dario and Piga, Dario},
  doi          = {10.1007/s10994-021-06039-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3095-3133},
  shortjournal = {Mach. Learn.},
  title        = {A unified framework for closed-form nonparametric regression, classification, preference and mixed problems with skew gaussian processes},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RB-CCR: Radial-based combined cleaning and resampling
algorithm for imbalanced data classification. <em>ML</em>,
<em>110</em>(11), 3059–3093. (<a
href="https://doi.org/10.1007/s10994-021-06012-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world classification domains, such as medicine, health and safety, and finance, often exhibit imbalanced class priors and have asynchronous misclassification costs. In such cases, the classification model must achieve a high recall without significantly impacting precision. Resampling the training data is the standard approach to improving classification performance on imbalanced binary data. However, the state-of-the-art methods ignore the local joint distribution of the data or correct it as a post-processing step. This can causes sub-optimal shifts in the training distribution, particularly when the target data distribution is complex. In this paper, we propose Radial-Based Combined Cleaning and Resampling (RB-CCR). RB-CCR utilizes the concept of class potential to refine the energy-based resampling approach of CCR. In particular, RB-CCR exploits the class potential to accurately locate sub-regions of the data-space for synthetic oversampling. The category sub-region for oversampling can be specified as an input parameter to meet domain-specific needs or be automatically selected via cross-validation. Our $$5\times 2$$ cross-validated results on 57 benchmark binary datasets with 9 classifiers show that RB-CCR achieves a better precision-recall trade-off than CCR and generally out-performs the state-of-the-art resampling methods in terms of AUC and G-mean.},
  archive      = {J_ML},
  author       = {Koziarski, Michał and Bellinger, Colin and Woźniak, Michał},
  doi          = {10.1007/s10994-021-06012-8},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3059-3093},
  shortjournal = {Mach. Learn.},
  title        = {RB-CCR: Radial-based combined cleaning and resampling algorithm for imbalanced data classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep learning and multivariate time series for cheat
detection in video games. <em>ML</em>, <em>110</em>(11), 3037–3057. (<a
href="https://doi.org/10.1007/s10994-021-06055-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online video games drive a multi-billion dollar industry dedicated to maintaining a competitive and enjoyable experience for players. Traditional cheat detection systems struggle when facing new exploits or sophisticated fraudsters. More advanced solutions based on machine learning are more adaptive but rely heavily on in-game data, which means that each game has to develop its own cheat detection system. In this work, we propose a novel approach to cheat detection that doesn’t require in-game data. Firstly, we treat the multimodal interactions between the player and the platform as multivariate time series. We then use convolutional neural networks to classify these time series as corresponding to legitimate or fraudulent gameplay. Our models achieve an average accuracy of respectively 99.2\% and 98.9\% in triggerbot and aimbot (two widespread cheats), in an experiment to validate the system’s ability to detect cheating in players never seen before. Because this approach is based solely on player behavior, it can be applied to any game or input method, and even various tasks related to modeling human activity.},
  archive      = {J_ML},
  author       = {Pinto, José Pedro and Pimenta, André and Novais, Paulo},
  doi          = {10.1007/s10994-021-06055-x},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3037-3057},
  shortjournal = {Mach. Learn.},
  title        = {Deep learning and multivariate time series for cheat detection in video games},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor decision trees for continual learning from drifting
data streams. <em>ML</em>, <em>110</em>(11), 3015–3035. (<a
href="https://doi.org/10.1007/s10994-021-06054-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream classification is one of the most vital areas of contemporary machine learning, as many real-life problems generate data continuously and in large volumes. However, most of research in this area focuses on vector-based representations, which are unsuitable for capturing properties of more complex multi-dimensional structures, such as images and video sequences. In this paper, we propose a novel methodology for learning adaptive decision trees from data streams of tensors. We introduce Chordal Kernel Decision Tree for continual learning from tensor data streams. In order to maintain the tensor characteristics, we propose to train and update classifiers in the kernel space designed to work with tensor representation. We use chordal distance to compute similarities between tensors and then apply it as a new feature space in which decision trees are trained. This allows for a direct decision tree induction on tensors. In order to accommodate the streaming and drifting nature of data, we propose a concept drift detection scheme based on tensor representation. It allows us to reconstruct the kernel feature space every time when change is detected. The proposed approach allows for fast and efficient induction of decision trees on streaming data with tensor representation. Experimental study, conducted on 4 real-world and 52 artificial large-scale tensor data streams, shows that using the native tensor feature space leads to more accurate classification than outperforms the vectorized representations.},
  archive      = {J_ML},
  author       = {Krawczyk, Bartosz},
  doi          = {10.1007/s10994-021-06054-y},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {3015-3035},
  shortjournal = {Mach. Learn.},
  title        = {Tensor decision trees for continual learning from drifting data streams},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MLife: A lite framework for machine learning lifecycle
initialization. <em>ML</em>, <em>110</em>(11), 2993–3013. (<a
href="https://doi.org/10.1007/s10994-021-06052-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) lifecycle is a cyclic process to build an efficient ML system. Though a lot of commercial and community (non-commercial) frameworks have been proposed to streamline the major stages in the ML lifecycle, they are normally overqualified and insufficient for an ML system in its nascent phase. Driven by real-world experience in building and maintaining ML systems, we find that it is more efficient to initialize the major stages of ML lifecycle first for trial and error, followed by the extension of specific stages to acclimatize towards more complex scenarios. For this, we introduce a simple yet flexible framework, MLife, for fast ML lifecycle initialization. This is built on the fact that data flow in MLife is in a closed loop driven by bad cases, especially those which impact ML model performance the most but also provide the most value for further ML model development—a key factor towards enabling enterprises to fast track their ML capabilities. Better yet, MLife is also flexible enough to be easily extensible to more complex scenarios for future maintenance. For this, we introduce two real-world use cases to demonstrate that MLife is particularly suitable for ML systems in their early phases.},
  archive      = {J_ML},
  author       = {Yang, Cong and Wang, Wenfeng and Zhang, Yunhui and Zhang, Zhikai and Shen, Lina and Li, Yipeng and See, John},
  doi          = {10.1007/s10994-021-06052-0},
  journal      = {Machine Learning},
  number       = {11},
  pages        = {2993-3013},
  shortjournal = {Mach. Learn.},
  title        = {MLife: A lite framework for machine learning lifecycle initialization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introduction to the special issue of the ECML PKDD 2021
journal track. <em>ML</em>, <em>110</em>(10), 2991–2992. (<a
href="https://doi.org/10.1007/s10994-021-06062-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Appice, Annalisa and Escalera, Sergio and Gámez, Jose A. and Trautmann, Heike},
  doi          = {10.1007/s10994-021-06062-y},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2991-2992},
  shortjournal = {Mach. Learn.},
  title        = {Introduction to the special issue of the ECML PKDD 2021 journal track},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust non-parametric regression via incoherent subspace
projections. <em>ML</em>, <em>110</em>(10), 2941–2989. (<a
href="https://doi.org/10.1007/s10994-021-06045-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes the algorithmic principle of alternating projections onto incoherent low-rank subspaces (APIS) as a unifying principle for designing robust regression algorithms that offer consistent model recovery even when a significant fraction of training points are corrupted by an adaptive adversary. APIS offers the first algorithm for robust non-parametric (kernel) regression with an explicit breakdown point that works for general PSD kernels under minimal assumptions. APIS also offers, as straightforward corollaries, robust algorithms for a much wider variety of well-studied settings, including robust linear regression, robust sparse recovery, and robust Fourier transforms. Algorithms offered by APIS enjoy formal guarantees that are frequently sharper than (especially in non-parametric settings) or competitive to existing results in these settings. They are also straightforward to implement and outperform existing algorithms in several experimental settings.},
  archive      = {J_ML},
  author       = {Mukhoty, Bhaskar and Dutta, Subhajit and Kar, Purushottam},
  doi          = {10.1007/s10994-021-06045-z},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2941-2989},
  shortjournal = {Mach. Learn.},
  title        = {Robust non-parametric regression via incoherent subspace projections},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional t-SNE: More informative t-SNE embeddings.
<em>ML</em>, <em>110</em>(10), 2905–2940. (<a
href="https://doi.org/10.1007/s10994-020-05917-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction and manifold learning methods such as t-distributed stochastic neighbor embedding (t-SNE) are frequently used to map high-dimensional data into a two-dimensional space to visualize and explore that data. Going beyond the specifics of t-SNE, there are two substantial limitations of any such approach: (1) not all information can be captured in a single two-dimensional embedding, and (2) to well-informed users, the salient structure of such an embedding is often already known, preventing that any real new insights can be obtained. Currently, it is not known how to extract the remaining information in a similarly effective manner. We introduce conditional t-SNE (ct-SNE), a generalization of t-SNE that discounts prior information in the form of labels. This enables obtaining more informative and more relevant embeddings. To achieve this, we propose a conditioned version of the t-SNE objective, obtaining an elegant method with a single integrated objective. We show how to efficiently optimize the objective and study the effects of the extra parameter that ct-SNE has over t-SNE. Qualitative and quantitative empirical results on synthetic and real data show ct-SNE is scalable, effective, and achieves its goal: it allows complementary structure to be captured in the embedding and provided new insights into real data.},
  archive      = {J_ML},
  author       = {Kang, Bo and García García, Darío and Lijffijt, Jefrey and Santos-Rodríguez, Raúl and De Bie, Tijl},
  doi          = {10.1007/s10994-020-05917-0},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2905-2940},
  shortjournal = {Mach. Learn.},
  title        = {Conditional t-SNE: More informative t-SNE embeddings},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ZipLine: An optimized algorithm for the elastic bulk
synchronous parallel model. <em>ML</em>, <em>110</em>(10), 2867–2903.
(<a href="https://doi.org/10.1007/s10994-021-06064-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bulk synchronous parallel (BSP) is a celebrated synchronization model for general-purpose parallel computing that has successfully been employed for distributed training of deep learning models. A shortcoming of the BSP is that it requires workers to wait for the straggler at every iteration. Therefore, employing BSP increases the waiting time of the faster workers of a cluster and results in an overall prolonged training time. To ameliorate this shortcoming of BSP, we propose ElasticBSP, a model that aims to relax its strict synchronization requirement with an elastic synchronization by allowing delayed synchronization to minimize the waiting time. ElasticBSP offers more flexibility and adaptability during the training phase, without sacrificing the accuracy of the trained model. ElasticBSP is realized by the algorithm named ZipLine, which consists of two phases. First, it estimates for each worker the end time points of its future iterations at run time, and then a one-pass algorithm over the estimated time points of all workers is employed to fast compute an optimal future time point for synchronization. We provide theoretical results about the correctness and performance of the ZipLine algorithm. Furthermore, we propose algorithmic and implementation optimizations of ZipLine, namely ZipLineOpt and ZipLineOptBS, which reduce the time complexity of ZipLine to linearithmic time. A thorough experimental evaluation demonstrates that our proposed ElasticBSP model, materialized by the proposed optimized ZipLine variants, converges faster and to a higher accuracy than the predominant BSP. The focus of the paper is on optimizing the synchronization scheduling over a parameter server architecture. It is orthogonal to other types of optimizations, such as the learning rate optimization.},
  archive      = {J_ML},
  author       = {Zhao, Xing and Papagelis, Manos and An, Aijun and Chen, Bao Xin and Liu, Junfeng and Hu, Yonggang},
  doi          = {10.1007/s10994-021-06064-w},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2867-2903},
  shortjournal = {Mach. Learn.},
  title        = {ZipLine: An optimized algorithm for the elastic bulk synchronous parallel model},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RADE: Resource-efficient supervised anomaly detection using
decision tree-based ensemble methods. <em>ML</em>, <em>110</em>(10),
2835–2866. (<a
href="https://doi.org/10.1007/s10994-021-06047-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability to perform anomaly detection in a resource-constrained setting, such as an edge device or a loaded server, is of increasing need due to emerging on-premises computation constraints as well as security, privacy and profitability reasons. Yet, the increasing size of datasets often results in current anomaly detection methods being too resource consuming, and in particular decision-tree based ensemble classifiers. To address this need, we present RADE—a new resource-efficient anomaly detection framework that augments standard decision-tree based ensemble classifiers to perform well in a resource constrained setting. The key idea behind RADE is first to train a small model that is sufficient to correctly classify the majority of the queries. Then, using only subsets of the training data, train expert models for these fewer harder cases where the small model is at high risk of making a classification mistake. We implement RADE as a scikit-learn classifier. Our evaluation indicates that RADE offers competitive anomaly detection capabilities as compared to standard methods while significantly improving memory footprint by up to $$12\times $$ , training-time by up to $$20\times $$ , and classification time by up to $$16\times $$ .},
  archive      = {J_ML},
  author       = {Vargaftik, Shay and Keslassy, Isaac and Orda, Ariel and Ben-Itzhak, Yaniv},
  doi          = {10.1007/s10994-021-06047-x},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2835-2866},
  shortjournal = {Mach. Learn.},
  title        = {RADE: Resource-efficient supervised anomaly detection using decision tree-based ensemble methods},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Provable training set debugging for linear regression.
<em>ML</em>, <em>110</em>(10), 2763–2834. (<a
href="https://doi.org/10.1007/s10994-021-06040-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate problems in penalized M-estimation, inspired by applications in machine learning debugging. Data are collected from two pools, one containing data with possibly contaminated labels, and the other which is known to contain only cleanly labeled points. We first formulate a general statistical algorithm for identifying buggy points and provide rigorous theoretical guarantees when the data follow a linear model. We then propose an algorithm for tuning parameter selection of our Lasso-based algorithm with theoretical guarantees. Finally, we consider a two-person â€śgameâ€ť played between a bug generator and a debugger, where the debugger can augment the contaminated data set with cleanly labeled versions of points in the original data pool. We develop and analyze a debugging strategy in terms of a Mixed Integer Linear Programming (MILP). Finally, we provide empirical results to verify our theoretical results and the utility of the MILP strategy.},
  archive      = {J_ML},
  author       = {Zhang, Xiaomin and Zhu, Xiaojin and Loh, Po-Ling},
  doi          = {10.1007/s10994-021-06040-4},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2763-2834},
  shortjournal = {Mach. Learn.},
  title        = {Provable training set debugging for linear regression},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ordinal regression with explainable distance metric learning
based on ordered sequences. <em>ML</em>, <em>110</em>(10), 2729–2762.
(<a href="https://doi.org/10.1007/s10994-021-06010-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to introduce a new distance metric learning algorithm for ordinal regression. Ordinal regression addresses the problem of predicting classes for which there is a natural ordering, but the real distances between classes are unknown. Since ordinal regression walks a fine line between standard regression and classification, it is a common pitfall to either apply a regression-like numerical treatment of variables or underrate the ordinal information applying nominal classification techniques. On a different note, distance metric learning is a discipline that has proven to be very useful when improving distance-based algorithms such as the nearest neighbors classifier. In addition, an appropriate distance can enhance the explainability of this model. In our study we propose an ordinal approach to learning a distance, called chain maximizing ordinal metric learning. It is based on the maximization of ordered sequences in local neighborhoods of the data. This approach takes into account all the ordinal information in the data without making use of any of the two extremes of classification or regression, and it is able to adapt to data for which the class separations are not clear. We also show how to extend the algorithm to learn in a non-linear setup using kernel functions. We have tested our algorithm on several ordinal regression problems, showing a high performance under the usual evaluation metrics in this domain. Results are verified through Bayesian non-parametric testing. Finally, we explore the capabilities of our algorithm in terms of explainability using the case-based reasoning approach. We show these capabilities empirically on two different datasets, experiencing significant improvements over the case-based reasoning with the traditional Euclidean nearest neighbors.},
  archive      = {J_ML},
  author       = {Suárez, Juan Luis and García, Salvador and Herrera, Francisco},
  doi          = {10.1007/s10994-021-06010-w},
  journal      = {Machine Learning},
  number       = {10},
  pages        = {2729-2762},
  shortjournal = {Mach. Learn.},
  title        = {Ordinal regression with explainable distance metric learning based on ordered sequences},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IntelligentPooling: Practical thompson sampling for mHealth.
<em>ML</em>, <em>110</em>(9), 2685–2727. (<a
href="https://doi.org/10.1007/s10994-021-05995-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile health (mHealth) smart devices deliver behavioral treatments repeatedly over time to a user with the goal of helping the user adopt and maintain healthy behaviors. Reinforcement learning appears ideal for learning how to optimally make these sequential treatment decisions. However, significant challenges must be overcome before reinforcement learning can be effectively deployed in a mobile healthcare setting. In this work we are concerned with the following challenges: (1) individuals who are in the same context can exhibit differential response to treatments (2) only a limited amount of data is available for learning on any one individual, and (3) non-stationary responses to treatment. To address these challenges we generalize Thompson-Sampling bandit algorithms to develop IntelligentPooling. IntelligentPooling learns personalized treatment policies thus addressing challenge one. To address the second challenge, IntelligentPooling updates each user’s degree of personalization while making use of available data on other users to speed up learning. Lastly, IntelligentPooling allows responsivity to vary as a function of a user’s time since beginning treatment, thus addressing challenge three.},
  archive      = {J_ML},
  author       = {Tomkins, Sabina and Liao, Peng and Klasnja, Predrag and Murphy, Susan},
  doi          = {10.1007/s10994-021-05995-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2685-2727},
  shortjournal = {Mach. Learn.},
  title        = {IntelligentPooling: Practical thompson sampling for mHealth},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic discovery of interpretable planning strategies.
<em>ML</em>, <em>110</em>(9), 2641–2683. (<a
href="https://doi.org/10.1007/s10994-021-05963-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When making decisions, people often overlook critical information or are overly swayed by irrelevant information. A common approach to mitigate these biases is to provide decision-makers, especially professionals such as medical doctors, with decision aids, such as decision trees and flowcharts. Designing effective decision aids is a difficult problem. We propose that recently developed reinforcement learning methods for discovering clever heuristics for good decision-making can be partially leveraged to assist human experts in this design process. One of the biggest remaining obstacles to leveraging the aforementioned methods for improving human decision-making is that the policies they learn are opaque to people. To solve this problem, we introduce AI-Interpret: a general method for transforming idiosyncratic policies into simple and interpretable descriptions. Our algorithm combines recent advances in imitation learning and program induction with a new clustering method for identifying a large subset of demonstrations that can be accurately described by a simple, high-performing decision rule. We evaluate our new AI-Interpret algorithm and employ it to translate information-acquisition policies discovered through metalevel reinforcement learning. The results of three large behavioral experiments showed that providing the decision rules generated by AI-Interpret as flowcharts significantly improved people’s planning strategies and decisions across three different classes of sequential decision problems. Moreover, our fourth experiment revealed that this approach is significantly more effective at improving human decision-making than training people by giving them performance feedback. Finally, a series of ablation studies confirmed that our AI-Interpret algorithm was critical to the discovery of interpretable decision rules and that it is ready to be applied to other reinforcement learning problems. We conclude that the methods and findings presented in this article are an important step towards leveraging automatic strategy discovery to improve human decision-making. The code for our algorithm and the experiments is available at https://github.com/RationalityEnhancement/InterpretableStrategyDiscovery .},
  archive      = {J_ML},
  author       = {Skirzyński, Julian and Becker, Frederic and Lieder, Falk},
  doi          = {10.1007/s10994-021-05963-2},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2641-2683},
  shortjournal = {Mach. Learn.},
  title        = {Automatic discovery of interpretable planning strategies},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partially observable environment estimation with uplift
inference for reinforcement learning based recommendation. <em>ML</em>,
<em>110</em>(9), 2603–2640. (<a
href="https://doi.org/10.1007/s10994-021-05969-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) aims at searching the best policy model for decision making, and has been shown powerful for sequential recommendations. The training of the policy by RL, however, is placed in an environment. In many real-world applications, the policy training in the real environment can cause an unbearable cost due to the exploration. Environment estimation from the past data is thus an appealing way to release the power of RL in these applications. The estimation of the environment is, basically, to extract the causal effect model from the data. However, real-world applications are often too complex to offer fully observable environment information. Therefore, quite possibly there are unobserved variables lying behind the data, which can obstruct an effective estimation of the environment. In this paper, by treating the hidden variables as a hidden policy, we propose a partially-observed multi-agent environment estimation (POMEE) approach to learn the partially-observed environment. To make a better extraction of the causal relationship between actions and rewards, we design a deep uplift inference network (DUIN) model to learn the causal effects of different actions. By implementing the environment model in the DUIN structure, we propose a POMEE with uplift inference (POMEE-UI) approach to generate a partially-observed environment with a causal reward mechanism. We analyze the effect of our method in both artificial and real-world environments. We first use an artificial recommender environment, abstracted from a real-world application, to verify the effectiveness of POMEE-UI. We then test POMEE-UI in the real application of Didi Chuxing. Experiment results show that POMEE-UI can effectively estimate the hidden variables, leading to a more reliable virtual environment. The online A/B testing results show that POMEE can derive a well-performing recommender policy in the real-world application.},
  archive      = {J_ML},
  author       = {Shang, Wenjie and Li, Qingyang and Qin, Zhiwei and Yu, Yang and Meng, Yiping and Ye, Jieping},
  doi          = {10.1007/s10994-021-05969-w},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2603-2640},
  shortjournal = {Mach. Learn.},
  title        = {Partially observable environment estimation with uplift inference for reinforcement learning based recommendation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lessons on off-policy methods from a notification component
of a chatbot. <em>ML</em>, <em>110</em>(9), 2577–2602. (<a
href="https://doi.org/10.1007/s10994-021-05978-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work serves as a review of our experience applying off-policy techniques to train and evaluate a contextual bandit model powering a troubleshooting notification in a chatbot. First, we demonstrate the effectiveness of off-policy evaluation when data volume is orders of magnitude less than typically found in the literature. We present our reward function and choices behind its design, as well as how we construct our logging policy to balance exploration and performance on key metrics. Next, we present a guided framework to update a model post-training called Post-Hoc Reward Distribution Hacking, which we employed to improve model performance and correct deficiencies in trained models stemming from the existence of a null action and a noisy reward signal. Throughout the work, we include discussions of various practical pitfalls encountered while using off-policy methods in hopes to expedite other applications of these techniques.},
  archive      = {J_ML},
  author       = {Rome, Scott and Chen, Tianwen and Kreisel, Michael and Zhou, Ding},
  doi          = {10.1007/s10994-021-05978-9},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2577-2602},
  shortjournal = {Mach. Learn.},
  title        = {Lessons on off-policy methods from a notification component of a chatbot},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dealing with multiple experts and non-stationarity in
inverse reinforcement learning: An application to real-life problems.
<em>ML</em>, <em>110</em>(9), 2541–2576. (<a
href="https://doi.org/10.1007/s10994-020-05939-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, inferring the intentions of expert agents (e.g., human operators) can be fundamental to understand how possibly conflicting objectives are managed, helping to interpret the demonstrated behavior. In this paper, we discuss how inverse reinforcement learning (IRL) can be employed to retrieve the reward function implicitly optimized by expert agents acting in real applications. Scaling IRL to real-world cases has proved challenging as typically only a fixed dataset of demonstrations is available and further interactions with the environment are not allowed. For this reason, we resort to a class of truly batch model-free IRL algorithms and we present three application scenarios: (1) the high-level decision-making problem in the highway driving scenario, and (2) inferring the user preferences in a social network (Twitter), and (3) the management of the water release in the Como Lake. For each of these scenarios, we provide formalization, experiments and a discussion to interpret the obtained results.},
  archive      = {J_ML},
  author       = {Likmeta, Amarildo and Metelli, Alberto Maria and Ramponi, Giorgia and Tirinzoni, Andrea and Giuliani, Matteo and Restelli, Marcello},
  doi          = {10.1007/s10994-020-05939-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2541-2576},
  shortjournal = {Mach. Learn.},
  title        = {Dealing with multiple experts and non-stationarity in inverse reinforcement learning: An application to real-life problems},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Air learning: A deep reinforcement learning gym for
autonomous aerial robot visual navigation. <em>ML</em>, <em>110</em>(9),
2501–2540. (<a
href="https://doi.org/10.1007/s10994-021-06006-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Air Learning, an open-source simulator, and a gym environment for deep reinforcement learning research on resource-constrained aerial robots. Equipped with domain randomization, Air Learning exposes a UAV agent to a diverse set of challenging scenarios. We seed the toolset with point-to-point obstacle avoidance tasks in three different environments and Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) trainers. Air Learning assesses the policies’ performance under various quality-of-flight (QoF) metrics, such as the energy consumed, endurance, and the average trajectory length, on resource-constrained embedded platforms like a Raspberry Pi. We find that the trajectories on an embedded Ras-Pi are vastly different from those predicted on a high-end desktop system, resulting in up to $$40\%$$ longer trajectories in one of the environments. To understand the source of such discrepancies, we use Air Learning to artificially degrade high-end desktop performance to mimic what happens on a low-end embedded system. We then propose a mitigation technique that uses the hardware-in-the-loop to determine the latency distribution of running the policy on the target platform (onboard compute on aerial robot). A randomly sampled latency from the latency distribution is then added as an artificial delay within the training loop. Training the policy with artificial delays allows us to minimize the hardware gap (discrepancy in the flight time metric reduced from 37.73\% to 0.5\%). Thus, Air Learning with hardware-in-the-loop characterizes those differences and exposes how the onboard compute’s choice affects the aerial robot’s performance. We also conduct reliability studies to assess the effect of sensor failures on the learned policies. All put together, Air Learning enables a broad class of deep RL research on UAVs. The source code is available at: https://github.com/harvard-edge/AirLearning .},
  archive      = {J_ML},
  author       = {Krishnan, Srivatsan and Boroujerdian, Behzad and Fu, William and Faust, Aleksandra and Reddi, Vijay Janapa},
  doi          = {10.1007/s10994-021-06006-6},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2501-2540},
  shortjournal = {Mach. Learn.},
  title        = {Air learning: A deep reinforcement learning gym for autonomous aerial robot visual navigation},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grounded action transformation for sim-to-real reinforcement
learning. <em>ML</em>, <em>110</em>(9), 2469–2499. (<a
href="https://doi.org/10.1007/s10994-021-05982-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning in simulation is a promising alternative to the prohibitive sample cost of reinforcement learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the target, physical system. Grounded simulation learning (gsl) is a general framework that promises to address this issue by altering the simulator to better match the real world (Farchy et al. 2013 in Proceedings of the 12th international conference on autonomous agents and multiagent systems (AAMAS)). This article introduces a new algorithm for gsl—Grounded Action Transformation (GAT)—and applies it to learning control policies for a humanoid robot. We evaluate our algorithm in controlled experiments where we show it to allow policies learned in simulation to transfer to the real world. We then apply our algorithm to learning a fast bipedal walk on a humanoid robot and demonstrate a 43.27\% improvement in forward walk velocity compared to a state-of-the art hand-coded walk. This striking empirical success notwithstanding, further empirical analysis shows that gat may struggle when the real world has stochastic state transitions. To address this limitation we generalize gat to the stochastic gat (sgat) algorithm and empirically show that sgat leads to successful real world transfer in situations where gat may fail to find a good policy. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for applying reinforcement learning to learn robot control policies entirely in simulation.},
  archive      = {J_ML},
  author       = {Hanna, Josiah P. and Desai, Siddharth and Karnan, Haresh and Warnell, Garrett and Stone, Peter},
  doi          = {10.1007/s10994-021-05982-z},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2469-2499},
  shortjournal = {Mach. Learn.},
  title        = {Grounded action transformation for sim-to-real reinforcement learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Challenges of real-world reinforcement learning:
Definitions, benchmarks and analysis. <em>ML</em>, <em>110</em>(9),
2419–2468. (<a
href="https://doi.org/10.1007/s10994-021-05961-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
  archive      = {J_ML},
  author       = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
  doi          = {10.1007/s10994-021-05961-4},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2419-2468},
  shortjournal = {Mach. Learn.},
  title        = {Challenges of real-world reinforcement learning: Definitions, benchmarks and analysis},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bandit algorithms to personalize educational chatbots.
<em>ML</em>, <em>110</em>(9), 2389–2418. (<a
href="https://doi.org/10.1007/s10994-021-05983-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To emulate the interactivity of in-person math instruction, we developed MathBot, a rule-based chatbot that explains math concepts, provides practice questions, and offers tailored feedback. We evaluated MathBot through three Amazon Mechanical Turk studies in which participants learned about arithmetic sequences. In the first study, we found that more than 40\% of our participants indicated a preference for learning with MathBot over videos and written tutorials from Khan Academy. The second study measured learning gains, and found that MathBot produced comparable gains to Khan Academy videos and tutorials. We solicited feedback from users in those two studies to emulate a real-world development cycle, with some users finding the lesson too slow and others finding it too fast. We addressed these concerns in the third and main study by integrating a contextual bandit algorithm into MathBot to personalize the pace of the conversation, allowing the bandit to either insert extra practice problems or skip explanations. We randomized participants between two conditions in which actions were chosen uniformly at random (i.e., a randomized A/B experiment) or by the contextual bandit. We found that the bandit learned a similarly effective pedagogical policy to that learned by the randomized A/B experiment while incurring a lower cost of experimentation. Our findings suggest that personalized conversational agents are promising tools to complement existing online resources for math education, and that data-driven approaches such as contextual bandits are valuable tools for learning effective personalization.},
  archive      = {J_ML},
  author       = {Cai, William and Grossman, Josh and Lin, Zhiyuan Jerry and Sheng, Hao and Wei, Johnny Tian-Zheng and Williams, Joseph Jay and Goel, Sharad},
  doi          = {10.1007/s10994-021-05983-y},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2389-2418},
  shortjournal = {Mach. Learn.},
  title        = {Bandit algorithms to personalize educational chatbots},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep reinforcement learning framework for continuous
intraday market bidding. <em>ML</em>, <em>110</em>(9), 2335–2387. (<a
href="https://doi.org/10.1007/s10994-021-06020-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large integration of variable energy resources is expected to shift a large part of the energy exchanges closer to real-time, where more accurate forecasts are available. In this context, the short-term electricity markets and in particular the intraday market are considered a suitable trading floor for these exchanges to occur. A key component for the successful renewable energy sources integration is the usage of energy storage. In this paper, we propose a novel modelling framework for the strategic participation of energy storage in the European continuous intraday market where exchanges occur through a centralized order book. The goal of the storage device operator is the maximization of the profits received over the entire trading horizon, while taking into account the operational constraints of the unit. The sequential decision-making problem of trading in the intraday market is modelled as a Markov Decision Process. An asynchronous version of the fitted Q iteration algorithm is chosen for solving this problem due to its sample efficiency. The large and variable number of the existing orders in the order book motivates the use of high-level actions and an alternative state representation. Historical data are used for the generation of a large number of artificial trajectories in order to address exploration issues during the learning process. The resulting policy is back-tested and compared against a number of benchmark strategies. Finally, the impact of the storage characteristics on the total revenues collected in the intraday market is evaluated.},
  archive      = {J_ML},
  author       = {Boukas, Ioannis and Ernst, Damien and Théate, Thibaut and Bolland, Adrien and Huynen, Alexandre and Buchwald, Martin and Wynants, Christelle and Cornélusse, Bertrand},
  doi          = {10.1007/s10994-021-06020-8},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2335-2387},
  shortjournal = {Mach. Learn.},
  title        = {A deep reinforcement learning framework for continuous intraday market bidding},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inverse reinforcement learning in contextual MDPs.
<em>ML</em>, <em>110</em>(9), 2295–2334. (<a
href="https://doi.org/10.1007/s10994-021-05984-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of Inverse Reinforcement Learning in Contextual Markov Decision Processes (MDPs). In this setting, contexts, which define the reward and transition kernel, are sampled from a distribution. In addition, although the reward is a function of the context, it is not provided to the agent. Instead, the agent observes demonstrations from an optimal policy. The goal is to learn the reward mapping, such that the agent will act optimally even when encountering previously unseen contexts, also known as zero-shot transfer. We formulate this problem as a non-differential convex optimization problem and propose a novel algorithm to compute its subgradients. Based on this scheme, we analyze several methods both theoretically, where we compare the sample complexity and scalability, and empirically. Most importantly, we show both theoretically and empirically that our algorithms perform zero-shot transfer (generalize to new and unseen contexts). Specifically, we present empirical experiments in a dynamic treatment regime, where the goal is to learn a reward function which explains the behavior of expert physicians based on recorded data of them treating patients diagnosed with sepsis.},
  archive      = {J_ML},
  author       = {Belogolovsky, Stav and Korsunsky, Philip and Mannor, Shie and Tessler, Chen and Zahavy, Tom},
  doi          = {10.1007/s10994-021-05984-x},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2295-2334},
  shortjournal = {Mach. Learn.},
  title        = {Inverse reinforcement learning in contextual MDPs},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on reinforcement learning for
real life. <em>ML</em>, <em>110</em>(9), 2291–2293. (<a
href="https://doi.org/10.1007/s10994-021-06041-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Li, Yuxi and Geramifard, Alborz and Li, Lihong and Szepesvari, Csaba and Wang, Tao},
  doi          = {10.1007/s10994-021-06041-3},
  journal      = {Machine Learning},
  number       = {9},
  pages        = {2291-2293},
  shortjournal = {Mach. Learn.},
  title        = {Guest editorial: Special issue on reinforcement learning for real life},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convex optimization with an interpolation-based projection
and its application to deep learning. <em>ML</em>, <em>110</em>(8),
2267–2289. (<a
href="https://doi.org/10.1007/s10994-021-06037-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex optimizers have known many applications as differentiable layers within deep neural architectures. One application of these convex layers is to project points into a convex set. However, both forward and backward passes of these convex layers are significantly more expensive to compute than those of a typical neural network. We investigate in this paper whether an inexact, but cheaper projection, can drive a descent algorithm to an optimum. Specifically, we propose an interpolation-based projection that is computationally cheap and easy to compute given a convex, domain defining, function. We then propose an optimization algorithm that follows the gradient of the composition of the objective and the projection and prove its convergence for linear objectives and arbitrary convex and Lipschitz domain defining inequality constraints. In addition to the theoretical contributions, we demonstrate empirically the practical interest of the interpolation projection when used in conjunction with neural networks in a reinforcement learning and a supervised learning setting.},
  archive      = {J_ML},
  author       = {Akrour, Riad and Atamna, Asma and Peters, Jan},
  doi          = {10.1007/s10994-021-06037-z},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2267-2289},
  shortjournal = {Mach. Learn.},
  title        = {Convex optimization with an interpolation-based projection and its application to deep learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Information-theoretic regularization for learning global
features by sequential VAE. <em>ML</em>, <em>110</em>(8), 2239–2266. (<a
href="https://doi.org/10.1007/s10994-021-06032-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential variational autoencoders (VAEs) with a global latent variable z have been studied for disentangling the global features of data, which is useful for several downstream tasks. To further assist the sequential VAEs in obtaining meaningful z, existing approaches introduce a regularization term that maximizes the mutual information (MI) between the observation and z. However, by analyzing the sequential VAEs from the information-theoretic perspective, we claim that simply maximizing the MI encourages the latent variable to have redundant information, thereby preventing the disentanglement of global features. Based on this analysis, we derive a novel regularization method that makes z informative while encouraging disentanglement. Specifically, the proposed method removes redundant information by minimizing the MI between z and the local features by using adversarial training. In the experiments, we trained two sequential VAEs, state-space and autoregressive model variants, using speech and image datasets. The results indicate that the proposed method improves the performance of downstream classification and data generation tasks, thereby supporting our information-theoretic perspective for the learning of global features.},
  archive      = {J_ML},
  author       = {Akuzawa, Kei and Iwasawa, Yusuke and Matsuo, Yutaka},
  doi          = {10.1007/s10994-021-06032-4},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2239-2266},
  shortjournal = {Mach. Learn.},
  title        = {Information-theoretic regularization for learning global features by sequential VAE},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian processes with skewed laplace spectral mixture
kernels for long-term forecasting. <em>ML</em>, <em>110</em>(8),
2213–2238. (<a
href="https://doi.org/10.1007/s10994-021-06031-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term forecasting involves predicting a horizon that is far ahead of the last observation. It is a problem of high practical relevance, for instance for companies in order to decide upon expensive long-term investments. Despite the recent progress and success of Gaussian processes (GPs) based on spectral mixture kernels, long-term forecasting remains a challenging problem for these kernels because they decay exponentially at large horizons. This is mainly due to their use of a mixture of Gaussians to model spectral densities. Characteristics of the signal important for long-term forecasting can be unravelled by investigating the distribution of the Fourier coefficients of (the training part of) the signal, which is non-smooth, heavy-tailed, sparse, and skewed. The heavy tail and skewness characteristics of such distributions in the spectral domain allow to capture long-range covariance of the signal in the time domain. Motivated by these observations, we propose to model spectral densities using a skewed Laplace spectral mixture (SLSM) due to the skewness of its peaks, sparsity, non-smoothness, and heavy tail characteristics. By applying the inverse Fourier Transform to this spectral density we obtain a new GP kernel for long-term forecasting. In addition, we adapt the lottery ticket method, originally developed to prune weights of a neural network, to GPs in order to automatically select the number of kernel components. Results of extensive experiments, including a multivariate time series, show the beneficial effect of the proposed SLSM kernel for long-term extrapolation and robustness to the choice of the number of mixture components.},
  archive      = {J_ML},
  author       = {Chen, Kai and van Laarhoven, Twan and Marchiori, Elena},
  doi          = {10.1007/s10994-021-06031-5},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2213-2238},
  shortjournal = {Mach. Learn.},
  title        = {Gaussian processes with skewed laplace spectral mixture kernels for long-term forecasting},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Density-based weighting for imbalanced regression.
<em>ML</em>, <em>110</em>(8), 2187–2211. (<a
href="https://doi.org/10.1007/s10994-021-06023-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real world settings, imbalanced data impedes model performance of learning algorithms, like neural networks, mostly for rare cases. This is especially problematic for tasks focusing on these rare occurrences. For example, when estimating precipitation, extreme rainfall events are scarce but important considering their potential consequences. While there are numerous well studied solutions for classification settings, most of them cannot be applied to regression easily. Of the few solutions for regression tasks, barely any have explored cost-sensitive learning which is known to have advantages compared to sampling-based methods in classification tasks. In this work, we propose a sample weighting approach for imbalanced regression datasets called DenseWeight and a cost-sensitive learning approach for neural network regression with imbalanced data called DenseLoss based on our weighting scheme. DenseWeight weights data points according to their target value rarities through kernel density estimation (KDE). DenseLoss adjusts each data point’s influence on the loss according to DenseWeight, giving rare data points more influence on model training compared to common data points. We show on multiple differently distributed datasets that DenseLoss significantly improves model performance for rare data points through its density-based weighting scheme. Additionally, we compare DenseLoss to the state-of-the-art method SMOGN, finding that our method mostly yields better performance. Our approach provides more control over model training as it enables us to actively decide on the trade-off between focusing on common or rare cases through a single hyperparameter, allowing the training of better models for rare data points.},
  archive      = {J_ML},
  author       = {Steininger, Michael and Kobs, Konstantin and Davidson, Padraig and Krause, Anna and Hotho, Andreas},
  doi          = {10.1007/s10994-021-06023-5},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2187-2211},
  shortjournal = {Mach. Learn.},
  title        = {Density-based weighting for imbalanced regression},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampled gromov wasserstein. <em>ML</em>, <em>110</em>(8),
2151–2186. (<a
href="https://doi.org/10.1007/s10994-021-06035-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal Transport (OT) has proven to be a powerful tool to compare probability distributions in machine learning, but dealing with probability measures lying in different spaces remains an open problem. To address this issue, the Gromov Wasserstein distance (GW) only considers intra-distribution pairwise (dis)similarities. However, for two (discrete) distributions with N points, the state of the art solvers have an iterative O(N4) complexity when using an arbitrary loss function, making most of the real world problems intractable. In this paper, we introduce a new iterative way to approximate GW, called Sampled Gromov Wasserstein, which uses the current estimate of the transport plan to guide the sampling of cost matrices. This simple idea, supported by theoretical convergence guarantees, comes with a O(N2) solver. A special case of Sampled Gromov Wasserstein, which can be seen as the natural extension of the well known Sliced Wasserstein to distributions lying in different spaces, reduces even further the complexity to O(N log N). Our contributions are supported by experiments on synthetic and real datasets.},
  archive      = {J_ML},
  author       = {Kerdoncuff, Tanguy and Emonet, Rémi and Sebban, Marc},
  doi          = {10.1007/s10994-021-06035-1},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2151-2186},
  shortjournal = {Mach. Learn.},
  title        = {Sampled gromov wasserstein},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AgFlow: Fast model selection of penalized PCA via implicit
regularization effects of gradient flow. <em>ML</em>, <em>110</em>(8),
2131–2150. (<a
href="https://doi.org/10.1007/s10994-021-06025-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) has been widely used as an effective technique for feature extraction and dimension reduction. In the High Dimension Low Sample Size setting, one may prefer modified principal components, with penalized loadings, and automated penalty selection by implementing model selection among these different models with varying penalties. The earlier work (Zou et al. in J Comput Graph Stat 15(2):265–286, 2006; Gaynanova et al. in J Comput Graph Stat 26(2):379–387, 2017) has proposed penalized PCA, indicating the feasibility of model selection in $$\ell _2$$ -penalized PCA through the solution path of Ridge regression, however, it is extremely time-consuming because of the intensive calculation of matrix inverse. In this paper, we propose a fast model selection method for penalized PCA, named approximated gradient flow (AgFlow), which lowers the computation complexity through incorporating the implicit regularization effect introduced by (stochastic) gradient flow (Ali et al. in: The 22nd international conference on artificial intelligence and statistics, pp 1370–1378, 2019; Ali et al. in: International conference on machine learning, 2020) and obtains the complete solution path of $$\ell _2$$ -penalized PCA under varying $$\ell _2$$ -regularization. We perform extensive experiments on real-world datasets. AgFlow outperforms existing methods (Oja and Karhunen in J Math Anal Appl 106(1):69–84, 1985; Hardt and Price in: Advances in neural information processing systems, pp 2861–2869, 2014; Shamir in: International conference on machine learning, pp 144–152, PMLR, 2015; and the vanilla Ridge estimators) in terms of computation costs.},
  archive      = {J_ML},
  author       = {Jiang, Haiyan and Xiong, Haoyi and Wu, Dongrui and Liu, Ji and Dou, Dejing},
  doi          = {10.1007/s10994-021-06025-3},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2131-2150},
  shortjournal = {Mach. Learn.},
  title        = {AgFlow: Fast model selection of penalized PCA via implicit regularization effects of gradient flow},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing conditional independence in supervised learning
algorithms. <em>ML</em>, <em>110</em>(8), 2107–2129. (<a
href="https://doi.org/10.1007/s10994-021-06030-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the conditional predictive impact (CPI), a consistent and unbiased estimator of the association between one or several features and a given outcome, conditional on a reduced feature set. Building on the knockoff framework of Candès et al. (J R Stat Soc Ser B 80:551–577, 2018), we develop a novel testing procedure that works in conjunction with any valid knockoff sampler, supervised learning algorithm, and loss function. The CPI can be efficiently computed for high-dimensional data without any sparsity constraints. We demonstrate convergence criteria for the CPI and develop statistical inference procedures for evaluating its magnitude, significance, and precision. These tests aid in feature and model selection, extending traditional frequentist and Bayesian techniques to general supervised learning tasks. The CPI may also be applied in causal discovery to identify underlying multivariate graph structures. We test our method using various algorithms, including linear regression, neural networks, random forests, and support vector machines. Empirical results show that the CPI compares favorably to alternative variable importance measures and other nonparametric tests of conditional independence on a diverse array of real and synthetic datasets. Simulations confirm that our inference procedures successfully control Type I error with competitive power in a range of settings. Our method has been implemented in an R package, cpi, which can be downloaded from https://github.com/dswatson/cpi .},
  archive      = {J_ML},
  author       = {Watson, David S. and Wright, Marvin N.},
  doi          = {10.1007/s10994-021-06030-6},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2107-2129},
  shortjournal = {Mach. Learn.},
  title        = {Testing conditional independence in supervised learning algorithms},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational learning from implicit bandit feedback.
<em>ML</em>, <em>110</em>(8), 2085–2105. (<a
href="https://doi.org/10.1007/s10994-021-06028-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendations are prevalent in Web applications (e.g., search ranking, item recommendation, advertisement placement). Learning from bandit feedback is challenging due to the sparsity of feedback limited to system-provided actions. In this work, we focus on batch learning from logs of recommender systems involving both bandit and organic feedbacks. We develop a probabilistic framework with a likelihood function for estimating not only explicit positive observations but also implicit negative observations inferred from the data. Moreover, we introduce a latent variable model for organic-bandit feedbacks to robustly capture user preference distributions. Next, we analyze the behavior of the new likelihood under two scenarios, i.e., with and without counterfactual re-weighting. For speedier item ranking, we further investigate the possibility of using Maximum-a-Posteriori (MAP) estimate instead of Monte Carlo (MC)-based approximation for prediction. Experiments on both real datasets as well as data from a simulation environment show substantial performance improvements over comparable baselines.},
  archive      = {J_ML},
  author       = {Truong, Quoc-Tuan and Lauw, Hady W.},
  doi          = {10.1007/s10994-021-06028-0},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2085-2105},
  shortjournal = {Mach. Learn.},
  title        = {Variational learning from implicit bandit feedback},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On testing transitivity in online preference learning.
<em>ML</em>, <em>110</em>(8), 2063–2084. (<a
href="https://doi.org/10.1007/s10994-021-06026-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of state-of-the-art algorithms for the dueling bandits problem is essentially due to a clever exploitation of (stochastic) transitivity properties of pairwise comparisons: If one arm is likely to beat a second one, which in turn is likely to beat a third one, then the first is also likely to beat the third one. By now, however, there is no way to test the validity of corresponding assumptions, although this would be a key prerequisite to guarantee the meaningfulness of the results produced by an algorithm. In this paper, we investigate the problem of testing different forms of stochastic transitivity in an online manner. We derive lower bounds on the expected sample complexity of any sequential hypothesis testing algorithm for various forms of stochastic transitivity, thereby providing additional motivation to focus on weak stochastic transitivity. To this end, we introduce an algorithmic framework for the dueling bandits problem, in which the statistical validity of weak stochastic transitivity can be tested, either actively or passively, based on a multiple binomial hypothesis test. Moreover, by exploiting a connection between weak stochastic transitivity and graph theory, we suggest an enhancement to further improve the efficiency of the testing algorithm. In the active setting, both variants achieve an expected sample complexity that is optimal up to a logarithmic factor.},
  archive      = {J_ML},
  author       = {Haddenhorst, Björn and Bengs, Viktor and Hüllermeier, Eyke},
  doi          = {10.1007/s10994-021-06026-2},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2063-2084},
  shortjournal = {Mach. Learn.},
  title        = {On testing transitivity in online preference learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TRU-NET: A deep learning approach to high resolution
prediction of rainfall. <em>ML</em>, <em>110</em>(8), 2035–2062. (<a
href="https://doi.org/10.1007/s10994-021-06022-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate models (CM) are used to evaluate the impact of climate change on the risk of floods and heavy precipitation events. However, these numerical simulators produce outputs with low spatial resolution that exhibit difficulties representing precipitation events accurately. This is mainly due to computational limitations on the spatial resolution used when simulating multi-scale weather dynamics in the atmosphere. To improve the prediction of high resolution precipitation we apply a Deep Learning (DL) approach using input data from a reanalysis product, that is comparable to a climate model’s output, but can be directly related to precipitation observations at a given time and location. Further, our input excludes local precipitation, but includes model fields (weather variables) that are more predictable and generalizable than local precipitation. To this end, we present TRU-NET (Temporal Recurrent U-Net), an encoder-decoder model featuring a novel 2D cross attention mechanism between contiguous convolutional-recurrent layers to effectively model multi-scale spatio-temporal weather processes. We also propose a non-stochastic variant of the conditional-continuous (CC) loss function to capture the zero-skewed patterns of rainfall. Experiments show that our models, trained with our CC loss, consistently attain lower RMSE and MAE scores than a DL model prevalent in precipitation downscaling and outperform a state-of-the-art dynamical weather model. Moreover, by evaluating the performance of our model under various data formulation strategies, for the training and test sets, we show that there is enough data for our deep learning approach to output robust, high-quality results across seasons and varying regions.},
  archive      = {J_ML},
  author       = {Adewoyin, Rilwan A. and Dueben, Peter and Watson, Peter and He, Yulan and Dutta, Ritabrata},
  doi          = {10.1007/s10994-021-06022-6},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2035-2062},
  shortjournal = {Mach. Learn.},
  title        = {TRU-NET: A deep learning approach to high resolution prediction of rainfall},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Triply stochastic gradient method for large-scale nonlinear
similar unlabeled classification. <em>ML</em>, <em>110</em>(8),
2005–2033. (<a
href="https://doi.org/10.1007/s10994-021-05980-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar unlabeled (SU) classification is pervasive in many real-world applications, where only similar data pairs (two data points have the same label) and unlabeled data points are available to train a classifier. Recent work has identified a practical SU formulation and has derived the corresponding estimation error bound. It evaluated SU learning with linear classifiers on medium-sized datasets. However, in practice, we often need to learn nonlinear classifiers on large-scale datasets for superior predictive performance. How this could be done in an efficient manner is still an open problem for SU classification. In this paper, we propose a scalable kernel learning algorithm for SU classification using a triply stochastic optimization framework, called TSGSU. Specifically, in each iteration, our method randomly samples an instance from the similar pairs set, an instance from the unlabeled set, and their random features to calculate the stochastic functional gradient for the model update. Theoretically, we prove that our method can converge to a stationary point at the rate of $$O(1/\sqrt{T})$$ after T iterations. Experiments on various benchmark datasets and high-dimensional datasets not only demonstrate the scalability of TSGSU but also show the efficiency of TSGSU compared with existing SU learning algorithms while retaining similar generalization performance.},
  archive      = {J_ML},
  author       = {Shi, Wanli and Gu, Bin and Li, Xiang and Deng, Cheng and Huang, Heng},
  doi          = {10.1007/s10994-021-05980-1},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {2005-2033},
  shortjournal = {Mach. Learn.},
  title        = {Triply stochastic gradient method for large-scale nonlinear similar unlabeled classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical comparison between stochastic and deterministic
centroid initialisation for k-means variations. <em>ML</em>,
<em>110</em>(8), 1975–2003. (<a
href="https://doi.org/10.1007/s10994-021-06021-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-Means is one of the most used algorithms for data clustering and the usual clustering method for benchmarking. Despite its wide application it is well-known that it suffers from a series of disadvantages; it is only able to find local minima and the positions of the initial clustering centres (centroids) can greatly affect the clustering solution. Over the years many K-Means variations and initialisation techniques have been proposed with different degrees of complexity. In this study we focus on common K-Means variations along with a range of deterministic and stochastic initialisation techniques. We show that, on average, more sophisticated initialisation techniques alleviate the need for complex clustering methods. Furthermore, deterministic methods perform better than stochastic methods. However, there is a trade-off: less sophisticated stochastic methods, executed multiple times, can result in better clustering. Factoring in execution time, deterministic methods can be competitive and result in a good clustering solution. These conclusions are obtained through extensive benchmarking using a range of synthetic model generators and real-world data sets.},
  archive      = {J_ML},
  author       = {Vouros, Avgoustinos and Langdell, Stephen and Croucher, Mike and Vasilaki, Eleni},
  doi          = {10.1007/s10994-021-06021-7},
  journal      = {Machine Learning},
  number       = {8},
  pages        = {1975-2003},
  shortjournal = {Mach. Learn.},
  title        = {An empirical comparison between stochastic and deterministic centroid initialisation for K-means variations},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear support vector regression with linear constraints.
<em>ML</em>, <em>110</em>(7), 1939–1974. (<a
href="https://doi.org/10.1007/s10994-021-06018-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the addition of linear constraints to the Support Vector Regression when the kernel is linear. Adding those constraints into the problem allows to add prior knowledge on the estimator obtained, such as finding positive vector, probability vector or monotone data. We prove that the related optimization problem stays a semi-definite quadratic problem. We also propose a generalization of the Sequential Minimal Optimization algorithm for solving the optimization problem with linear constraints and prove its convergence. We show that an efficient generalization of this iterative algorithm with closed-form updates can be used to obtain the solution of the underlying optimization problem. Then, practical performances of this estimator are shown on simulated and real datasets with different settings: non negative regression, regression onto the simplex for biomedical data and isotonic regression for weather forecast. These experiments show the usefulness of this estimator in comparison to more classical approaches.},
  archive      = {J_ML},
  author       = {Klopfenstein, Quentin and Vaiter, Samuel},
  doi          = {10.1007/s10994-021-06018-2},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1939-1974},
  shortjournal = {Mach. Learn.},
  title        = {Linear support vector regression with linear constraints},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint optimization of an autoencoder for clustering and
embedding. <em>ML</em>, <em>110</em>(7), 1901–1937. (<a
href="https://doi.org/10.1007/s10994-021-06015-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep embedded clustering has become a dominating approach to unsupervised categorization of objects with deep neural networks. The optimization of the most popular methods alternates between the training of a deep autoencoder and a k-means clustering of the autoencoder’s embedding. The diachronic setting, however, prevents the former to benefit from valuable information acquired by the latter. In this paper, we present an alternative where the autoencoder and the clustering are learned simultaneously. This is achieved by providing novel theoretical insight, where we show that the objective function of a certain class of Gaussian mixture models (GMM’s) can naturally be rephrased as the loss function of a one-hidden layer autoencoder thus inheriting the built-in clustering capabilities of the GMM. That simple neural network, referred to as the clustering module, can be integrated into a deep autoencoder resulting in a deep clustering model able to jointly learn a clustering and an embedding. Experiments confirm the equivalence between the clustering module and Gaussian mixture models. Further evaluations affirm the empirical relevance of our deep architecture as it outperforms related baselines on several data sets.},
  archive      = {J_ML},
  author       = {Boubekki, Ahcène and Kampffmeyer, Michael and Brefeld, Ulf and Jenssen, Robert},
  doi          = {10.1007/s10994-021-06015-5},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1901-1937},
  shortjournal = {Mach. Learn.},
  title        = {Joint optimization of an autoencoder for clustering and embedding},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor q-rank: New data dependent definition of tensor rank.
<em>ML</em>, <em>110</em>(7), 1867–1900. (<a
href="https://doi.org/10.1007/s10994-021-05987-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the $${ Tensor}~{ Nuclear}~{ Norm}~{ (TNN)}$$ regularization based on t-SVD has been widely used in various low tubal-rank tensor recovery tasks. However, these models usually require smooth change of data along the third dimension to ensure their low rank structures. In this paper, we propose a new definition of data dependent tensor rank named tensor Q-rank by a learnable orthogonal matrix $$\mathbf {Q}$$ , and further introduce a unified data dependent low rank tensor recovery model. According to the low rank hypothesis, we introduce two explainable selection methods of $$\mathbf {Q}$$ , under which the data tensor may have a more significant low tensor Q-rank structure than that of low tubal-rank structure. Specifically, maximizing the variance of singular value distribution leads to Variance Maximization Tensor Q-Nuclear norm (VMTQN), while minimizing the value of nuclear norm through manifold optimization leads to Manifold Optimization Tensor Q-Nuclear norm (MOTQN). Moreover, we apply these two models to the low rank tensor completion problem, and then give an effective algorithm and briefly analyze why our method works better than TNN based methods in the case of complex data with low sampling rate. Finally, experimental results on real-world datasets demonstrate the superiority of our proposed models in the tensor completion problem with respect to other tensor rank regularization models.},
  archive      = {J_ML},
  author       = {Kong, Hao and Lu, Canyi and Lin, Zhouchen},
  doi          = {10.1007/s10994-021-05987-8},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1867-1900},
  shortjournal = {Mach. Learn.},
  title        = {Tensor Q-rank: New data dependent definition of tensor rank},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of statistical relational learning and graph
neural networks for aggregate graph queries. <em>ML</em>,
<em>110</em>(7), 1847–1866. (<a
href="https://doi.org/10.1007/s10994-021-06007-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical relational learning (SRL) and graph neural networks (GNNs) are two powerful approaches for learning and inference over graphs. Typically, they are evaluated in terms of simple metrics such as accuracy over individual node labels. Complex aggregate graph queries (AGQ) involving multiple nodes, edges, and labels are common in the graph mining community and are used to estimate important network properties such as social cohesion and influence. While graph mining algorithms support AGQs, they typically do not take into account uncertainty, or when they do, make simplifying assumptions and do not build full probabilistic models. In this paper, we examine the performance of SRL and GNNs on AGQs over graphs with partially observed node labels. We show that, not surprisingly, inferring the unobserved node labels as a first step and then evaluating the queries on the fully observed graph can lead to sub-optimal estimates, and that a better approach is to compute these queries as an expectation under the joint distribution. We propose a sampling framework to tractably compute the expected values of AGQs. Motivated by the analysis of subgroup cohesion in social networks, we propose a suite of AGQs that estimate the community structure in graphs. In our empirical evaluation, we show that by estimating these queries as an expectation, SRL-based approaches yield up to a 50-fold reduction in average error when compared to existing GNN-based approaches.},
  archive      = {J_ML},
  author       = {Embar, Varun and Srinivasan, Sriram and Getoor, Lise},
  doi          = {10.1007/s10994-021-06007-5},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1847-1866},
  shortjournal = {Mach. Learn.},
  title        = {A comparison of statistical relational learning and graph neural networks for aggregate graph queries},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OWL2Vec*: Embedding of OWL ontologies. <em>ML</em>,
<em>110</em>(7), 1813–1845. (<a
href="https://doi.org/10.1007/s10994-021-05997-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic embedding of knowledge graphs has been widely studied and used for prediction and statistical analysis tasks across various domains such as Natural Language Processing and the Semantic Web. However, less attention has been paid to developing robust methods for embedding OWL (Web Ontology Language) ontologies, which contain richer semantic information than plain knowledge graphs, and have been widely adopted in domains such as bioinformatics. In this paper, we propose a random walk and word embedding based ontology embedding method named OWL2Vec*, which encodes the semantics of an OWL ontology by taking into account its graph structure, lexical information and logical constructors. Our empirical evaluation with three real world datasets suggests that OWL2Vec* benefits from these three different aspects of an ontology in class membership prediction and class subsumption prediction tasks. Furthermore, OWL2Vec* often significantly outperforms the state-of-the-art methods in our experiments.},
  archive      = {J_ML},
  author       = {Chen, Jiaoyan and Hu, Pan and Jimenez-Ruiz, Ernesto and Holter, Ole Magnus and Antonyrajah, Denvar and Horrocks, Ian},
  doi          = {10.1007/s10994-021-05997-6},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1813-1845},
  shortjournal = {Mach. Learn.},
  title        = {OWL2Vec*: Embedding of OWL ontologies},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distance metric learning for graph structured data.
<em>ML</em>, <em>110</em>(7), 1765–1811. (<a
href="https://doi.org/10.1007/s10994-021-06009-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are versatile tools for representing structured data. As a result, a variety of machine learning methods have been studied for graph data analysis. Although many such learning methods depend on the measurement of differences between input graphs, defining an appropriate distance metric for graphs remains a controversial issue. Hence, we propose a supervised distance metric learning method for the graph classification problem. Our method, named interpretable graph metric learning (IGML), learns discriminative metrics in a subgraph-based feature space, which has a strong graph representation capability. By introducing a sparsity-inducing penalty on the weight of each subgraph, IGML can identify a small number of important subgraphs that can provide insight into the given classification task. Because our formulation has a large number of optimization variables, an efficient algorithm that uses pruning techniques based on safe screening and working set selection methods is also proposed. An important property of IGML is that solution optimality is guaranteed because the problem is formulated as a convex problem and our pruning strategies only discard unnecessary subgraphs. Furthermore, we show that IGML is also applicable to other structured data such as itemset and sequence data, and that it can incorporate vertex-label similarity by using a transportation-based subgraph feature. We empirically evaluate the computational efficiency and classification performance of IGML on several benchmark datasets and provide some illustrative examples of how IGML identifies important subgraphs from a given graph dataset.},
  archive      = {J_ML},
  author       = {Yoshida, Tomoki and Takeuchi, Ichiro and Karasuyama, Masayuki},
  doi          = {10.1007/s10994-021-06009-3},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1765-1811},
  shortjournal = {Mach. Learn.},
  title        = {Distance metric learning for graph structured data},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inductive learning of answer set programs for autonomous
surgical task planning. <em>ML</em>, <em>110</em>(7), 1739–1763. (<a
href="https://doi.org/10.1007/s10994-021-06013-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of robot-assisted surgery can be improved and the use of hospital resources can be optimized by enhancing autonomy and reliability in the robot’s operation. Logic programming is a good choice for task planning in robot-assisted surgery because it supports reliable reasoning with domain knowledge and increases transparency in the decision making. However, prior knowledge of the task and the domain is typically incomplete, and it often needs to be refined from executions of the surgical task(s) under consideration to avoid sub-optimal performance. In this paper, we investigate the applicability of inductive logic programming for learning previously unknown axioms governing domain dynamics. We do so under answer set semantics for a benchmark surgical training task, the ring transfer. We extend our previous work on learning the immediate preconditions of actions and constraints, to also learn axioms encoding arbitrary temporal delays between atoms that are effects of actions under the event calculus formalism. We propose a systematic approach for learning the specifications of a generic robotic task under the answer set semantics, allowing easy knowledge refinement with iterative learning. In the context of 1000 simulated scenarios, we demonstrate the significant improvement in performance obtained with the learned axioms compared with the hand-written ones; specifically, the learned axioms address some critical issues related to the plan computation time, which is promising for reliable real-time performance during surgery.},
  archive      = {J_ML},
  author       = {Meli, Daniele and Sridharan, Mohan and Fiorini, Paolo},
  doi          = {10.1007/s10994-021-06013-7},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1739-1763},
  shortjournal = {Mach. Learn.},
  title        = {Inductive learning of answer set programs for autonomous surgical task planning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond graph neural networks with lifted relational neural
networks. <em>ML</em>, <em>110</em>(7), 1695–1738. (<a
href="https://doi.org/10.1007/s10994-021-06017-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a declarative differentiable programming framework, based on the language of Lifted Relational Neural Networks, where small parameterized logic programs are used to encode deep relational learning scenarios through the underlying symmetries. When presented with relational data, such as various forms of graphs, the logic program interpreter dynamically unfolds differentiable computation graphs to be used for the program parameter optimization by standard means. Following from the declarative, relational logic-based encoding, this results into a unified representation of a wide range of neural models in the form of compact and elegant learning programs, in contrast to the existing procedural approaches operating directly on the computational graph level. We illustrate how this idea can be used for a concise encoding of existing advanced neural architectures, with the main focus on Graph Neural Networks (GNNs). Importantly, using the framework, we also show how the contemporary GNN models can be easily extended towards higher expressiveness in various ways. In the experiments, we demonstrate correctness and computation efficiency through comparison against specialized GNN frameworks, while shedding some light on the learning performance of the existing GNN models.},
  archive      = {J_ML},
  author       = {Šourek, Gustav and Železný, Filip and Kuželka, Ondřej},
  doi          = {10.1007/s10994-021-06017-3},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1695-1738},
  shortjournal = {Mach. Learn.},
  title        = {Beyond graph neural networks with lifted relational neural networks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning hierarchical probabilistic logic programs.
<em>ML</em>, <em>110</em>(7), 1637–1693. (<a
href="https://doi.org/10.1007/s10994-021-06016-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic logic programming (PLP) combines logic programs and probabilities. Due to its expressiveness and simplicity, it has been considered as a powerful tool for learning and reasoning in relational domains characterized by uncertainty. Still, learning the parameter and the structure of general PLP is computationally expensive due to the inference cost. We have recently proposed a restriction of the general PLP language called hierarchical PLP (HPLP) in which clauses and predicates are hierarchically organized. HPLPs can be converted into arithmetic circuits or deep neural networks and inference is much cheaper than for general PLP. In this paper we present algorithms for learning both the parameters and the structure of HPLPs from data. We first present an algorithm, called parameter learning for hierarchical probabilistic logic programs (PHIL) which performs parameter estimation of HPLPs using gradient descent and expectation maximization. We also propose structure learning of hierarchical probabilistic logic programming (SLEAHP), that learns both the structure and the parameters of HPLPs from data. Experiments were performed comparing PHIL and SLEAHP with PLP and Markov Logic Networks state-of-the art systems for parameter and structure learning respectively. PHIL was compared with EMBLEM, ProbLog2 and Tuffy and SLEAHP with SLIPCOVER, PROBFOIL+, MLB-BC, MLN-BT and RDN-B. The experiments on five well known datasets show that our algorithms achieve similar and often better accuracies but in a shorter time.},
  archive      = {J_ML},
  author       = {Nguembang Fadja, Arnaud and Riguzzi, Fabrizio and Lamma, Evelina},
  doi          = {10.1007/s10994-021-06016-4},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1637-1693},
  shortjournal = {Mach. Learn.},
  title        = {Learning hierarchical probabilistic logic programs},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating symbolic domain knowledge into graph neural
networks. <em>ML</em>, <em>110</em>(7), 1609–1636. (<a
href="https://doi.org/10.1007/s10994-021-05966-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our interest is in scientific problems with the following characteristics: (1) Data are naturally represented as graphs; (2) The amount of data available is typically small; and (3) There is significant domain-knowledge, usually expressed in some symbolic form (rules, taxonomies, constraints and the like). These kinds of problems have been addressed effectively in the past by symbolic machine learning methods like Inductive Logic Programming (ILP), by virtue of 2 important characteristics: (a) The use of a representation language that easily captures the relation encoded in graph-structured data, and (b) The inclusion of prior information encoded as domain-specific relations, that can alleviate problems of data scarcity, and construct new relations. Recent advances have seen the emergence of deep neural networks specifically developed for graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have been shown to be able to handle graph-structured data, less has been done to investigate the inclusion of domain-knowledge. Here we investigate this aspect of GNNs empirically by employing an operation we term vertex-enrichment and denote the corresponding GNNs as VEGNNs. Using over 70 real-world datasets and substantial amounts of symbolic domain-knowledge, we examine the result of vertex-enrichment across 5 different variants of GNNs. Our results provide support for the following: (a) Inclusion of domain-knowledge by vertex-enrichment can significantly improve the performance of a GNN. That is, the performance of VEGNNs is significantly better than GNNs across all GNN variants; (b) The inclusion of domain-specific relations constructed using ILP improves the performance of VEGNNs, across all GNN variants. Taken together, the results provide evidence that it is possible to incorporate symbolic domain knowledge into a GNN, and that ILP can play an important role in providing high-level relationships that are not easily discovered by a GNN.},
  archive      = {J_ML},
  author       = {Dash, Tirtharaj and Srinivasan, Ashwin and Vig, Lovekesh},
  doi          = {10.1007/s10994-021-05966-z},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1609-1636},
  shortjournal = {Mach. Learn.},
  title        = {Incorporating symbolic domain knowledge into graph neural networks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning subtree pattern importance for weisfeiler-lehman
based graph kernels. <em>ML</em>, <em>110</em>(7), 1585–1607. (<a
href="https://doi.org/10.1007/s10994-021-05991-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph is an usual representation of relational data, which are ubiquitous in many domains such as molecules, biological and social networks. A popular approach to learning with graph structured data is to make use of graph kernels, which measure the similarity between graphs and are plugged into a kernel machine such as a support vector machine. Weisfeiler-Lehman (WL) based graph kernels, which employ WL labeling scheme to extract subtree patterns and perform node embedding, are demonstrated to achieve great performance while being efficiently computable. However, one of the main drawbacks of a general kernel is the decoupling of kernel construction and learning process. For molecular graphs, usual kernels such as WL subtree, based on substructures of the molecules, consider all available substructures having the same importance, which might not be suitable in practice. In this paper, we propose a method to learn the weights of subtree patterns in the framework of WWL kernels, the state of the art method for graph classification task (Togninalli et al., in: Advances in Neural Information Processing Systems, pp. 6439–6449, 2019). To overcome the computational issue on large scale data sets, we present an efficient learning algorithm and also derive a generalization gap bound to show its convergence. Finally, through experiments on synthetic and real-world data sets, we demonstrate the effectiveness of our proposed method for learning the weights of subtree patterns.},
  archive      = {J_ML},
  author       = {Nguyen, Dai Hai and Nguyen, Canh Hao and Mamitsuka, Hiroshi},
  doi          = {10.1007/s10994-021-05991-y},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1585-1607},
  shortjournal = {Mach. Learn.},
  title        = {Learning subtree pattern importance for weisfeiler-lehman based graph kernels},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal data collection design in machine learning: The case
of the fixed effects generalized least squares panel data model.
<em>ML</em>, <em>110</em>(7), 1549–1584. (<a
href="https://doi.org/10.1007/s10994-021-05976-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work belongs to the strand of literature that combines machine learning, optimization, and econometrics. The aim is to optimize the data collection process in a specific statistical model, commonly used in econometrics, employing an optimization criterion inspired by machine learning, namely, the generalization error conditioned on the training input data. More specifically, the paper is focused on the analysis of the conditional generalization error of the Fixed Effects Generalized Least Squares (FEGLS) panel data model, i.e., a linear regression model with applications in several fields, able to represent unobserved heterogeneity in the data associated with different units, for which distinct observations related to the same unit are corrupted by correlated measurement errors. The framework considered in this work differs from the classical FEGLS model for the additional possibility of controlling the conditional variance of the output variable given the associated unit and input variables, by changing the cost per supervision of each training example. Assuming an upper bound on the total supervision cost, i.e., the cost associated with the whole training set, the trade-off between the training set size and the precision of supervision (i.e., the reciprocal of the conditional variance of the output variable) is analyzed and optimized. This is achieved by formulating and solving in closed form suitable optimization problems, based on large-sample approximations of the generalization error associated with the FEGLS estimates of the model parameters, conditioned on the training input data. The results of the analysis extend to the FEGLS case and to various large-sample approximations of its conditional generalization error the ones obtained by the authors in recent works for simpler linear regression models. They highlight the importance of how the precision of supervision scales with respect to the cost per training example in determining the optimal trade-off between training set size and precision. Numerical results confirm the validity of the theoretical findings.},
  archive      = {J_ML},
  author       = {Gnecco, Giorgio and Nutarelli, Federico and Selvi, Daniela},
  doi          = {10.1007/s10994-021-05976-x},
  journal      = {Machine Learning},
  number       = {7},
  pages        = {1549-1584},
  shortjournal = {Mach. Learn.},
  title        = {Optimal data collection design in machine learning: The case of the fixed effects generalized least squares panel data model},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MODES: Model-based optimization on distributed embedded
systems. <em>ML</em>, <em>110</em>(6), 1527–1547. (<a
href="https://doi.org/10.1007/s10994-021-06014-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The predictive performance of a machine learning model highly depends on the corresponding hyper-parameter setting. Hence, hyper-parameter tuning is often indispensable. Normally such tuning requires the dedicated machine learning model to be trained and evaluated on centralized data to obtain a performance estimate. However, in a distributed machine learning scenario, it is not always possible to collect all the data from all nodes due to privacy concerns or storage limitations. Moreover, if data has to be transferred through low bandwidth connections it reduces the time available for tuning. Model-Based Optimization (MBO) is one state-of-the-art method for tuning hyper-parameters but the application on distributed machine learning models or federated learning lacks research. This work proposes a framework $$\textit{MODES}$$ that allows to deploy MBO on resource-constrained distributed embedded systems. Each node trains an individual model based on its local data. The goal is to optimize the combined prediction accuracy. The presented framework offers two optimization modes: (1) $$\textit{MODES}$$ -B considers the whole ensemble as a single black box and optimizes the hyper-parameters of each individual model jointly, and (2) $$\textit{MODES}$$ -I considers all models as clones of the same black box which allows it to efficiently parallelize the optimization in a distributed setting. We evaluate $$\textit{MODES}$$ by conducting experiments on the optimization for the hyper-parameters of a random forest and a multi-layer perceptron. The experimental results demonstrate that, with an improvement in terms of mean accuracy ( $$\textit{MODES}$$ -B), run-time efficiency ( $$\textit{MODES}$$ -I), and statistical stability for both modes, $$\textit{MODES}$$ outperforms the baseline, i.e., carry out tuning with MBO on each node individually with its local sub-data set.},
  archive      = {J_ML},
  author       = {Shi, Junjie and Bian, Jiang and Richter, Jakob and Chen, Kuan-Hsun and Rahnenführer, Jörg and Xiong, Haoyi and Chen, Jian-Jia},
  doi          = {10.1007/s10994-021-06014-6},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1527-1547},
  shortjournal = {Mach. Learn.},
  title        = {MODES: Model-based optimization on distributed embedded systems},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple clusterings of heterogeneous information networks.
<em>ML</em>, <em>110</em>(6), 1505–1526. (<a
href="https://doi.org/10.1007/s10994-021-06000-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional clustering algorithms focus on a single clustering result; as such, they cannot explore potential diverse patterns of complex real world data. To deal with this problem, approaches that exploit meaningful alternative clusterings in data have been developed in recent years. Existing algorithms, including single view/multi-view multiple clustering methods, are designed for applications with i.i.d. data samples, and cannot handle the data samples with dependency presented in networks, especially in heterogeneous information networks (HIN). In this paper, we propose a framework (NetMCs) that can explore multiple clusterings in HIN. Specifically, NetMCs adopts a set of meta-path schemes with different semantics on HIN, and considers each meta-path scheme as a base clustering aspect. Guided by the meta-path schemes, NetMCs then introduces a variation of the skip-gram framework that can jointly optimize multiple clustering aspects, and simultaneously obtain the respective embedding representations and individual clusterings therein. To reduce redundancy between alternative clusterings, NetMCs utilizes an explicit regularization term to control the embedding diversity of the same nodes among different clustering aspects. Experiments on benchmark HIN datasets confirm the performance of NetMCs in generating multiple clusterings with high quality and diversity.},
  archive      = {J_ML},
  author       = {Wei, Shaowei and Yu, Guoxian and Wang, Jun and Domeniconi, Carlotta and Zhang, Xiangliang},
  doi          = {10.1007/s10994-021-06000-y},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1505-1526},
  shortjournal = {Mach. Learn.},
  title        = {Multiple clusterings of heterogeneous information networks},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Early classification of time series. <em>ML</em>,
<em>110</em>(6), 1481–1504. (<a
href="https://doi.org/10.1007/s10994-021-05974-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of applications require to recognize the class of an incoming time series as quickly as possible without unduly compromising the accuracy of the prediction. In this paper, we put forward a new optimization criterion which takes into account both the cost of misclassification and the cost of delaying the decision. Based on this optimization criterion, we derived a family of non-myopic algorithms which try to anticipate the expected future gain in information in balance with the cost of waiting. In one class of algorithms, unsupervised-based, the expectations use the clustering of time series, while in a second class, supervised-based, time series are grouped according to the confidence level of the classifier used to label them. Extensive experiments carried out on real datasets using a large range of delay cost functions show that the presented algorithms are able to solve the earliness vs. accuracy trade-off, with the supervised partition based approaches faring better than the unsupervised partition based ones. In addition, all these methods perform better in a wide variety of conditions than a state of the art method based on a myopic strategy which is recognized as being very competitive. Furthermore, our experiments show that the non-myopic feature of the proposed approaches explains in large part the obtained performances.},
  archive      = {J_ML},
  author       = {Achenchabe, Youssef and Bondu, Alexis and Cornuéjols, Antoine and Dachraoui, Asma},
  doi          = {10.1007/s10994-021-05974-z},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1481-1504},
  shortjournal = {Mach. Learn.},
  title        = {Early classification of time series},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of multidimensional item response theory models
with correlated latent variables using variational autoencoders.
<em>ML</em>, <em>110</em>(6), 1463–1480. (<a
href="https://doi.org/10.1007/s10994-021-06005-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks with a specific autoencoding structure are capable of estimating parameters for the multidimensional logistic 2-parameter (ML2P) model in item response theory (Curi et al. in International joint conference on neural networks (IJCNN), 2019), but with limitations, such as uncorrelated latent traits. In this work, we extend variational auto encoders (VAE) to estimate item parameters and correlated latent abilities, and directly compare the ML2P-VAE method to more traditional parameter estimation methods, such as Monte Carlo expectation-maximization. The incorporation of a non-identity covariance matrix in a VAE requires a novel VAE architecture, which can be utilized in applications outside of education. In addition, we show that the ML2P-VAE method is capable of estimating parameters for models with a large number of latent variables with low computational cost, where traditional methods are infeasible for data with high-dimensional latent traits.},
  archive      = {J_ML},
  author       = {Converse, Geoffrey and Curi, Mariana and Oliveira, Suely and Templin, Jonathan},
  doi          = {10.1007/s10994-021-06005-7},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1463-1480},
  shortjournal = {Mach. Learn.},
  title        = {Estimation of multidimensional item response theory models with correlated latent variables using variational autoencoders},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated adaptation strategies for stream learning.
<em>ML</em>, <em>110</em>(6), 1429–1462. (<a
href="https://doi.org/10.1007/s10994-021-05992-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies when multiple strategies are available. Manually developing an adaptation strategy can be time consuming and costly. In this paper we address this issue by proposing the use of flexible adaptive mechanism deployment for automated development of adaptation strategies. Experimental results after using the proposed strategies with five adaptive algorithms on 36 datasets confirm their viability. These strategies achieve better or comparable performance to the custom adaptation strategies and the repeated deployment of any single adaptive mechanism.},
  archive      = {J_ML},
  author       = {Bakirov, Rashid and Fay, Damien and Gabrys, Bogdan},
  doi          = {10.1007/s10994-021-05992-x},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1429-1462},
  shortjournal = {Mach. Learn.},
  title        = {Automated adaptation strategies for stream learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for the fine-grained evaluation of the
instantaneous expected value of soccer possessions. <em>ML</em>,
<em>110</em>(6), 1389–1427. (<a
href="https://doi.org/10.1007/s10994-021-05989-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected possession value (EPV) of a soccer possession represents the likelihood of a team scoring or conceding the next goal at any time instance. In this work, we develop a comprehensive analysis framework for the EPV, providing soccer practitioners with the ability to evaluate the impact of observed and potential actions, both visually and analytically. The EPV expression is decomposed into a series of subcomponents that model the influence of passes, ball drives and shot actions on the expected outcome of a possession. We show we can learn from spatiotemporal tracking data and obtain calibrated models for all the components of the EPV. For the components related with passes, we produce visually-interpretable probability surfaces from a series of deep neural network architectures built on top of flexible representations of game states. Additionally, we present a series of novel practical applications providing coaches with an enriched interpretation of specific game situations. This is, to our knowledge, the first EPV approach in soccer that uses this decomposition and incorporates the dynamics of the 22 players and the ball through tracking data.},
  archive      = {J_ML},
  author       = {Fernández, Javier and Bornn, Luke and Cervone, Daniel},
  doi          = {10.1007/s10994-021-05989-6},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1389-1427},
  shortjournal = {Mach. Learn.},
  title        = {A framework for the fine-grained evaluation of the instantaneous expected value of soccer possessions},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based semi-supervised learning via improving the
quality of the graph dynamically. <em>ML</em>, <em>110</em>(6),
1345–1388. (<a
href="https://doi.org/10.1007/s10994-021-05975-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semi-supervised learning (GSSL) is an important paradigm among semi-supervised learning approaches and includes the two processes of graph construction and label inference. In most traditional GSSL methods, the two processes are completed independently. Once the graph is constructed, the result of label inference cannot be changed. Therefore, the quality of the graph directly determines the GSSL’s performance. Most traditional graph construction methods make certain assumptions about the data distribution, resulting in the quality of the graph heavily depends on the correctness of these assumptions. Therefore, it is difficult to handle complex and various data distribution for traditional graph construction methods. To overcome such issues, this paper proposes a framework named Graph-based Semi-supervised Learning via Improving the Quality of the Graph Dynamically. In it, the graph construction based on the weighted fusion of multiple clustering results and the label inference are integrated into a unified framework to achieve their mutual guidance and dynamic improvement. Moreover, the proposed framework is a general framework, and most existing GSSL methods can be embedded into it so as to improve their performance. Finally, the working mechanism, the effectiveness in improving the performance of GSSL methods and the advantage compared with other GSSL methods based on dynamic graph construction methods of the proposal are verified through systematic experiments.},
  archive      = {J_ML},
  author       = {Liang, Jiye and Cui, Junbiao and Wang, Jie and Wei, Wei},
  doi          = {10.1007/s10994-021-05975-y},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1345-1388},
  shortjournal = {Mach. Learn.},
  title        = {Graph-based semi-supervised learning via improving the quality of the graph dynamically},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient weingarten map and curvature estimation on
manifolds. <em>ML</em>, <em>110</em>(6), 1319–1344. (<a
href="https://doi.org/10.1007/s10994-021-05953-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient method to estimate the Weingarten map for point cloud data sampled from manifold embedded in Euclidean space. A statistical model is established to analyze the asymptotic property of the estimator. In particular, we show the convergence rate as the sample size tends to infinity. We verify the convergence rate through simulated data and apply the estimated Weingarten map to curvature estimation and point cloud simplification to multiple real data sets.},
  archive      = {J_ML},
  author       = {Cao, Yueqi and Li, Didong and Sun, Huafei and Assadi, Amir H. and Zhang, Shiqiang},
  doi          = {10.1007/s10994-021-05953-4},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1319-1344},
  shortjournal = {Mach. Learn.},
  title        = {Efficient weingarten map and curvature estimation on manifolds},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Importance sampling in reinforcement learning with an
estimated behavior policy. <em>ML</em>, <em>110</em>(6), 1267–1317. (<a
href="https://doi.org/10.1007/s10994-020-05938-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reinforcement learning, importance sampling is a widely used method for evaluating an expectation under the distribution of data of one policy when the data has in fact been generated by a different policy. Importance sampling requires computing the likelihood ratio between the action probabilities of a target policy and those of the data-producing behavior policy. In this article, we study importance sampling where the behavior policy action probabilities are replaced by their maximum likelihood estimate of these probabilities under the observed data. We show this general technique reduces variance due to sampling error in Monte Carlo style estimators. We introduce two novel estimators that use this technique to estimate expected values that arise in the RL literature. We find that these general estimators reduce the variance of Monte Carlo sampling methods, leading to faster learning for policy gradient algorithms and more accurate off-policy policy evaluation. We also provide theoretical analysis showing that our new estimators are consistent and have asymptotically lower variance than Monte Carlo estimators.},
  archive      = {J_ML},
  author       = {Hanna, Josiah P. and Niekum, Scott and Stone, Peter},
  doi          = {10.1007/s10994-020-05938-9},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1267-1317},
  shortjournal = {Mach. Learn.},
  title        = {Importance sampling in reinforcement learning with an estimated behavior policy},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-objective multi-armed bandit with lexicographically
ordered and satisficing objectives. <em>ML</em>, <em>110</em>(6),
1233–1266. (<a
href="https://doi.org/10.1007/s10994-021-05956-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multi-objective multi-armed bandit with (i) lexicographically ordered and (ii) satisficing objectives. In the first problem, the goal is to select arms that are lexicographic optimal as much as possible without knowing the arm reward distributions beforehand. We capture this goal by defining a multi-dimensional form of regret that measures the loss due to not selecting lexicographic optimal arms, and then, propose an algorithm that achieves $${\tilde{O}}(T^{2/3})$$ gap-free regret and prove a regret lower bound of $$\varOmega (T^{2/3})$$ . We also consider two additional settings where the learner has prior information on the expected arm rewards. In the first setting, the learner only knows for each objective the lexicographic optimal expected reward. In the second setting, it only knows for each objective a near-lexicographic optimal expected reward. For both settings, we prove that the learner achieves expected regret uniformly bounded in time. Then, we show that the algorithm we propose for the second setting of lexicographically ordered objectives with prior information also attains bounded regret for satisficing objectives. Finally, we experimentally evaluate the proposed algorithms in a variety of multi-objective learning problems.},
  archive      = {J_ML},
  author       = {Hüyük, Alihan and Tekin, Cem},
  doi          = {10.1007/s10994-021-05956-1},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1233-1266},
  shortjournal = {Mach. Learn.},
  title        = {Multi-objective multi-armed bandit with lexicographically ordered and satisficing objectives},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward optimal probabilistic active learning using a
bayesian approach. <em>ML</em>, <em>110</em>(6), 1199–1231. (<a
href="https://doi.org/10.1007/s10994-021-05986-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gathering labeled data to train well-performing machine learning models is one of the critical challenges in many applications. Active learning aims at reducing the labeling costs by an efficient and effective allocation of costly labeling resources. In this article, we propose a decision-theoretic selection strategy that (1) directly optimizes the gain in misclassification error, and (2) uses a Bayesian approach by introducing a conjugate prior distribution to determine the class posterior to deal with uncertainties. By reformulating existing selection strategies within our proposed model, we can explain which aspects are not covered in current state-of-the-art and why this leads to the superior performance of our approach. Extensive experiments on a large variety of datasets and different kernels validate our claims.},
  archive      = {J_ML},
  author       = {Kottke, Daniel and Herde, Marek and Sandrock, Christoph and Huseljic, Denis and Krempl, Georg and Sick, Bernhard},
  doi          = {10.1007/s10994-021-05986-9},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1199-1231},
  shortjournal = {Mach. Learn.},
  title        = {Toward optimal probabilistic active learning using a bayesian approach},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reachable sets of classifiers and regression models:
(Non-)robustness analysis and robust training. <em>ML</em>,
<em>110</em>(6), 1175–1197. (<a
href="https://doi.org/10.1007/s10994-021-05973-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks achieve outstanding accuracy in classification and regression tasks. However, understanding their behavior still remains an open challenge that requires questions to be addressed on the robustness, explainability and reliability of predictions. We answer these questions by computing reachable sets of neural networks, i.e. sets of outputs resulting from continuous sets of inputs. We provide two efficient approaches that lead to over- and under-approximations of the reachable set. This principle is highly versatile, as we show. First, we use it to analyze and enhance the robustness properties of both classifiers and regression models. This is in contrast to existing works, which are mainly focused on classification. Specifically, we verify (non-)robustness, propose a robust training procedure, and show that our approach outperforms adversarial attacks as well as state-of-the-art methods of verifying classifiers for non-norm bound perturbations. Second, we provide techniques to distinguish between reliable and non-reliable predictions for unlabeled inputs, to quantify the influence of each feature on a prediction, and compute a feature ranking.},
  archive      = {J_ML},
  author       = {Kopetzki, Anna-Kathrin and Günnemann, Stephan},
  doi          = {10.1007/s10994-021-05973-0},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1175-1197},
  shortjournal = {Mach. Learn.},
  title        = {Reachable sets of classifiers and regression models: (non-)robustness analysis and robust training},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of regularized least-squares in reproducing kernel
kreĭn spaces. <em>ML</em>, <em>110</em>(6), 1145–1173. (<a
href="https://doi.org/10.1007/s10994-021-05955-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the asymptotic properties of regularized least squares with indefinite kernels in reproducing kernel Kreĭn spaces (RKKS). By introducing a bounded hyper-sphere constraint to such non-convex regularized risk minimization problem, we theoretically demonstrate that this problem has a globally optimal solution with a closed form on the sphere, which makes approximation analysis feasible in RKKS. Regarding to the original regularizer induced by the indefinite inner product, we modify traditional error decomposition techniques, prove convergence results for the introduced hypothesis error based on matrix perturbation theory, and derive learning rates of such regularized regression problem in RKKS. Under some conditions, the derived learning rates in RKKS are the same as that in reproducing kernel Hilbert spaces (RKHS). To the best of our knowledge, this is the first work on approximation analysis of regularized learning algorithms in RKKS.},
  archive      = {J_ML},
  author       = {Liu, Fanghui and Shi, Lei and Huang, Xiaolin and Yang, Jie and Suykens, Johan A. K.},
  doi          = {10.1007/s10994-021-05955-2},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1145-1173},
  shortjournal = {Mach. Learn.},
  title        = {Analysis of regularized least-squares in reproducing kernel kreĭn spaces},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo-marginal bayesian inference for gaussian process
latent variable models. <em>ML</em>, <em>110</em>(6), 1105–1143. (<a
href="https://doi.org/10.1007/s10994-021-05971-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian inference framework for supervised Gaussian process latent variable models is introduced. The framework overcomes the high correlations between latent variables and hyperparameters by collapsing the statistical model through approximate integration of the latent variables. Using an unbiased pseudo estimate for the marginal likelihood, the exact hyperparameter posterior can then be explored using collapsed Gibbs sampling and, conditional on these samples, the exact latent posterior can be explored through elliptical slice sampling. The framework is tested on both simulated and real examples. When compared with the standard approach based on variational inference, this approach leads to significant improvements in the predictive accuracy and quantification of uncertainty, as well as a deeper insight into the challenges of performing inference in this class of models.},
  archive      = {J_ML},
  author       = {Gadd, C. and Wade, S. and Shah, A. A.},
  doi          = {10.1007/s10994-021-05971-2},
  journal      = {Machine Learning},
  number       = {6},
  pages        = {1105-1143},
  shortjournal = {Mach. Learn.},
  title        = {Pseudo-marginal bayesian inference for gaussian process latent variable models},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive covariate acquisition for minimizing total cost of
classification. <em>ML</em>, <em>110</em>(5), 1067–1104. (<a
href="https://doi.org/10.1007/s10994-021-05958-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some applications, acquiring covariates comes at a cost which is not negligible. For example in the medical domain, in order to classify whether a patient has diabetes or not, measuring glucose tolerance can be expensive. Assuming that the cost of each covariate, and the cost of misclassification can be specified by the user, our goal is to minimize the (expected) total cost of classification, i.e. the cost of misclassification plus the cost of the acquired covariates. We formalize this optimization goal using the (conditional) Bayes risk and describe the optimal solution using a recursive procedure. Since the procedure is computationally infeasible, we consequently introduce two assumptions: (1) the optimal classifier can be represented by a generalized additive model, (2) the optimal sets of covariates are limited to a sequence of sets of increasing size. We show that under these two assumptions, a computationally efficient solution exists. Furthermore, on several medical datasets, we show that the proposed method achieves in most situations the lowest total costs when compared to various previous methods. Finally, we weaken the requirement on the user to specify all misclassification costs by allowing the user to specify the minimally acceptable recall (target recall). Our experiments confirm that the proposed method achieves the target recall while minimizing the false discovery rate and the covariate acquisition costs better than previous methods.},
  archive      = {J_ML},
  author       = {Andrade, Daniel and Okajima, Yuzuru},
  doi          = {10.1007/s10994-021-05958-z},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1067-1104},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive covariate acquisition for minimizing total cost of classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topic extraction from extremely short texts with variational
manifold regularization. <em>ML</em>, <em>110</em>(5), 1029–1066. (<a
href="https://doi.org/10.1007/s10994-021-05962-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emerging of massive short texts, e.g., social media posts and question titles from Q&amp;A systems, discovering valuable information from them is increasingly significant for many real-world applications of content analysis. The family of topic modeling can effectively explore the hidden structures of documents through the assumptions of latent topics. However, due to the sparseness of short texts, the existing topic models, e.g., latent Dirichlet allocation, lose effectiveness on them. To this end, an effective solution, namely Dirichlet multinomial mixture (DMM), supposing that each short text is only associated with a single topic, indirectly enriches document-level word co-occurrences. However, DMM is sensitive to noisy words, where it often learns inaccurate topic representations at the document level. To address this problem, we extend DMM to a novel Laplacian Dirichlet Multinomial Mixture (LapDMM) topic model for short texts. The basic idea of LapDMM is to preserve local neighborhood structures of short texts, enabling to spread topical signals among neighboring documents, so as to modify the inaccurate topic representations. This is achieved by incorporating the variational manifold regularization into the variational objective of DMM, constraining the close short texts with similar variational topic representations. To find nearest neighbors of short texts, before model inference, we construct an offline document graph, where the distances of short texts can be computed by the word mover’s distance. We further develop an online version of LapDMM, namely Online LapDMM, to achieve inference speedup on massive short texts. Carrying this implications, we exploit the spirit of stochastic optimization with mini-batches and an up-to-date document graph that can efficiently find approximate nearest neighbors instead. To evaluate our models, we compare against the state-of-the-art short text topic models on several traditional tasks, i.e., topic quality, document clustering and classification. The empirical results demonstrate that our models achieve very significant performance gains over the baseline models.},
  archive      = {J_ML},
  author       = {Li, Ximing and Wang, Yang and Ouyang, Jihong and Wang, Meng},
  doi          = {10.1007/s10994-021-05962-3},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {1029-1066},
  shortjournal = {Mach. Learn.},
  title        = {Topic extraction from extremely short texts with variational manifold regularization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoBOT: Evolving neuro-symbolic representations for
explainable low resource text classification. <em>ML</em>,
<em>110</em>(5), 989–1028. (<a
href="https://doi.org/10.1007/s10994-021-05968-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from texts has been widely adopted throughout industry and science. While state-of-the-art neural language models have shown very promising results for text classification, they are expensive to (pre-)train, require large amounts of data and tuning of hundreds of millions or more parameters. This paper explores how automatically evolved text representations can serve as a basis for explainable, low-resource branch of models with competitive performance that are subject to automated hyperparameter tuning. We present autoBOT (automatic Bags-Of-Tokens), an autoML approach suitable for low resource learning scenarios, where both the hardware and the amount of data required for training are limited. The proposed approach consists of an evolutionary algorithm that jointly optimizes various sparse representations of a given text (including word, subword, POS tag, keyword-based, knowledge graph-based and relational features) and two types of document embeddings (non-sparse representations). The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level. The proposed method offers competitive classification performance on fourteen real-world classification tasks when compared against a competitive autoML approach that evolves ensemble models, as well as state-of-the-art neural language models such as BERT and RoBERTa. Moreover, the approach is explainable, as the importance of the parts of the input space is part of the final solution yielded by the proposed optimization procedure, offering potential for meta-transfer learning.},
  archive      = {J_ML},
  author       = {Škrlj, Blaž and Martinc, Matej and Lavrač, Nada and Pollak, Senja},
  doi          = {10.1007/s10994-021-05968-x},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {989-1028},
  shortjournal = {Mach. Learn.},
  title        = {AutoBOT: Evolving neuro-symbolic representations for explainable low resource text classification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large scale multi-label learning using gaussian processes.
<em>ML</em>, <em>110</em>(5), 965–987. (<a
href="https://doi.org/10.1007/s10994-021-05952-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a Gaussian process latent factor model for multi-label classification that can capture correlations among class labels by using a small set of latent Gaussian process functions. To address computational challenges, when the number of training instances is very large, we introduce several techniques based on variational sparse Gaussian process approximations and stochastic optimization. Specifically, we apply doubly stochastic variational inference that sub-samples data instances and classes which allows us to cope with Big Data. Furthermore, we show it is possible and beneficial to optimize over inducing points, using gradient-based methods, even in very high dimensional input spaces involving up to hundreds of thousands of dimensions. We demonstrate the usefulness of our approach on several real-world large-scale multi-label learning problems.},
  archive      = {J_ML},
  author       = {Panos, Aristeidis and Dellaportas, Petros and Titsias, Michalis K.},
  doi          = {10.1007/s10994-021-05952-5},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {965-987},
  shortjournal = {Mach. Learn.},
  title        = {Large scale multi-label learning using gaussian processes},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convex programming based spectral clustering. <em>ML</em>,
<em>110</em>(5), 933–964. (<a
href="https://doi.org/10.1007/s10994-020-05940-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental task in data analysis, and spectral clustering has been recognized as a promising approach to it. Given a graph describing the relationship between data, spectral clustering explores the underlying cluster structure in two stages. The first stage embeds the nodes of the graph in real space, and the second stage groups the embedded nodes into several clusters. The use of the k-means method in the grouping stage is currently standard practice. We present a spectral clustering algorithm that uses convex programming in the grouping stage and study how well it works. This algorithm is designed based on the following observation. If a graph is well-clustered, then the nodes with the largest degree in each cluster can be found by computing an enclosing ellipsoid of the nodes embedded in real space, and the clusters can be identified by using those nodes. We show that, for well-clustered graphs, the algorithm can find clusters of nodes with minimal conductance. We also give an experimental assessment of the algorithm’s performance.},
  archive      = {J_ML},
  author       = {Mizutani, Tomohiko},
  doi          = {10.1007/s10994-020-05940-1},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {933-964},
  shortjournal = {Mach. Learn.},
  title        = {Convex programming based spectral clustering},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust supervised topic models under label noise.
<em>ML</em>, <em>110</em>(5), 907–931. (<a
href="https://doi.org/10.1007/s10994-021-05967-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some statistical topic modeling approaches have been widely applied in the field of supervised document classification. However, there are few researches on these approaches under label noise, which widely exists in real-world applications. For example, many large-scale datasets are collected from websites or annotated by varying quality human-workers, and then have a few mislabeled items. In this paper, we propose two robust topic models for document classification problems: Smoothed Labeled LDA (SL-LDA) and Adaptive Labeled LDA (AL-LDA). SL-LDA is an extension of Labeled LDA (L-LDA), which is a classical supervised topic model. The proposed model overcomes the shortcoming of L-LDA, i.e., overfitting on noisy labels, through Dirichlet smoothing. AL-LDA is an iterative optimization framework based on SL-LDA. At each iterative procedure, we update the Dirichlet prior, which incorporates the observed labels, by a concise algorithm based on maximizing entropy and minimizing cross-entropy principles. This method avoids identifying the noisy label, which is a common difficulty existing in label noise cleaning algorithms. Quantitative experimental results on noisy completely at random (NCAR) and Multiple Noisy Sources (MNS) settings demonstrate our models have outstanding performance under noisy labels. Specially, the proposed AL-LDA has significant advantages relative to the state-of-the-art topic modeling approaches under massive label noise.},
  archive      = {J_ML},
  author       = {Wang, Wei and Guo, Bing and Shen, Yan and Yang, Han and Chen, Yaosen and Suo, Xinhua},
  doi          = {10.1007/s10994-021-05967-y},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {907-931},
  shortjournal = {Mach. Learn.},
  title        = {Robust supervised topic models under label noise},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QuicK-means: Accelerating inference for k-means by learning
fast transforms. <em>ML</em>, <em>110</em>(5), 881–905. (<a
href="https://doi.org/10.1007/s10994-021-05965-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means—and the celebrated Lloyd’s algorithm—is more than the clustering method it was originally designed to be. It has indeed proven pivotal to help increase the speed of many machine learning, data analysis techniques such as indexing, nearest-neighbor search and prediction, data compression and, lately, inference with kernel machines. Here, we introduce an efficient extension of K-means, dubbed QuicK-means, that rests on the idea of expressing the matrix of the $$K$$ cluster centroids as a product of sparse matrices, a feat made possible by recent results devoted to find approximations of matrices as a product of sparse factors. Using such a decomposition squashes the complexity of the matrix-vector product between the factorized $$K\times D$$ centroid matrix $${\mathbf {U}}$$ and any vector from $${\mathcal {O}}\left( KD\right)$$ to $${\mathcal {O}}\left( A \log B~ +B\right)$$ , with $$A=\min \left( K,D\right)$$ and $$B=\max \left( K,D\right)$$ , where $$D$$ is the dimension of the data. This drastic computational saving has a direct impact in the assignment process of a point to a cluster. We propose to learn such a factorization during the Lloyd’s training procedure. We show that resorting to a factorization step at each iteration does not impair the convergence of the optimization scheme, and demonstrate the benefits of our approach experimentally.},
  archive      = {J_ML},
  author       = {Giffon, Luc and Emiya, Valentin and Kadri, Hachem and Ralaivola, Liva},
  doi          = {10.1007/s10994-021-05965-0},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {881-905},
  shortjournal = {Mach. Learn.},
  title        = {QuicK-means: Accelerating inference for K-means by learning fast transforms},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian optimization with approximate set kernels.
<em>ML</em>, <em>110</em>(5), 857–879. (<a
href="https://doi.org/10.1007/s10994-021-05949-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a practical Bayesian optimization method over sets, to minimize a black-box function that takes a set as a single input. Because set inputs are permutation-invariant, traditional Gaussian process-based Bayesian optimization strategies which assume vector inputs can fall short. To address this, we develop a Bayesian optimization method with set kernel that is used to build surrogate functions. This kernel accumulates similarity over set elements to enforce permutation-invariance, but this comes at a greater computational cost. To reduce this burden, we propose two key components: (i) a more efficient approximate set kernel which is still positive-definite and is an unbiased estimator of the true set kernel with upper-bounded variance in terms of the number of subsamples, (ii) a constrained acquisition function optimization over sets, which uses symmetry of the feasible region that defines a set input. Finally, we present several numerical experiments which demonstrate that our method outperforms other methods.},
  archive      = {J_ML},
  author       = {Kim, Jungtaek and McCourt, Michael and You, Tackgeun and Kim, Saehoon and Choi, Seungjin},
  doi          = {10.1007/s10994-021-05949-0},
  journal      = {Machine Learning},
  number       = {5},
  pages        = {857-879},
  shortjournal = {Mach. Learn.},
  title        = {Bayesian optimization with approximate set kernels},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning programs by learning from failures. <em>ML</em>,
<em>110</em>(4), 801–856. (<a
href="https://doi.org/10.1007/s10994-020-05934-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe an inductive logic programming (ILP) approach called learning from failures. In this approach, an ILP system (the learner) decomposes the learning problem into three separate stages: generate, test, and constrain. In the generate stage, the learner generates a hypothesis (a logic program) that satisfies a set of hypothesis constraints (constraints on the syntactic form of hypotheses). In the test stage, the learner tests the hypothesis against training examples. A hypothesis fails when it does not entail all the positive examples or entails a negative example. If a hypothesis fails, then, in the constrain stage, the learner learns constraints from the failed hypothesis to prune the hypothesis space, i.e. to constrain subsequent hypothesis generation. For instance, if a hypothesis is too general (entails a negative example), the constraints prune generalisations of the hypothesis. If a hypothesis is too specific (does not entail all the positive examples), the constraints prune specialisations of the hypothesis. This loop repeats until either (i) the learner finds a hypothesis that entails all the positive and none of the negative examples, or (ii) there are no more hypotheses to test. We introduce Popper, an ILP system that implements this approach by combining answer set programming and Prolog. Popper supports infinite problem domains, reasoning about lists and numbers, learning textually minimal programs, and learning recursive programs. Our experimental results on three domains (toy game problems, robot strategies, and list transformations) show that (i) constraints drastically improve learning performance, and (ii) Popper can outperform existing ILP systems, both in terms of predictive accuracies and learning times.},
  archive      = {J_ML},
  author       = {Cropper, Andrew and Morel, Rolf},
  doi          = {10.1007/s10994-020-05934-z},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {801-856},
  shortjournal = {Mach. Learn.},
  title        = {Learning programs by learning from failures},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AUTOMAT[r]IX: Learning simple matrix pipelines. <em>ML</em>,
<em>110</em>(4), 779–799. (<a
href="https://doi.org/10.1007/s10994-021-05950-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrices are a very common way of representing and working with data in data science and artificial intelligence. Writing a small snippet of code to make a simple matrix transformation is frequently frustrating, especially for those people without an extensive programming expertise. We present AUTOMAT[R]IX, a system that is able to induce R program snippets from a single (and possibly partial) matrix transformation example provided by the user. Our learning algorithm is able to induce the correct matrix pipeline snippet by composing primitives from a library. Because of the intractable search space—exponential on the size of the library and the number of primitives to be combined in the snippet, we speed up the process with (1) a typed system that excludes all combinations of primitives with inconsistent mapping between input and output matrix dimensions, and (2) a probabilistic model to estimate the probability of each sequence of primitives from their frequency of use and a text hint provided by the user. We validate AUTOMAT[R]IX with a set of real programming queries involving matrices from Stack Overflow, showing that we can learn the transformations efficiently, from just one partial example.},
  archive      = {J_ML},
  author       = {Contreras-Ochando, Lidia and Ferri, Cèsar and Hernández-Orallo, José},
  doi          = {10.1007/s10994-021-05950-7},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {779-799},
  shortjournal = {Mach. Learn.},
  title        = {AUTOMAT[R]IX: Learning simple matrix pipelines},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Top program construction and reduction for polynomial time
meta-interpretive learning. <em>ML</em>, <em>110</em>(4), 755–778. (<a
href="https://doi.org/10.1007/s10994-020-05945-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-Interpretive Learners, like most ILP systems, learn by searching for a correct hypothesis in the hypothesis space, the powerset of all constructible clauses. We show how this exponentially-growing search can be replaced by the construction of a Top program: the set of clauses in all correct hypotheses that is itself a correct hypothesis. We give an algorithm for Top program construction and show that it constructs a correct Top program in polynomial time and from a finite number of examples. We implement our algorithm in Prolog as the basis of a new MIL system, Louise, that constructs a Top program and then reduces it by removing redundant clauses. We compare Louise to the state-of-the-art search-based MIL system Metagol in experiments on grid world navigation, graph connectedness and grammar learning datasets and find that Louise improves on Metagol’s predictive accuracy when the hypothesis space and the target theory are both large, or when the hypothesis space does not include a correct hypothesis because of “classification noise” in the form of mislabelled examples. When the hypothesis space or the target theory are small, Louise and Metagol perform equally well.},
  archive      = {J_ML},
  author       = {Patsantzis, S. and Muggleton, S. H.},
  doi          = {10.1007/s10994-020-05945-w},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {755-778},
  shortjournal = {Mach. Learn.},
  title        = {Top program construction and reduction for polynomial time meta-interpretive learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic inductive constraint logic. <em>ML</em>,
<em>110</em>(4), 723–754. (<a
href="https://doi.org/10.1007/s10994-020-05911-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic logical models deal effectively with uncertain relations and entities typical of many real world domains. In the field of probabilistic logic programming usually the aim is to learn these kinds of models to predict specific atoms or predicates of the domain, called target atoms/predicates. However, it might also be useful to learn classifiers for interpretations as a whole: to this end, we consider the models produced by the inductive constraint logic system, represented by sets of integrity constraints, and we propose a probabilistic version of them. Each integrity constraint is annotated with a probability, and the resulting probabilistic logical constraint model assigns a probability of being positive to interpretations. To learn both the structure and the parameters of such probabilistic models we propose the system PASCAL for “probabilistic inductive constraint logic”. Parameter learning can be performed using gradient descent or L-BFGS. PASCAL has been tested on 11 datasets and compared with a few statistical relational systems and a system that builds relational decision trees (TILDE): we demonstrate that this system achieves better or comparable results in terms of area under the precision–recall and receiver operating characteristic curves, in a comparable execution time.},
  archive      = {J_ML},
  author       = {Riguzzi, Fabrizio and Bellodi, Elena and Zese, Riccardo and Alberti, Marco and Lamma, Evelina},
  doi          = {10.1007/s10994-020-05911-6},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {723-754},
  shortjournal = {Mach. Learn.},
  title        = {Probabilistic inductive constraint logic},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beneficial and harmful explanatory machine learning.
<em>ML</em>, <em>110</em>(4), 695–721. (<a
href="https://doi.org/10.1007/s10994-020-05941-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.},
  archive      = {J_ML},
  author       = {Ai, Lun and Muggleton, Stephen H. and Hocquette, Céline and Gromowski, Mark and Schmid, Ute},
  doi          = {10.1007/s10994-020-05941-0},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {695-721},
  shortjournal = {Mach. Learn.},
  title        = {Beneficial and harmful explanatory machine learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPEED: Secure, PrivatE, and efficient deep learning.
<em>ML</em>, <em>110</em>(4), 675–694. (<a
href="https://doi.org/10.1007/s10994-021-05970-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a deep learning framework able to deal with strong privacy constraints. Based on collaborative learning, differential privacy and homomorphic encryption, the proposed approach advances state-of-the-art of private deep learning against a wider range of threats, in particular the honest-but-curious server assumption. We address threats from both the aggregation server, the global model and potentially colluding data holders. Building upon distributed differential privacy and a homomorphic argmax operator, our method is specifically designed to maintain low communication loads and efficiency. The proposed method is supported by carefully crafted theoretical results. We provide differential privacy guarantees from the point of view of any entity having access to the final model, including colluding data holders, as a function of the ratio of data holders who kept their noise secret. This makes our method practical to real-life scenarios where data holders do not trust any third party to process their datasets nor the other data holders. Crucially the computational burden of the approach is maintained reasonable, and, to the best of our knowledge, our framework is the first one to be efficient enough to investigate deep learning applications while addressing such a large scope of threats. To assess the practical usability of our framework, experiments have been carried out on image datasets in a classification context. We present numerical results that show that the learning procedure is both accurate and private.},
  archive      = {J_ML},
  author       = {Grivet Sébert, Arnaud and Pinot, Rafaël and Zuber, Martin and Gouy-Pailler, Cédric and Sirdey, Renaud},
  doi          = {10.1007/s10994-021-05970-3},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {675-694},
  shortjournal = {Mach. Learn.},
  title        = {SPEED: Secure, PrivatE, and efficient deep learning},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Protect privacy of deep classification networks by
exploiting their generative power. <em>ML</em>, <em>110</em>(4),
651–674. (<a href="https://doi.org/10.1007/s10994-021-05951-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research showed that deep learning models are vulnerable to membership inference attacks, which aim to determine if an example is in the training set of the model. We propose a new framework to defend against this sort of attack. Our key insight is that if we retrain the original classifier with a new dataset that is independent of the original training set while their elements are sampled from the same distribution, the retrained classifier will leak no information that cannot be inferred from the distribution about the original training set. Our framework consists of three phases. First, we transferred the original classifier to a Joint Energy-based Model (JEM) to exploit the model’s implicit generative power. Then, we sampled from the JEM to create a new dataset. Finally, we used the new dataset to retrain or fine-tune the original classifier. We empirically studied different transfer learning schemes for the JEM and fine-tuning/retraining strategies for the classifier against shadow-model attacks. Our evaluation shows that our framework can suppress the attacker’s membership advantage to a negligible level while keeping the classifier’s accuracy acceptable. We compared it with other state-of-the-art defenses considering adaptive attackers and showed our defense is effective even under the worst-case scenario. Besides, we also found that combining other defenses with our framework often achieves better robustness. Our code will be made available at https://github.com/ChenJiyu/meminf-defense.git .},
  archive      = {J_ML},
  author       = {Chen, Jiyu and Guo, Yiwen and Zheng, Qianjun and Chen, Hao},
  doi          = {10.1007/s10994-021-05951-6},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {651-674},
  shortjournal = {Mach. Learn.},
  title        = {Protect privacy of deep classification networks by exploiting their generative power},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An extended DEIM algorithm for subset selection and class
identification. <em>ML</em>, <em>110</em>(4), 621–650. (<a
href="https://doi.org/10.1007/s10994-021-05954-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete empirical interpolation method (DEIM) has been shown to be a viable index-selection technique for identifying representative subsets in data. Having gained some popularity in reducing dimensionality of physical models involving differential equations, its use in subset-/pattern-identification tasks is not yet broadly known within the machine learning community. While it has much to offer as is, the DEIM algorithm is limited in that the number of selected indices cannot exceed the rank of the corresponding data matrix. Although this is not an issue for many data sets, there are cases in which the number of classes represented in a given data set is greater than the rank of the data matrix; in such cases, it is impossible for the standard DEIM algorithm to identify all classes. To overcome this issue, we present a novel extension of DEIM, called E-DEIM. With the proposed algorithm, we also provide some theoretical results for using extensions of DEIM to form the CUR matrix factorization in identifying both rows and columns to approximate the original data matrix. Results from applying variations of E-DEIM to two different data sets indicate that the presented extension can indeed allow for the identification of additional classes along with those selected by standard DEIM. In addition, comparing these results to those of some more familiar methods demonstrates that the proposed deterministic E-DEIM approach including coherence performs comparably to or better than the other evaluated methods and should be considered in future class-identification tasks.},
  archive      = {J_ML},
  author       = {Hendryx, Emily P. and Rivière, Béatrice M. and Rusin, Craig G.},
  doi          = {10.1007/s10994-021-05954-3},
  journal      = {Machine Learning},
  number       = {4},
  pages        = {621-650},
  shortjournal = {Mach. Learn.},
  title        = {An extended DEIM algorithm for subset selection and class identification},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Correction to: Fast and accurate pseudoinverse with sparse
matrix reordering and incremental approach. <em>ML</em>,
<em>110</em>(3), 619–620. (<a
href="https://doi.org/10.1007/s10994-020-05936-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article [Fast and accurate pseudoinverse with sparse matrix reordering and incremental approach], written by [Jinhong Jung and Lee Sael], was originally published Online First without Open Access. After publication in volume [109], issue [12], pages [2333–2347] the author decided to opt for Open Choice and to make the article an Open Access publication.},
  archive      = {J_ML},
  author       = {Jung, Jinhong and Sael, Lee},
  doi          = {10.1007/s10994-020-05936-x},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {619-620},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Fast and accurate pseudoinverse with sparse matrix reordering and incremental approach},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Concentration bounds for temporal difference learning with
linear function approximation: The case of batch data and uniform
sampling. <em>ML</em>, <em>110</em>(3), 559–618. (<a
href="https://doi.org/10.1007/s10994-020-05912-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a stochastic approximation (SA) based method with randomization of samples for policy evaluation using the least squares temporal difference (LSTD) algorithm. Our proposed scheme is equivalent to running regular temporal difference learning with linear function approximation, albeit with samples picked uniformly from a given dataset. Our method results in an O(d) improvement in complexity in comparison to LSTD, where d is the dimension of the data. We provide non-asymptotic bounds for our proposed method, both in high probability and in expectation, under the assumption that the matrix underlying the LSTD solution is positive definite. The latter assumption can be easily satisfied for the pathwise LSTD variant proposed by Lazaric (J Mach Learn Res 13:3041–3074, 2012). Moreover, we also establish that using our method in place of LSTD does not impact the rate of convergence of the approximate value function to the true value function. These rate results coupled with the low computational complexity of our method make it attractive for implementation in big data settings, where d is large. A similar low-complexity alternative for least squares regression is well-known as the stochastic gradient descent (SGD) algorithm. We provide finite-time bounds for SGD. We demonstrate the practicality of our method as an efficient alternative for pathwise LSTD empirically by combining it with the least squares policy iteration algorithm in a traffic signal control application. We also conduct another set of experiments that combines the SA-based low-complexity variant for least squares regression with the LinUCB algorithm for contextual bandits, using the large scale news recommendation dataset from Yahoo.},
  archive      = {J_ML},
  author       = {Prashanth, L. A. and Korda, Nathaniel and Munos, Rémi},
  doi          = {10.1007/s10994-020-05912-5},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {559-618},
  shortjournal = {Mach. Learn.},
  title        = {Concentration bounds for temporal difference learning with linear function approximation: The case of batch data and uniform sampling},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coupling matrix manifolds assisted optimization for optimal
transport problems. <em>ML</em>, <em>110</em>(3), 533–558. (<a
href="https://doi.org/10.1007/s10994-020-05931-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal transport (OT) is a powerful tool for measuring the distance between two probability distributions. In this paper, we introduce a new manifold named as the coupling matrix manifold (CMM), where each point on this novel manifold can be regarded as a transportation plan of the optimal transport problem. We firstly explore the Riemannian geometry of CMM with the metric expressed by the Fisher information. These geometrical features can be exploited in many essential optimization methods as a framework solving all types of OT problems via incorporating numerical Riemannian optimization algorithms such as gradient descent and trust region algorithms in CMM manifold. The proposed approach is validated using several OT problems in comparison with recent state-of-the-art related works. For the classic OT problem and its entropy regularized variant, it is shown that our method is comparable with the classic algorithms such as linear programming and Sinkhorn algorithms. For other types of non-entropy regularized OT problems, our proposed method has shown superior performance to other works, whereby the geometric information of the OT feasible space was not incorporated within.},
  archive      = {J_ML},
  author       = {Shi, Dai and Gao, Junbin and Hong, Xia and Boris Choy, S. T. and Wang, Zhiyong},
  doi          = {10.1007/s10994-020-05931-2},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {533-558},
  shortjournal = {Mach. Learn.},
  title        = {Coupling matrix manifolds assisted optimization for optimal transport problems},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reshaped tensor nuclear norms for higher order tensor
completion. <em>ML</em>, <em>110</em>(3), 507–531. (<a
href="https://doi.org/10.1007/s10994-020-05927-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate optimal conditions for inducing low-rankness of higher order tensors by using convex tensor norms with reshaped tensors. We propose the reshaped tensor nuclear norm as a generalized approach to reshape tensors to be regularized by using the tensor nuclear norm. Furthermore, we propose the reshaped latent tensor nuclear norm to combine multiple reshaped tensors using the tensor nuclear norm. We analyze the generalization bounds for tensor completion models regularized by the proposed norms and show that the novel reshaping norms lead to lower Rademacher complexities. Through simulation and real-data experiments, we show that our proposed methods are favorably compared to existing tensor norms consolidating our theoretical claims.},
  archive      = {J_ML},
  author       = {Wimalawarne, Kishan and Mamitsuka, Hiroshi},
  doi          = {10.1007/s10994-020-05927-y},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {507-531},
  shortjournal = {Mach. Learn.},
  title        = {Reshaped tensor nuclear norms for higher order tensor completion},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aleatoric and epistemic uncertainty in machine learning: An
introduction to concepts and methods. <em>ML</em>, <em>110</em>(3),
457–506. (<a href="https://doi.org/10.1007/s10994-021-05946-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  archive      = {J_ML},
  author       = {Hüllermeier, Eyke and Waegeman, Willem},
  doi          = {10.1007/s10994-021-05946-3},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {457-506},
  shortjournal = {Mach. Learn.},
  title        = {Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). F*: An interpretable transformation of the f-measure.
<em>ML</em>, <em>110</em>(3), 451–456. (<a
href="https://doi.org/10.1007/s10994-021-05964-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The F-measure, also known as the F1-score, is widely used to assess the performance of classification algorithms. However, some researchers find it lacking in intuitive interpretation, questioning the appropriateness of combining two aspects of performance as conceptually distinct as precision and recall, and also questioning whether the harmonic mean is the best way to combine them. To ease this concern, we describe a simple transformation of the F-measure, which we call $$F^*$$ (F-star), which has an immediate practical interpretation.},
  archive      = {J_ML},
  author       = {Hand, David J. and Christen, Peter and Kirielle, Nishadi},
  doi          = {10.1007/s10994-021-05964-1},
  journal      = {Machine Learning},
  number       = {3},
  pages        = {451-456},
  shortjournal = {Mach. Learn.},
  title        = {F*: An interpretable transformation of the F-measure},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Correction to: Fast and accurate pseudoinverse with sparse
matrix reordering and incremental approach. <em>ML</em>,
<em>110</em>(2), 449. (<a
href="https://doi.org/10.1007/s10994-020-05933-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The following acknowledgments were inadvertently left out of the published article.},
  archive      = {J_ML},
  author       = {Jung, Jinhong and Sael, Lee},
  doi          = {10.1007/s10994-020-05933-0},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {449},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Fast and accurate pseudoinverse with sparse matrix reordering and incremental approach},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global optimization based on active preference learning with
radial basis functions. <em>ML</em>, <em>110</em>(2), 417–448. (<a
href="https://doi.org/10.1007/s10994-020-05935-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method for solving optimization problems in which the decision-maker cannot evaluate the objective function, but rather can only express a preference such as “this is better than that” between two candidate decision vectors. The algorithm described in this paper aims at reaching the global optimizer by iteratively proposing the decision maker a new comparison to make, based on actively learning a surrogate of the latent (unknown and perhaps unquantifiable) objective function from past sampled decision vectors and pairwise preferences. A radial-basis function surrogate is fit via linear or quadratic programming, satisfying if possible the preferences expressed by the decision maker on existing samples. The surrogate is used to propose a new sample of the decision vector for comparison with the current best candidate based on two possible criteria: minimize a combination of the surrogate and an inverse weighting distance function to balance between exploitation of the surrogate and exploration of the decision space, or maximize a function related to the probability that the new candidate will be preferred. Compared to active preference learning based on Bayesian optimization, we show that our approach is competitive in that, within the same number of comparisons, it usually approaches the global optimum more closely and is computationally lighter. Applications of the proposed algorithm to solve a set of benchmark global optimization problems, for multi-objective optimization, and for optimal tuning of a cost-sensitive neural network classifier for object recognition from images are described in the paper. MATLAB and a Python implementations of the algorithms described in the paper are available at http://cse.lab.imtlucca.it/~bemporad/glis .},
  archive      = {J_ML},
  author       = {Bemporad, Alberto and Piga, Dario},
  doi          = {10.1007/s10994-020-05935-y},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {417-448},
  shortjournal = {Mach. Learn.},
  title        = {Global optimization based on active preference learning with radial basis functions},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularisation of neural networks by enforcing lipschitz
continuity. <em>ML</em>, <em>110</em>(2), 393–416. (<a
href="https://doi.org/10.1007/s10994-020-05929-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the effect of explicitly enforcing the Lipschitz continuity of neural networks with respect to their inputs. To this end, we provide a simple technique for computing an upper bound to the Lipschitz constant—for multiple p-norms—of a feed forward neural network composed of commonly used layer types. Our technique is then used to formulate training a neural network with a bounded Lipschitz constant as a constrained optimisation problem that can be solved using projected stochastic gradient methods. Our evaluation study shows that the performance of the resulting models exceeds that of models trained with other common regularisers. We also provide evidence that the hyperparameters are intuitive to tune, demonstrate how the choice of norm for computing the Lipschitz constant impacts the resulting model, and show that the performance gains provided by our method are particularly noticeable when only a small amount of training data is available.},
  archive      = {J_ML},
  author       = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J.},
  doi          = {10.1007/s10994-020-05929-w},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {393-416},
  shortjournal = {Mach. Learn.},
  title        = {Regularisation of neural networks by enforcing lipschitz continuity},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel machines for current status data. <em>ML</em>,
<em>110</em>(2), 349–391. (<a
href="https://doi.org/10.1007/s10994-020-05930-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis, estimating the failure time distribution is an important and difficult task, since usually the data is subject to censoring. Specifically, in this paper we consider current status data, a type of data where the failure time cannot be directly observed. The format of the data is such that the failure time is restricted to knowledge of whether or not the failure time exceeds a random monitoring time. We propose a flexible kernel machine approach for estimation of the failure time expectation as a function of the covariates, with current status data. In order to obtain the kernel machine decision function, we minimize a regularized version of the empirical risk with respect to a new loss function. Using finite sample bounds and novel oracle inequalities, we prove that the obtained estimator converges to the true conditional expectation for a large family of probability measures. Finally, we present a simulation study and an analysis of real-world data that compares the performance of the proposed approach to existing methods. We show empirically that our approach is comparable to current state of the art, and in some cases is even better.},
  archive      = {J_ML},
  author       = {Travis-Lumer, Yael and Goldberg, Yair},
  doi          = {10.1007/s10994-020-05930-3},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {349-391},
  shortjournal = {Mach. Learn.},
  title        = {Kernel machines for current status data},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional variance penalties and domain shift robustness.
<em>ML</em>, <em>110</em>(2), 303–348. (<a
href="https://doi.org/10.1007/s10994-020-05924-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) ‘core’ or ‘conditionally invariant’ features $$C$$ whose distribution $$C\vert Y$$ , conditional on the class Y, does not change substantially across domains and (ii) ‘style’ features $$S$$ whose distribution $$S\vert Y$$ can change substantially across domains. Examples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable. We do assume that we can sometimes observe a typically discrete identifier or “ $$\mathrm {ID}$$ variable”. In some applications we know, for example, that two images show the same person, and $$\mathrm {ID}$$ then refers to the identity of the person. The proposed method requires only a small fraction of images to have $$\mathrm {ID}$$ information. We group observations if they share the same class and identifier $$(Y,\mathrm {ID})=(y,\mathrm {id})$$ and penalize the conditional variance of the prediction or the loss if we condition on $$(Y,\mathrm {ID})$$ . Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables in a partially linear structural equation model. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.},
  archive      = {J_ML},
  author       = {Heinze-Deml, Christina and Meinshausen, Nicolai},
  doi          = {10.1007/s10994-020-05924-1},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {303-348},
  shortjournal = {Mach. Learn.},
  title        = {Conditional variance penalties and domain shift robustness},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LoRAS: An oversampling approach for imbalanced datasets.
<em>ML</em>, <em>110</em>(2), 279–301. (<a
href="https://doi.org/10.1007/s10994-020-05913-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Synthetic Minority Oversampling TEchnique (SMOTE) is widely-used for the analysis of imbalanced datasets. It is known that SMOTE frequently over-generalizes the minority class, leading to misclassifications for the majority class, and effecting the overall balance of the model. In this article, we present an approach that overcomes this limitation of SMOTE, employing Localized Random Affine Shadowsampling (LoRAS) to oversample from an approximated data manifold of the minority class. We benchmarked our algorithm with 14 publicly available imbalanced datasets using three different Machine Learning (ML) algorithms and compared the performance of LoRAS, SMOTE and several SMOTE extensions that share the concept of using convex combinations of minority class data points for oversampling with LoRAS. We observed that LoRAS, on average generates better ML models in terms of F1-Score and Balanced accuracy. Another key observation is that while most of the extensions of SMOTE we have tested, improve the F1-Score with respect to SMOTE on an average, they compromise on the Balanced accuracy of a classification model. LoRAS on the contrary, improves both F1 Score and the Balanced accuracy thus produces better classification models. Moreover, to explain the success of the algorithm, we have constructed a mathematical framework to prove that LoRAS oversampling technique provides a better estimate for the mean of the underlying local data distribution of the minority class data space.},
  archive      = {J_ML},
  author       = {Bej, Saptarshi and Davtyan, Narek and Wolfien, Markus and Nassar, Mariam and Wolkenhauer, Olaf},
  doi          = {10.1007/s10994-020-05913-4},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {279-301},
  shortjournal = {Mach. Learn.},
  title        = {LoRAS: An oversampling approach for imbalanced datasets},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The voice of optimization. <em>ML</em>, <em>110</em>(2),
249–277. (<a href="https://doi.org/10.1007/s10994-020-05893-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the idea that using optimal classification trees (OCTs) and optimal classification trees with-hyperplanes (OCT-Hs), interpretable machine learning algorithms developed by Bertsimas and Dunn (Mach Learn 106(7):1039–1082, 2017), we are able to obtain insight on the strategy behind the optimal solution in continuous and mixed-integer convex optimization problem as a function of key parameters that affect the problem. In this way, optimization is not a black box anymore. Instead, we redefine optimization as a multiclass classification problem where the predictor gives insights on the logic behind the optimal solution. In other words, OCTs and OCT-Hs give optimization a voice. We show on several realistic examples that the accuracy behind our method is in the 90–100\% range, while even when the predictions are not correct, the degree of suboptimality or infeasibility is very low. We compare optimal strategy predictions of OCTs and OCT-Hs and feedforward neural networks (NNs) and conclude that the performance of OCT-Hs and NNs is comparable. OCTs are somewhat weaker but often competitive. Therefore, our approach provides a novel insightful understanding of optimal strategies to solve a broad class of continuous and mixed-integer optimization problems.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Stellato, Bartolomeo},
  doi          = {10.1007/s10994-020-05893-5},
  journal      = {Machine Learning},
  number       = {2},
  pages        = {249-277},
  shortjournal = {Mach. Learn.},
  title        = {The voice of optimization},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imputation of clinical covariates in time series.
<em>ML</em>, <em>110</em>(1), 185–248. (<a
href="https://doi.org/10.1007/s10994-020-05923-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common problem in longitudinal datasets which include multiple instances of the same individual observed at different points in time. We introduce a new approach, MedImpute, for imputing missing clinical covariates in multivariate panel data. This approach integrates patient specific information into an optimization formulation that can be adjusted for different imputation algorithms. We present the formulation for a K-nearest neighbors model and derive a corresponding scalable first-order method med.knn. Our algorithm provides imputations for datasets with both continuous and categorical features and observations occurring at arbitrary points in time. In computational experiments on three real-world clinical datasets, we test its performance on imputation and downstream predictive tasks, varying the percentage of missing data, the number of observations per patient, and the mechanism of missing data. The proposed method improves upon both the imputation accuracy and downstream predictive performance relative to the best of the benchmark imputation methods considered. We show that this edge is consistently present both in longitudinal and electronic health records datasets as well as in binary classification and regression settings. On computational experiments on synthetic data, we test the scalability of this algorithm on large datasets, and we show that an efficient method for hyperparameter tuning scales to datasets with 10,000’s of observations and 100’s of covariates while maintaining high imputation accuracy.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Orfanoudaki, Agni and Pawlowski, Colin},
  doi          = {10.1007/s10994-020-05923-2},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {185-248},
  shortjournal = {Mach. Learn.},
  title        = {Imputation of clinical covariates in time series},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistical hierarchical clustering algorithm for outlier
detection in evolving data streams. <em>ML</em>, <em>110</em>(1),
139–184. (<a href="https://doi.org/10.1007/s10994-020-05905-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a hard data analysis process that requires constant creation and improvement of data analysis algorithms. Using traditional clustering algorithms to analyse data streams is impossible due to processing power and memory issues. To solve this, the traditional clustering algorithm complexity needed to be reduced, which led to the creation of sequential clustering algorithms. The usual approach is two-phase clustering, which uses online phase to relax data details and complexity, and offline phase to cluster concepts created in the online phase. Detecting anomalies in a data stream is usually solved in the online phase, as it requires unreduced data. Contrarily, producing good macro-clustering is done in the offline phase, which is the reason why two-phase clustering algorithms have difficulty being equally good in anomaly detection and macro-clustering. In this paper, we propose a statistical hierarchical clustering algorithm equally suitable for both detecting anomalies and macro-clustering. The proposed algorithm is single-phased and uses statistical inference on the input data stream, resulting in statistical distributions that are constantly updated. This makes the classification adaptable, allowing agglomeration of outliers into clusters, tracking population evolution, and to be used without knowing the expected number of clusters and outliers. The proposed algorithm was tested against typical clustering algorithms, including two-phase algorithms suitable for data stream analysis. A number of typical test cases were selected, to show the universality and qualities of the proposed clustering algorithm.},
  archive      = {J_ML},
  author       = {Krleža, Dalibor and Vrdoljak, Boris and Brčić, Mario},
  doi          = {10.1007/s10994-020-05905-4},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {139-184},
  shortjournal = {Mach. Learn.},
  title        = {Statistical hierarchical clustering algorithm for outlier detection in evolving data streams},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable clustering: An optimization approach.
<em>ML</em>, <em>110</em>(1), 89–138. (<a
href="https://doi.org/10.1007/s10994-020-05896-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art clustering algorithms provide little insight into the rationale for cluster membership, limiting their interpretability. In complex real-world applications, the latter poses a barrier to machine learning adoption when experts are asked to provide detailed explanations of their algorithms’ recommendations. We present a new unsupervised learning method that leverages Mixed Integer Optimization techniques to generate interpretable tree-based clustering models. Utilizing a flexible optimization-driven framework, our algorithm approximates the globally optimal solution leading to high quality partitions of the feature space. We propose a novel method which can optimize for various clustering internal validation metrics and naturally determines the optimal number of clusters. It successfully addresses the challenge of mixed numerical and categorical data and achieves comparable or superior performance to other clustering methods on both synthetic and real-world datasets while offering significantly higher interpretability.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Orfanoudaki, Agni and Wiberg, Holly},
  doi          = {10.1007/s10994-020-05896-2},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {89-138},
  shortjournal = {Mach. Learn.},
  title        = {Interpretable clustering: An optimization approach},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Node classification over bipartite graphs through
projection. <em>ML</em>, <em>110</em>(1), 37–87. (<a
href="https://doi.org/10.1007/s10994-020-05898-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world large datasets correspond to bipartite graph data settings—think for example of users rating movies or people visiting locations. Although there has been some prior work on data analysis with such bigraphs, no general network-oriented methodology has been proposed yet to perform node classification. In this paper we propose a three-stage classification framework that effectively deals with the typical very large size of such datasets. The stages are: (1) top node weighting, (2) projection to a weighted unigraph, and (3) application of a relational classifier. This paper has two major contributions. Firstly, this general framework allows us to explore the design space, by applying different choices at the three stages, introducing new alternatives and mixing-and-matching to create new techniques. We present an empirical study of the predictive and run-time performances for different combinations of functions in the three stages over a large collection of bipartite datasets with sizes of up to $$20\,\hbox {million} \times 30\,\hbox {million}$$ nodes. Secondly, thinking of classification on bigraph data in terms of the three-stage framework opens up the design space of possible solutions, where existing and novel functions can be mixed and matched, and tailored to the problem at hand. Indeed, in this work a novel, fast, accurate and comprehensible method emerges, called the SW-transformation, as one of the best-performing combinations in the empirical study.},
  archive      = {J_ML},
  author       = {Stankova, Marija and Praet, Stiene and Martens, David and Provost, Foster},
  doi          = {10.1007/s10994-020-05898-0},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {37-87},
  shortjournal = {Mach. Learn.},
  title        = {Node classification over bipartite graphs through projection},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CPAS: The UK’s national machine learning-based hospital
capacity planning system for COVID-19. <em>ML</em>, <em>110</em>(1),
15–35. (<a href="https://doi.org/10.1007/s10994-020-05921-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) global pandemic poses the threat of overwhelming healthcare systems with unprecedented demands for intensive care resources. Managing these demands cannot be effectively conducted without a nationwide collective effort that relies on data to forecast hospital demands on the national, regional, hospital and individual levels. To this end, we developed the COVID-19 Capacity Planning and Analysis System (CPAS)—a machine learning-based system for hospital resource planning that we have successfully deployed at individual hospitals and across regions in the UK in coordination with NHS Digital. In this paper, we discuss the main challenges of deploying a machine learning-based decision support system at national scale, and explain how CPAS addresses these challenges by (1) defining the appropriate learning problem, (2) combining bottom-up and top-down analytical approaches, (3) using state-of-the-art machine learning algorithms, (4) integrating heterogeneous data sources, and (5) presenting the result with an interactive and transparent interface. CPAS is one of the first machine learning-based systems to be deployed in hospitals on a national scale to address the COVID-19 pandemic—we conclude the paper with a summary of the lessons learned from this experience.},
  archive      = {J_ML},
  author       = {Qian, Zhaozhi and Alaa, Ahmed M. and van der Schaar, Mihaela},
  doi          = {10.1007/s10994-020-05921-4},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {15-35},
  shortjournal = {Mach. Learn.},
  title        = {CPAS: The UK’s national machine learning-based hospital capacity planning system for COVID-19},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How artificial intelligence and machine learning can help
healthcare systems respond to COVID-19. <em>ML</em>, <em>110</em>(1),
1–14. (<a href="https://doi.org/10.1007/s10994-020-05928-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 global pandemic is a threat not only to the health of millions of individuals, but also to the stability of infrastructure and economies around the world. The disease will inevitably place an overwhelming burden on healthcare systems that cannot be effectively dealt with by existing facilities or responses based on conventional approaches. We believe that a rigorous clinical and societal response can only be mounted by using intelligence derived from a variety of data sources to better utilize scarce healthcare resources, provide personalized patient management plans, inform policy, and expedite clinical trials. In this paper, we introduce five of the most important challenges in responding to COVID-19 and show how each of them can be addressed by recent developments in machine learning (ML) and artificial intelligence (AI). We argue that the integration of these techniques into local, national, and international healthcare systems will save lives, and propose specific methods by which implementation can happen swiftly and efficiently. We offer to extend these resources and knowledge to assist policymakers seeking to implement these techniques.},
  archive      = {J_ML},
  author       = {van der Schaar, Mihaela and Alaa, Ahmed M. and Floto, Andres and Gimson, Alexander and Scholtes, Stefan and Wood, Angela and McKinney, Eoin and Jarrett, Daniel and Lio, Pietro and Ercole, Ari},
  doi          = {10.1007/s10994-020-05928-x},
  journal      = {Machine Learning},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Learn.},
  title        = {How artificial intelligence and machine learning can help healthcare systems respond to COVID-19},
  volume       = {110},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
