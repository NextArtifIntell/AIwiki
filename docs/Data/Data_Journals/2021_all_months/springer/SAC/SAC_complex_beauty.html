<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---82">SAC - 82</h2>
<ul>
<li><details>
<summary>
(2021). Unrestricted permutation forces extrapolation: Variable
importance requires at least one more model, or there is no free
variable importance. <em>SAC</em>, <em>31</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-021-10057-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reviews and advocates against the use of permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because they are both model-agnostic and depend only on the pre-trained model output, making them computationally efficient and widely available in software. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. The purpose of our work here is to (i) review this growing body of literature, (ii) provide further demonstrations of these drawbacks along with a detailed explanation as to why they occur, and (iii) advocate for alternative measures that involve additional modeling. In particular, we describe how breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects across various model setups and find support for previous claims in the literature that PaP metrics can vastly over-emphasize correlated features in both variable importance measures and partial dependence plots. As an alternative, we discuss and recommend more direct approaches that involve measuring the change in model performance after muting the effects of the features under investigation.},
  archive      = {J_SAC},
  author       = {Hooker, Giles and Mentch, Lucas and Zhou, Siyu},
  doi          = {10.1007/s11222-021-10057-z},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Unrestricted permutation forces extrapolation: Variable importance requires at least one more model, or there is no free variable importance},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A metropolis-class sampler for targets with non-convex
support. <em>SAC</em>, <em>31</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s11222-021-10044-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to improve upon the exploration of the general-purpose random walk Metropolis algorithm when the target has non-convex support $$A\subset {\mathbb {R}}^d$$ , by reusing proposals in $$A^c$$ which would otherwise be rejected. The algorithm is Metropolis-class and under standard conditions the chain satisfies a strong law of large numbers and central limit theorem. Theoretical and numerical evidence of improved performance relative to random walk Metropolis are provided. Issues of implementation are discussed and numerical examples, including applications to global optimisation and rare event sampling, are presented.},
  archive      = {J_SAC},
  author       = {Moriarty, John and Vogrinc, Jure and Zocca, Alessandro},
  doi          = {10.1007/s11222-021-10044-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {A metropolis-class sampler for targets with non-convex support},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the performance of particle filters with adaptive number
of particles. <em>SAC</em>, <em>31</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-10056-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the performance of a class of particle filters (PFs) that can automatically tune their computational complexity by evaluating online certain predictive statistics which are invariant for a broad class of state-space models. To be specific, we propose a family of block-adaptive PFs based on the methodology of Elvira et al. (IEEE Trans Signal Process 65(7):1781–1794, 2017). In this class of algorithms, the number of Monte Carlo samples (known as particles) is adjusted periodically, and we prove that the theoretical error bounds of the PF actually adapt to the updates in the number of particles. The evaluation of the predictive statistics that lies at the core of the methodology is done by generating fictitious observations, i.e., particles in the observation space. We study, both analytically and numerically, the impact of the number K of these particles on the performance of the algorithm. In particular, we prove that if the predictive statistics with K fictitious observations converged exactly, then the particle approximation of the filtering distribution would match the first K elements in a series of moments of the true filter. This result can be understood as a converse to some convergence theorems for PFs. From this analysis, we deduce an alternative predictive statistic that can be computed (for some models) without sampling any fictitious observations at all. Finally, we conduct an extensive simulation study that illustrates the theoretical results and provides further insights into the complexity, performance and behavior of the new class of algorithms.},
  archive      = {J_SAC},
  author       = {Elvira, Víctor and Miguez, Joaquín and Djurić, Petar M.},
  doi          = {10.1007/s11222-021-10056-0},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {On the performance of particle filters with adaptive number of particles},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistent online gaussian process regression without the
sample complexity bottleneck. <em>SAC</em>, <em>31</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-10051-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes provide a framework for nonlinear nonparametric Bayesian inference widely applicable across science and engineering. Unfortunately, their computational burden scales cubically with the training sample size, which in the case that samples arrive in perpetuity, approaches infinity. This issue necessitates approximations for use with streaming data, which to date mostly lack convergence guarantees. Thus, we develop the first online Gaussian process approximation that preserves convergence to the population posterior, i.e., asymptotic posterior consistency, while ameliorating its intractable complexity growth with the sample size. We propose an online compression scheme that, following each a posteriori update, fixes an error neighborhood with respect to the Hellinger metric centered at the current posterior, and greedily tosses out past kernel dictionary elements until its boundary is hit. We call the resulting method Parsimonious Online Gaussian Processes (POG). For diminishing error radius, asymptotic statistical stationarity is achieved (Theorem 1ii) at the cost of unbounded memory in the limit. On the other hand, for constant error radius, POG converges to a neighborhood of stationarity (Theorem 1ii) but with finite memory at-worst determined by the metric entropy of the feature space (Theorem 2). Here stationarity refers to the distributional distance between sequential marginal posteriors approaching null with the time index. Experimental results are presented on several nonlinear regression problems which illuminates the merits of this approach as compared with alternatives that fix the subspace dimension defining the history of past points.},
  archive      = {J_SAC},
  author       = {Koppel, Alec and Pradhan, Hrusikesha and Rajawat, Ketan},
  doi          = {10.1007/s11222-021-10051-5},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Consistent online gaussian process regression without the sample complexity bottleneck},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-dimensional structure learning of sparse vector
autoregressive models using fractional marginal pseudo-likelihood.
<em>SAC</em>, <em>31</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-10049-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning vector autoregressive models from multivariate time series is conventionally approached through least squares or maximum likelihood estimation. These methods typically assume a fully connected model which provides no direct insight to the model structure and may lead to highly noisy estimates of the parameters. Because of these limitations, there has been an increasing interest towards methods that produce sparse estimates through penalized regression. However, such methods are computationally intensive and may become prohibitively time-consuming when the number of variables in the model increases. In this paper we adopt an approximate Bayesian approach to the learning problem by combining fractional marginal likelihood and pseudo-likelihood. We propose a novel method, PLVAR, that is both faster and produces more accurate estimates than the state-of-the-art methods based on penalized regression. We prove the consistency of the PLVAR estimator and demonstrate the attractive performance of the method on both simulated and real-world data.},
  archive      = {J_SAC},
  author       = {Suotsalo, Kimmo and Xu, Yingying and Corander, Jukka and Pensar, Johan},
  doi          = {10.1007/s11222-021-10049-z},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {High-dimensional structure learning of sparse vector autoregressive models using fractional marginal pseudo-likelihood},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained minimum energy designs. <em>SAC</em>,
<em>31</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s11222-021-10054-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-filling designs are important in computer experiments, which are critical for building a cheap surrogate model that adequately approximates an expensive computer code. Many design construction techniques in the existing literature are only applicable for rectangular bounded space, but in real-world applications, the input space can often be non-rectangular because of constraints on the input variables. One solution to generate designs in a constrained space is to first generate uniformly distributed samples in the feasible region, and then use them as the candidate set to construct the designs. Sequentially constrained Monte Carlo (SCMC) is the state-of-the-art technique for candidate generation, but it still requires large number of constraint evaluations, which is problematic especially when the constraints are expensive to evaluate. Thus, to reduce constraint evaluations and improve efficiency, we propose the constrained minimum energy design (CoMinED) that utilizes recent advances in deterministic sampling methods. Extensive simulation results on 15 benchmark problems with dimensions ranging from 2 to 13 are provided for demonstrating the improved performance of CoMinED over the existing methods.},
  archive      = {J_SAC},
  author       = {Huang, Chaofan and Joseph, V. Roshan and Ray, Douglas M.},
  doi          = {10.1007/s11222-021-10054-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Constrained minimum energy designs},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient importance sampling for large sums of independent
and identically distributed random variables. <em>SAC</em>,
<em>31</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11222-021-10055-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss estimating the probability that the sum of nonnegative independent and identically distributed random variables falls below a given threshold, i.e., $$\mathbb {P}(\sum _{i=1}^{N}{X_i} \le \gamma )$$ , via importance sampling (IS). We are particularly interested in the rare event regime when N is large and/or $$\gamma $$ is small. The exponential twisting is a popular technique for similar problems that, in most cases, compares favorably to other estimators. However, it has some limitations: (i) It assumes the knowledge of the moment-generating function of $$X_i$$ and (ii) sampling under the new IS PDF is not straightforward and might be expensive. The aim of this work is to propose an alternative IS PDF that approximately yields, for certain classes of distributions and in the rare event regime, at least the same performance as the exponential twisting technique and, at the same time, does not introduce serious limitations. The first class includes distributions whose probability density functions (PDFs) are asymptotically equivalent, as $$x \rightarrow 0$$ , to $$bx^{p}$$ , for $$p&gt;-1$$ and $$b&gt;0$$ . For this class of distributions, the Gamma IS PDF with appropriately chosen parameters retrieves approximately, in the rare event regime corresponding to small values of $$\gamma $$ and/or large values of N, the same performance of the estimator based on the use of the exponential twisting technique. In the second class, we consider the Log-normal setting, whose PDF at zero vanishes faster than any polynomial, and we show numerically that a Gamma IS PDF with optimized parameters clearly outperforms the exponential twisting IS PDF. Numerical experiments validate the efficiency of the proposed estimator in delivering a highly accurate estimate in the regime of large N and/or small $$\gamma $$ .},
  archive      = {J_SAC},
  author       = {Ben Rached, Nadhir and Haji-Ali, Abdul-Lateef and Rubino, Gerardo and Tempone, Raúl},
  doi          = {10.1007/s11222-021-10055-1},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Efficient importance sampling for large sums of independent and identically distributed random variables},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal design of multifactor experiments via grid
exploration. <em>SAC</em>, <em>31</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s11222-021-10046-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an algorithm for computing efficient approximate experimental designs that can be applied in the case of very large grid-like design spaces. Such a design space typically corresponds to the set of all combinations of multiple genuinely discrete factors or densely discretized continuous factors. The proposed algorithm alternates between two key steps: (1) the construction of exploration sets composed of star-shaped components and separate, highly informative design points and (2) the application of a conventional method for computing optimal approximate designs on medium-sized design spaces. For a given design, the star-shaped components are constructed by selecting all points that differ in at most one coordinate from some support point of the design. Because of the reliance on these star sets, we call our algorithm the galaxy exploration method (GEX). We demonstrate that GEX significantly outperforms several state-of-the-art algorithms when applied to D-optimal design problems for linear, generalized linear and nonlinear regression models with continuous and mixed factors. Importantly, we provide a free R code that permits direct verification of the numerical results and allows researchers to easily compute optimal or nearly optimal experimental designs for their own statistical models.},
  archive      = {J_SAC},
  author       = {Harman, Radoslav and Filová, Lenka and Rosa, Samuel},
  doi          = {10.1007/s11222-021-10046-2},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Optimal design of multifactor experiments via grid exploration},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting conflicting summary statistics in likelihood-free
inference. <em>SAC</em>, <em>31</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s11222-021-10053-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian likelihood-free methods implement Bayesian inference using simulation of data from the model to substitute for intractable likelihood evaluations. Most likelihood-free inference methods replace the full data set with a summary statistic before performing Bayesian inference, and the choice of this statistic is often difficult. The summary statistic should be low-dimensional for computational reasons, while retaining as much information as possible about the parameter. Using a recent idea from the interpretable machine learning literature, we develop some regression-based diagnostic methods which are useful for detecting when different parts of a summary statistic vector contain conflicting information about the model parameters. Conflicts of this kind complicate summary statistic choice, and detecting them can be insightful about model deficiencies and guide model improvement. The diagnostic methods developed are based on regression approaches to likelihood-free inference, in which the regression model estimates the posterior density using summary statistics as features. Deletion and imputation of part of the summary statistic vector within the regression model can remove conflicts and approximate posterior distributions for summary statistic subsets. A larger than expected change in the estimated posterior density following deletion and imputation can indicate a conflict in which inferences of interest are affected. The usefulness of the new methods is demonstrated in a number of real examples.},
  archive      = {J_SAC},
  author       = {Mao, Yinan and Wang, Xueou and Nott, David J. and Evans, Michael},
  doi          = {10.1007/s11222-021-10053-3},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Detecting conflicting summary statistics in likelihood-free inference},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Penalized cox’s proportional hazards model for
high-dimensional survival data with grouped predictors. <em>SAC</em>,
<em>31</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s11222-021-10052-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of next-generation sequencing technologies has made it possible to measure the expression profiles of thousands of genes simultaneously. Often, there exist group structures among genes manifesting biological pathways and functional relationships. Analyzing such high-dimensional and structural datasets can be computationally expensive and results in the complicated models that are hard to interpret. To address this, variable selection such as penalized methods are often taken. Here, we focus on the Cox’s proportional hazards model to deal with censoring data. Most of the existing penalized methods for Cox’s model are the group lasso methods that show deficiencies, including the over-shrinkage problem. In addition, the contemporary algorithms either exhibit the loss of efficiency or require the group-wise orthonormality assumption. Hence, efficient algorithms for general design matrices are needed to enable practical applications. In this paper, we investigate and comprehensively evaluate three group penalized methods for Cox’s model: the group lasso and two nonconvex penalization methods—group SCAD and group MCP—that have several advantages over the group lasso. These methods are able to perform group selection in both non-overlapping and overlapping cases. We have developed the fast and stable algorithms and a new package grpCox to fit these models without the initial orthonormalization step. The runtime of grpCox is improved significantly over the existing packages, such as grpsurv (for the non-overlapping case), grpregOverlap (overlapping), and SGL. In addition, grpCox is better than grpsurv and comparable with SGL in terms of variable selection performances. Comprehensive studies on both simulation and real-world cancer datasets demonstrate the statistical properties of our grpCox implementations with the group lasso, SCAD, and MCP regularization terms.},
  archive      = {J_SAC},
  author       = {Dang, Xuan and Huang, Shuai and Qian, Xiaoning},
  doi          = {10.1007/s11222-021-10052-4},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Penalized cox’s proportional hazards model for high-dimensional survival data with grouped predictors},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep state-space gaussian processes. <em>SAC</em>,
<em>31</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s11222-021-10050-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with a state-space approach to deep Gaussian process (DGP) regression. We construct the DGP by hierarchically putting transformed Gaussian process (GP) priors on the length scales and magnitudes of the next level of Gaussian processes in the hierarchy. The idea of the state-space approach is to represent the DGP as a non-linear hierarchical system of linear stochastic differential equations (SDEs), where each SDE corresponds to a conditional GP. The DGP regression problem then becomes a state estimation problem, and we can estimate the state efficiently with sequential methods by using the Markov property of the state-space DGP. The computational complexity scales linearly with respect to the number of measurements. Based on this, we formulate state-space MAP as well as Bayesian filtering and smoothing solutions to the DGP regression problem. We demonstrate the performance of the proposed models and methods on synthetic non-stationary signals and apply the state-space DGP to detection of the gravitational waves from LIGO measurements.},
  archive      = {J_SAC},
  author       = {Zhao, Zheng and Emzir, Muhammad and Särkkä, Simo},
  doi          = {10.1007/s11222-021-10050-6},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Deep state-space gaussian processes},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anytime parallel tempering. <em>SAC</em>, <em>31</em>(6),
1–23. (<a href="https://doi.org/10.1007/s11222-021-10048-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing efficient MCMC algorithms is indispensable in Bayesian inference. In parallel tempering, multiple interacting MCMC chains run to more efficiently explore the state space and improve performance. The multiple chains advance independently through local moves, and the performance enhancement steps are exchange moves, where the chains pause to exchange their current sample amongst each other. To accelerate the independent local moves, they may be performed simultaneously on multiple processors. Another problem is then encountered: depending on the MCMC implementation and inference problem, local moves can take a varying and random amount of time to complete. There may also be infrastructure-induced variations, such as competing jobs on the same processors, which arises in cloud computing. Before exchanges can occur, all chains must complete the local moves they are engaged in to avoid introducing a potentially substantial bias (Proposition 1). To solve this issue of randomly varying local move completion times in multi-processor parallel tempering, we adopt the Anytime Monte Carlo framework of (Murray, L. M., Singh, S., Jacob, P. E., and Lee, A.: Anytime Monte Carlo. arXiv preprint arXiv:1612.03319 , (2016): we impose real-time deadlines on the parallel local moves and perform exchanges at these deadlines without any processor idling. We show our methodology for exchanges at real-time deadlines does not introduce a bias and leads to significant performance enhancements over the naïve approach of idling until every processor’s local moves complete. The methodology is then applied in an ABC setting, where an Anytime ABC parallel tempering algorithm is derived for the difficult task of estimating the parameters of a Lotka–Volterra predator-prey model, and similar efficiency enhancements are observed.},
  archive      = {J_SAC},
  author       = {Marie d’Avigneau, Alix and Singh, Sumeetpal S. and Murray, Lawrence M.},
  doi          = {10.1007/s11222-021-10048-0},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Anytime parallel tempering},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tree based credible set estimation. <em>SAC</em>,
<em>31</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s11222-021-10045-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating a joint Highest Posterior Density credible set for a multivariate posterior density is challenging as dimension gets larger. Credible intervals for univariate marginals are usually presented for ease of computation and visualisation. There are often two layers of approximation, as we may need to compute a credible set for a target density which is itself only an approximation to the true posterior density. We obtain joint Highest Posterior Density credible sets for density estimation trees given by Li et al. (in: Lee, Sugiyama, Luxburg, Guyon, Garnett (eds) Advances in neural information processing systems, Curran Associates Inc, Red Hook, 2016) approximating a density truncated to a compact subset of $$\mathbb {R}^d$$ as this is preferred to a copula construction. These trees approximate a joint posterior distribution from posterior samples using a piecewise constant function defined by sequential binary splits. We use a consistent estimator to measure of the symmetric difference between our credible set estimate and the true HPD set of the target density samples. This quality measure can be computed without the need to know the true set. We show how the true-posterior-coverage of an approximate credible set estimated for an approximate target density may be estimated in doubly intractable cases where posterior samples are not available. We illustrate our methods with simulation studies and find that our estimator is competitive with existing methods.},
  archive      = {J_SAC},
  author       = {Lee, Jeong Eun and Nicholls, Geoff K.},
  doi          = {10.1007/s11222-021-10045-3},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Tree based credible set estimation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational bayes on manifolds. <em>SAC</em>,
<em>31</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10047-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational Bayes (VB) has become a widely-used tool for Bayesian inference in statistics and machine learning. Nonetheless, the development of the existing VB algorithms is so far generally restricted to the case where the variational parameter space is Euclidean, which hinders the potential broad application of VB methods. This paper extends the scope of VB to the case where the variational parameter space is a Riemannian manifold. We develop an efficient manifold-based VB algorithm that exploits both the geometric structure of the constraint parameter space and the information geometry of the manifold of VB approximating probability distributions. Our algorithm is provably convergent and achieves a convergence rate of order $${\mathcal {O}}(1/\sqrt{T})$$ and $$\mathcal O(1/T^{2-2\epsilon })$$ for a non-convex evidence lower bound function and a strongly retraction-convex evidence lower bound function, respectively. We develop in particular two manifold VB algorithms, Manifold Gaussian VB and Manifold Wishart VB, and demonstrate through numerical experiments that the proposed algorithms are stable, less sensitive to initialization and compares favourably to existing VB methods.},
  archive      = {J_SAC},
  author       = {Tran, Minh-Ngoc and Nguyen, Dang H. and Nguyen, Duy},
  doi          = {10.1007/s11222-021-10047-1},
  journal      = {Statistics and Computing},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayes on manifolds},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatiotemporal blocking of the bouncy particle sampler for
efficient inference in state-space models. <em>SAC</em>, <em>31</em>(5),
1–15. (<a href="https://doi.org/10.1007/s11222-021-10034-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel blocked version of the continuous-time bouncy particle sampler of Bouchard-Côté et al. (J Am Stat Assoc 113(522):855–867, 2018) which is applicable to any differentiable probability density. This alternative implementation is motivated by blocked Gibbs sampling for state-space models (Singh et al. in Biometrika 104(4):953–969, 2017) and leads to significant improvement in terms of effective sample size per second, and furthermore, allows for significant parallelization of the resulting algorithm. The new algorithms are particularly efficient for latent state inference in high-dimensional state-space models, where blocking in both space and time is necessary to avoid degeneracy of MCMC. The efficiency of our blocked bouncy particle sampler, in comparison with both the standard implementation of the bouncy particle sampler and the particle Gibbs algorithm of Andrieu et al. (J R Stat Soc Ser B Stat Methodol 72(3):269–342, 2010), is illustrated numerically for both simulated data and a challenging real-world financial dataset.},
  archive      = {J_SAC},
  author       = {Goldman, Jacob Vorstrup and Singh, Sumeetpal S.},
  doi          = {10.1007/s11222-021-10034-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Spatiotemporal blocking of the bouncy particle sampler for efficient inference in state-space models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast generation of gaussian random fields for direct
numerical simulations of stochastic transport. <em>SAC</em>,
<em>31</em>(5), 1–15. (<a
href="https://doi.org/10.1007/s11222-021-10035-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel discrete method of constructing Gaussian Random Fields based on a combination of modified spectral representations, Fourier and Blob. The method is intended for Direct Numerical Simulations of the V-Langevin equations. The latter are stereotypical descriptions of anomalous stochastic transport in various physical systems. From an Eulerian perspective, our method is designed to exhibit improved convergence rates. From a Lagrangian perspective, our method offers a pertinent description of particle trajectories in turbulent velocity fields: the exact Lagrangian invariant laws are well reproduced. From a computational perspective, the computing time is reduced by a factor of two in comparison with Fourier-like or Blob-like methods and an order of magnitude in comparison with FFT algorithms.},
  archive      = {J_SAC},
  author       = {Palade, D. I. and Vlad, M.},
  doi          = {10.1007/s11222-021-10035-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Fast generation of gaussian random fields for direct numerical simulations of stochastic transport},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian inference for continuous-time hidden markov models
with an unknown number of states. <em>SAC</em>, <em>31</em>(5), 1–15.
(<a href="https://doi.org/10.1007/s11222-021-10032-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the modeling of data generated by a latent continuous-time Markov jump process with a state space of finite but unknown dimensions. Typically in such models, the number of states has to be pre-specified, and Bayesian inference for a fixed number of states has not been studied until recently. In addition, although approaches to address the problem for discrete-time models have been developed, no method has been successfully implemented for the continuous-time case. We focus on reversible jump Markov chain Monte Carlo which allows the trans-dimensional move among different numbers of states in order to perform Bayesian inference for the unknown number of states. Specifically, we propose an efficient split-combine move which can facilitate the exploration of the parameter space, and demonstrate that it can be implemented effectively at scale. Subsequently, we extend this algorithm to the context of model-based clustering, allowing numbers of states and clusters both determined during the analysis. The model formulation, inference methodology, and associated algorithm are illustrated by simulation studies. Finally, we apply this method to real data from a Canadian healthcare system in Quebec.},
  archive      = {J_SAC},
  author       = {Luo, Yu and Stephens, David A.},
  doi          = {10.1007/s11222-021-10032-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian inference for continuous-time hidden markov models with an unknown number of states},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized parallel tempering on bayesian inverse problems.
<em>SAC</em>, <em>31</em>(5), 1–26. (<a
href="https://doi.org/10.1007/s11222-021-10042-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current work we present two generalizations of the Parallel Tempering algorithm in the context of discrete-time Markov chain Monte Carlo methods for Bayesian inverse problems. These generalizations use state-dependent swapping rates, inspired by the so-called continuous time Infinite Swapping algorithm presented in Plattner et al. (J Chem Phys 135(13):134111, 2011). We analyze the reversibility and ergodicity properties of our generalized PT algorithms. Numerical results on sampling from different target distributions, show that the proposed methods significantly improve sampling efficiency over more traditional sampling algorithms such as Random Walk Metropolis, preconditioned Crank–Nicolson, and (standard) Parallel Tempering.},
  archive      = {J_SAC},
  author       = {Latz, Jonas and Madrigal-Cianci, Juan P. and Nobile, Fabio and Tempone, Raúl},
  doi          = {10.1007/s11222-021-10042-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Generalized parallel tempering on bayesian inverse problems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating sequential monte carlo with surrogate
likelihoods. <em>SAC</em>, <em>31</em>(5), 1–26. (<a
href="https://doi.org/10.1007/s11222-021-10036-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delayed-acceptance is a technique for reducing computational effort for Bayesian models with expensive likelihoods. Using a delayed-acceptance kernel for Markov chain Monte Carlo can reduce the number of expensive likelihoods evaluations required to approximate a posterior expectation. Delayed-acceptance uses a surrogate, or approximate, likelihood to avoid evaluation of the expensive likelihood when possible. Within the sequential Monte Carlo framework, we utilise the history of the sampler to adaptively tune the surrogate likelihood to yield better approximations of the expensive likelihood and use a surrogate first annealing schedule to further increase computational efficiency. Moreover, we propose a framework for optimising computation time whilst avoiding particle degeneracy, which encapsulates existing strategies in the literature. Overall, we develop a novel algorithm for computationally efficient SMC with expensive likelihood functions. The method is applied to static Bayesian models, which we demonstrate on toy and real examples.},
  archive      = {J_SAC},
  author       = {Bon, Joshua J. and Lee, Anthony and Drovandi, Christopher},
  doi          = {10.1007/s11222-021-10036-4},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Accelerating sequential monte carlo with surrogate likelihoods},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Outlier detection in non-elliptical data by kernel MRCD.
<em>SAC</em>, <em>31</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-10041-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum regularized covariance determinant method (MRCD) is a robust estimator for multivariate location and scatter, which detects outliers by fitting a robust covariance matrix to the data. Its regularization ensures that the covariance matrix is well-conditioned in any dimension. The MRCD assumes that the non-outlying observations are roughly elliptically distributed, but many datasets are not of that form. Moreover, the computation time of MRCD increases substantially when the number of variables goes up, and nowadays datasets with many variables are common. The proposed kernel minimum regularized covariance determinant (KMRCD) estimator addresses both issues. It is not restricted to elliptical data because it implicitly computes the MRCD estimates in a kernel-induced feature space. A fast algorithm is constructed that starts from kernel-based initial estimates and exploits the kernel trick to speed up the subsequent computations. Based on the KMRCD estimates, a rule is proposed to flag outliers. The KMRCD algorithm performs well in simulations, and is illustrated on real-life data.},
  archive      = {J_SAC},
  author       = {Schreurs, Joachim and Vranckx, Iwein and Hubert, Mia and Suykens, Johan A. K. and Rousseeuw, Peter J.},
  doi          = {10.1007/s11222-021-10041-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Outlier detection in non-elliptical data by kernel MRCD},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble slice sampling. <em>SAC</em>, <em>31</em>(5), 1–18.
(<a href="https://doi.org/10.1007/s11222-021-10038-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Slice sampling has emerged as a powerful Markov Chain Monte Carlo algorithm that adapts to the characteristics of the target distribution with minimal hand-tuning. However, Slice Sampling’s performance is highly sensitive to the user-specified initial length scale hyperparameter and the method generally struggles with poorly scaled or strongly correlated distributions. This paper introduces Ensemble Slice Sampling (ESS), a new class of algorithms that bypasses such difficulties by adaptively tuning the initial length scale and utilising an ensemble of parallel walkers in order to efficiently handle strong correlations between parameters. These affine-invariant algorithms are trivial to construct, require no hand-tuning, and can easily be implemented in parallel computing environments. Empirical tests show that Ensemble Slice Sampling can improve efficiency by more than an order of magnitude compared to conventional MCMC methods on a broad range of highly correlated target distributions. In cases of strongly multimodal target distributions, Ensemble Slice Sampling can sample efficiently even in high dimensions. We argue that the parallel, black-box and gradient-free nature of the method renders it ideal for use in scientific fields such as physics, astrophysics and cosmology which are dominated by a wide variety of computationally expensive and non-differentiable models.},
  archive      = {J_SAC},
  author       = {Karamanis, Minas and Beutler, Florian},
  doi          = {10.1007/s11222-021-10038-2},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Ensemble slice sampling},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A branch-and-bound algorithm for the exact optimal
experimental design problem. <em>SAC</em>, <em>31</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11222-021-10043-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss a generalisation of the approximate optimal experimental design problem, in which the weight of each regression point needs to stay in a closed interval. We work with Kiefer’s optimality criteria which include the well-known D- and A-optimality as special cases. We propose a first-order algorithm for the generalised problem that redistributes the weights of two regression points in each iteration. We develop a branch-and-bound algorithm for exact optimal experimental design problems under Kiefer’s criteria where the subproblems in the search tree are equivalent to the generalized approximate design problem, and therefore, can be solved efficiently by the first-order method. We observe that our branch-and-bound algorithm is favourable to a popular exchange heuristic for certain problem instances.},
  archive      = {J_SAC},
  author       = {Ahipaşaoğlu, Selin Damla},
  doi          = {10.1007/s11222-021-10043-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {A branch-and-bound algorithm for the exact optimal experimental design problem},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Min–max crossover designs for two treatments binary and
poisson crossover trials. <em>SAC</em>, <em>31</em>(5), 1–11. (<a
href="https://doi.org/10.1007/s11222-021-10029-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article min–max crossover designs for binary and Poisson crossover trials with two treatments are proposed. Models with and without carryover effects are considered. Min–max designs for periods 2 and 3 are discussed in details. A sensitivity analysis is performed to assess the robustness of proposed designs when compared to existing optimal designs. An equivalence theorem is provided to verify optimality of min–max designs.},
  archive      = {J_SAC},
  author       = {Singh, Satya Prakash and Mukhopadhyay, Siuli and Raj, Harsh},
  doi          = {10.1007/s11222-021-10029-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Min–max crossover designs for two treatments binary and poisson crossover trials},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying uncertainty with a derivative tracking SDE model
and application to wind power forecast data. <em>SAC</em>,
<em>31</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-021-10040-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a data-driven methodology based on parametric Itô’s Stochastic Differential Equations (SDEs) to capture the real asymmetric dynamics of forecast errors, including the uncertainty of the forecast at time zero. Our SDE framework features time-derivative tracking of the forecast, time-varying mean-reversion parameter, and an improved state-dependent diffusion term. Proofs of the existence, strong uniqueness, and boundedness of the SDE solutions are shown by imposing conditions on the time-varying mean-reversion parameter. We develop the structure of the drift term based on sound mathematical theory. A truncation procedure regularizes the prediction function to ensure that the trajectories do not reach the boundaries almost surely in a finite time. Inference based on approximate likelihood, constructed through the moment-matching technique both in the original forecast error space and in the Lamperti space, is performed through numerical optimization procedures. We propose a fixed-point likelihood optimization approach in the Lamperti space. Another novel contribution is the characterization of the uncertainty of the forecast at time zero, which turns out to be crucial in practice. We extend the model specification by considering the length of the unknown time interval preceding the first time a forecast is provided through an additional parameter in the density of the initial transition. All the procedures are agnostic of the forecasting technology, and they enable comparisons between different forecast providers. We apply our SDE framework to model historical Uruguayan normalized wind power production and forecast data between April and December 2019. Sharp empirical confidence bands of wind power production forecast error are obtained for the best-selected model.},
  archive      = {J_SAC},
  author       = {Caballero, Renzo and Kebaier, Ahmed and Scavino, Marco and Tempone, Raúl},
  doi          = {10.1007/s11222-021-10040-8},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Quantifying uncertainty with a derivative tracking SDE model and application to wind power forecast data},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty modelling and computational aspects of data
association. <em>SAC</em>, <em>31</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s11222-021-10039-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel solution to the smoothing problem for multi-object dynamical systems is proposed and evaluated. The systems of interest contain an unknown and varying number of dynamical objects that are partially observed under noisy and corrupted observations. In order to account for the lack of information about the different aspects of this type of complex system, an alternative representation of uncertainty based on possibility theory is considered. It is shown how analogues of usual concepts such as Markov chains and hidden Markov models (HMMs) can be introduced in this context. In particular, the considered statistical model for multiple dynamical objects can be formulated as a hierarchical model consisting of conditionally independent HMMs. This structure is leveraged to propose an efficient method in the context of Markov chain Monte Carlo (MCMC) by relying on an approximate solution to the corresponding filtering problem, in a similar fashion to particle MCMC. This approach is shown to outperform existing algorithms in a range of scenarios.},
  archive      = {J_SAC},
  author       = {Houssineau, Jeremie and Zeng, Jiajie and Jasra, Ajay},
  doi          = {10.1007/s11222-021-10039-1},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Uncertainty modelling and computational aspects of data association},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nonnegative bayesian nonparametric factor models with
completely random measures. <em>SAC</em>, <em>31</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s11222-021-10037-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Bayesian nonparametric Poisson factorization model for modeling dense network data with an unknown and potentially growing number of overlapping communities. The construction is based on completely random measures and allows the number of communities to either increase with the number of nodes at a specified logarithmic or polynomial rate, or be bounded. We develop asymptotics for the number and size of the communities of the network and derive a Markov chain Monte Carlo algorithm for targeting the exact posterior distribution for this model. The usefulness of the approach is illustrated on various real networks.},
  archive      = {J_SAC},
  author       = {Ayed, Fadhel and Caron, François},
  doi          = {10.1007/s11222-021-10037-3},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Nonnegative bayesian nonparametric factor models with completely random measures},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data fusion using factor analysis and low-rank matrix
completion. <em>SAC</em>, <em>31</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s11222-021-10033-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data fusion involves the integration of multiple related datasets. The statistical file-matching problem is a canonical data fusion problem in multivariate analysis, where the objective is to characterise the joint distribution of a set of variables when only strict subsets of marginal distributions have been observed. Estimation of the covariance matrix of the full set of variables is challenging given the missing-data pattern. Factor analysis models use lower-dimensional latent variables in the data-generating process, and this introduces low-rank components in the complete-data matrix and the population covariance matrix. The low-rank structure of the factor analysis model can be exploited to estimate the full covariance matrix from incomplete data via low-rank matrix completion. We prove the identifiability of the factor analysis model in the statistical file-matching problem under conditions on the number of factors and the number of shared variables over the observed marginal subsets. Additionally, we provide an EM algorithm for parameter estimation. On several real datasets, the factor model gives smaller reconstruction errors in file-matching problems than the common approaches for low-rank matrix completion.},
  archive      = {J_SAC},
  author       = {Ahfock, Daniel and Pyne, Saumyadipta and McLachlan, Geoffrey J.},
  doi          = {10.1007/s11222-021-10033-7},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Data fusion using factor analysis and low-rank matrix completion},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty calibration for probabilistic projection
methods. <em>SAC</em>, <em>31</em>(5), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10031-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical Krylov subspace projection methods for the solution of linear problem $$Ax = b$$ output an approximate solution $${\widetilde{x}}\simeq x$$ . Recently, it has been recognized that projection methods can be understood from a statistical perspective. These probabilistic projection methods return a distribution $$p({\widetilde{x}})$$ in place of a point estimate $${\widetilde{x}}$$ . The resulting uncertainty, codified as a distribution, can, in theory, be meaningfully combined with other uncertainties, can be propagated through computational pipelines, and can be used in the framework of probabilistic decision theory. The problem we address is that the current probabilistic projection methods lead to the poorly calibrated posterior distribution. We improve the covariance matrix from previous works in a way that it does not contain such undesirable objects as $$A^{-1}$$ or $$A^{-1}A^{-T}$$ , results in nontrivial uncertainty, and reproduces an arbitrary projection method as a mean of the posterior distribution. We also propose a variant that is numerically inexpensive in the case the uncertainty is calibrated a priori. Since it usually is not, we put forward a practical way to calibrate uncertainty that performs reasonably well, albeit at the expense of roughly doubling the numerical cost of the underlying projection method.},
  archive      = {J_SAC},
  author       = {Fanaskov, Vladimir},
  doi          = {10.1007/s11222-021-10031-9},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Uncertainty calibration for probabilistic projection methods},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian numerical methods for nonlinear partial
differential equations. <em>SAC</em>, <em>31</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s11222-021-10030-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The numerical solution of differential equations can be formulated as an inference problem to which formal statistical approaches can be applied. However, nonlinear partial differential equations (PDEs) pose substantial challenges from an inferential perspective, most notably the absence of explicit conditioning formula. This paper extends earlier work on linear PDEs to a general class of initial value problems specified by nonlinear PDEs, motivated by problems for which evaluations of the right-hand-side, initial conditions, or boundary conditions of the PDE have a high computational cost. The proposed method can be viewed as exact Bayesian inference under an approximate likelihood, which is based on discretisation of the nonlinear differential operator. Proof-of-concept experimental results demonstrate that meaningful probabilistic uncertainty quantification for the unknown solution of the PDE can be performed, while controlling the number of times the right-hand-side, initial and boundary conditions are evaluated. A suitable prior model for the solution of PDEs is identified using novel theoretical analysis of the sample path properties of Matérn processes, which may be of independent interest.},
  archive      = {J_SAC},
  author       = {Wang, Junyang and Cockayne, Jon and Chkrebtii, Oksana and Sullivan, T. J. and Oates, Chris. J.},
  doi          = {10.1007/s11222-021-10030-w},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian numerical methods for nonlinear partial differential equations},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fisher scoring for crossed factor linear mixed models.
<em>SAC</em>, <em>31</em>(5), 1–25. (<a
href="https://doi.org/10.1007/s11222-021-10026-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of longitudinal, heterogeneous or unbalanced clustered data is of primary importance to a wide range of applications. The linear mixed model (LMM) is a popular and flexible extension of the linear model specifically designed for such purposes. Historically, a large proportion of material published on the LMM concerns the application of popular numerical optimization algorithms, such as Newton–Raphson, Fisher Scoring and expectation maximization to single-factor LMMs (i.e. LMMs that only contain one “factor” by which observations are grouped). However, in recent years, the focus of the LMM literature has moved towards the development of estimation and inference methods for more complex, multi-factored designs. In this paper, we present and derive new expressions for the extension of an algorithm classically used for single-factor LMM parameter estimation, Fisher Scoring, to multiple, crossed-factor designs. Through simulation and real data examples, we compare five variants of the Fisher Scoring algorithm with one another, as well as against a baseline established by the R package lme4, and find evidence of correctness and strong computational efficiency for four of the five proposed approaches. Additionally, we provide a new method for LMM Satterthwaite degrees of freedom estimation based on analytical results, which does not require iterative gradient estimation. Via simulation, we find that this approach produces estimates with both lower bias and lower variance than the existing methods.},
  archive      = {J_SAC},
  author       = {Maullin-Sapey, Thomas and Nichols, Thomas E.},
  doi          = {10.1007/s11222-021-10026-6},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Fisher scoring for crossed factor linear mixed models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence graphs for graphical model selection.
<em>SAC</em>, <em>31</em>(5), 1–21. (<a
href="https://doi.org/10.1007/s11222-021-10027-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce the concept of confidence graphs (CG) for graphical model selection. CG first identifies two nested graphical models—called small and large confidence graphs (SCG and LCG)—trapping the true graphical model in between at a given level of confidence, just like the endpoints of traditional confidence interval capturing the population parameter. Therefore, SCG and LCG provide us with more insights about the simplest and most complex forms of dependence structure the true model can possibly be, and their difference also offers us a measure of model selection uncertainty. In addition, rather than relying on a single selected model, CG consists of a group of graphical models between SCG and LCG as the candidates. The proposed method can be coupled with many popular model selection methods, making it an ideal tool for comparing model selection uncertainty as well as measuring reproducibility. We also propose a new residual bootstrap procedure for graphical model settings to approximate the sampling distribution of the selected models and to obtain CG. To visualize the distribution of selected models and its associated uncertainty, we further develop new graphical tools, such as grouped model selection distribution plot. Numerical studies further illustrate the advantages of the proposed method.},
  archive      = {J_SAC},
  author       = {Wang, Linna and Qin, Yichen and Li, Yang},
  doi          = {10.1007/s11222-021-10027-5},
  journal      = {Statistics and Computing},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Confidence graphs for graphical model selection},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved inference for areal unit count data using
graph-based optimisation. <em>SAC</em>, <em>31</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10025-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal count data relating to a set of non-overlapping areal units are prevalent in many fields, including epidemiology and social science. The spatial autocorrelation inherent in these data is typically modelled by a set of random effects that are assigned a conditional autoregressive prior distribution, which is a special case of a Gaussian Markov random field. The autocorrelation structure implied by this model depends on a binary neighbourhood matrix, where two random effects are assumed to be partially autocorrelated if their areal units share a common border, and are conditionally independent otherwise. This paper proposes a novel graph-based optimisation algorithm for estimating either a static or a temporally varying neighbourhood matrix for the data that better represents its spatial correlation structure, by viewing the areal units as the vertices of a graph and the neighbour relations as the set of edges. The improved estimation performance of our methodology compared to the commonly used border sharing rule is evidenced by simulation, before the method is applied to a new respiratory disease surveillance study in Scotland between 2011 and 2017.},
  archive      = {J_SAC},
  author       = {Lee, Duncan and Meeks, Kitty and Pettersson, William},
  doi          = {10.1007/s11222-021-10025-7},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Improved inference for areal unit count data using graph-based optimisation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust and efficient algorithm to find profile likelihood
confidence intervals. <em>SAC</em>, <em>31</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10012-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Profile likelihood confidence intervals are a robust alternative to Wald’s method if the asymptotic properties of the maximum likelihood estimator are not met. However, the constrained optimization problem defining profile likelihood confidence intervals can be difficult to solve in these situations, because the likelihood function may exhibit unfavorable properties. As a result, existing methods may be inefficient and yield misleading results. In this paper, we address this problem by computing profile likelihood confidence intervals via a trust-region approach, where steps computed based on local approximations are constrained to regions where these approximations are sufficiently precise. As our algorithm also accounts for numerical issues arising if the likelihood function is strongly non-linear or parameters are not estimable, the method is applicable in many scenarios where earlier approaches are shown to be unreliable. To demonstrate its potential in applications, we apply our algorithm to benchmark problems and compare it with 6 existing approaches to compute profile likelihood confidence intervals. Our algorithm consistently achieved higher success rates than any competitor while also being among the quickest methods. As our algorithm can be applied to compute both confidence intervals of parameters and model predictions, it is useful in a wide range of scenarios.},
  archive      = {J_SAC},
  author       = {Fischer, Samuel M. and Lewis, Mark A.},
  doi          = {10.1007/s11222-021-10012-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {A robust and efficient algorithm to find profile likelihood confidence intervals},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control variate selection for monte carlo integration.
<em>SAC</em>, <em>31</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s11222-021-10011-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo integration with variance reduction by means of control variates can be implemented by the ordinary least squares estimator for the intercept in a multiple linear regression model with the integrand as response and the control variates as covariates. Even without special knowledge on the integrand, significant efficiency gains can be obtained if the control variate space is sufficiently large. Incorporating a large number of control variates in the ordinary least squares procedure may however result in (i) a certain instability of the ordinary least squares estimator and (ii) a possibly prohibitive computation time. Regularizing the ordinary least squares estimator by preselecting appropriate control variates via the Lasso turns out to increase the accuracy without additional computational cost. The findings in the numerical experiment are confirmed by concentration inequalities for the integration error.},
  archive      = {J_SAC},
  author       = {Leluc, Rémi and Portier, François and Segers, Johan},
  doi          = {10.1007/s11222-021-10011-z},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Control variate selection for monte carlo integration},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast and calibrated computer model emulator: An empirical
bayes approach. <em>SAC</em>, <em>31</em>(4), 1–26. (<a
href="https://doi.org/10.1007/s11222-021-10024-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical models implemented on a computer have become the driving force behind the acceleration of the cycle of scientific processes. This is because computer models are typically much faster and economical to run than physical experiments. In this work, we develop an empirical Bayes approach to predictions of physical quantities using a computer model, where we assume that the computer model under consideration needs to be calibrated and is computationally expensive. We propose a Gaussian process emulator and a Gaussian process model for the systematic discrepancy between the computer model and the underlying physical process. This allows for closed-form and easy-to-compute predictions given by a conditional distribution induced by the Gaussian processes. We provide a rigorous theoretical justification of the proposed approach by establishing posterior consistency of the estimated physical process. The computational efficiency of the methods is demonstrated in an extensive simulation study and a real data example. The newly established approach makes enhanced use of computer models both from practical and theoretical standpoints.},
  archive      = {J_SAC},
  author       = {Kejzlar, Vojtech and Son, Mookyong and Bhattacharya, Shrijita and Maiti, Tapabrata},
  doi          = {10.1007/s11222-021-10024-8},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {A fast and calibrated computer model emulator: An empirical bayes approach},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast incremental expectation maximization for finite-sum
optimization: Nonasymptotic convergence. <em>SAC</em>, <em>31</em>(4),
1–24. (<a href="https://doi.org/10.1007/s11222-021-10023-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast incremental expectation maximization (FIEM) is a version of the EM framework for large datasets. In this paper, we first recast FIEM and other incremental EM type algorithms in the Stochastic Approximation within EM framework. Then, we provide nonasymptotic bounds for the convergence in expectation as a function of the number of examples n and of the maximal number of iterations $$K_\mathrm {max}$$ . We propose two strategies for achieving an $$\epsilon $$ -approximate stationary point, respectively with $$K_\mathrm {max}= O(n^{2/3}/\epsilon )$$ and $$K_\mathrm {max}= O(\sqrt{n}/\epsilon ^{3/2})$$ , both strategies relying on a random termination rule before $$K_\mathrm {max}$$ and on a constant step size in the Stochastic Approximation step. Our bounds provide some improvements on the literature. First, they allow $$K_\mathrm {max}$$ to scale as $$\sqrt{n}$$ which is better than $$n^{2/3}$$ which was the best rate obtained so far; it is at the cost of a larger dependence upon the tolerance $$\epsilon $$ , thus making this control relevant for small to medium accuracy with respect to the number of examples n. Second, for the $$n^{2/3}$$ -rate, the numerical illustrations show that thanks to an optimized choice of the step size and of the bounds in terms of quantities characterizing the optimization problem at hand, our results design a less conservative choice of the step size and provide a better control of the convergence in expectation.},
  archive      = {J_SAC},
  author       = {Fort, G. and Gach, P. and Moulines, E.},
  doi          = {10.1007/s11222-021-10023-9},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Fast incremental expectation maximization for finite-sum optimization: Nonasymptotic convergence},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A closed-form filter for binary time series. <em>SAC</em>,
<em>31</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s11222-021-10022-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Gaussian state-space models arise in several applications, and within this framework the binary time series setting provides a relevant example. However, unlike for Gaussian state-space models — where filtering, predictive and smoothing distributions are available in closed form — binary state-space models require approximations or sequential Monte Carlo strategies for inference and prediction. This is due to the apparent absence of conjugacy between the Gaussian states and the likelihood induced by the observation equation for the binary data. In this article we prove that the filtering, predictive and smoothing distributions in dynamic probit models with Gaussian state variables are, in fact, available and belong to a class of unified skew-normals (sun) whose parameters can be updated recursively in time via analytical expressions. Also the key functionals of these distributions are, in principle, available, but their calculation requires the evaluation of multivariate Gaussian cumulative distribution functions. Leveraging sun properties, we address this issue via novel Monte Carlo methods based on independent samples from the smoothing distribution, that can easily be adapted to the filtering and predictive case, thus improving state-of-the-art approximate and sequential Monte Carlo inference in small-to-moderate dimensional studies. Novel sequential Monte Carlo procedures that exploit the sun properties are also developed to deal with online inference in high dimensions. Performance gains over competitors are outlined in a financial application.},
  archive      = {J_SAC},
  author       = {Fasano, Augusto and Rebaudo, Giovanni and Durante, Daniele and Petrone, Sonia},
  doi          = {10.1007/s11222-021-10022-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {A closed-form filter for binary time series},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal experimental design for linear time invariant
state–space models. <em>SAC</em>, <em>31</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s11222-021-10020-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear time invariant state–space model representation is common to systems from several areas ranging from engineering to biochemistry. We address the problem of systematic optimal experimental design for this class of model. We consider two distinct scenarios: (i) steady-state model representations and (ii) dynamic models described by discrete-time representations. We use our approach to construct locally D-optimal designs by incorporating the calculation of the determinant of the Fisher Information Matrix and the parametric sensitivity computation in a Nonlinear Programming formulation. A global optimization solver handles the resulting numerical problem. The Fisher Information Matrix at convergence is used to determine model identifiability. We apply the methodology proposed to find approximate and exact optimal experimental designs for static and dynamic experiments for models representing a biochemical reaction network where the experimental purpose is to estimate kinetic constants.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Atkinson, Anthony C. and Oliveira, Nuno M. C.},
  doi          = {10.1007/s11222-021-10020-y},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Optimal experimental design for linear time invariant state–space models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian fisher-EM algorithm for discriminative gaussian
subspace clustering. <em>SAC</em>, <em>31</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s11222-021-10018-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data clustering has become and remains a challenging task for modern statistics and machine learning, with a wide range of applications. We consider in this work the powerful discriminative latent mixture model, and we extend it to the Bayesian framework. Modeling data as a mixture of Gaussians in a low-dimensional discriminative subspace, a Gaussian prior distribution is introduced over the latent group means and a family of twelve submodels are derived considering different covariance structures. Model inference is done with a variational EM algorithm, while the discriminative subspace is estimated via a Fisher-step maximizing an unsupervised Fisher criterion. An empirical Bayes procedure is proposed for the estimation of the prior hyper-parameters, and an integrated classification likelihood criterion is derived for selecting both the number of clusters and the submodel. The performances of the resulting Bayesian Fisher-EM algorithm are investigated in two thorough simulated scenarios, regarding both dimensionality as well as noise and assessing its superiority with respect to state-of-the-art Gaussian subspace clustering models. In addition to standard real data benchmarks, an application to single image denoising is proposed, displaying relevant results. This work comes with a reference implementation for the software in the package accompanying the paper and available on CRAN.},
  archive      = {J_SAC},
  author       = {Jouvin, Nicolas and Bouveyron, Charles and Latouche, Pierre},
  doi          = {10.1007/s11222-021-10018-6},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {A bayesian fisher-EM algorithm for discriminative gaussian subspace clustering},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient EM-variational inference for nonparametric hawkes
process. <em>SAC</em>, <em>31</em>(4), 1–11. (<a
href="https://doi.org/10.1007/s11222-021-10021-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic Hawkes process assumes the baseline intensity to be constant and the triggering kernel to be a parametric function. Differently, we present a generalization of the parametric Hawkes process by using a Bayesian nonparametric model called quadratic Gaussian Hawkes process. We model the baseline intensity and trigger kernel as the quadratic transformation of random trajectories drawn from a Gaussian process (GP) prior. We derive an analytical expression for the EM-variational inference algorithm by augmenting the latent branching structure of the Hawkes process to embed the variational Gaussian approximation into the EM framework naturally. We also use a series of schemes based on the sparse GP approximation to accelerate the inference algorithm. The results of synthetic and real data experiments show that the underlying baseline intensity and triggering kernel can be recovered efficiently and our model achieved superior performance in fitting capability and prediction accuracy compared to the state-of-the-art approaches.},
  archive      = {J_SAC},
  author       = {Zhou, Feng and Luo, Simon and Li, Zhidong and Fan, Xuhui and Wang, Yang and Sowmya, Arcot and Chen, Fang},
  doi          = {10.1007/s11222-021-10021-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Efficient EM-variational inference for nonparametric hawkes process},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating gaussian process metamodels and sequential
designs for noisy level set estimation. <em>SAC</em>, <em>31</em>(4),
1–21. (<a href="https://doi.org/10.1007/s11222-021-10014-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning the level set for which a noisy black-box function exceeds a given threshold. To efficiently reconstruct the level set, we investigate Gaussian process (GP) metamodels. Our focus is on strongly stochastic simulators, in particular with heavy-tailed simulation noise and low signal-to-noise ratio. To guard against noise misspecification, we assess the performance of three variants: (i) GPs with Student-t observations; (ii) Student-t processes (TPs); and (iii) classification GPs modeling the sign of the response. In conjunction with these metamodels, we analyze several acquisition functions for guiding the sequential experimental designs, extending existing stepwise uncertainty reduction criteria to the stochastic contour-finding context. This also motivates our development of (approximate) updating formulas to efficiently compute such acquisition functions. Our schemes are benchmarked by using a variety of synthetic experiments in 1–6 dimensions. We also consider an application of level set estimation for determining the optimal exercise policy of Bermudan options in finance.},
  archive      = {J_SAC},
  author       = {Lyu, Xiong and Binois, Mickaël and Ludkovski, Michael},
  doi          = {10.1007/s11222-021-10014-w},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Evaluating gaussian process metamodels and sequential designs for noisy level set estimation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A two-stage bayesian semiparametric model for novelty
detection with robust prior information. <em>SAC</em>, <em>31</em>(4),
1–19. (<a href="https://doi.org/10.1007/s11222-021-10017-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novelty detection methods aim at partitioning the test units into already observed and previously unseen patterns. However, two significant issues arise: there may be considerable interest in identifying specific structures within the novelty, and contamination in the known classes could completely blur the actual separation between manifest and new groups. Motivated by these problems, we propose a two-stage Bayesian semiparametric novelty detector, building upon prior information robustly extracted from a set of complete learning units. We devise a general-purpose multivariate methodology that we also extend to handle functional data objects. We provide insights on the model behavior by investigating the theoretical properties of the associated semiparametric prior. From the computational point of view, we propose a suitable $$\varvec{\xi }$$ -sequence to construct an independent slice-efficient sampler that takes into account the difference between manifest and novelty components. We showcase our model performance through an extensive simulation study and applications on both multivariate and functional datasets, in which diverse and distinctive unknown patterns are discovered.},
  archive      = {J_SAC},
  author       = {Denti, Francesco and Cappozzo, Andrea and Greselin, Francesca},
  doi          = {10.1007/s11222-021-10017-7},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {A two-stage bayesian semiparametric model for novelty detection with robust prior information},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parallel algorithm for ridge-penalized estimation of the
multivariate exponential family from data of mixed types. <em>SAC</em>,
<em>31</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s11222-021-10013-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computationally efficient evaluation of penalized estimators of multivariate exponential family distributions is sought. These distributions encompass among others Markov random fields with variates of mixed type (e.g., binary and continuous) as special case of interest. The model parameter is estimated by maximization of the pseudo-likelihood augmented with a convex penalty. The estimator is shown to be consistent. With a world of multi-core computers in mind, a computationally efficient parallel Newton–Raphson algorithm is presented for numerical evaluation of the estimator alongside conditions for its convergence. Parallelization comprises the division of the parameter vector into subvectors that are estimated simultaneously and subsequently aggregated to form an estimate of the original parameter. This approach may also enable efficient numerical evaluation of other high-dimensional estimators. The performance of the proposed estimator and algorithm are evaluated and compared in a simulation study. Finally, the presented methodology is applied to data of an integrative omics study.},
  archive      = {J_SAC},
  author       = {Laman Trip, Diederik S. and Wieringen, Wessel N. van},
  doi          = {10.1007/s11222-021-10013-x},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {A parallel algorithm for ridge-penalized estimation of the multivariate exponential family from data of mixed types},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximum likelihood estimation of the fisher–bingham
distribution via efficient calculation of its normalizing constant.
<em>SAC</em>, <em>31</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s11222-021-10015-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an efficient numerical integration formula to compute the normalizing constant of Fisher–Bingham distributions. This formula uses a numerical integration formula with the continuous Euler transform to a Fourier-type integral representation of the normalizing constant. As this method is fast and accurate, it can be applied to the calculation of the normalizing constant of high-dimensional Fisher–Bingham distributions. More precisely, the error decays exponentially with an increase in the integration points, and the computation cost increases linearly with the dimensions. In addition, this formula is useful for calculating the gradient and Hessian matrix of the normalizing constant. Therefore, we apply this formula to efficiently calculate the maximum likelihood estimation (MLE) of high-dimensional data. Finally, we apply the MLE to the hyperspherical variational auto-encoder (S-VAE), a deep-learning-based generative model that restricts the latent space to a unit hypersphere. We use the S-VAE trained with images of handwritten numbers to estimate the distributions of each label. This application is useful for adding new labels to the models.},
  archive      = {J_SAC},
  author       = {Chen, Yici and Tanaka, Ken’ichiro},
  doi          = {10.1007/s11222-021-10015-9},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Maximum likelihood estimation of the Fisher–Bingham distribution via efficient calculation of its normalizing constant},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of stochastic gradient descent in continuous time.
<em>SAC</em>, <em>31</em>(4), 1–25. (<a
href="https://doi.org/10.1007/s11222-021-10016-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent is an optimisation method that combines classical gradient descent with random subsampling within the target functional. In this work, we introduce the stochastic gradient process as a continuous-time representation of stochastic gradient descent. The stochastic gradient process is a dynamical system that is coupled with a continuous-time Markov process living on a finite state space. The dynamical system—a gradient flow—represents the gradient descent part, the process on the finite state space represents the random subsampling. Processes of this type are, for instance, used to model clonal populations in fluctuating environments. After introducing it, we study theoretical properties of the stochastic gradient process: We show that it converges weakly to the gradient flow with respect to the full target function, as the learning rate approaches zero. We give conditions under which the stochastic gradient process with constant learning rate is exponentially ergodic in the Wasserstein sense. Then we study the case, where the learning rate goes to zero sufficiently slowly and the single target functions are strongly convex. In this case, the process converges weakly to the point mass concentrated in the global minimum of the full target function; indicating consistency of the method. We conclude after a discussion of discretisation strategies for the stochastic gradient process and numerical experiments.},
  archive      = {J_SAC},
  author       = {Latz, Jonas},
  doi          = {10.1007/s11222-021-10016-8},
  journal      = {Statistics and Computing},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Analysis of stochastic gradient descent in continuous time},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A piecewise deterministic monte carlo method for diffusion
bridges. <em>SAC</em>, <em>31</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s11222-021-10008-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the use of the Zig-Zag sampler to the problem of sampling conditional diffusion processes (diffusion bridges). The Zig-Zag sampler is a rejection-free sampling scheme based on a non-reversible continuous piecewise deterministic Markov process. Similar to the Lévy–Ciesielski construction of a Brownian motion, we expand the diffusion path in a truncated Faber–Schauder basis. The coefficients within the basis are sampled using a Zig-Zag sampler. A key innovation is the use of the fully local algorithm for the Zig-Zag sampler that allows to exploit the sparsity structure implied by the dependency graph of the coefficients and by the subsampling technique to reduce the complexity of the algorithm. We illustrate the performance of the proposed methods in a number of examples.},
  archive      = {J_SAC},
  author       = {Bierkens, Joris and Grazzi, Sebastiano and van der Meulen, Frank and Schauer, Moritz},
  doi          = {10.1007/s11222-021-10008-8},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {A piecewise deterministic monte carlo method for diffusion bridges},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locally induced gaussian processes for large-scale
simulation experiments. <em>SAC</em>, <em>31</em>(3), 1–21. (<a
href="https://doi.org/10.1007/s11222-021-10007-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes (GPs) serve as flexible surrogates for complex surfaces, but buckle under the cubic cost of matrix decompositions with big training data sizes. Geospatial and machine learning communities suggest pseudo-inputs, or inducing points, as one strategy to obtain an approximation easing that computational burden. However, we show how placement of inducing points and their multitude can be thwarted by pathologies, especially in large-scale dynamic response surface modeling tasks. As remedy, we suggest porting the inducing point idea, which is usually applied globally, over to a more local context where selection is both easier and faster. In this way, our proposed methodology hybridizes global inducing point and data subset-based local GP approximation. A cascade of strategies for planning the selection of local inducing points is provided, and comparisons are drawn to related methodology with emphasis on computer surrogate modeling applications. We show that local inducing points extend their global and data subset component parts on the accuracy–computational efficiency frontier. Illustrative examples are provided on benchmark data and a large-scale real-simulation satellite drag interpolation problem.},
  archive      = {J_SAC},
  author       = {Cole, D. Austin and Christianson, Ryan B. and Gramacy, Robert B.},
  doi          = {10.1007/s11222-021-10007-9},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Locally induced gaussian processes for large-scale simulation experiments},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of time-varying autoregressive stochastic
volatility models with stable innovations. <em>SAC</em>, <em>31</em>(3),
1–19. (<a href="https://doi.org/10.1007/s11222-021-09995-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new time-varying autoregressive stochastic volatility model with $$\alpha $$ -stable innovations (TVAR $$\alpha $$ SV) is proposed. This new model for time series data combines a time-varying autoregressive component and a stochastic scaling as known from stochastic volatility models with $$\alpha $$ -stable distributed noise. Hence, the model can cover extreme events better than classical stochastic volatility models. Furthermore, we develop a Gibbs sampling procedure for the estimation of the model parameters. The procedure is based on the estimation strategy by Kim et al. (Rev Econ Stud 65(3): 361–393, 1998) for classical stochastic volatility models, however, the estimation procedure requires a deliberate approximation of $$\alpha $$ -stable distributions by finite mixtures of normal distributions and the application of a simulation smoother for linear Gaussian state space models. A simulation study for the new estimation procedure illustrates the appealing accuracy. Finally, we apply the model to electricity spot price data.},
  archive      = {J_SAC},
  author       = {Müller, Gernot and Uhl, Sebastian},
  doi          = {10.1007/s11222-021-09995-5},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of time-varying autoregressive stochastic volatility models with stable innovations},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast and efficient smoothing approach to lasso regression
and an application in statistical genetics: Polygenic risk scores for
chronic obstructive pulmonary disease (COPD). <em>SAC</em>,
<em>31</em>(3), 1–11. (<a
href="https://doi.org/10.1007/s11222-021-10010-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional linear regression problems are often fitted using Lasso approaches. Although the Lasso objective function is convex, it is not differentiable everywhere, making the use of gradient descent methods for minimization not straightforward. To avoid this technical issue, we apply Nesterov smoothing to the original (unsmoothed) Lasso objective function. We introduce a closed-form smoothed Lasso which preserves the convexity of the Lasso function, is uniformly close to the unsmoothed Lasso, and allows us to obtain closed-form derivatives everywhere for efficient and fast minimization via gradient descent. Our simulation studies are focused on polygenic risk scores using genetic data from a genome-wide association study (GWAS) for chronic obstructive pulmonary disease (COPD). We compare accuracy and runtime of our approach to the current gold standard in the literature, the FISTA algorithm. Our results suggest that the proposed methodology provides estimates with equal or higher accuracy than the FISTA algorithm while having the same asymptotic runtime scaling. The proposed methodology is implemented in the R-package smoothedLasso, available on the Comprehensive R Archive Network (CRAN).},
  archive      = {J_SAC},
  author       = {Hahn, Georg and Lutz, Sharon M. and Laha, Nilanjana and Cho, Michael H. and Silverman, Edwin K. and Lange, Christoph},
  doi          = {10.1007/s11222-021-10010-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {A fast and efficient smoothing approach to lasso regression and an application in statistical genetics: Polygenic risk scores for chronic obstructive pulmonary disease (COPD)},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple method for rejection sampling efficiency
improvement on SIMT architectures. <em>SAC</em>, <em>31</em>(3), 1–11.
(<a href="https://doi.org/10.1007/s11222-021-10003-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a probability distribution for the possible number of iterations required for a SIMT (single instruction multiple thread) program using rejection sampling to finish creating a sample across all threads. This distribution is found to match a recently proposed distribution from Chakraborty and Gupta (in: Communications in statistics: theory and methods, 2015) that was shown as a good approximation of certain datasets. This work demonstrates an exact application of this distribution. The distribution can be used to evaluate the relative merit of some sampling methods on the GPU without resort to numerical tests. The distribution reduces to the expected geometric distribution in the single thread per warp limit. A simplified formula to approximate the expected number of iterations required to obtain rejection iteration samples is provided. With this new result, a simple, efficient layout for assigning sampling tasks to threads on a GPU is found as a function of the rejection probability without recourse to more complicated rejection sampling methods.},
  archive      = {J_SAC},
  author       = {Ridley, Gavin and Forget, Benoit},
  doi          = {10.1007/s11222-021-10003-z},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {A simple method for rejection sampling efficiency improvement on SIMT architectures},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle-based energetic variational inference.
<em>SAC</em>, <em>31</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10009-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new variational inference (VI) framework, called energetic variational inference (EVI). It minimizes the VI objective function based on a prescribed energy-dissipation law. Using the EVI framework, we can derive many existing particle-based variational inference (ParVI) methods, including the popular Stein variational gradient descent (SVGD). More importantly, many new ParVI schemes can be created under this framework. For illustration, we propose a new particle-based EVI scheme, which performs the particle-based approximation of the density first and then uses the approximated density in the variational procedure, or “Approximation-then-Variation” for short. Thanks to this order of approximation and variation, the new scheme can maintain the variational structure at the particle level, and can significantly decrease the KL-divergence in each iteration. Numerical experiments show the proposed method outperforms some existing ParVI methods in terms of fidelity to the target distribution.},
  archive      = {J_SAC},
  author       = {Wang, Yiwei and Chen, Jiuhai and Liu, Chun and Kang, Lulu},
  doi          = {10.1007/s11222-021-10009-7},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Particle-based energetic variational inference},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularized bi-directional co-clustering. <em>SAC</em>,
<em>31</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10006-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous clustering of documents and words, known as co-clustering, has proved to be more effective than one-sided clustering in dealing with sparse high-dimensional datasets. By their nature, text data are also generally unbalanced and directional. Recently, the von Mises–Fisher (vMF) mixture model was proposed to handle unbalanced data while harnessing the directional nature of text. In this paper, we propose a general co-clustering framework based on a matrix formulation of vMF model-based co-clustering. This formulation leads to a flexible framework for text co-clustering that can easily incorporate both word–word semantic relationships and document–document similarities. By contrast with existing methods, which generally use an additive incorporation of similarities, we propose a bi-directional multiplicative regularization that better encapsulates the underlying text data structure. Extensive evaluations on various real-world text datasets demonstrate the superior performance of our proposed approach over baseline and competitive methods, both in terms of clustering results and co-cluster topic coherence.},
  archive      = {J_SAC},
  author       = {Affeldt, Séverine and Labiod, Lazhar and Nadif, Mohamed},
  doi          = {10.1007/s11222-021-10006-w},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Regularized bi-directional co-clustering},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating the estimation of renewal hawkes self-exciting
point processes. <em>SAC</em>, <em>31</em>(3), 1–17. (<a
href="https://doi.org/10.1007/s11222-021-10002-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The renewal Hawkes process is a nascent point process model that generalizes the Hawkes process. Although it has shown strong application potential, fitting the renewal Hawkes process to data remains a challenging task, especially on larger datasets. This article tackles this challenge by providing two approaches that significantly reduce the time required to fit renewal Hawkes processes. Since derivative-based methods for optimization, in general, converge faster than derivative-free methods, our first approach is to derive algorithms for evaluating the gradient and Hessian of the log-likelihood function and then use a derivative-based method, such as the Newton–Raphson method, in maximizing the likelihood, instead of the derivative-free method currently being used. Our second approach is to seek linear time algorithms that produce accurate approximations to the likelihood function, and then directly optimize the approximation to the log-likelihood function. Our simulation experiments show that the Newton–Raphson method reduces the computational time by about 30\%. Furthermore, the approximate likelihood methods produce equally accurate estimates compared to the methods based on the exact likelihood and are about 20–40 times faster on datasets with about 10,000 events. We conclude with an analysis of price changes of several currencies relative to the US Dollar.},
  archive      = {J_SAC},
  author       = {Stindl, Tom and Chen, Feng},
  doi          = {10.1007/s11222-021-10002-0},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Accelerating the estimation of renewal hawkes self-exciting point processes},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mixed model approach to measurement error in
semiparametric regression. <em>SAC</em>, <em>31</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11222-021-10005-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential assumption in traditional regression techniques is that predictors are measured without errors. Failing to take into account measurement error in predictors may result in severely biased inferences. Correcting measurement-error bias is an extremely difficult problem when estimating a regression function nonparametrically. We propose an approach to deal with measurement errors in predictors when modelling flexible regression functions. This approach depends on directly modelling the mean and the variance of the response variable after integrating out the true unobserved predictors in a penalized splines model. We demonstrate through simulation studies that our approach provides satisfactory prediction accuracy largely outperforming previously suggested local polynomial estimators even when the model is incorrectly specified and is competitive with the Bayesian estimator.},
  archive      = {J_SAC},
  author       = {Hattab, Mohammad W. and Ruppert, David},
  doi          = {10.1007/s11222-021-10005-x},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A mixed model approach to measurement error in semiparametric regression},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional particle filters with diffuse initial
distributions. <em>SAC</em>, <em>31</em>(3), 1–14. (<a
href="https://doi.org/10.1007/s11222-020-09975-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional particle filters (CPFs) are powerful smoothing algorithms for general nonlinear/non-Gaussian hidden Markov models. However, CPFs can be inefficient or difficult to apply with diffuse initial distributions, which are common in statistical applications. We propose a simple but generally applicable auxiliary variable method, which can be used together with the CPF in order to perform efficient inference with diffuse initial distributions. The method only requires simulatable Markov transitions that are reversible with respect to the initial distribution, which can be improper. We focus in particular on random walk type transitions which are reversible with respect to a uniform initial distribution (on some domain), and autoregressive kernels for Gaussian initial distributions. We propose to use online adaptations within the methods. In the case of random walk transition, our adaptations use the estimated covariance and acceptance rate adaptation, and we detail their theoretical validity. We tested our methods with a linear Gaussian random walk model, a stochastic volatility model, and a stochastic epidemic compartment model with time-varying transmission rate. The experimental findings demonstrate that our method works reliably with little user specification and can be substantially better mixing than a direct particle Gibbs algorithm that treats initial states as parameters.},
  archive      = {J_SAC},
  author       = {Karppinen, Santeri and Vihola, Matti},
  doi          = {10.1007/s11222-020-09975-1},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Conditional particle filters with diffuse initial distributions},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient stochastic optimisation by unadjusted langevin
monte carlo. <em>SAC</em>, <em>31</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-020-09986-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic approximation methods play a central role in maximum likelihood estimation problems involving intractable likelihood functions, such as marginal likelihoods arising in problems with missing or incomplete data, and in parametric empirical Bayesian estimation. Combined with Markov chain Monte Carlo algorithms, these stochastic optimisation methods have been successfully applied to a wide range of problems in science and industry. However, this strategy scales poorly to large problems because of methodological and theoretical difficulties related to using high-dimensional Markov chain Monte Carlo algorithms within a stochastic approximation scheme. This paper proposes to address these difficulties by using unadjusted Langevin algorithms to construct the stochastic approximation. This leads to a highly efficient stochastic optimisation methodology with favourable convergence properties that can be quantified explicitly and easily checked. The proposed methodology is demonstrated with three experiments, including a challenging application to statistical audio analysis and a sparse Bayesian logistic regression with random effects problem.},
  archive      = {J_SAC},
  author       = {De Bortoli, Valentin and Durmus, Alain and Pereyra, Marcelo and Vidal, Ana F.},
  doi          = {10.1007/s11222-020-09986-y},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Efficient stochastic optimisation by unadjusted langevin monte carlo},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian ODE solvers: The maximum a posteriori estimate.
<em>SAC</em>, <em>31</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-09993-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in probabilistic numerical solutions to ordinary differential equations. In this paper, the maximum a posteriori estimate is studied under the class of $$\nu $$ times differentiable linear time-invariant Gauss–Markov priors, which can be computed with an iterated extended Kalman smoother. The maximum a posteriori estimate corresponds to an optimal interpolant in the reproducing kernel Hilbert space associated with the prior, which in the present case is equivalent to a Sobolev space of smoothness $$\nu +1$$ . Subject to mild conditions on the vector field, convergence rates of the maximum a posteriori estimate are then obtained via methods from nonlinear analysis and scattered data approximation. These results closely resemble classical convergence results in the sense that a $$\nu $$ times differentiable prior process obtains a global order of $$\nu $$ , which is demonstrated in numerical examples.},
  archive      = {J_SAC},
  author       = {Tronarp, Filip and Särkkä, Simo and Hennig, Philipp},
  doi          = {10.1007/s11222-021-09993-7},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian ODE solvers: The maximum a posteriori estimate},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unbiased estimation of the gradient of the log-likelihood in
inverse problems. <em>SAC</em>, <em>31</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-09994-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating a parameter $$\theta \in \Theta \subseteq {\mathbb {R}}^{d_{\theta }}$$ associated with a Bayesian inverse problem. Typically one must resort to a numerical approximation of gradient of the log-likelihood and also adopt a discretization of the problem in space and/or time. We develop a new methodology to unbiasedly estimate the gradient of the log-likelihood with respect to the unknown parameter, i.e. the expectation of the estimate has no discretization bias. Such a property is not only useful for estimation in terms of the original stochastic model of interest, but can be used in stochastic gradient algorithms which benefit from unbiased estimates. Under appropriate assumptions, we prove that our estimator is not only unbiased but of finite variance. In addition, when implemented on a single processor, we show that the cost to achieve a given level of error is comparable to multilevel Monte Carlo methods, both practically and theoretically. However, the new algorithm is highly amenable to parallel computation.},
  archive      = {J_SAC},
  author       = {Jasra, Ajay and Law, Kody J. H. and Lu, Deng},
  doi          = {10.1007/s11222-021-09994-6},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Unbiased estimation of the gradient of the log-likelihood in inverse problems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble sampler for infinite-dimensional inverse problems.
<em>SAC</em>, <em>31</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s11222-021-10004-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new Markov chain Monte Carlo (MCMC) sampler for infinite-dimensional inverse problems. Our new sampler is based on the affine invariant ensemble sampler, which uses interacting walkers to adapt to the covariance structure of the target distribution. We extend this ensemble sampler for the first time to infinite-dimensional function spaces, yielding a highly efficient gradient-free MCMC algorithm. Because our new ensemble sampler does not require gradients or posterior covariance estimates, it is simple to implement and broadly applicable.},
  archive      = {J_SAC},
  author       = {Coullon, Jeremie and Webber, Robert J.},
  doi          = {10.1007/s11222-021-10004-y},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Stat. Comput.},
  title        = {Ensemble sampler for infinite-dimensional inverse problems},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential bayesian optimal experimental design for
structural reliability analysis. <em>SAC</em>, <em>31</em>(3), 1–29. (<a
href="https://doi.org/10.1007/s11222-021-10000-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural reliability analysis is concerned with estimation of the probability of a critical event taking place, described by $$P(g(\mathbf{X} ) \le 0)$$ for some n-dimensional random variable $$\mathbf{X} $$ and some real-valued function g. In many applications the function g is practically unknown, as function evaluation involves time consuming numerical simulation or some other form of experiment that is expensive to perform. The problem we address in this paper is how to optimally design experiments, in a Bayesian decision theoretic fashion, when the goal is to estimate the probability $$P(g(\mathbf{X} ) \le 0)$$ using a minimal amount of resources. As opposed to existing methods that have been proposed for this purpose, we consider a general structural reliability model given in hierarchical form. We therefore introduce a general formulation of the experimental design problem, where we distinguish between the uncertainty related to the random variable $$\mathbf{X} $$ and any additional epistemic uncertainty that we want to reduce through experimentation. The effectiveness of a design strategy is evaluated through a measure of residual uncertainty, and efficient approximation of this quantity is crucial if we want to apply algorithms that search for an optimal strategy. The method we propose is based on importance sampling combined with the unscented transform for epistemic uncertainty propagation. We implement this for the myopic (one-step look ahead) alternative, and demonstrate the effectiveness through a series of numerical experiments.},
  archive      = {J_SAC},
  author       = {Agrell, Christian and Dahl, Kristina Rognlien},
  doi          = {10.1007/s11222-021-10000-2},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Sequential bayesian optimal experimental design for structural reliability analysis},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian process learning via fisher scoring of vecchia’s
approximation. <em>SAC</em>, <em>31</em>(3), 1–8. (<a
href="https://doi.org/10.1007/s11222-021-09999-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a single-pass algorithm for computing the gradient and Fisher information of Vecchia’s Gaussian process loglikelihood approximation, which provides a computationally efficient means for applying the Fisher scoring algorithm for maximizing the loglikelihood. The advantages of the optimization techniques are demonstrated in numerical examples and in an application to Argo ocean temperature data. The new methods find the maximum likelihood estimates much faster and more reliably than an optimization method that uses only function evaluations, especially when the covariance function has many parameters. This allows practitioners to fit nonstationary models to large spatial and spatial–temporal datasets.},
  archive      = {J_SAC},
  author       = {Guinness, Joseph},
  doi          = {10.1007/s11222-021-09999-1},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-8},
  shortjournal = {Stat. Comput.},
  title        = {Gaussian process learning via fisher scoring of vecchia’s approximation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep mixtures of unigrams for uncovering topics in textual
data. <em>SAC</em>, <em>31</em>(3), 1–10. (<a
href="https://doi.org/10.1007/s11222-020-09989-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of unigrams are one of the simplest and most efficient tools for clustering textual data, as they assume that documents related to the same topic have similar distributions of terms, naturally described by multinomials. When the classification task is particularly challenging, such as when the document-term matrix is high-dimensional and extremely sparse, a more composite representation can provide better insight into the grouping structure. In this work, we developed a deep version of mixtures of unigrams for the unsupervised classification of very short documents with a large number of terms, by allowing for models with further deeper latent layers; the proposal is derived in a Bayesian framework. The behavior of the deep mixtures of unigrams is empirically compared with that of other traditional and state-of-the-art methods, namely k-means with cosine distance, k-means with Euclidean distance on data transformed according to semantic analysis, partition around medoids, mixture of Gaussians on semantic-based transformed data, hierarchical clustering according to Ward’s method with cosine dissimilarity, latent Dirichlet allocation, mixtures of unigrams estimated via the EM algorithm, spectral clustering and affinity propagation clustering. The performance is evaluated in terms of both correct classification rate and Adjusted Rand Index. Simulation studies and real data analysis prove that going deep in clustering such data highly improves the classification accuracy.},
  archive      = {J_SAC},
  author       = {Viroli, Cinzia and Anderlucci, Laura},
  doi          = {10.1007/s11222-020-09989-9},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {Deep mixtures of unigrams for uncovering topics in textual data},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian additive regression trees with model trees.
<em>SAC</em>, <em>31</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s11222-021-09997-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian additive regression trees (BART) is a tree-based machine learning method that has been successfully applied to regression and classification problems. BART assumes regularisation priors on a set of trees that work as weak learners and is very flexible for predicting in the presence of nonlinearity and high-order interactions. In this paper, we introduce an extension of BART, called model trees BART (MOTR-BART), that considers piecewise linear functions at node levels instead of piecewise constants. In MOTR-BART, rather than having a unique value at node level for the prediction, a linear predictor is estimated considering the covariates that have been used as the split variables in the corresponding tree. In our approach, local linearities are captured more efficiently and fewer trees are required to achieve equal or better performance than BART. Via simulation studies and real data applications, we compare MOTR-BART to its main competitors. R code for MOTR-BART implementation is available at https://github.com/ebprado/MOTR-BART .},
  archive      = {J_SAC},
  author       = {Prado, Estevão B. and Moral, Rafael A. and Parnell, Andrew C.},
  doi          = {10.1007/s11222-021-09997-3},
  journal      = {Statistics and Computing},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive regression trees with model trees},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal representative sample weighting. <em>SAC</em>,
<em>31</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-021-10001-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of assigning weights to a set of samples or data records, with the goal of achieving a representative weighting, which happens when certain sample averages of the data are close to prescribed values. We frame the problem of finding representative sample weights as an optimization problem, which in many cases is convex and can be efficiently solved. Our formulation includes as a special case the selection of a fixed number of the samples, with equal weights, i.e., the problem of selecting a smaller representative subset of the samples. While this problem is combinatorial and not convex, heuristic methods based on convex optimization seem to perform very well. We describe our open-source implementation rsw and apply it to a skewed sample of the CDC BRFSS dataset.},
  archive      = {J_SAC},
  author       = {Barratt, Shane and Angeris, Guillermo and Boyd, Stephen},
  doi          = {10.1007/s11222-021-10001-1},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Optimal representative sample weighting},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A wavelet-based approach for imputation in nonstationary
multivariate time series. <em>SAC</em>, <em>31</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-021-09998-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multivariate time series observed in practice are second order nonstationary, i.e. their covariance properties vary over time. In addition, missing observations in such data are encountered in many applications of interest, due to recording failures or sensor dropout, hindering successful analysis. This article introduces a novel method for data imputation in multivariate nonstationary time series, based on the so-called locally stationary wavelet modelling paradigm. Our methodology is shown to perform well across a range of simulation scenarios, with a variety of missingness structures, as well as being competitive in the stationary time series setting. We also demonstrate our technique on data arising in a health monitoring application.},
  archive      = {J_SAC},
  author       = {Wilson, Rebecca E. and Eckley, Idris A. and Nunes, Matthew A. and Park, Timothy},
  doi          = {10.1007/s11222-021-09998-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A wavelet-based approach for imputation in nonstationary multivariate time series},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tree-structured scale effects in binary and ordinal
regression. <em>SAC</em>, <em>31</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11222-020-09992-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In binary and ordinal regression one can distinguish between a location component and a scaling component. While the former determines the location within the range of the response categories, the scaling indicates variance heterogeneity. In particular since it has been demonstrated that misleading effects can occur if one ignores the presence of a scaling component, it is important to account for potential scaling effects in the regression model, which is not possible in available recursive partitioning methods. The proposed recursive partitioning method yields two trees: one for the location and one for the scaling. They show in a simple interpretable way how variables interact to determine the binary or ordinal response. The developed algorithm controls for the global significance level and automatically selects the variables that have an impact on the response. The modeling approach is illustrated by several real-world applications.},
  archive      = {J_SAC},
  author       = {Tutz, Gerhard and Berger, Moritz},
  doi          = {10.1007/s11222-020-09992-0},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Tree-structured scale effects in binary and ordinal regression},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularizing axis-aligned ensembles via data rotations that
favor simpler learners. <em>SAC</em>, <em>31</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s11222-020-09973-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the inherent limitations of axis-aligned base learners in ensemble learning, several methods of rotating the feature space have been discussed in the literature. In particular, smoother decision boundaries can often be obtained from axis-aligned ensembles by rotating the feature space. In the present paper, we introduce a low-cost regularization technique that favors rotations which produce compact base learners. The restated problem adds a shrinkage term to the loss function that explicitly accounts for the complexity of the base learners. For example, for tree-based ensembles, we apply a penalty based on the median number of nodes and the median depth of the trees in the forest. Rather than jointly minimizing prediction error and model complexity, which is computationally infeasible, we first generate a prioritized weighting of the available feature rotations that promotes lower model complexity and subsequently minimize prediction errors on each of the selected rotations. We show that the resulting ensembles tend to be significantly more dense, faster to evaluate, and competitive at generalizing in out-of-sample predictions.},
  archive      = {J_SAC},
  author       = {Blaser, Rico and Fryzlewicz, Piotr},
  doi          = {10.1007/s11222-020-09973-3},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Regularizing axis-aligned ensembles via data rotations that favor simpler learners},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicitly adaptive importance sampling. <em>SAC</em>,
<em>31</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-020-09982-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive importance sampling is a class of techniques for finding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive.},
  archive      = {J_SAC},
  author       = {Paananen, Topi and Piironen, Juho and Bürkner, Paul-Christian and Vehtari, Aki},
  doi          = {10.1007/s11222-020-09982-2},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Implicitly adaptive importance sampling},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian wavelet-packet historical functional linear models.
<em>SAC</em>, <em>31</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11222-020-09981-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical functional linear models (HFLMs) quantify associations between a functional predictor and functional outcome where the predictor is an exposure variable that occurs before, or at least concurrently with, the outcome. Prior work on the HFLM has largely focused on estimation of a surface that represents a time-varying association between the functional outcome and the functional exposure. This existing work has employed frequentist and spline-based estimation methods, with little attention paid to formal inference or adjustment for multiple testing and no approaches that implement wavelet bases. In this work, we propose a new functional regression model that estimates the time-varying, lagged association between a functional outcome and a functional exposure. Building off of recently developed function-on-function regression methods, the model employs a novel use the wavelet-packet decomposition of the exposure and outcome functions that allows us to strictly enforce the temporal ordering of exposure and outcome, which is not possible with existing wavelet-based functional models. Using a fully Bayesian approach, we conduct formal inference on the time-varying lagged association, while adjusting for multiple testing. We investigate the operating characteristics of our wavelet-packet HFLM and compare them to those of two existing estimation procedures in simulation. We also assess several inference techniques and use the model to analyze data on the impact of lagged exposure to particulate matter finer than 2.5 $$\upmu $$ g, or PM $$_{2.5}$$ , on heart rate variability in a cohort of journeyman boilermakers during the morning of a typical day’s shift.},
  archive      = {J_SAC},
  author       = {Meyer, Mark J. and Malloy, Elizabeth J. and Coull, Brent A.},
  doi          = {10.1007/s11222-020-09981-3},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian wavelet-packet historical functional linear models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale stick-breaking mixture models. <em>SAC</em>,
<em>31</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s11222-020-09991-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian nonparametric density estimation is dominated by single-scale methods, typically exploiting mixture model specifications, exception made for Pólya trees prior and allied approaches. In this paper we focus on developing a novel family of multiscale stick-breaking mixture models that inherits some of the advantages of both single-scale nonparametric mixtures and Pólya trees. Our proposal is based on a mixture specification exploiting an infinitely deep binary tree of random weights that grows according to a multiscale generalization of a large class of stick-breaking processes; this multiscale stick-breaking is paired with specific stochastic processes generating sequences of parameters that induce stochastically ordered kernel functions. Properties of this family of multiscale stick-breaking mixtures are described. Focusing on a Gaussian specification, a Markov Chain Monte Carlo algorithm for posterior computation is introduced. The performance of the method is illustrated analyzing both synthetic and real datasets consistently showing competitive results both in scenarios favoring single-scale and multiscale methods. The results suggest that the method is well suited to estimate densities with varying degree of smoothness and local features.},
  archive      = {J_SAC},
  author       = {Stefanucci, Marco and Canale, Antonio},
  doi          = {10.1007/s11222-020-09991-1},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Multiscale stick-breaking mixture models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convergence rates for optimised adaptive importance
samplers. <em>SAC</em>, <em>31</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-020-09983-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate expectations with respect to some target distribution which adapt themselves to obtain better estimators over a sequence of iterations. Although it is straightforward to show that they have the same $$\mathcal {O}(1/\sqrt{N})$$ convergence rate as standard importance samplers, where N is the number of Monte Carlo samples, the behaviour of adaptive importance samplers over the number of iterations has been left relatively unexplored. In this work, we investigate an adaptation strategy based on convex optimisation which leads to a class of adaptive importance samplers termed optimised adaptive importance samplers (OAIS). These samplers rely on the iterative minimisation of the $$\chi ^2$$ -divergence between an exponential family proposal and the target. The analysed algorithms are closely related to the class of adaptive importance samplers which minimise the variance of the weight function. We first prove non-asymptotic error bounds for the mean squared errors (MSEs) of these algorithms, which explicitly depend on the number of iterations and the number of samples together. The non-asymptotic bounds derived in this paper imply that when the target belongs to the exponential family, the $$L_2$$ errors of the optimised samplers converge to the optimal rate of $$\mathcal {O}(1/\sqrt{N})$$ and the rate of convergence in the number of iterations are explicitly provided. When the target does not belong to the exponential family, the rate of convergence is the same but the asymptotic $$L_2$$ error increases by a factor $$\sqrt{\rho ^\star } &gt; 1$$ , where $$\rho ^\star - 1$$ is the minimum $$\chi ^2$$ -divergence between the target and an exponential family proposal.},
  archive      = {J_SAC},
  author       = {Akyildiz, Ömer Deniz and Míguez, Joaquín},
  doi          = {10.1007/s11222-020-09983-1},
  journal      = {Statistics and Computing},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Convergence rates for optimised adaptive importance samplers},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust fitting for generalized additive models for location,
scale and shape. <em>SAC</em>, <em>31</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-020-09979-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The validity of estimation and smoothing parameter selection for the wide class of generalized additive models for location, scale and shape (GAMLSS) relies on the correct specification of a likelihood function. Deviations from such assumption are known to mislead any likelihood-based inference and can hinder penalization schemes meant to ensure some degree of smoothness for nonlinear effects. We propose a general approach to achieve robustness in fitting GAMLSSs by limiting the contribution of observations with low log-likelihood values. Robust selection of the smoothing parameters can be carried out either by minimizing information criteria that naturally arise from the robustified likelihood or via an extended Fellner–Schall method. The latter allows for automatic smoothing parameter selection and is particularly advantageous in applications with multiple smoothing parameters. We also address the challenge of tuning robust estimators for models with nonlinear effects by proposing a novel median downweighting proportion criterion. This enables a fair comparison with existing robust estimators for the special case of generalized additive models, where our estimator competes favorably. The overall good performance of our proposal is illustrated by further simulations in the GAMLSS setting and by an application to functional magnetic resonance brain imaging using bivariate smoothing splines.},
  archive      = {J_SAC},
  author       = {Aeberhard, William H. and Cantoni, Eva and Marra, Giampiero and Radice, Rosalba},
  doi          = {10.1007/s11222-020-09979-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Robust fitting for generalized additive models for location, scale and shape},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting low-rank covariance structures for computing
high-dimensional normal and student-t probabilities. <em>SAC</em>,
<em>31</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11222-020-09978-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a preconditioned Monte Carlo method for computing high-dimensional multivariate normal and Student-t probabilities arising in spatial statistics. The approach combines a tile-low-rank representation of covariance matrices with a block-reordering scheme for efficient quasi-Monte Carlo simulation. The tile-low-rank representation decomposes the high-dimensional problem into many diagonal-block-size problems and low-rank connections. The block-reordering scheme reorders between and within the diagonal blocks to reduce the impact of integration variables from right to left, thus improving the Monte Carlo convergence rate. Simulations up to dimension 65,536 suggest that the new method can improve the run time by an order of magnitude compared with the hierarchical quasi-Monte Carlo method and two orders of magnitude compared with the dense quasi-Monte Carlo method. Our method also forms a strong substitute for the approximate conditioning methods as a more robust estimation with error guarantees. An application study to wind stochastic generators is provided to illustrate that the new computational method makes the maximum likelihood estimation feasible for high-dimensional skew-normal random fields.},
  archive      = {J_SAC},
  author       = {Cao, Jian and Genton, Marc G. and Keyes, David E. and Turkiyyah, George M.},
  doi          = {10.1007/s11222-020-09978-y},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Exploiting low-rank covariance structures for computing high-dimensional normal and student-t probabilities},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parallel evolutionary multiple-try metropolis markov chain
monte carlo algorithm for sampling spatial partitions. <em>SAC</em>,
<em>31</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11222-020-09977-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an Evolutionary Markov Chain Monte Carlo (EMCMC) algorithm for sampling spatial partitions that lie within a large, complex, and constrained spatial state space. Our algorithm combines the advantages of evolutionary algorithms (EAs) as optimization heuristics for state space traversal and the theoretical convergence properties of Markov Chain Monte Carlo algorithms for sampling from unknown distributions. Local optimality information that is identified via a directed search by our optimization heuristic is used to adaptively update a Markov chain in a promising direction within the framework of a Multiple-Try Metropolis Markov Chain model that incorporates a generalized Metropolis-Hastings ratio. We further expand the reach of our EMCMC algorithm by harnessing the computational power afforded by massively parallel computing architecture through the integration of a parallel EA framework that guides Markov chains running in parallel.},
  archive      = {J_SAC},
  author       = {Cho, Wendy K. Tam and Liu, Yan Y.},
  doi          = {10.1007/s11222-020-09977-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {A parallel evolutionary multiple-try metropolis markov chain monte carlo algorithm for sampling spatial partitions},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble kalman inversion: Mean-field limit and convergence
analysis. <em>SAC</em>, <em>31</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11222-020-09976-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble Kalman inversion (EKI) has been a very popular algorithm used in Bayesian inverse problems (Iglesias et al. in Inverse Probl 29: 045001, 2013). It samples particles from a prior distribution and introduces a motion to move the particles around in pseudo-time. As the pseudo-time goes to infinity, the method finds the minimizer of the objective function, and when the pseudo-time stops at 1, the ensemble distribution of the particles resembles, in some sense, the posterior distribution in the linear setting. The ideas trace back further to ensemble Kalman filter and the associated analysis  (Evensen in J Geophys Res: Oceans 99: 10143–10162, 1994; Reich in BIT Numer Math 51: 235–249, 2011), but to today, when viewed as a sampling method, why EKI works, and in what sense with what rate the method converges is still largely unknown. In this paper, we analyze the continuous version of EKI, a coupled SDE system, and prove the mean-field limit of this SDE system. In particular, we will show that 1. as the number of particles goes to infinity, the empirical measure of particles following SDE converges to the solution to a Fokker–Planck equation in Wasserstein 2-distance with an optimal rate, for both linear and weakly nonlinear case; 2. the solution to the Fokker–Planck equation reconstructs the target distribution in finite time in the linear case, as suggested in Iglesias et al. (Inverse Probl 29: 045001, 2013).},
  archive      = {J_SAC},
  author       = {Ding, Zhiyan and Li, Qin},
  doi          = {10.1007/s11222-020-09976-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Ensemble kalman inversion: Mean-field limit and convergence analysis},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank-one multi-reference factor analysis. <em>SAC</em>,
<em>31</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s11222-020-09990-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there is a growing need for processing methods aimed at extracting useful information from large datasets. In many cases, the challenge is to discover a low-dimensional structure in the data, often concealed by the existence of nuisance parameters and noise. Motivated by such challenges, we consider the problem of estimating a signal from its scaled, cyclically shifted and noisy observations. We focus on the particularly challenging regime of low signal-to-noise ratio (SNR), where different observations cannot be shift-aligned. We show that an accurate estimation of the signal from its noisy observations is possible, and derive a procedure which is proved to consistently estimate the signal. The asymptotic sample complexity (the number of observations required to recover the signal) of the procedure is $$1{/}{\text {SNR}}^4$$ . Additionally, we propose a procedure which is experimentally shown to improve the sample complexity by a factor equal to the signal’s length. Finally, we present numerical experiments which demonstrate the performance of our algorithms and corroborate our theoretical findings.},
  archive      = {J_SAC},
  author       = {Aizenbud, Yariv and Landa, Boris and Shkolnisky, Yoel},
  doi          = {10.1007/s11222-020-09990-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Rank-one multi-reference factor analysis},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast sampling from <span
class="math display"><em>β</em></span> -ensembles. <em>SAC</em>,
<em>31</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s11222-020-09984-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate sampling $$\beta $$ -ensembles with time complexity less than cubic in the cardinality of the ensemble. Following Dumitriu and Edelman (J Math Phys 43(11):5830–5847, 2002), we see the ensemble as the eigenvalues of a random tridiagonal matrix, namely a random Jacobi matrix. First, we provide a unifying and elementary treatment of the tridiagonal models associated with the three classical Hermite, Laguerre, and Jacobi ensembles. For this purpose, we use simple changes of variables between successive reparametrizations of the coefficients defining the tridiagonal matrix. Second, we derive an approximate sampler for the simulation of more general $$\beta $$ -ensembles and illustrate how fast it can be for polynomial potentials. This method combines a Gibbs sampler on Jacobi matrices and the diagonalization of these matrices. In practice, even for large ensembles, only a few Gibbs passes suffice for the marginal distribution of the eigenvalues to fit the expected theoretical distribution. When the conditionals in the Gibbs sampler can be simulated exactly, the same fast empirical convergence is observed for the fluctuations of the largest eigenvalue. Our experimental results support a conjecture by Krishnapur et al. (Commun Pure Appl Math 69(1): 145–199, 2016), that the Gibbs chain on Jacobi matrices of size N mixes in $$\mathcal {O}(\log N)$$ .},
  archive      = {J_SAC},
  author       = {Gautier, Guillaume and Bardenet, Rémi and Valko, Michal},
  doi          = {10.1007/s11222-020-09984-0},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Fast sampling from $$\beta $$ -ensembles},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive MCMC method for bayesian variable selection in
logistic and accelerated failure time regression models. <em>SAC</em>,
<em>31</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11222-020-09974-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian variable selection is an important method for discovering variables which are most useful for explaining the variation in a response. The widespread use of this method has been restricted by the challenging computational problem of sampling from the corresponding posterior distribution. Recently, the use of adaptive Monte Carlo methods has been shown to lead to performance improvement over traditionally used algorithms in linear regression models. This paper looks at applying one of these algorithms (the adaptively scaled independence sampler) to logistic regression and accelerated failure time models. We investigate the use of this algorithm with data augmentation, Laplace approximation and the correlated pseudo-marginal method. The performance of the algorithms is compared on several genomic data sets.},
  archive      = {J_SAC},
  author       = {Wan, Kitty Yuen Yi and Griffin, Jim E.},
  doi          = {10.1007/s11222-020-09974-2},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {An adaptive MCMC method for bayesian variable selection in logistic and accelerated failure time regression models},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modal clustering using semiparametric mixtures and mode
flattening. <em>SAC</em>, <em>31</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11222-020-09985-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modal clustering has a clear population goal, where density estimation plays a critical role. In this paper, we study how to provide better density estimation so as to serve the objective of modal clustering. In particular, we use semiparametric mixtures for density estimation, aided with a novel mode-flattening technique. The use of semiparametric mixtures helps to produce better density estimates, especially in the multivariate situation, and the mode-flattening technique is intended to identify and smooth out spurious and minor modes. With mode flattening, the number of clusters can be sequentially reduced until there is only one mode left. In addition, we adopt the likelihood function in a coherent manner to measure the relative importance of a mode and let the current least important mode disappear in each step. For both simulated and real-world data sets, the proposed method performs very well, as compared with some well-known clustering methods in the literature, and can successfully solve some fairly difficult clustering problems.},
  archive      = {J_SAC},
  author       = {Hu, Shengwei and Wang, Yong},
  doi          = {10.1007/s11222-020-09985-z},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Modal clustering using semiparametric mixtures and mode flattening},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable bayesian inference for self-excitatory stochastic
processes applied to big american gunfire data. <em>SAC</em>,
<em>31</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11222-020-09980-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hawkes process and its extensions effectively model self-excitatory phenomena including earthquakes, viral pandemics, financial transactions, neural spike trains and the spread of memes through social networks. The usefulness of these stochastic process models within a host of economic sectors and scientific disciplines is undercut by the processes’ computational burden: complexity of likelihood evaluations grows quadratically in the number of observations for both the temporal and spatiotemporal Hawkes processes. We show that, with care, one may parallelize these calculations using both central and graphics processing unit implementations to achieve over 100-fold speedups over single-core processing. Using a simple adaptive Metropolis–Hastings scheme, we apply our high-performance computing framework to a Bayesian analysis of big gunshot data generated in Washington D.C. between the years of 2006 and 2019, thereby extending a past analysis of the same data from under 10,000 to over 85,000 observations. To encourage widespread use, we provide hpHawkes, an open-source R package, and discuss high-level implementation and program design for leveraging aspects of computational hardware that become necessary in a big data setting.},
  archive      = {J_SAC},
  author       = {Holbrook, Andrew J. and Loeffler, Charles E. and Flaxman, Seth R. and Suchard, Marc A.},
  doi          = {10.1007/s11222-020-09980-4},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Scalable bayesian inference for self-excitatory stochastic processes applied to big american gunfire data},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toeplitz monte carlo. <em>SAC</em>, <em>31</em>(1), 1–15.
(<a href="https://doi.org/10.1007/s11222-020-09987-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated mainly by applications to partial differential equations with random coefficients, we introduce a new class of Monte Carlo estimators, called Toeplitz Monte Carlo (TMC) estimator, for approximating the integral of a multivariate function with respect to the direct product of an identical univariate probability measure. The TMC estimator generates a sequence $$x_1,x_2,\ldots $$ of i.i.d. samples for one random variable and then uses $$(x_{n+s-1},x_{n+s-2}\ldots ,x_n)$$ with $$n=1,2,\ldots $$ as quadrature points, where s denotes the dimension. Although consecutive points have some dependency, the concatenation of all quadrature nodes is represented by a Toeplitz matrix, which allows for a fast matrix–vector multiplication. In this paper, we study the variance of the TMC estimator and its dependence on the dimension s. Numerical experiments confirm the considerable efficiency improvement over the standard Monte Carlo estimator for applications to partial differential equations with random coefficients, particularly when the dimension s is large.},
  archive      = {J_SAC},
  author       = {Dick, Josef and Goda, Takashi and Murata, Hiroya},
  doi          = {10.1007/s11222-020-09987-x},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Toeplitz monte carlo},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-local spatially varying finite mixture models for image
segmentation. <em>SAC</em>, <em>31</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s11222-020-09988-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new Bayesian model for unsupervised image segmentation based on a combination of the spatially varying finite mixture models (SVFMMs) and the non-local means (NLM) framework. The probabilistic NLM weighting function is successfully integrated into a varying Gauss–Markov random field, yielding a prior density that adaptively imposes a local regularization to simultaneously preserve edges and enforce smooth constraints in homogeneous regions of the image. Two versions of our model are proposed: a pixel-based model and a patch-based model, depending on the design of the probabilistic NLM weighting function. Contrary to previous methods proposed in the literature, our approximation does not introduce new parameters to be estimated into the model, because the NLM weighting function is completely known once the neighborhood of a pixel is fixed. The proposed model can be estimated in closed-form solution via a maximum a posteriori (MAP) estimation in an expectation–maximization scheme. We have compared our model with previously proposed SVFMMs using two public datasets: the Berkeley Segmentation dataset and the BRATS 2013 dataset. The proposed model performs favorably to previous approaches in the literature, achieving better results in terms of Rand Index and Dice metrics in our experiments.},
  archive      = {J_SAC},
  author       = {Juan-Albarracín, Javier and Fuster-Garcia, Elies and Juan, Alfons and García-Gómez, Juan M.},
  doi          = {10.1007/s11222-020-09988-w},
  journal      = {Statistics and Computing},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {Non-local spatially varying finite mixture models for image segmentation},
  volume       = {31},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
