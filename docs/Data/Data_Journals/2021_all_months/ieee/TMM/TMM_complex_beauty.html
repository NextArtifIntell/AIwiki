<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm---353">TMM - 353</h2>
<ul>
<li><details>
<summary>
(2021). On reliable multi-view affinity learning for subspace
clustering. <em>TMM</em>, <em>23</em>, 4555–4566. (<a
href="https://doi.org/10.1109/TMM.2020.3045259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-view subspace clustering, the low-rankness of the stacked self-representation tensor is widely accepted to capture the high-order cross-view correlation. However, using the nuclear norm as a convex surrogate of the rank function, the self-representation tensor exhibits strong connectivity with dense coefficients. When noise exists in the data, the generated affinity matrix may be unreliable for subspace clustering as it retains the connections across inter-cluster samples due to the lack of sparsity. Since both the connectivity and sparsity of the self-representation coefficients are curial for subspace clustering, we propose a Reliable Multi-View Affinity Learning (RMVAL) method so as to optimize both properties in a single model. Specifically, RMVAL employs the low-rank tensor constraint to yield a well-connected yet dense solution, and purifies the densely connected self-representation tensor by preserving only the connections in local neighborhoods using the $l_1$ -norm regularization. This way, the strong connections on the self-representation tensor are retained and the trivial coefficients corresponding to the inter-cluster connections are suppressed, leading to a “clean” self-representation tensor and also a reliable affinity matrix. We propose an efficient algorithm to solve RMVAL using the alternating direction method of multipliers. Extensive experiments on benchmark databases have demonstrated the superiority of RMVAL.},
  archive      = {J_TMM},
  author       = {Xiaolin Xiao and Yue-Jiao Gong and Zhongyun Hua and Wei-Neng Chen},
  doi          = {10.1109/TMM.2020.3045259},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4555-4566},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {On reliable multi-view affinity learning for subspace clustering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic-driven interpretable deep multi-modal hashing for
large-scale multimedia retrieval. <em>TMM</em>, <em>23</em>, 4541–4554.
(<a href="https://doi.org/10.1109/TMM.2020.3044473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal hashing focuses on fusing different modalities and exploring the complementarity of heterogeneous multi-modal data for compact hash learning. However, existing multi-modal hashing methods still suffer from several problems, including: 1) Almost all existing methods generate unexplainable hash codes. They roughly assume that the contribution of each hash code bit to the retrieval results is the same, ignoring the discriminative information embedded in hash learning and semantic similarity in hash retrieval. Moreover, the length of hash code is empirically set, which will cause bit redundancy and affect retrieval accuracy. 2) Most existing methods exploit shallow models which fail to fully capture higher-level correlation of multi-modal data. 3) Most existing methods adopt online hashing strategy based on immutable direct projection, which generates query codes for new samples without considering the differences of semantic categories. In this paper, we propose a Semantic-driven Interpretable Deep Multi-modal Hashing (SIDMH) method to generate interpretable hash codes driven by semantic categories within a deep hashing architecture, which can solve all these three problems in an integrated model. The main contributions are: 1) A novel deep multi-modal hashing network is developed to progressively extract hidden representations of heterogeneous modality features and deeply exploit the complementarity of multi-modal data. 2) Learning interpretable hash codes, with discriminant information of different categories distinctively embedded into hash codes and their different impacts on hash retrieval intuitively explained. Besides, the code length depends on the number of categories in the dataset, which can reduce the bit redundancy and improve the retrieval accuracy. 3) The semantic-driven online hashing strategy encodes the significant branches and discards the negligible branches of each query sample according to the semantics contained in it, therefore it could capture different semantics in dynamic queries. Finally, we consider both the nearest neighbor similarity and semantic similarity of hash codes. Experiments on several public multimedia retrieval datasets validate the superiority of the proposed method.},
  archive      = {J_TMM},
  author       = {Xu Lu and Li Liu and Liqiang Nie and Xiaojun Chang and Huaxiang Zhang},
  doi          = {10.1109/TMM.2020.3044473},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4541-4554},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic-driven interpretable deep multi-modal hashing for large-scale multimedia retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting user quitting ratio in adaptive bitrate video
streaming. <em>TMM</em>, <em>23</em>, 4526–4540. (<a
href="https://doi.org/10.1109/TMM.2020.3044452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve user engagement such as viewing time, this paper addresses the understanding and prediction of the user quitting ratio for users watching videos using adaptive bit rate video streaming. The user quitting ratio is defined as the percentage of users still watching videos at a given time. To perform this study, five subjective experiments involving up to 264 participants were conducted in a laboratory setting. Results indicated the effects of coding quality, initial buffering, and midway stalling on user quitting ratio . Then, a framework was defined to predict the user quitting ratio as a function of time. This framework achieves good prediction accuracy and can be used in multiple scenarios including when quality adaptation and stalling occur. Finally, it is suitable for monitoring applications where bitstream are encrypted and low processing cost is required.},
  archive      = {J_TMM},
  author       = {Pierre Lebreton and Kazuhisa Yamagishi},
  doi          = {10.1109/TMM.2020.3044452},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4526-4540},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Predicting user quitting ratio in adaptive bitrate video streaming},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptively clustering-driven learning for visual
relationship detection. <em>TMM</em>, <em>23</em>, 4515–4525. (<a
href="https://doi.org/10.1109/TMM.2020.3043084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual relationship detection aims to describe the interactions between pairs of objects, such as person-ride-bike and bike-next to-car triplets. In reality, it is often the case that there exist some groups of strongly correlated relationships, while others are weakly related. Intuitively, the common relationships can be roughly categorized into several types such as geometric (e.g., next to), action (e.g., ride), and so on. However, previous studies ignore the relatedness discovery among multiple relationships, which only lie on a unified space to leverage visual features or statistical dependencies into categories. To tackle this problem, we propose an adaptively clustering-driven network for visual relationship detection, which can implicitly divide the unified relationship space into several subspaces with specific characteristics. Particularly, we propose two novel modules to discover the common distribution space and latent relationship association, respectively, which map pairs of object features into translation subspaces to induce the discriminative relationship clustering. Then, a fused inference is designed to integrate the group-induced representations with the language prior to facilitate the predicate inference. Especially, we design the Frobenius-norm regularization to boost the clustering. To the best of our knowledge, the proposed method is the first supervised framework to realize subject-predicate-object relationship-aware clustering for visual relationship detection. Extensive experiments show that the proposed method can achieve competing performances against the state-of-the-art methods on the Visual Genome dataset. Additional ablation studies further validate its effectiveness.},
  archive      = {J_TMM},
  author       = {An-An Liu and Yanhui Wang and Ning Xu and Weizhi Nie and Jie Nie and Yongdong Zhang},
  doi          = {10.1109/TMM.2020.3043084},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4515-4525},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptively clustering-driven learning for visual relationship detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep texture exemplar extraction based on trimmed t-CNN.
<em>TMM</em>, <em>23</em>, 4502–4514. (<a
href="https://doi.org/10.1109/TMM.2020.3043130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture exemplar has been widely used in synthesizing 3D movie scenes and appearances of virtual objects. Unfortunately, conventional texture synthesis methods usually only emphasized on generating optimal target textures with arbitrary sizes or diverse effects, and put little attention to automatic texture exemplar extraction. Obtaining texture exemplars is still a labor intensive task, which usually requires carefully cropping and post-processing. In this paper, we present an automatic texture exemplar extraction based on Trimmed Texture Convolutional Neural Network (Trimmed T-CNN). Specifically, our Trimmed T-CNN is filter banks for texture exemplar classification and recognition. Our Trimmed T-CNN is learned with a standard ideal exemplar dataset containing thousands of desired texture exemplars, which were collected and cropped by our invited artists. To efficiently identify the exemplar candidates from an input image, we employ a selective search algorithm to extract the potential texture exemplar patches. We then put all candidates into our Trimmed T-CNN for learning ideal texture exemplars based on our filter banks. Finally, optimal texture exemplars are identified with a scoring and ranking scheme. Our method is evaluated with various kinds of textures and user studies. Comparisons with different feature-based methods and different deep CNN architectures (AlexNet, VGG-M, Deep-TEN and FV-CNN) are also conducted to demonstrate its effectiveness.},
  archive      = {J_TMM},
  author       = {Huisi Wu and Wei Yan and Ping Li and Zhenkun Wen},
  doi          = {10.1109/TMM.2020.3043130},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4502-4514},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep texture exemplar extraction based on trimmed T-CNN},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid approach for detecting prerequisite relations in
multi-modal food recipes. <em>TMM</em>, <em>23</em>, 4491–4501. (<a
href="https://doi.org/10.1109/TMM.2020.3042706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the structure of culinary recipes is the core of recipe representation learning. Current approaches mostly focus on extracting the workflow graph from recipes based on text descriptions. Process images, which constitute an important part of cooking recipes, has rarely been investigated in recipe structure modeling. We study this recipe structure problem from a multi-modal learning perspective, by proposing a prerequisite tree to represent recipes with cooking images at a step-level granularity. We propose a simple-yet-effective two-stage framework to automatically construct the prerequisite tree for a recipe by (1) utilizing a trained classifier to detect pairwise prerequisite relations that fuses multi-modal features as input; then (2) applying different strategies (greedy method, maximum weight, and beam search) to build the tree structure. Experiments on the MM-ReS dataset demonstrates the advantages of introducing process images for recipe structure modeling. Also, compared with neural methods which require large numbers of training data, we show that our two-stage pipeline can achieve promising results using only 400 labeled prerequisite trees as training data.},
  archive      = {J_TMM},
  author       = {Liangming Pan and Jingjing Chen and Shaoteng Liu and Chong-Wah Ngo and Min-Yen Kan and Tat-Seng Chua},
  doi          = {10.1109/TMM.2020.3042706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4491-4501},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hybrid approach for detecting prerequisite relations in multi-modal food recipes},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From edge to keypoint: An end-to-end framework for indoor
layout estimation. <em>TMM</em>, <em>23</em>, 4483–4490. (<a
href="https://doi.org/10.1109/TMM.2020.3042669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of spatial layout estimation of monocular image is to segment an RGB image of indoor scenes with semantic surface labels (i.e., ceiling, floor, front wall, left wall, and right wall). Most recent methods have to produce layout hypotheses based on the estimated edge map or semantic labels, and then rank the layout hypotheses. In this paper, we present an end-to-end framework that can directly output the layout type and keypoint coordinates (defined in the LSUN challenge). The proposed method takes advantage of transfer learning via learning on the fake samples, i.e., plenty of artificial {type, keypoints, edge map} triplets are generated to learn the mapping from edge maps to keypoint coordinates. Generative adversarial network (GAN) is implemented in this work for domain adaptation of the edge maps. Experimental results show that the proposed method can achieve state-of-the-art layout estimation performance on benchmark datasets.},
  archive      = {J_TMM},
  author       = {Weidong Zhang and Qian Zhang and Wei Zhang and Jianjun Gu and Yibin Li},
  doi          = {10.1109/TMM.2020.3042669},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4483-4490},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {From edge to keypoint: An end-to-end framework for indoor layout estimation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). APSE: Attention-aware polarity-sensitive embedding for
emotion-based image retrieval. <em>TMM</em>, <em>23</em>, 4469–4482. (<a
href="https://doi.org/10.1109/TMM.2020.3042664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of social media, an increasing number of people are accustomed to expressing their feelings and emotions online using images and videos. An emotion-based image retrieval (EBIR) system is useful for obtaining visual contents with desired emotions from a massive repository. Existing EBIR methods mainly focus on modeling the global characteristics of visual content without considering the crucial role of informative regions of interest in conveying emotions. Further, they ignore the hierarchical relationships between coarse polarities and fine categories of emotions. In this paper, we design an attention-aware polarity-sensitive embedding (APSE) network to address these issues. First, we develop a hierarchical attention mechanism to automatically discover and model the informative regions of interest. Specifically, both polarity- and emotion-specific attended representations are aggregated for discriminative feature embedding. Second, we propose a generated emotion-pair (GEP) loss to simultaneously consider the inter- and intra-polarity relationships of the emotion labels. Moreover, we adaptively generate negative examples of different hard levels in the feature space guided by the attention module to further improve the performance of feature embedding. Extensive experiments on four popular benchmark datasets demonstrate that the proposed APSE method outperforms the state-of-the-art EBIR approaches by a large margin.},
  archive      = {J_TMM},
  author       = {Xingxu Yao and Sicheng Zhao and Yu-Kun Lai and Dongyu She and Jie Liang and Jufeng Yang},
  doi          = {10.1109/TMM.2020.3042664},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4469-4482},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {APSE: Attention-aware polarity-sensitive embedding for emotion-based image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PVC-SLP: Perceptual vibrotactile-signal compression based-on
sparse linear prediction. <em>TMM</em>, <em>23</em>, 4455–4468. (<a
href="https://doi.org/10.1109/TMM.2020.3042674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a signal compression technique that is able to achieve a low bit rate while maintaining high perceptual signal quality is a classical signal processing problem vigorously studied for audio, speech, image, and video type of signals. Yet, until recently, there has been limited effort directed toward the compression of vibrotactile signals, which represent a crucial element of rich touch (haptic) information. A vibrotactile signal; produced when stroking a textured surface with a tool-tip or bare-finger; like other signals contains a great deal of redundant and imperceptible information that can be exploited for efficient compression. This paper presents PVC-SLP, a vibrotactile perceptual coding approach. PVC-SLP employs a model of tactile sensitivity; called ASF (Acceleration Sensitivity Function); for perceptual coding. The ASF is inspired by the four channels model that mediate the perception of vibrotactile stimuli in the glabrous skin. The compression algorithm introduces sparsity constraints in a linear prediction scheme both on the residual and the predictor coefficients. The perceptual quantization of the residual is developed through the use of ASF. The quantization parameters of the residual and the predictor coefficients were jointly optimized; by means of both squared error and perceptual quality measures; to find the sweet spot of the rate-distortion curve. PVC-SLP coding performance is evaluated using two publicly available databases that collectively comprise 1281 vibrotactile signals covering 193 material classes. Furthermore, we compare PVC-SLP with a recent vibrotactile compression method and show that PVC-SLP perceptually outperforms existing method by a sizable margin. Most recently, PVC-SLP has been selected to become part of the haptic codec standard currently under preparation by IEEE P1918.1.1, aka Haptic Codecs for the Tactile Internet.},
  archive      = {J_TMM},
  author       = {Rania Hassen and Basak Gülecyüz and Eckehard Steinbach},
  doi          = {10.1109/TMM.2020.3042674},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4455-4468},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PVC-SLP: Perceptual vibrotactile-signal compression based-on sparse linear prediction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal disentangled domain adaption for social media
event rumor detection. <em>TMM</em>, <em>23</em>, 4441–4454. (<a
href="https://doi.org/10.1109/TMM.2020.3042055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of social media and the increasing scale of social media data, the rumor detection on social media platforms has become vitally crucial. The key challenges for rumor detection on social media platforms are how to identify rumors deeply entangled with the specific content and how to detect rumors for the emerging social media events without labeled data. Unfortunately, most of the existing approaches can hardly handle these challenges since they tend to learn event-specific features and cannot transfer the learned features to newly emerged events. To tackle the above challenges, we propose a novel Multimodal Disentangled Domain Adaption (MDDA) method which can derive event-invariant features and thus benefit the detection of rumors on emerging social media events. The model consists of two components: the multimodal disentangled representation learning and the unsupervised domain adaptation. The multimodal disentangled representation learning is responsible for disentangling the multimedia posts into the content features and the rumor style features, and removing the content-specific features from post representation. The unsupervised domain adaptation aims to filter out the event-specific features and keep shared rumor style features among events. Based on the final event-invariant rumor style features, we train a robust social media rumor detector that can transfer knowledge from source events to the target events, which can perform well on the newly emerged events. Extensive experiments on two Twitter benchmark datasets demonstrate that our rumor detection model outperforms state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Huaiwen Zhang and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3042055},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4441-4454},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal disentangled domain adaption for social media event rumor detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Referring expression comprehension: A survey of methods and
datasets. <em>TMM</em>, <em>23</em>, 4426–4440. (<a
href="https://doi.org/10.1109/TMM.2020.3042066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-defined, the REC problem only can observe the queries during the test. It is more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we first examine the state-of-the-art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC approaches. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for this field, in particular the compositional referring expression comprehension that requires more reasoning steps to address.},
  archive      = {J_TMM},
  author       = {Yanyuan Qiao and Chaorui Deng and Qi Wu},
  doi          = {10.1109/TMM.2020.3042066},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4426-4440},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Referring expression comprehension: A survey of methods and datasets},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Parameter sharing exploration and hetero-center triplet
loss for visible-thermal person re-identification. <em>TMM</em>,
<em>23</em>, 4414–4425. (<a
href="https://doi.org/10.1109/TMM.2020.3042080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the visible-thermal cross-modality person re-identification (VT Re-ID) task, whose goal is to match person images between the daytime visible modality and the nighttime thermal modality. The two-stream network is usually adopted to address the cross-modality discrepancy, the most challenging problem for VT Re-ID, by learning the multi-modality person features. In this paper, we explore how many parameters a two-stream network should share, which is still not well investigated in the existing literature. By splitting the ResNet50 model to construct the modality-specific feature extraction network and modality-sharing feature embedding network, we experimentally demonstrate the effect of parameter sharing of two-stream network for VT Re-ID. Moreover, in the framework of part-level person feature learning, we propose the hetero-center triplet loss to relax the strict constraint of traditional triplet loss by replacing the comparison of the anchor to all the other samples by the anchor center to all the other centers . With extremely simple means, the proposed method can significantly improve the VT Re-ID performance. The experimental results on two datasets show that our proposed method distinctly outperforms the state-of-the-art methods by large margins, especially on the RegDB dataset achieving superior performance, rank1/mAP/mINP 91.05%/83.28%/68.84%. It can be a new baseline for VT Re-ID, with a simple but effective strategy.},
  archive      = {J_TMM},
  author       = {Haijun Liu and Xiaoheng Tan and Xichuan Zhou},
  doi          = {10.1109/TMM.2020.3042080},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4414-4425},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Parameter sharing exploration and hetero-center triplet loss for visible-thermal person re-identification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning fundamental visual concepts based on evolved
multi-edge concept graph. <em>TMM</em>, <em>23</em>, 4400–4413. (<a
href="https://doi.org/10.1109/TMM.2020.3042072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, visual media comprises a set of elements of basic semantics, named fundamental visual concepts, that may not be semantically decomposed, such as objects, scenes and actions. This paper proposes a dynamic learning framework for fundamental visual concept learning from image-textual description paired data based on an evolved multi-edge concept graph (EMCG). First, we construct a multi-edge concept graph to represent the relationships between visual concept instances, in which we introduce two types of edges named visual edges and semantic edges to describe the connection strength in terms of visual appearance and semantic content. Second, we evolve the graph by updating connection strength based on the predicted results of concept learning. Finally, we present a growth algorithm for the multi-edge concept graph to handle cross-dataset concept learning. Driven by the predictions, the multi-edge concept graph can dynamically evolve over time by adjusting the connection strength to adapt better to the observations. In addition, our approach can be considered a weakly-supervised learning algorithm since no labeled concepts are employed for learning. Experimental results demonstrate that evolution can significantly improve the learning of fundamental visual concepts by $\text{14.2}\%$ , $\text{7.9}\%$ and $\text{12.7}\%$ in terms of F1-score for the MSRC, VOC2012 and MSCOCO datasets, respectively, and that the proposed EMCG approach largely outperforms the compared approaches.},
  archive      = {J_TMM},
  author       = {Hang Wang and Youtian Du and Guangxun Zhang and Zhongmin Cai and Chang Su},
  doi          = {10.1109/TMM.2020.3042072},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4400-4413},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning fundamental visual concepts based on evolved multi-edge concept graph},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast multi-type tree partitioning for versatile video coding
using a lightweight neural network. <em>TMM</em>, <em>23</em>,
4388–4399. (<a href="https://doi.org/10.1109/TMM.2020.3042062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a fast decision scheme using a lightweight neural network (LNN) to avoid redundant block partitioning in versatile video coding (VVC). A more versatile block structure, named the multi-type tree (MTT) structure, which includes binary trees (BTs) and ternary trees (TTs), is adopted by VCC, in addition to the traditional quadtree structure. The MTT improved the coding efficiency compared with previous video coding standards. However, the new tree structures, mainly TT, significantly increased the complexity of the VVC encoder. Although widespread application of VVC has been inhibited, this problem has not yet been investigated thoroughly in the literature. In this study, we first determine the statistical characteristics of coded parameters that exhibit correlation with the TT and develop two useful types of features —explicit VVC features (EVFs) and derived VVC features (DVFs) — to facilitate the intra coding of VVC. These features can be obtained efficiently during the intra prediction before the determination of the best block partitioning during rate-distortion optimization in VVC encoding. Our LNN model decides whether to terminate the nested TT block structures subsequent to a quadtree based on the features. The experimental results confirm that the proposed method substantially decreases the encoding complexity of VVC with a slight coding loss under the All Intra configuration. Our code, models, and dataset are available at https://github.com/foriamweak/MTTPartitioning_LNN .},
  archive      = {J_TMM},
  author       = {Sang-hyo Park and Je-Won Kang},
  doi          = {10.1109/TMM.2020.3042062},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4388-4399},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast multi-type tree partitioning for versatile video coding using a lightweight neural network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person retrieval in surveillance videos via deep attribute
mining and reasoning. <em>TMM</em>, <em>23</em>, 4376–4387. (<a
href="https://doi.org/10.1109/TMM.2020.3042068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person retrieval largely relies on the appearance features of pedestrians. This task is rather more difficult in surveillance videos due to the limitations of extracting robust appearance features brought by the cross-view and cross-camera data with lower image resolution, motion blur, occlusion and other kinds of image degradation. To build up a more reliable person retrieval system, recent works introduced appearance attribute models to describe and distinguish different persons with high-level semantic concepts. Despite the progress of previous works, the value of utilizing appearance attributes is still under-explored. On one hand, existing methods lack for concise and precise attribute representations that are specific for each attribute category and, in the meantime, are able to filter noisy information in irrelevant spatial locations and useless patterns. On the other hand, correlation and reasoning between different attributes are neglected, which could generate more useful information and add more robustness to the retrieval system. In this paper, we propose an Attribute Mining and Reasoning (AMR) framework which is capable to handle the issues in question. The AMR makes better use of appearance attributes with two main components. First, the AMR disentangles the representations of different attributes by localizing their spatial positions and identifying their effective patterns in a weakly supervised manner. To achieve more reliable localization, we propose the Attribute Localization Ensemble (ALE) module that is consisted of multiple localization heads and a voting mechanism. Second, we introduce the Attribute Reasoning (AR) module to correlate different attributes together with the global appearance features and discover their latent relations to generate more comprehensive descriptions of pedestrians. Extensive experiments on DukeMTMC-ReID and Market-1501 datasets demonstrate the effectiveness of the proposed AMR framework as well as its superiority over the existing state-of-the-art methods. The AMR model also shows great generalization ability on the unseen CUHK03 dataset when it is only trained on Market-1501 dataset.},
  archive      = {J_TMM},
  author       = {Yuxuan Shi and Zhen Wei and Hefei Ling and Ziyang Wang and Jialie Shen and Ping Li},
  doi          = {10.1109/TMM.2020.3042068},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4376-4387},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Person retrieval in surveillance videos via deep attribute mining and reasoning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal action localization using long short-term
dependency. <em>TMM</em>, <em>23</em>, 4363–4375. (<a
href="https://doi.org/10.1109/TMM.2020.3042077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization in untrimmed videos is an important but difficult task. Difficulties are encountered in the application of existing methods when modeling the temporal structures of videos. In the present study, we develop a novel method, referred to as the Gemini Network, for effective modeling of temporal structures and achieving high-performance temporal action localization. The significant improvements afforded by the proposed method are due to three major factors. First, temporal dependencies are explicitly distinguished as long-term temporal dependencies and short-term temporal dependencies and are separately captured by two dedicated subnets. Second, a long-range temporal dependency capture module combined with a self-adaptive pooling module is proposed to capture long-term temporal dependency. Third, the proposed method uses auxiliary supervision, with the auxiliary classifier losses affording additional constraints for improving the modeling capability of the network. As a demonstration of its effectiveness, the Gemini Network is used to achieve a state-of-the-art temporal action localization performance on two challenging datasets, namely, THUMOS14 and ActivityNet.},
  archive      = {J_TMM},
  author       = {Yuan Zhou and Ruolin Wang and Hongru Li and Sun-Yuan Kung},
  doi          = {10.1109/TMM.2020.3042077},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4363-4375},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Temporal action localization using long short-term dependency},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEA: Sentence encoder assembly for video retrieval by
textual queries. <em>TMM</em>, <em>23</em>, 4351–4362. (<a
href="https://doi.org/10.1109/TMM.2020.3042067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that uses only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.},
  archive      = {J_TMM},
  author       = {Xirong Li and Fangming Zhou and Chaoxi Xu and Jiaqi Ji and Gang Yang},
  doi          = {10.1109/TMM.2020.3042067},
  journal      = {IEEE Transactions on Multimedia},
  month        = {12},
  pages        = {4351-4362},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SEA: Sentence encoder assembly for video retrieval by textual queries},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Redundancy and optimization of tANS entropy encoders.
<em>TMM</em>, <em>23</em>, 4341–4350. (<a
href="https://doi.org/10.1109/TMM.2020.3040547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays entropy encoders are part of almost all data compression methods, with the Asymmetrical Numeral Systems (ANS) family of entropy encoders having recently risen in popularity. Entropy encoders based on the tabled variant of ANS are known to provide varying performances depending on their internal design. In this paper, we present a method that calculates encoder redundancies in almost linear time, which translates in practice to thousand-fold speedups in redundancy calculations for small automatons, and allows redundancy calculations for automatons with tens of millions of states that would be otherwise prohibitive. We also address the problem of improving tabled ANS encoder designs, by employing the aforementioned redundancy calculation method in conjunction with a stochastic hill climbing strategy. The proposed approach consistently outperforms state-of-the-art methods in tabled ANS encoder design. For automatons of twice the alphabet size, experimental results show redundancy reductions around 10% over the default initialization method and over 30% for random initialization.},
  archive      = {J_TMM},
  author       = {Ian Blanes and Miguel Hernández-Cabronero and Joan Serra-Sagristà and Michael W. Marcellin},
  doi          = {10.1109/TMM.2020.3040547},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4341-4350},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Redundancy and optimization of tANS entropy encoders},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). TTL-IQA: Transitive transfer learning based no-reference
image quality assessment. <em>TMM</em>, <em>23</em>, 4326–4340. (<a
href="https://doi.org/10.1109/TMM.2020.3040529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment (IQA) based on deep learning faces the overfitting problem due to limited training samples available in existing IQA databases. Transfer learning is a plausible solution to the problem, in which the shared features derived from the large-scale Imagenet source domain could be transferred from the original recognition task to the intended IQA task. However, the Imagenet source domain and the IQA target domain as well as their corresponding tasks are not directly related. In this paper, we propose a new transitive transfer learning method for no-reference image quality assessment (TTL-IQA). First, the architecture of the multi-domain transitive transfer learning for IQA is developed to transfer the Imagenet source domain to the auxiliary domain, and then to the IQA target domain. Second, the auxiliary domain and the auxiliary task are constructed by a new generative adversarial network based on distortion translation (DT-GAN). Furthermore, a TTL network of the semantic features transfer (SFTnet) is proposed to optimize the shared features for the TTL-IQA. Experiments are conducted to evaluate the performance of the proposed method on various IQA databases, including the LIVE, TID2013, CSIQ, LIVE multiply distorted and LIVE challenge. The results show that the proposed method significantly outperforms the state-of-the-art methods. In addition, our proposed method demonstrates a strong generalization ability.},
  archive      = {J_TMM},
  author       = {Xiaohan Yang and Fan Li and Hantao Liu},
  doi          = {10.1109/TMM.2020.3040529},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4326-4340},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TTL-IQA: Transitive transfer learning based no-reference image quality assessment},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative social-aware and QoE-driven video caching and
adaptation in edge network. <em>TMM</em>, <em>23</em>, 4311–4325. (<a
href="https://doi.org/10.1109/TMM.2020.3040532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emerging demand for high-definition videos in recent years, Multi-access Edge Computing (MEC) has become a promising solution to leverage Quality of Experience (QoE) of users in the 5G mobile network, which provides computing and cache resource at network edges to serve end users with less latency. Also, since mobile users tend to be influenced by the trends in social media, the performance of video caching will become more effective if we can extract the hidden information from interaction among them. In this paper, we propose a novel Collaborative Social-aware QoE-driven video Caching and Adaption (CSQCA) framework. Specifically, we first design a 2-tier MEC collaborative video caching architecture, which partially caches popular videos among multiple edge servers. Second, we propose a social-aware proactive cache strategy, which embeds interactions of users and video dissemination process in social networks into the caching mechanism. Third, a QoE-driven video adaptation algorithm is presented to dynamically transcode the cached videos into appropriate resolution on edge server for each request. Finally, we conduct our simulation based on real-world datasets. The simulation results show that the proposed CSQCA framework outperforms traditional cache algorithms, in terms of the average hit ratio and QoE.},
  archive      = {J_TMM},
  author       = {Yao Chiang and Chih-Ho Hsu and Hung-Yu Wei},
  doi          = {10.1109/TMM.2020.3040532},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4311-4325},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Collaborative social-aware and QoE-driven video caching and adaptation in edge network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative region mining for object detection.
<em>TMM</em>, <em>23</em>, 4297–4310. (<a
href="https://doi.org/10.1109/TMM.2020.3040539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In generic object detection, detectors are often susceptible to foreground objects and background regions that share similar appearances. In this paper, we propose a novel discriminative region mining (DRM) module for object detection, which enables discriminative region localization and representation for accurate object identification. The DRM module is collaboratively optimized by an extra intramodule classification loss in addition to the usual detection loss, which ensures its adequate discriminative capability. Specifically, two derivatives of the DRM module, namely a local DRM module and a contextual DRM module are proposed to excavate local and contextual discriminative regions, respectively. Furthermore, we extend the local DRM module to capture multiple local discriminative regions with a diversity constraint. To explore informative local features, an image upsampling branch is introduced to generate fine-grained representation for the local DRM module. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate the effectiveness of the proposed method. Simple baseline detectors with the built-in DRM can achieve state-of-the-art detection performance. For example, the proposed detector achieves a mean average precision of 81.0% on PASCAL VOC 2007 with an input size of $\text{300} \times \text{300}$ using a ResNet-18 backbone, which runs at 24.2 fps on an Nvidia Titan X GPU.},
  archive      = {J_TMM},
  author       = {Lvran Chen and Huicheng Zheng and Zhiwei Yan and Ye Li},
  doi          = {10.1109/TMM.2020.3040539},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4297-4310},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Discriminative region mining for object detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dual-pooling graph neural networks for few-shot
video classification. <em>TMM</em>, <em>23</em>, 4285–4296. (<a
href="https://doi.org/10.1109/TMM.2020.3039329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of few-shot video classification that learns classifiers for novel concepts from only a few examples. Most current methods ignore to explicitly consider the relations in both intra-video and inter-video domains, thus cannot take full advantage of the structural information in few-shot learning. In this paper, we propose to exploit the comprehensive intra-video and inter-video relations via Graph Neural Networks (GNNs). To improve the discriminative ability for accurately selecting the representative video content and refining video relations, a Dual-Pooling GNN (DPGNN) is constructed, which stacks customized graph pooling layers in a hierarchical fashion. Specifically, to select the most representative frames in a video, we build intra-video graphs and utilize a node pooling module to extract robust video-level features. We construct an inter-video graph by taking the video-level features as nodes. By designing an edge pooling module, the proposed method can adaptively eliminate the negative relations in the inter-video graph. Extensive experimental results show that our method consistently outperforms the state-of-the-art on two benchmarks.},
  archive      = {J_TMM},
  author       = {Yufan Hu and Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3039329},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4285-4296},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning dual-pooling graph neural networks for few-shot video classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DSLR: Deep stacked laplacian restorer for low-light image
enhancement. <em>TMM</em>, <em>23</em>, 4272–4284. (<a
href="https://doi.org/10.1109/TMM.2020.3039361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various images captured in complicated lighting conditions often suffer from deterioration of the image quality. Such poor quality not only dissatisfies the user expectation but also may lead to a significant performance drop in many applications. In this paper, anovel method for low-light image enhancement is proposed by leveraging useful propertiesof the Laplacian pyramid both in image and feature spaces. Specifically, the proposed method, so-called a deep stacked Laplacian restorer (DSLR), is capable of separately recovering the global illumination and local details from the original input, and progressively combining them in the image space. Moreover, the Laplacian pyramid defined in the feature space makes such recovering processes more efficient based on abundant connectionsof higher-order residuals in a multiscale structure. This decomposition-based scheme is fairly desirable for learning the highly nonlinear relation between degraded images and their enhanced results. Experimental results on various datasets demonstrate that the proposed DSLR outperforms state-of-the-art methods. The code and model are publicly available at: https://github.com/SeokjaeLIM/DSLR-release .},
  archive      = {J_TMM},
  author       = {Seokjae Lim and Wonjun Kim},
  doi          = {10.1109/TMM.2020.3039361},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4272-4284},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSLR: Deep stacked laplacian restorer for low-light image enhancement},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind quality assessment of screen content images via
macro-micro modeling of tensor domain dictionary. <em>TMM</em>,
<em>23</em>, 4259–4271. (<a
href="https://doi.org/10.1109/TMM.2020.3039382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screen content images (SCIs) have been rapidly and widely applied in interactive multimedia applications. The problem of quality assessment for SCIs is an interesting research topic. Most of the existing methods use subjective and independent features in gray domain to predict the image quality, which cannot comprehensively characterize the image properties or lack unified mathematical explanation for SCIs. To address these problems, we propose a novel blind quality assessment method based on macro-micro modeling of tensor domain dictionary for SCIs in this article. In the proposed method, the tensor decomposition is explored first to avoid the loss of color information, and then a target dictionary is learned more effectively with the principal components. Furthermore, a macro-micro model is established to characterize the micro and macro features in the target dictionary space, which can provide a systematic mathematical interpretation for feature extraction. For the micro features, a log-normal pooling scheme is designed to enhance the effectiveness of feature aggregation by analyzing the particularity of the statistical distribution of sparse codes. Additionally, the statistical properties are mainly discussed and studied based on the Bernoulli law of large numbers, and then a reliable macro feature is generated to describe the relationship between the statistical distribution and quality degradation of SCIs. Experimental results determined by using three public SCI databases show that the proposed method can perform better than relevant existing methods in the prediction of the visual quality of SCIs, especially in terms of the generalization for distortion type and interpretability for feature generation.},
  archive      = {J_TMM},
  author       = {Yongqiang Bai and Zhongjie Zhu and Gangyi Jiang and Huifang Sun},
  doi          = {10.1109/TMM.2020.3039382},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4259-4271},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind quality assessment of screen content images via macro-micro modeling of tensor domain dictionary},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-visiting discriminator for blind free-viewpoint image
quality assessment. <em>TMM</em>, <em>23</em>, 4245–4258. (<a
href="https://doi.org/10.1109/TMM.2020.3038305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate measurement of perceptual quality is important for various immersive multimedia, which demand real-time quality control or quality-based bench-marking for relevant algorithms. For instance, virtual views rendering in Free-Viewpoint (FV) navigation scenarios is a typical case that introduces challenging distortions, particularly the ones around dis-occluded regions. Existing quality metrics, most of which are targeting for impairments caused by compression or network condition, fail to quantify such non-uniform structure-related distortions. Moreover, the lack of quality databases for such distortions makes it even more challenging to develop robust quality metrics. In this work, a Generative Adversarial Networks based No-Reference (NR) quality Metric, namely GANs-NRM, is proposed. We first present an approach to create masks mimicking dis-occlusions/textureless regions, which is applicable on large-scale 2D image databases publicly available in the computer vision domain. Using these synthetic data, we then train a GANs-based context renderer with the capability of rendering those masked regions. Since the naturalness of the rendered dis-occluded regions strongly relates to the perceptual quality, we assume that the discriminator of the trained GANs has an intrinsic ability for quality assessment. We thus use the features extracted from the discriminator to learn a Bag-of-Distortion-Word (BDW) codebook. We show that a quality predictor can be then well trained using only a small amount of subjective quality data for the FV views rendering. Moreover, in the proposed framework, the discriminator is also adapted as a distortion-detector to locate possible distorted regions. According to the experimental results, the proposed model outperforms significantly the state-of-the-art quality metrics. The corresponding context renderer also shows appealing visualized results over other rendering algorithms.},
  archive      = {J_TMM},
  author       = {Suiyi Ling and Jing Li and Zhaohui Che and Wei Zhou and Junle Wang and Patrick Le Callet},
  doi          = {10.1109/TMM.2020.3038305},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4245-4258},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Re-visiting discriminator for blind free-viewpoint image quality assessment},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capturing relevant context for visual tracking.
<em>TMM</em>, <em>23</em>, 4232–4244. (<a
href="https://doi.org/10.1109/TMM.2020.3038310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have shown that contextual information can promote the robustness of trackers. However, trackers based on convolutional neural networks (CNNs) only capture local features, which limits their performance. We propose a novel relevant context block (RCB), which employs graph convolutional networks to capture the relevant context. In particular, it selects the $k$ largest contributors as nodes for each query position (unit) that contain meaningful and discriminative contextual information and updates the nodes by aggregating the differences between the query position and its contributors. This operation can be easily incorporated into the existing networks and can be easily end-to-end trained using a standard backpropagation algorithm. To verify the effectiveness of RCB, we apply it to two trackers, SiamFC and GlobalTrack, respectively, and the two improved trackers are referred to as Siam-RCB and GlobalTrack-RCB. Extensive experiments on OTB, VOT, UAV123, LaSOT, TrackingNet, OxUvA, and VOT2018LT show the superiority of our method. For example, our Siam-RCB outperforms SiamFC by a very large margin (up to 11.2% in the success score and 7.8% in the precision score) on the OTB-100 benchmark.},
  archive      = {J_TMM},
  author       = {Yuping Zhang and Bo Ma and Jiahao Wu and Lianghua Huang and Jianbing Shen},
  doi          = {10.1109/TMM.2020.3038310},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4232-4244},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Capturing relevant context for visual tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adaptation for food intake classification with
teacher/student learning. <em>TMM</em>, <em>23</em>, 4220–4231. (<a
href="https://doi.org/10.1109/TMM.2020.3038315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic dietary monitoring (ADM) stands as a challenging application in wearable healthcare technologies. In this paper, we define an ADM to perform food intake classification (FIC) over throat microphone recordings. We investigate the use of transfer learning to design an improved FIC system. Although labeled data with acoustic close-talk microphones are abundant, throat data is scarce. Therefore, we propose a new adaptation framework based on teacher/student learning. The teacher network is trained over high-quality acoustic microphone recordings, whereas the student network distills deep feature extraction capacity of the teacher over a parallel dataset. Our approach allows us to transfer the representational capacity, adds robustness to the resulting model, and improves the FIC through throat microphone recordings. The classification problem is formulated as a spectra-temporal sequence recognition using the Convolutional LSTM (ConvLSTM) models. We evaluate the proposed approach using a large scale acoustic dataset collected from online recordings, an in-house food intake throat microphone dataset, and a parallel speech dataset. The bidirectional ConvLSTM network with the proposed domain adaptation approach consistently outperforms the SVM- and CNN-based baseline methods and attains 85.2% accuracy for the classification of 10 different food intake items. This translates to 17.8% accuracy improvement with the proposed domain adaptation.},
  archive      = {J_TMM},
  author       = {M. A. Tuğtekin Turan and Engin Erzin},
  doi          = {10.1109/TMM.2020.3038315},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4220-4231},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Domain adaptation for food intake classification with Teacher/Student learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and efficient RGB-d SLAM in dynamic environments.
<em>TMM</em>, <em>23</em>, 4208–4219. (<a
href="https://doi.org/10.1109/TMM.2020.3038323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) using an RGB-D camera is a key enabling technique for many augmented reality (AR) applications. However, most existing RGB-D SLAM methods could fail in dynamic scenarios due to non-trivial pose estimation errors arising from moving objects. In this study, we present an accurate and robust RGB-D SLAM system for dynamic scenarios which can run real-time on a single dual-core CPU. The core of our system is a robust and efficient dynamic keypoint exclusion method which consists of three steps: 1) grouping spatially and appearance related pixels of a keyframe into regions; 2) identifying dynamic regions by checking motion consistency of keypoints in every region; 3) excluding keypoints in the identified dynamic regions as well as the matching points in the 3D local map. The dynamic keypoint exclusion method can be easily integrated into any keypoint based RGB-D SLAM system for improving the accuracy and robustness in dynamic scenes with trivial time increase (16.6ms per frame). Experimental results on the TUM dataset demonstrates that our method which runs on an Intel i7-4900 CPU is even 2.3X faster than the state-of-the-art method DS-SLAM [1] which runs parallel on a P4000 GPU and a comparable CPU. In addition, our system outperforms the state-of-the-art methods [1]–[4] in terms of smaller absolute trajectory errors (ATE). We also apply our system to a real AR application and live experiments with a hand-held RGB-D camera demonstrate the robustness and generalizability of our method in practical scenarios. 1 1A demo video is provided on https://github.com/cc-qy/Dynamic-RGB-D-SLAM},
  archive      = {J_TMM},
  author       = {Xin Yang and Zikang Yuan and Dongfu Zhu and Cheng Chi and Kun Li and Chunyuan Liao},
  doi          = {10.1109/TMM.2020.3038323},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4208-4219},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust and efficient RGB-D SLAM in dynamic environments},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving driver gaze prediction with reinforced attention.
<em>TMM</em>, <em>23</em>, 4198–4207. (<a
href="https://doi.org/10.1109/TMM.2020.3038311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of driver gaze prediction: estimating where the location of the focus of a driver should be, based on a raw video of the outside environment. In practice, we output a probability map that gives the normalized probability of each point in a given scene being the object of the driver attention. Most existing methods ( i.e. , Coarse-to-Fine and Multi-branch ) take an image or a video as input and directly output the fixation map. While successful, these methods can often produce highly scattered predictions, rendering them unreliable for real-world usage. Motivated by this observation, we propose the reinforced attention (RA) model as a regulatory mechanism to increase prediction density. Our method is built directly on top of existing methods, making it complementary to current approaches. Specifically, we first use Multi-branch to obtain an initial fixation map. Then, RA is trained using deep reinforcement learning to learn a location prediction policy, producing a reinforced attention. Finally, in order to obtain the final gaze prediction result, we combine the fixation map and the reinforced attention by a mask-guided multiplication. Experimental results show that our framework improves the accuracy of gaze prediction, and provides state-of-the-art performance on the DR(eye)VE dataset.},
  archive      = {J_TMM},
  author       = {Kai Lv and Hao Sheng and Zhang Xiong and Wei Li and Liang Zheng},
  doi          = {10.1109/TMM.2020.3038311},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4198-4207},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving driver gaze prediction with reinforced attention},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A resource-efficient parallel connected component labeling
algorithm and its hardware implementation. <em>TMM</em>, <em>23</em>,
4184–4197. (<a href="https://doi.org/10.1109/TMM.2020.3037511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected Component labeling (CCL) is usually time-consuming, so a dedicated hardware accelerator of CCL is essential in the embedded vision and multimedia system. In this paper, we propose a single-scan resource-efficient parallel CCL algorithm. Our CCL method scans two adjacent rows simultaneously to extract runs and detect equivalent runs; an equivalent label set is used to resolve equivalences. After each row scan, the finished objects are output in time, and the freed memory resources are reused to reduce memory requirements. Both pixel-based labeled image (PLI) and run-based labeled image (RLI) can be generated by our CCL method. In addition, the steps of our CCL method are executed concurrently to improve labeling performance. The hardware architecture based on our CCL method is implemented with Verilog. The evaluation results illustrate that our CCL architecture can label more than 40 2048 × 1536 benchmark images per second on average, and outperforms previous CCL architectures in terms of labeling performance or memory resource consumption.},
  archive      = {J_TMM},
  author       = {Chen Zhao and Wu Gao and Feiping Nie},
  doi          = {10.1109/TMM.2020.3037511},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4184-4197},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A resource-efficient parallel connected component labeling algorithm and its hardware implementation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformer encoder with multi-modal multi-head attention
for continuous affect recognition. <em>TMM</em>, <em>23</em>, 4171–4183.
(<a href="https://doi.org/10.1109/TMM.2020.3037496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous affect recognition is becoming an increasingly attractive research topic in affective computing. Previous works mainly focused on modelling the temporal dependency within a sensor modality, or adopting early or late fusion for multi-modal affective state recognition. However, early fusion suffers from the curse of dimensionality, and late fusion ignores the complementarity and redundancy between multiple modal streams. In this paper, we first introduce the transformer-encoder with a self-attention mechanism and propose a Convolutional Neural Network-Transformer Encoder (CNN-TE) framework to model the temporal dependency for single modal affect recognition. Further, to effectively consider the complementarity and redundancy between multiple streams we propose a Transformer Encoder with Multi-modal Multi-head Attention (TEMMA) for multi-modal affect recognition. TEMMA allows to progressively and simultaneously refine the inter-modality interactions and intra-modality temporal dependency. The learned multi-modal representations are fed to an Inference Sub-network with fully connected layers to estimate the affective state. The proposed framework is trained in a nutshell and demonstrates its effectiveness on the AVEC2016 and AVEC2019 datasets. Compared to state-of-the-art models, our approach obtains remarkable improvements on both arousal and valence in terms of concordance correlation coefficient (CCC) reaching 0.583 for arousal and 0.564 for valence on the AVEC2019 test set.},
  archive      = {J_TMM},
  author       = {Haifeng Chen and Dongmei Jiang and Hichem Sahli},
  doi          = {10.1109/TMM.2020.3037496},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4171-4183},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transformer encoder with multi-modal multi-head attention for continuous affect recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative knowledge distillation for automatic check-out.
<em>TMM</em>, <em>23</em>, 4158–4170. (<a
href="https://doi.org/10.1109/TMM.2020.3037502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Check-Out (ACO) provides an object detection based mechanism for retailers to process the purchases of customers automatically. However, it suffers a lot from the domain shift problem because of different data distribution between the single item in training exemplar images and mixed items in testing checkout images. In this paper, we propose a new iterative knowledge distillation method to solve the domain adaptation problem for this task. First, we develop a new augmentation data strategy to generate synthesized checkout images. It can extract segmented items from the training images by the coarse-to-fine strategy and filter items with unrealistic poses by pose pruning. Second, we propose a dual pyramid scale network (DPSNet) to exploit the multi-scale feature representation in joint detection and counting views. Third, the iterative knowledge distillation training strategy is developed to make full use of both image-level and instance-level samples to narrow the semantic gap between source domain and target domain. Extensive experiments on the large-scale Retail Product Checkout (RPC) dataset show the proposed DPSNet can achieve state-of-the-art performance compared with existing methods. The source codes can be found at https://isrc.iscas.ac.cn/gitlab/research/dpsnet .},
  archive      = {J_TMM},
  author       = {Libo Zhang and Dawei Du and Congcong Li and Yanjun Wu and Tiejian Luo},
  doi          = {10.1109/TMM.2020.3037502},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4158-4170},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Iterative knowledge distillation for automatic check-out},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling fashion influence from photos. <em>TMM</em>,
<em>23</em>, 4143–4157. (<a
href="https://doi.org/10.1109/TMM.2020.3037459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of clothing styles and their migration across the world is intriguing, yet difficult to describe quantitatively. We propose to discover and quantify fashion influences from catalog and social media photos. We explore fashion influence along two channels: geolocation and fashion brands. We introduce an approach that detects which of these entities influence which other entities in terms of propagating their styles. We then leverage the discovered influence patterns to inform a novel forecasting model that predicts the future popularity of any given style within any given city or brand. To demonstrate our idea, we leverage public large-scale datasets of 7.7M Instagram photos from 44 major world cities (where styles are worn with variable frequency) as well as 41K Amazon product photos (where styles are purchased with variable frequency). Our model learns directly from the image data how styles move between locations and how certain brands affect each other&#39;s designs in a predictable way. The discovered influence relationships reveal how both cities and brands exert and receive fashion influence for an array of visual styles inferred from the images. Furthermore, the proposed forecasting model achieves state-of-the-art results for challenging style forecasting tasks. Our results indicate the advantage of grounding visual style evolution both spatially and temporally, and for the first time, they quantify the propagation of inter-brand and inter-city influences. Project page: https://www.cs.utexas.edu/~ziad/influence_from_photos.html},
  archive      = {J_TMM},
  author       = {Ziad Al-Halah and Kristen Grauman},
  doi          = {10.1109/TMM.2020.3037459},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4143-4157},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling fashion influence from photos},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CAA-net: Conditional atrous CNNs with attention for
explainable device-robust acoustic scene classification. <em>TMM</em>,
<em>23</em>, 4131–4142. (<a
href="https://doi.org/10.1109/TMM.2020.3037534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic Scene Classification (ASC) aims to classify the environment in which the audio signals are recorded. Recently, Convolutional Neural Networks (CNNs) have been successfully applied to ASC. However, the data distributions of the audio signals recorded with multiple devices are different. There has been little research on the training of robust neural networks on acoustic scene datasets recorded with multiple devices, and on explaining the operation of the internal layers of the neural networks. In this article, we focus on training and explaining device-robust CNNs on multi-device acoustic scene data. We propose conditional atrous CNNs with attention for multi-device ASC. Our proposed system contains an ASC branch and a device classification branch, both modelled by CNNs. We visualise and analyse the intermediate layers of the atrous CNNs. A time-frequency attention mechanism is employed to analyse the contribution of each time-frequency bin of the feature maps in the CNNs. On the Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 ASC dataset, recorded with three devices, our proposed model performs significantly better than CNNs trained on single-device data.},
  archive      = {J_TMM},
  author       = {Zhao Ren and Qiuqiang Kong and Jing Han and Mark D. Plumbley and Björn W. Schuller},
  doi          = {10.1109/TMM.2020.3037534},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4131-4142},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CAA-net: Conditional atrous CNNs with attention for explainable device-robust acoustic scene classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Acoustic room modelling using 360 stereo cameras.
<em>TMM</em>, <em>23</em>, 4117–4130. (<a
href="https://doi.org/10.1109/TMM.2020.3037537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a pipeline for estimating acoustic 3D room structure with geometry and attribute prediction using spherical 360 $^{\circ }$ cameras. Instead of setting microphone arrays with loudspeakers to measure acoustic parameters for specific rooms, a simple and practical single-shot capture of the scene using a stereo pair of 360 cameras can be used to simulate those acoustic parameters. We assume that the room and objects can be represented as cuboids aligned to the main axes of the room coordinate (Manhattan world). The scene is captured as a stereo pair using off-the-shelf consumer spherical 360 cameras. A cuboid-based 3D room geometry model is estimated by correspondence matching between captured images and semantic labelling using a convolutional neural network (SegNet). The estimated geometry is used to produce frequency-dependent acoustic predictions of the scene. This is, to our knowledge, the first attempt in the literature to use visual geometry estimation and object classification algorithms to predict acoustic properties. Results are compared to measurements through calculated reverberant spatial audio object parameters used for reverberation reproduction customized to the given loudspeaker set up.},
  archive      = {J_TMM},
  author       = {Hansung Kim and Luca Remaggi and Sam Fowler and Philip JB Jackson and Adrian Hilton},
  doi          = {10.1109/TMM.2020.3037537},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4117-4130},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Acoustic room modelling using 360 stereo cameras},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-encoder towards effective anomaly detection in videos.
<em>TMM</em>, <em>23</em>, 4106–4116. (<a
href="https://doi.org/10.1109/TMM.2020.3037538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given normal training samples, anomaly detection in videos can be regarded as a challenging problem of identifying unexpected events. The state-of-the-art approaches generally resort to the autoencoder model by using a single encoder to capture the motion and content patterns jointly. Nevertheless, due to the lack of accurate labels of normal and abnormal samples, how to detect anomalies is decided by the subjective understanding of models. It infers that different models will prefer to mine different patterns according to the characteristics of models. We call this problem as a pattern bias problem. To alleviate this problem, a novel Multi-Encoder Single-Decoder network, termed as MESDnet, is proposed in the spirit of encoding motion and content cues individually with multiple encoders. MESDnet is of end-to-end learning ability and real-time running speed. Particularly, the differences between adjacent frames and the raw frames are used as the motion and content sources, respectively. Then, a decoder takes charge of detecting anomalies in the way of observing reconstructing error towards the video frames by using the multi-stream encoded motion and content features simultaneously. The experiments on the CUHK Avenue dataset, the UCSD Pedestrian dataset, and the ShanghaiTech Campus dataset verify the effectiveness of MESDnet.},
  archive      = {J_TMM},
  author       = {Zhiwen Fang and Joey Tianyi Zhou and Yang Xiao and Yanan Li and Feng Yang},
  doi          = {10.1109/TMM.2020.3037538},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4106-4116},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-encoder towards effective anomaly detection in videos},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TBEFN: A two-branch exposure-fusion network for low-light
image enhancement. <em>TMM</em>, <em>23</em>, 4093–4105. (<a
href="https://doi.org/10.1109/TMM.2020.3037526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images obtained under low-light conditions are usually accompanied by varied and highly unpredictable degradation. The uncertainty of the imaging environment makes the enhancement even more challenging. In this paper, we present a two-branch exposure-fusion network to tackle the problem of blind low-light image enhancement. In the first part of the paper, we provide a basic insight into the degradation mechanism of low-light images, and propose a quick and effective enhancement strategy by estimating the transfer function for varied illumination levels. To further deal with the challenge brought about by the blindness of input images, a novel generation-and-fusion strategy is then introduced, where the enhancements for slightly and heavily distorted images are carried out respectively in the two enhancing branches, followed by a self-adaptive attention unit to perform the final fusion. Moreover, a two-stage denoising strategy is also proposed to ensure effective noise reduction in a data-driven manner. To evaluate the performance of the proposed method, three commonly used datasets are adopted for quantitative evaluation and six for visual evaluation, where our method outperforms many of the existing state-of-the-art ones, showing great effectiveness and potential.},
  archive      = {J_TMM},
  author       = {Kun Lu and Lihong Zhang},
  doi          = {10.1109/TMM.2020.3037526},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4093-4105},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TBEFN: A two-branch exposure-fusion network for low-light image enhancement},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive partial multi-view hashing for efficient social
image retrieval. <em>TMM</em>, <em>23</em>, 4079–4092. (<a
href="https://doi.org/10.1109/TMM.2020.3037456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks allow users to actively upload images and descriptive tags, which has led to an explosive growth in the number of social images. Multi-view hashing is an efficient technique for supporting large-scale social image retrieval because of its desirable capabilities of encoding multi-view features into compact binary hash codes with extremely low storage costs and fast retrieval speeds. However, existing methods require multi-view features to be fully paired at both the offline model training and online query stages. This requirement cannot be easily satisfied for social image retrieval, where social images that lack descriptive tags are common in social networks. In this paper, we propose an Unsupervised Adaptive Partial Multi-view Hashing (UAPMH) method to handle the partial-view hashing problem for efficient social image retrieval. Specifically, the shared and view-specific latent representations of fully paired and partial-view images, respectively, are learned separately by an adaptive partial multi-view matrix factorization module within the identical semantic space. In particular, instead of adopting simple fixed view combination weights, we develop a parameter-free weight learning scheme to adaptively learn the weights to capture the view variations and the discriminative capabilities of different views. With such a design, our model can sufficiently exploit the available partial-view samples with separate hash code learning and effectively preserve the latent relations of images and tags in hash codes with semantic space sharing. Moreover, to avoid relaxing errors and improve the learning efficiency, binary hash codes are directly learned in a fast mode with simple and efficient operations. Finally, we extend UAPMH to the supervised learning paradigm as Supervised Adaptive Partial Multi-view Hashing (SAPMH) with the supervision of pair-wise semantic labels to further enhance the discriminative capability of hash codes. The experiments demonstrate the state-of-the-art performance of the proposed approaches on public social image retrieval datasets. Our source codes and testing datasets can be obtained at https://github.com/ChaoqunZheng/APMH .},
  archive      = {J_TMM},
  author       = {Chaoqun Zheng and Lei Zhu and Zhiyong Cheng and Jingjing Li and An-An Liu},
  doi          = {10.1109/TMM.2020.3037456},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4079-4092},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive partial multi-view hashing for efficient social image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel-level non-local image smoothing with objective
evaluation. <em>TMM</em>, <em>23</em>, 4065–4078. (<a
href="https://doi.org/10.1109/TMM.2020.3037535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, imagesmoothing has gained increasing attention due to its prerequisite role in other image processing tasks, e.g., image enhancement and editing. However, the evaluation of image smoothing algorithms is usually performed by subjective observation on images without corresponding ground truths. To promote the development of image smoothing algorithms, in this paper, we construct a novel Nankai Smoothing (NKS) dataset containing 200 images blended by versatile structure images and natural textures. The structure images are inherently smooth and naturally taken as ground truths. On our NKS dataset, we comprehensively evaluate 14 popular image smoothing algorithms. Moreover, we propose a Pixel-level Non-Local Smoothing (PNLS) method to well preserve the structure of the smoothed images, by exploiting the pixel-level non-local self-similarity prior of natural images. Extensive experiments on several benchmark datasets demonstrate that our PNLS outperforms previous algorithms on the image smoothing task. Ablation studies also reveal the work mechanism of our PNLS on image smoothing. To further show its effectiveness, we apply our PNLS on several applications such as semantic region smoothing, detail/edge enhancement, and image abstraction. The dataset and code are available at https://github.com/zal0302/PNLS .},
  archive      = {J_TMM},
  author       = {Jun Xu and Zhi-Ang Liu and Ying-Kun Hou and Xian-Tong Zhen and Ling Shao and Ming-Ming Cheng},
  doi          = {10.1109/TMM.2020.3037535},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4065-4078},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pixel-level non-local image smoothing with objective evaluation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud rendering after coding: Impacts on subjective
and objective quality. <em>TMM</em>, <em>23</em>, 4049–4064. (<a
href="https://doi.org/10.1109/TMM.2020.3037481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, point clouds have shown to be a promising way to represent 3D visual data for a wide range of immersive applications, from augmented reality to autonomous cars. Emerging imaging sensors have made easier to perform richer and denser point cloud acquisition, notably with millions of points, thus raising the need for efficient point cloud coding solutions. In such scenario, it is important to evaluate the impact and performance of several processing steps in a point cloud communication system, notably the degradations associated to point cloud coding solutions. Moreover, since point clouds are not directly visualized but rather processed with a rendering algorithm before shown on any display, the perceived quality of point cloud data highly depends on the rendering solution. In this context, the main objective of this paper is to study the impact of several coding and rendering solutions on the perceived user quality and in the performance of available objective assessment metrics. Another contribution regards the assessment of recent MPEG point cloud coding solutions for several popular rendering methods, which was never presented before. The conclusions regard the visibility of three types of coding artifacts for the three considered rendering approaches as well as the strengths and weaknesses of objective metrics when point clouds are rendered after coding.},
  archive      = {J_TMM},
  author       = {Alireza Javaheri and Catarina Brites and Fernando Pereira and João Ascenso},
  doi          = {10.1109/TMM.2020.3037481},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4049-4064},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Point cloud rendering after coding: Impacts on subjective and objective quality},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MaD-DLS: Mean and deviation of deep and local similarity for
image quality assessment. <em>TMM</em>, <em>23</em>, 4037–4048. (<a
href="https://doi.org/10.1109/TMM.2020.3037482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When human visual system (HVS) looks at a scene, it extracts various features from the image about the scene to understand it. The extracted features are compared with the stored memory on the analogous scene to judge their similarity [1] . By analyzing to the similarity, HVS understands the scene presented on eyes. Based on the neurobiological basis, we propose a 2D full reference (FR) image quality assessment (IQA) method, named mean and deviation of deep and local similarity (MaD-DLS) that compares similarity between many original and distorted deep feature maps from convolutional neural networks (CNNs). MaD-DLS uses a deep learning algorithm, but since it uses the convolutional layers of a pre-trained model, it is free from training. For pooling of local quality scores within a deep similarity map, we employ two important descriptive statistics, (weighted) mean and standard deviation and name it mean and deviation (MaD) pooling. The two statistics each have the physical meaning: the weighted mean reflects effect of visual saliency on quality, whereas the standard deviation reflects effect of distortion distribution within the image on it. Experimental results show that MaD-DLS is superior or competitive to the existing methods and the MaD pooling is effective. The MATLAB source code of MaD-DLS will be available online soon.},
  archive      = {J_TMM},
  author       = {Kyohoon Sim and Jiachen Yang and Wen Lu and Xinbo Gao},
  doi          = {10.1109/TMM.2020.3037482},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4037-4048},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MaD-DLS: Mean and deviation of deep and local similarity for image quality assessment},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Story-driven video editing. <em>TMM</em>, <em>23</em>,
4027–4036. (<a href="https://doi.org/10.1109/TMM.2020.3037461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel multimedia task: story-driven video editing. Given a story paragraph, this task aims to retrieve related video segments from a gallery of collected video segments and compose them into a video sequence by the storyline order. Our proposed baseline solution consists of three modules: a retrieval module, which returns lists of candidate segments for all query sentences in the story paragraph using an object-aware sentence-segment matching method; a sequence candidate proposal module, which aggregates the retrieved segment sets into a sequence proposal by the submodular optimization method; a sorting module, which arranges the candidates according to the storyline of the paragraph using the Sinkhorn network. We build a benchmark for this task including a reorganized version of the ActivityNet Captions dataset, a well-defined quantitative metric called Evaluation of Segment-to-Sequence Matching (ESSM) for measuring the difference between the generated video segment sequence and the ground truth. Quantitative results of the proposed baseline solution are reported. We hope this new task and benchmark will bring broad research attention and push forward a lot of novel online short video editing applications.},
  archive      = {J_TMM},
  author       = {Zheng Wang and Jianguo Li and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2020.3037461},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4027-4036},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Story-driven video editing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image-text multimodal emotion classification via multi-view
attentional network. <em>TMM</em>, <em>23</em>, 4014–4026. (<a
href="https://doi.org/10.1109/TMM.2020.3035277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with single-modal content, multimodal data can express users’ feelings and sentiments more vividly and interestingly. Therefore, multimodal sentiment analysis has become a popular research topic. However, most existing methods either learn modal sentiment feature independently, without considering their correlations, or they simply integrate multimodal features. In addition, most publicly available multimodal datasets are labeled by sentiment polarities, while the emotions expressed by users are specific. Based on this observation, in this paper, we build a large-scale image-text emotion dataset (i.e., labeled by different emotions), called TumEmo, with more than 190,000 instances from Tumblr. 1 We further propose a novel multimodal emotion analysis model based on the Multi-view Attentional Network (MVAN), which utilizes a memory network that is continually updated to obtain the deep semantic features of image-text. The model includes three stages: feature mapping, interactive learning, and feature fusion. In the feature mapping stage, we leverage image features from an object viewpoint and a scene viewpoint to capture effective information for multimodal emotion analysis. Then, an interactive learning mechanism is adopted that uses the memory network; this mechanism extracts single-modal emotion features and interactively models the cross-view dependencies between the image and text. In the feature fusion stage, multiple features are deeply fused using a multilayer perceptron and a stacking-pooling module. The experimental results on the MVSA-Single, MVSA-Multiple, and TumEmo datasets show that the proposed MVAN outperforms strong baseline models by large margins.},
  archive      = {J_TMM},
  author       = {Xiaocui Yang and Shi Feng and Daling Wang and Yifei Zhang},
  doi          = {10.1109/TMM.2020.3035277},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {4014-4026},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image-text multimodal emotion classification via multi-view attentional network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emotion knowledge driven video highlight detection.
<em>TMM</em>, <em>23</em>, 3999–4013. (<a
href="https://doi.org/10.1109/TMM.2020.3035285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses video highlight detection which aims to select a small subset of frames according to user&#39;s major or special interest. The performances of conventional methods highly depend on large-scale manually labeled training data which are time-consuming and labor-intensive to collect. To deal with this problem, we trace back to the original problem definition and find that whether a user is interested in a specific video segment heavily depends on human&#39;s subjective emotions. Leveraging this insight, we introduce an emotion knowledge driven video detection framework for modeling human&#39;s general emotion and inferencing highlight strength. Firstly, we obtain the concept-level representation of the video clip with a front-end network. The concepts are used as nodes to build an emotion-related knowledge graph, and their relationships in the graph are modeled via external public knowledge graphs. Then we adopt Siamese GCNs to model the dependencies between nodes in the graph and propagate messages along the edges. Finally, we compute the emotion-aware representation of the video clip based on the GCN layers and further use it to predict the highlight score. Our framework, including the front-end network, graph convolution layers and the highlight mapping network, can be trained in an end-to-end manner with the constraint of a ranking loss. Experiments on two benchmark datasets show that our proposed method performs favorably against the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Fan Qi and Xiaoshan Yang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3035285},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3999-4013},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Emotion knowledge driven video highlight detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic motion estimation and evolution video prediction
network. <em>TMM</em>, <em>23</em>, 3986–3998. (<a
href="https://doi.org/10.1109/TMM.2020.3035281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future video prediction provides valuable information that helps a computer machine understand the surrounding environment and make critical decisions in real-time. However, long-term video prediction remains a challenging problem due to the complicated spatiotemporal dynamics in a video. In this paper, we propose a dynamic motion estimation and evolution (DMEE) network model to generate unseen future videos from the observed videos in the past. Our primary contribution is to use trained kernels in convolutional neural network (CNN) and long short-term memory (LSTM) architectures, adapted to each time step and sample position, to efficiently manage spatiotemporal dynamics. DMEE uses the motion estimation (ME) and motion update (MU) kernels to predict the future video using an end-to-end prediction-update process. In the prediction, the ME kernel estimates the temporal changes. In the update step, the MU kernel combines the estimates with the previously generated frames as reference frames using a weighted average. The kernels are not only used for a current frame, but also are evolved to generate successive frames to enable temporally specific filtering. We perform qualitative performance analysis and quantitative performance analysis based on the peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and video classification score developed for examining the visual quality of the generated video. It is demonstrated with experiments that our algorithm provides better qualitative and quantitative performance superior to the current state-of-the-art algorithms. Our source codes are available in https://github.com/Nayoung-Kim-ICP/Video-Generation .},
  archive      = {J_TMM},
  author       = {Nayoung Kim and Je-Won Kang},
  doi          = {10.1109/TMM.2020.3035281},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3986-3998},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic motion estimation and evolution video prediction network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Environmental sound classification using local binary
pattern and audio features collaboration. <em>TMM</em>, <em>23</em>,
3978–3985. (<a href="https://doi.org/10.1109/TMM.2020.3035275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach to classify environmental sounds using a texture feature local binary pattern (LBP) and audio features collaboration. To our knowledge, this is the first time that the LBP (or its variants), which has a proven track record in the field of image recognition and classification, has been generalized for 1D and combined with audio features for an environmental sound classification task. To this end, we have generalized and defined LBP-1D and local phase quantization (LPQ)-1D on the 1-dimensional (1D) audio signal and have applied the original LBP, the variance LBP (VARLBP) and the extended LBP (ELBP) thus generated to the spectrogram of the audio signal in order to model the sound texture. We have also extensively compared these new LBP-based features to the classical audio descriptors commonly used in environmental sound classification, such as MFCC, GFCC, CQT, chromagram, STE and ZCR. We have evaluated our algorithm on ESC-10 and ESC-50 datasets using classical machine learning algorithms, such as support vector machines (SVM), random forest and k-nearest neighbor (kNN). The results showed that the LBP features outperform the classical audio features. We mix the LBP features with the audio descriptors, and our best mixed model achieves state-of-the-art results for environmental sound classification: 88.5 $\%$ on ESC-10 and 64.6 $\%$ on ESC-50. Those results outperform the results of methods that used handcrafted features with classical machine learning algorithms and are similar to some convolutional neural network-based methods. Although our method is not the cutting edge of the state-of-the-art methods, it is faster than any convolutional neural network methods and represents a better choice when there is data scarcity or minimal computing power.},
  archive      = {J_TMM},
  author       = {Ohini Kafui Toffa and Max Mignotte},
  doi          = {10.1109/TMM.2020.3035275},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3978-3985},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Environmental sound classification using local binary pattern and audio features collaboration},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggregating global and local visual representation for
vehicle re-IDentification. <em>TMM</em>, <em>23</em>, 3968–3977. (<a
href="https://doi.org/10.1109/TMM.2020.3035279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Re-Identification targets at searching for vehicle instances of the same identity with a given query. It has gained an increasing attention recently with a wide application prospects in video surveillance and intelligent transportation. The main challenge lies in how to distinguish the subtle differences between different vehicles, while capturing the slight similarity between the instance of the same vehicle in different viewpoints or illuminations. To this end, most existing methods focus on learning discriminative global representations, which leave the unique local details such as stickers, inspection labels and driver wearing being unexploited. In this paper, we present a novel coarse-to-fine scheme that aggregates global and local visual representations to boost the retrieval accuracy. Specifically, a multi-task learning framework combining an Attribute Learning branch and a Deep Ranking branch (termed ALDR) is first adopted to learn robust global features, which produces an initial ranking list. Then the local similarities between image patches in the initial ranking list and in the query is computed via a Multi-Channel and Multi-Scale Siamese network (termed MCMS-Siam). Finally, the retrieval result is returned after re-ranking the initial list according to such a combination of global and local similarities. Experimental results on the widely-used VehicleID dataset and VECH-WILD dataset demonstrate the merits of the proposed method over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xianming Lin and Run Li and Xiawu Zheng and Pai Peng and Yongjian Wu and Feiyue Huang and Rongrong Ji},
  doi          = {10.1109/TMM.2020.3035279},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3968-3977},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Aggregating global and local visual representation for vehicle re-IDentification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User identity linkage across social media via attentive
time-aware user modeling. <em>TMM</em>, <em>23</em>, 3957–3967. (<a
href="https://doi.org/10.1109/TMM.2020.3034540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we work towards linking users’ identities on different social media platforms by exploring the user-generated contents (UGCs). This task is non-trivial due to the following challenges. 1) As UGCs involve multiple modalities (e.g., text and image), how to accurately characterize the user account based on their heterogeneous multi-modal UGCs poses the main challenge. 2) As people tend to post similar UGCs on different social media platforms during the same period, how to effectively model the temporal post correlation is a crucial challenge. And 3) no public benchmark dataset is available to support our user identity linkage based on heterogeneous UGCs with timestamps. Towards this end, we present an attentive time-aware user identity linkage scheme, which seamlessly integrates the temporal post correlation modeling and attentive user similarity modeling. To facilitate the evaluation, we create a comprehensive large-scale user identity linkage dataset from two popular social media platforms: Instagram and Twitter. Extensive experiments have been conducted on our dataset and the results verify the effectiveness of the proposed scheme. As a residual product, we have released the dataset, codes, and parameters to facilitate other researchers.},
  archive      = {J_TMM},
  author       = {Xiaolin Chen and Xuemeng Song and Siwei Cui and Tian Gan and Zhiyong Cheng and Liqiang Nie},
  doi          = {10.1109/TMM.2020.3034540},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3957-3967},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {User identity linkage across social media via attentive time-aware user modeling},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep hashing with weighted spatial importance. <em>TMM</em>,
<em>23</em>, 3778–3792. (<a
href="https://doi.org/10.1109/TMM.2020.3031092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing method has been widely used in big data retrieval because of its low computational complexity. Most of existing hashing methods learn the final hash code from the semantic information of the whole image. However, different spatial regions of an image have different influences during the hash learning. To tackle this issue, we propose a new deep hashing with weighted spatial importance (DWSH) in this paper. Specifically, the proposed DWSH first utilizes a spatial attention model to learn the importance of different spatial regions in the original image, and then assigns different weights to these spatial regions according to their importance. The final hash codes are learned based on the weighted spatial information. In addition, two strategies are designed to utilize the spatial importance, including discrete weight strategy and continuous weight strategy, which weight the spatial information with discrete and continuous values, respectively. The results of extensive experiments conducted on three benchmark datasets show that the proposed DWSH method is superior to the state-of-the-art hashing method based on different evaluation protocols.},
  archive      = {J_TMM},
  author       = {Yang Shi and Xiushan Nie and Meng Chen and Li Lian and Yilong Yin},
  doi          = {10.1109/TMM.2020.3031092},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3778-3792},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep hashing with weighted spatial importance},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hard pixel mining for depth privileged semantic
segmentation. <em>TMM</em>, <em>23</em>, 3738–3751. (<a
href="https://doi.org/10.1109/TMM.2020.3035231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has achieved remarkable progress but remains challenging due to the complex scene, object occlusion, and so on. Some research works have attempted to use extra information such as a depth map to help RGB based semantic segmentation because the depth map could provide complementary geometric cues. However, due to the inaccessibility of depth sensors, depth information is usually unavailable for the test images. In this paper, we leverage only the depth of training images as the privileged information to mine the hard pixels in semantic segmentation, in which depth information is only available for training images but not available for test images. Specifically, we propose a novel Loss Weight Module, which outputs a loss weight map by employing two depth-related measurements of hard pixels: Depth Prediction Error and Depth-aware Segmentation Error. The loss weight map is then applied to segmentation loss, with the goal of learning a more robust model by paying more attention to the hard pixels. Besides, we also explore a curriculum learning strategy based on the loss weight map. Meanwhile, to fully mine the hard pixels on different scales, we apply our loss weight module to multi-scale side outputs. Our hard pixels mining method achieves the state-of-the-art results on three benchmark datasets, and even outperforms the methods which need depth input during testing.},
  archive      = {J_TMM},
  author       = {Zhangxuan Gu and Li Niu and Haohua Zhao and Liqing Zhang},
  doi          = {10.1109/TMM.2020.3035231},
  journal      = {IEEE Transactions on Multimedia},
  month        = {11},
  pages        = {3738-3751},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hard pixel mining for depth privileged semantic segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank-consistency deep hashing for scalable multi-label image
search. <em>TMM</em>, <em>23</em>, 3943–3956. (<a
href="https://doi.org/10.1109/TMM.2020.3034534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As hashing becomes an increasingly appealing technique for large-scale image retrieval, multi-label hashing is also attracting more attention for the ability to exploit multi-level semantic contents. In this paper, we propose a novel deep hashing method for scalable multi-label image search. Unlike existing approaches with conventional objectives such as contrast and triplet losses, we employ a rank list, rather than pairs or triplets, to provide sufficient global supervision information for all the samples. Specifically, a new rank-consistency objective is applied to align the similarity orders from two spaces, the original space and the hamming space. A powerful loss function is designed to penalize the samples whose semantic similarity and hamming distance are mismatched in two spaces. Besides, a multi-label softmax cross-entropy loss is presented to enhance the discriminative power with a concise formulation of the derivative function. In order to manipulate the neighborhood structure of the samples with different labels, we design a multi-label clustering loss to cluster the hashing vectors of the samples with the same labels by reducing the distances between the samples and their multiple corresponding class centers. The state-of-the-art experimental results achieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and NUS-WIDE, demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Cheng Ma and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TMM.2020.3034534},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3943-3956},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rank-consistency deep hashing for scalable multi-label image search},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anisotropic graph convolutional network for semi-supervised
learning. <em>TMM</em>, <em>23</em>, 3931–3942. (<a
href="https://doi.org/10.1109/TMM.2020.3034530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks learn effective node embeddings that have proven to be useful in achieving high-accuracy prediction results in semi-supervised learning tasks, such as node classification. However, these networks suffer from the issue of over-smoothing and shrinking effect of the graph due in large part to the fact that they diffuse features across the edges of the graph using a linear Laplacian flow. This limitation is especially problematic for the task of node classification, where the goal is to predict the label associated with a graph node. To address this issue, we propose an anisotropic graph convolutional network for semi-supervised node classification by introducing a nonlinear function that captures informative features from nodes, while preventing oversmoothing. The proposed framework is largely motivated by the good performance of anisotropic diffusion in image and geometry processing, and learns nonlinear representations based on local graph structure and node features. The effectiveness of our approach is demonstrated on three citation networks and two image datasets, achieving better or comparable classification accuracy results compared to the standard baseline methods.},
  archive      = {J_TMM},
  author       = {Mahsa Mesgaran and A. Ben Hamza},
  doi          = {10.1109/TMM.2020.3034530},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3931-3942},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Anisotropic graph convolutional network for semi-supervised learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain-oriented semantic embedding for zero-shot learning.
<em>TMM</em>, <em>23</em>, 3919–3930. (<a
href="https://doi.org/10.1109/TMM.2020.3033124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Learning (ZSL) targets to recognize images from new classes. Existing methods focus on learning a projection function to associate the visual features and category descriptions in the seen domain, which is directly transferred to the unseen domain. However, due to the inherent domain shift, a single shared projection cannot fully capture the domain difference and similarity, thereby making the unseen samples tend to be recognized as seen categories. In this paper, we propose a novel Domain-Oriented Semantic Embedding (DOSE) network that learns specific projections for different domains to better capture the domain characteristics for unbiased ZSL. Besides a domain-shared projection, DOSE learns two auxiliary domain-specific sub-projections to model the semantic-visual association in respective seen and unseen domains. Specifically, the domain-specific projections are learned in a cycle consistency way to capture domain characteristics, and a domain division constraint is developed to penalize the margin between two domain embeddings. Furthermore, to boost semantic-visual association, a semantic-visual dual attention module is designed to automatically remove trivial information in both visual and semantic embeddings under a co-guidance learning manner. Experiments on four public benchmarks prove that the proposed DOSE is robust to the domain shift problem in ZSL and obtains an averaged 5.6% improvement in terms of harmonic mean.},
  archive      = {J_TMM},
  author       = {Shaobo Min and Hantao Yao and Hongtao Xie and Zheng-Jun Zha and Yongdong Zhang},
  doi          = {10.1109/TMM.2020.3033124},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3919-3930},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Domain-oriented semantic embedding for zero-shot learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to hash with dimension analysis based quantizer for
image retrieval. <em>TMM</em>, <em>23</em>, 3907–3918. (<a
href="https://doi.org/10.1109/TMM.2020.3033118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last few years have witnessed the rise of the big data era, in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results demonstrate that hashing can achieve promising performance due to its appealing storage and search efficiency. Since the complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash codes learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by Hamming distance is very efficient. However, the huge information loss produced by the quantization step should be reduced in applications, such as image retrieval where high search accuracy is required. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis based quantization method (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than the others, then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.},
  archive      = {J_TMM},
  author       = {Yuan Cao and Heng Qi and Jie Gui and Keqiu Li and Yuan Yan Tang and James Tin-Yau Kwok},
  doi          = {10.1109/TMM.2020.3033118},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3907-3918},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to hash with dimension analysis based quantizer for image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical group-level emotion recognition. <em>TMM</em>,
<em>23</em>, 3892–3906. (<a
href="https://doi.org/10.1109/TMM.2020.3033125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group-level emotion recognition is a technique for estimating the emotion of a group of people. In this paper, we propose a novel method for group-level emotion recognition. Our method lies in the two-fold contributions: (1) recognition of group-level emotion using a hierarchical classification approach; (2) incorporation of novel features to contribute to the description of the group-level emotion. We consider that the use of facial expressions of people will only be effective in differentiating images labeled as “Positive” because those labeled as “Neutral” or “Negative” are likely to include similar facial expressions. Therefore, we first perform binary classification based on facial expression recognition to distinguish “Positive” labels that include discriminative facial expressions (e.g., smile) from the others. We evaluate outcomes that are not classified as “Positive” during the first classification by exploiting scene features that describe what type of events (e.g., demonstration or funeral) are shown in the image. The other novelty of our method lies in two-fold. The first is the exploitation of visual attention for the first classification. It allows us to estimate which faces are the main subjects in the target image, thereby suppressing the influences of faces in the background that contribute less to group-level emotion. The second is the exploitation of object-wise semantic information (labels) for the second classification. This allows a more detailed description of the scene context in the image and enables performance enhancement in the second classification. We demonstrate the effectiveness of our method through experiments using public datasets.},
  archive      = {J_TMM},
  author       = {Katsuya Fujii and Daisuke Sugimura and Takayuki Hamamoto},
  doi          = {10.1109/TMM.2020.3033125},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3892-3906},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical group-level emotion recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the perceptual quality of point cloud: A 3D-to-2D
projection-based exploration. <em>TMM</em>, <em>23</em>, 3877–3891. (<a
href="https://doi.org/10.1109/TMM.2020.3033117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is emerged as a promising media format to represent realistic 3D objects or scenes in applications, such as virtual reality, teleportation, etc. How to accurately quantify the subjective point cloud quality for application-driven optimization, however, is still a challenging and open problem. In this paper, we attempt to tackle this problem in a systematic means. First, we produce a fairly large point cloud dataset where ten popular point clouds are augmented with seven types of impairments (e.g., compression, photometry/color noise, geometry noise, scaling) at six different distortion levels, and organize a formal subjective assessment with tens of subjects to collect mean opinion scores (MOS) for all 420 processed point cloud samples (PPCS). We then try to develop an objective metric that can accurately estimate the subjective quality. Towards this goal, we choose to project the 3D point cloud onto six perpendicular image planes of a cube for the color texture image and corresponding depth image, and aggregate image-based global (e.g., Jensen-Shannon (JS) divergence) and local features (e.g., edge, depth, pixel-wise similarity, complexity) among all projected planes for a final objective index. Model parameters are fixed constants after performing the regression using a small and independent dataset previously published. The proposed metric has demonstrated the state-of-the-art performance for predicting the subjective point cloud quality compared with multiple full-reference and no-reference models, e.g., the weighted peak signal-to-noise ratio (PSNR), structural similarity (SSIM), feature similarity (FSIM) and natural image quality evaluator (NIQE). The dataset is made publicly accessible at http://smt.sjtu.edu.cn or http://vision.nju.edu.cn for all interested audiences.},
  archive      = {J_TMM},
  author       = {Qi Yang and Hao Chen and Zhan Ma and Yiling Xu and Rongjun Tang and Jun Sun},
  doi          = {10.1109/TMM.2020.3033117},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3877-3891},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Predicting the perceptual quality of point cloud: A 3D-to-2D projection-based exploration},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). String prediction for 4:2:0 format screen content coding and
its implementation in AVS3. <em>TMM</em>, <em>23</em>, 3867–3876. (<a
href="https://doi.org/10.1109/TMM.2020.3033092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past, string prediction (also known as string matching) was applied only to RGB and YUV 4:4:4 format screen content coding. This paper proposes a string prediction approach to 4:2:0 format screen content coding implemented in the third generation of Audio Video Standard (AVS3) in China. String prediction is applied to both YUV CU and Y CU. To further improve the coding performance, several improved technicals of string prediction are presented, including a mixed string searching strategy for finding the optimal reference string, a joint picture-level, CU-level, and pixel-level early termination strategy to reduce coding complexity, and two effective coding methods for string prediction parameters. For low-complexity hardware implementation of string prediction decoder, the memory access bandwidth is reduced by introducing string constraints. Meanwhile, string prediction reuses the reference pixel buffer of intra block copy (IBC). Compared with the newest AVS3 reference software HPM7.0 with string prediction disabled, the proposed string prediction approach achieves up to 18.48% Y BD-rate reduction. Using AVS3 Screen Content Coding (SCC) Common Test Condition and YUV test sequences in Text and Graphics with Motion category, the proposed technique achieves an average Y BD-rate reduction of 10.33%, 8.47%, 6.91% for All Intra (AI), Random Access (RA) and Low Delay (LD) configurations, respectively, with low additional encoding and decoding complexity. The proposed string prediction approach has been adopted in the newest AVS3 reference software HPM7.0.},
  archive      = {J_TMM},
  author       = {Qingyang Zhou and Liping Zhao and Kailun Zhou and Tao Lin and Huihui Wang and Shuhui Wang and Mengcao Jiao},
  doi          = {10.1109/TMM.2020.3033092},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3867-3876},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {String prediction for 4:2:0 format screen content coding and its implementation in AVS3},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparkle: User-aware viewport prediction in 360-degree video
streaming. <em>TMM</em>, <em>23</em>, 3853–3866. (<a
href="https://doi.org/10.1109/TMM.2020.3033127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 360-degree video streaming, users commonly watch a video scene within a Field of View (FoV) . Such observation provides an opportunity to save bandwidth consumption by predicting and then prefetching video tiles within the FoV. However, existing FoV prediction methods seldom consider the diversity among user behaviors and the impact of different video genres. Thus, previous one-size-fits-all models cannot make accurate prediction for users with different behavior patterns. In this paper, we propose a user-aware viewport prediction algorithm called Sparkle , which is a practical whitebox approach for FoV prediction. Instead of training a single learning model to predict the behaviors for all users, our proposed algorithm is tailored to fit each individual user. In particular, unlike other learning models, our prediction model is completely explainable and all the parameters have their physical meanings. We first conduct a measurement study to analyze real user behaviors and observe that there exists sharp fluctuation of view orientation and user posture has significant impact on the viewport movement of users. Moreover, cross-user similarity is diverse across different video genres. Inspired by these insights, we further design a user-aware viewport prediction algorithm by mimicking a user&#39;s viewport movement on the tile map, and determine how a user will change the viewport angle based on his (or her) trajectory and other similar users’ behaviors in the past time window. Extensive evaluations with real datasets demonstrate that, our proposed algorithm significantly outperforms the state-of-the-art benchmark methods (e.g., LSTM-based methods) by over $\text{5}\%$ , and the prediction accuracy is much more stable on various types of 360-degree videos than previous methods.},
  archive      = {J_TMM},
  author       = {Jinyu Chen and Xianzhuo Luo and Miao Hu and Di Wu and Yipeng Zhou},
  doi          = {10.1109/TMM.2020.3033127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3853-3866},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sparkle: User-aware viewport prediction in 360-degree video streaming},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive multi-feature reliability re-determinative
correlation filter for visual tracking. <em>TMM</em>, <em>23</em>,
3841–3852. (<a href="https://doi.org/10.1109/TMM.2020.3032043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model drift is a challenging issue for visual object tracking. Most available approaches aim to address this issue by constructing of a stronger discriminative prediction model with the integration of multiple features. In this article, we also address the issue of model drift using multiple features, but we consider the fusion problem from a different point of view, namely, strengthening the features that are suitable for the current scenario while weakening the remaining ones. Therefore, an advanced tracker that adaptively redetermines the reliability for each feature during the tracking process is proposed. Furthermore, to correctly evaluate and redetermine these reliabilities, two different solutions, called model evaluation and numerical optimization, are proposed, and two independent trackers corresponding to these two solutions are implemented. Extensive experiments have been designed on five large datasets to validate the following: 1) The proposed tracking framework is superior for making the tracking model more robust, and 2) the two solutions proposed for redetermining the reliability for each feature are effective. As expected, the two implemented trackers do indeed improve the accuracy and robustness compared to state-of-the-art trackers. Especially on VOT2016, the proposed trackers based on model evaluation and numerical optimization achieve outstanding EAO scores (i.e., 0.453 and 0.428 , respectively), outperforming the recently developed top trackers by a large margin.},
  archive      = {J_TMM},
  author       = {Mingyang Guan and Changyun Wen},
  doi          = {10.1109/TMM.2020.3032043},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3841-3852},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive multi-feature reliability re-determinative correlation filter for visual tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Kernelized multiview subspace analysis by self-weighted
learning. <em>TMM</em>, <em>23</em>, 3828–3840. (<a
href="https://doi.org/10.1109/TMM.2020.3032023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of multimedia technology, information is always represented from multiple views. Even though multiview data can reflect the same sample from different perspectives, multiple views are consistent to some extent because they are representations of the same sample. Most of the existing algorithms are graph-based ones to learn the complex structures within multiview data but overlook the information within data representations. Furthermore, many existing works treat multiple views discriminatively by introducing some hyperparameters, which is undesirable in practice. To this end, abundant multiview-based methods have been proposed for dimension reduction. However, there is still no research that leverages the existing work into a unified framework. In this paper, we propose a general framework for multiview data dimension reduction, named kernelized multiview subspace analysis (KMSA) to handle multiview feature representation in the kernel space, providing a feasible channel for multiview data with different dimensions. Compared with the graph-based methods, KMSA can fully exploit information from multiview data with nothing to lose. Since different views have different influences on KMSA, we propose a self-weighted strategy to treat different views discriminatively. A co-regularized term is proposed to promote the mutual learning from multiviews. KMSA combines self-weighted learning with the co-regularized term to learn the appropriate weights for all views. We evaluate our proposed framework on 6 multiview datasets for classification and image retrieval. The experimental results validate the advantages of our proposed method.},
  archive      = {J_TMM},
  author       = {Huibing Wang and Yang Wang and Zhao Zhang and Xianping Fu and Li Zhuo and Mingliang Xu and Meng Wang},
  doi          = {10.1109/TMM.2020.3032023},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3828-3840},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Kernelized multiview subspace analysis by self-weighted learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wide color gamut image content characterization: Method,
evaluation, and applications. <em>TMM</em>, <em>23</em>, 3817–3827. (<a
href="https://doi.org/10.1109/TMM.2020.3032026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework to characterize a wide color gamut image content based on perceived quality due to the processes that change color gamut, and demonstrate two practical use cases where the framework can be applied. We first introduce the main framework and implementation details. Then, we provide analysis for understanding of existing wide color gamut datasets with quantitative characterization criteria on their characteristics, where four criteria, i.e., coverage, total coverage, uniformity, and total uniformity, are proposed. Finally, the framework is applied to content selection in a gamut mapping evaluation scenario in order to enhance reliability and robustness of the evaluation results. As a result, the framework fulfils content characterization for studies where quality of experience of wide color gamut stimuli is involved.},
  archive      = {J_TMM},
  author       = {Junghyuk Lee and Toinon Vigier and Patrick Le Callet and Jong-Seok Lee},
  doi          = {10.1109/TMM.2020.3032026},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3817-3827},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Wide color gamut image content characterization: Method, evaluation, and applications},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Controlling P2P-CDN live streaming services at SDN-enabled
multi-access edge datacenters. <em>TMM</em>, <em>23</em>, 3805–3816. (<a
href="https://doi.org/10.1109/TMM.2020.3032042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing the shortcomings of current hybrid peer-to-peer (P2P) content-distribution network (CDN) video solutions and the potential of emerging multi-access edge datacenters, we propose a novel P2P-CDN service model that is hosted at software defined networks (SDN)-enabled multi-access edge datacenters operated by network service providers (NSP). An important feature of the proposed service architecture is that both CDN access by peers and P2P video streaming between peers within edge access networks are fully controlled by cooperation of the video content provider (VCP) and NSP to optimize video service key performance indicators (KPI). The proposed fully controlled P2P-CDN architecture with P2P group formation and chunk scheduling managed at edge datacenters reduces the load on CDN servers while overcoming quality of experience (QoE) fluctuations per flow and unfairness between multiple heterogeneous video-resolution clients over reserved access network slices. Other advantages of this service include: i) better video quality and lower delay for clients; ii) better use of edge network resources; iii) avoiding illegal, unauthorized P2P content sharing. To the best of our knowledge, there are no solutions in the literature that address P2P-CDN services managed at NSP-edge datacenters combining P2P-assisted CDN, SDN-assisted edge computing, and premium service over reserved slices. Experimental results show that the proposed P2P-CDN service deployed at SDN-enabled edge datacenters provides excellent service KPI compared to other state-of-the-art solutions.},
  archive      = {J_TMM},
  author       = {Selin Nacakli and A. Murat Tekalp},
  doi          = {10.1109/TMM.2020.3032042},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3805-3816},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Controlling P2P-CDN live streaming services at SDN-enabled multi-access edge datacenters},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). C-GCN: Correlation based graph convolutional network for
audio-video emotion recognition. <em>TMM</em>, <em>23</em>, 3793–3804.
(<a href="https://doi.org/10.1109/TMM.2020.3032037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of both hardware and deep neural network technologies, tremendous improvements have been achieved in the performance of automatic emotion recognition (AER) based on the video data. However, AER is still a challenging task due to subtle expression, abstract concept of emotion and the representation of multi-modal information. Most proposed approaches focus on the multi-modal feature learning and fusion strategy, which pay more attention to the characteristic of a single video and ignore the correlation among the videos. To explore this correlation, in this paper, we propose a novel correlation-based graph convolutional network (C-GCN) for AER, which can comprehensively consider the correlation of the intra-class and inter-class videos for feature learning and information fusion. More specifically, we introduce the graph model to represent the correlation among the videos. This correlated information can help to improve the discrimination of node features in the progress of graph convolutional network. Meanwhile, the multi-head attention mechanism is applied to predict the hidden relationship among the videos, which can strengthen the inter-class correlation to improve the performance of classifiers. The C-GCN is evaluated on the AFEW datasets and eNTERFACE 05 dataset. The final experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Weizhi Nie and Minjie Ren and Jie Nie and Sicheng Zhao},
  doi          = {10.1109/TMM.2020.3032037},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3793-3804},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {C-GCN: Correlation based graph convolutional network for audio-video emotion recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new approach for character recognition of multi-style
vehicle license plates. <em>TMM</em>, <em>23</em>, 3768–3777. (<a
href="https://doi.org/10.1109/TMM.2020.3031074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of vehicle license plate is an important part of the modern intelligent traffic management system, which has been widely used in many fields. On the Hong Kong-Zhuhai-Macao Bridge, the vehicles may have multiple license plates (LPs) with three different styles, and the traditional contour-based vehicle license plate recognition methods cause a considerable miss rate for multi-style license plates. With such a background, this paper proposes a multi-style license plate recognition method based on feature pyramid network with instance segmentation, which translates the license plate recognition into object instance detection and gets rid of the steps of segmentation and optical character recognition of traditional methods. In the scheme, we design a novel license plate recognition network to precisely locate and classify characters and LP regions concurrently, wherein an assembly layer is added for combining the characters into license plates and outputting license plate strings. The experimental results show that the proposed method achieves 98.57% recognition rate of multi-style LPs on the real world applications. Moreover, we also select the standard license plate datasets, that only contain single style license plates, to test the proposed license plate recognition method, and the corresponding results show that the proposed method achieves competitive performance.},
  archive      = {J_TMM},
  author       = {Qiuying Huang and Zhanchuan Cai and Ting Lan},
  doi          = {10.1109/TMM.2020.3031074},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3768-3777},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A new approach for character recognition of multi-style vehicle license plates},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-cost anti-copying 2D barcode by exploiting channel noise
characteristics. <em>TMM</em>, <em>23</em>, 3752–3767. (<a
href="https://doi.org/10.1109/TMM.2020.3031083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, to overcome the drawbacks of prior approaches for defending against Illegally-Copying (IC) attacks, such as low generality, high cost, and high overhead, we propose a Low-Cost Anti-Copying (LCAC) 2D barcode by exploiting the difference between the noise characteristics of legal and illegal channels. An embedding strategy is proposed, and for a variant of this strategy, we also perform a corresponding analysis. To accurately evaluate the performance of our approach, a theoretical model of the noise in an illegal channel is established by using a generalized Gaussian distribution. By comparing the experimental results based on various printers, scanners, and mobile phones, it can be found that the sample histogram and fitting curve of the theoretical model match well, so it can be concluded that the theoretical model works well. To evaluate the security of the proposed LCAC code, in addition to the Direct-Copying (DC) attack, an improved version, called the Synthesized-Copying (SC) attack, is also considered in this paper. Based on the theoretical model, we build a prediction function to optimize the parameters of our approach. Parameters optimization seeks a tradeoff between the production cost and the cost of IC attacks. The experimental results show that the proposed LCAC code with two printers and two scanners can detect DC attacks effectively and resist SC attacks up to the access of 14 legal copies.},
  archive      = {J_TMM},
  author       = {Ning Xie and Qiqi Zhang and Yicong Chen and Ji Hu and Gang Luo and Changsheng Chen},
  doi          = {10.1109/TMM.2020.3031083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3752-3767},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low-cost anti-copying 2D barcode by exploiting channel noise characteristics},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). One-shot texture retrieval using global grouping metric.
<em>TMM</em>, <em>23</em>, 3726–3737. (<a
href="https://doi.org/10.1109/TMM.2020.3031062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture retrieval is widely used in the fields of fashion and e-commerce. This paper presents the problem of one-shot texture retrieval: given an example of a new reference texture, we aim to detect and segment all pixels of the same texture category within an arbitrary image. To address this problem, an OS-TR network is proposed to encode both reference and query images into a texture representation space, and a better comparison is made based on the global grouping information. Because the learned texture representation should be invariant to the spatial layout while preserving the rough semantic concepts, we introduce an adaptive directionality-aware module to finely discriminate the orderless texture details. To make full use of the global context information given only a few examples, we incorporate a grouping-attention mechanism into the relation network, resulting in the per-channel modulation of the local relation features. Extensive experiments on two benchmark datasets (i.e., the DTD and ADE20K dataset) and real scenarios demonstrate that our proposed method can achieve above-par segmentation performance and robust generalization across domains.},
  archive      = {J_TMM},
  author       = {Kai Zhu and Yang Cao and Wei Zhai and Zheng-Jun Zha},
  doi          = {10.1109/TMM.2020.3031062},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3726-3737},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {One-shot texture retrieval using global grouping metric},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approximation algorithm to maximize user capacity for an
auto-scaling VoD system. <em>TMM</em>, <em>23</em>, 3714–3725. (<a
href="https://doi.org/10.1109/TMM.2020.3031068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a video-on-demand (VoD) service, blockbuster videos have stable and predictable popularity, but the traffic can vary significantly within short timescale. To efficiently serve the user pool in a geographic region, we consider a regional auto-scaling cloud-based data center consisting of multiple servers. For efficient storage, we partition the videos into fixed-size blocks. To respond to dynamic user traffic in a timely and cost-effective manner, we may activate or deactivate each server according to the traffic while keeping at least one replica for each block in the active servers. We maximize the user capacity of the active servers (and hence minimizing the number of active servers at any time) by jointly optimizing block allocation in the servers, server selection at each traffic level, and request dispatching to a server. We believe that this is the first work to study such problem for an auto-scaling cloud-based VoD data center. We first formulate the problem and show its NP-hardness. We then propose AVARDO ( A uto-scaling V ideo A llocation and R equest D istribution O ptimization), a simple but efficient approximation algorithm with proven optimality. AVARDO operates the servers like a stack, with a server being pushed into or popped from the existing active server set according to some optimized traffic thresholds. We prove that AVARDO approaches the theoretical optimum as the block size reduces. Trace-driven experimental results based on large-scale real-world video data further validate that AVARDO is closely optimal. It achieves significantly higher user capacity as compared with other state-of-the-art and traditional schemes, and reduces the optimality gap by multiple times.},
  archive      = {J_TMM},
  author       = {Zhangyu Chang and S.-H. Gary Chan},
  doi          = {10.1109/TMM.2020.3031068},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3714-3725},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An approximation algorithm to maximize user capacity for an auto-scaling VoD system},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative perceptual assessment of visual signals using
free energy features. <em>TMM</em>, <em>23</em>, 3700–3713. (<a
href="https://doi.org/10.1109/TMM.2020.3029891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we put forward the concept of comparative perceptual quality assessment (C-PQA), which refers to the judgment of relative qualities of two visual signals of the same content, but subject to different types and levels of distortions. While it is straightforward for human observers to fulfill the CPQA task in daily lives, it remains a difficult challenge for the current research of perceptual quality assessment (PQA). Among the existing PQA algorithms, the full-reference (FR) and reducedreference (RR) methods both need prior knowledge of the original images while the no-reference (NR) algorithms usually work with a single input image. C-PQA is inherently different from those existing methods in that it takes an image pair as input and predicts their relative quality without using any knowledge about the original image. In this paper, we propose a brain theory inspired approach to C-PQA that emulates the process of comparing the relative quality of two visual stimuli as performed by the human visual system (HVS) within the framework of free energy minimization. The brain&#39;s internal generative models initialized on the inputs are then used to explain both images. During the internal generative modeling, a group of features are extracted and then integrated to determine the relative quality of two images. We designed a dedicated image database to test the proposed C-PQA algorithm. Experimental results show that the proposed method achieves up to 98% prediction accuracy in line with the subjective ratings, outperforming many state of the art PQA algorithms.},
  archive      = {J_TMM},
  author       = {Guangtao Zhai and Yucheng Zhu and Xiongkuo Min},
  doi          = {10.1109/TMM.2020.3029891},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3700-3713},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Comparative perceptual assessment of visual signals using free energy features},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Privacy-preserving in-home fall detection using visual
shielding sensing and private information-embedding. <em>TMM</em>,
<em>23</em>, 3684–3699. (<a
href="https://doi.org/10.1109/TMM.2020.3029904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falls are the main cause of accidental injuries, and even death among elderly people, especially those who live alone in their homes. The absence of a reliable fall detection system has long been a serious problem for home health monitoring. A video surveillance system can be used to monitor elderly people at home to detect falls, but the traditional implementation of such intelligent detection falls short of personal privacy-related considerations; additionally, many people do not want to be watched in their homes. To solve this problem, we propose a fall detection system with visual shielding that can ensure the safety of elderly people in their homes while preserving their personal privacy. Multilayer compressed sensing is first used to achieve visually shielded video frames. By combining low-rank sparse decomposition theory with the improved local binary pattern on the three orthogonal planes, the object features are extracted from the shielded video frames. Finally, to compensate for the information lost in the compressed video to a certain extent, a private information-embedded classification model is proposed to identify fall-related behavior. The experimental results on two public fall datasets show that the proposed method delivers impressive accuracy and a low error rate while effectively distinguishing between fall- and nonfall-related behaviors in videos.},
  archive      = {J_TMM},
  author       = {Jixin Liu and Rong Tan and Guang Han and Ning Sun and Sam Kwong},
  doi          = {10.1109/TMM.2020.3029904},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3684-3699},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privacy-preserving in-home fall detection using visual shielding sensing and private information-embedding},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal wireless streaming of multi-quality 360 VR video by
exploiting natural, relative smoothness-enabled, and transcoding-enabled
multicast opportunities. <em>TMM</em>, <em>23</em>, 3670–3683. (<a
href="https://doi.org/10.1109/TMM.2020.3029880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we would like to investigate the optimal wireless streaming of a multi-quality tiled 360 virtual reality (VR) video from a server to multiple users. To this end, we propose to maximally exploit potential multicast opportunities by effectively utilizing characteristics of multi-quality tiled 360 VR videos and computation resources at the users&#39; side. In particular, we consider two requirements for quality variation in one field-of-view (FoV), i.e., the absolute smoothness requirement and the relative smoothness requirement, and two video playback modes, i.e., the direct-playback mode (without user transcoding) and transcode-playback mode (with user transcoding). Besides natural multicast opportunities, we introduce two new types of multicast opportunities, namely, relative smoothness-enabled multicast opportunities, which allow a flexible tradeoff between viewing quality and communications resource consumption, and transcoding-enabled multicast opportunities, which allow a flexible tradeoff between computation and communications resource consumptions. Then, we establish a novel mathematical model that reflects the impacts of natural, relative smoothness-enabled, and transcoding-enabled multicast opportunities on the average transmission energy and transcoding energy. Based on this model, we optimize the transmission resource allocation, playback quality level selection, and transmission quality level selection to minimize the energy consumption in the four cases with different requirements for quality variation and video playback modes. By comparing the optimal values in the four cases, we prove that the energy consumption reduces when more multicast opportunities can be utilized. Finally, numerical results show substantial gains of the proposed solutions over existing schemes, and demonstrate the importance of exploiting of the three types of multicast opportunities.},
  archive      = {J_TMM},
  author       = {Kaixuan Long and Ying Cui and Chencheng Ye and Zhi Liu},
  doi          = {10.1109/TMM.2020.3029880},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3670-3683},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimal wireless streaming of multi-quality 360 VR video by exploiting natural, relative smoothness-enabled, and transcoding-enabled multicast opportunities},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric shape estimation of human body under wide
clothing. <em>TMM</em>, <em>23</em>, 3657–3669. (<a
href="https://doi.org/10.1109/TMM.2020.3029941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shape of the human body plays an important role in many applications, such as those involving personal healthcare and virtual clothing try-ons. However, accurate body shape measurements typically require the user to be wearing a minimal amount of clothing, which is not practical in many situations. To resolve this issue using deep learning techniques, we need a paired dataset of ground-truth naked human body shapes and their corresponding color images with clothes. As it is practically impossible to collect enough of this kind of data from real-world environments to train a deep neural network, in this paper, we present the Synthetic dataset of Human Avatars under wiDE gaRment (SHADER). The SHADER dataset consists of 300,000 paired ground-truth naked and dressed images of 1,500 synthetic humans with different body shapes, poses, garments, skin tones, and backgrounds. To take full advantage of SHADER, we propose a novel silhouette confidence measure and show that our silhouette confidence prediction network can help improve the performance of state-of-the-art shape estimation networks for human bodies under clothing. The experimental results demonstrate the effectiveness of the proposed approach. The code and dataset are available at https://github.com/YCL92/SHADER .},
  archive      = {J_TMM},
  author       = {Yucheng Lu and Jin-Hyuck Cha and Se-Kyoung Youm and Seung-Won Jung},
  doi          = {10.1109/TMM.2020.3029941},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3657-3669},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Parametric shape estimation of human body under wide clothing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative image relevance learning for visual
re-ranking. <em>TMM</em>, <em>23</em>, 3646–3656. (<a
href="https://doi.org/10.1109/TMM.2020.3029886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In content-based image retrieval, the initial retrieval result may be unsatisfactory, which can be refined with visual re-ranking techniques, such as query expansion, geometric verification, etc . In this work, we approach visual re-ranking from a novel perspective. Observing that the contextual similarity of images from a retrieval result list exhibits strong visual relevance, we propose to collaboratively learn the semantic relevance among images for visual re-ranking. In our approach, we represent the image set of a fixed-length retrieval list into a correlation matrix, and learn the relevance of all image pairs simultaneously with a lightweight CNN model. To optimize the CNN model, a weighted MSE loss is defined, which takes into account the sparsity of labels. To find the optimal length of retrieval result list for different queries, we present a query sensitive selection method. We conduct comprehensive experiments on five benchmark datasets, and demonstrate the generality, and effectiveness of the proposed visual re-ranking method.},
  archive      = {J_TMM},
  author       = {Jianbo Ouyang and Wengang Zhou and Min Wang and Qi Tian and Houqiang Li},
  doi          = {10.1109/TMM.2020.3029886},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3646-3656},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Collaborative image relevance learning for visual re-ranking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CoLEAP: Cooperative learning-based edge scheme with caching
and prefetching for DASH video delivery. <em>TMM</em>, <em>23</em>,
3631–3645. (<a href="https://doi.org/10.1109/TMM.2020.3029893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outstanding increase in video traffic, puts increasing pressure on network transmission. Since the Dynamic Adaptive Streaming over HTTP (DASH) adjusts the delivery to the dynamic network conditions, it has emerged as a popular approach for video transmissions. However, bitrate switching and video rebuffering may still occur and influence negatively quality of experience (QoE). Additionally the popular videos are transmitted multiple times, which leads to high bandwidth consumption, despite large transmission redundancy. In this context, we propose a Cooperative Learning-based scheme for the smart Edge servers with cAching and Prefetching (CoLEAP) to improve the QoE of adaptive video streaming. CoLEAP employs edge servers which cache the most beneficial contents to reduce redundant video transmissions and prefetches content to decrease network transmission delay. Considering user-related information and the state of network, CoLEAP intelligently makes the most advantageous decisions of caching and prefetching by employing a novel QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we test the proposed solution in comprehensive simulated scenarios and against four alternative solutions. When compared with the existing schemes, CoLEAP increases average bitrate by up to 181.8%, reduces video rebuffering by up to 70.8% as well as decreases response time by up to 28.0%. These values result in minimum improvements of 57.4% and 29.0%, respectively in terms of cache hit rate and QoE.},
  archive      = {J_TMM},
  author       = {Wanxin Shi and Chao Wang and Yong Jiang and Qing Li and Gengbiao Shen and Gabriel-Miro Muntean},
  doi          = {10.1109/TMM.2020.3029893},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3631-3645},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CoLEAP: Cooperative learning-based edge scheme with caching and prefetching for DASH video delivery},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning the relation between interested objects and
aesthetic region for image cropping. <em>TMM</em>, <em>23</em>,
3618–3630. (<a href="https://doi.org/10.1109/TMM.2020.3029882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the fundamental techniques for image editing, image cropping discards irrelevant contents and remains the pleasing portions of the image to enhance the overall composition and achieve better visual/aesthetic perception. In this paper, we primarily focus on improving the efficiency of automatic image cropping, and on further exploring its potential in public datasets with high accuracy. From this perspective, we propose a deep learning based framework to learn the objects composition from photos with high aesthetic qualities, where an interested object region is detected through a convolutional neural network (CNN) based on the saliency map. The features of the detected interested objects are then fed into a regression network to obtain the final cropping result. Unlike the conventional methods that multiple candidates are proposed and evaluated iteratively, only a single interested object region is produced in our model, which is mapped to the final output directly. Thus, low computational resources are required for the proposed approach. The experimental results on the public datasets show that as a weakly supervised method, the proposed network outperforms the other weakly supervised methods on FLMS and FCD datasets and achieves comparable results to the existing methods on CUHK dataset. Furthermore, the proposed method is more efficient than these methods, where the processing speed is as fast as 20 ms per image.},
  archive      = {J_TMM},
  author       = {Peng Lu and Hao Zhang and Xujun Peng and Xiaofu Jin},
  doi          = {10.1109/TMM.2020.3029882},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3618-3630},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning the relation between interested objects and aesthetic region for image cropping},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wildfish++: A comprehensive fish benchmark for multimedia
research. <em>TMM</em>, <em>23</em>, 3603–3617. (<a
href="https://doi.org/10.1109/TMM.2020.3028482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a large-scale vision-language fish benchmark, namely WildFish++, for comprehensive studies in multimedia research. Concretely, WildFish++ consists of 2,348 fish categories with 103,034 images in the wild, and 3,817 fish descriptions with 213,858 words. Based on these distinct characteristics, we mainly introduce four challenging research tasks on WildFish++. (1) Fine-Grained Recognition with Comparison Texts . WildFish++ naturally contains subtle difference among fish categories, which leads to fine-grained classification. Most approaches resort to tackle this problem by capturing discriminative regions in the view of each image. However, this paradigm may be still far way from extracting the most distinct features when the context on visual difference is not available. In this case, we propose to introduce comparison fish descriptions, a unique corpus that can directly point out subtle difference between highly-confused species and naturally serve as a kind of valuable context information. With such texts, we creatively elaborate a multi-modal fish network, aiming at incorporating those comparison textual information as prior knowledge and consequently leveraging it to guide CNNs to find subtle yet distinct regions in the context of comparison texts. (2) Open-Set Classification . We often confront with unknown categories in practice, e.g., there may still exist unknown fishes in our planet. Hence, we creatively adapt WildFish++ for a novel open-set classification task, which aims at correctly assigning each test image into the unknown class or one of known classes. More importantly, we investigate a number of practical designs to boost accuracy of deep learning models in open-set scenarios. (3) Cross-Modal Retrieval . WildFish++ not only contains diversified fish images in the wild but also has rich fish descriptions about morphology diagnosis, biology information, etc. Hence, we design a challenging cross-modal retrieval task, which leverages three subtasks such as text-to-text, text-to-image, image-to-text retrieval in a unified end-to-end framework. (4) Automatic Fish Classification . Automatic fish classification is a long-term research in marine biology, while current studies are unsatisfactory due to the lack of large-scale data. In this case, we train a number of CNNs with WildFish++, and use its pre-trained models to boost fish classification on most existing benchmarks of wild fishes. We will release WildFish++ with codes/protocols ( https://github.com/PeiqinZhuang/WildFish++ ). We believe it can promote relevant studies in multimedia and beyond.},
  archive      = {J_TMM},
  author       = {Peiqin Zhuang and Yali Wang and Yu Qiao},
  doi          = {10.1109/TMM.2020.3028482},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3603-3617},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Wildfish++: A comprehensive fish benchmark for multimedia research},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep semantic parsing of freehand sketches with homogeneous
transformation, soft-weighted loss, and staged learning. <em>TMM</em>,
<em>23</em>, 3590–3602. (<a
href="https://doi.org/10.1109/TMM.2020.3028466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel deep framework for part-level semantic parsing of freehand sketches, which makes three main contributions that are experimentally shown to have substantial practical merit. First, we propose a homogeneous transformation method to address the problem of domain adaptation. For the task of sketch parsing, there is no available data of labeled freehand sketches that can be directly used for model training. An alternative solution is to learn from datasets of real image parsing, while the domain adaptation is an inevitable problem. Unlike existing methods that utilize the edge maps of real images to approximate freehand sketches, the proposed homogeneous transformation method transforms the data from domains of real images and freehand sketches into a homogeneous space to minimize the semantic gap. Second, we design a soft-weighted loss function as guidance for the training process, which gives attention to both the ambiguous label boundary and class imbalance. Third, we present a staged learning strategy to improve the parsing performance of the trained model, which takes advantage of the shared information and specific characteristic from different sketch categories. Extensive experimental results demonstrate the effectiveness of the above three methods. Specifically, to evaluate the generalization ability of our homogeneous transformation method, additional experiments for the task of sketch-based image retrieval are conducted on the QMUL FG-SBIR dataset. Finally, by integrating the proposed three methods into a unified framework of deep semantic sketch parsing (DeepSSP), we achieve the state-of-the-art on the public SketchParse dataset.},
  archive      = {J_TMM},
  author       = {Ying Zheng and Hongxun Yao and Xiaoshuai Sun},
  doi          = {10.1109/TMM.2020.3028466},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3590-3602},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep semantic parsing of freehand sketches with homogeneous transformation, soft-weighted loss, and staged learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new multihypothesis-based compressed video sensing
reconstruction system. <em>TMM</em>, <em>23</em>, 3577–3589. (<a
href="https://doi.org/10.1109/TMM.2020.3028479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multihypothesis-based compressed video sensing scheme attracts wide attention in the research of resource-constrained video application scenarios. However, high-accuracy weight prediction of hypotheses is always challenging especially for the high-motion sequences. To solve this problem, this paper proposes a novel multihypothesis-based distributed compressed video sensing (NMH-DCVS) system. The new multihypothesis system contains two components: hypotheses acquisition, and weight prediction. First, to acquire more high-quality hypotheses, a new hypotheses acquisition scheme is proposed by constructing the search window based on the temporal, and spatial correlation of the image blocks, respectively. The optimal matching block can be quickly determined. Second, to improve the accuracy of the multihypothesis weight prediction, a new residual transforming preprocessing-based weight prediction algorithm is proposed by transforming the original hypothesis set to residual hypothesis set. The influence of the quality fluctuation of the hypotheses on prediction accuracy is effectively suppressed. Moreover, the improved hypotheses further improve the sparsity of the residual hypothesis set, leading to the additional improvement of the accuracy of the proposed residual-based weight prediction algorithm. Experiment results show that compared with the state-of-the-art methods reported in the literature, the proposed new multihypothesis system significantly improves the decoding performance both in objective, and subjective quality.},
  archive      = {J_TMM},
  author       = {Shuai Zheng and Jian Chen and Xiao-Ping Zhang and Yonghong Kuo},
  doi          = {10.1109/TMM.2020.3028479},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3577-3589},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A new multihypothesis-based compressed video sensing reconstruction system},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning spatial-temporal representations over walking
tracklet for long-term person re-identification in the wild.
<em>TMM</em>, <em>23</em>, 3562–3576. (<a
href="https://doi.org/10.1109/TMM.2020.3028461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term person re-identification (re-ID) aims to build identity correspondence of the Target Subject of Interest (TSI) exposed under surveillance cameras over a long time interval. Compared to the conventional short-term re-ID studied by most existing works, it suffers an additional problem: significant dressing change observed with time lapsing. Unfortunately, this variation in long-term person re-ID case contradicts the assumption of prior short-term re-ID approaches, and thus causes significant difficulties if conventional short-term re-ID methods are applied. To address the problem, this paper proposes to learn hybrid feature representation via a two-stream network named SpTSkM, including a spatial-temporal stream and a skeleton motion stream. The former performs directly on image sequences, which tends to learn identity-related spatial-temporal patterns such as body geometric structure and body movement. The latter operates on normalized 3D skeletons by adapting graph convolutional network, which tends to learn pure motion patterns from skeleton sequences. Both streams extract fine-grained level time-gap stable information that is robust to appearance changes in long-term re-ID and meanwhile maintains sufficient discriminability to differentiate different people. The final matching metric is obtained by mixing information of the two streams in a score-level fusion strategy. In addition, we collect a Cloth-Varying vIDeo re-ID (CVID-reID) dataset particularly for long-term re-ID. It contains video tracklets of celebrities posted on the Internet. These videos are snapshots under extremely different scenarios that include highly dynamic background, diverse camera views and abundant cloth variations on each TSI. These factors cause CVID-reID more complicated and closer to practice. Our experiments demonstrate the difficulty of long-term person re-ID and also validate the effectiveness of the proposed SpTSkM, showing the best performance.},
  archive      = {J_TMM},
  author       = {Peng Zhang and Jingsong Xu and Qiang Wu and Yan Huang and Xianye Ben},
  doi          = {10.1109/TMM.2020.3028461},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3562-3576},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning spatial-temporal representations over walking tracklet for long-term person re-identification in the wild},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MVANet: Multi-task guided multi-view attention network for
chinese food recognition. <em>TMM</em>, <em>23</em>, 3551–3561. (<a
href="https://doi.org/10.1109/TMM.2020.3028478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food recognition plays a much critical role in various health-care applications. However, it poses many challenges to current approaches due to the diverse appearances of food dishes and the non-uniform composition of ingredients for the foods in the same category. Current methods primarily focus on the appearance of foods without considering their semantic information, easily finding the wrong attention areas of food images. Second, these methods lack the dynamic weighting of multiple semantic features in the modeling process. Thus this paper proposes a novel Multi-View Attention Network within the multi-task learning framework that incorporates multiple semantic features into the food recognition task from both ingredient recognition and recipe modeling. It also utilizes the multi-view attention mechanism to automatically adjust the weights of different semantic features and enables different tasks to interact with each other so as to obtain a more comprehensive feature representation. The experiments conducted on both ChineseFoodNet and VIREO Food-172 benchmark databases validate the proposed method with the obvious improvement of the performance and the lower parameter size.},
  archive      = {J_TMM},
  author       = {Haozan Liang and Guihua Wen and Yang Hu and Mingnan Luo and Pei Yang and Yingxue Xu},
  doi          = {10.1109/TMM.2020.3028478},
  journal      = {IEEE Transactions on Multimedia},
  month        = {10},
  pages        = {3551-3561},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MVANet: Multi-task guided multi-view attention network for chinese food recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular 3D facial expression features for continuous
affect recognition. <em>TMM</em>, <em>23</em>, 3540–3550. (<a
href="https://doi.org/10.1109/TMM.2020.3026894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated facial expression analysis from image sequences for continuous emotion recognition is a very challenging task due to the loss of the three-dimensional information during the image formation process. State-of-the-art relied on estimating dynamic textures features and convolutional neural network features to derive spatio-temporal features. Despite their great success, such features are insensitive to micro facial muscle deformations and are affected by identity, face pose, illumination variation, and self-occlusion. In this work, we argue that retrieving, from image sequences, 3D facial spatio-temporal information, which describes the natural facial muscle deformation, provides a semantical and efficient way of representation and is useful for emotion recognition. In this paper, we propose a framework for extracting three-dimensional facial spatio-temporal features from monocular image sequences using an extended 3D Morphable Model (3DMM) which disentangles the identity factor from the facial expressions of a specific person. An LSTM model is used to evaluate the effectiveness of the proposed spatio-temporal features on video-based facial expression recognition task and continuous affect recognition task. Experimental results, on the AFEW6.0 datasets for facial expression recognition, and the RECOLA and SEMAINE datasets for continuous emotion prediction, illustrate the potential of the proposed 3D spatio-temporal features for facial expressions analysis and continuous affect recognition, as well as their efficiency compared to recent state-of-the-art features.},
  archive      = {J_TMM},
  author       = {Ercheng Pei and Meshia Cédric Oveneke and Yong Zhao and Dongmei Jiang and Hichem Sahli},
  doi          = {10.1109/TMM.2020.3026894},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3540-3550},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Monocular 3D facial expression features for continuous affect recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised deep TripleNet for video object
segmentation. <em>TMM</em>, <em>23</em>, 3530–3539. (<a
href="https://doi.org/10.1109/TMM.2020.3026913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of previous video object segmentation methods require a large amount of pixel-level annotated video data to construct a robust model. It is quite expensive to label segmentation mask in video. In this article, we propose a self-supervised triplenet for video object segmentation, which only leverages nearly unlimited unlabeled video data in training phase. Our method consists of two modules, i.e., the temporal motion module and the appearance matching module. The temporal motion module is trained based on the pixel correspondence between two video frames in a self-supervised manner, which models the motion patterns between two video frames and propagates the labels from one frame to another. Meanwhile, the appearance matching module encodes the reference frame and its corresponding mask to generate the segmentation mask of the same object in target frame. The appearance matching module can adjust and refine the output results of temporal motion module, and avoid error accumulation by matching the reference appearance. In order to train the appearance matching module in self-supervised manner, we propose two mask generation strategies: foreground region mask generation and random color region mask generation. Extensive experiments conducted on four challenging video object segmentation datasets, i.e., DAVIS-2017, Youtube-VOS, DAVIS-2016 and SegTrack v2, demonstrate that the proposed method performs favorable against the state-of-the-art self-supervised methods, and performs even competitively with fully-supervised methods. We also show our self-supervised approach has actually superior generalizability to the majority of supervised methods.},
  archive      = {J_TMM},
  author       = {Kai Xu and Longyin Wen and Guorong Li and Qingming Huang},
  doi          = {10.1109/TMM.2020.3026913},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3530-3539},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised deep TripleNet for video object segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual question answering with dense inter- and
intra-modality interactions. <em>TMM</em>, <em>23</em>, 3518–3529. (<a
href="https://doi.org/10.1109/TMM.2020.3026892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning effective interactions between multi-modal features is at the heart of visual question answering (VQA). A common defect of the existing VQA approaches is that they only consider a very limited amount of inter-modality interactions, which may be not enough to model latent complex image-question relations that are necessary for accurately answering questions. Besides, most methods neglect the modeling of the intra-modality interactions that is also important to VQA. In this work, we propose a novel DenIII framework for modeling dense inter-, and intra-modality interactions. It densely connects all pairwise layers of the network via the proposed Inter-, and Intra-modality Attention Connectors, capturing fine-grained interplay across all hierarchical levels. The Inter-modality Attention Connector efficiently connects the multi-modality features at any two layers with bidirectional attention, capturing the inter-modality interactions. While the Intra-modality Attention Connector connects the features of the same modality with unidirectional attention, and models the intra-modality interactions. Extensive ablation studies, and visualizations validate the effectiveness of our method, and DenIII achieves state-of-the-art or competitive performance on three publicly available datasets.},
  archive      = {J_TMM},
  author       = {Fei Liu and Jing Liu and Zhiwei Fang and Richang Hong and Hanqing Lu},
  doi          = {10.1109/TMM.2020.3026892},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3518-3529},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual question answering with dense inter- and intra-modality interactions},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A serial image copy-move forgery localization scheme with
source/target distinguishment. <em>TMM</em>, <em>23</em>, 3506–3517. (<a
href="https://doi.org/10.1109/TMM.2020.3026868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we improve the parallel deep neural network (DNN) scheme BusterNet for image copy-move forgery localization with source/target region distinguishment. BusterNet is based on two branches, i.e., Simi-Det and Mani-Det, and suffers from two main drawbacks: (a) it should ensure that both branches correctly locate regions; (b) the Simi-Det branch only extracts single-level and low-resolution features using VGG16 with four pooling layers. To ensure the identification of the source and target regions, we introduce two subnetworks that are constructed serially: the copy-move similarity detection network (CMSDNet) and the source/target region distinguishment network (STRDNet). Regarding the second drawback, the CMSDNet subnetwork improves Simi-Det by removing the last pooling layer in VGG16 and by introducing atrous convolution into VGG16 to preserve field-of-views of filters after the removal of the fourth pooling layer; double-level self-correlation is also considered for matching hierarchical features. Moreover, atrous spatial pyramid pooling and attention mechanism allow the capture of multiscale features and provide evidence for important information. Finally, STRDNet is designed to determine the similar regions obtained from CMSDNet directly as tampered regions and untampered regions. It determines regions at the image-level rather than at the pixel-level as made by Mani-Det of BusterNet. Experimental results on four publicly available datasets (new synthetic dataset, CASIA, CoMoFoD, and COVERAGE) demonstrate that the proposed algorithm is superior to the state-of-the-art algorithms in terms of similarity detection ability and source/target distinguishment ability.},
  archive      = {J_TMM},
  author       = {Beijing Chen and Weijin Tan and Gouenou Coatrieux and Yuhui Zheng and Yun-Qing Shi},
  doi          = {10.1109/TMM.2020.3026868},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3506-3517},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A serial image copy-move forgery localization scheme with Source/Target distinguishment},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving student learning satisfaction by using an
innovative DASH-based multiple sensorial media delivery solution.
<em>TMM</em>, <em>23</em>, 3494–3505. (<a
href="https://doi.org/10.1109/TMM.2020.3025669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, innovative technologies such as Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), and Multi-Sensorial Media (mulsemedia) have introduced new sensorial effects including vibration, smell, airflow, etc. to human life. These effects which have been largely deployed for entertainment, and gaming have positively impacted user satisfaction. This paper explores the potential of mulsemedia in the education context. It describes a novel Dynamic Adaptive Streaming over HTTP (DASH)-based Multi-sensory Media Delivery Solution (DASHMS) which supports adaptive mulsemedia content distribution based on the operational environment which includes network, device, and user settings.DASHMS was evaluated in a real-life educational experiment involving 44 students in an Irish university. The evaluation focused on both learner satisfaction, and the impact on learning. The results demonstrate the potential of adaptive multi-sensorial media delivery to result in a statistically significant increase in user experience. In terms of benefit to learning outcomes however, it was only memory recall which was statistically improved in the experiment.},
  archive      = {J_TMM},
  author       = {Ting Bi and Roisin Lyons and Grace Fox and Gabriel-Miro Muntean},
  doi          = {10.1109/TMM.2020.3025669},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3494-3505},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving student learning satisfaction by using an innovative DASH-based multiple sensorial media delivery solution},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-view subspace clustering with unified and
discriminative learning. <em>TMM</em>, <em>23</em>, 3483–3493. (<a
href="https://doi.org/10.1109/TMM.2020.3025666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep multi-view subspace clustering has achieved promising performance compared with other multi-view clustering. However, existing deep multi-view subspace clustering only considers the global structure for all views, and they ignore the local geometric structure among each view. In addition, they cannot learn discriminative feature on different clusters of different views, i.e., inter-cluster difference. To solve these problems, in this paper, we propose a novel Deep Multi-view Subspace Clustering with Unified and Discriminative Learning (DMSC-UDL). DMSC-UDL combines global and local structures with self-expression layer. The global and local structures help each other forward and achieve small distance between samples of the same cluster. To make samples in different clusters of different views farther, DMSC-UDL uses a discriminative constraint between different views. In this way, DMSC-UDL makes the same cluster&#39;s samples have large weights, while different clusters’ samples have small weights. Thus, it can learn a better shared connection matrix for multi-view clustering. Extensive experimental results reveal that the proposed multi-view clustering method is superior to several state-of-the-art multi-view clustering methods in terms of performance.},
  archive      = {J_TMM},
  author       = {Qianqian Wang and Jiafeng Cheng and Quanxue Gao and Guoshuai Zhao and Licheng Jiao},
  doi          = {10.1109/TMM.2020.3025666},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3483-3493},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep multi-view subspace clustering with unified and discriminative learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind 3D-printing watermarking using moment alignment and
surface norm distribution. <em>TMM</em>, <em>23</em>, 3467–3482. (<a
href="https://doi.org/10.1109/TMM.2020.3025660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent development of 3D printing technology has brought concerns about its potential misuse, such as in copyright infringement, and crimes. Although there have been many studies on blind 3D mesh watermarking for the copyright protection of digital objects, methods applicable to 3D printed objects are rare. In this paper, we propose a novel blind watermarking algorithm for 3D printed objects with applications for copyright protection, traitor tracing, object identification, and crime investigation. Our method allows us to embed a few bits of data into a 3D-printed object, and retrieve it by 3D scanning without requiring any information about the original mesh. The payload is embedded on the object&#39;s surface by slightly modifying the distribution of surface norms, that is, the distance between the surface, and the center of gravity. It is robust to resampling and can work with any 3D printer, and scanner technology. In addition, our method increases the capacity, and resistance by subdividing the mesh into a set of bins, and spreading the data over the entire surface to negate the effect of local printing artifacts. The method&#39;s novelties include extending the vertex norm histogram to a continuous surface, and the use of 3D moments to synchronize a watermark signal in a 3D-printing context. In the experiments, our method was evaluated using a public dataset against center, orientation, minimum, and maximum norm misalignments; a printing simulation; and actual print/scan experiments using a standard 3D printer, and scanner.},
  archive      = {J_TMM},
  author       = {Arnaud Delmotte and Kenichiro Tanaka and Hiroyuki Kubo and Takuya Funatomi and Yasuhiro Mukaigawa},
  doi          = {10.1109/TMM.2020.3025660},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3467-3482},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind 3D-printing watermarking using moment alignment and surface norm distribution},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explore video clip order with self-supervised and curriculum
learning for video applications. <em>TMM</em>, <em>23</em>, 3454–3466.
(<a href="https://doi.org/10.1109/TMM.2020.3025661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a self-supervised spatiotemporal learning approach by exploring the temporal coherence of videos. The chronological order of shuffled clips from the video is used as the supervisory signal to guide the 3D Convolutional Neural Networks (CNNs) to learn meaningful visual knowledge. Unlike the existing approaches which use frames, we utilize dynamic video clips to reduce the uncertainty of order. We test three types of representative 3D CNNs, all of which benefit from the proposed approach. The learned 3D CNNs can be used either as a feature extractor or a pre-trained model for further fine-tuning on downstream tasks. We also propose two curriculum learning strategies to make the 3D CNNs easier to train and get the state-of-the-art results in nearest neighbor retrieval and action recognition tasks compared with other self-supervised learning methods. Meanwhile, it is further extended to the field of visual question answering application and has achieved promising results. Besides, comprehensive and extensive experimental results and analyses are provided for readers to better understand the video clip order we explore with self-supervised and curriculum learning for video application.},
  archive      = {J_TMM},
  author       = {Jun Xiao and Lin Li and Dejing Xu and Chengjiang Long and Jian Shao and Shifeng Zhang and Shiliang Pu and Yueting Zhuang},
  doi          = {10.1109/TMM.2020.3025661},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3454-3466},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Explore video clip order with self-supervised and curriculum learning for video applications},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal-to-specific framework for complex action
recognition. <em>TMM</em>, <em>23</em>, 3441–3453. (<a
href="https://doi.org/10.1109/TMM.2020.3025665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based action recognition has recently attracted much attention in the field of computer vision. To solve more complex recognition tasks, it has become necessary to distinguish different levels of interclass variations. Inspired by a common flowchart based on the human decision-making process that first narrows down the probable classes, and then applies a “rethinking” process for finer-level recognition, we propose an effective universal-to-specific (U2S) framework for complex action recognition. The U2S framework is composed of three subnetworks: a universal network, a category-specific network, and a mask network. The universal network first learns universal feature representations. The mask network then generates attention masks for confusing classes through category regularization based on the output of the universal network. The mask is further used to guide the category-specific network for class-specific feature representations. The entire framework is optimized in an end-to-end manner. Experiments on a variety of benchmark datasets, e.g., the Something-Something, UCF101, and HMDB51 datasets, demonstrate the effectiveness of the U2S framework; i.e., U2S can focus on discriminative spatiotemporal regions for confusing categories. We further visualize the relationship between different classes, showing that U2S indeed improves the discriminability of learned features. Moreover, the proposed U2S model is a general framework, and may adopt any base recognition network.},
  archive      = {J_TMM},
  author       = {Peisen Zhao and Lingxi Xie and Ya Zhang and Qi Tian},
  doi          = {10.1109/TMM.2020.3025665},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3441-3453},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Universal-to-specific framework for complex action recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LCSegNet: An efficient semantic segmentation network for
large-scale complex chinese character recognition. <em>TMM</em>,
<em>23</em>, 3427–3440. (<a
href="https://doi.org/10.1109/TMM.2020.3025696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex scene character recognition is a challenging yet important task in machine learning, especially for languages with large character sets, such as Chinese, which is composed of hieroglyphics with large-scale categories and similar glyphs. Recently, state-of-the-art methods based on semantic segmentation have achieved great success in scene parsing and have been applied in scene text recognition. However, because of limitations in terms of memory and computation, they are only applied in the small category recognition tasks, such as tasks involving English alphabets and digits. In this paper, we propose an efficient semantic segmentation model based on label coding (LC), called LCSegNet, to recognize large-scale Chinese characters. First, to reduce the number of labels, we design a new label coding method based on the Wubi Chinese characters code, called Wubi-CRF. In this method, glyphs and structure information of Chinese characters are encoded into 140-bit labels. Second, we employ an efficient semantic segmentation model for pixel-wise prediction and utilize a conditional random field (CRF) module to learn the constraint rules of Wubi-like coding. Finally, experiments are conducted on three benchmarks: a large Chinese text dataset in the wild (CTW), ICDAR2019-ReCTS, and HIT-OR3C dataset. Results show that the proposed method achieves state-of-the-art performances in both complex scene and handwritten character recognition tasks.},
  archive      = {J_TMM},
  author       = {Xiangping Wu and Qingcai Chen and Yulun Xiao and Wei Li and Xin Liu and Baotian Hu},
  doi          = {10.1109/TMM.2020.3025696},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3427-3440},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LCSegNet: An efficient semantic segmentation network for large-scale complex chinese character recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speech personality recognition based on annotation
classification using log-likelihood distance and extraction of essential
audio features. <em>TMM</em>, <em>23</em>, 3414–3426. (<a
href="https://doi.org/10.1109/TMM.2020.3025108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech personality recognition relies on training models that require an excessive number of features and are, in most cases, designed specifically for certain databases. As a result, when tested on different datasets, overfitted classifier models are not always reliable because their accuracy changes with changes in the domain of speakers. Moreover, personality annotations are often subjective, which creates variability in raters perception during labeling. These problems inhibit the effectiveness of speech personality recognition applications. To reduce the unexplained variance caused by unknown differences in raters perception, a structure that uses Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm is proposed. Furthermore, a feature extraction method is proposed to filter out undesirable adulterations be it noise, silence, or uncertain pitch segments, while extracting essential audio features, i.e., signal power roll-off, pitch, and pause rate. Experiments on the standard SSPNet dataset records a relative 4% increase in overall accuracy when log-likelihood based annotations are used. Moreover, improved consistency in accuracy is observed when this method is tested on male and female subsets.},
  archive      = {J_TMM},
  author       = {Zhen-Tao Liu and Abdul Rehman and Min Wu and Wei-Hua Cao and Man Hao},
  doi          = {10.1109/TMM.2020.3025108},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3414-3426},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Speech personality recognition based on annotation classification using log-likelihood distance and extraction of essential audio features},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep unsupervised self-evolutionary hashing for image
retrieval. <em>TMM</em>, <em>23</em>, 3400–3413. (<a
href="https://doi.org/10.1109/TMM.2020.3025000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods have proven to be effective in the field of large-scale image retrieval. In recent years, the performance of hashing algorithms based on deep learning has greatly exceeded that of non-deep methods. However, most of the outstanding hashing methods are supervised models that heavily rely on annotated labels. In order to circumvent the huge overhead of labeling large-scale datasets, some unsupervised hashing algorithms have been proposed, such as pseudo labels and pseudo pairs. Since the image labels are strictly unavailable, some hyper-parameters in these methods are difficult to be selected, e.g., the final result is very sensitive to the picked number of categories or the chosen threshold of similarity for pairs. In addition, the calculation of pseudo-labels in high-dimensional space is not only computationally complex, but also has low precision. Therefore, in order to alleviate these issues in this paper, we propose a simple but effective Deep Unsupervised Self-evolutionary Hashing (DUSH) algorithm, which utilizes a curriculum learning strategy to iteratively select pseudo pairs from easy to hard in low dimensional Hamming space. Extensive experiments are conducted on four popular datasets, including two single-label datasets and two multi-label datasets, and the results show that our method can significantly outperform the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Haofeng Zhang and Yifan Gu and Yazhou Yao and Zheng Zhang and Li Liu and Jian Zhang and Ling Shao},
  doi          = {10.1109/TMM.2020.3025000},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3400-3413},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep unsupervised self-evolutionary hashing for image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient object detection in stereoscopic 3D images using a
deep convolutional residual autoencoder. <em>TMM</em>, <em>23</em>,
3388–3399. (<a href="https://doi.org/10.1109/TMM.2020.3025166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the detection of distinctive objects in stereoscopic 3D images has drawn increasing attention. Unlike 2D salient object detection, salient object detection in stereoscopic 3D images is highly challenging. Hence, we propose a novel Deep Convolutional Residual Autoencoder (DCRA) for end-to-end salient object detection in stereoscopic 3D images. The core trainable architecture of the salient object detection model employs raw stereoscopic 3D images as the inputs and their corresponding ground truth saliency masks as the labels. A convolutional residual module is applied to both the encoder and the decoder as a basic building block in the DCRA, and long-range skip connections are employed to bypass the equal-sized feature maps between the encoder and the decoder. To explore the complex relationships and exploit the complementarity between RGB (photometric) and depth (geometric) information, multiple feature map fusion modules are constructed. These modules integrate texture and structure information between the RGB and depth branches of the encoder and fuse their features over several multiscale layers. Finally, to efficiently optimize DCRA parameters, a supervision pyramid based on boundary loss and background prior loss is adopted, which employs supervised learning over the multiscale layers in the decoder to prevent vanishing gradients and accelerate the training at the fusion stage. We compare the proposed DCRA with state-of-the-art methods on two challenging benchmark datasets. The results of these experiments demonstrate that our proposed DCRA performs favorably against the comparison models.},
  archive      = {J_TMM},
  author       = {Wujie Zhou and Junwei Wu and Jingsheng Lei and Jenq-Neng Hwang and Lu Yu},
  doi          = {10.1109/TMM.2020.3025166},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3388-3399},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Salient object detection in stereoscopic 3D images using a deep convolutional residual autoencoder},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Speaker clustering by co-optimizing deep representation
learning and cluster estimation. <em>TMM</em>, <em>23</em>, 3377–3387.
(<a href="https://doi.org/10.1109/TMM.2020.3024667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker clustering is a task to merge speech segments uttered by the same speaker into a single cluster, which is an effective tool for alleviating the management of massive amount of audio documents. In this paper, we present a work for co-optimizing the two main steps of speaker clustering, namely, feature learning and cluster estimation. In our method, the deep representation feature is learned by a deep convolutional autoencoder network (DCAN), while the cluster estimation is realized by a softmax layer that is combined with the DCAN. We devise an integrated loss function to simultaneously minimize the reconstruction loss (for deep representation learning) and the clustering loss (for cluster estimation). Many state-of-the-art audio features and clustering methods are evaluated on experimental datasets selected from two publicly available speech corpora (the AISHELL-2 and the VoxCeleb1). The results show that the proposed method exceeds other speaker clustering methods in regard to the normalized mutual information (NMI) and the clustering accuracy (CA). Additionally, the proposed deep representation feature outperforms other features that were widely used in previous works, in terms of both NMI and CA.},
  archive      = {J_TMM},
  author       = {Yanxiong Li and Wucheng Wang and Mingle Liu and Zhongjie Jiang and Qianhua He},
  doi          = {10.1109/TMM.2020.3024667},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3377-3387},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Speaker clustering by co-optimizing deep representation learning and cluster estimation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PFAN++: Bi-directional image-text retrieval with position
focused attention network. <em>TMM</em>, <em>23</em>, 3362–3376. (<a
href="https://doi.org/10.1109/TMM.2020.3024822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bi-directional image-text retrieval and matching attract much attention recently. This cross-domain task demands a fine understanding of both modalities for learning a measure of different modality data. In this paper, we propose a novel position focused attention network to investigate the relation between the visual and the textual views. This work integrates the prior object position to enhance the visual-text joint-embedding learning. The image is first split into blocks, which are treated as the basic position cells, and the position of an image region is inferred. Then, we propose a position attention to model the relations between the image region and position cells. Finally, we generate a valuable position feature to further enhance the region expression and model a more reliable relationship between the visual image and the textual sentence. Experiments on the popular datasets Flickr30K and MS-COCO show the effectiveness of the proposed method. Besides the public datasets, we also conduct experiments on our collected practical large-scale news dataset (Tencent-News) to validate the practical application value of the proposed method. As far as we know, this is the first attempt to test the performance on the practical application. Our method achieves the competitive performance on all of these three datasets.},
  archive      = {J_TMM},
  author       = {Yaxiong Wang and Hao Yang and Xiuxiu Bai and Xueming Qian and Lin Ma and Jing Lu and Biao Li and Xin Fan},
  doi          = {10.1109/TMM.2020.3024822},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3362-3376},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PFAN++: Bi-directional image-text retrieval with position focused attention network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal constraint background-aware correlation filter with
saliency map. <em>TMM</em>, <em>23</em>, 3346–3361. (<a
href="https://doi.org/10.1109/TMM.2020.3023794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlation filter (CF) based trackers have recently drawn great attention in visual tracking community due to their impressive performance, and computational efficiency on benchmark datasets. However, the performance of most existing trackers using correlation filter is hampered by two aspects: i) Included background information in the selected rectangular target patch is considered as part of the target, and they are treated as important as the real target in training new filter model, it causes the filter easily drift when target shape changes dramatically. ii) Existing filters use a moving average operation with an empirical weight to update the filter model in each frame, such per frame adaptation constantly introduces new information of the target patch, but never consider the consistence of the historical information, and the newly obtained one, further increases the risk of drifting. This paper presents a new framework including saliency map, and a novel CF regression model. We reformulate the original optimization problem, and provide a closed form solution for multidimensional features which is solved efficiently using alternating direction method of multipliers (ADMM), and accelerated using Sherman-Morrison lemma, our algorithm as a new framework can be easily integrated into CF base trackers to boost their tracking performance. We perform comprehensive experiments on five benchmarks: OTB-2015, VOT2016, VOT2018, UAV123, and TempleColor-128. Results show that the proposed method performs favorably against lots of state-of-the-art methods with a speed close to real-time. Our method with deep features performs much better on all 5 datasets.},
  archive      = {J_TMM},
  author       = {Jiawen Liao and Chun Qi and Jianzhong Cao},
  doi          = {10.1109/TMM.2020.3023794},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3346-3361},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Temporal constraint background-aware correlation filter with saliency map},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DCR: A unified framework for holistic/partial person ReID.
<em>TMM</em>, <em>23</em>, 3332–3345. (<a
href="https://doi.org/10.1109/TMM.2020.3023784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the occlusions, Person reidentification (ReID) can be divided into holistic person ReID and partial person ReID tasks. Occlusions commonly exist in the partial person ReID task but pose or observation perspective changes often occur in the holistic person ReID task; thus, many different network architectures have been designed for each task. However, this approach increases the cost in practice and hinders the development of ReID techniques. To solve this problem, in this work, a unified framework is proposed for holistic/partial person ReID, which can effectively and efficiently address changes in pose or observation perspective and the occlusions in both tasks. In detail, we first employ a fully convolutional network to generate feature maps for an arbitrarily sized image and then use spatial pyramid pooling to obtain its spatial pyramid feature. Thereafter, to efficiently solve the matching problem between the query image and gallery images, we build a deep spatial pyramid feature collaborative reconstruction model (DCR). Experimental results on two partial person ReID datasets and three holistic person ReID datasets demonstrate that DCR outperforms state-of-the-art approaches on both tasks and all datasets. Specifically, it outperforms all competitors with a large margin and achieves an improvement of 9.07% and 5.95% over DSR on Partial ReID and Partial-iLIDS datasets with Rank-1, respectively. Similarly, it also achieves an improvement of 5.08% over VPM on the DukeMTMC-ReID dataset with Rank-1. Additionally, the running time of our method for each query is more than 7× faster than that of DSR or DuATM methods.},
  archive      = {J_TMM},
  author       = {Zan Gao and Lishuai Gao and Hua Zhang and Zhiyong Cheng and Richang Hong and Shengyong Chen},
  doi          = {10.1109/TMM.2020.3023784},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3332-3345},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCR: A unified framework for Holistic/Partial person ReID},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KTransGAN: Variational inference-based knowledge transfer
for unsupervised conditional generative learning. <em>TMM</em>,
<em>23</em>, 3318–3331. (<a
href="https://doi.org/10.1109/TMM.2020.3023792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-conditional generative models have gained popularity due to their characteristics of learning disentangled representations. However, these models typically require labeled examples in training. In this paper, we explore the feasibility of training these models on completely unlabeled data, under the assumption that we have access to other labeled data. The labeled data share the same label space, while their domain is shifted. Our model, which we refer to as KTransGAN, incorporates a classifier to transfer knowledge from the labeled data and performs collaborative learning with the conditional generator. By adopting these measures, KTransGAN is able to approximate the conditional distribution of the unlabeled data and simultaneously introduces a new solution to the unsupervised domain adaptation problem. To mitigate the training difficulty of our generative adversarial networks-based model, variational encoding and feature matching are also considered. From the empirical results, KTransGAN exhibits outstanding performance on a number of synthetic datasets and multiple real-world benchmarks. The quality of the synthesized instances is far superior to the pure variational autoencoding model. For example, on the CIFAR-10 dataset, our model scores 35.3 in FID, while the other model scores 128.45. In addition, the synthesis quality is close to the case when the model is trained in a fully supervised setting over the same number of training iterations. Regarding the classification performance, for instance, our model surpasses the highest state-of-the-art results (89.19%) by a large margin and achieves a test accuracy of 95.31% on the unlabeled data SVHN, while MNIST represents the labeled data. These results highlight the effectiveness of our proposed framework.},
  archive      = {J_TMM},
  author       = {Mohamed Azzam and Wenhao Wu and Wenming Cao and Si Wu and Hau-San Wong},
  doi          = {10.1109/TMM.2020.3023792},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3318-3331},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {KTransGAN: Variational inference-based knowledge transfer for unsupervised conditional generative learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Temporal textual localization in video via adversarial
bi-directional interaction networks. <em>TMM</em>, <em>23</em>,
3306–3317. (<a href="https://doi.org/10.1109/TMM.2020.3023339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a natural language description, temporal textual localization aims to localize the most relevant segment in an untrimmed video, which is a natural and imperative extension of temporal action localization. Most existing temporal textual localization works neglect the long-range semantic modeling in video contents and lack accurate textual understanding. Moreover, they remain in single-task learning and fail to exploit multi-view supervised information. Based on these observations, we introduce a novel adversarial bi-directional interaction network, which is a global framework to retrieve the target segment directly. Specifically, we propose a bi-directional attention mechanism to build bi-directional information interaction, which captures long-range semantic dependencies from video context and enhances textual representation learning. After localization, we further advise an auxiliary discriminator network to verify the localization result and boost the performance by adversarial training process. We adopt multi-task learning approach to train our model, including: (1) predicting coordinate probability distribution task, which selects start and end frame to localize target segment; (2) predicting frame-level correlation distribution task, which calculates the correlation between frame and description; (3) auxiliary adversarial learning task, which calculates matched score between localization and description to boost the performance. The extensive experiments on ActivityNet Captions and TACoS show the significant effectiveness and efficiency of our method.},
  archive      = {J_TMM},
  author       = {Zijian Zhang and Zhou Zhao and Zhu Zhang and Zhijie Lin and Qi Wang and Richang Hong},
  doi          = {10.1109/TMM.2020.3023339},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3306-3317},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Temporal textual localization in video via adversarial bi-directional interaction networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial 3D convolutional auto-encoder for abnormal event
detection in videos. <em>TMM</em>, <em>23</em>, 3292–3305. (<a
href="https://doi.org/10.1109/TMM.2020.3023303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal event detection aims to identify the events that deviate from expected normal patterns. Existing methods usually extract normal spatio-temporal patterns of appearance and motion in a separate manner, which ignores low-level correlations between appearance and motion patterns and may fall short of capturing fine-grained spatio-temporal patterns. In this paper, we propose to simultaneously learn appearance and motion to obtain fine-grained spatio-temporal patterns. To this end, we present an adversarial 3D convolutional auto-encoder to learn the normal spatio-temporal patterns and then identify abnormal events by diverging them from the learned normal patterns in videos. The encoder captures the low-level correlations between spatial and temporal dimensions of videos, and generates distinctive features representing visual spatio-temporal information. The decoder reconstrucccts the original video from the encoded features representing by 3D de-convolutions and learns the normal spatio-temporal patterns in an unsupervised manner. We introduce the denoising reconstruction error and adversarial learning strategy to train the 3D convolutional auto-encoder to implicitly learn accurate data distributions that are considered normal patterns, which benefits enhancing the reconstruction ability of the auto-encoder to discriminate abnormal events. Both the theoretical analysis and the extensive experiments on four publicly available datasets demonstrate the effectiveness of our method.},
  archive      = {J_TMM},
  author       = {Che Sun and Yunde Jia and Hao Song and Yuwei Wu},
  doi          = {10.1109/TMM.2020.3023303},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3292-3305},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial 3D convolutional auto-encoder for abnormal event detection in videos},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based joint bit allocation between geometry and color
for video-based 3D point cloud compression. <em>TMM</em>, <em>23</em>,
3278–3291. (<a href="https://doi.org/10.1109/TMM.2020.3023294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video-based 3D point cloud compression, the quality of the reconstructed 3D point cloud depends on both the geometry, and color distortions. Finding an optimal allocation of the total bitrate between the geometry coder, and the color coder is a challenging task due to the large number of possible solutions. To solve this bit allocation problem, we first propose analytical distortion, and rate models for the geometry, and color information. Using these models, we formulate the joint bit allocation problem as a constrained convex optimization problem, and solve it with an interior point method. Experimental results show that the rate-distortion performance of the proposed solution is close to that obtained with exhaustive search but at only 0.66 $\%$ of its time complexity.},
  archive      = {J_TMM},
  author       = {Qi Liu and Hui Yuan and Junhui Hou and Raouf Hamzaoui and Honglei Su},
  doi          = {10.1109/TMM.2020.3023294},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3278-3291},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Model-based joint bit allocation between geometry and color for video-based 3D point cloud compression},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive and robust partition learning for person retrieval
with policy gradient. <em>TMM</em>, <em>23</em>, 3264–3277. (<a
href="https://doi.org/10.1109/TMM.2020.3023272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person retrieval aims at effectively matching the pedestrian images over an extensive database given a specified identity. As extracting effective features is crucial in a high-performance retrieval system, recent significant progress was achieved by part-based models that have constructed robust local representations on top of vertically striped part features. However, this kind of models use predefined partitioning strategies, making the number and size of each partition identical even when input images vary a lot. This unchangeable setting usually leads to less flexibility and robustness in capturing visual variance. The primary reason for such a negative effect is that a fixed partitioning strategy is unable to deal with (a) the significant variance from pose, illumination and viewpoint which is common in a pedestrian image dataset, and (b) also the inference error and misalignment of human bodies introduced by the prepositive pedestrian detection module or human pose estimation module. In this paper, we tackle this problem via introducing the novel Adaptive Partition Network (APN). The APN utilizes deep reinforcement learning and applies an agent to generate optimal partitioning strategies dynamically for different input images. The agent inside the APN is optimized with the policy gradient algorithm and maximizes the reward of choosing the best partition setting. By leveraging the supervision cues from the objective partitioning strategies that are generated on a set of held-out training images, the agent is trained jointly with other parts of APN, which ensures the APN&#39;s robustness and generalization ability. Extensive experimental results on multiple datasets, including CUHK03, DukeMTMC and Market-1501, demonstrate the superiority of APN over the state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Yuxuan Shi and Zhen Wei and Hefei Ling and Ziyang Wang and Pengfei Zhu and Jialie Shen and Ping Li},
  doi          = {10.1109/TMM.2020.3023272},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3264-3277},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive and robust partition learning for person retrieval with policy gradient},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NDN-MMRA: Multi-stage multicast rate adaptation in named
data networking WLAN. <em>TMM</em>, <em>23</em>, 3250–3263. (<a
href="https://doi.org/10.1109/TMM.2020.3023282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Data Networking (NDN) is considered as a prominent architecture towards future Wireless Local Area Networks (WLAN), and multicast plays an important role in data delivery such as media streaming, multipoint videoconferencing, etc. However, to achieve high-efficiency multicast in NDN WLAN is challenging for two significant reasons. First, without feedback mechanism in IEEE 802.11 standards, to guarantee reliability, the current multicast scheme transmits the multicast data with the basic rate (e.g., 1 Mbps for IEEE 802.11b), which inevitably increases the transmission delay for high-speed consumers. Second, as a NDN multicast group is constituted by consumers who are requesting the same content, multicast groups are easy to form and evolve rapidly, where a data rate adaptation scheme is requisite to accommodate differential multicast groups. In this paper, we propose a multi-stage multicast rate adaptation scheme for NDN WLAN, named NDN-MMRA , to minimize the total transmission time with reliability guarantee for multicast group members. In NDN-MMRA , by checking the Pending Interest Table (PIT) status information, the number of consumers in each multicast group as well as their receiving capabilities are known ahead; with the available data rates in a specific 802.11 standard, NDN-MMRA determines: 1) how many transmission stages are required; and 2) in each stage, which data rate should be adopted. The merit is that with multi-stage transmissions, the data rate can be adapted in descending order to accommodate high-speed consumers with delay minimized, and low-speed consumers with reliability guaranteed. We implement NDN-MMRA in NS-3 by adopting the ndnSIM module, and conduct extensive experiments to demonstrate its efficacy under different IEEE 802.11 standards and various underlying WLAN topologies.},
  archive      = {J_TMM},
  author       = {Fan Wu and Wang Yang and Ju Ren and Feng Lyu and Peng Yang and Yaoxue Zhang and Xuemin Shen},
  doi          = {10.1109/TMM.2020.3023282},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3250-3263},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {NDN-MMRA: Multi-stage multicast rate adaptation in named data networking WLAN},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global manifold learning for interactive image segmentation.
<em>TMM</em>, <em>23</em>, 3239–3249. (<a
href="https://doi.org/10.1109/TMM.2020.3021979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an interactive image segmen-tation algorithm, in which the segmentation problem is formulated as a global manifold learning process. Based on the principle that the label of each element depends on the influence of all the elements in the image, we extend the conventional local neighborhood or the long range regional relationships to the global relationships over the whole image. A probabilistic framework is established to measure the global effect of each element on all other elements. Based on two different manifold learning styles, the semi-global manifold learning (SGML) and the fully-global manifold learning (FGML) algorithms are proposed to capture the global geometry structure of the data manifold. SGML learns the intrinsic effects of each element separately, equivalent to a matrix diffusion process on an affinity graph. FGML learns the intrinsic effects of all elements together, equivalent to a label pair diffusion process on a higher-order tensor product graph. The global manifold learning helps to overcome the low contrast, weak boundary and texture problems. Extensive experiments on three public interactive segmentation datasets demonstrate the superior performance of the proposed algorithms both in accuracy and efficiency.},
  archive      = {J_TMM},
  author       = {Tao Wang and Zexuan Ji and Jian Yang and Quansen Sun and Peng Fu},
  doi          = {10.1109/TMM.2020.3021979},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3239-3249},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Global manifold learning for interactive image segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based deep reinforcement learning for virtual
cinematography of 360<span class="math inline"><sup>∘</sup></span>
videos. <em>TMM</em>, <em>23</em>, 3227–3238. (<a
href="https://doi.org/10.1109/TMM.2020.3021984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual cinematography refers to automatically selecting a natural-looking normal field-of-view (NFOV) from an entire 360 $^{\circ}$ video. In fact, virtual cinematography can be modeled as a deep reinforcement learning (DRL) problem, in which an agent makes actions related to NFOV selection according to the environment of 360 $^{\circ}$ video frames. More importantly, we find from our data analysis that the selected NFOVs attract significantly more attention than other regions, i.e., the NFOVs have high saliency. Therefore, in this paper, we propose an attention-based DRL (A-DRL) approach for virtual cinematography in 360 $^{\circ}$ video. Specifically, we develop a new DRL framework for automatic NFOV selection with the input of both the content, and saliency map of each 360 $^{\circ}$ frame. Then, we propose a new reward function for the DRL framework in our approach, which considers the saliency values, ground-truth, and smooth transition for NFOV selection. Subsequently, a simplified DenseNet (called Mini-DenseNet) is designed to learn the optimal policy via maximizing the reward. Based on the learned policy, the actions of NFOV can be made in our A-DRL approach for virtual cinematography of 360 $^{\circ}$ video. Extensive experiments show that our A-DRL approach outperforms other state-of-the-art virtual cinematography methods, over the datasets of Sports-360 video, and Pano2Vid.},
  archive      = {J_TMM},
  author       = {Jianyi Wang and Mai Xu and Lai Jiang and Yuhang Song},
  doi          = {10.1109/TMM.2020.3021984},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3227-3238},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attention-based deep reinforcement learning for virtual cinematography of 360$^{\circ}$ videos},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handling outliers by robust m-estimation in blind image
deblurring. <em>TMM</em>, <em>23</em>, 3215–3226. (<a
href="https://doi.org/10.1109/TMM.2020.3021989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major task of traditional motion deblurring methods is to estimate the blur kernel and restore the latent image. In low-light conditions, the pointolite is likely to produce saturated light streaks in captured blurred images. The light streaks are usually double-edged swords—outliers to the deconvolution, but a cue to kernel estimation. In this paper, we propose a novel blind motion deblurring method for blurred images including light streaks. The main idea is to model the non-linear blur caused by outliers as the Huber&#39;s M-estimation in blind deconvolution and take the shape of the light streak as a cue to estimate the blur kernel. Specifically, the optimal light streak patch is selected automatically according to the characteristics of light streaks and the blur kernel. This simple yet effective selection strategy solves the problems of false detection of candidate light streaks and optimal light streak in existing methods. Then, the optimal light streak patch is parameterized as a prior and is combined with other regularizers to estimate the blur kernel. Compared with the state-of-the-art kernel estimation methods, the proposed algorithm reduces the influence of outliers on deconvolution and utilizes more information. Thus, the restored image is more accurate. Experimental results on both synthetic and real images demonstrate the high accuracy of our algorithm.},
  archive      = {J_TMM},
  author       = {Xinxin Zhang and Ronggang Wang and Da Chen and Yang Zhao and Wen Gao},
  doi          = {10.1109/TMM.2020.3021989},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3215-3226},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Handling outliers by robust M-estimation in blind image deblurring},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised video summarization via relation-aware
assignment learning. <em>TMM</em>, <em>23</em>, 3203–3214. (<a
href="https://doi.org/10.1109/TMM.2020.3021980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of unsupervised video summarization that automatically selects key video clips. Most state-of-the-art approaches suffer from two issues: (1) they model video clips without explicitly exploiting their relations, and (2) they learn soft importance scores over all the video clips to generate the summary representation. However, a meaningful video summary should be inferred by taking the relation-aware context of the original video into consideration, and directly selecting a subset of clips with a hard assignment. In this paper, we propose to exploit clip-clip relations to learn relation-aware hard assignments for selecting key clips in an unsupervised manner. First, we consider the clips as graph nodes to construct an assignment-learning graph. Then, we utilize the magnitude of the node features to generate hard assignments as the summary selection. Finally, we optimize the whole framework via a proposed multi-task loss including a reconstruction constraint, and a contrastive constraint. Extensive experimental results on three popular benchmarks demonstrate the favourable performance of our approach.},
  archive      = {J_TMM},
  author       = {Junyu Gao and Xiaoshan Yang and Yingying Zhang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3021980},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3203-3214},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised video summarization via relation-aware assignment learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-saliency detection via a general optimization model and
adaptive graph learning. <em>TMM</em>, <em>23</em>, 3193–3202. (<a
href="https://doi.org/10.1109/TMM.2020.3021251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-saliency detection is an important research problem, and has been widely used in computer vision area. One main challenge for co-saliency detection problem is how to explore both interactive information among different images and individual salient information within each image simultaneously in co-saliency estimation. In this paper, we propose a novel general optimization framework with adaptive graph learning for co-saliency estimation problem. The proposed model integrates multiple cues including background, and foreground priors, structural information of images, and image feature representation together to obtain a uniform, and accurate co-saliency estimation. One main benefit of the proposed co-saliency method is that it conducts co-saliency propagation, and prediction across different images while maintains the individual salient information of each image, which ensures the consistency, and communication across different images effectively in co-saliency estimation. To improve the accuracy of co-saliency estimation, we adaptively learn a neighborhood, and structured graph to conduct co-saliency propagation among superpixels. An effective optimization algorithm has been designed to seek the optimal solution for the proposed co-saliency optimization model. Experimental results on several widely used datasets show that our method outperforms some other related co-saliency detection methods.},
  archive      = {J_TMM},
  author       = {Bo Jiang and Xingyue Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TMM.2020.3021251},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3193-3202},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Co-saliency detection via a general optimization model and adaptive graph learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frame-wise detection of double HEVC compression by learning
deep spatio-temporal representations in compression domain.
<em>TMM</em>, <em>23</em>, 3179–3192. (<a
href="https://doi.org/10.1109/TMM.2020.3021234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of double compression is regarded as one primary step in analyzing the integrity of digital videos, which is of prominent importance in video forensics. However, current methods are vulnerable with the severe lossy quantization in the recompression process such that it is challenging to obtain reliable frame-wise detection results, especially for the high efficiency video coding (HEVC) standard. In view of these issues, in this paper, a hybrid neural network is proposed to reveal abnormal frames in HEVC videos with double compression by learning robust spatio-temporal representations from coding information in the compression domain. Based on the statistical analysis of Coding Units (CUs), it is interesting to find that HEVC video streams contain “rich” coding information that could be leveraged to identify abnormal traces caused by double compression. Two types of coding information maps, including CU Size Map (CSM) and CU Prediction mode Map (CPM), are exploited. In contrast with the conventional paradigm relying on pixel-level representations of decoded frames, CSMs and CPMs of a short-time video clip are treated as the input, aiming to achieve high robustness against recompression of low quality. In our hybrid neural network, an attention-based two-stream residual network is proposed to learn hierarchical representations from CSM and CPM, which are then jointly optimized by the attention-based fusion module. Finally, the temporal variation is modeled by Long Short-Term Memory (LSTM) to obtain frame-wise detection results. We have conducted extensive experiments considering various video content and coding parameters, such as bitrates and sizes of Group of Picture. Experimental results show that our approach can obtain state-of-the-art performance compared with conventional methods, especially when videos are recompressed in the low bitrate coding scenarios.},
  archive      = {J_TMM},
  author       = {Peisong He and Haoliang Li and Hongxia Wang and Shiqi Wang and Xinghao Jiang and Ruimei Zhang},
  doi          = {10.1109/TMM.2020.3021234},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3179-3192},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frame-wise detection of double HEVC compression by learning deep spatio-temporal representations in compression domain},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing video QoE for mobile eMBMS users in cellular
networks. <em>TMM</em>, <em>23</em>, 3166–3178. (<a
href="https://doi.org/10.1109/TMM.2020.3021229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolved Multimedia Broadcast Multicast Service (eMBMS) is used in cellular networks to improve the utilization of scarce wireless resources in high user density service areas. However, eMBMS configuration involves interwoven decisions including which base stations (eNB) to synchronize to form Single Frequency Networks (SFN), which video qualities to be serviced, and how to distribute resources among different videos. These decisions should accommodate disparate channel conditions for eMBMS users, and the impact of eNB&#39;s unicast-load in the service area. In this paper, we formulate eMBMS configuration as an optimization problem that maximizes the video QoE for users. Additionally, we present NIMBLE as an eMBMS configuration heuristic, guided by our optimization framework, to solve the problem in realtime. Furthermore, NIMBLE&#39;s design integrates elements to accommodate the dynamic nature of cellular networks resulting from changes in both user, and network state over time. We developed a simulation testbed, and performed extensive experiments to show that, in comparison to state-of-the-art schemes, NIMBLE can increase the average user throughput by 150%, and reduce the bitrate switches by 75%.},
  archive      = {J_TMM},
  author       = {Ahmed Khalid and Ahmed H. Zahran and Cormac J. Sreenan},
  doi          = {10.1109/TMM.2020.3021229},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3166-3178},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Optimizing video QoE for mobile eMBMS users in cellular networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Luminance-aware pyramid network for low-light image
enhancement. <em>TMM</em>, <em>23</em>, 3153–3165. (<a
href="https://doi.org/10.1109/TMM.2020.3021243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement based on deep convolutional neural networks (CNNs) has revealed prominent performance in recent years. However, it is still a challenging task since the underexposed regions and details are always imperceptible. Moreover, deep learning models are always accompanied by complex structures and enormous computational burden, which hinders their deployment on mobile devices. To remedy these issues, in this paper, we present a lightweight and efficient Luminance-aware Pyramid Network (LPNet) to reconstruct normal-light images in a coarse-to-fine strategy. The architecture is comprised of two coarse feature extraction branches and a luminance-aware refinement branch with an auxiliary subnet learning the luminance map of the input and target images. Besides, we propose a multi-scale contrast feature block (MSCFB) that involves channel split, channel shuffle strategies, and contrast attention mechanism. MSCFB is the essential component of our network, which achieves an excellent balance between image quality and model size. In this way, our method can not only brighten up low-light images with rich details and high contrast but also significantly ameliorate the execution speed. Extensive experiments demonstrate that our LPNet outperforms state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_TMM},
  author       = {Jiaqian Li and Juncheng Li and Faming Fang and Fang Li and Guixu Zhang},
  doi          = {10.1109/TMM.2020.3021243},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3153-3165},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Luminance-aware pyramid network for low-light image enhancement},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent representation learning model for multi-band images
fusion via low-rank and sparse embedding. <em>TMM</em>, <em>23</em>,
3137–3152. (<a href="https://doi.org/10.1109/TMM.2020.3020695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of multi-band images including far-infrared image (FIRI), near-infrared image (NIRI), and visible image (VISI) primarily suffers from four challenges. One is the problem of simultaneous fusion for multiple images. Most existing methods are oriented towards the fusion of two objects, which is generally achieved with a sequential fusion method. This means that intermediate fusion results are repeatedly integrated with the unprocessed images until all images have been fused. However, this may amplify the blurring effect, and even engender artifacts. Second, consistent training labels for image fusion cannot currently be obtained for some types of images (e.g., medical images, and multi-band images), which may lead to the failed application of supervised learning methods. Third, the existing methods often do not directly focus on the potential mapping relationship between the original, and resulting images, which usually increases the unpredictability of the fusion results. Fourth, redundant features or singularities are often not eliminated in the general fusion process, and both may interfere with or even obscure significant features in the source images. To address the abovementioned problems, this paper proposes a latent representation learning model that can synchronously integrate multi-band images without samples. Specifically, the model can capture the clean, and distinctive features of the originals via latent low-rank, and sparse embedding. The extracted intrinsic features are projected onto the target fusion space through an assumed mapping relationship. The final results were obtained through the designed optimization algorithm. In addition, numerous experiments were implemented to prove the rationality, and feasibility of the proposed fusion model with subjective evaluation, objective indexes, and convergence analysis.},
  archive      = {J_TMM},
  author       = {Bin Wang and Huifang Niu and Jianchao Zeng and Guifeng Bai and Suzhen Lin and Yanbo Wang},
  doi          = {10.1109/TMM.2020.3020695},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3137-3152},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Latent representation learning model for multi-band images fusion via low-rank and sparse embedding},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Graph regularized encoder-decoder networks for image
representation learning. <em>TMM</em>, <em>23</em>, 3124–3136. (<a
href="https://doi.org/10.1109/TMM.2020.3020697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image representation learning with encoder-decoder networks plays a fundamental role in multimedia processing. Recent findings show that traditional encoder-decoders can be negatively affected by small visual perturbations. The learned non-smooth feature embedding cannot guarantee to capture semantic-meaningful geometric distance between visually-similar image samples. Inspired by manifold learning, we propose a graph regularized encoder-decoder network, which can preserve local geometric information of the code embedding space. More discriminative feature embedding is learnt to attain both high-level image semantic and neighbor relationship of image clusters. The proposed graph regularizer is formulated upon multi-layer perceptions. It uses the local invariance principle to explicitly reconstruct the geometric similarity graph. Theoretical analysis is provided to show the connection between our deep regularizer and traditional graph Laplacian regularizer. Practically, the network complexity is alleviated by anchor based bipartite graph, and this leverages our method into large scale scenario. Experimental evaluations show the comparable results of the proposed method with state-of-the-art models on different tasks.},
  archive      = {J_TMM},
  author       = {Shijie Yang and Liang Li and Shuhui Wang and Weigang Zhang and Qingming Huang and Qi Tian},
  doi          = {10.1109/TMM.2020.3020697},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3124-3136},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph regularized encoder-decoder networks for image representation learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to segment video object with accurate boundaries.
<em>TMM</em>, <em>23</em>, 3112–3123. (<a
href="https://doi.org/10.1109/TMM.2020.3020698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation has attracted considerable research interest these years. Top-performing video object segmentation methods mainly rely on fully convolutional neural networks which are specifically trained for predicting high-performance masks, resulting in a lack of preciseness in boundary details. This paper tackles the problem of predicting both mask-accurate and boundary-precise segmentation masks in videos. To solve this problem, we propose a simple and efficient network structure: the Mask-boundAry-Consistent Network ( MAC-Net ). The MAC-Net is an end-to-end fully convolutional network, where both mask and boundaries are jointly optimized during training, enabling it to predict masks along with accurate boundaries. An inner-net boundary-computing module is incorporated in the MAC-Net for producing spontaneously mask-consistent boundaries. We analyze the influence of parameter settings, network constructions of the MAC-Net , and compare with state-of-the-art algorithms on three widely-adopted datasets. Experimental results show that the MAC-Net achieves state-of-the-art performance, demonstrating the effectiveness of its mask-boundary-consistent network structure. We also propose that the boundary module in MAC-Net has high compatibility, and can be easily adapted to other segmentation-related techniques.},
  archive      = {J_TMM},
  author       = {Jingchun Cheng and Yuhui Yuan and Yali Li and Jingdong Wang and Shengjin Wang},
  doi          = {10.1109/TMM.2020.3020698},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3112-3123},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to segment video object with accurate boundaries},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mesh convolution: A novel feature extraction method for 3D
nonrigid object classification. <em>TMM</em>, <em>23</em>, 3098–3111.
(<a href="https://doi.org/10.1109/TMM.2020.3020693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying convolution methods to domains that lack regular underlying structures is a challenging task for 3D vision. Existing methods require the manual design of feature representations suitable for the task or full-voxel-level analysis, which is memory intensive. In this paper, we propose a novel feature extraction method to facilitate 3D nonrigid shape analysis. Our approach, called 3D-MConv, extends convolution operations from regular grids to irregular mesh sets by parametrizing a series of convolutional templates and adopts a novel local perspective to ensure that the algorithm is more invariant against global isometric deformation and articulation. We carefully design the convolutional template as a polynomial function that flexibly represents the local shape. An unsupervised learning method is adopted to learn the convolutional template function. By using the convolution operation and the movement of the template on the model surface, we can obtain the distribution of the typical template shapes. We combine this distribution feature with the spatial co-occurrence information of typical template shapes modelled by Markov chains to form a high-level descriptor of a 3D model. The support vector machine method is used to classify the nonrigid 3D objects. Experiments on SHREC10 and SHREC15 demonstrate that 3D-MConv achieves state-of-the-art accuracy on standard benchmarks.},
  archive      = {J_TMM},
  author       = {Yu Chen and Jieyu Zhao and Congwei Shi and Dongdong Yuan},
  doi          = {10.1109/TMM.2020.3020693},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3098-3111},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mesh convolution: A novel feature extraction method for 3D nonrigid object classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attribute-aware pedestrian detection in a crowd.
<em>TMM</em>, <em>23</em>, 3085–3097. (<a
href="https://doi.org/10.1109/TMM.2020.3020691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is an initial step to perform outdoor scene analysis, which plays an essential role in many real-world applications. Although having enjoyed the merits of deep learning frameworks from the generic object detectors, pedestrian detection is still a very challenging task due to heavy occlusions, and highly crowded group. Generally, the conventional detectors are unable to differentiate individuals from each other effectively under such a dense environment. To tackle this critical problem, we propose an attribute-aware pedestrian detector to explicitly model people&#39;s semantic attributes in a high-level feature detection fashion. Besides the typical semantic features, center position, target&#39;s scale, and offset, we introduce a pedestrian-oriented attribute feature to encode the high-level semantic differences among the crowd. Moreover, a novel attribute-feature-based Non-Maximum Suppression (NMS) is proposed to distinguish the person from a highly overlapped group by adaptively rejecting the false-positive results in a very crowd settings. Furthermore, an enhanced ground truth target is designed to alleviate the difficulties caused by the attribute configuration, and to ease the class imbalance issue during training. Finally, we evaluate our proposed attribute-aware pedestrian detector on three benchmark datasets including CityPerson, CrowdHuman, and EuroCityPerson, and achieves the state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Jialiang Zhang and Lixiang Lin and Jianke Zhu and Yang Li and Yun-chen Chen and Yao Hu and Steven C. H. Hoi},
  doi          = {10.1109/TMM.2020.3020691},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {3085-3097},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-aware pedestrian detection in a crowd},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised multi-view clustering by squeezing hybrid
knowledge from cross view and each view. <em>TMM</em>, <em>23</em>,
2943–2956. (<a href="https://doi.org/10.1109/TMM.2020.3019683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering methods have been a focus in recent years because of their superiority in clustering performance. However, typical traditional multi-view clustering algorithms still have shortcomings in some aspects, such as removal of redundant information, utilization of various views and fusion of multi-view features. In view of these problems, this paper proposes a new multi-view clustering method, low-rank subspace multi-view clustering based on adaptive graph regularization. We construct two new data matrix decomposition models into a unified optimization model. In this framework, we address the significance of the common knowledge shared by the cross view and the unique knowledge of each view by presenting new low-rank and sparse constraints on the sparse subspace matrix. To ensure that we achieve effective sparse representation and clustering performance on the original data matrix, adaptive graph regularization and unsupervised clustering constraints are also incorporated in the proposed model to preserve the internal structural features of the data. Finally, the proposed method is compared with several state-of-the-art algorithms. Experimental results for five widely used multi-view benchmarks show that our proposed algorithm surpasses other state-of-the-art methods by a clear margin.},
  archive      = {J_TMM},
  author       = {Junpeng Tan and Yukai Shi and Zhijing Yang and Caizhen Wen and Liang Lin},
  doi          = {10.1109/TMM.2020.3019683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {9},
  pages        = {2943-2956},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised multi-view clustering by squeezing hybrid knowledge from cross view and each view},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interclass-relativity-adaptive metric learning for
cross-modal matching and beyond. <em>TMM</em>, <em>23</em>, 3073–3084.
(<a href="https://doi.org/10.1109/TMM.2020.3019710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training under supervision of triplet ranking loss is a dominant methodology for cross-modal matching models, while good-performing losses in this domain are immensely under-explored since the majority of advanced metric losses are inapplicable due to the particularity of cross-modal setting. Current prominent approaches of metric learning have developed various weighting schemes that assign weights to separate positive or negative samples. It is the interclass relative order in a triplet, however, that matters. In this work, we propose a new Interclass-Relativity-Adaptive (IRA) loss that assigns weights to the relative similarities between positive and negative pairs instead of separate pairs, which allows us to regard a whole triplet as a weighable entity and achieve maximum utilization of sole positive under cross-modal setting. Our method outperforms the baselines by a large margin and obtains competitive results on two video-text matching benchmarks and two image-text matching benchmarks. We also further extend our method to two unimodal image retrieval benchmarks to test its generality and achieve new state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Feiyu Chen and Jie Shao and Yonghui Zhang and Xing Xu and Heng Tao Shen},
  doi          = {10.1109/TMM.2020.3019710},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3073-3084},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Interclass-relativity-adaptive metric learning for cross-modal matching and beyond},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive composite residual network for robust rain removal
from single images. <em>TMM</em>, <em>23</em>, 3059–3072. (<a
href="https://doi.org/10.1109/TMM.2020.3019680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rainy conditions, imaging devices often capture degraded and blurry images. Most existing works on this problem focus on rain streak removal, but these approaches cannot handle the various types of rain found in images. In this paper, we propose a robust rain removal method for use with single images using an attentive composite residual network. We put forth a single-to-dual encoder-decoder structure, which consists of an attentive net that identifies regions containing rain components during encoding, followed by a dual-channel architecture which recovers the background and detail components of the identified regions during decoding. For the detail subnet, we designed a novel building block, namely a composite residual block (CRB), by constructing multiple residual connections among Res2Net modules. Additionally, we designed another attentive-CRB for the attentive net that uses a squeeze-and-excitation (SE)-Res2Net module, to build a channel-wise attention mechanism. We show that such a deep network can be trained end-to-end from rainy images and that it outperforms the previous state-of-the-art methods on datasets containing different types of rainy images. Experimental results also demonstrate the proposed model&#39;s superiority over the competitor&#39;s on real rain-affected images, recovering visually clean images and retaining good detail.},
  archive      = {J_TMM},
  author       = {Yue Que and Suli Li and Hyo Jong Lee},
  doi          = {10.1109/TMM.2020.3019680},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {3059-3072},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attentive composite residual network for robust rain removal from single images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning compact multifeature codes for palmprint
recognition from a single training image per palm. <em>TMM</em>,
<em>23</em>, 2930–2942. (<a
href="https://doi.org/10.1109/TMM.2020.3019701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a multifeature learning method to jointly learn compact multifeature codes (LCMFCs) for palmprint recognition with a single training sample per palm. Unlike most existing hand-crafted methods that extract single-type features from raw pixels, we first form the multi-type data vectors such as the direction-data, and texture-data to completely sample the multiple information of a palmprint image. Then, we learn the discriminative multifeatures from multi-type data vectors by maximizing the inter-palm distance, and minimizing the energy loss between the learned codes, and the original data. Moreover, our LCMFC method adaptively learns the optimal weights of multi-type features to jointly learn the compact multifeature codes. Finally, we cluster the nonoverlapping blockwise histograms of the compact multifeature codes into a feature vector for palmprint representation. Extensive experimental results on six benchmark palmprint databases are presented to show the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Lunke Fei and Bob Zhang and Lin Zhang and Wei Jia and Jie Wen and Jigang Wu},
  doi          = {10.1109/TMM.2020.3019701},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2930-2942},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning compact multifeature codes for palmprint recognition from a single training image per palm},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid-attention enhanced two-stream fusion network for
video venue prediction. <em>TMM</em>, <em>23</em>, 2917–2929. (<a
href="https://doi.org/10.1109/TMM.2020.3019714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video venue category prediction has been drawing more attention in the multimedia community for various applications such as personalized location recommendation and video verification. Most of existing works resort to the information from either multiple modalities or other platforms for strengthening video representations. However, noisy acoustic information, sparse textual descriptions and incompatible cross-platform data could limit the performance gain and reduce the universality of the model. Therefore, we focus on discriminative visual feature extraction from videos by introducing a hybrid-attention structure. Particularly, we propose a novel Global-Local Attention Module (GLAM), which can be inserted to neural networks to generate enhanced visual features from video content. In GLAM, the Global Attention (GA) is used to catch contextual scene-oriented information via assigning channels with various weights while the Local Attention (LA) is employed to learn salient object-oriented features via allocating different weights for spatial regions. Moreover, GLAM can be extended to ones with multiple GAs and LAs for further visual enhancement. These two types of features respectively captured by GAs and LAs are integrated via convolution layers, and then delivered into convolutional Long Short-Term Memory (convLSTM) to generate spatial-temporal representations, constituting the content stream. In addition, video motions are explored to learn long-term movement variations, which also contributes to video venue prediction. The content and motion stream constitute our proposed Hybrid-Attention Enhanced Two-Stream Fusion Network (HA-TSFN). HA-TSFN finally merges the features from two streams for comprehensive representations. Extensive experiments demonstrate that our method achieves the state-of-the-art performance in the large-scale dataset Vine. The visualization also shows that the proposed GLAM can capture complementary scene-oriented and object-oriented visual features from videos. Our code is available at: https://github.com/zhangyanchao1014/HA-TSFN .},
  archive      = {J_TMM},
  author       = {Yanchao Zhang and Weiqing Min and Liqiang Nie and Shuqiang Jiang},
  doi          = {10.1109/TMM.2020.3019714},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2917-2929},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hybrid-attention enhanced two-stream fusion network for video venue prediction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency detection using deep features and affinity-based
robust background subtraction. <em>TMM</em>, <em>23</em>, 2902–2916. (<a
href="https://doi.org/10.1109/TMM.2020.3019688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing saliency methods measure fore- ground saliency by using the contrast of a foreground region to its local context, or boundary priors and spatial compactness. These methods are not powerful enough to extract a precise salient region from noisy and cluttered backgrounds. To evaluate the contrast of salient and background regions effectively, we consider high-level features from both supervised and unsupervised methods. We propose an affinity-based robust background subtraction technique and maximum attention map using a pre-trained convolution neural network. This affinity-based technique uses pixel similarities to propagate the values of salient pixels among foreground and background regions and their union. The salient pixel value controls the foreground and background information by using multiple pixel affinities. The maximum attention map is derived from the convolution neural network using features of the Pooling and Relu layers. This method can detect salient regions from images that have noisy and cluttered backgrounds. Our experimental results demonstrate the effectiveness of the proposed approach on six different saliency data sets and benchmarks and show that it improves the quality of detection beyond current saliency detection methods.},
  archive      = {J_TMM},
  author       = {Mehmood Nawaz and Hong Yan},
  doi          = {10.1109/TMM.2020.3019688},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2902-2916},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Saliency detection using deep features and affinity-based robust background subtraction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning diverse fashion collocation by neural graph
filtering. <em>TMM</em>, <em>23</em>, 2894–2901. (<a
href="https://doi.org/10.1109/TMM.2020.3018021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion recommendation systems are highly desired by customers to find visually-collocated fashion items, such as clothes, shoes, bags, etc. While existing methods demonstrate promising results, they remain lacking in flexibility and diversity, e.g. assuming a fixed number of items or favoring safe but boring recommendations. In this paper, we propose a novel fashion collocation framework, Neural Graph Filtering , that models a flexible set of fashion items via a graph neural network. Specifically, we consider the visual embeddings of each garment as a node in the graph, and describe the inter-garment relationship as the edge between nodes. By applying symmetric operations on the edge vectors, this framework allows varying numbers of inputs/outputs and is invariant to their ordering. We further include a style classifier augmented with focal loss to enable the collocation of significantly diverse styles, which are inherently imbalanced in the training set. To facilitate a comprehensive study on diverse fashion collocation, we reorganize Amazon Fashion dataset with carefully designed evaluation protocols. We evaluate the proposed approach on three popular benchmarks, the Polyvore dataset, the Polyvore-D dataset, and our reorganized Amazon Fashion dataset. Extensive experimental results show that our approach significantly outperforms the state-of-the-art methods with over 10% improvements on the standard AUC metric. More importantly, 82.5% of the users prefer our diverse-style recommendations over other alternatives in a real-world perception study.},
  archive      = {J_TMM},
  author       = {Xin Liu and Yongbin Sun and Ziwei Liu and Dahua Lin},
  doi          = {10.1109/TMM.2020.3018021},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2894-2901},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning diverse fashion collocation by neural graph filtering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive arbitrary multiresolution decomposition for
multiscale geometric analysis. <em>TMM</em>, <em>23</em>, 2883–2893. (<a
href="https://doi.org/10.1109/TMM.2020.3017921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonsubsampled Laplacian pyramid (NSLP) is widely used as a common multiresolution decomposition method in various nonsubsampled image transforms. However, the NSLP has a fixed spectrum partition, and thus cannot represent images accurately, and flexibly. We propose a new adaptive arbitrary multiresolution decomposition to solve these problems. First, using the affine characteristics of a pseudopolar Fourier transform (PPFT), we apply a 1-D nonuniform filter bank to the modulated PPFT to obtain a 2-D arbitrary resolution filter bank. This filter surpasses the limitation of fixed spectrum partitioning of the traditional tree structure. We then demonstrate that the proposed method satisfies the compact frame condition, and has translation invariance, and a linear phase. Furthermore, we propose an adaptive spectrum division approach at various scales based on image spectrum information based on a 2-D filter bank of arbitrary multiresolution, so our method can capture important visual information more accurately. Finally, we combine our method with a nonsubsampled directional filter bank of the nonsubsampled contourlet transform to create a new multiscale geometric analysis (MGA) method, and verify that the new method can have perfect reconstruction properties. The new MGA also performs better in image denoising, and recognition experiments than state-of-the-art MGA methods with translation invariance.},
  archive      = {J_TMM},
  author       = {Zhengzhi Lu and Guoan Yang and Junjie Yang and Yuhao Wang},
  doi          = {10.1109/TMM.2020.3017921},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2883-2893},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An adaptive arbitrary multiresolution decomposition for multiscale geometric analysis},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine granularity access in interactive compression of
360-degree images based on rate-adaptive channel codes. <em>TMM</em>,
<em>23</em>, 2868–2882. (<a
href="https://doi.org/10.1109/TMM.2020.3017890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new interactive compression scheme for omnidirectional images. This requires two characteristics: efficient compression of data, to lower the storage cost, and random access ability to extract part of the compressed stream requested by the user (for reducing the transmission rate). For efficient compression, data needs to be predicted by a series of references that have been pre-defined and compressed. This contrasts with the spirit of random accessibility. We propose a solution for this problem based on incremental codes implemented by rate-adaptive channel codes. This scheme encodes the image while adapting to any user request and leads to an efficient coding that is flexible in extracting data depending on the available information at the decoder. Therefore, only the information that is needed to be displayed at the user&#39;s side is transmitted during the user&#39;s request, as if the request was already known at the encoder. The experimental results demonstrate that our coder obtains a better transmission rate than the state-of-the-art tile-based methods at a small cost in storage. Moreover, the transmission rate grows gradually with the size of the request and avoids a staircase effect, which shows the perfect suitability of our coder for interactive transmission.},
  archive      = {J_TMM},
  author       = {Navid Mahmoudian Bidgoli and Thomas Maugey and Aline Roumy},
  doi          = {10.1109/TMM.2020.3017890},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2868-2882},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine granularity access in interactive compression of 360-degree images based on rate-adaptive channel codes},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Hardness-aware dictionary learning: Boosting dictionary for
recognition. <em>TMM</em>, <em>23</em>, 2857–2867. (<a
href="https://doi.org/10.1109/TMM.2020.3017916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation is a powerful tool in many visual applications since images can be represented effectively and efficiently with a dictionary. Conventional dictionary learning methods usually treat each training sample equally, which would lead to the degradation of recognition performance when the samples from same category distribute dispersedly. This is because the dictionary focuses more on easy samples (known as highly clustered samples), and those hard samples (known as widely distributed samples) are easily ignored. As a result, the test samples which exhibit high dissimilarities to most of intra-category samples tend to be misclassified. To circumvent this issue, this paper proposes a simple and effective hardness-aware dictionary learning (HADL) method, which considers training samples discriminatively based on the AdaBoost mechanism. Different from learning one optimal dictionary, HADL learns a set of dictionaries and corresponding sub-classifiers jointly in an iterative fashion. In each iteration, HADL learns a dictionary and a sub-classifier, and updates the weights based on the classification errors given by current sub-classifier. Those correctly classified samples are assigned with small weights while those incorrectly classified samples are assigned with large weights. Through the iterated learning procedure, the hard samples are associated with different dictionaries. Finally, HADL combines the learned sub-classifiers linearly to form a strong classifier, which improves the overall recognition accuracy effectively. Experiments on well-known benchmarks show that HADL achieves promising classification results.},
  archive      = {J_TMM},
  author       = {Lichun Wang and Shuang Li and Shaofan Wang and Dehui Kong and Baocai Yin},
  doi          = {10.1109/TMM.2020.3017916},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2857-2867},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hardness-aware dictionary learning: Boosting dictionary for recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DLGAN: Depth-preserving latent generative adversarial
network for 3D reconstruction. <em>TMM</em>, <em>23</em>, 2843–2856. (<a
href="https://doi.org/10.1109/TMM.2020.3017924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep networks based methods outperform traditional 3D reconstruction methods which require multiocular images or class labels to recover the full 3D geometry, they may produce incomplete recovery and unfaithful reconstruction when facing occluded parts of 3D objects. To address these issues, we propose Depth-preserving Latent Generative Adversarial Network (DLGAN) which consists of 3D Encoder-Decoder based GAN (EDGAN, serving as a generator and a discriminator) and Extreme Learning Machine (ELM, serving as a classifier) for 3D reconstruction from a monocular depth image of an object. Firstly, EDGAN decodes a latent vector from the 2.5D voxel grid representation of an input image, and generates the initial 3D occupancy grid under common GAN losses, a latent vector loss and a depth loss. For the latent vector loss, we design 3D deep AutoEncoder (AE) to learn a target latent vector from ground truth 3D voxel grid and utilize the vector to penalize the latent vector encoded from the input 2.5D data. For the depth loss, we utilize the input 2.5D data to penalize the initial 3D voxel grid from 2.5D views. Afterwards, ELM transforms float values of the initial 3D voxel grid to binary values under a binary reconstruction loss. Experimental results show that DLGAN not only outperforms several state-of-the-art methods by a large margin on both a synthetic dataset and a real-world dataset, but also predicts more occluded parts of 3D objects accurately without class labels.},
  archive      = {J_TMM},
  author       = {Caixia Liu and Dehui Kong and Shaofan Wang and Jinghua Li and Baocai Yin},
  doi          = {10.1109/TMM.2020.3017924},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2843-2856},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DLGAN: Depth-preserving latent generative adversarial network for 3D reconstruction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StyleGuide: Zero-shot sketch-based image retrieval using
style-guided image generation. <em>TMM</em>, <em>23</em>, 2833–2842. (<a
href="https://doi.org/10.1109/TMM.2020.3017918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of zero-shot sketch-based image retrieval is to retrieve relevant images from a search set against a hand-drawn sketch query, which belongs to a class, previously unseen by the model. The knowledge gap between such unseen and seen classes along with the domain-gap between the query and search-set makes the problem extremely challenging. In this work, we address this problem by proposing a novel retrieval methodology, StyleGuide using style-guided fake -image generation. In addition, we further study the scenario of generalized zero-shot sketch-based image retrieval, where the search set contains images from both seen and unseen categories. Specifically, we propose a detection approach for unseen class samples in the search-set, based on pre-computed seen class-prototypes, to obtain a refined search-set for a particular unseen-class query. Thus, the query sketch needs to be compared only to those image data which are more likely to belong to the unseen classes, resulting in improved retrieval performance. Extensive experiments on two large-scale sketch-image datasets, Sketchy extended and TU-Berlin show that the proposed approach performs better or comparable to the state-of-the-art for ZS-SBIR and gives significant improvements over the state-of-the-art for generalized ZS-SBIR.},
  archive      = {J_TMM},
  author       = {Titir Dutta and Anurag Singh and Soma Biswas},
  doi          = {10.1109/TMM.2020.3017918},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2833-2842},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {StyleGuide: Zero-shot sketch-based image retrieval using style-guided image generation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An accurate, robust visual odometry and detail-preserving
reconstruction system. <em>TMM</em>, <em>23</em>, 2820–2832. (<a
href="https://doi.org/10.1109/TMM.2020.3017886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking and mapping functions in a monocular SLAM system remain active due to their challenging nature. In this paper, we propose a novel approach to perform the accurate and robust ego-motion estimation and provide the detail-preserving reconstruction in indoor environments. More specifically, we design a new algorithm called synchronous event measurement (SEM) to create event-based difference images (EDIs) so as to highlight frame-to-frame (F2F) difference. The observation indicates that F2F difference is highly correlated with the camera&#39;s motion change. We hereby feed EDIs into a deep convolutional neural network, in order to infer ego-motion of the camera. Subsequently, based on a monocular reconstruction framework (REMODE), we devise an algorithm named event region search or briefly ERS, to reduce possibility of mismatch on the depth estimation stage. Evaluations on a variety of datasets demonstrate the satisfactory performance of our proposed method: the ego-motion estimation is more accurate than some geometric based Visual Odometry (VO) and learning based approaches. The results are robust under extreme situations, such as brightness variation and motion blur. Meanwhile, our approach can provide more precise depth map with relatively rich textural information.},
  archive      = {J_TMM},
  author       = {Xiaoxi Gong and Yuanpeng Liu and Qiaoyun Wu and Jiayi Huang and Hua Zong and Jun Wang},
  doi          = {10.1109/TMM.2020.3017886},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2820-2832},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An accurate, robust visual odometry and detail-preserving reconstruction system},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient projected frame padding for video-based point
cloud compression. <em>TMM</em>, <em>23</em>, 2806–2819. (<a
href="https://doi.org/10.1109/TMM.2020.3016894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art 2D-based dynamic point cloud (DPC) compression algorithm is the video-based point cloud compression (V-PCC) developed by the Moving Pictures Experts Group (MPEG). It first projects the DPC patch by patch from 3D to 2D and organizes the projected patches into a video. The video is then efficiently compressed by High Efficiency Video Coding. However, there are many unoccupied pixels that may have a significant influence on the coding efficiency. These unoccupied pixels are currently padded using either the average of 4-neighbors for the geometry or the push-pull algorithm for the color attribute. While these algorithms are simple, the unoccupied pixels are not handled in the most efficient way. In this paper, we divide the unoccupied pixels into two groups: those that should be occupied and those that should not be occupied according to the occupancy map. We then design padding algorithms tailored to each group to improve the rate-distortion performance of the V-PCC reference software, for both the geometry and the color attribute. The first group is the unoccupied pixels that should be occupied according to the block-based occupancy map. We attempt to pad those pixels using the real points in the original DPC to improve the quality of the reconstructed DPC. Additionally, we attempt to maintain the smoothness of each block so as not to negatively influence the video compression efficiency. The second group is the unoccupied pixels that were correctly identified as unoccupied according to the block-based occupancy map. These pixels are useless for improving the reconstructed quality of the DPC. Therefore, we attempt to minimize the bit cost of these pixels without considering their reconstruction qualities. The bit cost is determined by the residue of these pixels obtained by subtracting the prediction pixels from the original pixels. Therefore, we propose padding the residue using the average residue of the occupied pixels in order to minimize the bit cost. The proposed algorithms are implemented in the V-PCC and the corresponding HEVC reference software. The experimental results show the proposed algorithms can bring significant bitrate savings compared with the V-PCC.},
  archive      = {J_TMM},
  author       = {Li Li and Zhu Li and Shan Liu and Houqiang Li},
  doi          = {10.1109/TMM.2020.3016894},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2806-2819},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient projected frame padding for video-based point cloud compression},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the representativity of art paintings.
<em>TMM</em>, <em>23</em>, 2794–2805. (<a
href="https://doi.org/10.1109/TMM.2020.3016887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Art painting evaluation is sophisticated for a novice with no or limited knowledge on art criticism, and history. In this study, we propose the concept of representativity to evaluate paintings instead of using professional concepts, such as genre, media, and style, which may be confusing to non-professionals. We define the concept of representativity to evaluate quantitatively the extent to which a painting can represent the characteristics of an artists creations. We begin by proposing a novel deep representation of art paintings, which is enhanced by style information through a weighted pooling feature fusion module. In contrast to existing feature extraction approaches, the proposed framework embeds painting styles, and authorship information, and learns specific artwork characteristics in a single framework. Subsequently, we propose a graph-based learning method for representativity learning, which considers intra-category, and extra-category information. In view of the significance of historical factors in the art domain, we introduce the creation time of a painting into the learning process. User studies demonstrate our approach helps the public effectively access the creation characteristics of artists through sorting paintings by representativity from highest to lowest.},
  archive      = {J_TMM},
  author       = {Yingying Deng and Fan Tang and Weiming Dong and Chongyang Ma and Feiyue Huang and Oliver Deussen and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3016887},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2794-2805},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring the representativity of art paintings},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A matrix factorization based framework for fusion of
physical and social sensors. <em>TMM</em>, <em>23</em>, 2782–2793. (<a
href="https://doi.org/10.1109/TMM.2020.3016222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our world is witnessing the on-going substantial increase in the number of multimodal physical and social sensors that are ubiquitously distributed and observing or reporting what is happening in their surroundings. These sensors provide massive amounts of spatio-temporal digital footprints which can be analyzed for various tasks such as event detection or situation awareness. However, inherent noise due to the nature of these sensors result in imprecise data and hence imprecise analysis. Also, the heterogeneous data from different modalities, formats and sources make interpreting different levels of information a big challenge. To overcome these limitations, we propose a novel unified matrix factorization-based model to fuse physical and social sensor signals for spatio-temporal analysis. Readings of physical sensor signals are represented by a spatio-temporal situation matrix, which then incorporates social content that can provide explanations for the signal strengths. We test our framework on large-scale real-world data including PSI stations data, traffic CCTV camera images, and tweets for situation prediction as well as for filtering noise to detect events of diverse situations. The experimental results suggest that the proposed matrix factorization approach can utilize the sources correlation, resulting in better performances in various situational understanding tasks.},
  archive      = {J_TMM},
  author       = {Yuhui Wang and Francesco Gelli and Christian von derWeth and Mohan Kankanhalli},
  doi          = {10.1109/TMM.2020.3016222},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2782-2793},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A matrix factorization based framework for fusion of physical and social sensors},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep unsupervised binary descriptor learning through
locality consistency and self distinctiveness. <em>TMM</em>,
<em>23</em>, 2770–2781. (<a
href="https://doi.org/10.1109/TMM.2020.3016122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been successfully applied to learn local feature descriptors in recent years. However, most of existing methods are supervised methods relying on a large number of labeled training patches, which are also proposed for learning real valued descriptors. In this paper, we propose a novel unsupervised deep learning method for binary descriptor learning. The binary descriptors are much more compact and efficient than the real valued descriptors and unsupervised leaning is highly required in many applications due to its label-free characteristic as the annotations are sometimes expensive to obtain. The core idea of our method is to explore the locality consistency in the descriptor space as well as to distinguish different patches while maintaining the ability to match a patch with its geometric transformed ones. We also give a theorical analysis about the role of batch normalization in learning effective binary descriptors. Benefited from this analysis, there is no need to append two additional losses on minimizing the quantization error and maximizing the entropy to the final learning objective like previous works did, thus simplifying our network training. Experiments on four benchmarks demonstrate that the proposed method is able to learn binary descriptors significantly outperforming previous unsupervised binary descriptors, even superior to most supervised ones. Especially, it obtains 21.2% of improvement on the UBC Phototour dataset, and 19.8%, 26.7%, 26.0% of improvements for patch verification, matching, retrieval tasks respectively on the HPatches dataset compared to the previous best unsupervised method.},
  archive      = {J_TMM},
  author       = {Bin Fan and Hongmin Liu and Hui Zeng and Jiyong Zhang and Xin Liu and Junwei Han},
  doi          = {10.1109/TMM.2020.3016122},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2770-2781},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep unsupervised binary descriptor learning through locality consistency and self distinctiveness},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quality evaluation for image retargeting with instance
semantics. <em>TMM</em>, <em>23</em>, 2757–2769. (<a
href="https://doi.org/10.1109/TMM.2020.3016124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the ever-increasing demand for devices with diversified displays, image retargeting has become a prevalent technique for adaptive image resizing. In practice, the retargeting operation inevitably causes impairments in the images; thus, image retargeting quality assessment (IRQA) is urgently needed and, can be used to guide algorithm optimization, selection and design. Unlike traditional image quality assessment, image retargeting introduces geometric distortions, which typically affect high-level image semantics. With this motivation, this paper presents a quality evaluation model for image retargeting based on INstance SEMantics (INSEM). Considering that the human visual system (HVS) perceives images highly dependent on apprehensible areas and that impairments in image retargeting mainly degrade the salient instances, an image instance is utilized as the basic semantic unit, and a top-down method is devised to extract instance-level semantic features for IRQA. In addition, taking into account the influence of semantic categories on the perception of retargeting quality, we further propose Semantic-based self-adaptive pooling (SSAP) to integrate instance-based semantic features. Finally, global features are incorporated to generate quality scores that are more consistent with people&#39;s perceptions. Extensive experiments and comparisons of three public databases, in terms of both intradatabase and cross-database settings, demonstrate the superiority of the proposed metric over state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Leida Li and Yixuan Li and Jinjian Wu and Lin Ma and Yuming Fang},
  doi          = {10.1109/TMM.2020.3016124},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2757-2769},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality evaluation for image retargeting with instance semantics},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning specific and general realm feature representations
for image fusion. <em>TMM</em>, <em>23</em>, 2745–2756. (<a
href="https://doi.org/10.1109/TMM.2020.3016123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A universal fusion framework for handling multi-realm image fusion reduces the cost of manual selection in varied applications. Addressing the generality of multiple realms and the sensitivity of specific realm, we propose a novel universal framework for multi-realm image fusion through learning realm-specific and realm-general feature representations. Shared principle network, adaptive realm feature extraction strategy and realm activation mechanism are designed for facilitating high generalization of across-realm and sensitivity of specific-realm simultaneously. In addition, we present realm-specific no-reference perceptual metric losses based on the edge details and contrast for optimizing the learning process, making the fused image exhibit more specific appearance. Moreover, we collect a new multi-realm image fusion dataset (MRIF), consisting of infrared and visual images, medical images and multispectral images, to facilitate our training and testing. Experimental results show that the fused image obtained by the proposed method achieves superior performance compared with the state-of-the-art methods on MRIF and the other three datasets including infrared and visual images, medical images and remote sensing images, respectively.},
  archive      = {J_TMM},
  author       = {Fan Zhao and Wenda Zhao},
  doi          = {10.1109/TMM.2020.3016123},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2745-2756},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning specific and general realm feature representations for image fusion},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial network with multiple classifiers for open set
domain adaptation. <em>TMM</em>, <em>23</em>, 2732–2744. (<a
href="https://doi.org/10.1109/TMM.2020.3016126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to transfer knowledge from a domain with adequate labeled samples to a domain with scarce labeled samples. Prior research has introduced various open set domain adaptation settings in the literature to extend the applications of domain adaptation methods in real-world scenarios. This paper focuses on the type of open set domain adaptation setting where the target domain has both private (‘unknown classes’) label space and the shared (‘known classes’) label space. However, the source domain only has the ‘known classes’ label space. Prevalent distribution-matching domain adaptation methods are inadequate in such a setting that demands adaptation from a smaller source domain to a larger and diverse target domain with more classes. For addressing this specific open set domain adaptation setting, prior research introduces a domain adversarial model that uses a fixed threshold for distinguishing known from unknown target samples and lacks at handling negative transfers. We extend their adversarial model and propose a novel adversarial domain adaptation model with multiple auxiliary classifiers. The proposed multi-classifier structure introduces a weighting module that evaluates distinctive domain characteristics for assigning the target samples with weights which are more representative to whether they are likely to belong to the known and unknown classes to encourage positive transfers during adversarial training and simultaneously reduces the domain gap between the shared classes of the source and target domains. A thorough experimental investigation shows that our proposed method outperforms existing domain adaptation methods on a number of domain adaptation datasets.},
  archive      = {J_TMM},
  author       = {Tasfia Shermin and Guojun Lu and Shyh Wei Teng and Manzur Murshed and Ferdous Sohel},
  doi          = {10.1109/TMM.2020.3016126},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2732-2744},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial network with multiple classifiers for open set domain adaptation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal cross-domain 3D model retrieval. <em>TMM</em>,
<em>23</em>, 2721–2731. (<a
href="https://doi.org/10.1109/TMM.2020.3015554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in 3D modeling technologies such as 3D scanning, reconstruction and printing produce an explosive increasing of 3D models, consequently 3D model management becomes urgent to facilitate related applications such as CAD, VR/AR and autonomous driving. However, we usually lack the labels of the recently emerging 3D models and even have no prior knowledge toward the label set relationship between new datasets and existing labeled datasets, which makes the management challenging. In this paper, a universal cross-domain 3D model retrieval framework is proposed for utilizing the labeled 2D images or 3D models to manage unlabeled 3D models with no prior knowledge about label sets. Specifically, a sample-level weighting mechanism is adopted to automatically detect the samples from the common label set for both domains. Then, both the domain-level and class-level alignments are performed for domain adaptation. Finally, the adapted features are used for 3D model retrieval. We conduct experiments on the cross-domain 3D model retrieval dataset NTU-PSB (PSB-NTU) and image-based 3D model retrieval dataset MI3DOR, and the results validate the superiority and effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Dan Song and Tian-Bao Li and Wen-Hui Li and Wei-Zhi Nie and Wu Liu and An-An Liu},
  doi          = {10.1109/TMM.2020.3015554},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2721-2731},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Universal cross-domain 3D model retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Arbitrarily-oriented text detection in low light natural
scene images. <em>TMM</em>, <em>23</em>, 2706–2720. (<a
href="https://doi.org/10.1109/TMM.2020.3015037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text detection in low light natural scene images is challenging due to poor image quality and low contrast. Unlike most existing methods that focus on well-lit (normally daylight) images, the proposed method considers much darker natural scene images. For this task, our method first integrates spatial and frequency domain features through fusion to enhance fine details in the image. Next, we use Maximally Stable Extremal Regions (MSER) for detecting text candidates from the enhanced images. We then introduce Cloud of Line Distribution (COLD) features, which capture the distribution of pixels of text candidates in the polar domain. The extracted features are sent to a Convolution Neural Network (CNN) to correct the bounding boxes for arbitrarily oriented text lines by removing false positives. Experiments are conducted on a dataset of low light images to evaluate the proposed enhancement step. The results show our approach is more effective compared to existing methods in terms of standard quality measures, namely, BRISQE, NIQE and PIQE. In addition, experimental results on a variety of standard benchmark datasets, namely, ICDAR 2013, ICDAR 2015, SVT, Total-Text, ICDAR 2017-MLT and CTW1500, show that the proposed approach not only produces better results for low light images, at the same time it is also competitive for daylight images.},
  archive      = {J_TMM},
  author       = {Minglong Xue and Palaiahnakote Shivakumara and Chao Zhang and Yao Xiao and Tong Lu and Umapada Pal and Daniel Lopresti and Zhibo Yang},
  doi          = {10.1109/TMM.2020.3015037},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2706-2720},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Arbitrarily-oriented text detection in low light natural scene images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Staged sketch-to-image synthesis via semi-supervised
generative adversarial networks. <em>TMM</em>, <em>23</em>, 2694–2705.
(<a href="https://doi.org/10.1109/TMM.2020.3015015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based image synthesis is a challenging problem in computer graphics and vision. Existing approaches either require exact edge maps or rely on the retrieval of existing photographs, which limits their applications in real-world scenarios. Accordingly in this work, we propose a staged semi-supervised generative adversarial networks based method for sketch-to-image synthesis, which can directly generate realistic images from novice sketches. More specifically, we first adopt a conditional generative adversarial network (CGAN) to extract class-wise representations from unpaired images. These class-wise representations are then exploited and incorporated with another CGAN, which are used to generate realistic images from sketches. By incorporating the class-wise representations, our method can leverage both the general class information from unpaired images and the targeted object information from input sketches. Additionally, this network architecture also enables us to take full advantage of widely available unpaired images and learn more accurate class representations. Extensive experiments demonstrate, compared with state-of-the-art image translation methods, our approach can achieve more promising results and synthesize images with significantly better Inception Scores and Fréchet Inception Distance.},
  archive      = {J_TMM},
  author       = {Zeyu Li and Cheng Deng and Erkun Yang and Dacheng Tao},
  doi          = {10.1109/TMM.2020.3015015},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2694-2705},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Staged sketch-to-image synthesis via semi-supervised generative adversarial networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VehicleNet: Learning robust visual representation for
vehicle re-identification. <em>TMM</em>, <em>23</em>, 2683–2693. (<a
href="https://doi.org/10.1109/TMM.2020.3014488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One fundamental challenge of vehicle re-identification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of $\text{86.07}\%$ mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.},
  archive      = {J_TMM},
  author       = {Zhedong Zheng and Tao Ruan and Yunchao Wei and Yi Yang and Tao Mei},
  doi          = {10.1109/TMM.2020.3014488},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2683-2693},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VehicleNet: Learning robust visual representation for vehicle re-identification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AFNet: Temporal locality-aware network with dual structure
for accurate and fast action detection. <em>TMM</em>, <em>23</em>,
2672–2682. (<a href="https://doi.org/10.1109/TMM.2020.3014555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by Faster R-CNN, current state-of-the-art region-based action detection approaches like R-C3D and TAL-Net creatively proposed Temporal Region Proposal Network (TRPN) to generate proposals, which greatly improved action detection accuracy. However, since smooth L1 loss adopted in TRPN focuses on relative offset to pre-set anchor segments and is not sensitive enough to action boundaries and temporal regions, there is still room for improvement in temporal proposal generation. In this work, we elaborately design a Temporal Locality-Aware Network (TLAN) to learn a binary classifier using frame-level annotations. This allows our framework to effectively distinguish action instance (positive temporal regions) from background (negative temporal regions) by jointly optimizing temporal regions classification and temporal reference boxes regression, thus enabling precise localization. We further introduce a novel pooling method named Contextual Structured Spatial Temporal Pooling (CSSTP) to better exploit context and spatial-temporal information in an end-to-end fashion. Finally, TLAN and CSSTP are incorporated into a unified framework named AFNet. Extensive experiments have been conducted to evaluate the performance of our method. We achieve state-of-the-art performance on THUMOS’14 (20.6% higher than R-C3D, 6.7% higher than TAL-Net mAP @0.5) and competitive performance on Charades and ActivityNet. Besides, our inference speed reaches 1024 FPS, which is 250× faster than TAL-Net (3.5 FPS) and comparable to R-C3D (1030 FPS).},
  archive      = {J_TMM},
  author       = {Guang Chen and Can Zhang and Yuexian Zou},
  doi          = {10.1109/TMM.2020.3014555},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2672-2682},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AFNet: Temporal locality-aware network with dual structure for accurate and fast action detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust coding of encrypted images via 2D compressed sensing.
<em>TMM</em>, <em>23</em>, 2656–2671. (<a
href="https://doi.org/10.1109/TMM.2020.3014489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical scenarios, image encryption should be implemented before image compression. This leads to the requirement of compressing encrypted images. Compressed sensing (CS), a breakthrough in signal processing, has been demonstrated to be an effective method for compressing encrypted images with robustness. However, for the exiting CS-based image encryption-then-compression (ETC) systems, image encryption is usually performed by using linear operations. When linear operations are used, we cannot achieve low computational complexity and high security in the meantime. To solve this problem, a novel 2D CS (2DCS) based ETC (2DCS-ETC) scheme is proposed in this paper. First, two nonlinear operations, including global random permutation (GRP) and negative-positive transformation (NPT), are utilized to encrypt the original image for high security purpose. Second, the encrypted image is compressed by using 2DCS for low computational complexity purpose. Furthermore, a gray mapping operation is embedded prior to CS encoding. Since gray mapping strategy can reduce the dynamic range of the CS samples, this strategy is also helpful for the rate distortion (R-D) performance improvement. Third, a 2D projected gradient with embedding decryption (2DPG-ED) algorithm is proposed, which can be utilized for the original image reconstruction even if the encrypted image is not sparse anymore. Compared with the previous CS-based ETC methods, the proposed approach can simultaneously achieve high security and low computational complexity with better robustness.},
  archive      = {J_TMM},
  author       = {Bo Zhang and Di Xiao and Yong Xiang},
  doi          = {10.1109/TMM.2020.3014489},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2656-2671},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust coding of encrypted images via 2D compressed sensing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector-based feature representations for speech signals:
From supervector to latent vector. <em>TMM</em>, <em>23</em>, 2641–2655.
(<a href="https://doi.org/10.1109/TMM.2020.3014559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two basic types of feature representations for speech signals. The first type refers to probabilistic models, such as the Gaussian mixture model (GMM). The second type refers to vector-based feature representations, such as the Gaussian supervector (GSV). Since vector-based feature representations are easier to use and process, they are more popular than probabilistic model-based feature representations. In this paper, we begin by explaining the rationale behind two widely used vector-based feature representations, viz. GSV and the i-vector, and then make extensions. GSV is a supervector (SV) based on maximum a posteriori (MAP) adaptation. Its computation is simple and fast, but its dimensionality is high and fixed. While the i-vector is a latent vector (LV) based on factor analysis (FA). Although the computation can be time-consuming because of additional model parameters, its dimensionality is changeable. To generalize GSV, we propose the MAP SV, which is also based on MAP adaptation but can have an even higher dimensionality and thus carry more information. To boost the computational efficiency of the i-vector, we adopt the concept of the mixture of factor analyzers (MFA) and propose the MFA LV, which exhibits a similar flexibility in dimensionality but is faster in computation. The experimental results for speaker identification and verification tasks demonstrate that, MAP SV can be more robust than GSV, and MFALV is comparable to or even better than the i-vector in effectiveness and meanwhile maintains a higher computational efficiency. With a powerful backend, GSV and MAP SV are comparable to the i-vector and MFALV, but the latter two are more flexible in dimensionality.},
  archive      = {J_TMM},
  author       = {Yuechi Jiang and Frank H. F. Leung},
  doi          = {10.1109/TMM.2020.3014559},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2641-2655},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Vector-based feature representations for speech signals: From supervector to latent vector},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-channel deep networks for block-based image
compressive sensing. <em>TMM</em>, <em>23</em>, 2627–2640. (<a
href="https://doi.org/10.1109/TMM.2020.3014561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating deep neural networks in image compressive sensing (CS) receives intensive attentions in multimedia technology and applications recently. As deep network approaches learn the inverse mapping directly from the CS measurements, the reconstruction speed is significantly faster than the conventional CS algorithms. However, for existing network-based approaches, a CS sampling procedure has to map a separate network model. This may potentially degrade the performance of image CS with block-wise sampling because of blocking artifacts, especially when multiple sampling rates are assigned to different blocks within an image. In this paper, we develop a multi-channel deep network for block-based image CS by exploiting inter-block correlation with performance significantly exceeding the current state-of-the-art methods. The significant performance improvement is attributed to block-wise approximation but full-image removal of blocking artifacts. Specifically, with our multi-channel structure, the image blocks with a variety of sampling rates can be reconstructed in a single model. The initially reconstructed blocks are then capable of being reassembled into a full image to improve the recovered images by unrolling a hand-designed block-based CS recovery algorithm. Experimental results demonstrate that the proposed method outperforms the state-of-the-art CS methods by a large margin in terms of objective metrics and subjective visual image quality. Our source codes are available at https://github.com/siwangzhou/DeepBCS .},
  archive      = {J_TMM},
  author       = {Siwang Zhou and Yan He and Yonghe Liu and Chengqing Li and Jianming Zhang},
  doi          = {10.1109/TMM.2020.3014561},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2627-2640},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-channel deep networks for block-based image compressive sensing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group re-identification with group context graph neural
networks. <em>TMM</em>, <em>23</em>, 2614–2626. (<a
href="https://doi.org/10.1109/TMM.2020.3013531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group re-identification aims to match groups of people across disjoint cameras. In this task, the contextual information from neighbor individuals can be exploited for re-identifying each individual within the group as well as the entire group. However, compared with single person re-identification, it brings new challenges including group layout and group membership changes. Motivated by the observation that individuals who are close together are more likely to keep in the same group under different cameras than those who are far apart, we propose to model each group as a spatial K-nearest neighbor graph (SKNNG) and design a group context graph neural network (GCGNN) for graph representation learning. Specifically, for each node in the graph, the proposed GCGNN learns an embedding which aggregates the contextual information from neighbor nodes. We design multiple weighting kernels for neighborhood aggregation based on the graph properties including node in-degrees and spatial relationship attributes. We compute the similarity scores between node embeddings of two graphs for group member association and obtain the matching score between the two graphs by summing up the similarity scores of all linked node pairs. Experimental results on three public datasets show that our approach performs favorably against state-of-the-art methods and achieves high efficiency.},
  archive      = {J_TMM},
  author       = {Ji Zhu and Hua Yang and Weiyao Lin and Nian Liu and Jia Wang and Wenjun Zhang},
  doi          = {10.1109/TMM.2020.3013531},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2614-2626},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Group re-identification with group context graph neural networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind image clustering for camera source identification via
row-sparsity optimization. <em>TMM</em>, <em>23</em>, 2602–2613. (<a
href="https://doi.org/10.1109/TMM.2020.3013449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of images with the number of cameras providing those images unknown, how to blindly identify the sources of the images has been a critical problem in digital forensics. Although state-of-the-art methods have achieved impressive results, they have failed at suppressing outliers. When they deal with a noisy dataset, the performance is significantly degraded. To address this issue, we propose an optimization approach with sparsity constraints to simultaneously handle the how-many subproblem ( i.e. , the number of cameras) and the which-from-which subproblem ( i.e. , the image–camera relationship). In our approach, we first formulate the blind camera source clustering as a row-sparsity optimization problem, in which the representation errors are minimized and the outliers caused by noisy features are suppressed. Then, a new two-stage refinement method based on inter- and the intra-class differences is proposed to achieve a more accurate estimation of the number of cameras. Because strong sparsity constraints have been adopted and the interactive relationship among data points can be fully explored to distinguish the images originated from different cameras, the proposed method can effectively handle outliers. Extensive experiments on the popular Dresden dataset show that the proposed method outperforms existing methods in both identification accuracy and efficiency.},
  archive      = {J_TMM},
  author       = {Xiang Jiang and Shikui Wei and Ting Liu and Ruizhen Zhao and Yao Zhao and Heng Huang},
  doi          = {10.1109/TMM.2020.3013449},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2602-2613},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind image clustering for camera source identification via row-sparsity optimization},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven bandwidth prediction models and automated model
selection for low latency. <em>TMM</em>, <em>23</em>, 2588–2601. (<a
href="https://doi.org/10.1109/TMM.2020.3013387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s HTTP adaptive streaming solutions use a variety of algorithms to measure the available network bandwidth and predict its future values. Bandwidth prediction, which is already a difficult task, must be more accurate when lower latency is desired due to the shorter time available to react to bandwidth changes, and when mobile networks are involved due to their inherently more frequent and potentially larger bandwidth fluctuations. Any inaccuracy in bandwidth prediction results in flawed adaptation decisions, which will in turn translate into a diminished viewer experience. We propose an Automated Model for Prediction (AMP) that encompasses techniques for bandwidth prediction and model auto-selection specifically designed for low-latency live steaming with chunked transfer encoding. We first study statistical and computational intelligence techniques to implement a suite of bandwidth prediction models that can work accurately under a broad range of network conditions, and second, we introduce an automated prediction model selection method. We confirm the effectiveness of our solution through trace-driven live streaming experiments.},
  archive      = {J_TMM},
  author       = {Abdelhak Bentaleb and Ali C. Begen and Saad Harous and Roger Zimmermann},
  doi          = {10.1109/TMM.2020.3013387},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2588-2601},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Data-driven bandwidth prediction models and automated model selection for low latency},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust CAPTCHAs towards malicious OCR. <em>TMM</em>,
<em>23</em>, 2575–2587. (<a
href="https://doi.org/10.1109/TMM.2020.3013376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turing test was originally proposed to examine whether machine&#39;s behavior is indistinguishable from a human. The most popular and practical Turing test is CAPTCHA, which is to discriminate algorithm from human by offering recognition-alike questions. The recent development of deep learning has significantly advanced the capability of algorithm in solving CAPTCHA questions, forcing CAPTCHA designers to increase question complexity. Instead of designing questions difficult for both algorithm and human, this study attempts to employ the limitations of algorithm to design robust CAPTCHA questions easily solvable to human. Specifically, our data analysis observes that human and algorithm demonstrates different vulnerability to visual distortions: adversarial perturbation is significantly annoying to algorithm yet friendly to human. We are motivated to employ adversarially perturbed images for robust CAPTCHA design in the context of character-based questions. Four modules of multi-target attack, ensemble adversarial training, image preprocessing differentiable approximation, and expectation are proposed to address the characteristics of character-based CAPTCHA cracking. Qualitative and quantitative experimental results demonstrate the effectiveness of the proposed solution. We hope this study can lead to the discussions around adversarial attack/defense in CAPTCHA design and also inspire the future attempts in employing algorithm limitation for practical usage.},
  archive      = {J_TMM},
  author       = {Jiaming Zhang and Jitao Sang and Kaiyuan Xu and Shangxi Wu and Xian Zhao and Yanfeng Sun and Yongli Hu and Jian Yu},
  doi          = {10.1109/TMM.2020.3013376},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2575-2587},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust CAPTCHAs towards malicious OCR},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to generate multi-exposure stacks with cycle
consistency for high dynamic range imaging. <em>TMM</em>, <em>23</em>,
2561–2574. (<a href="https://doi.org/10.1109/TMM.2020.3013378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse tone mapping aims at recovering the lost scene radiances from a single exposure image. With the successful use of deep learning in numerous applications, many inverse tone mapping methods use convolution neural networks in a supervised manner. As these approaches are trained with many pre-fixed high dynamic range (HDR) images, they fail to flexibly expand the dynamic ranges of images. To overcome this limitation, we consider a multiple exposure image synthesis approach for HDR imaging. In particular, we propose a pair of neural networks that represent mappings between images that have exposure levels one unit apart (stop-up/down network). Therefore, it is possible to construct two positive-feedback systems to generate images with greater or lesser exposure. Compared to previous works using the conditional generative adversarial learning framework, the stop-up/down network employs HDR friendly network structures and several techniques to stabilize the training processes. Experiments on HDR datasets demonstrate the advantages of the proposed method compared to conventional methods. Consequently, we apply our approach to restore the full dynamic range of scenes agilely with only two networks and generate photorealistic images in complex lighting situations.},
  archive      = {J_TMM},
  author       = {Siyeong Lee and So Yeon Jo and Gwon Hwan An and Suk-Ju Kang},
  doi          = {10.1109/TMM.2020.3013378},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2561-2574},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to generate multi-exposure stacks with cycle consistency for high dynamic range imaging},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emotion attention-aware collaborative deep reinforcement
learning for image cropping. <em>TMM</em>, <em>23</em>, 2545–2560. (<a
href="https://doi.org/10.1109/TMM.2020.3013350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a collaborative deep reinforcement learning model for automatic image cropping (called CDRL-IC). By modeling image cropping as a decision-making process of reinforcement learning, our model could generate optimal cropping result in a few moving and zooming steps. An image with good composition is a comprehensive result by considering the relative importance of objects and also the spatial organization of visual elements. Therefore, emotion attention information which indicates the relationship and importance between objects is applied together with contextual information of color image for image cropping. In order to sufficiently use the emotion attention map and the color image, they are processed by two collaborative agents. The two agents make their primary learning separately and then share information through an information interaction module for making joint action prediction. In order to efficiently evaluate the cropping quality in the reward function, weighted Intersection Over Union (WIoU) is designed by integrating emotion attention map in the traditional IoU. Our CDRL-IC model is tested on a variety of datasets for both image cropping and thumbnail generation. The experiments show that our CDRL-IC model outperforms state-of-the-art methods on these benchmark datasets.},
  archive      = {J_TMM},
  author       = {Xiaoyan Zhang and Zhuopeng Li and Jianmin Jiang},
  doi          = {10.1109/TMM.2020.3013350},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2545-2560},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Emotion attention-aware collaborative deep reinforcement learning for image cropping},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). YuvConv: Multi-scale non-uniform convolution structure based
on YUV color model. <em>TMM</em>, <em>23</em>, 2533–2544. (<a
href="https://doi.org/10.1109/TMM.2020.3013352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since digital images are able to be encoded through the luminance-bandwidth-chrominance (YUV) mode, and the contribution of luminance information is greater than that of chrominance information for human visual perception, it can be inferred that the appropriate reduction of chrominance information in convolutional neural network does not disturb image object recognition. In this paper, we propose a new multi-scale non-uniform convolution called YuvConv, wherein the output feature map of the convolutional layer is regarded as an image. First, the output channels in the new convolution are divided into three kinds of components: Y, U, and V tensors. Then, the tensor Y is used to process luminance information, which is high-resolution and occupies more output channels. Next, the tensors U and V are low-resolution and use fewer channels to process chrominance information. Finally, the adjacent tensors (Y-U, Y-U-V, and U-V) are fused as the output of YuvConv. Experimental results indicate that the use of the YuvConv instead of the standard convolution can improve the performance of deep learning tasks, and it can also reduce memory consumption and computation cost.},
  archive      = {J_TMM},
  author       = {Youqing Xiao and Zhanchuan Cai and Xixi Yuan},
  doi          = {10.1109/TMM.2020.3013352},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2533-2544},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {YuvConv: Multi-scale non-uniform convolution structure based on YUV color model},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object-aware multimodal named entity recognition in social
media posts with adversarial learning. <em>TMM</em>, <em>23</em>,
2520–2532. (<a href="https://doi.org/10.1109/TMM.2020.3013398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) in social media posts is challenging since texts are usually short and contexts are lacking. Most recent works show that visual information can boost the NER performance since images can provide complementary contextual information for texts. However, the image-level features ignore the mapping relations between fine-grained visual objects and textual entities, which results in error detection in entities with different types. To better exploit visual and textual information in NER, we propose an adversarial gated bilinear attention neural network (AGBAN). The model jointly extracts entity-related features from both visual objects and texts, and leverages an adversarial training to map two different representations into a shared representation. As a result, domain information contained in an image can be transferred and applied for extracting named entities in the text associated with the image. Experimental results on Tweets dataset demonstrate that our model outperforms the state-of-the-art methods. Moreover, we systematically evaluate the effectiveness of the proposed gated bilinear attention network in capturing the interactions of mutimodal features visual objects and textual words. Our results indicate that the adversarial training can effectively exploit commonalities across heterogeneous data sources, which leads to improved performance in NER when compared to models purely exploiting text data or combining the image-level visual features.},
  archive      = {J_TMM},
  author       = {Changmeng Zheng and Zhiwei Wu and Tao Wang and Yi Cai and Qing Li},
  doi          = {10.1109/TMM.2020.3013398},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2520-2532},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Object-aware multimodal named entity recognition in social media posts with adversarial learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimation of quality scores from subjective tests-beyond
subjects’ MOS. <em>TMM</em>, <em>23</em>, 2505–2519. (<a
href="https://doi.org/10.1109/TMM.2020.3013349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subjective tests for the assessment of the quality of experience (QoE) are typically run with a pool of subjects providing their opinion scores using a 5-point scale. The subjects’ mean opinion score (MOS) is generally assumed as the best estimation of the average score in the target population. Indeed, for a large enough sample, we may assume that the mean of the variations across the subjects approaches zero, but this is not the case for the limited number of subjects typically considered in subjective tests. In this paper, we propose an approach based on generalized linear models (GLMs) for estimation of the population average QoE. The motivating dataset is composed of the individual scores assigned by 25 subjects to a set of gaming videos evaluated under different resolutions and compression ratios. The approach recognizes the multinomial nature of the data and allows for correlation between scores of the same subject. The resulting estimated average QoE is shown to follow more credible patterns than the MOS, particularly for higher bitrates, for which the model estimates present more coherent behavior. Similar convincing results are found on a second dataset, showing the validity of the approach.},
  archive      = {J_TMM},
  author       = {Sergio Pezzulli and Maria G. Martini and Nabajeet Barman},
  doi          = {10.1109/TMM.2020.3013349},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2505-2519},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Estimation of quality scores from subjective tests-beyond subjects’ MOS},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive graph completion based incomplete multi-view
clustering. <em>TMM</em>, <em>23</em>, 2493–2504. (<a
href="https://doi.org/10.1109/TMM.2020.3013408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, it is often that the collected multi-view data are incomplete, i.e., some views of samples are absent. Existing clustering methods for incomplete multi-view data all focus on obtaining a common representation or graph from the available views but neglect the hidden information of missing views and information imbalance of different views. To solve these problems, a novel method, called adaptive graph completion based incomplete multi-view clustering (AGC_IMC), is proposed in this paper. Specifically, AGC_IMC develops a joint framework for graph completion and consensus representation learning, which mainly contains three components, i.e., within-view preservation, between-view inferring, and consensus representation learning. To reduce the negative influence of information imbalance, AGC_IMC introduces some adaptive weights to balance the importance of different views during the consensus representation learning. Importantly, AGC_IMC has the potential to recover the similarity graphs of all views with the optimal cluster structure, which encourages it to obtain a more discriminative consensus representation. Experimental results on five well-known datasets show that AGC_IMC significantly outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jie Wen and Ke Yan and Zheng Zhang and Yong Xu and Junqian Wang and Lunke Fei and Bob Zhang},
  doi          = {10.1109/TMM.2020.3013408},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2493-2504},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive graph completion based incomplete multi-view clustering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep single image deraining via modeling haze-like effect.
<em>TMM</em>, <em>23</em>, 2481–2492. (<a
href="https://doi.org/10.1109/TMM.2020.3013383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Removing rain from images is of a great importance to various applications such as autonomous driving, drone piloting, and photo editing. Conventional methods rely on some heuristics to handcraft various priors to remove or separate rain from images. Recently, deep learning models are proposed to learn various end-to-end methods to complete this task. However, these methods might fail in obtaining satisfactory results in some real-world scenarios, especially when the captured images suffer from heavy rain that brings not only rain streaks but also a haze-like effect (caused by the accumulation of tiny raindrops). Different from most of the existing deep learning deraining methods that focus on handling rain streaks, we add a new variable to model the haze-like effect in a general model for rain, based on which a deep neural network is designed accordingly. Specifically, in our method, two branches are designed to handle rain streaks and the haze-like effect, respectively. The output of such branch structure is fed to an additional module to further enhance the performance. Three modules are trained jointly, leading to an end-to-end network, which supports a adjustment to the strength of removing the haze-like effect. Extensive experiments on several datasets show that our method outperforms several state-of-the-art methods in both objective assessment and visual quality.},
  archive      = {J_TMM},
  author       = {Yinglong Wang and Dong Gong and Jie Yang and Qinfeng Shi and Anton van denHengel and Dehua Xie and Bing Zeng},
  doi          = {10.1109/TMM.2020.3013383},
  journal      = {IEEE Transactions on Multimedia},
  month        = {8},
  pages        = {2481-2492},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep single image deraining via modeling haze-like effect},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SRD: A tree structure based decoder for online handwritten
mathematical expression recognition. <em>TMM</em>, <em>23</em>,
2471–2480. (<a href="https://doi.org/10.1109/TMM.2020.3011316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, recognition of online handwritten mathe- matical expression has been greatly improved by employing encoder-decoder based methods. Existing encoder-decoder models use string decoders to generate LaTeX strings for mathematical expression recognition. However, in this paper, we importantly argue that string representations might not be the most natural for mathematical expressions – mathematical expressions are inherently tree structures other than flat strings. For this purpose, we propose a novel sequential relation decoder (SRD) that aims to decode expressions into tree structures for online handwritten mathematical expression recognition. At each step of tree construction, a sub-tree structure composed of a relation node and two symbol nodes is computed based on previous sub-tree structures. This is the first work that builds a tree structure based decoder for encoder-decoder based mathematical expression recognition. Compared with string decoders, a decoder that better understands tree structures is crucial for mathematical expression recognition as it brings a more reasonable learning objective and improves overall generalization ability. We demonstrate how the proposed SRD outperforms state-of-the-art string decoders through a set of experiments on CROHME database, which is currently the largest benchmark for online handwritten mathematical expression recognition.},
  archive      = {J_TMM},
  author       = {Jianshu Zhang and Jun Du and Yongxin Yang and Yi-Zhe Song and Lirong Dai},
  doi          = {10.1109/TMM.2020.3011316},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2471-2480},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SRD: A tree structure based decoder for online handwritten mathematical expression recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GAC-GAN: A general method for appearance-controllable human
video motion transfer. <em>TMM</em>, <em>23</em>, 2457–2470. (<a
href="https://doi.org/10.1109/TMM.2020.3011290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human video motion transfer has a wide range of applications in multimedia, computer vision, and graphics. Recently, due to the rapid development of Generative Adversarial Networks (GANs), there has been significant progress in the field. However, almost all existing GAN-based works are prone to address the mapping from human motions to video scenes, with scene appearances encoded individually in the trained models. Therefore, each trained model can only generate videos with a specific scene appearance, and new models are required to be trained to generate new appearances. Besides, existing works lack the capability of appearance control. For example, users have to provide video records of wearing new clothes or performing in new backgrounds to enable clothes or background changing in their synthetic videos, which greatly limits the application flexibility. In this paper, we propose General Appearance-Controllable GAN (GAC-GAN), a general method for appearance-controllable human video motion transfer. To enable general-purpose appearance synthesis, we propose to include appearance information in the conditioning inputs. Thus, once trained, our model can generate new appearances by altering the input appearance information. To achieve appearance control, we first obtain the appearance-controllable conditioning inputs, and then utilize a two-stage GAC-GAN to generate the corresponding appearance-controllable outputs, where we utilize an Appearance-Consistency GAN (ACGAN) loss, and a shadow extraction module for output foreground, and background appearance control respectively. We further build a solo dance dataset containing a large number of dance videos for training, and evaluation. Experimental results on our solo dance dataset, and iPER dataset show that our proposed GAC-GAN can not only support appearance-controllable human video motion transfer but also achieve higher video quality than state-of-art methods.},
  archive      = {J_TMM},
  author       = {Dongxu Wei and Xiaowei Xu and Haibin Shen and Kejie Huang},
  doi          = {10.1109/TMM.2020.3011290},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2457-2470},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GAC-GAN: A general method for appearance-controllable human video motion transfer},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient design and control for network-assisted
device-to-device content delivery network. <em>TMM</em>, <em>23</em>,
2442–2456. (<a href="https://doi.org/10.1109/TMM.2020.3011330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proliferation of mobile devices over the past few years has explosively increased demand for multimedia content in various service scenarios such as video and game streaming. This growth in demand for mobile content has brought a significant challenge of how to efficiently deliver such content to users. Content delivery networks (CDNs) have grown in popularity as the most feasible solution to overcome this challenge. Many mobile network operators have tried to develop customized CDN solutions suited to their network systems in practice. In this paper, we focus on an intriguing paradigm where mobile devices are used as cache servers and localized content sharing is activated via device-to-device (D2D) communication. As the most feasible network system, we consider a network-assisted D2D CDN where a mobile network assists and controls D2D communication processes. We first investigate inherent characteristics of the network system, resulting in the following findings: 1) D2D communication utilization and 2) network assisted control. As a new approach to devise efficient CDN design and control algorithms that simultaneously consider the inherent characteristics, we adopt the Lyapunov stability theory. As a result, we can derive efficient algorithms including content placement, content request routing, and content scheduling as a total solution including CDN design as well as control based on a cross-layer design approach. Through packet simulations, it is demonstrated that the performance yielded by the proposed strategies is superior to that delivered by other strategies in terms of delay to directly represent content delivery performance.},
  archive      = {J_TMM},
  author       = {Jihoon Sung and Dujeong Lee},
  doi          = {10.1109/TMM.2020.3011330},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2442-2456},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient design and control for network-assisted device-to-device content delivery network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint cross-modal and unimodal features for RGB-d salient
object detection. <em>TMM</em>, <em>23</em>, 2428–2441. (<a
href="https://doi.org/10.1109/TMM.2020.3011327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D salient object detection is one of the basic tasks in computer vision. Most existing models focus on investigating efficient ways of fusing the complementary information from RGB and depth images for better saliency detection. However, for many real-life cases, where one of the input images has poor visual quality or contains affluent saliency cues, fusing cross-modal features does not help to improve the detection accuracy, when compared to using unimodal features only. In view of this, a novel RGB-D salient object detection model is proposed by simultaneously exploiting the cross-modal features from the RGB-D images and the unimodal features from the input RGB and depth images for saliency detection. To this end, a Multi-branch Feature Fusion Module is presented to effectively capture the cross-level and cross-modal complementary information between RGB-D images, as well as the cross-level unimodal features from the RGB images and the depth images separately. On top of that, a Feature Selection Module is designed to adaptively select those highly discriminative features for the final saliency prediction from the fused cross-modal features and the unimodal features. Extensive evaluations on four benchmark datasets demonstrate that the proposed model outperforms the state-of-the-art approaches by a large margin.},
  archive      = {J_TMM},
  author       = {Nianchang Huang and Yi Liu and Qiang Zhang and Jungong Han},
  doi          = {10.1109/TMM.2020.3011327},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2428-2441},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint cross-modal and unimodal features for RGB-D salient object detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained image captioning with global-local
discriminative objective. <em>TMM</em>, <em>23</em>, 2413–2427. (<a
href="https://doi.org/10.1109/TMM.2020.3011317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made in recent years in image captioning, an active topic in the fields of vision and language. However, existing methods tend to yield overly general captions and consist of some of the most frequent words/phrases, resulting in inaccurate and indistinguishable descriptions (see Fig. 1). This is primarily due to (i) the conservative characteristic of traditional training objectives that drives the model to generate correct but hardly discriminative captions for similar images and (ii) the uneven word distribution of the ground-truth captions, which encourages generating highly frequent words/phrases while suppressing the less frequent but more concrete ones. In this work, we propose a novel global-local discriminative objective that is formulated on top of a reference model to facilitate generating fine-grained descriptive captions. Specifically, from a global perspective, we design a novel global discriminative constraint that pulls the generated sentence to better discern the corresponding image from all others in the entire dataset. From the local perspective, a local discriminative constraint is proposed to increase attention such that it emphasizes the less frequent but more concrete words/phrases, thus facilitating the generation of captions that better describe the visual details of the given images. We evaluate the proposed method on the widely used MS-COCO dataset, where it outperforms the baseline methods by a sizable margin and achieves competitive performance over existing leading approaches. We also conduct self-retrieval experiments to demonstrate the discriminability of the proposed method.},
  archive      = {J_TMM},
  author       = {Jie Wu and Tianshui Chen and Hefeng Wu and Zhi Yang and Guangchun Luo and Liang Lin},
  doi          = {10.1109/TMM.2020.3011317},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2413-2427},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained image captioning with global-local discriminative objective},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QoE-driven UAV-enabled pseudo-analog wireless video
broadcast: A joint optimization of power and trajectory. <em>TMM</em>,
<em>23</em>, 2398–2412. (<a
href="https://doi.org/10.1109/TMM.2020.3011319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive demands for high quality mobile video services have caused heavy overload to the existing cellular networks. Although the small cell has been proposed to alleviate such a problem, the network operators may not be interested in deploying numerous base stations (BSs) due to expensive infrastructure construction and maintenance. The unmanned aerial vehicles (UAVs) can provide the low-cost and quick deployment, which can support high-quality line-of-sight communications and have become promising mobile BSs. In this paper, we propose a quality-of-experience (QoE)-driven UAV-enabled pseudo-analog wireless video broadcast scheme, which provides mobile video broadcast services for ground users (GUs). Due to limited energy available in UAV, the aim of the proposed scheme is to maximize the minimum peak signal-to-noise ratio (PSNR) of GUs’ video reconstruction quality by jointly optimizing the transmission power allocation strategy and the UAV trajectory. Firstly, the reconstructed video quality at GUs is defined under the constraints of the UAV&#39;s total energy and motion mechanism, and the proposed scheme is formulated as a complex non-convex optimization problem. Then, the optimization problem is simplified to obtain a tractable suboptimal solution with the help of the block coordinate descent model and the successive convex approximation model. Finally, the experimental results are presented to show the effectiveness of the proposed scheme. Specifically, the proposed scheme can achieve over 1.6 dB PSNR gains in terms of GUs’ minimum PSNR, compared with the state-of-the-art schemes, e.g., DVB, SoftCast, and SharpCast.},
  archive      = {J_TMM},
  author       = {Xiao-Wei Tang and Xin-Lin Huang and Fei Hu},
  doi          = {10.1109/TMM.2020.3011319},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2398-2412},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {QoE-driven UAV-enabled pseudo-analog wireless video broadcast: A joint optimization of power and trajectory},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning coarse-to-fine graph neural networks for video-text
retrieval. <em>TMM</em>, <em>23</em>, 2386–2397. (<a
href="https://doi.org/10.1109/TMM.2020.3011288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of video-text retrieval that searches videos via natural language description or vice versa. Most state-of-the-art methods only consider cross-modal learning for two or three data points in isolation, ignoring to get benefit from the structural information of other data points from a global view. In this paper, we propose to exploit the comprehensive relationships among cross-modal samples via Graph Neural Networks (GNN). To improve the discriminative ability for accurately finding the positive sample, a Coarse-to-Fine GNN is constructed, which can progressively optimize the retrieval results via multi-step reasoning. Specifically, we first adopt heuristic edge features to represent relationships. Then we design a scoring module in each layer to rank the edges connected to the query node and drop the edges with lower scores. Finally, to alleviate the class imbalance issue, we propose a random-drop focal loss to optimize the whole framework. Extensive experimental results show that our method consistently outperforms the state-of-the-arts on four benchmarks.},
  archive      = {J_TMM},
  author       = {Wei Wang and Junyu Gao and Xiaoshan Yang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3011288},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2386-2397},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning coarse-to-fine graph neural networks for video-text retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal chosen-ciphertext attack for a family of image
encryption schemes. <em>TMM</em>, <em>23</em>, 2372–2385. (<a
href="https://doi.org/10.1109/TMM.2020.3011315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, there has been considerable popularity in employing nonlinear dynamics and permutation-substitution structures for image encryption. Three procedures generally exist in such image encryption schemes: the key schedule module for producing encryption elements, permutation for image scrambling and substitution for pixel modification. This paper cryptanalyzes a family of image encryption schemes that adopt pixel-level permutation and modular addition-based substitution. The security analysis first reveals a common defect in the studied image encryption schemes. Specifically, the mapping from the differentials of the ciphertexts to those of the plaintexts is found to be linear and independent of the key schedules, permutation techniques and encryption rounds. On this theory basis, a universal chosen-ciphertext attack is further proposed. Experimental results demonstrate that the proposed attack can recover the plaintexts of the studied image encryption schemes without a security key or any encryption elements. Related cryptographic discussions are also given.},
  archive      = {J_TMM},
  author       = {Junxin Chen and Lei Chen and Yicong Zhou},
  doi          = {10.1109/TMM.2020.3011315},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2372-2385},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Universal chosen-ciphertext attack for a family of image encryption schemes},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward multi-modal conditioned fashion image translation.
<em>TMM</em>, <em>23</em>, 2361–2371. (<a
href="https://doi.org/10.1109/TMM.2020.3009500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Having the capability to synthesize photo-realistic fashion product images conditioned on multiple attributes or modalities would bring many new exciting applications. In this work, we propose an end-to-end network architecture that built upon a new generative adversarial network for automatically synthesizing photo-realistic images of fashion products under multiple conditions. Given an input pose image that consists of a 2D skeleton pose and a sentence description of products, our model synthesizes a fashion image preserving the same pose and wearing the fashion products described as the text. Specifically, the generator $G$ tries to generate realistic-looking fashion images based on a $\langle \mathsf {pose}, \mathsf {text} \rangle$ pair condition to fool the discriminator. An attention network is added for enhancing the generator, which predicts a probability map indicating which part of the image needs to be attended for translation. In contrast, the discriminator $D$ distinguishes real images from the translated ones based on the input pose image and text description. The discriminator is divided into two multi-scale sub-discriminators for improving image distinguishing task. Quantitative and qualitative analysis demonstrates that our method is capable of synthesizing realistic images that retain the poses of given images while matching the semantics of provided sentence descriptions.},
  archive      = {J_TMM},
  author       = {Xiaoling Gu and Jun Yu and Yongkang Wong and Mohan S. Kankanhalli},
  doi          = {10.1109/TMM.2020.3009500},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2361-2371},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Toward multi-modal conditioned fashion image translation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance-level heterogeneous domain adaptation for
limited-labeled sketch-to-photo retrieval. <em>TMM</em>, <em>23</em>,
2347–2360. (<a href="https://doi.org/10.1109/TMM.2020.3009476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although sketch-to-photo retrieval has a wide range of applications, it is costly to obtain paired and rich-labeled ground truth. Differently, photo retrieval data is easier to acquire. Therefore, previous works pre-train their models on rich-labeled photo retrieval data (i.e., source domain) and then fine-tune them on the limited-labeled sketch-to-photo retrieval data (i.e., target domain). However, without co-training source and target data, source domain knowledge might be forgotten during the fine-tuning process, while simply co-training them may cause negative transfer due to domain gaps. Moreover, identity label spaces of source data and target data are generally disjoint and therefore conventional category-level Domain Adaptation (DA) is not directly applicable. To address these issues, we propose an Instance-level Heterogeneous Domain Adaptation (IHDA) framework. We apply the fine-tuning strategy for identity label learning, aiming to transfer the instance-level knowledge in an inductive transfer manner. Meanwhile, labeled attributes from the source data are selected to form a shared label space for source and target domains. Guided by shared attributes, DA is utilized to bridge cross-dataset domain gaps and heterogeneous domain gaps, which transfers instance-level knowledge in a transductive transfer manner. Experiments show that our method has set a new state of the art on three sketch-to-photo image retrieval benchmarks without extra annotations, which opens the door to train more effective models on limited-labeled heterogeneous image retrieval tasks.},
  archive      = {J_TMM},
  author       = {Fan Yang and Yang Wu and Zheng Wang and Xiang Li and Sakriani Sakti and Satoshi Nakamura},
  doi          = {10.1109/TMM.2020.3009476},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2347-2360},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instance-level heterogeneous domain adaptation for limited-labeled sketch-to-photo retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HAPGN: Hierarchical attentive pooling graph network for
point cloud segmentation. <em>TMM</em>, <em>23</em>, 2335–2346. (<a
href="https://doi.org/10.1109/TMM.2020.3009499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among different 3D data representations, point cloud stands out for its efficiency and flexibility. Hence, many researchers have been involved in the point cloud analysis recently. Existing approaches for point cloud segmentation task typically suffer from two limitations: 1) They usually treat different neighbor points as equals which cannot characterize the correlation between the center point and its neighborhoods well. Moreover, different parts may have different local structures for a point cloud, but they just learn a single representation space which is not sufficient and stable. 2) They often capture hierarchical information by heuristic sampling approaches which cannot reveal the spatial relationships of points well to learn global features. To overcome these limitations, we propose a novel hierarchical attentive pooling graph network (HAPGN) which utilizes the gated graph attention network (GGAN) and hierarchical graph pooling module (HiGPool) as building blocks for point cloud segmentation. Specifically, GGAN can highlight not only the importance of different neighbor points but also the importance of different representation spaces to enhance the local feature extraction. HiGPool is a novel pooling module that can capture the spatial layouts of points to learn the hierarchical features adequately. Experimental results on the ShapeNet part dataset and S3DIS dataset show that HAPGN can achieve superior performance over the state-of-the-art segmentation approaches. Furthermore, we also combine our proposed HiGPool with some recent approaches for point cloud classification and achieve better results on the ModelNet40 dataset.},
  archive      = {J_TMM},
  author       = {Chaofan Chen and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3009499},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2335-2346},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HAPGN: Hierarchical attentive pooling graph network for point cloud segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heterogeneous community question answering via social-aware
multi-modal co-attention convolutional matching. <em>TMM</em>,
<em>23</em>, 2321–2334. (<a
href="https://doi.org/10.1109/TMM.2020.3009491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, community-based question answering (CQA) systems are popular and have accumulated a large number of questions and answers provided by users. How to accurately match relevant answers for a given question is an essential function in CQA tasks. Recent effective methods utilize word-pair interactions between questions and answers for CQA matching. However, these approaches usually encode questions and answers independently and ignore the fact that they can complement and enhance each other to provide better representations and thus more implicit interactions can be captured. In addition, the visual information, social information and the variable-length problem are usually ignored by most existing approaches. In this paper, a Social-aware Multi-modal Co-attention Convolutional Matching method (SMCACM) is proposed, which models the multi-modal content and social context of questions and answers in a unified framework for CQA matching. A novel co-attention network is proposed to extract complementary information from questions and answers to enhance each other for obtaining better representations, through which our model can capture more implicit interactions between questions and answers. In addition to textual content, our model uses object detection techniques and a meta-path based heterogeneous social representation learning approach to take advantage of the visual content and social context in CQA systems, respectively. Finally, a pooling-based convolutional matching network is designed to infer the matching score based on the complemented questions and answers, which can accept variable-length answers as inputs without padding or cutting. Experimental results on two real-world datasets demonstrate the superior performance of SMCACM compared with other state-of-the-art algorithms.},
  archive      = {J_TMM},
  author       = {Jun Hu and Shengsheng Qian and Quan Fang and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3009491},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2321-2334},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Heterogeneous community question answering via social-aware multi-modal co-attention convolutional matching},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving jigsaw puzzles via nonconvex quadratic programming
with the projected power method. <em>TMM</em>, <em>23</em>, 2310–2320.
(<a href="https://doi.org/10.1109/TMM.2020.3009501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jigsaw puzzles consist of reconstructing a picture that has been divided into many interlocking pieces. This paper describes an automatic global method for solving the square-piece jigsaw puzzle problem in which neither the orientations nor the locations of the jigsaw pieces are known. This hard combinatorial sorting task is formulated as a nonconvex quadratic programming problem that is solved via the projected power method. Specifically, this work aims to specify the locations and orientations of puzzle pieces by maximizing a constrained quadratic function that resolves an optimized permutation matrix composed of the noisy pairwise affinities between jigsaw pieces. The experimental results obtained in the MIT, McGill and Pomeranz datasets indicate that our method outperforms state-of-the-art techniques.},
  archive      = {J_TMM},
  author       = {Fang Yan and Yuanjie Zheng and Jinyu Cong and Liu Liu and Dacheng Tao and Sujuan Hou},
  doi          = {10.1109/TMM.2020.3009501},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2310-2320},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Solving jigsaw puzzles via nonconvex quadratic programming with the projected power method},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image denoising using superpixel-based PCA. <em>TMM</em>,
<em>23</em>, 2297–2309. (<a
href="https://doi.org/10.1109/TMM.2020.3009502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising is a fundamental task in image processing, aimed at estimating an unknown image from its noisy observation. In this paper, we develop a computationally simple paradigm for image denoising using superpixel grouping and principal component analysis (PCA) of similar patches within the superpixels. Our method comprises three steps. First, we perform a superpixel segmentation on the noisy images. Next, similar patches within the superpixels are grouped in order to preserve the local image structures. Finally, each group of similar patches is factorized by PCA transform and estimated by performing coefficient shrinkage in the PCA domain to remove the noise. The proposed method exploits the optimal energy compaction property of PCA on groups of similar patches in the least squares sense. The performance of our approach is experimentally verified on a variety of synthetic images at various noise levels, and on real world noisy images. Our proposed method achieves very competitive denoising performance, especially in preserving the fine image structures, compared with many existing denoising algorithms with respect to both objective measurement and visual evaluation. We also show that our proposed method is computationally more efficient than other local PCA based methods.},
  archive      = {J_TMM},
  author       = {Sree Ramya S. P. Malladi and Sundaresh Ram and Jeffrey J. Rodríguez},
  doi          = {10.1109/TMM.2020.3009502},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2297-2309},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image denoising using superpixel-based PCA},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Separable reversible data hiding for encrypted
three-dimensional models based on spatial subdivision and space
encoding. <em>TMM</em>, <em>23</em>, 2286–2296. (<a
href="https://doi.org/10.1109/TMM.2020.3009492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding for encrypted media not only preserves the privacy of the media content but also can convey additional information during message transmission. Some studies advocate separability, that is, the message can be correctly extracted irrespective of whether the encrypted media has been decrypted. However, current research has focused on encrypted images. Urgent research is required on encrypted three-dimensional (3D) models. This paper proposes a separable reversible data hiding method based on spatial subdivision and space encoding for encrypted 3D models. A bounding volume is first constructed using the vertices with boundary values in the processing model. Each vertex coordinate value is then converted into a ratio (between 0 and 1) of the distance between the vertex and minimum boundary point to the side length of the bounding volume. The owner of the 3D model then uses a secret key to encrypt all ratios except those of the boundary vertices to obtain an encrypted 3D model of the same size as the original model. The spatial subdivision technique and a subdivision threshold are subsequently used to divide the bounding volume into a series of blocks and simultaneously control the vertex distortion. The secret message is embedded in the encrypted vertex by using the space encoding method with an embedding threshold. Experimental results indicate that the proposed algorithm enables high privacy, performs separable reversible data hiding, and has low computational complexity, high embedding capacity, and controllable distortion.},
  archive      = {J_TMM},
  author       = {Yuan-Yu Tsai},
  doi          = {10.1109/TMM.2020.3009492},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2286-2296},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Separable reversible data hiding for encrypted three-dimensional models based on spatial subdivision and space encoding},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enabling artistic control over pattern density and stroke
strength. <em>TMM</em>, <em>23</em>, 2273–2285. (<a
href="https://doi.org/10.1109/TMM.2020.3009484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable results and numerous advancements in neural style transfer, achieving artistic control is still a challenging feat, primarily since existing methodologies treat the style representation as a black-box model. This oversight significantly limits the range of possible artistic manipulations. In this paper, we propose a method to enable artistic control on any correlation-based style transfer models along with guiding intuitions. Our focus is on controlling two perceptual factors: Pattern Density and Stroke Strength. To achieve this, we introduce the centered Gram style representation and manipulate it with our variance-aware adaptive weighting and correlation-based selective masking. Through several experiments and comparisons with the state-of-the-art, we show that we can achieve artistic control with competitive stylization quality. Additionally, since our method involves manipulating style representation, it can easily be adapted to popular style transfer models. We analyze different style representation properties to propose rules that govern the style transfer process, which is critical towards achieving artistic control over pattern density and stroke strength.},
  archive      = {J_TMM},
  author       = {John Jethro Virtusio and Daniel Stanley Tan and Wen-Huang Cheng and M. Tanveer and Kai-Lung Hua},
  doi          = {10.1109/TMM.2020.3009484},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2273-2285},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enabling artistic control over pattern density and stroke strength},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning low-rank sparse representations with robust
relationship inference for image memorability prediction. <em>TMM</em>,
<em>23</em>, 2259–2272. (<a
href="https://doi.org/10.1109/TMM.2020.3009485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image memorability prediction aims to estimate the degree to which an image will be remembered by observers. Generally, the core problem in image memorability prediction is how to obtain effective representations to characterize the visual content of an image. In contrast to existing methods, which focus more on exploring the factors that make images memorable, in this paper, we first propose a general framework for learning joint low-rank and sparse principal feature representations, called the LSPFR framework, to obtain the lowest-rank intrinsic representation for image memorability prediction. By considering the joint optimization of the nuclear and $\ell _{1}$ -norms, the global low-rank structure and the local patterns embedded in data can be exploited to make the learned features more robust and informative. To improve our framework based on the exploitation of sample relationship structure information, we present an extended version of LSPFR, named E-LSPFR, in which the underlying relationship structure matrix is inferred through a negative log-likelihood term with a sparsity constraint. The results of experiments conducted on four publicly available datasets confirm the superior performance of our proposed approaches.},
  archive      = {J_TMM},
  author       = {Peiguang Jing and Yuechen Shang and Liqiang Nie and Yuting Su and Jing Liu and Meng Wang},
  doi          = {10.1109/TMM.2020.3009485},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2259-2272},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning low-rank sparse representations with robust relationship inference for image memorability prediction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial special section on hybrid human-artificial
intelligence for multimedia computing. <em>TMM</em>, <em>23</em>,
2185–2187. (<a href="https://doi.org/10.1109/TMM.2021.3091731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on hybrid human-artificial intelligene (AI) for multimedia computing. Multimedia computing has experienced a tremendous growth in the last decades, with applications ranging from multimedia information retrieval and analysis to multimedia compression and communication. However, the increasing volume and complexity of multimedia data driven by the large-scale spread of various new devices and sensors is posing a serious challenge to traditional multimedia computing algorithms. Artificial intelligence (AI), in particular deep learning techniques, has improved the performance of multimedia computing algorithms for many tasks, including computer vision and natural language processing. But unlike humans, AI is poor at solving tasks across multiple domains or in dealing with an uncontrolled dynamic environment. Hybrid Human-Artificial Intelligence (HH-AI) is an emerging field that aims at combining the benefits of human intelligence, such as semantic association, inference, and generalization with the computing power of AI.},
  archive      = {J_TMM},
  author       = {Raouf Hamzaoui and Huansheng Ning and Chonggang Wang and Reza Malekian and Wei Ding},
  doi          = {10.1109/TMM.2021.3091731},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2185-2187},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guest editorial special section on hybrid human-artificial intelligence for multimedia computing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large factor image super-resolution with cascaded
convolutional neural networks. <em>TMM</em>, <em>23</em>, 2172–2184. (<a
href="https://doi.org/10.1109/TMM.2020.3008041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural networks (CNNs) have attracted considerable attention in single image super-resolution (SISR) and have enabled great performance improvements. However, most of the existing methods super-resolve input images to the desired size with an interpolation operation during the beginning stage, which brings about heavy aliasing artifacts and high computational costs. Especially for large upsampling factors (e.g., 8×), it remains a challenge to restore high-quality results for deeply degraded images. To tackle this problem, we propose a cascaded super-resolution convolutional neural network (CSRCNN), which takes a single low-resolution (LR) image as an input and reconstructs high-resolution (HR) images in a progressive way. At each cascaded level, to help converge and improve the accuracy, a novel U-net based block with backprojection is first introduced, which exploits the mutual relation between HR and LR feature spaces. A refined block following the U-net block is also used to reconstruct the realistic texture details. In addition, we naturally utilize the strategy of curriculum learning, organizing the learning process from easy (small factors) to hard (large factors). Comprehensive experiments on benchmark datasets demonstrate that the proposed network achieves superior results compared with those of other state-of-the-art methods, particularly with the 8× upsampling factor.},
  archive      = {J_TMM},
  author       = {Dongyang Zhang and Jie Shao and Zhenwen Liang and Lianli Gao and Heng Tao Shen},
  doi          = {10.1109/TMM.2020.3008041},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2172-2184},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Large factor image super-resolution with cascaded convolutional neural networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STGL: Spatial-temporal graph representation and learning for
visual tracking. <em>TMM</em>, <em>23</em>, 2162–2171. (<a
href="https://doi.org/10.1109/TMM.2020.3008035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking-by-detection framework has been normally adopted in visual tracking methods. It aims to localize the visual target object with a bounding box. However, the bounding box is usually difficult to describe the target object accurately and thus easily introduces noisy background information, which usually degrades the final tracking results. Recently, weighted patch representation of the object has been shown very effectively for suppressing the undesirable background information and thus can obviously improve the tracking results. In this paper, we propose a novel Spatial-Temporal Graph representation and Learning (STGL) model to generate a kind of robust target representation for visual tracking problem. The main aspect of STGL is that it aims to exploit both spatial (within each frame) and temporal (between consecutive frames) structure of patches simultaneously in a unified graph representation and semi-supervised learning model. Comparing with existing works, STGL naturally exploits the learned representation of object in previous frame and thus can obtain the representation of object in current frame more accurately and robustly. A new ADMM algorithm is derived to solve the proposed STGL model. Based on the proposed object representation, we then adapt the structured SVM by introducing scale estimation to achieve object tracking. Extensive experiments show that our method outperforms the state-of-the-art patch based tracking methods on two standard benchmark datasets.},
  archive      = {J_TMM},
  author       = {Bo Jiang and Yuan Zhang and Bin Luo and Xiaochun Cao and Jin Tang},
  doi          = {10.1109/TMM.2020.3008035},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2162-2171},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {STGL: Spatial-temporal graph representation and learning for visual tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving generative modelling in VAEs using multimodal
prior. <em>TMM</em>, <em>23</em>, 2153–2161. (<a
href="https://doi.org/10.1109/TMM.2020.3008053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a conditional generative modelling (CGM) approach for unsupervised disentangled representation learning using variational autoencoder (VAE). CGM employs a multimodal/categorical conditional prior distribution in the latent space to learn global uncertainty in data by modelling the variations at local level. Thus, the proposed framework enforces the model to independently estimate the inherent patterns within each category, which improves the interpretability of the latent representations learned by the VAE model. The evidence lower bound objective for training the generative model is maximized using a mutual information criterion between the global latent categorical variable and the encoded inputs. Further, the approach has a built-in mechanism for bounding the information flow between the encoder and the decoder which addresses the problems of posterior collapse in conventional VAE models. Experiments on a variety of datasets demonstrate that our objective can learn disentangled representations and the proposed approach achieves competitive results on various task such as generative modelling, image classification and image denoising.},
  archive      = {J_TMM},
  author       = {Vinayak Abrol and Pulkit Sharma and Arijit Patra},
  doi          = {10.1109/TMM.2020.3008053},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2153-2161},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving generative modelling in VAEs using multimodal prior},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind image denoising via dynamic dual learning.
<em>TMM</em>, <em>23</em>, 2139–2152. (<a
href="https://doi.org/10.1109/TMM.2020.3008057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing discriminative learning methods for image denoising use either a single residual learning or a nonresidual learning design. However, we observe that these two schemes perform differently with the same noise level, and yet, there have been no explorations regarding whether residual or nonresidual designs are better suited for denoising. Additionally, many discriminative denoisers are designed to learn a model that corresponds to a fixed noise level, which means that multiple models are required to recover corrupted images with noise at different levels. In this paper, we propose a dynamic dual learning network for blind image denoising, namely, DualBDNet. Instead of modeling a sole task prediction network, the proposed DualBDNet investigates the inherent relations between the residual estimation and the nonresidual estimation. In particular, DualBDNet produces task-dependent feature maps, and each part of the features is devoted to one specific task (residual/nonresidual mapping). To address different noise levels with a single network or even cases where the statistics of noise are unknown, we further introduce an embedded subnetwork into DualBDNet. One output of the subnetwork is the learning of a dynamic compositional attention to highlight the more significant task-dependent feature maps, adaptively coinciding with the extent of corruption. The other output is the learning of a weight used for fusion of the results to ensure an end-to-end manner. Extensive experiments demonstrate that the proposed DualBDNet outperforms the state-of-the-art methods on both synthetic and real noisy images without estimating the noise levels as input.},
  archive      = {J_TMM},
  author       = {Yong Du and Guoqiang Han and Yinjie Tan and Chufeng Xiao and Shengfeng He},
  doi          = {10.1109/TMM.2020.3008057},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2139-2152},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind image denoising via dynamic dual learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint intermediate domain generation and distribution
alignment for 2D image-based 3D objects retrieval. <em>TMM</em>,
<em>23</em>, 2127–2138. (<a
href="https://doi.org/10.1109/TMM.2020.3008056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2D image-based 3D object retrieval provides a convenient way to manage 3D big data with easily accessed 2D images. It is also a challenging task due to the significant differences between 2D images and 3D objects. In this paper, we propose a 2D image-based 3D object retrieval method, which can reduce the distribution discrepancy between 2D images and 3D objects and learn invariant features between them. Specifically, we first construct an intermediate domain module based on maximum mean discrepancy (MMD) in an unsupervised way, which can reduce the 2D and 3D distribution discrepancy by marginal distribution constraint. Second, to further reduce conditional distribution discrepancy and learn invariant features, we use source domain labels as semantic information to dynamically guide distribution alignment. Moreover, in order to support the research in 3D object retrieval, we contribute a new dataset, MDI3D. We conducted extensive experiments on MDI3D and some popular datasets, such as MI3DOR and SHREC2013. The experimental results demonstrate the superiority of the proposed method by comparing with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yuting Su and Yuqian Li and Dan Song and Anan Liu and Jie Nie},
  doi          = {10.1109/TMM.2020.3008056},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2127-2138},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint intermediate domain generation and distribution alignment for 2D image-based 3D objects retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning deep multi-level similarity for thermal infrared
object tracking. <em>TMM</em>, <em>23</em>, 2114–2126. (<a
href="https://doi.org/10.1109/TMM.2020.3008028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep Thermal InfraRed (TIR) trackers only use semantic features to represent the TIR object, which lack the sufficient discriminative capacity for handling distractors. This becomes worse when the feature extraction network is only trained on RGB images. To address this issue, we propose a multi-level similarity model under a Siamese framework for robust TIR object tracking. Specifically, we compute different pattern similarities using the proposed multi-level similarity network. One of them focuses on the global semantic similarity and the other computes the local structural similarity of the TIR object. These two similarities complement each other and hence enhance the discriminative capacity of the network for handling distractors. In addition, we design a simple while effective relative entropy based ensemble subnetwork to integrate the semantic and structural similarities. This subnetwork can adaptive learn the weights of the semantic and structural similarities at the training stage. To further enhance the discriminative capacity of the tracker, we propose a large-scale TIR video sequence dataset for training the proposed model. To the best of our knowledge, this is the first and the largest TIR object tracking training dataset to date. The proposed TIR dataset not only benefits the training for TIR object tracking but also can be applied to numerous TIR visual tasks. Extensive experimental results on three benchmarks demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Qiao Liu and Xin Li and Zhenyu He and Nana Fan and Di Yuan and Hongpeng Wang},
  doi          = {10.1109/TMM.2020.3008028},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2114-2126},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning deep multi-level similarity for thermal infrared object tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Subjective and objective quality assessment for stereoscopic
image retargeting. <em>TMM</em>, <em>23</em>, 2100–2113. (<a
href="https://doi.org/10.1109/TMM.2020.3008054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binocular stereoscopic image retargeting (SIR) aims to adjust 3D images into target aspect ratios. In recent years, various SIR methods have been proposed, but there are few researches on visual quality assessment. As a consequence, we construct a benchmark stereoscopic image retargeting quality assessment database (NBU-SIRQA), which contains 720 stereoscopic retargeted images generated by eight representative SIR operators. Subjective test is conducted to obtain the mean opinion score (MOS) for each stereoscopic retargeted image. Additionally, we propose an objective SIRQA metric based on grid deformation and information loss (GDIL). The main idea of GDIL is to decompose the SIR operator into two transformations: monocular image retargeting transformation and viewpoint transformation. In each transformation, grid deformation and information loss are extracted simultaneously to represent image quality and 3D perception quality. Experimental results validated on our established NBU-SIRQA database show the superiority of our metric in measuring the quality of stereoscopic retargeted images over the existing approaches.},
  archive      = {J_TMM},
  author       = {Zhenqi Fu and Feng Shao and Qiuping Jiang and Xiangchao Meng and Yo-Sung Ho},
  doi          = {10.1109/TMM.2020.3008054},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2100-2113},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Subjective and objective quality assessment for stereoscopic image retargeting},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding more about human and machine attention in deep
neural networks. <em>TMM</em>, <em>23</em>, 2086–2099. (<a
href="https://doi.org/10.1109/TMM.2020.3007321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human visual system can selectively attend to parts of a scene for quick perception, a biological mechanism known as Human attention . Inspired by this, recent deep learning models encode attention mechanisms to focus on the most task-relevant parts of the input signal for further processing, which is called Machine/Neural/Artificial attention . Understanding the relation between human and machine attention is important for interpreting and designing neural networks. Many works claim that the attention mechanism offers an extra dimension of interpretability by explaining where the neural networks look. However, recent studies demonstrate that artificial attention maps do not always coincide with common intuition. In view of these conflicting evidence, here we make a systematic study on using artificial attention and human attention in neural network design. With three example computer vision tasks (i.e., salient object segmentation, video action recognition, and fine-grained image classification), diverse representative backbones (i.e., AlexNet, VGGNet, ResNet) and famous architectures (i.e., Two-stream, FCN), corresponding real human gaze data, and systematically conducted large-scale quantitative studies, we quantify the consistency between artificial attention and human visual attention and offer novel insights into existing artificial attention mechanisms by giving preliminary answers to several key questions related to human and artificial attention mechanisms. Overall results demonstrate that human attention can benchmark the meaningful ‘ground-truth’ in attention-driven tasks, where the more the artificial attention is close to human attention, the better the performance; for higher-level vision tasks, it is case-by-case. It would be advisable for attention-driven tasks to explicitly force a better alignment between artificial and human attention to boost the performance; such alignment would also improve the network explainability for higher-level computer vision tasks.},
  archive      = {J_TMM},
  author       = {Qiuxia Lai and Salman Khan and Yongwei Nie and Hanqiu Sun and Jianbing Shen and Ling Shao},
  doi          = {10.1109/TMM.2020.3007321},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2086-2099},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Understanding more about human and machine attention in deep neural networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RealVAD: A real-world dataset and a method for voice
activity detection by body motion analysis. <em>TMM</em>, <em>23</em>,
2071–2085. (<a href="https://doi.org/10.1109/TMM.2020.3007350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an automatic voice activity detection (VAD) method that is solely based on visual cues. Unlike traditional approaches processing audio, we show that upper body motion analysis is desirable for the VAD task. The proposed method consists of components for body motion representation, feature extraction from a Convolutional Neural Network (CNN) architecture and unsupervised domain adaptation. The body motion representations as images are used by the feature extraction component, which is generic and person-invariant, thus, can be applied to a subject who has never been seen. The endmost component handles the domain-shift problem, which appears due to the fact that the way people move/ gesticulate while speaking might vary from subject to subject, which results in disparate body motion features and consequently poorer VAD performance. The experimental analyses applied on a publicly available real-world VAD dataset show that the proposed method performs better than the state-of-the-art video-only and multimodal VAD approaches. Moreover, the proposed method has a better generalization ability as VAD results are more consistent across different subjects. As another major contribution, we present a new multimodal dataset (called RealVAD), created from a real-world (no role-plays) panel discussion. This dataset contains many actual situations/ challenges that are missing in the previous VAD datasets. We benchmarked the RealVAD dataset by applying the proposed method as well as cross-dataset analyses. Particularly, the results of cross-dataset experiments highlight the remarkable positive contribution of the unsupervised domain adaptation applied.},
  archive      = {J_TMM},
  author       = {Cigdem Beyan and Muhammad Shahid and Vittorio Murino},
  doi          = {10.1109/TMM.2020.3007350},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2071-2085},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RealVAD: A real-world dataset and a method for voice activity detection by body motion analysis},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold transfer learning via discriminant regression
analysis. <em>TMM</em>, <em>23</em>, 2056–2070. (<a
href="https://doi.org/10.1109/TMM.2020.3007340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transfer learning, how to effectively transfer useful information from the source domain to the target domain is crucial. In this paper, we propose a novel transfer learning method for image classification, named manifold transfer learning via discriminant regression analysis (MTL-DRA), to transfer the local geometry structure information from the source domain to the target domain and ensure that the transform matrix is robust or sparse so that samples from different domains can be well combined. In MTL-DRA, we encode discriminant information of the source domain to the target domain by introducing between- and within-class graphs to preserve within-class similarity and reduce between-class similarity. With different norms as constraints, MTL-DRA overcomes the disturbance of noise and avoids negative transfer learning. To improve the robustness of MTL-DRA, we encode a nuclear norm constraint and propose robust MTL-DRA (RMTL-DRA). We analyzed the convergence and complexity of the two proposed methods. To verify the performance of the proposed methods, we conducted extensive experiments on five public image benchmarks. The experimental results show that the proposed methods outperform state-of-the-art transfer learning methods.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Wenjing Wang and Chun Yuan and Xuelong Li and Zhihui Lai},
  doi          = {10.1109/TMM.2020.3007340},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2056-2070},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Manifold transfer learning via discriminant regression analysis},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic context encoding for accurate 3D point cloud
segmentation. <em>TMM</em>, <em>23</em>, 2045–2055. (<a
href="https://doi.org/10.1109/TMM.2020.3007331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic context plays a significant role in image segmentation. However, few prior works have explored semantic contexts for 3D point cloud segmentation. In this paper, we propose a simple yet effective Point Context Encoding (PointCE) module to capture semantic contexts of a point cloud and adaptively highlight intermediate feature maps. We also introduce a Semantic Context Encoding loss (SCE-loss) to supervise the network to learn rich semantic context features. To avoid hyperparameter tuning and achieve better convergence performance, we further propose a geometric mean loss to integrate both SCE-loss and segmentation loss. Our PointCE module is general and lightweight, and can be integrated into any point cloud segmentation architecture to improve its segmentation performance with only marginal extra overheads. Experimental results on the ScanNet, S3DIS and Semantic3D datasets show that consistent and significant improvement can be achieved for several different networks by integrating our PointCE module.},
  archive      = {J_TMM},
  author       = {Hao Liu and Yulan Guo and Yanni Ma and Yinjie Lei and Gongjian Wen},
  doi          = {10.1109/TMM.2020.3007331},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2045-2055},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic context encoding for accurate 3D point cloud segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised emotion intensity prediction for
recognition of emotions in images. <em>TMM</em>, <em>23</em>, 2033–2044.
(<a href="https://doi.org/10.1109/TMM.2020.3007352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of emotions in images is attracting increasing research attention. Recent studies show that using local region information helps to improve the recognition performance. Intuitively, emotion intensity maps provide more detailed information than image regions. Inspired by this intuition, we propose an end-to-end deep neural network for image emotion recognition leveraging emotion intensity learning. The proposed network is composed of a first classification stream, an intensity prediction stream and a second classification stream. The intensity prediction stream is built on top of the feature pyramid network to extract multilevel features. The class activation mapping technique is used to generate pseudo intensity maps from the first classification stream to guide the proposed network for emotion intensity learning. The predicted intensity map is integrated into the second classification stream for final emotion recognition. The three streams are trained cooperatively to improve the performance. We evaluate the proposed network for both emotion recognition and sentiment classification on different benchmark datasets. The experimental results demonstrate that the proposed network achieves improved performance compared to previous state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Haimin Zhang and Min Xu},
  doi          = {10.1109/TMM.2020.3007352},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2033-2044},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly supervised emotion intensity prediction for recognition of emotions in images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-dependent propagating-based video recommendation in
multimodal heterogeneous information networks. <em>TMM</em>,
<em>23</em>, 2019–2032. (<a
href="https://doi.org/10.1109/TMM.2020.3007330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of online social networks (OSNs), video recommendation has come to play a crucial role in mitigating the semantic gap between users and videos. Conventional approaches to video recommendation primarily focus on exploiting content features or simple user-video interactions to model the users’ preferences. Although these methods have achieved promising results, they fail to model the complex video context interdependency, which is obscure/hidden in heterogeneous auxiliary data from OSNs. In this paper, we study the problem of video recommendation in Heterogeneous Information Networks (HINs) due to its excellence in characterizing heterogeneous and complex context information. We propose a Context-Dependent Propagating Recommendation network (CDPRec) to obtain accurate video embedding and capture global context cues among videos in HINs. The CDPRec can iteratively propagate the contexts of a video along links in a graph-structured HIN and explore multiple types of dependencies among the surrounding video nodes. Then, each video is represented as the composition of the multimodal content feature and global dependency structure information using an attention network. The learned video embedding with sequential based recommendation are jointly optimized for the final rating prediction. Experimental results on real-world YouTube video recommendation scenarios demonstrate the effectiveness of the proposed methods compared with strong baselines.},
  archive      = {J_TMM},
  author       = {Lei Sang and Min Xu and Shengsheng Qian and Matt Martin and Peter Li and Xindong Wu},
  doi          = {10.1109/TMM.2020.3007330},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2019-2032},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Context-dependent propagating-based video recommendation in multimodal heterogeneous information networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised moving object detection in complex scenes using
adversarial regularizations. <em>TMM</em>, <em>23</em>, 2005–2018. (<a
href="https://doi.org/10.1109/TMM.2020.3006419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object detection (MOD) is a fundamental step in many high-level vision-based applications, such as human activity analysis, visual object tracking, autonomous vehicles, surveillance, and security. Most of the existing MOD algorithms observe performance degradation in the presence of complex scenes containing camouflage objects, shadows, dynamic backgrounds, and varying illumination conditions, and captured by static cameras. To appropriately handle these challenges, we propose a Generative Adversarial Network (GAN) based on a moving object detection algorithm, called MOD_GAN. In the proposed algorithm, scene-specific GANs are trained in an unsupervised MOD setting, thereby enabling the algorithm to learn generating background sequences using input from uniformly distributed random noise samples. In addition to adversarial loss, during training, norm-based loss in the image space and discriminator feature-space is also minimized between the generated images and the training data. The additional losses enable the generator to learn subtle background details, resulting in a more realistic complex scene generation. During testing, a novel back-propagation based algorithm is used to generate images with statistics similar to the test images. More appropriate random noise samples are searched by directly minimizing the loss function between the test and generated images both in the image and discriminator feature-spaces. The network is not updated in this step; only the input noise samples are iteratively modified to minimize the loss function. Moreover, motion information is used to ensure that this loss is only computed on small-motion pixels. A novel dataset containing outdoor time-lapsed images from dawn to dusk with a full illumination variation cycle is also proposed to better compare the MOD algorithms in outdoor scenes. Accordingly, extensive experiments on five benchmark datasets and comparison with 30 existing methods demonstrate the strength of the proposed algorithm.},
  archive      = {J_TMM},
  author       = {Maryam Sultana and Arif Mahmood and Soon Ki Jung},
  doi          = {10.1109/TMM.2020.3006419},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {2005-2018},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised moving object detection in complex scenes using adversarial regularizations},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Character detection in animated movies using multi-style
adaptation and visual attention. <em>TMM</em>, <em>23</em>, 1990–2004.
(<a href="https://doi.org/10.1109/TMM.2020.3006372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic identification of fictional characters is one of the primary analysis techniques for video content. A common approach to detect characters in live-action movies involves detecting human faces; however, this approach cannot be used in non-realistic domains, such as animated movies. Detection of characters in animated movies presents two major challenges: the same subject of character can be expressed in various unique styles, and there are no stylistic or other restrictions on the nature and design of character objects. To address these challenges, we introduce the “animation adaptive region-based convolutional neural network” model to detect characters in animated movies and determine whether the detected characters are human or non-human types. Our model extends the Faster R-CNN model, which is a two-stage object detector, in the following manner: 1) we add a hierarchical animation adaptation module to learn the variety of unique styles from animated movies using a single model; 2) we incorporate a double-detector architecture to focus on the regions that are visually important in determining the character class. We build a new dataset for the animated character detection task. Experiments on this dataset show that our model outperforms other existing representative object detector models in terms of character detection. Furthermore, our model achieves significant performance improvements compared with previous state-of-the-art methods used for the character dictionary generation task. Our model is robust for a variety of animation styles and can find common visual representations of all types of characters, providing an effective way to detect animated characters.},
  archive      = {J_TMM},
  author       = {Hayeon Kim and Eun-Cheol Lee and Yongseok Seo and Dong-Hyuck Im and In-Kwon Lee},
  doi          = {10.1109/TMM.2020.3006372},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1990-2004},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Character detection in animated movies using multi-style adaptation and visual attention},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural collaborative preference learning with pairwise
comparisons. <em>TMM</em>, <em>23</em>, 1977–1989. (<a
href="https://doi.org/10.1109/TMM.2020.3006373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Ranking (CR), as an effective recommendation framework, has attracted increasing attention in recent years. Most CR methods simply adopt the inner product between user/item embeddings as the rating score function, with an assumption that the interacted items are preferred to non-interacted ones. However, such fixed score functions and assumptions might not be sufficient to capture the real preference ranking list from the complicated interactions in real-world data. To alleviate this issue, we develop a novel collaborative ranking framework that learns an arbitrary utility function for item ranking with user preference concerned. In the core of our framework, a neural network is employed to model the utility function for personalized ranking with the strength of its nonlinearity. On top of this, we further adopt a pairwise ranking loss for user-item pairs to preserve the preference order of items for users. Besides, such a utility function enables us to generate the final top- $K$ preference list in a much easier way. Finally, extensive experiments on four real-world datasets show the validity of our proposed method.},
  archive      = {J_TMM},
  author       = {Zhaopeng Li and Qianqian Xu and Yangbangyan Jiang and Ke Ma and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TMM.2020.3006373},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1977-1989},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neural collaborative preference learning with pairwise comparisons},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). M-GCN: Multi-branch graph convolution network for 2D
image-based on 3D model retrieval. <em>TMM</em>, <em>23</em>, 1962–1976.
(<a href="https://doi.org/10.1109/TMM.2020.3006371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2D image based 3D model retrieval is a challenging research topic in the field of 3D model retrieval. The huge gap between two modalities - 2D image and 3D model, extremely constrains the retrieval performance. In order to handle this problem, we propose a novel multi-branch graph convolution network (M-GCN) to address the 2D image based 3D model retrieval problem. First, we compute the similarity between 2D image and 3D model based on visual information to construct one cross-modalities graph model, which can provide the original relationship between image and 3D model. However, this relationship is not accurate because of the difference of modalities. Thus, the multi-head attention mechanism is employed to generate a set of fully connected edge-weighted graphs, which can predict the hidden relationship between 2D image and 3D model to further strengthen the correlation for the embedding generation of nodes. Finally, we apply the max-pooling operation to fuse the multi-graphs information and generate the fusion embeddings of nodes for retrieval. To validate the performance of our method, we evaluated M-GCN on the MI3DOR dataset, Shrec 2018 track and Shrec 2014 track. The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Wei-Zhi Nie and Min-Jie Ren and An-An Liu and Zhendong Mao and Jie Nie},
  doi          = {10.1109/TMM.2020.3006371},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1962-1976},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {M-GCN: Multi-branch graph convolution network for 2D image-based on 3D model retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An automated and robust image watermarking scheme based on
deep neural networks. <em>TMM</em>, <em>23</em>, 1951–1961. (<a
href="https://doi.org/10.1109/TMM.2020.3006415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital image watermarking is the process of embedding and extracting a watermark covertly on a cover-image. To dynamically adapt image watermarking algorithms, deep learning–based image watermarking schemes have attracted increased attention during recent years. However, existing deep learning–based watermarking methods neither fully apply the fitting ability to learn and automate the embedding and extracting algorithms, nor achieve the properties of robustness and blindness simultaneously. In this paper, a robust and blind image watermarking scheme based on deep learning neural networks is proposed. To minimize the requirement of domain knowledge, the fitting ability of deep neural networks is exploited to learn and generalize an automated image watermarking algorithm. A deep learning architecture is specially designed for image watermarking tasks, which will be trained in an unsupervised manner to avoid human intervention and annotation. To facilitate flexible applications, the robustness of the proposed scheme is achieved without requiring any prior knowledge or adversarial examples of possible attacks. A challenging case of watermark extraction from phone camera–captured images demonstrates the robustness and practicality of the proposal. The experiments, evaluation, and application cases confirm the superiority of the proposed scheme.},
  archive      = {J_TMM},
  author       = {Xin Zhong and Pei-Chi Huang and Spyridon Mastorakis and Frank Y. Shih},
  doi          = {10.1109/TMM.2020.3006415},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1951-1961},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An automated and robust image watermarking scheme based on deep neural networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised pixel-wise GAN for face super-resolution.
<em>TMM</em>, <em>23</em>, 1938–1950. (<a
href="https://doi.org/10.1109/TMM.2020.3006414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many face-related multimedia applications, low-resolution face images may greatly degrade the face recognition performance and necessitate face super-resolution (SR). Among the current SR methods, MSE-oriented SR methods often produce over-smoothed outputs and could miss some texture details while GAN-oriented SR methods may generate artifacts which are harmful to face recognition. To resolve the above issues, this paper presents a supervised pixel-wise Generative Adversarial Network (SPGAN) that can resolve a very low-resolution face image of $16\times 16$ or smaller pixel-size to its larger version of multiple scaling factors ( $2\times$ , $4\times$ , $8\times$ and even $16\times$ ) in a unified framework. Being different from traditional unsupervised discriminators which generate a single number to represent the likelihood whether the input image is real or fake, the proposed supervised pixel-wise discriminator mainly focus on whether each pixel of the generated SR face image is as photo-realistic as its corresponding pixel in the ground-truth HR (high-resolution) face image. To further improve the face recognition performance of SPGAN, we take advantage of the face identity prior by sending two inputs to the discriminator, including an input face image (either a real HR face image or its corresponding SR face image) and its face features which are extracted from a pre-trained face recognition model. Due to the introduced face identity prior, the identity-based discriminator can pay more attention to texture details which are closely related to face recognition. Extensive experiments demonstrate that the proposed SPGAN can achieve more photo-realistic SR images and higher face recognition accuracy than some state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Menglei Zhang and Qiang Ling},
  doi          = {10.1109/TMM.2020.3006414},
  journal      = {IEEE Transactions on Multimedia},
  month        = {7},
  pages        = {1938-1950},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Supervised pixel-wise GAN for face super-resolution},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural style palette: A multimodal and interactive style
transfer from a single style image. <em>TMM</em>, <em>23</em>,
2245–2258. (<a href="https://doi.org/10.1109/TMM.2021.3087026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the myriad of attributes found in a single style image, existing neural style transfer methods produce outputs with limited variety–typically only a single realization of the style image. They also do not provide an easy way to control the stylization process, limiting the creative freedom of users. In this paper, we propose Neural Style Palette ( NSP ), a method for interactively generating a variety of stylized images from only a single style input. Our approach allows human influence in the stylization process, a design inspired by Hybrid Human-Artificial Intelligence. Like a color palette, NSP enables a meaningful interaction by presenting a collection of sub-textures, which we also refer to as anchor styles, that act as a visual guide for the users. These anchor styles capture different attributes in the single style image that the users can creatively blend to create their desired realizations. To offer a diversified selection in the NSP , we constrain the anchor styles to be distant from one another while maintaining faithfulness to the original style image. This is possible through our two proposed novel losses: a style-separation loss that encourages the sub-textures to be distinct and a unification loss to ensure that the sub-textures center around the original style while encouraging additional diversity. We perform several experiments to prove the effectiveness of our method and generalize to improve existing methods.},
  archive      = {J_TMM},
  author       = {John Jethro Virtusio and Jose Jaena Mari Ople and Daniel Stanley Tan and M. Tanveer and Neeraj Kumar and Kai-Lung Hua},
  doi          = {10.1109/TMM.2021.3087026},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2245-2258},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neural style palette: A multimodal and interactive style transfer from a single style image},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep image coding scheme with generative network to learn
from correlated images. <em>TMM</em>, <em>23</em>, 2235–2244. (<a
href="https://doi.org/10.1109/TMM.2021.3087011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a method to build a deep learning image coding system based on inverse problem, choosing a suitable measurement operator to reduce the amount of information transmitted at the sender, and reconstructing the original image by tackling the inverse problem at the receiver. Unlike most compressed sensing (CS) methods, the proposed coding scheme does not rely on sparsity but uses the structural priors of the generative adversarial networks (GAN) to solve the inverse problem. The proposed model trains the GAN to learn a mapping from the latent space to the sample space formed by correlated images on the cloud. Then the measurements are used to localize the optimal latent variable in the representation space which corresponding to the original image in the sample space. The proposed method encodes and transmits the measurements instead of the original image, which greatly reduces the cost of transmission while ensuring the quality of the reconstructed the image at high compression ratios. To the best of our knowledge, this is the first time to introduce the GAN-based inverse problem in the field of the deep image coding area. The experimental results show that the visual quality of the images generated by the proposed scheme is better than the traditional encoding scheme JPEG2000. Especially in the case of extremely high compression ratios, the proposed scheme can still maintain good performance.},
  archive      = {J_TMM},
  author       = {Yihao Chen and Bin Tan and Jun Wu and Zhifeng Zhang and Haoqi Ren},
  doi          = {10.1109/TMM.2021.3087011},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {2235-2244},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A deep image coding scheme with generative network to learn from correlated images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate and efficient image super-resolution via
global-local adjusting dense network. <em>TMM</em>, <em>23</em>,
1924–1937. (<a href="https://doi.org/10.1109/TMM.2020.3005025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network-based (CNN-based) method has shown its superior performance on the image super-resolution (SR) task. However, several researches have shown that obtaining a better reconstruction result often leads to the significant increase in parameters and computation. To alleviate the burden in computational needs, we propose a novel global-local adjusting dense super-resolution network (GLADSR) to build a powerful yet lightweight CNN-based SR model. To enhance the network capacity, we present a global-local adjusting module (GLAM) which can adaptively reallocate the processing resources with local selective block (LSB) and global guided block (GGB). The GLAMs are linked with nested dense connections to make better use of the global-local adjusted features. In addition, we also introduce a separable pyramid upsampling (SPU) module to replace the regular upsampling operation, which thus brings a substantial reduction of its parameters and obtains better results. Furthermore, we show that the proposed refinement structure is capable of reducing image artifacts in SR processing. Extensive experiments on benchmark datasets show that the proposed GLADSR outperforms the state-of-the-art methods with much fewer parameters and much less computational cost.},
  archive      = {J_TMM},
  author       = {Xinyan Zhang and Peng Gao and Sunxiangyu Liu and Kongya Zhao and Guitao Li and Liuguo Yin and Chang Wen Chen},
  doi          = {10.1109/TMM.2020.3005025},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1924-1937},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Accurate and efficient image super-resolution via global-local adjusting dense network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion compensated virtual view synthesis using novel
particle cell. <em>TMM</em>, <em>23</em>, 1908–1923. (<a
href="https://doi.org/10.1109/TMM.2020.3004966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wide interest in advanced multimedia experience, free-viewpoint communication is being greatly developed in recent years. In the free-viewpoint communication, viewers can perceive a view from any angle and any position of a scene. Even though the preferred views are not captured, we can generate the views through virtual view synthesis that synthesizes an arbitrary view from captured reference view(s). For daily use, only one or few cameras in baseline distance are given to capture the scene that makes the virtual view synthesis challenging. The task is more difficult when the camera is continuously moving. In this paper, we propose a particle cell to model a reference view sequence to a set of moving particles for virtual view synthesis. Using our novel hybrid motion estimation scheme, the projected coordinates of particles in each frame are obtained even they are occluded. The particles are warped to a virtual view and synthesized as a virtual view sequence. Our method is applicable for both dynamic camera setting and static camera setting. The experimental results show our method outperforms the state-of-the-art algorithms in dynamic camera datasets and presents improvement in static camera datasets in general.},
  archive      = {J_TMM},
  author       = {Chi Ho Cheung and Lu Sheng and King Ngi Ngan},
  doi          = {10.1109/TMM.2020.3004966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1908-1923},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Motion compensated virtual view synthesis using novel particle cell},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoFoley: Artificial synthesis of synchronized sound tracks
for silent videos with deep learning. <em>TMM</em>, <em>23</em>,
1895–1907. (<a href="https://doi.org/10.1109/TMM.2020.3005033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In movie productions, the Foley artist is responsible for creating an overlay soundtrack that helps the movie come alive for the audience. This requires the artist identify sounds that enhance the experience for the listener, reinforcing the director&#39;s intention for the scene. The artist must decide what artificial sound captures the essence of the sound and action depicted in the scene. In this paper, we present AutoFoley, an automated deep-learning tool that is used to synthesize a representative audio track for videos. AutoFoley is used to associate audio files with soundless video or to identify critical scenarios and provide a synthesized, reinforced and time-synchronized soundtrack. Our algorithm is capable of precise recognition of actions as well as interframe relations in fast- moving video clips through incorporating interpolation technique and temporal relational networks (TRN). We employ a robust multiscale recurrent neural network (RNN) and a convolutional neural network (CNN) for better understanding of the intricate input-to-output associations. To evaluate AutoFoley, we create an audio-video dataset containing a variety of sounds frequently used as Foley effects in movies. While the Foley dataset was limited to short-duration videos off the representative activities, this dataset demonstrates the capabilities of our proposed system. We show the synthesized sounds are portrayed with accurate temporal synchronization of the associated visual inputs. Human qualitative testing of AutoFoley shows more than 73% of the test subjects considered the generated soundtrack as original, which is a noteworthy improvement in comparable cross-modal research.},
  archive      = {J_TMM},
  author       = {Sanchita Ghose and John Jeffrey Prevost},
  doi          = {10.1109/TMM.2020.3005033},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1895-1907},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AutoFoley: Artificial synthesis of synchronized sound tracks for silent videos with deep learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning feature representation and partial correlation for
multimodal multi-label data. <em>TMM</em>, <em>23</em>, 1882–1894. (<a
href="https://doi.org/10.1109/TMM.2020.3004963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-provided annotations in existing multimodal datasets sometimes are inappropriate for model learning and can hinder the task of cross-modal retrieval. To handle this issue, we propose a discriminative and noise-robust cross-modal retrieval method, called FLPCL, which consists of deep feature learning and partial correlation learning. Deep feature learning is implemented by utilizing label supervised information to guide the training of deep neural network for each modality, which aims to find modality-specific deep feature representations that preserve the similarity and discrimination information among multimodal data. Based on deep feature learning, partial correlation learning is proposed to infer direct association between different modalities by removing the effect of common underlying semantics from each modality. It is achieved by maximizing the canonical correlation of the feature representations of different modalities conditioned on the label modality. Different from existing works that build indirect association between modalities via incorporating semantic labels, our FLPCL method can learn more effective and robust multimodal latent representations by explicitly preserving both intra-modal and inter-modal relationship among multimodal data. Extensive experiments on three cross-modal datasets show that our method outperforms state-of-the-art methods on cross-modal retrieval tasks.},
  archive      = {J_TMM},
  author       = {Guoli Song and Shuhui Wang and Qingming Huang and Qi Tian},
  doi          = {10.1109/TMM.2020.3004963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1882-1894},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning feature representation and partial correlation for multimodal multi-label data},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online hashing with bit selection for image retrieval.
<em>TMM</em>, <em>23</em>, 1868–1881. (<a
href="https://doi.org/10.1109/TMM.2020.3004962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online hashing methods have been intensively investigated in semantic image retrieval due to their efficiency in learning the hash functions with one pass through the streaming data. Among the online hashing methods, those based on the target codes are usually superior to others. However, the target codes in these methods are generated heuristically in advance and cannot be learned online to capture the characteristics of the data. In this paper, we propose a new online hashing method in which the target codes are constructed according to the data characteristics and are used to learn the hash functions online. By designing a metric to select the effective bits online for constructing the target codes, the learned hash functions are resistant to the bit-flipping error. At the same time, the correlation between the hash functions is also considered in the designed metric. Hence, the hash functions have low redundancy. Extensive experiments show that our method can achieve comparable or better performance than other online hashing methods on both the static database and the dynamic database.},
  archive      = {J_TMM},
  author       = {Zhenyu Weng and Yuesheng Zhu},
  doi          = {10.1109/TMM.2020.3004962},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1868-1881},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Online hashing with bit selection for image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Shared low-rank correlation embedding for multiple feature
fusion. <em>TMM</em>, <em>23</em>, 1855–1867. (<a
href="https://doi.org/10.1109/TMM.2020.3003747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diversity of multimedia data in the real world usually forms heterogeneous types of feature sets. How to explore the structure information and the relationships among multiple features is still an open problem. In this paper, we propose an unsupervised subspace learning method, named the shared low-rank correlation embedding (SLRCE) for multiple feature fusion. First, in the learned subspace, we implement the low-rank representation on each feature set and enforce a shared low-rank constraint to uncover the common structure information of multiple features. Second, we develop an enhanced correlation analysis in the learned subspace for simultaneously removing the redundancy of each feature set and exploring the correlation of multiple features. Finally, we incorporate the shared low-rank representation and the correlation analysis into a unified framework. The shared low-rank constraint not only depicts the data distribution consistency among multiple features, but also assists robust subspace learning. Our method is robust to noise in practice and can be extended to the kernel case to handle the nonlinear feature fusion. Experimental results on several typical datasets demonstrate the superior performance of the proposed methods.},
  archive      = {J_TMM},
  author       = {Zhan Wang and Lizhi Wang and Jun Wan and Hua Huang},
  doi          = {10.1109/TMM.2020.3003747},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1855-1867},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Shared low-rank correlation embedding for multiple feature fusion},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D skeletal gesture recognition via discriminative coding on
time-warping invariant riemannian trajectories. <em>TMM</em>,
<em>23</em>, 1841–1854. (<a
href="https://doi.org/10.1109/TMM.2020.3003783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning 3D skeleton-based representation for gesture recognition has progressively stood out because of its invariance to the viewpoint and background dynamics of video. Typically, existing techniques use absolute coordinates to determine human motion features. The recognition of gestures, however, is irrespective of the position of the performer, and the extracted features should be invariant to body size. In addition, when comparing and classifying gestures, the problem of temporal dynamics can greatly distort the distance metric. In this paper, we represent a 3D skeleton as a point in the special orthogonal group $SO(3)$ product space that expressly models the 3D geometric relationships between body parts. As such, a gesture skeletal sequence can be described by a trajectory on a Riemannian manifold. Following that, we propose to generalize the transported square-root vector field to obtain a time-warping invariant metric for comparing these trajectories (identifying these gestures). Moreover, by specifically considering the labeling information with encoding, a sparse coding scheme of skeletal trajectories is presented to enforce the discriminant validity of atoms in the dictionary. Experimental results indicate that the proposed approach has achieved state-of-the-art performance on many challenging gesture recognition benchmarks.},
  archive      = {J_TMM},
  author       = {Xin Liu and Guoying Zhao},
  doi          = {10.1109/TMM.2020.3003783},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1841-1854},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3D skeletal gesture recognition via discriminative coding on time-warping invariant riemannian trajectories},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangling, embedding and ranking label cues for
multi-label image recognition. <em>TMM</em>, <em>23</em>, 1827–1840. (<a
href="https://doi.org/10.1109/TMM.2020.3003779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image recognition is a fundamental but challenging computer vision and multimedia task. Great progress has been achieved by exploiting label correlations among these multiple labels associated with a single image, which is the most crucial issue for multi-label image recognition. In this paper, to explicitly model label correlations, we propose a unified deep learning framework to Disentangle, Embed and Rank (DER) the corresponding label cues. Specifically, we first obtain class-aware disentangled maps (CADMs) by reforming deep activations in accordance with the class-specific recognition weights. Then, after transforming CADMs into the corresponding label vectors, we propose an embedding operation from a metric learning perspective to pull the relevant label vectors together and push irrelevant label vectors away. Furthermore, a ranking operation is employed, which aims to accurately and robustly measure the similarity/dissimilarity of these label vectors. Our model can be trained in an end-to-end manner with only image-level supervision, during which the proposed embedding and ranking operations can contribute to the CADMs learning through back-propagation. In addition, the obtained CADMs are aggregated and further used as an essential feature stream for the final multi-label classification. We conduct extensive experiments on three commonly used multi-label benchmark datasets. Quantitative results show that our model can significantly and consistently outperform previous competitive methods. Moreover, qualitative analysis of our DER proposal also reveals the effectiveness of our proposed model.},
  archive      = {J_TMM},
  author       = {Zhao-Min Chen and Quan Cui and Xiu-Shen Wei and Xin Jin and Yanwen Guo},
  doi          = {10.1109/TMM.2020.3003779},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1827-1840},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Disentangling, embedding and ranking label cues for multi-label image recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-FoV viewport-based visual saliency model using
adaptive weighting losses for 360<span
class="math inline"><sup>∘</sup></span> images. <em>TMM</em>,
<em>23</em>, 1811–1826. (<a
href="https://doi.org/10.1109/TMM.2020.3003642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360 $^\circ$ media allows observers to explore the scene in all directions. The consequence is that the human visual attention is guided by not only the perceived area in the viewport but also the overall content in 360 $^\circ$ . In this paper, we propose a method to estimate the 360 $^\circ$ saliency map which extracts salient features from the entire 360 $^\circ$ image in each viewport in three different Field of Views (FoVs). Our model is first pretrained with a large-scale 2D image dataset to enable the interpretation of semantic contents, then fine-tuned with a relative small 360 $^\circ$ image dataset. A novel weighting loss function attached with stretch weighted maps is introduced to adaptively weight the losses of three evaluation metrics and attenuate the impact of stretched regions in equirectangular projection during training process. Experimental results demonstrate that our model achieves better performance with the integration of three FoVs and its diverse viewport images. Results also show that the adaptive weighting losses and stretch weighted maps effectively enhance the evaluation scores compared to the fixed weighting losses solutions. Comparing to other state of the art models, our method surpasses them on three different datasets and ranks the top using 5 performance evaluation metrics on the Salient360! benchmark set. The code is available at https://github.com/FannyChao/MV-SalGAN360 .},
  archive      = {J_TMM},
  author       = {Fang-Yi Chao and Lu Zhang and Wassim Hamidouche and Olivier Déforges},
  doi          = {10.1109/TMM.2020.3003642},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1811-1826},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A multi-FoV viewport-based visual saliency model using adaptive weighting losses for 360$^\circ$ images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense video captioning using graph-based sentence
summarization. <em>TMM</em>, <em>23</em>, 1799–1810. (<a
href="https://doi.org/10.1109/TMM.2020.3003592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, dense video captioning has made attractive progress in detecting and captioning all events in a long untrimmed video. Despite promising results were achieved, most existing methods do not sufficiently explore the scene evolution within an event temporal proposal for captioning, and therefore perform less satisfactorily when the scenes and objects change over a relatively long proposal. To address this problem, we propose a graph-based partition-and-summarization (GPaS) framework for dense video captioning within two stages. For the “partition” stage, a whole event proposal is split into short video segments for captioning at a finer level. For the “summarization” stage, the generated sentences carrying rich description information for each segment are summarized into one sentence to describe the whole event. We particularly focus on the “summarization” stage, and propose a framework that effectively exploits the relationship between semantic words for summarization. We achieve this goal by treating semantic words as the nodes in a graph and learning their interactions by coupling Graph Convolutional Network (GCN) and Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN and LSTM. The effectiveness of our approach is demonstrated via an extensive comparison with the state-of-the-arts methods on the two benchmarks ActivityNet Captions dataset and YouCook II dataset.},
  archive      = {J_TMM},
  author       = {Zhiwang Zhang and Dong Xu and Wanli Ouyang and Luping Zhou},
  doi          = {10.1109/TMM.2020.3003592},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1799-1810},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dense video captioning using graph-based sentence summarization},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LD-MAN: Layout-driven multimodal attention network for
online news sentiment recognition. <em>TMM</em>, <em>23</em>, 1785–1798.
(<a href="https://doi.org/10.1109/TMM.2020.3003648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing use of both images and text to express opinions on the web leads to the need for multimodal sentiment recognition. Some commonly used social media data containing short text and few images, such as tweets and product reviews, have been well studied. However, it is still challenging to predict the readers’ sentiment after reading online news articles, since news articles often have more complicated structures, e.g., longer text and more images. To address this problem, we propose a layout-driven multimodal attention network (LD-MAN) to recognize news sentiment in an end-to-end manner. Rather than modeling text and images individually, LD-MAN uses the layout of online news to align images with the corresponding text. Specifically, it exploits a set of distance-based coefficients to model the image locations and measure the contextual relationship between images and text. LD-MAN then learns the affective representations of the articles from the aligned text and images using a multimodal attention mechanism. Considering the lack of relevant datasets in this field, we collect two multimodal online news datasets, containing a total of 14,566 articles with 56,260 images and 251,202 words. Experimental results demonstrate that the proposed method performs favorably compared with state-of-the-art approaches. We will release all the codes, models and datasets to the community.},
  archive      = {J_TMM},
  author       = {Wenya Guo and Ying Zhang and Xiangrui Cai and Lei Meng and Jufeng Yang and Xiaojie Yuan},
  doi          = {10.1109/TMM.2020.3003648},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1785-1798},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LD-MAN: Layout-driven multimodal attention network for online news sentiment recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep reinforcement polishing network for video captioning.
<em>TMM</em>, <em>23</em>, 1772–1784. (<a
href="https://doi.org/10.1109/TMM.2020.3002669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video captioning task aims to describe video content using several natural-language sentences. Although one-step encoder-decoder models have achieved promising progress, the generations always involve many errors, which are mainly caused by the large semantic gap between the visual domain and the language domain and by the difficulty in long-sequence generation. The underlying challenge of video captioning, i.e., sequence-to-sequence mapping across different domains, is still not well handled. Inspired by the proofreading procedure of human beings, the generated caption can be gradually polished to improve its quality. In this paper, we propose a deep reinforcement polishing network (DRPN) to refine the caption candidates, which consists of a word-denoising network (WDN) to revise word errors and a grammar-checking network (GCN) to revise grammar errors. On the one hand, the long-term reward in deep reinforcement learning benefits the long-sequence generation, which takes the global quality of caption sentences into account. On the other hand, the caption candidate can be considered a bridge between visual and language domains, where the semantic gap is gradually reduced with better candidates generated by repeated revisions. In experiments, we present adequate evaluations to show that the proposed DRPN achieves comparable and even better performance than the state-of-the-art methods. Furthermore, the DRPN is model-irrelevant and can be integrated into any video captioning models to refine their generated caption sentences.},
  archive      = {J_TMM},
  author       = {Wanru Xu and Jian Yu and Zhenjiang Miao and Lili Wan and Yi Tian and Qiang Ji},
  doi          = {10.1109/TMM.2020.3002669},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1772-1784},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep reinforcement polishing network for video captioning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk optimization for revenue-driven wireless video
broadcasting systems: A copula-based framework. <em>TMM</em>,
<em>23</em>, 1757–1771. (<a
href="https://doi.org/10.1109/TMM.2020.3002612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The revenue of wireless service providers (WSPs) relies on their ability to efficiently satisfy the variable demands from end users (EUs). However, emerging video services create new risks owing to the diverse content requirements of heterogeneous EUs operating in uncertain markets. It is challenging as the risks created by demands and prices are highly uncertain. In this work, a risk problem of high revenues is studied in which multiple video content items with different prices are broadcast to wireless EUs. The video content prices differ in their value functions in terms of popularity, ratings, and types. The objective is to maximize the revenue of WSPs under a certain Value-at-Risk (VaR) by adjusting the content prices and allocation of bandwidth resources. Furthermore, a VaR-based optimization framework for wireless video broadcasting systems is presented. First, the content characteristics are analyzed, and a copula model is then used to build the content value structure. The copula of a multivariate distribution corresponds to the description of the price-dependent structure. Second, a risk analysis for the effects of price fluctuations on revenues caused by uncertainty is conducted. A VaR model is associated with changes in the prices and allocated rates. Copulas are used to derive a bound on the VaR for functions of dependent risks. Subsequently, detailed representations are provided to identify the distributional bounds for revenue functions of dependent risks. Lastly, a VaR-based framework that optimizes pricing and bandwidth provision is presented. For the solution, the risk regions of WSPs are modeled as polymatroidal structures to minimize the risk caused by different service demands and variable market prices. Experiments on different price markets demonstrated that the proposed method is effective, thereby verifying the feasibility of the proposed method.},
  archive      = {J_TMM},
  author       = {Wen Ji and H. Vincent Poor},
  doi          = {10.1109/TMM.2020.3002612},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1757-1771},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Risk optimization for revenue-driven wireless video broadcasting systems: A copula-based framework},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial multimodal network for movie story question
answering. <em>TMM</em>, <em>23</em>, 1744–1756. (<a
href="https://doi.org/10.1109/TMM.2020.3002667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering by using information from multiple modalities has attracted more and more attention in recent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions) based on generative adversarial networks. Moreover, a self-attention mechanism is developed to enforce our newly introduced consistency constraint in order to preserve the self-correlation between the visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the benchmark MovieQA and TVQA datasets show the effectiveness of our proposed AMN over other published state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhaoquan Yuan and Siyuan Sun and Lixin Duan and Changsheng Li and Xiao Wu and Changsheng Xu},
  doi          = {10.1109/TMM.2020.3002667},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1744-1756},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial multimodal network for movie story question answering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to visualize music through shot sequence for
automatic concert video mashup. <em>TMM</em>, <em>23</em>, 1731–1743.
(<a href="https://doi.org/10.1109/TMM.2020.3003631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An experienced director usually switches among different types of shots to make visual storytelling more touching. When filming a musical performance, appropriate switching shots can produce some special effects, such as enhancing the expression of emotion or heating up the atmosphere. However, while the visual storytelling technique is often used in making professional recordings of a live concert, amateur recordings of audiences often lack such storytelling concepts and skills when filming the same event. Thus a versatile system that can perform video mashup to create a refined high-quality video from such amateur clips is desirable. To this end, we aim at translating the music into an attractive shot (type) sequence by learning the relation between music and visual storytelling of shots. The resulting shot sequence can then be used to better portray the visual storytelling of a song and guide the concert video mashup process. To achieve the task, we first introduces a novel probabilistic-based fusion approach, named as multi-resolution fused recurrent neural networks (MF-RNNs) with film-language, which integrates multi-resolution fused RNNs and a film-language model for boosting the translation performance. We then distill the knowledge in MF-RNNs with film-language into a lightweight RNN, which is more efficient and easier to deploy. The results from objective and subjective experiments demonstrate that both MF-RNNs with film-language and lightweight RNN can generate attractive shot sequences for music, thereby enhancing the viewing and listening experience.},
  archive      = {J_TMM},
  author       = {Wen-Li Wei and Jen-Chun Lin and Tyng-Luh Liu and Hsiao-Rong Tyan and Hsin-Min Wang and Hong-Yuan Mark Liao},
  doi          = {10.1109/TMM.2020.3003631},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1731-1743},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to visualize music through shot sequence for automatic concert video mashup},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COMO: Efficient deep neural networks expansion with
COnvolutional MaxOut. <em>TMM</em>, <em>23</em>, 1722–1730. (<a
href="https://doi.org/10.1109/TMM.2020.3002614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we extend the classic MaxOut strategy, originally designed for Multiple Layer Preceptors (MLPs), into CO nvolutional M ax O ut (COMO) — a new strategy making deep convolutional neural networks wider with parameter efficiency. Compared to the existing solutions, such as ResNeXt for ResNet or Inception for VGG-alikes, COMO works well on both linear architectures and the ones with skipped connections and residual blocks. More specifically, COMO adopts a novel split-transform-merge paradigm that extends the layers with spatial resolution reduction into multiple parallel splits. For the layer with COMO, each split passes the input feature maps through a 4D convolution operator with independent batch normalization operators for transformation, then merge into the aggregated output of the original sizes through max-pooling . Such a strategy is expected to tackle the potential classification accuracy degradation due to the spatial resolution reduction, by incorporating the multiple splits and max-pooling-based feature selection. Our experiment using a wide range of deep architectures shows that COMO can significantly improve the classification accuracy of ResNet/VGG-alike networks based on a large number of benchmark datasets. COMO further outperforms the existing solutions, e.g., Inceptions, ResNeXts, SE-ResNet, and Xception, that make networks wider, and it dominates in the comparison of accuracy versus parameter sizes.},
  archive      = {J_TMM},
  author       = {Baoxin Zhao and Haoyi Xiong and Jiang Bian and Zhishan Guo and Cheng-Zhong Xu and Dejing Dou},
  doi          = {10.1109/TMM.2020.3002614},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1722-1730},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {COMO: Efficient deep neural networks expansion with COnvolutional MaxOut},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-world cross-modal retrieval via sequential learning.
<em>TMM</em>, <em>23</em>, 1708–1721. (<a
href="https://doi.org/10.1109/TMM.2020.3002177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval is playing an increasingly important role in our daily life with the explosive growth of multimedia data. However, its learning paradigm under real-life environments is less studied, and most existing approaches are developed in the pre-desired settings (e.g., unchanging modalities and explicitly modal-aligned samples). Inspired by the recent achievement in the field of cognition mechanism on how the human brain acquires knowledge, we present a new sequential learning method for real-world cross-modal retrieval. In this method, a unified model is maintained to capture the common knowledge of various modalities but are learned in a sequential manner such that it behaves adaptively according to the evolving distribution of different modalities, and needs no laborious alignment operations among multimodal data before learning. Furthermore, we reformulate the objective of optimization-based meta-learning and propose a novel meta-learning method to overcome the catastrophic forgetting encountered in sequential learning. Extensive experiments are conducted on four popular image-text multimodal datasets and a five-modal dataset, showing that our method achieves state-of-the-art cross-modal retrieval performance without explicit modal-alignment.},
  archive      = {J_TMM},
  author       = {Ge Song and Xiaoyang Tan},
  doi          = {10.1109/TMM.2020.3002177},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1708-1721},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Real-world cross-modal retrieval via sequential learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint input and output space learning for multi-label image
classification. <em>TMM</em>, <em>23</em>, 1696–1707. (<a
href="https://doi.org/10.1109/TMM.2020.3002185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image classification aims to predict the labels associated with a given image. While most existing methods utilize unified image representations, extracting label-specific features through input space learning would improve the discriminative power of the learned features. On the other hand, most feature learning studies often ignore the learning in the output label space, although taking advantage of label correlations can boost the classification performance. In this paper, we propose a deep learning framework that incorporates flexible modules which can learn from both input and output spaces for multi-label image classification. For the input space learning, we devise a label-specific feature pooling method to refine convolutional features for obtaining features specific to each label. For the output space learning, we design a Two-Stream Graph Convolutional Network (TSGCN) to learn multi-label classifiers by mapping spatial object relationships and semantic label correlations. More specifically, we build object spatial graphs to characterize the spatial relationships among objects in an image, which supplements the label semantic graphs modelling the semantic label correlations. Experimental results on two popular benchmark datasets (i.e., Pascal VOC and MS-COCO) show that our proposed method achieves superior performance over the state-of-the-arts.},
  archive      = {J_TMM},
  author       = {Jiahao Xu and Hongda Tian and Zhiyong Wang and Yang Wang and Wenxiong Kang and Fang Chen},
  doi          = {10.1109/TMM.2020.3002185},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1696-1707},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint input and output space learning for multi-label image classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Part-aware progressive unsupervised domain adaptation for
person re-identification. <em>TMM</em>, <em>23</em>, 1681–1695. (<a
href="https://doi.org/10.1109/TMM.2020.3001522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to mitigate the domain shift that occurs when transferring knowledge from a labeled source domain to an unlabeled target domain. While it has been studied for application in unsupervised person re-identification (ReID), the relations of feature distribution across the source and target domains remain underexplored, as they either ignore the local relations or omit the in-depth consideration of negative transfer when two domains do not share identical label spaces. In light of the above, this paper presents an innovative part-aware progressive adaptation network (PPAN) that exploits global and local relations for UDA-based ReID across domains. A multi-branch network is developed that explicitly learns discriminative feature representation from both whole-body images and body-part images under the supervision of a labeled source domain. Within each network branch, an independent UDA constraint is designed that aligns the global and local feature distributions from a labeled source domain with those of an unlabeled target domain. In addition, a novel progressive adaptation strategy (PAS) is designed that effectively alleviates the negative influence of outlier source identities. The proposed unsupervised ReID model is evaluated on five widely used datasets (Market-1501, DukeMTMC-reID, CUHK03, VIPeR and PRID), and experimental results demonstrate its superior robustness and effectiveness relative to state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Fan Yang and Ke Yan and Shijian Lu and Huizhu Jia and Don Xie and Zongqiao Yu and Xiaowei Guo and Feiyue Huang and Wen Gao},
  doi          = {10.1109/TMM.2020.3001522},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1681-1695},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Part-aware progressive unsupervised domain adaptation for person re-identification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank pairwise alignment bilinear network for few-shot
fine-grained image classification. <em>TMM</em>, <em>23</em>, 1666–1680.
(<a href="https://doi.org/10.1109/TMM.2020.3001510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have demonstrated advanced abilities on various visual classification tasks, which heavily rely on the large-scale training samples with annotated ground-truth. However, it is unrealistic always to require such annotation in real-world applications. Recently, Few-Shot learning (FS), as an attempt to address the shortage of training samples, has made significant progress in generic classification tasks. Nonetheless, it is still challenging for current FS models to distinguish the subtle differences between fine-grained categories given limited training data. To filling the classification gap, in this paper, we address the Few-Shot Fine-Grained (FSFG) classification problem, which focuses on tackling the fine-grained classification under the challenging few-shot learning setting. A novel low-rank pairwise bilinear pooling operation is proposed to capture the nuanced differences between the support and query images for learning an effective distance metric. Moreover, a feature alignment layer is designed to match the support image features with query ones before the comparison. We name the proposed model Low-Rank Pairwise Alignment Bilinear Network (LRPABN), which is trained in an end-to-end fashion. Comprehensive experimental results on four widely used fine-grained classification data sets demonstrate that our LRPABN model achieves the superior performances compared to state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Huaxi Huang and Junjie Zhang and Jian Zhang and Jingsong Xu and Qiang Wu},
  doi          = {10.1109/TMM.2020.3001510},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1666-1680},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Low-rank pairwise alignment bilinear network for few-shot fine-grained image classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic example guided image-to-image translation.
<em>TMM</em>, <em>23</em>, 1654–1665. (<a
href="https://doi.org/10.1109/TMM.2020.3001536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many image-to-image (I2I) translation problems are in nature of high diversity that a single input may have various counterparts. The multi-modal network that can build a many-to-many mapping between two visual domains has been proposed in prior works. However, most of them are guided by sampled noises. Some others encode the reference image into a latent vector, which would eliminate the semantic information of the reference image. In this work, we aim to provide a solution to control the output based on references semantically. Given a reference image and an input in another domain, we first perform semantic matching between the two visual content and generate an auxiliary image, which explicitly encourages the semantic characteristic to be preserved. A deep network then is used for I2I translation and the final outputs are expected to be semantically similar to both the input and the reference. However, few paired data can satisfy that dual-similarity in a supervised fashion, and so we build up a self-supervised framework in the training stage. We improve the quality and diversity of the outputs by employing non-local blocks and a multi-task architecture. We assess the proposed method through extensive qualitative and quantitative evaluations and also present comparisons with several state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Jialu Huang and Jing Liao and Sam Kwong},
  doi          = {10.1109/TMM.2020.3001536},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1654-1665},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantic example guided image-to-image translation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive deep metric learning for affective image retrieval
and classification. <em>TMM</em>, <em>23</em>, 1640–1653. (<a
href="https://doi.org/10.1109/TMM.2020.3001527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An image is worth a thousand words. Many researchers have conducted extensive studies to understand visual emotions since an increasing number of users express emotions via images and videos online. However, most existing methods based on convolutional neural networks aim to retrieve and classify affective images in a discrete label space while ignoring both the hierarchical and complex nature of emotions. On the one hand, different from concrete and isolated object concepts ( e.g ., cat and dog), a hierarchical relationship exists among emotions. On the other hand, most widely used deep methods depend on the representation from fully connected layers, which lacks the essential texture information for recognizing emotions. In this work, we address the above problems via adaptive deep metric learning. Specifically, we design an adaptive sentiment similarity loss, which is able to embed affective images considering the emotion polarity and adaptively adjust the margin between different image pairs. To effectively distinguish affective images, we further propose the sentiment vector that captures the texture information extracted from multiple convolutional layers. Finally, we develop a unified multi-task deep framework to simultaneously optimize both retrieval and classification goals. Extensive and thorough evaluations on four benchmark datasets demonstrate that the proposed framework performs favorably against the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xingxu Yao and Dongyu She and Haiwei Zhang and Jufeng Yang and Ming-Ming Cheng and Liang Wang},
  doi          = {10.1109/TMM.2020.3001527},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1640-1653},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive deep metric learning for affective image retrieval and classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel depth and color feature fusion framework for 6D
object pose estimation. <em>TMM</em>, <em>23</em>, 1630–1639. (<a
href="https://doi.org/10.1109/TMM.2020.3001533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to solve the problem of estimating the 6D pose of an object under occlusion using RGB-D images. Most existing methods typically use the information of color and depth images separately to make predictions, which limits their performances in the presence of occlusion. Instead, we propose a pipeline to effectively fuse color and depth information and perform region-level pose estimation. Our method first uses a CNN to extract the color features, and then we obtain the fusion features by combining the color features into the point cloud. Unlike existing methods, the fusion features are in the form of point sets instead of feature maps. We further use a PointNet++-like network to process the fusion features, obtaining several region-level features. Each region-level feature can predict a pose with confidence. The pose with the highest confidence is chosen as the final output. Experiments show that the proposed method outperforms the state-of-the-art methods on both the LINEMOD and Occlusion LINEMOD datasets, indicating that the proposed pipeline can obtain accurate pose estimation results and is robust to occlusion.},
  archive      = {J_TMM},
  author       = {Guangliang Zhou and Yi Yan and Deming Wang and Qijun Chen},
  doi          = {10.1109/TMM.2020.3001533},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1630-1639},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel depth and color feature fusion framework for 6D object pose estimation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SparseFusion: Dynamic human avatar modeling from sparse RGBD
images. <em>TMM</em>, <em>23</em>, 1617–1629. (<a
href="https://doi.org/10.1109/TMM.2020.3001506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach to reconstruct 3D human body shapes based on a sparse set of RGBD frames using a single RGBD camera. We specifically focus on the realistic settings where human subjects move freely during the capture. The main challenge is how to robustly fuse these sparse frames into a canonical 3D model, under pose changes and surface occlusions. This is addressed by our new framework consisting of the following steps. First, based on a generative human template, for every two frames having sufficient overlap, an initial pairwise alignment is performed; It is followed by a global non-rigid registration procedure, in which partial results from RGBD frames are collected into a unified 3D shape, under the guidance of correspondences from the pairwise alignment; Finally, the texture map of the reconstructed human model is optimized to deliver a clear and spatially consistent texture. Empirical evaluations on synthetic and real datasets demonstrate both quantitatively and qualitatively the superior performance of our framework in reconstructing complete 3D human models with high fidelity. It is worth noting that our framework is flexible, with potential applications going beyond shape reconstruction. As an example, we showcase its use in reshaping and reposing to a new avatar.},
  archive      = {J_TMM},
  author       = {Xinxin Zuo and Sen Wang and Jiangbin Zheng and Weiwei Yu and Minglun Gong and Ruigang Yang and Li Cheng},
  doi          = {10.1109/TMM.2020.3001506},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1617-1629},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SparseFusion: Dynamic human avatar modeling from sparse RGBD images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tag propagation and cost-sensitive learning for music
auto-tagging. <em>TMM</em>, <em>23</em>, 1605–1616. (<a
href="https://doi.org/10.1109/TMM.2020.3001521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of music auto-tagging depends on the quality of training data. In practice, the links between songs and tags in the manually labeled training data can be incorrect (false positive) or missing (false negative). In this paper, we propose a cost-sensitive tag propagation learning method to improve auto-tagging. Specifically, we exploit music context to determine similar songs and propagate tags between them. Both propagated tags and original tags are used to optimize the auto-tagging models, and cost-sensitivity is incorporated into the loss function to enhance the robustness by adjusting the weight of relevant ( positive ) links with respect to irrelevant ( negative ) links. The proposed method is tested on three auto-tagging models: 2D-CNN, CRNN, and SampleCNN. The Million Song Dataset is used for training, and four music contexts, artist, playlist, tag, and listener, are used for song similarity measurement. The experimental results show 1) The proposed method can successfully improve the performance of the three auto-tagging models, 2) The cost-sensitive loss function helps reduce the impact of missing tags, and 3) The artist music context is more powerful for tag propagation than the other three music contexts.},
  archive      = {J_TMM},
  author       = {Yi-Hsun Lin and Homer H. Chen},
  doi          = {10.1109/TMM.2020.3001521},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1605-1616},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tag propagation and cost-sensitive learning for music auto-tagging},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image quality assessment using kernel sparse coding.
<em>TMM</em>, <em>23</em>, 1592–1604. (<a
href="https://doi.org/10.1109/TMM.2020.3001472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One key in image quality assessment (IQA) is the design of image representations that can capture the changes of image structures caused by distortions. Recent studies show that sparse coding has emerged as a promising approach to analyzing image structures for IQA. However, existing sparse-coding-based IQA approaches use linear coding models, which ignore the nonlinearities of manifolds of image patches and thus cannot analyze complex image structures well. To overcome such a weakness, in this paper, we introduce nonlinear sparse coding to IQA. A kernel dictionary construction scheme is proposed, which combines analytic dictionaries and learnable dictionaries to guarantee both the stability and effectiveness of kernel sparse coding in the context of IQA. Built upon the kernel dictionary construction, an effective full-reference IQA metric is developed. Benefiting from the considerations on nonlinearities during sparse coding, the proposed IQA metric not only characterizes image distortions better, but also achieves improvement on the consistency with subjective perception, when compared to the metrics built upon linear sparse coding. Such benefits are demonstrated with the experimental results on eight benchmark datasets in terms of common criteria.},
  archive      = {J_TMM},
  author       = {Zihan Zhou and Jing Li and Yuhui Quan and Ruotao Xu},
  doi          = {10.1109/TMM.2020.3001472},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1592-1604},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image quality assessment using kernel sparse coding},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthogonalization-guided feature fusion network for
multimodal 2D+3D facial expression recognition. <em>TMM</em>,
<em>23</em>, 1581–1591. (<a
href="https://doi.org/10.1109/TMM.2020.3001497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As 2D and 3D data present different views of the same face, the features extracted from them can be both complementary and redundant. In this paper, we present a novel and efficient orthogonalization-guided feature fusion network, namely OGF $^2$ Net, to fuse the features extracted from 2D and 3D faces for facial expression recognition. While 2D texture maps are fed into a 2D feature extraction pipeline (FE2DNet), the attribute maps generated from 3D data are concatenated as input of the 3D feature extraction pipeline (FE3DNet). The two networks are separately trained at the first stage and frozen in the second stage for late feature fusion, which can well address the unavailability of a large number of 3D+2D face pairs. To reduce the redundancies among features extracted from 2D and 3D streams, we design an orthogonal loss-guided feature fusion network to orthogonalize the features before fusing them. Experimental results show that the proposed method significantly outperforms the state-of-the-art algorithms on both the BU-3DFE and Bosphorus databases. While accuracies as high as 89.05% (P1 protocol) and 89.07% (P2 protocol) are achieved on the BU-3DFE database, an accuracy of 89.28% is achieved on the Bosphorus database. The complexity analysis also suggests that our approach achieves a higher processing speed while simultaneously requiring lower memory costs.},
  archive      = {J_TMM},
  author       = {Shisong Lin and Mengchao Bai and Feng Liu and Linlin Shen and Yicong Zhou},
  doi          = {10.1109/TMM.2020.3001497},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1581-1591},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Orthogonalization-guided feature fusion network for multimodal 2D+3D facial expression recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). V-eye: A vision-based navigation system for the visually
impaired. <em>TMM</em>, <em>23</em>, 1567–1580. (<a
href="https://doi.org/10.1109/TMM.2020.3001500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous systems for helping visually impaired people navigate in unfamiliar places have been proposed. However, few can detect and warn about moving obstacles, provide correct orientation in real time, or support navigation between indoor and outdoor spaces. Accordingly, this paper proposes V-Eye, which fulfills these needs by utilizing a novel global localization method (VB-GPS) and image-segmentation techniques to achieve better scene understanding with a single camera. Our experiments establish that the proposed system can reliably provide precise locations and orientation information (with a median error of approximately 0.27 m and 0.95°); detect unpredictable obstacles; and support navigating both within and between indoor and outdoor environments. The results of a user-experience study of V-eye further indicate that it helped the participants not only with navigation, but also improved their awareness of obstacles, enhanced their spatial awareness more generally, and led them to feel more secure and independent while walking.},
  archive      = {J_TMM},
  author       = {Ping-Jung Duh and Yu-Cheng Sung and Liang-Yu Fan Chiang and Yung-Ju Chang and Kuan-Wen Chen},
  doi          = {10.1109/TMM.2020.3001500},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1567-1580},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {V-eye: A vision-based navigation system for the visually impaired},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind image quality assessment based on multi-scale KLT.
<em>TMM</em>, <em>23</em>, 1557–1566. (<a
href="https://doi.org/10.1109/TMM.2020.3001537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image quality assessment (BIQA) plays an important role in image services as independent of the reference image. Herein, the perceptual relevant feature design is the core of BIQA methods, but their performance is still not satisfied at present. In this work, we propose an unsupervised feature extraction approach for BIQA based on Karhunen-Loéve transform (KLT). Specifically, a normalization operation is firstly applied to the test image by calculating its mean subtracted contrast normalized (MSCN) coefficient. Then, KLT is employed as a data-driven feature extraction approach to extract image structural features, wherein kernels with different sizes are utilized to perform multi-scale analysis. Finally, generalized Gaussian distribution (GGD) is employed to model the KLT coefficients distribution in different spectral components as quality relevant features. Extensive experiments conducted on four widely utilized IQA databases have demonstrated that the proposed Multi-scale KLT (MsKLT) BIQA metric compares favorably with existing BIQA methods in terms of high accordance with human subjective scores on both common and uncommon distortion types.},
  archive      = {J_TMM},
  author       = {Chao Yang and Xinfeng Zhang and Ping An and Liquan Shen and C.-C. Jay Kuo},
  doi          = {10.1109/TMM.2020.3001537},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1557-1566},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind image quality assessment based on multi-scale KLT},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building high-fidelity human body models from user-generated
data. <em>TMM</em>, <em>23</em>, 1542–1556. (<a
href="https://doi.org/10.1109/TMM.2020.3001540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a key point-based approach, refers to as KPhub-PC , to estimate high-fidelity human body models from low-quality point clouds acquired with an affordable 3D scanner and a variation KPhub-I that can achieve the same purpose based on low-resolution single images taken by smartphones. In KPhub-PC, a sparse set of key points is annotated to guide the deformation of a parametric 3D human body model SMPL and then a high-fidelity human body model that can explain the target point clouds is built. Besides building 3D human body models from point clouds, KPhub-I is designed to estimate accurate 3D human body models from single 2D images. The SMPL model is fitted to 2D joints and the boundary of the human body which are detected using CNN based methods automatically. Considering that people are in stable poses most of the time, a stable pose prior is defined from CMU motion capture dataset for further improving accuracy. Extensive experiments demonstrate that in both types of user-generated data, the proposed approaches can build believable and animatable human body models robustly. Our approach outperforms the state-of-the-arts in the accuracy of both human body shape and pose estimation.},
  archive      = {J_TMM},
  author       = {Zongyi Xu and Wei Chang and Yindi Zhu and Le Dong and Huiyu Zhou and Qianni Zhang},
  doi          = {10.1109/TMM.2020.3001540},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1542-1556},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Building high-fidelity human body models from user-generated data},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QoE-driven HAS live video channel placement in the media
cloud. <em>TMM</em>, <em>23</em>, 1530–1541. (<a
href="https://doi.org/10.1109/TMM.2020.2999176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HTTP adaptive streaming (HAS) technology has been increasingly employed by video service providers (VSPs) due to its prominent benefits such as reducing interruptions of video playback and achieving higher bandwidth utilization and outstanding quality of experience (QoE). And many VSPs have deployed HAS applications in the media cloud to provide large-scale video streaming services. At present, research into the media cloud typically focuses on the management and optimization of cloud resources, such as the placement and migration of virtual machines in media cloud data centers. However, considering the HAS live video streaming service, existing related works have not adequately discussed the specific impact of the consumption of computing and bandwidth resources of media cloud servers on the user experience (QoE), particularly under the resource constraints in the media cloud. In this paper, we first investigate and formulate the computing and bandwidth resource consumption characteristics of HAS live video streaming with different frame rates and resolutions, and we further establish a resources-aware QoE model to quantify the user experience of live video channels (i.e., programs). Then, based on the model, we present a QoE-driven HAS live video channel placement approach (including a placement algorithm HCP and a rescheduling algorithm HCR ) to optimize the channel allocation in media cloud servers, aiming to maximize the average user QoE. We abstract the maximization problem into an MMKP problem, and employ a heuristic solution to address this problem. The experimental results demonstrate the effectiveness of our proposed approach compared with benchmark solutions.},
  archive      = {J_TMM},
  author       = {Junquan Liu and Weizhan Zhang and Shouqin Huang and Haipeng Du and Qinghua Zheng},
  doi          = {10.1109/TMM.2020.2999176},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1530-1541},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {QoE-driven HAS live video channel placement in the media cloud},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual image hashing with texture and invariant vector
distance for copy detection. <em>TMM</em>, <em>23</em>, 1516–1529. (<a
href="https://doi.org/10.1109/TMM.2020.2999188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-based image copy detection has become one of the important technologies in copyright protection, where two major processes, content-based feature extraction and matching are included. However, it is certainly true that enough storage space is required to establish feature database for matching, which greatly increases time and storage consumption, as well as lacks flexibility. Fortunately, perceptual image hashing is a good strategy to address these problems, in which content-based features are extracted and further encoded to hash codes. On the one hand, content-based features provide and ensure higher copy detection accuracy, while on the other hand, hash codes instead of feature database reduce storage space and improve time efficiency. Meanwhile, a better balance between robustness and discrimination is one of the most objectives of image hashing, which is conducive to its application in multimedia management and security. Consequently, we present an effective image hashing method for copy detection. Specifically, to obtain perceptual robustness against to copy attacks, we extract the global statistical characteristics in gray-level co-occurrence matrix (GLCM) to reveal texture changes. Then, to make up the discrimination limitation, we leverage the local dominant DCT coefficients from the first row/column in each sub-image to calculate vector distance. Finally, two kinds of complementary information (global feature via texture and local feature via vector distance) are simultaneously preserved to generate hash codes. Various experiments performed on benchmark database indicate that our proposed perceptual image hashing provides higher detection accuracy and better balance between robustness and discrimination than the state-of-the-art algorithms.},
  archive      = {J_TMM},
  author       = {Ziqing Huang and Shiguang Liu},
  doi          = {10.1109/TMM.2020.2999188},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1516-1529},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Perceptual image hashing with texture and invariant vector distance for copy detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transferable knowledge-based multi-granularity fusion
network for weakly supervised temporal action detection. <em>TMM</em>,
<em>23</em>, 1503–1515. (<a
href="https://doi.org/10.1109/TMM.2020.2999184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite remarkable progress, temporal action detection is still limited for real application due to the great amount of manual annotations. This issue motivates interest in addressing this task under weak supervision, namely, locating the action instances using only video-level class labels. Many current works on this task are mainly based on the Class Activation Sequence (CAS), which is generated by the video classification network to describe the probability of each snippet being in a specific action class of the video. However, the CAS generated by a simple classification network can only focus on local discriminative parts instead of locating the entire interval of target actions. In this paper, we present a novel framework to handle this issue. Specifically, we propose to utilize convolutional kernels with varied dilation rates to enlarge the receptive fields, which can transfer the discriminative information to the surrounding non-discriminative regions. Then, we design a cascaded module with the proposed Online Adversarial Erasing (OAE) mechanism to further mine more relevant regions of target actions by feeding the erased-feature maps of discovered regions back into the system. In addition, inspired by the transfer learning method, we adopt an additional module to transfer the knowledge from trimmed videos to untrimmed videos to promote the classification performance on untrimmed videos. Finally, we employ a boundary regression module embedded with Outer-Inner-Contrastive (OIC) loss to automatically predict the boundaries based on the enhanced CAS. Extensive experiments are conducted on two challenging datasets, THUMOS14 and ActivityNet-1.3, and the experimental results clearly demonstrate the superiority of our unified framework.},
  archive      = {J_TMM},
  author       = {Haisheng Su and Xu Zhao and Tianwei Lin and Shuming Liu and Zhilan Hu},
  doi          = {10.1109/TMM.2020.2999184},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1503-1515},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transferable knowledge-based multi-granularity fusion network for weakly supervised temporal action detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse-to-fine CNN for image super-resolution. <em>TMM</em>,
<em>23</em>, 1489–1502. (<a
href="https://doi.org/10.1109/TMM.2020.2999182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have been popularly adopted in image super-resolution (SR). However, deep CNNs for SR often suffer from the instability of training, resulting in poor image SR performance. Gathering complementary contextual information can effectively overcome the problem. Along this line, we propose a coarse-to-fine SR CNN (CFSRCNN) to recover a high-resolution (HR) image from its low-resolution version. The proposed CFSRCNN consists of a stack of feature extraction blocks (FEBs), an enhancement block (EB), a construction block (CB) and, a feature refinement block (FRB) to learn a robust SR model. Specifically, the stack of FEBs learns the long- and short-path features, and then fuses the learned features by expending the effect of the shallower layers to the deeper layers to improve the representing power of learned features. A compression unit is then used in each FEB to distill important information of features so as to reduce the number of parameters. Subsequently, the EB utilizes residual learning to integrate the extracted features to prevent from losing edge information due to repeated distillation operations. After that, the CB applies the global and local LR features to obtain coarse features, followed by the FRB to refine the features to reconstruct a high-resolution image. Extensive experiments demonstrate the high efficiency and good performance of our CFSRCNN model on benchmark datasets compared with state-of-the-art SR models. The code of CFSRCNN is accessible on https://github.com/hellloxiaotian/CFSRCNN .},
  archive      = {J_TMM},
  author       = {Chunwei Tian and Yong Xu and Wangmeng Zuo and Bob Zhang and Lunke Fei and Chia-Wen Lin},
  doi          = {10.1109/TMM.2020.2999182},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1489-1502},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Coarse-to-fine CNN for image super-resolution},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multi-patch matching network for visible thermal person
re-identification. <em>TMM</em>, <em>23</em>, 1474–1488. (<a
href="https://doi.org/10.1109/TMM.2020.2999180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible Thermal Person Re-Identification (VTReID) is a cross-modality retrieval problem in computer vision. Accurate VTReID is very challenging due to large modality discrepancies. In this work, we design a novel Multi-Patch Matching Network (MPMN) framework to simultaneously mitigate the heterogeneity of coarse-grained and fine-grained visual semantics. In view of cross-modality matching, we verify that aligning modality distributions of the original features is likely to suffer from the selective alignment behavior, i.e., only focuses on easiest dimensions or subspaces. Inspired by adversarial learning, we propose a new Multi-Patch Modality Alignment (MPMA) loss to jointly balance and reduce the modality discrepancies of multi-patch features by mining hard subspaces and abandoning easy subspaces. Since multi-patch features are potentially complementary to each other, the semantic correlations between different patches should be exploited during training. Motivated by knowledge distillation, we put forward a new Cross-Patch Correlation Distillation (CPCD) loss to transfer the semantic knowledges across different patches. To balance multi-patch tasks, an effective Patch-Aware Priority Attention (PAPA) method is further introduced to dynamically prioritize hard patch tasks during training. This paper experimentally demonstrates the effectiveness of the proposed methods, achieving superior performance over the state-of-the-art methods on RegDB and SYSU-MM01 datasets.},
  archive      = {J_TMM},
  author       = {Pingyu Wang and Zhicheng Zhao and Fei Su and Yanyun Zhao and Haiying Wang and Lei Yang and Yang Li},
  doi          = {10.1109/TMM.2020.2999180},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1474-1488},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep multi-patch matching network for visible thermal person re-identification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High capacity reversible data hiding in encrypted image
based on intra-block lossless compression. <em>TMM</em>, <em>23</em>,
1466–1473. (<a href="https://doi.org/10.1109/TMM.2020.2999187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cover image is generally encrypted by a stream cipher in existing reversible data hiding in encrypted image (RDHEI) methods. As pixel correlation is seriously damaged, more than one pixel should be employed to carry one bit such that the quite limited capacity is achieved. To overcome this issue, a new RDHEI method with high capacity, that preserves pixel correlation and exploits it to vacate embedding room, is proposed in this paper. First, we propose a block-level encryption scheme which combines block-level stream cipher and block-level permutation, and all blocks are classified into usable blocks (UBs) and unusable blocks (NUBs) by preserving the correlation of pixels in blocks. Then, UB is reconstructed to vacate room for data embedding, because the pixels in blocks share the same most significant bits (MSBs). To ensure reversibility, the number of NUBs between current UB and the previous one is also embedded along with additional data, and the blocks are rearranged in a reversible way such that UBs are always in front of NUBs. Experimental results show that not only the embedding capacity is significantly improved but also the hidden data can be losslessly extracted, and the cover image can be perfectly recovered.},
  archive      = {J_TMM},
  author       = {Yaomin Wang and Zhanchuan Cai and Wenguang He},
  doi          = {10.1109/TMM.2020.2999187},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1466-1473},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {High capacity reversible data hiding in encrypted image based on intra-block lossless compression},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A brain-media deep framework towards seeing imaginations
inside brains. <em>TMM</em>, <em>23</em>, 1454–1465. (<a
href="https://doi.org/10.1109/TMM.2020.2999183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While current research on multimedia is essentially dealing with the information derived from our observations of the world, internal activities inside human brains, such as imaginations and memories of past events etc., could become a brand new concept of multimedia, for which we coin as “brain-media”. In this paper, we pioneer this idea by directly applying natural images to stimulate human brains and then collect the corresponding electroencephalogram (EEG) sequences to drive a deep framework to learn and visualize the corresponding brain activities. By examining the relevance between the visualized image and the stimulation image, we are able to assess the performance of our proposed deep framework in terms of not only the quality of such visualization but also the feasibility of introducing the new concept of “brain-media”. To ensure that our explorative research is meaningful, we introduce a dually conditioned learning mechanism in the proposed deep framework. One condition is analyzing EEG sequences through deep learning to extract a more compact and class-dependent brain features via exploiting those unique characteristics of human brains such as hemispheric lateralization and biological neurons myelination (neurons importance), and the other is to analyze the content of images via computing approaches and extract representative visual features to exploit artificial intelligence in assisting our automated analysis of brain activities and their visualizations. By combining the brain feature space with the associated visual feature space of those images that are candidates of the stimuli, we are able to generate a combined-conditional space to support the proposed dual-conditioned and lateralization-supported GAN framework. Extensive experiments carried out illustrate that our proposed deep framework significantly outperforms the existing relevant work, indicating that our proposed does provide a good potential for further research upon the introduced concept of “brain-media”, a new member for the big family of multimedia. To encourage more research along this direction, we make our source codes publicly available for downloading at GitHub. 1 1https://github.com/aneeg/LS-GAN.},
  archive      = {J_TMM},
  author       = {Jianmin Jiang and Ahmed Fares and Sheng-Hua Zhong},
  doi          = {10.1109/TMM.2020.2999183},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1454-1465},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A brain-media deep framework towards seeing imaginations inside brains},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid refinement-correction heatmaps for human pose
estimation. <em>TMM</em>, <em>23</em>, 1330–1342. (<a
href="https://doi.org/10.1109/TMM.2020.2999181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method (Hybrid-Pose) to improve human pose estimation in images. We adopt Stacked Hourglass Networks to design two convolutional neural network models, RNet for pose refinement and CNet for pose correction. The CNet (Correction Network) guides the pose refinement RNet (Refinement Network) to correct the joint location before generating the final pose. Each of the two models is composed of four hourglasses, and each hourglass generates a group of detection heatmaps for the joints. The RNet model hourglasses have the same structure. However, the CNet model is designed with hourglasses of different structures for pose guidance. Since the pose estimation in RGB images is very sensitive to the image scene, our proposed approach generates multiple outputs of detection heatmaps to broaden the searching scope for the correct joints locations. We use the RNet model to refine the joints locations in each hourglass stage horizontally, then the heatmaps of each stage are fused with the heatmaps of all the CNet model hourglasses vertically in a hybrid manner. Our method shows competitive results with the existing state-of-the-art approaches on MPII and FLIC benchmark datasets. Although our proposed method focuses on improving single-person pose estimation, we also show the influence of this improvement on multi-person pose estimation by detecting multiple people using SSD detector, then estimating the pose of each person individually.},
  archive      = {J_TMM},
  author       = {Aouaidjia Kamel and Bin Sheng and Ping Li and Jinman Kim and David Dagan Feng},
  doi          = {10.1109/TMM.2020.2999181},
  journal      = {IEEE Transactions on Multimedia},
  month        = {6},
  pages        = {1330-1342},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hybrid refinement-correction heatmaps for human pose estimation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient object detection by fusing local and global
contexts. <em>TMM</em>, <em>23</em>, 1442–1453. (<a
href="https://doi.org/10.1109/TMM.2020.2997178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the powerful discriminative feature learning capability of convolutional neural networks (CNNs), deep learning techniques have achieved remarkable performance improvement for the task of salient object detection (SOD) in recent years. However, most existing deep SOD models do not fully exploit informative contextual features, which often leads to suboptimal detection performance in the presence of a cluttered background. This paper presents a context-aware attention module that detects salient objects by simultaneously constructing connections between each image pixel and its local and global contextual pixels. Specifically, each pixel and its neighbors bidirectionally exchange semantic information by computing their correlation coefficients, and this process aggregates contextual attention features both locally and globally. In addition, an attention-guided hierarchical network architecture is designed to capture fine-grained spatial details by transmitting contextual information from deeper to shallower network layers in a top-down manner. Extensive experiments on six public SOD datasets show that our proposed model demonstrates superior SOD performance against most of the current state-of-the-art models under different evaluation metrics.},
  archive      = {J_TMM},
  author       = {Qinghua Ren and Shijian Lu and Jinxia Zhang and Renjie Hu},
  doi          = {10.1109/TMM.2020.2997178},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1442-1453},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Salient object detection by fusing local and global contexts},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weighted adaptive image super-resolution scheme based on
local fractal feature and image roughness. <em>TMM</em>, <em>23</em>,
1426–1441. (<a href="https://doi.org/10.1109/TMM.2020.2997126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution aims to reconstruct a high-resolution image from the known low-resolution version. During this process, it should keep the degree of image roughness non-decreasing, which reflects various texture features and appearance. However, this point is not well addressed in the current work. This work argues that reducing roughness during image super-resolution is the key reason causing various problems such as artificial texture and/or edge blur. In this work, keeping the image roughness non-decreasing during super-resolution is being well investigated for the first time to our best knowledge. Image super-resolution is cast as an optimization problem to keep image roughness non-decreasing. In order to tackle this problem, the image super-resolution is approached based on the theory of fractal, where adaptive fractal interpolation function is proposed. In this way, the rational fractal interpolation model is adaptive to every local region. Thus, the roughness of every image region can be best maintained while super-resolution is carried out through fractal interpolation. In this work, the image roughness is reflected by the fractal dimension, which is a key element affecting the construction of fractal interpolation model. That is, the image roughness is measurable using fractal dimension. Mathematically, the overall image super-resolution process can be converted into a fractal interpolation optimization problem where the local fractal dimension is maintained. Although adaptive super-resolution on image segments may best maintain image roughness using the proposed method, it still generates unnecessary block artifacts. To tackle this problem, this work proposes a fine-grained pixel-wise fractal function. Our extensive experimental results demonstrate that the proposed method achieves encouraging performance with the state-of-the-art super-resolution algorithms.},
  archive      = {J_TMM},
  author       = {Xunxiang Yao and Qiang Wu and Peng Zhang and Fangxun Bao},
  doi          = {10.1109/TMM.2020.2997126},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1426-1441},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weighted adaptive image super-resolution scheme based on local fractal feature and image roughness},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image-only real-time incremental UAV image mosaic for
multi-strip flight. <em>TMM</em>, <em>23</em>, 1410–1425. (<a
href="https://doi.org/10.1109/TMM.2020.2997193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited by aircraft flight altitude and camera parameters, it is necessary to obtain wide-angle panoramas quickly by stitching aerial images, which is helpful in rapid disaster investigation, recovery after earthquakes, and aerial reconnaissance. However, most existing stitching algorithms do not simultaneously meet practical real-time, robustness, and accuracy requirements, especially in the case of a long-distance multistrip flight. In this paper, we propose a novel image-only real-time UAV image mosaic framework for long-distance multistrip flights that does not require any auxiliary information, such as GPS or GCPs. The framework has a complete structure, mainly consisting of the three tasks of automatic initialization, current frame tracking, and real-time mosaic generation. The stitching plane is determined in the initialization process, the homography transformation of the current image is estimated in the tracking task, and the image is mapped to the stitching plane to generate and update the panorama in the real-time mosaic process. The core idea is that, in the tracking task, we introduce and develop a keyframe insertion strategy to generate a keyframe list and, on this basis, design a homography matrix estimation based on a local optimization strategy to reduce the accumulated error when continuously stitching image sequences collected online by UAVs and to realize real-time, effective UAV image mosaic construction. In addition, this framework has good scalability, which is not limited to a specific algorithm. To evaluate the effectiveness of the proposed framework, we carry out a large number of experiments on the AirSim simulation platform and present an exhaustive evaluation in some sequences from a popular dataset. Qualitative and quantitative experimental results in simulation and real environments demonstrate that our algorithm can obtain an effective and robust mosaic image in real-time. Through strategy comparison experiments, it is proven that the keyframe insertion strategy and the local optimization strategy both improve the stitching performance. Compared with five state-of-art image stitching approaches, the mosaic effect of the proposed method is comparable or better. In terms of algorithm speed, its performance is superior to them. Additionally, experiments of illumination change and feature replacement in the framework verify the good adaptability and scalability of the algorithm.},
  archive      = {J_TMM},
  author       = {Fangbing Zhang and Tao Yang and LinFeng Liu and Bang Liang and Yi Bai and Jing Li},
  doi          = {10.1109/TMM.2020.2997193},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1410-1425},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image-only real-time incremental UAV image mosaic for multi-strip flight},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stacked u-shape network with channel-wise attention for
salient object detection. <em>TMM</em>, <em>23</em>, 1397–1409. (<a
href="https://doi.org/10.1109/TMM.2020.2997192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the core issue of how to learn powerful features for saliency. We have two major observations. First, feature maps of different layers in convolutional neural networks play different roles in saliency detection. Second, different feature channels in the same layer are not of equal importance to saliency, and they often have different response to foreground or background. To address these problems, a stacked U-shape network with channel-wise attention is presented to effectively utilize these features, which mainly consists of a parallel dilated convolution (PDC) module and a multi-level attention cascaded feedback (MACF) module. More specifically, PDC aims to enlarge the receptive field without increasing the computation and effectively avoid the gridding problem. MACF is innovatively designed to adaptively select the cross-layer complementary information, and the inter-dependencies between different channel maps in the same layer can be depicted well. Finally, we adopt a multi-layer loss function to improve the commonly used binary cross entropy loss which treats all pixels equally. The extensive experiments on five saliency detection datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Junxia Li and Zefeng Pan and Qingshan Liu and Ziyang Wang},
  doi          = {10.1109/TMM.2020.2997192},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1397-1409},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Stacked U-shape network with channel-wise attention for salient object detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AttentionFGAN: Infrared and visible image fusion using
attention-based generative adversarial networks. <em>TMM</em>,
<em>23</em>, 1383–1396. (<a
href="https://doi.org/10.1109/TMM.2020.2997127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion aims to describe the same scene from different aspects by combining complementary information of multi-modality images. The existing Generative adversarial networks (GAN) based infrared and visible image fusion methods cannot perceive the most discriminative regions, and hence fail to highlight the typical parts existing in infrared and visible images. To this end, we integrate multi-scale attention mechanism into both generator and discriminator of GAN to fuse infrared and visible images (AttentionFGAN). The multi-scale attention mechanism aims to not only capture comprehensive spatial information to help generator focus on the foreground target information of infrared image and background detail information of visible image, but also constrain the discriminators focus more on the attention regions rather than the whole input image. The generator of AttentionFGAN consists of two multi-scale attention networks and an image fusion network. Two multi-scale attention networks capture the attention maps of infrared and visible images respectively, so that the fusion network can reconstruct the fused image by paying more attention to the typical regions of source images. Besides, two discriminators are adopted to force the fused result keep more intensity and texture information from infrared and visible image respectively. Moreover, to keep more information of attention region from source images, an attention loss function is designed. Finally, the ablation experiments illustrate the effectiveness of the key parts of our method, and extensive qualitative and quantitative experiments on three public datasets demonstrate the advantages and effectiveness of AttentionFGAN compared with the other state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jing Li and Hongtao Huo and Chang Li and Renhua Wang and Qi Feng},
  doi          = {10.1109/TMM.2020.2997127},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1383-1396},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AttentionFGAN: Infrared and visible image fusion using attention-based generative adversarial networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prominent local representation for dynamic textures based on
high-order gaussian-gradients. <em>TMM</em>, <em>23</em>, 1367–1382. (<a
href="https://doi.org/10.1109/TMM.2020.2997202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding dynamic textures (DTs) is a challenge in various computer vision applications due to the negative impacts of noise, changes of environment, illumination, and scales on capturing turbulent characteristics. In this work, we propose an efficient shallow framework for DT representation by addressing the following novel concepts. First, it is the first time in DT analysis that 2D/3D Gaussian-gradient filterings are taken into account as a pre-processing step to point out robust components against those influences in effect. Second, high-order partial derivatives of the Gaussian kernels and their informative magnitudes are exploited to forcefully capture multi-order Gaussian-gradient features. Third, these gradient kernels are investigated in multi-scale analysis of different orders and standard deviations in order to enrich more useful scale-gradient information. Finally, the obtained complementary components are shallowly encoded using a simple local operator to construct robust descriptors of High-order 2D/3D Gaussian-gradient-based Features ( $\mathrm{HoGF}^{2D/3D}$ ) against the well-known issues of DT description. Experiments for DT classification on various benchmarks have validated the interest of our approach since its performance is comparable to state-of-the-art results, including that of deep-learning methods, while it only has a small dimension.},
  archive      = {J_TMM},
  author       = {Thanh Tuan Nguyen and Thanh Phuong Nguyen and Frédéric Bouchara},
  doi          = {10.1109/TMM.2020.2997202},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1367-1382},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prominent local representation for dynamic textures based on high-order gaussian-gradients},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Snowball: Iterative model evolution and confident sample
discovery for semi-supervised learning on very small labeled datasets.
<em>TMM</em>, <em>23</em>, 1354–1366. (<a
href="https://doi.org/10.1109/TMM.2020.2997185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop a joint sample discovery and iterative model evolution method for semi-supervised learning on very small labeled training sets. We propose a master-teacher-student model framework to provide multi-layer guidance during the model evolution process with multiple iterations and generations. The teacher model is constructed by performing an exponential moving average of the student models obtained from past training steps. The master network combines the knowledge of the student and teacher models with additional access to newly discovered samples. The master and teacher models are then used to guide the training of the student network by enforcing the consistency between their predictions of unlabeled samples and evolve all models when more and more samples are discovered. Our extensive experiments demonstrate that the process of discovering confident samples from the unlabeled dataset, once coupled with the master-teacher-student network evolution, can significantly improve the overall semi-supervised learning performance. For example, on the CIFAR-10 dataset, with a small set of 250 labeled samples, our method achieves an error rate of 11.58%, more than 38% lower than Mean-Teacher (49.91%). When coupled with the MixMatch augmentation and loss function, the improvements are also significant.},
  archive      = {J_TMM},
  author       = {Yang Li and Zhiqun Zhao and Hao Sun and Yigang Cen and Zhihai He},
  doi          = {10.1109/TMM.2020.2997185},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1354-1366},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Snowball: Iterative model evolution and confident sample discovery for semi-supervised learning on very small labeled datasets},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CmSalGAN: RGB-d salient object detection with cross-view
generative adversarial networks. <em>TMM</em>, <em>23</em>, 1343–1353.
(<a href="https://doi.org/10.1109/TMM.2020.2997184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image salient object detection (SOD) is an active research topic in computer vision and multimedia area. Fusing complementary information of RGB and depth has been demonstrated to be effective for image salient object detection which is known as RGB-D salient object detection problem. The main challenge for RGB-D salient object detection is how to exploit the salient cues of both intra-modality (RGB, depth) and cross-modality simultaneously which is known as cross-modality detection problem. In this paper, we tackle this challenge by designing a novel cross-modality Saliency Generative Adversarial Network ( cm SalGAN). cm SalGAN aims to learn an optimal view-invariant and consistent pixel-level representation for RGB and depth images via a novel adversarial learning framework, which thus incorporates both information of intra-view and correlation information of cross-view images simultaneously for RGB-D saliency detection problem. To further improve the detection results, the attention mechanism and edge detection module are also incorporated into cm SalGAN. The entire cm SalGAN can be trained in an end-to-end manner by using the standard deep neural network framework. Experimental results show that cm SalGAN achieves the new state-of-the-art RGB-D saliency detection performance on several benchmark datasets.},
  archive      = {J_TMM},
  author       = {Bo Jiang and Zitai Zhou and Xiao Wang and Jin Tang and Bin Luo},
  doi          = {10.1109/TMM.2020.2997184},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1343-1353},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CmSalGAN: RGB-D salient object detection with cross-view generative adversarial networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). R-net: A relationship network for efficient and accurate
scene text detection. <em>TMM</em>, <em>23</em>, 1316–1329. (<a
href="https://doi.org/10.1109/TMM.2020.2995290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel bi-directional con-volutional framework to cope with the large-variance scale problem in scene text detection. Due to the lack of scale normalization in recent CNN-based methods, text instances with large-variance scale are activated inconsistently in feature maps, which makes it hard for CNN-based methods to accurately locate multi-size text instances. Thus, we propose the relationship network (R-Net) that maps multi-scale convolutional features to a scale-invariant space to obtain consistent activation of multi-size text instances. Firstly, we implement an FPN-like backbone with a Spatial Relationship Module (SPM) to extract multi-scale features with powerful spatial semantics. Then, a Scale Relationship Module (SRM) constructed on feature pyramid propagates contextual scale information in sequential features through a bi-directional convolutional operation. SRM supplements the multi-scale information in different feature maps to obtain consistent activation of multi-size text instances. Compared with previous approaches, R-Net effectively handles the large-variance scale problem without complicated post processing and complex hand-crafted hyperparameter setting. Extensive experiments conducted on several benchmarks verify that our R-Net obtains state-of-the-art performance on both accuracy and efficiency. More specifically, R-Net achieves an F-measure of 85.6% at 21.4 frames/s and an F-measure of 81.7% at 11.8 frames/s for ICDAR 2015 and MSRA-TD500 datasets respectively, which is the latest SOTA. The code is available on https://github.com/wangyuxin87/R-Net.},
  archive      = {J_TMM},
  author       = {Yuxin Wang and Hongtao Xie and Zhengjun Zha and Youliang Tian and Zilong Fu and Yongdong Zhang},
  doi          = {10.1109/TMM.2020.2995290},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1316-1329},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {R-net: A relationship network for efficient and accurate scene text detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel image representation method under a non-standard
positional numeral system. <em>TMM</em>, <em>23</em>, 1301–1315. (<a
href="https://doi.org/10.1109/TMM.2020.2995258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image representation is an active research area in the field of image processing. This paper proposes a novel image representation method under a non-standard positional numeral system, wherein complex numbers are used as bases in such non-standard positional numeral system. It is different with binary code and decimal code, where the digit 2 is used as the base in the binary system, and the digit 10 is used as the base in the decimal system. In the proposed image representation method, a two-dimensional image is transformed into a one-dimensional 0 $\sim$ 1 sequence, and its Gaussian integer is calculated based on the derived one-dimensional 0 $\sim$ 1 sequence. On the contrary, the original two-dimensional image can be recovered from its Gaussian integer. When images are represented as Gaussian integers, the classical geometrical operations are introduced into image processing. Then, the relationship of different images is established by the methods of plane geometry, i.e., the addition, subtraction, multiplication, division, conjugate, and inverse operations of images are defined. The experimental results show that the selection of complex number as base in the positional numeral system is a very special coding method, a given digital image is effectively converted to a Gaussian integer by using the proposed image representation method, and the image arithmetic is also successfully achieved. In addition, three applications based on the proposed image representation method including image camouflage, image sharing, and image scrambling are selected to demonstrate that the new image representation has good and potential practical applicability in the field of secure encryption of digital images.},
  archive      = {J_TMM},
  author       = {Ting Lan and Zhanchuan Cai},
  doi          = {10.1109/TMM.2020.2995258},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1301-1315},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel image representation method under a non-standard positional numeral system},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object cosegmentation in noisy videos with multilevel
hypergraph. <em>TMM</em>, <em>23</em>, 1287–1300. (<a
href="https://doi.org/10.1109/TMM.2020.2995266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the target of simultaneously segmenting semantically related videos to identify the common objects, video object cosegmentation has attracted the attention of researchers in recent years. Existing methods are primarily based on pair-wise relations between adjacent pixels and regions, which are susceptible to performance degradation from object entries/exists or occlusions. Specifically, we refer these video frames without the common objects present as the “empty” frames. In this paper, we propose a multilevel hypergraph-based full Video object CoSegmentation (VCS) method, which incorporates high-level semantics and low-level appearance/motion/saliency to construct the hyperedge among multiple spatially and temporally adjacent regions. Specifically, the high-level semantic model fuses multiple object proposals from each frame instead of relying on a single object proposal per frame. A hypergraph cut is subsequently utilized to calculate the object cosegmentation. Experiments on four video object segmentation/cosegmentation datasets against state-of-the-art methods with both objective and subjective results manifest the effectiveness of the proposed VCS method, including the SegTrack and VCoSeg datasets without “empty” frames, the XJTU-Stevens dataset with 3.7% “empty” frames, and the Noisy-ViCoSeg dataset proposed together with our method with 30.3% “empty” frames.},
  archive      = {J_TMM},
  author       = {Le Wang and Xin Lv and Qilin Zhang and Zhenxing Niu and Nanning Zheng and Gang Hua},
  doi          = {10.1109/TMM.2020.2995266},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1287-1300},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Object cosegmentation in noisy videos with multilevel hypergraph},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep collaborative discrete hashing with semantic-invariant
structure construction. <em>TMM</em>, <em>23</em>, 1274–1286. (<a
href="https://doi.org/10.1109/TMM.2020.2995267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep hashing has made great progress in large-scale multimedia retrieval, most of the existing approaches under-explore the semantic correlations and neglect the effect of context-aware visual learning. In this paper, we propose a dual-stream learning framework, termed as Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantics. Specifically, DCDH generates context-aware representations by employing the outer product of visual embeddings and semantic encodings. To further preserve the original semantics and alleviate the class imbalance problem, we introduce the focal loss to take advantage of frequent and rare concepts. Furthermore, a common binary code space is constructed based on the joint learning of the visual representations, the context-aware representations, and the label distribution calibration. Three losses, i.e., the pairwise similarity loss, the quantization loss, and the balanced classification loss, are collaboratively optimized in the general learning framework of DCDH. Extensive experiments conducted on three large-scale benchmark datasets demonstrate the superiority of the proposed method, yielding the state-of-the-art image retrieval performance.},
  archive      = {J_TMM},
  author       = {Zijian Wang and Zheng Zhang and Yandan Luo and Zi Huang and Heng Tao Shen},
  doi          = {10.1109/TMM.2020.2995267},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1274-1286},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep collaborative discrete hashing with semantic-invariant structure construction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-adaptive neural module transformer for visual question
answering. <em>TMM</em>, <em>23</em>, 1264–1273. (<a
href="https://doi.org/10.1109/TMM.2020.2995278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision and language understanding is one of the most fundamental and difficult tasks in Multimedia Intelligence. Simultaneously Visual Question Answering (VQA) is even more challenging since it requires complex reasoning steps to the correct answer. To achieve this, Neural Module Network (NMN) and its variants rely on parsing the natural language question into a module layout (i.e., a problem-solving program). In particular, this process follows a feedforward encoder-decoder pipeline: the encoder embeds the question into a static vector and the decoder generates the layout. However, we argue that such conventional encoder-decoder neglects the dynamic nature of question comprehension (i.e., we should attend to different words from step to step) and per-module intermediate results (i.e., we should discard module performing badly) in the reasoning steps. In this paper, we present a novel NMN, called Self-Adaptive Neural Module Transformer (SANMT), which adaptively adjusts both of the question feature encoding and the layout decoding by considering intermediate Q&amp;A results. Specifically, we encode the intermediate results with the given question features by a novel transformer module to generate dynamic question feature embedding which evolves over reasoning steps. Besides, the transformer utilizes the intermediate results from each reasoning step to guide subsequent layout arrangement. Extensive experimental evaluations demonstrate the superiority of the proposed SANMT over NMN and its variants on four challenging benchmarks, including CLEVR, CLEVR-CoGenT, VQAv1.0, and VQAv2.0 (on average the relative improvement over NMN are 1.5, 2.3, 0.7 and 0.5 points with respect to accuracy).},
  archive      = {J_TMM},
  author       = {Huasong Zhong and Jingyuan Chen and Chen Shen and Hanwang Zhang and Jianqiang Huang and Xian-Sheng Hua},
  doi          = {10.1109/TMM.2020.2995278},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1264-1273},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-adaptive neural module transformer for visual question answering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual neural networks coupling data regression with explicit
priors for monocular 3D face reconstruction. <em>TMM</em>, <em>23</em>,
1252–1263. (<a href="https://doi.org/10.1109/TMM.2020.2994506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the challenging issue of reconstructing a 3D face from one single image under various expressions and illuminations, which is widely applied in multimedia tasks. Methods built upon classical parametric morphable models (3DMMs) gain success on reconstructing the global geometry of a 3D face, but fail to precisely characterize local facial details. Recently, deep neural networks (DNN) have been applied to the reconstruction that directly predicts depth maps, showing compelling performance on detail recovery. Unfortunately, their reconstruction is prone to structural distortions owing to the lack of explicit prior constraints. In this paper, we propose dual neural networks that optimize one energy coupling data fitting with local explicit geometric prior. Specifically, we build one residual network upon traditional convolution layers in order to directly predict 3D structures by fitting an input image. Meanwhile, we devise a novel architecture stacking shallow networks to refine 3D clouds with geometric priors given by Markov random fields (MRFs). Quantitative evaluations demonstrate the superior performance of the dual networks over either end-to-end DNNs or parametric models. Comparisons with the state-of-the-art also show competitive reconstruction quality on various conditions.},
  archive      = {J_TMM},
  author       = {Xin Fan and Shichao Cheng and Kang Huyan and Minjun Hou and Risheng Liu and Zhongxuan Luo},
  doi          = {10.1109/TMM.2020.2994506},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1252-1263},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual neural networks coupling data regression with explicit priors for monocular 3D face reconstruction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive bilateral-context driven model for
post-processing person re-identification. <em>TMM</em>, <em>23</em>,
1239–1251. (<a href="https://doi.org/10.1109/TMM.2020.2994524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing person re-identification methods compute pairwise similarity by extracting robust visual features and learning the discriminative metric. Owing to visual ambiguities, these content-based methods that determine the pairwise relationship only based on the similarity between them, inevitably produce a suboptimal ranking list. Instead, the pairwise similarity can be estimated more accurately along the geodesic path of the underlying data manifold by exploring the rich contextual information of the sample. In this paper, we propose a lightweight post-processing person re-identification method in which the pairwise measure is determined by the relationship between the sample and the counterpart&#39;s context in an unsupervised way. We translate the point-to-point comparison into the bilateral point-to-set comparison. The sample&#39;s context is composed of its neighbor samples with two different definition ways: the first order context and the second order context, which are used to compute the pairwise similarity in sequence, resulting in a progressive post-processing model. The experiments on four large-scale person re-identification benchmark datasets indicate that (1) the proposed method can consistently achieve higher accuracies by serving as a post-processing procedure after the content-based person re-identification methods, showing its state-of-the-art results, (2) the proposed lightweight method only needs about 6 milliseconds for optimizing the ranking results of one sample, showing its high-efficiency. Code is available at: https://github.com/123ci/PBCmodel .},
  archive      = {J_TMM},
  author       = {Min Cao and Chen Chen and Hao Dou and Xiyuan Hu and Silong Peng and Arjan Kuijper},
  doi          = {10.1109/TMM.2020.2994524},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1239-1251},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive bilateral-context driven model for post-processing person re-identification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Factorized tensor dictionary learning for visual tensor data
completion. <em>TMM</em>, <em>23</em>, 1225–1238. (<a
href="https://doi.org/10.1109/TMM.2020.2994512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at developing a dictionary-learning-based method for completing the visual tensor data with missing elements. Traditional dictionary learning approaches suffer from very high computational costs when processing high-dimensional tensor data. Some existing approaches for acceleration impose orthogonality constraints or rank-one decompositions on dictionary atoms; however, the expressibility of the resulting dictionary is rather limited. To address such issues, we propose a convolutional analysis model for tensor dictionary learning, where the update of sparse coefficients during dictionary learning is simple and fast. Furthermore, we propose an orthogonality-constrained convolutional factorization scheme for dictionary construction, in which each tensor dictionary atom is factorized by the convolution of two atoms selected from two orthogonal factor dictionaries respectively. This factorization scheme enables us to efficiently learn an expressive dictionary with over-completeness and non-rank-one atoms. Based on our convolutional analysis model and factorization scheme, an effective yet efficient dictionary learning method is proposed for visual tensor completion. Extensive experiments show that, our method not only outperforms existing dictionary-based approaches with relatively-low time cost, but also outperforms recent low-rank approaches.},
  archive      = {J_TMM},
  author       = {Ruotao Xu and Yong Xu and Yuhui Quan},
  doi          = {10.1109/TMM.2020.2994512},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1225-1238},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Factorized tensor dictionary learning for visual tensor data completion},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complementary incremental hashing with query-adaptive
re-ranking for image retrieval. <em>TMM</em>, <em>23</em>, 1210–1224.
(<a href="https://doi.org/10.1109/TMM.2020.2994509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is prevalent in non-stationary data environments but is rarely researched in image retrieval. Therefore, more research is needed on image retrieval in non-stationary data environments so that highly relevant images can still be retrieved when concept drifts happen. Hashing is a key technique to allow efficient image retrieval, so incremental hashing technique emerges in recent years for image retrieval in non-stationary environments. A state-of-the-art method is Incremental Hashing (ICH). ICH trains new hash tables on new data without considering the performance of previous hash tables, so the dependency of successive hash tables is ignored. To make use of this dependency in order to improve the performance of image retrieval in non-stationary environments, Complementary Incremental Hashing with query-adaptive Re-ranking (CIHR) is proposed in this paper. CIHR trains multiple hash tables incrementally, one for each data chunk of images. A new hash table is trained on a new data chunk of images as well as those images badly hashed by previous hash tables, thus the new hash table is complementary to the previous hash tables. To use the hash tables more effectively, a query-adaptive re-ranking method is used to weight all hash functions in each hash table according to their retrieval performance with respect to a given query. Weighted Hamming distance is finally used to evaluate the similarity between the query and the images in the database, as the basis of image retrieval. Experimental results on simulated non-stationary scenarios show that the proposed CIHR method achieves higher retrieval accuracy than all methods being compared, thus setting a new state of the art in image retrieval in non-stationary data environments.},
  archive      = {J_TMM},
  author       = {Xing Tian and Wing W. Y. Ng and Hui Wang and Sam Kwong},
  doi          = {10.1109/TMM.2020.2994509},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1210-1224},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Complementary incremental hashing with query-adaptive re-ranking for image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attribute-guided feature learning for few-shot image
recognition. <em>TMM</em>, <em>23</em>, 1200–1209. (<a
href="https://doi.org/10.1109/TMM.2020.2993952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image recognition has become an essential problem in the field of machine learning and image recognition, and has attracted more and more research attention. Typically, most few-shot image recognition methods are trained across tasks. However, these methods are apt to learn an embedding network for discriminative representations of training categories, and thus could not distinguish well for novel categories. To establish connections between training and novel categories, we use attribute-related representations for few-shot image recognition and propose an attribute-guided two-layer learning framework, which is capable of learning general feature representations. Specifically, few-shot image recognition trained over tasks and attribute learning trained over images share the same network in a multi-task learning framework. In this way, few-shot image recognition learns feature representations guided by attributes, and is thus less sensitive to novel categories compared with feature representations only using category supervision. Meanwhile, the multi-layer features associated with attributes are aligned with category learning on multiple levels respectively. Therefore we establish a two-layer learning mechanism guided by attributes to capture more discriminative representations, which are complementary compared with a single-layer learning mechanism. Experimental results on CUB-200, AWA and MiniImageNet datasets demonstrate our method effectively improves the performance.},
  archive      = {J_TMM},
  author       = {Yaohui Zhu and Weiqing Min and Shuqiang Jiang},
  doi          = {10.1109/TMM.2020.2993952},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1200-1209},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attribute-guided feature learning for few-shot image recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained visual categorization by localizing object
parts with single image. <em>TMM</em>, <em>23</em>, 1187–1199. (<a
href="https://doi.org/10.1109/TMM.2020.2993960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization (FGVC) refers to assigning fine-grained labels to images which belong to the same base category. Due to the high inter-class similarity, it is challenging to distinguish fine-grained images under different subcategories. Recently, researchers have proposed to firstly localize key object parts within images and then find discriminative clues on object parts. To localize object parts, existing methods train detectors for different kinds of object parts. However, due to the fact that the same kind of object part in different images often changes intensely in appearance, the existing methods face two shortages: 1) Training part detector for object parts with diverse appearance is laborious; 2) Discriminative parts with unusual appearance may be neglected by the trained part detectors. To localize the key object parts efficiently and accurately, a novel FGVC method is proposed in the paper. The main novelty is that the proposed method localizes the key object parts within each image only depending on a single image and hence avoid the influence of diversity between parts in different images. The proposed FGVC method consists of two key steps. Firstly, the proposed method localizes the key parts in each image independently. To this end, potential object parts in each image are identified and then these potential parts are merged to generate the final representative object parts. Secondly, two kinds of features are extracted for simultaneously describing the discriminative clues within each part and the relationship between object parts. In addition, a part based dropout learning technique is adopted to boost the classification performance further in the paper. The proposed method is evaluated in comparison experiments and the experiment results show that the proposed method can achieve comparable or better performance than state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Xiangtao Zheng and Lei Qi and Yutao Ren and Xiaoqiang Lu},
  doi          = {10.1109/TMM.2020.2993960},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1187-1199},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fine-grained visual categorization by localizing object parts with single image},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measuring coarse-to-fine texture and geometric distortions
for quality assessment of DIBR-synthesized images. <em>TMM</em>,
<em>23</em>, 1173–1186. (<a
href="https://doi.org/10.1109/TMM.2020.2993942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A synthesized view can be generated via Depth-Image-Based Rendering (DIBR) technique using one (or more) color images and the associated depth maps. However, several artifacts may occur in the synthesized views due to the imperfect color images, depth maps or texture inpainting techniques, which cannot be effectively estimated by the conventional quality metrics designed for natural images. In this paper, a new quality metric is proposed to evaluate DIBR-synthesized images by measuring texture and geometric distortions. The artifacts are first analyzed on different phases of the synthesis process, and the associated features are extracted to estimate the degree of texture and geometric distortions from both coarse and fine scales. Finally, individual quality scores are aggregated into an overall quality via regression. Experimental results on three publicly available DIBR datasets demonstrate the superiority of the proposed method over the state-of-the-art quality models.},
  archive      = {J_TMM},
  author       = {Xuejin Wang and Feng Shao and Qiuping Jiang and Xiangchao Meng and Yo-Sung Ho},
  doi          = {10.1109/TMM.2020.2993942},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1173-1186},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Measuring coarse-to-fine texture and geometric distortions for quality assessment of DIBR-synthesized images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D face reconstruction from a single image assisted by 2D
face images in the wild. <em>TMM</em>, <em>23</em>, 1160–1172. (<a
href="https://doi.org/10.1109/TMM.2020.2993962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D face reconstruction from a single image is an important task in many multimedia applications. Recent works typically learn a CNN-based 3D face model that regresses coefficients of a 3D Morphable Model (3DMM) from 2D images to perform 3D face reconstruction. However, the shortage of training data with 3D annotations considerably limits performance of these methods. To alleviate this issue, we propose a novel 2D-Assisted Learning (2DAL) method that can effectively use “in the wild” 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmark heatmaps as additional information, 2DAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the landmark self-prediction consistency for 2D and 3D faces respectively, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark prediction. Using these four self-supervision schemes, 2DAL significantly relieves the demands for the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on AFLW2000-3D, AFLW-LFPA and Florence benchmarks show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.},
  archive      = {J_TMM},
  author       = {Xiaoguang Tu and Jian Zhao and Mei Xie and Zihang Jiang and Akshaya Balamurugan and Yao Luo and Yang Zhao and Lingxiao He and Zheng Ma and Jiashi Feng},
  doi          = {10.1109/TMM.2020.2993962},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1160-1172},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3D face reconstruction from a single image assisted by 2D face images in the wild},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic undirected graph based denoising method for
dynamic vision sensor. <em>TMM</em>, <em>23</em>, 1148–1159. (<a
href="https://doi.org/10.1109/TMM.2020.2993957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Vision Sensor (DVS) is a new type of neuromorphic event-based sensor, which has an innate advantage in capturing fast-moving objects. Due to the interference of DVS hardware itself and many external factors, noise is unavoidable in the output of DVS. Different from frame/image with structural data, the output of DVS is in the form of address-event representation (AER), which means that the traditional denoising methods cannot be used for the output (i.e., event stream) of the DVS. In this paper, we propose a novel event stream denoising method based on probabilistic undirected graph model (PUGM). The motion of objects always shows a certain regularity/trajectory in space and time, which reflects the spatio-temporal correlation between effective events in the stream. Meanwhile, the event stream of DVS is composed by the effective events and random noise. Thus, a probabilistic undirected graph model is constructed to describe such priori knowledge (i.e., spatio-temporal correlation). The undirected graph model is factorized into the product of the cliques energy function, and the energy function is defined to obtain the complete expression of the joint probability distribution. Better denoising effect means a higher probability (lower energy), which means the denoising problem can be transfered into energy optimization problem. Thus, the iterated conditional modes (ICM) algorithm is used to optimize the model to remove the noise. Experimental results on denoising show that the proposed algorithm can effectively remove noise events. Moreover, with the preprocessing of the proposed algorithm, the recognition accuracy on AER data can be remarkably promoted. The source code of the proposed method is available at https://web.xidian.edu.cn/wjj/paper.html .},
  archive      = {J_TMM},
  author       = {Jinjian Wu and Chuanwei Ma and Leida Li and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TMM.2020.2993957},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1148-1159},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Probabilistic undirected graph based denoising method for dynamic vision sensor},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An artificial intelligence-based system to assess nutrient
intake for hospitalised patients. <em>TMM</em>, <em>23</em>, 1136–1147.
(<a href="https://doi.org/10.1109/TMM.2020.2993948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular monitoring of nutrient intake in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition. Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve data accuracy and reduce both the burden on participants and health costs. In this paper, we propose a novel system based on artificial intelligence (AI) to accurately estimate nutrient intake, by simply processing RGB Depth (RGB-D) image pairs captured before and after meal consumption. The system includes a novel multi-task contextual network for food segmentation, a few-shot learning-based classifier built by limited training samples for food recognition, and an algorithm for 3D surface construction. This allows sequential food segmentation, recognition, and estimation of the consumed food volume, permitting fully automatic estimation of the nutrient intake for each meal. For the development and evaluation of the system, a dedicated new database containing images and nutrient recipes of 322 meals is assembled, coupled to data annotation using innovative strategies. Experimental results demonstrate that the estimated nutrient intake is highly correlated (&gt;0.91) to the ground truth and shows very small mean relative errors (&lt;20%), outperforming existing techniques proposed for nutrient intake assessment.},
  archive      = {J_TMM},
  author       = {Ya Lu and Thomai Stathopoulou and Maria F. Vasiloglou and Stergios Christodoulidis and Zeno Stanga and Stavroula Mougiakakou},
  doi          = {10.1109/TMM.2020.2993948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1136-1147},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An artificial intelligence-based system to assess nutrient intake for hospitalised patients},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A 460 GOPS/w improved mnemonic descent method-based
hardwired accelerator for face alignment. <em>TMM</em>, <em>23</em>,
1122–1135. (<a href="https://doi.org/10.1109/TMM.2020.2993943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mnemonic descent method (MDM) algorithm is the first end-to-end recurrent convolutional system for high-accuracy face alignment. However, the heavy computational complexity and high memory access demands make it difficult to satisfy the requirements of real-time applications. To address this problem, an improved MDM (I-MDM) algorithm is proposed for efficient hardware implementation based on several hardware-oriented optimizations. First, a patch merging mechanism is introduced to dynamically cluster and eliminate redundant landmarks, which significantly reduces computational complexity with minimal accuracy loss. Second, a dedicated convolutional layer is inserted to halve the number of computations and memory access of the subsequent fully connected layer, yielding a 4.42% decrease in the failure rate. Third, a lightweight preprocessing method named dual regressors is proposed to reinitialize face images, which can greatly improve the overall accuracy. Moreover, compared with a similar method, the DR method can reduce computations and memory storage by nearly 99.9%. Overall and compared with the MDM algorithm, I-MDM not only reduces the number of computations by 23.5% but also decreases the failure rate by 17.9% on the 300 W test set. Based on the proposed I-MDM algorithm, an I-MDM-based hardwired accelerator is presented using the TSMC 65 nm CMOS process. First, compared with similar solutions, the gradient calculation operation is rearranged and loaded pixels are reused in the HoG feature extraction to eliminate all division operations and 25% off-chip memory access. Second, patch-independent central activations are used to enable patch-level pipelined operations, yielding a 2×  acceleration in the overall process. This accelerator achieves 460 GOPS/W energy efficiency at 330 MHz, which is 38×  higher than the most recent face alignment accelerator with the same process.},
  archive      = {J_TMM},
  author       = {Huiyu Mo and Leibo Liu and Wenping Zhu and Qiang Li and Shouyi Yin and Shaojun Wei},
  doi          = {10.1109/TMM.2020.2993943},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1122-1135},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A 460 GOPS/W improved mnemonic descent method-based hardwired accelerator for face alignment},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rate control method based on deep reinforcement learning for
dynamic video sequences in HEVC. <em>TMM</em>, <em>23</em>, 1106–1121.
(<a href="https://doi.org/10.1109/TMM.2020.2992968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate control (RC) plays a critical role in the transmission of high-quality video data under certain bandwidth restrictions in High Efficiency Video Coding (HEVC). Most current HEVC RC algorithms based on spatio-temporal information for rate-distortion (R-D) model parameters cannot effectively handle the cases with dynamic video sequences that contain fast moving objects, significant object occlusion or scene changes. In this paper, we propose an RC method based on deep reinforcement learning (DRL) for dynamic video sequences in HEVC to improve the coding efficiency. First, the rate control problem is formulated as a Markov decision process (MDP) problem. Second, with the MDP model, we develop a DRL-based algorithm to find the optimal quantization parameters (QPs) by training a deep neural network. The resulting intelligent agent selects the optimal RC strategy to reduce distortion, buffer and quality fluctuations by observing the current state of the encoder. The asynchronous advantage actor-critic (A3C) method is used to solve the MDP problem. Finally, the proposed DRL-based RC method is implemented in the newest video coding standard. Experimental results show that the proposed method offers substantially enhanced RC accuracy and consistently outperforms HEVC reference software and other state-of-the-art algorithms.},
  archive      = {J_TMM},
  author       = {Mingliang Zhou and Xuekai Wei and Sam Kwong and Weijia Jia and Bin Fang},
  doi          = {10.1109/TMM.2020.2992968},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1106-1121},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rate control method based on deep reinforcement learning for dynamic video sequences in HEVC},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Less is (just as good as) more - an investigation of odor
intensity and hedonic valence in mulsemedia QoE using heart rate and eye
tracking. <em>TMM</em>, <em>23</em>, 1095–1105. (<a
href="https://doi.org/10.1109/TMM.2020.2992948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using olfactory media to enhance traditional multimedia content opens up novel opportunities for user interactions. Whilst the influence of olfaction on user experience in mulsemedia (multiple sensorial media) environments has been previously studied, the impact of the fundamental dimensions of scent intensity and valence (odor hedonic dimension or pleasantness) have been largely unexplored. This is precisely what we target in this paper, which reports the results of an empirical investigation examining how scent intensity and valence impact mulsemedia Quality of Experience (QoE). Accordingly, 54 participants were exposed to different odor valences and scent intensity levels when viewing three short multimedia clips. In particular, we examine both subjective (self-reported) as well as objective QoE metrics, as evidenced by user heart rates and eye gaze patterns. Results show that whilst eye gaze patterns are largely unaffected by the experimental conditions, valence does have a statistically significant impact upon user heart rates, as does intensity for two of the three clips employed in our study. In terms of subjective QoE, results indicate that hedonic valence impacts on the sense of reality and enjoyment; however varying odor intensity levels do not seem to differentially impact on user experience, bringing into question the need for strong scent intensities.},
  archive      = {J_TMM},
  author       = {Gebremariam Mesfin and Estêvão Bissoli Saleme and Oluwakemi A. Ademoye and Elahe Kani-Zabihi and Celso A.S. Santos and Gheorghiţă Ghinea},
  doi          = {10.1109/TMM.2020.2992948},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1095-1105},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Less is (Just as good as) more - an investigation of odor intensity and hedonic valence in mulsemedia QoE using heart rate and eye tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial learning for personalized tag recommendation.
<em>TMM</em>, <em>23</em>, 1083–1094. (<a
href="https://doi.org/10.1109/TMM.2020.2992941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have recently seen great progress in image classification due to the success of deep convolutional neural networks and the availability of large-scale datasets. Most of the existing work focuses on single-label image classification. However, there are usually multiple tags associated with an image. The existing works on multi-label classification are mainly based on lab curated labels. Humans assign tags to their images differently, which is mainly based on their interests and personal tagging behavior. In this paper, we address the problem of personalized tag recommendation and propose an end-to-end deep network which can be trained on large-scale datasets. The user-preference is learned within the network in an unsupervised way where the network performs joint optimization for user-preference and visual encoding. A joint training of user-preference and visual encoding allows the network to efficiently integrate the visual preference with tagging behavior for a better user recommendation. In addition, we propose the use of adversarial learning, which enforces the network to predict tags resembling user-generated tags. We demonstrate the effectiveness of the proposed model on two different large-scale and publicly available datasets, YFCC100 M and NUS-WIDE. The proposed method achieves significantly better performance on both the datasets when compared to the baselines and other state-of-the-art methods. The code is publicly available at https://github.com/vyzuer/ALTReco.},
  archive      = {J_TMM},
  author       = {Erik Quintanilla and Yogesh Rawat and Andrey Sakryukin and Mubarak Shah and Mohan Kankanhalli},
  doi          = {10.1109/TMM.2020.2992941},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1083-1094},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial learning for personalized tag recommendation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new image compression algorithm based on non-uniform
partition and u-system. <em>TMM</em>, <em>23</em>, 1069–1082. (<a
href="https://doi.org/10.1109/TMM.2020.2992940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JPEG lossy image compression is a still image compression algorithm model that is currently widely used in major network media. However, it is unsatisfactory in the quality of compressed images at low bit rates. The objective of this paper is to improve the quality of compressed images and suppress blocking artifacts by improving the JPEG image compression model at low bit rates. First, the image texture adaptive non-uniform rectangular partition (ITANRP) algorithm is proposed which partitions the image into $8\times 8$ size image blocks with high texture complexity and $16\times 16$ size image blocks with low texture complexity. Then, a new transform coding based on the complete orthogonal U-system and all-phase digital filter (APDF) is proposed for coding image blocks with different sizes. Next, a flexible adaptive quantization scheme is designed to quantize image blocks with different sizes by considering the sensitivity of the human visual system (HVS) to different texture complexities. Finally, combining the above method with the JPEG model, a novel image compression algorithm model with low algorithm complexity is proposed to solve the problem in JPEG. The experimental results demonstrate that the performance of our algorithm model outperforms the JPEG image compression algorithms, the quality of the compressed image is greatly improved, and the blocking artifacts are also significantly suppressed.},
  archive      = {J_TMM},
  author       = {Yumo Zhang and Zhanchuan Cai and Gangqiang Xiong},
  doi          = {10.1109/TMM.2020.2992940},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1069-1082},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A new image compression algorithm based on non-uniform partition and U-system},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DENet: A universal network for counting crowd with varying
densities and scales. <em>TMM</em>, <em>23</em>, 1060–1068. (<a
href="https://doi.org/10.1109/TMM.2020.2992979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counting people or objects with significantly varying scales and densities has attracted much interest from the research community and yet it remains an open problem. In this paper, we propose a simple but efficient and effective network, named DENet, which is composed of two components, i.e. , a detection network (DNet) and an encoder-decoder estimation network (ENet). We first run the DNet on the input image to detect and count individuals who can be segmented clearly. Then, the ENet is utilized to estimate the density maps of the remaining areas, typically with low resolution and high densities where individuals cannot be detected. For this purpose, we propose a modified Xception network as the encoder for feature extraction and a combination of dilated convolution and transposed convolution as the decoder. When evaluated on the ShanghaiTech Part A, UCF and WorldExpo’10 datasets, our DENet has achieved lower Mean Absolute Error (MAE) than those of the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Lei Liu and Jie Jiang and Wenjing Jia and Saeed Amirgholipour and Yi Wang and Michelle Zeibots and Xiangjian He},
  doi          = {10.1109/TMM.2020.2992979},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1060-1068},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DENet: A universal network for counting crowd with varying densities and scales},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep battery saver: End-to-end learning for power
constrained contrast enhancement. <em>TMM</em>, <em>23</em>, 1049–1059.
(<a href="https://doi.org/10.1109/TMM.2020.2992962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the problems of power-hungry displays and limited battery life in electronic devices, the concept of “green computing,” which entails a reduction in power consumption, is proposed. One often seen green computing is the power-constrained contrast enhancement (PCCE), yet it is much more challenging because of the noticeable local intensity suppressions in images. This paper aims at developing an image-quality-lossless end-to-end learning network called deep battery saver to achieve power savings in emissive displays, i.e., produce power-saved images with high perceptual quality and less power consumption. Built upon the end-to-end network of the displayed image, we propose a variational loss function for enhancing the visual quality and suppressing the power consumption, simultaneously. The basic idea is to integrate both high-level perceptual losses and low-level pixel losses by a deep residual convolutional neural network (CNN) over a devised variational loss function with strong human perceptual consistency. Such deep residual CNN network leads to a visually pleasing image representation during the suppression of power consumption. Experimental results demonstrated the superiority of our deep battery saver to existing PCCE methods.},
  archive      = {J_TMM},
  author       = {Jia-Li Yin and Bo-Hao Chen and Yan-Tsung Peng and Chung-Chi Tsai},
  doi          = {10.1109/TMM.2020.2992962},
  journal      = {IEEE Transactions on Multimedia},
  month        = {5},
  pages        = {1049-1059},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep battery saver: End-to-end learning for power constrained contrast enhancement},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light field image coding using VVC standard and view
synthesis based on dual discriminator GAN. <em>TMM</em>, <em>23</em>,
2972–2985. (<a href="https://doi.org/10.1109/TMM.2021.3068563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) technology is considered as a promising way for providing a high-quality virtual reality (VR) content. However, such an imaging technology produces a large amount of data requiring efficient LF image compression solutions. In this paper, we propose a LF image coding method based on a view synthesis and view quality enhancement techniques. Instead of transmitting all the LF views, only a sparse set of reference views are encoded and transmitted, while the remaining views are synthesized at the decoder side. The transmitted views are encoded using the versatile video coding (VVC) standard and are used as reference views to synthesize the dropped views. The selection of non-reference dropped views is performed using a rate-distortion optimization based on the VVC temporal scalability. The dropped views are reconstructed using the LF dual discriminator GAN (LF-D2GAN) model. In addition, to ensure that the quality of the views is consistent, at the decoder, a quality enhancement procedure is performed on the reconstructed views allowing smooth navigation across views. Experimental results show that the proposed method provides high coding performance and overcomes the state-of-the-art LF image compression methods by –36.22% in terms of BD-BR and 1.35 dB in BD-PSNR. The web page of this work is available at https://naderbakir79.github.io/LFD2GAN.html .},
  archive      = {J_TMM},
  author       = {Nader Bakir and Wassim Hamidouche and Sid Ahmed Fezza and Khouloud Samrouth and Olivier Déforges},
  doi          = {10.1109/TMM.2021.3068563},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2972-2985},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Light field image coding using VVC standard and view synthesis based on dual discriminator GAN},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual try-on network with attribute transformation and
local rendering. <em>TMM</em>, <em>23</em>, 2222–2234. (<a
href="https://doi.org/10.1109/TMM.2021.3070972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A virtual try-on network has gradually become a popular topic in recent years. It aims to transfer images of in-shop clothes onto the image of a target person. Owing to the diversity of clothing attributes, developing an image-based virtual try-on network is a complicated task for computers to perform and requires significant effort. Existing methods are unsatisfactory as they cannot preserve the characteristics of the clothes or the target person&#39;s identity well, thereby affecting the perception of the generated images; therefore, further research is required. To address this problem, we propose a novel try-on method that combines attribute transformation and local rendering. First, we employ pixel-level semantic segmentation to identify the try-on area and provide implementation conditions for local rendering. Second, we construct a learnable attribute transformation module to complete the try-on task for different attributes. Third, we use a learnable clothing warping module to fit the pose and figure of the target person well and establish a novel loss function, called modified style loss (M-SL), to handle clothes with rich details. Finally, we adopt a local rendering strategy, using which only renders the clothing area to ensure that the details of the non-target area are not lost. Extensive experiments are performed to test our method. The results demonstrate that our method outperforms other state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jun Xu and Yuanyuan Pu and Rencan Nie and Dan Xu and Zhengpeng Zhao and Wenhua Qian},
  doi          = {10.1109/TMM.2021.3070972},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {2222-2234},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Virtual try-on network with attribute transformation and local rendering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAL: Selection and attention losses for weakly supervised
semantic segmentation. <em>TMM</em>, <em>23</em>, 1035–1048. (<a
href="https://doi.org/10.1109/TMM.2020.2991592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a fully supervised semantic segmentation network requires a large amount of expensive pixel-level annotations in manual labor. In this work, we focus on studying the semantic segmentation problem using only image-level supervision. An effective scheme for weakly supervised segmentation is employed to produce the proxy annotations via image tags firstly. Then the segmentation network is retrained on the generated noisy proxy annotations. However, learning from noisy annotations is risky, as proxy annotations of poor quality may deteriorate the performance of the baseline segmentation and classification networks. In order to train the segmentation network using noisy annotations more effectively, two novel loss functions are proposed in this paper, namely, the selection loss and attention loss. Firstly, a selection loss is designed by weighting the proxy annotations based on a coarse-to-fine strategy for evaluating the quality of segmentation masks. Secondly, an attention loss taking the clean image tags as supervision is utilized to correct the classification errors caused by ambiguous pixel-level labels. Finally, we propose an end-to-end semantic segmentation network SAL-Net guided by the above two losses. From the extensive experiments conducted on PASCAL VOC 2012 dataset, SAL-Net reaches state-of-the-art performance with mean IoU (mIoU) as 62.5% and 66.6% on the test set by taking VGG16 network and ResNet101 network as the baselines respectively, which demonstrates the superiority of the proposed algorithm over eight representative weakly supervised segmentation methods. The code and models are available at https://github.com/zmbhou/SALTMM .},
  archive      = {J_TMM},
  author       = {Lei Zhou and Chen Gong and Zhi Liu and Keren Fu},
  doi          = {10.1109/TMM.2020.2991592},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1035-1048},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SAL: Selection and attention losses for weakly supervised semantic segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D pose estimation based on reinforce learning for 2D
image-based 3D model retrieval. <em>TMM</em>, <em>23</em>, 1021–1034.
(<a href="https://doi.org/10.1109/TMM.2020.2991532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel characteristic view selection model (CVSM) to address the 2D image-based 3D object retrieval problem. This work includes two key contributions: 1) we propose a novel reinforcement learning model to estimate the 3D pose based on a 2D image; and 2) we render the pose-specific model to generate a representative angle view for retrieval applications. First, we define state, policy, action and reward functions to train an agent with the reinforcement learning framework, by which the agent can effectively reduce the computational cost of the characteristic view selection and directly obtain the 3D model pose. Second, to resolve the problem of computing similarity in the cross-domain between the virtual 3D model view and the real query image, we project them into the skeleton domain, and the skeleton information can effectively bridge the gap between the image and 3D model view for cross-media retrieval. To demonstrate the performance of our approach, we compare with some classic 3D pose estimation methods using the popular Pascal3D dataset. To demonstrate the performance of our approach in model retrieval, we collect a new dataset that includes pairs of 2D images and 3D objects, where 3D objects are based on the ModelNet40 dataset and 2D images are based on the ImageNet dataset, and we experiment with our method using the SHREC 2018 and SHREC 2019 databases. The experimental results demonstrate the superiority of our method.},
  archive      = {J_TMM},
  author       = {Wei-Zhi Nie and Wen-Wu Jia and Wen-Hui Li and An-An Liu and Si-Cheng Zhao},
  doi          = {10.1109/TMM.2020.2991532},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1021-1034},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3D pose estimation based on reinforce learning for 2D image-based 3D model retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-reference sonar image quality assessment based on task
and visual perception. <em>TMM</em>, <em>23</em>, 1008–1020. (<a
href="https://doi.org/10.1109/TMM.2020.2991546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In submarine and underwater detection tasks, conventional optical imaging and analysis methods are not universally applicable due to the limited penetration depth of visible light. Instead, sonar imaging has become a preferred alternative. However, the capture and transmission conditions in complicated and dynamic underwater environments inevitably lead to visual quality degradation of sonar images, which might also impede further recognition, analysis and understanding. To measure this quality decrease and provide a solid quality indicator for sonar image enhancement, we propose a task- and perception-oriented sonar image quality assessment (TPSIQA) method, in which a semi-reference (SR) approach is applied to adapt to the limited bandwidth of underwater communication channels. In particular, we exploit reduced visual features that are critical for both human perception of and object recognition in sonar images. The final quality indicator is obtained through ensemble learning, which aggregates an optimal subset of multiple base learners to achieve both high accuracy and a high generalization ability. In this way, we are able to develop a compact but generalized quality metric using a small database of sonar images. Experimental results demonstrate competitive performance, high efficiency, and strong robustness of our method compared to the latest available image quality metrics.},
  archive      = {J_TMM},
  author       = {Weiling Chen and Ke Gu and Tiesong Zhao and Gangyi Jiang and Patrick Le Callet},
  doi          = {10.1109/TMM.2020.2991546},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {1008-1020},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semi-reference sonar image quality assessment based on task and visual perception},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Query reconstruction network for referring expression image
segmentation. <em>TMM</em>, <em>23</em>, 995–1007. (<a
href="https://doi.org/10.1109/TMM.2020.2991504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring expression image segmentation aims at segmenting out the object described by a natural language query. Due to the diversity of visual content and language descriptions, it is very challenging to accurately model the correspondence between the vision and language, which inevitably produces some undesired segmentation objects from the queries. In this paper, we propose a query reconstruction network (QRN) to build more consistent corresponding relations between the language queries and object segmentation results. QRN not only generates segmentations from the queries and images but also reversely reconstructs the queries from the segmentations and the images. Through query reconstruction, QRN can confirm the vision-language consistency between the segmentations and queries. In the inference stage, for inconsistent segmentations and queries, we propose an iterative segmentation correction (ISC) method to correct them. ISC takes the difference between the reconstructed and input queries as a loss to optimize the proposed QRN. Then, the proposed QRN can generate new segmentations and queries. By iterative optimization, the segmentations can be gradually corrected. Extensive experiments on four referring expression image segmentation databases demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Hengcan Shi and Hongliang Li and Qingbo Wu and King Ngi Ngan},
  doi          = {10.1109/TMM.2020.2991504},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {995-1007},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Query reconstruction network for referring expression image segmentation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QoS-aware multicast for scalable video streaming in
software-defined networks. <em>TMM</em>, <em>23</em>, 982–994. (<a
href="https://doi.org/10.1109/TMM.2020.2991539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scalable Video Coding (SVC) is a promising coding technique that enables flexible video transmission to support heterogeneous devices. However, the current best-effort delivery model characterized by the Internet cannot fully exploit the scalability feature of SVC because the network nodes are transparent for multimedia streaming applications. Software-defined networking (SDN) has emerged as an innovative network paradigm that decouples the control and forwarding planes while routing. This architecture with the OpenFlow protocol enables network operators to obtain per-flow QoS control in a more scalable, flexible, and finely granular manner compared to the conventional network architecture. Inspired by these facts, this paper designs a comprehensive QoS-aware multicast scheme for scalable video streaming over SDN networks. First, we present the designed multimedia streaming multicast system architecture and formulate the QoS-aware multicast routing problem as a non-linear programming model. We then propose a fundamental tree construction algorithm that decomposes the video streaming requests into subrequests and serves them in a bottom-up manner. Furthermore, we design a layer switching strategy based on the video distortion model to mitigate network-induced video quality distortion. Finally, the experimental results are presented to validate the performance of our methods in terms of scalability, network utility, video playout quality, and playback interruption ratio.},
  archive      = {J_TMM},
  author       = {Hong Zhong and Fei Wu and Yan Xu and Jie Cui},
  doi          = {10.1109/TMM.2020.2991539},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {982-994},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {QoS-aware multicast for scalable video streaming in software-defined networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive cross-modal fusion network for RGB-d saliency
detection. <em>TMM</em>, <em>23</em>, 967–981. (<a
href="https://doi.org/10.1109/TMM.2020.2991523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an attentive cross-modal fusion (ACMF) network is proposed for RGB-D salient object detection. The proposed method selectively fuses features in a cross-modal manner and uses a fusion refinement module to fuse output features from different resolutions. Our attentive cross-modal fusion network is built based on residual attention. In each level of ResNet output, both the RGB and depth features are turned into an identity map and a weighted attention map. The identity map is reweighted by the attention map of the paired modality. Moreover, the lower level features with higher resolution are adopted to refine the boundary of detected targets. The entire architecture can be trained end-to-end. The proposed ACMF is compared with state-of-the-art methods on eight recent datasets. The results demonstrate that our model can achieve advanced performance on RGB-D salient object detection.},
  archive      = {J_TMM},
  author       = {Di Liu and Kao Zhang and Zhenzhong Chen},
  doi          = {10.1109/TMM.2020.2991523},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {967-981},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Attentive cross-modal fusion network for RGB-D saliency detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Blind quality assessment for tone-mapped images by analysis
of gradient and chromatic statistics. <em>TMM</em>, <em>23</em>,
955–966. (<a href="https://doi.org/10.1109/TMM.2020.2991528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tone-mapped image (TMI) obtained from the corresponding high dynamic range (HDR) image induces artifacts and distortion, which might result in the loss of structure information and impaired color. By analyzing the visual characteristics of TMIs, this work proposes a robust blind visual quality evaluation method for TMIs by using gradient and chromatic statistics (VQGC). First, motivated by the perceptual mechanism that the human visual system (HVS) is sensitive to image structure variation, we employ the gradient features to measure structure degradation in TMIs. To predict structure distortion accurately, we compute the gradient magnitude and orientation to measure image structure variation, and the relative gradient magnitude and orientation are also computed to capture microstructure change. Second, the color invariance descriptors are utilized to capture the visual degradation of colorfulness by local binary pattern (LBP) on four chromatic feature maps. Finally, the gradient and chromatic features are combined together as the final quality-aware feature vector, which is applied to assess the perceptual quality of TMIs by support vector regression (SVR). Comparison experiments show that the performance of the proposed method is better than other existing blind quality assessment methods on public databases.},
  archive      = {J_TMM},
  author       = {Yuming Fang and Jiebin Yan and Rengang Du and Yifan Zuo and Wenying Wen and Yan Zeng and Leida Li},
  doi          = {10.1109/TMM.2020.2991528},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {955-966},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind quality assessment for tone-mapped images by analysis of gradient and chromatic statistics},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep loss driven multi-scale hashing based on pyramid
connected network. <em>TMM</em>, <em>23</em>, 939–954. (<a
href="https://doi.org/10.1109/TMM.2020.2991513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the great success of the deep learning, deep hashing for large-scale multimedia retrieval has made significant progress recently. However, most existing deep hashing algorithms suffer from slow convergence due to the gradient vanishing problem, caused by deep network structures and saturated activation functions. Moreover, a single convolution layer is often followed by down-sampling such as max pooling, resulting in local information loss that might affect the overall system robustness and performance. In this work, we propose a novel deep supervised hashing, Deep Loss Driven Multi-Scale Hashing (DLDMSH), which learns the high-quality approximate binary codes through an end-to-end network and improves the representative capacity of hash codes for large-scale image retrieval. Specifically, we design a Loss Driven Multi-Scale (LDMS) feature which is aggregated from convolutional feature maps. Moreover, a Pyramid Connected Convolutional Neural Network (PCNet) architecture is devised to generate LDMS feature, which inputs pairs of images during the training and outputs an image to approximate discrete values. In particular, 1 × 1 convolution kernels are applied to make a linear combination of features for realizing feature reduction, and the reduced features are fused in the fusion layer. This effectively improves the performance of deep features. A novel loss function preserving semantic information is integrated into an end-to-end learning scheme, which enhances the representative capacity of binary codes. Extensive experiments over four benchmark datasets show that DLDMSH significantly outperforms several other state-of-the-art hashing methods.},
  archive      = {J_TMM},
  author       = {Lingchen Gu and Ju Liu and Xiaoxi Liu and Jiande Sun},
  doi          = {10.1109/TMM.2020.2991513},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {939-954},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep loss driven multi-scale hashing based on pyramid connected network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A coarse-to-fine facial landmark detection method based on
self-attention mechanism. <em>TMM</em>, <em>23</em>, 926–938. (<a
href="https://doi.org/10.1109/TMM.2020.2991507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark detection in the wild remains a challenging problem in computer vision. Deep learning-based methods currently play a leading role in solving this. However, these approaches generally focus on local feature learning and ignore global relationships. Therefore, in this study, a self-attention mechanism is introduced into facial landmark detection. Specifically, a coarse-to-fine facial landmark detection method is proposed that uses two stacked hourglasses as the backbone, with a new landmark-guided self-attention (LGSA) block inserted between them. The LGSA block learns the global relationships between different positions on the feature map and allows feature learning to focus on the locations of landmarks with the help of a landmark-specific attention map, which is generated in the first-stage hourglass model. A novel attentional consistency loss is also proposed to ensure the generation of an accurate landmark-specific attention map. A new channel transformation block is used as the building block of the hourglass model to improve the model&#39;s capacity. The coarse-to-fine strategy is adopted during and between phases to reduce complexity. Extensive experimental results on public datasets demonstrate the superiority of our proposed method against state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Pengcheng Gao and Ke Lu and Jian Xue and Ling Shao and Jiayi Lyu},
  doi          = {10.1109/TMM.2020.2991507},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {926-938},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A coarse-to-fine facial landmark detection method based on self-attention mechanism},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling QoE for buffered video streaming in
interference-limited cellular networks. <em>TMM</em>, <em>23</em>,
911–925. (<a href="https://doi.org/10.1109/TMM.2020.2990078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile networks have to cope with an ever increasing demand for video streaming, an application that imposes high quality requirements for user satisfaction. In this paper, we present an analytical model to calculate two important quality measures for streaming traffic, namely the video startup delay distribution and the buffer starvation probability. The queuing-theoretic model differs from related work by incorporating data flow dynamics of the considered cell, as well as the dynamics of fluctuating interference from neighboring cells, with the goal of accurately representing a multi-cellular environment. In this regard, we propose a finite-volume method to approximate the solution of the involved system of partial differential equations, where other approaches from comparable work were faced with numerical problems. We also evaluate two simplified versions of the model, where only the interference dynamics or all coupling terms are omitted, respectively. The model allows us to study the impact of different parameters on buffered video streaming performance. Additionally, we propose a user-centric metric to measure quality of experience (QoE). To the best of our knowledge, our approach is novel and has not been covered by comparable work. The presented results can help to design future cellular networks with enhanced video streaming experience.},
  archive      = {J_TMM},
  author       = {Philipp Schulz and Henrik Klessig and Meryem Simsek and Gerhard Fettweis},
  doi          = {10.1109/TMM.2020.2990078},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {911-925},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling QoE for buffered video streaming in interference-limited cellular networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mutually attentive co-training framework for
semi-supervised recognition. <em>TMM</em>, <em>23</em>, 899–910. (<a
href="https://doi.org/10.1109/TMM.2020.2990063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-training plays an important role in practical recognition applications where sufficient clean labels are unavailable. Existing methods focus on generating reliable pseudo labels to retrain a model, while ignoring the importance of improving model reliability to those inevitably mislabeled data. In this paper, we propose a novel Mutually Attentive Co-training Framework (MACF) that can effectively alleviate the negative impacts of incorrect labels on model retraining by exploring deep model disagreements. Specifically, MACF trains two symmetrical sub-networks that have the same input and are connected by several attention modules at different layers. Each attention module analyzes the inferred features from two sub-networks for the same input and feedback attention maps for them to indicate noisy gradients. This is realized by exploring the back-propagation process of incorrect labels at different layers to design attention modules. By multi-layer interception, the noisy gradients caused by incorrect labels can be effectively reduced for both sub-networks, leading to robust training to potential incorrect labels. In addition, a hierarchical distillation strategy is developed to improve the pseudo labels by aggregating the predictions from multi-models and data transformations. The experiments on six general benchmarks, including classification and biomedical segmentation, demonstrate that MACF is much robust to noisy labels than previous methods.},
  archive      = {J_TMM},
  author       = {Shaobo Min and Xuejin Chen and Hongtao Xie and Zheng-Jun Zha and Yongdong Zhang},
  doi          = {10.1109/TMM.2020.2990063},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {899-910},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A mutually attentive co-training framework for semi-supervised recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical soft quantization for skeleton-based human
action recognition. <em>TMM</em>, <em>23</em>, 883–898. (<a
href="https://doi.org/10.1109/TMM.2020.2990082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In daily life, human beings rely on hands and body parts to complete particular actions cooperatively. These selected body parts and their cooperative relationships are essential cues to distinguish these actions. However, most existing action recognition methods, which try to model the body appearance or spatial relations in skeleton sequences, often ignore the essential cooperation relationship among joints. Differently, in this paper, we propose a spatio-temporal hierarchical soft quantization method to extract the congenerous motion features, which reflect the cooperation relations among joints and body parts. Specifically, we design a hierarchical network with multiple soft quantization layers to extract congenerous features. The hierarchical network not only models the spatial hierarchy of skeleton structure for joint, part, and body, but also extracts the temporal hierarchy with sliding windows for frame, fragment, and sequence. Moreover, the features in each layer are visually explainable, which reflect the cooperation among body parts. The trainable parameters in the network are also significantly reduced, which reduces computational cost. Extensive experiments conducted on four benchmarks demonstrate that our method can provide competitive results compared with state-of-the-arts. The visualized congenerous features also validate that our approach can effectively perceive the essential cooperation relations.},
  archive      = {J_TMM},
  author       = {Jianyu Yang and Wu Liu and Junsong Yuan and Tao Mei},
  doi          = {10.1109/TMM.2020.2990082},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {883-898},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical soft quantization for skeleton-based human action recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive learning of low-precision networks for image
classification. <em>TMM</em>, <em>23</em>, 871–882. (<a
href="https://doi.org/10.1109/TMM.2020.2990087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a great advance of deep learning in a variety of vision tasks. Many state-of-the-art deep neural networks suffer from large size and high complexity, which makes them difficult to deploy in resource-limited platforms such as mobile devices. To this end, low-precision neural networks are widely studied that quantize weights or activations into the low-bit format. Although efficient, low-precision networks are usually difficult to train and encounter severe accuracy degradation. In this paper, we propose a new training strategy based on progressive learning for image classification. First, we equip each low-precision convolutional layer with an ancillary full-precision convolutional layer based on a low-precision network structure. Second, a decay method is introduced to reduce the output of the added full-precision convolution gradually, which keeps the resulting topology structure the same as the original low-precision convolution. Extensive experiments on SVHN, CIFAR and ILSVRC-2012 datasets reveal that the proposed method can bring faster convergence and higher accuracy for low-precision neural networks.},
  archive      = {J_TMM},
  author       = {Zhengguang Zhou and Wengang Zhou and Xutao Lv and Xuan Huang and Xiaoyu Wang and Houqiang Li},
  doi          = {10.1109/TMM.2020.2990087},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {871-882},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive learning of low-precision networks for image classification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). CAT: Corner aided tracking with deep regression network.
<em>TMM</em>, <em>23</em>, 859–870. (<a
href="https://doi.org/10.1109/TMM.2020.2990089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single object tracking in visual media is an important yet challenging task. Various challenges, especially target scale variation, shape deformation and occlusion, can have large effects on the performances of trackers. Current deep regression based trackers only pay close attention to regression on the center key point of the tracking target, meanwhile employ the image pyramid based multi-scale testing method to deal with scale estimation. Such procedure can not properly handle the three challenges. We address these challenges in a principled way by the aid of auxiliary regressions on the four bounding box corners of the tracking target. In this work, we propose the novel Corner Aided Tracker with deep regression network, abbreviated as CAT. Different from RPN-based trackers, in CAT, four corners along with the center key point of the bounding box for tracking target are simultaneously obtained by five corresponding response maps. Furthermore, to robustly and accurately generate tight bounding boxes for the tracking target and collect reliable samples for online training of the network, we propose an adaptive key point selection method to select the subset of reliable key points and drop the unreliable ones, based on the qualities of their corresponding response maps as well as the constraints from shape, scale and location. We demonstrate that the regressed corners can help naturally locate the tracking target with tight bounding boxes. The challenges of scale variation, shape deformation and occlusion can be handled explicitly. The commonly used time-consuming image pyramid based multi-scale testing method can also be discarded. Extensive experiments on OTB2013, OTB2015, UAV123, LaSOT, VOT2016 and VOT2018 datasets are conducted to report new state-of-the-art performances and demonstrate the effectiveness of CAT.},
  archive      = {J_TMM},
  author       = {Shiquan Zhang and Xu Zhao and Liangji Fang},
  doi          = {10.1109/TMM.2020.2990089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {859-870},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CAT: Corner aided tracking with deep regression network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single shot video object detector. <em>TMM</em>,
<em>23</em>, 846–858. (<a
href="https://doi.org/10.1109/TMM.2020.2990070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single shot detectors that are potentially faster and simpler than two-stage detectors tend to be more applicable to object detection in videos. Nevertheless, the extension of such object detectors from image to video is not trivial especially when appearance deterioration exists in videos, e.g., motion blur or occlusion. A valid question is how to explore temporal coherence across frames for boosting detection. In this paper, we propose to address the problem by enhancing per-frame features through aggregation of neighboring frames. Specifically, we present Single Shot Video Object Detector (SSVD) - a new architecture that novelly integrates feature aggregation into a one-stage detector for object detection in videos. Technically, SSVD takes Feature Pyramid Network (FPN) as backbone network to produce multi-scale features. Unlike the existing feature aggregation methods, SSVD, on one hand, estimates the motion and aggregates the nearby features along the motion path, and on the other, hallucinates features by directly sampling features from the adjacent frames in a two-stream structure. Extensive experiments are conducted on ImageNet VID dataset, and competitive results are reported when comparing to state-of-the-art approaches. More remarkably, for $448 \times 448$ input, SSVD achieves 79.2% mAP on ImageNet VID, by processing one frame in 85 ms on an Nvidia Titan X Pascal GPU. The code is available at https://github.com/ddjiajun/SSVD.},
  archive      = {J_TMM},
  author       = {Jiajun Deng and Yingwei Pan and Ting Yao and Wengang Zhou and Houqiang Li and Tao Mei},
  doi          = {10.1109/TMM.2020.2990070},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {846-858},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Single shot video object detector},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CaptionNet: A tailor-made recurrent neural network for
generating image descriptions. <em>TMM</em>, <em>23</em>, 835–845. (<a
href="https://doi.org/10.1109/TMM.2020.2990074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is a challenging task of visual understanding and has drawn more attention of researchers. In general, two inputs are required at each time step by the Long Short-Term Memory (LSTM) network used in popular attention based image captioning frameworks, including image features and previous generated words. However, error will be accumulated if the previous words are not accurate and the related semantic is not efficient enough. Facing these challenges, a novel model named CaptionNet is proposed in this work as an improved LSTM specially designed for image captioning. Concretely, only attended image features are allowed to be fed into the memory of CaptionNet through input gates. In this way, the dependency on the previous predicted words can be reduced, forcing model to focus on more visual clues of images at the current time step. Moreover, a memory initialization method called image feature encoding is designed to capture richer semantics of the target image. The evaluation on the benchmark MSCOCO and Flickr30K datasets demonstrates the effectiveness of the proposed CaptionNet model, and extensive ablation studies are performed to verify each of the proposed methods. The project page can be found in https://mic.tongji.edu.cn/3f/9c/c9778a147356/page.htm.},
  archive      = {J_TMM},
  author       = {Longyu Yang and Hanli Wang and Pengjie Tang and Qinyu Li},
  doi          = {10.1109/TMM.2020.2990074},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {835-845},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CaptionNet: A tailor-made recurrent neural network for generating image descriptions},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy optimization and QoE satisfaction for wireless visual
sensor networks in multi target tracking scenario. <em>TMM</em>,
<em>23</em>, 823–834. (<a
href="https://doi.org/10.1109/TMM.2020.2990077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, by emerging new technologies, demand for a wireless visual sensor networks (WVSN) has been significantly increased. By providing the vital visual data from such networks, they play a substantial role in the surveillance applications. The wireless visual sensors (VSes) are the main component of these networks, which are equipped with camera and transceiver module. Energy optimization and satisfying the quality of the captured visual data, are two main contradictable issues in this area of research, especially in the target tracking applications. Therefore, these two issues have been simultaneously investigated in this paper. The coverage of the tracked targets and the quality of the captured visual data are considered as the quality of experienced (QoE). The desired threshold of QoE is defined by user. To optimize energy consumption with regard to QoE constraints in a multi target tracking scenario, an effective VSes selection is proposed. This method has been developed based on the convex optimization framework. Besides, the focal length of the VSes is set optimally by executing the proposed algorithm. Simulation results show efficiency of the proposed method compared with the optimal method (exhaustive search).},
  archive      = {J_TMM},
  author       = {Reza Ghazalian and Ali Aghagolzadeh and Seyed Mehdi Hosseini Andargoli},
  doi          = {10.1109/TMM.2020.2990077},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {823-834},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Energy optimization and QoE satisfaction for wireless visual sensor networks in multi target tracking scenario},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intermittent contextual learning for keyfilter-aware UAV
object tracking using deep convolutional feature. <em>TMM</em>,
<em>23</em>, 810–822. (<a
href="https://doi.org/10.1109/TMM.2020.2990064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking, one of the most favorable multimedia applications, has been widely used in unmanned aerial vehicle (UAV) for civil infrastructure monitoring, aerial cinematography, autonomous navigation, etc. Most existing trackers utilize deep convolutional feature to enhance tracking robustness in scenarios of various appearance variation. However, they commonly neglect speed which is crucial for UAV with restricted calculation resources. In this work, a novel correlation filter-based keyfilter-aware tracker with a new intermittent context learning strategy is proposed to efficiently and effectively alleviate the problems of background clutter, deficient description, occlusion, illumination change, etc. Specifically, context information is utilized to empower the filter higher discriminating ability through response repression of the omnidirectional context patches. Furthermore, keyfilter is produced from the periodically selected keyframe. The latest produced keyfilter is used to restrain the current filter&#39;s corrupted changes. Most importantly, context learning of correlation filter is implemented intermittently to fully increase the tracking efficiency. This intermittent learning strategy can ensure every filter maintain context awareness owing to the restriction of keyfilter, periodically enhancing the context awareness. Substantial experiments on three challenging UAV benchmarks totally with 213 image sequences have shown that our tracker surpasses the state-of-the-art results, and exhibits a remarkable generality in short-term and long-term UAV tracking tasks as well as a variety of challenging attributes.},
  archive      = {J_TMM},
  author       = {Yiming Li and Changhong Fu and Ziyuan Huang and Yinqiang Zhang and Jia Pan},
  doi          = {10.1109/TMM.2020.2990064},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {810-822},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Intermittent contextual learning for keyfilter-aware UAV object tracking using deep convolutional feature},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Panoramic video quality assessment based on non-local
spherical CNN. <em>TMM</em>, <em>23</em>, 797–809. (<a
href="https://doi.org/10.1109/TMM.2020.2990075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoramic video and stereoscopic panoramic video are essential carriers of virtual reality content, so it is very crucial to establish their quality assessment models for the standardization of virtual reality industry. However, it is very challenging to evaluate the quality of the panoramic video at present. One reason is that the spatial information of the panoramic video is warped due to the projection process, and the conventional video quality assessment (VQA) method is difficult to deal with this problem. Another reason is that the traditional VQA method is problematic to capture the complex global time information in the panoramic video. In response to the above questions, this paper presents an end-to-end neural network model to evaluate the quality of panoramic video and stereoscopic panoramic video. Compared to other panoramic video quality assessment methods, our proposed method combines spherical convolutional neural networks (CNN) and non-local neural networks, which can effectively extract complex spatiotemporal information of the panoramic video. We evaluate the method in two databases, VRQ-TJU and VR-VQA48. Experiments show the effectiveness of different modules in our method, and our method outperforms state-of-the-art other related methods.},
  archive      = {J_TMM},
  author       = {Jiachen Yang and Tianlin Liu and Bin Jiang and Wen Lu and Qinggang Meng},
  doi          = {10.1109/TMM.2020.2990075},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {797-809},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Panoramic video quality assessment based on non-local spherical CNN},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An attention-based unsupervised adversarial model for movie
review spam detection. <em>TMM</em>, <em>23</em>, 784–796. (<a
href="https://doi.org/10.1109/TMM.2020.2990085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of the Internet, online reviews have become a valuable information resource for people. However, the authenticity of online reviews remains a concern, and deceptive reviews have become one of the most urgent network security problems to be solved. Review spams will mislead users into making suboptimal choices and inflict their trust in online reviews. Most existing research manually extracted features and labeled training samples, which are usually complicated and time-consuming. This paper focuses primarily on a neglected emerging domain - movie review, and develops a novel unsupervised spam detection model with an attention mechanism. By extracting the statistical features of reviews, it is revealed that users will express their sentiments on different aspects of movies in reviews. An attention mechanism is introduced in the review embedding, and the conditional generative adversarial network is exploited to learn users’ review style for different genres of movies. The proposed model is evaluated on movie reviews crawled from Douban, a Chinese online community where people could express their feelings about movies. The experimental results demonstrate the superior performance of the proposed approach.},
  archive      = {J_TMM},
  author       = {Yuan Gao and Maoguo Gong and Yu Xie and A. K. Qin},
  doi          = {10.1109/TMM.2020.2990085},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {784-796},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An attention-based unsupervised adversarial model for movie review spam detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequency-dependent depth map enhancement via iterative
depth-guided affine transformation and intensity-guided refinement.
<em>TMM</em>, <em>23</em>, 772–783. (<a
href="https://doi.org/10.1109/TMM.2020.2987706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural network sho-ws significant improvement for intensity-guided depth map enhancement. The most networks focus on either increasing depth or easing features propagation via residual learning and dense connection. However, it has not been explicitly considered yet to mitigate the artifacts caused by the differences of the distributions between the depth map and the corresponding color image, e.g., edge misalignment. In this paper, a novel depth-guided affine transformation is used to filter out the unrelated intensity features, which is further used to refine the depth features. Since the quality of initial depth features is low, the depth-guided intensity features filtering and the intensity-guided depth features refinement are iteratively performed, which progressively promotes effects of such tasks. To make full use of the iterations, all the refined depth features are dense connected followed by a 1 × 1 convolution layer. In addition, to improve the performance in the case of large upsampling factors (e.g., 16×), the depth features are enhanced from coarse to fine. In each frequency-dependent refinement of the depth features, the above iterative subnetwork as well as the residual learning are introduced. The proposed method is tested for the noise-free and noisy cases which compares against 16 state-of-the-art methods. Our experimental results show the improved performances based on the qualitative and quantitative evaluations.},
  archive      = {J_TMM},
  author       = {Yifan Zuo and Yuming Fang and Ping An and Xiwu Shang and Junnan Yang},
  doi          = {10.1109/TMM.2020.2987706},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {772-783},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frequency-dependent depth map enhancement via iterative depth-guided affine transformation and intensity-guided refinement},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning crisp boundaries using deep refinement network and
adaptive weighting loss. <em>TMM</em>, <em>23</em>, 761–771. (<a
href="https://doi.org/10.1109/TMM.2020.2987685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made in boundary detection with the help of convolutional neural networks. Recent boundary detection models not only focus on real object boundary detection but also “crisp” boundaries (precisely localized along the object&#39;s contour). There are two methods to evaluate crisp boundary performance. One uses more strict tolerance to measure the distance between the ground truth and the detected contour. The other focuses on evaluating the contour map without any postprocessing. In this study, we analyze both methods and conclude that both methods are two aspects of crisp contour evaluation. Accordingly, we propose a novel network named deep refinement network (DRNet) that stacks multiple refinement modules to achieve richer feature representation and a novel loss function, which combines cross-entropy and dice loss through effective adaptive fusion. Experimental results demonstrated that we achieve state-of-the-art performance for several available datasets.},
  archive      = {J_TMM},
  author       = {Yi-Jun Cao and Chuan Lin and Yong-Jie Li},
  doi          = {10.1109/TMM.2020.2987685},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {761-771},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning crisp boundaries using deep refinement network and adaptive weighting loss},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewport-dependent saliency prediction in 360° video.
<em>TMM</em>, <em>23</em>, 748–760. (<a
href="https://doi.org/10.1109/TMM.2020.2987682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency prediction in traditional images and videos has drawn extensive research interests in recent years. Few works have been proposed for saliency prediction over 360° videos. They focus on directly predicting fixations over the whole panorama. When viewing 360° videos, a person can only observe the content in her viewport, which means that only a fraction of the 360° scene can be seen at any given time. In this paper, we study human attention over viewport of 360° videos and propose a novel visual saliency model, dubbed viewport saliency, to predict fixations over 360° videos. Two contributions are introduced. First, we find that where people look is affected by the content and location of the viewport in 360° video. We study this over 200+ 360° videos viewed by 30+ subjects over two recent benchmark databases. Second, we propose a Multi-Task Deep Neural Network (MT-DNN) method for Viewport Saliency (VS) prediction in 360° video, which considers the input content and location of the viewport. Extensive experiments and analyses show that our method outperforms other state-of-the-art methods in this task. In particular, over the two recent 360° video databases, our MT-DNN raises the average CC score by 0.149 and 0.205, compared to SalGAN and DeepVS methods, respectively.},
  archive      = {J_TMM},
  author       = {Minglang Qiao and Mai Xu and Zulin Wang and Ali Borji},
  doi          = {10.1109/TMM.2020.2987682},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {748-760},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Viewport-dependent saliency prediction in 360° video},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patch based video summarization with block sparse
representation. <em>TMM</em>, <em>23</em>, 732–747. (<a
href="https://doi.org/10.1109/TMM.2020.2987683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sparse representation has been successfully utilized for video summarization (VS). However, most of the sparse representation based VS methods characterize each video frame with global features. As a result, some important local details could be neglected by global features, which may compromise the performance of summarization. In this paper, we propose to partition each video frame into a number of patches and characterize each patch with global features. Instead of concatenating the features of each patch and utilizing conventional sparse representation, we formulate the VS problem with such video frame representation as block sparse representation by considering each video frame as a block containing a number of patches. By taking the reconstruction constraint into account, we devise a simultaneous version of block-based OMP (Orthogonal Matching Pursuit) algorithm, namely SBOMP, to solve the proposed model. The proposed model is further extended to a neighborhood based model which considers temporally adjacent frames as a super block. This is one of the first sparse representation based VS methods taking both spatial and temporal contexts into account with blocks. Experimental results on two widely used VS datasets have demonstrated that our proposed methods present clear superiority over existing sparse representation based VS methods and are highly comparable to some deep learning ones requiring supervision information for extra model training.},
  archive      = {J_TMM},
  author       = {Shaohui Mei and Mingyang Ma and Shuai Wan and Junhui Hou and Zhiyong Wang and David Dagan Feng},
  doi          = {10.1109/TMM.2020.2987683},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {732-747},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Patch based video summarization with block sparse representation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive adaptive streaming to enable mobile 360-degree
and VR experiences. <em>TMM</em>, <em>23</em>, 716–731. (<a
href="https://doi.org/10.1109/TMM.2020.2987693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As 360-degree videos and virtual reality (VR) applications become popular for consumer and enterprise use cases, the desire to enable truly mobile experiences also increases. Delivering 360-degree videos and cloud/edge-based VR applications require ultra-high bandwidth and ultra-low latency [1] , challenging to achieve with mobile networks. A common approach to reduce bandwidth is streaming only the field of view (FOV). However, extracting and transmitting the FOV in response to user head motion can add high latency, adversely affecting user experience. In this paper, we propose a predictive adaptive streaming approach, where the predicted view with high predictive probability is adaptively encoded in relatively high quality according to bandwidth conditions and transmitted in advance, leading to a simultaneous reduction in bandwidth and latency. The predictive adaptive streaming method is based on a deep-learning-based viewpoint prediction model we develop, which uses past head motions to predict where a user will be looking in the 360-degree view. Using a very large dataset consisting of head motion traces from over 36,000 viewers for nineteen 360-degree/VR videos, we validate the ability of our predictive adaptive streaming method to offer high-quality view while simultaneously significantly reducing bandwidth.},
  archive      = {J_TMM},
  author       = {Xueshi Hou and Sujit Dey and Jianzhong Zhang and Madhukar Budagavi},
  doi          = {10.1109/TMM.2020.2987693},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {716-731},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Predictive adaptive streaming to enable mobile 360-degree and VR experiences},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A hierarchical visual feature-based approach for image
sonification. <em>TMM</em>, <em>23</em>, 706–715. (<a
href="https://doi.org/10.1109/TMM.2020.2987710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new image sonification system that strives to help visually impaired users access visual information via an audio (easily decodable) signal that is generated in real time when the users explore the image on a touch screen or with a pointer. The sonified signal, which is generated for each position within the image, tries to capture the most useful and discriminant local information about the image content at different levels of abstraction, ranging from low-level (at the pixel level) to high-level (segmentation) and combining low-level (color edges and texture), mid-level and high-level (gradient or color distribution for each region of the image) features. The proposed system mainly uses musical notes at several octaves, the notion of timbre, and loudness but also uses pitch, rhythm and the distortion effect in an intuitive way to sonify the image content both locally and globally. To this end, we use perceptually meaningful mappings, in which the properties of an image are directly reflected in the audio domain, in a very predictable way. The listener can then draw simple and reliable conclusions about the image by quickly decoding the sonified result.},
  archive      = {J_TMM},
  author       = {Ohini Kafui Toffa and Max Mignotte},
  doi          = {10.1109/TMM.2020.2987710},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {706-715},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hierarchical visual feature-based approach for image sonification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting local degradation characteristics and global
statistical properties for blind quality assessment of tone-mapped HDR
images. <em>TMM</em>, <em>23</em>, 692–705. (<a
href="https://doi.org/10.1109/TMM.2020.2986583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tone mapping operators (TMOs) are developed to convert a high dynamic range (HDR) image into a low dynamic range (LDR) one for display with the goal of preserving as much visual information as possible. However, image quality degradation is inevitable due to the dynamic range compression during the tone-mapping process. This accordingly raises an urgent demand for effective quality evaluation methods to select a high-quality tone-mapped image (TMI) from a set of candidates generated by distinct TMOs or the same TMO with different parameter settings. A key element to the success of TMI quality evaluation is to extract effective features that are highly consistent with human perception. Towards this end, this paper proposes a novel blind TMI quality metric by exploiting both local degradation characteristics and global statistical properties for feature extraction. Several image attributes including texture, structure, colorfulness and naturalness are considered either locally or globally. The extracted local and global features are aggregated into an overall quality via regression. Experimental results on two benchmark databases demonstrate the superiority of the proposed metric over both the state-of-the-art blind quality models designed for synthetically distorted images (SDIs) and the blind quality models specifically developed for TMIs.},
  archive      = {J_TMM},
  author       = {Xuejin Wang and Qiuping Jiang and Feng Shao and Ke Gu and Guangtao Zhai and Xiaokang Yang},
  doi          = {10.1109/TMM.2020.2986583},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {692-705},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploiting local degradation characteristics and global statistical properties for blind quality assessment of tone-mapped HDR images},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking image retrieval diversification techniques for
social media. <em>TMM</em>, <em>23</em>, 677–691. (<a
href="https://doi.org/10.1109/TMM.2020.2986579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval has been an active research domain for over 30 years and historically it has focused primarily on precision as an evaluation criterion. Similar to text retrieval, where the number of indexed documents became large and many relevant documents exist, it is of high importance to highlight diversity in the search results to provide better results for the user. The Retrieving Diverse Social Images Task of the MediaEval benchmarking campaign has addressed exactly this challenge of retrieving diverse and relevant results for the past years, specifically in the social media context. Multimodal data (e.g., images, text) was made available to the participants including metadata assigned to the images, user IDs, and precomputed visual and text descriptors. Many teams have participated in the task over the years. The large number of publications employing the data and also citations of the overview articles underline the importance of this topic. In this paper, we introduce these publicly available data resources as well as the evaluation framework, and provide an in-depth analysis of the crucial aspects of social image search diversification, such as the capabilities and the evolution of existing systems. These evaluation resources will help researchers for the coming years in analyzing aspects of multimodal image retrieval and diversity of the search results.},
  archive      = {J_TMM},
  author       = {Bogdan Ionescu and Maia Rohm and Bogdan Boteanu and Alexandru Lucian Gînscă and Mihai Lupu and Henning Müller},
  doi          = {10.1109/TMM.2020.2986579},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {677-691},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Benchmarking image retrieval diversification techniques for social media},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Utilizing two-phase processing with FBLS for single image
deraining. <em>TMM</em>, <em>23</em>, 664–676. (<a
href="https://doi.org/10.1109/TMM.2020.2987703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain removal from a single image is a challenging problem and has attracted much attention in recent years. In this paper, we revisit the single image deraining problem, and present a novel solution. The central idea of our solution is to merge the merits of two-phase processing methods and the Fuzzy Broad Learning System (FBLS). Specifically, our solution first uses the dehazing algorithm to preprocess the input rainy image and separates it into the detail layer and the base layer. After that, it puts the Y-channel image of the detail layer into the FBLS to obtain the derained Y channel image, which is then combined with the Cb and Cr channel images to obtain the derained detail layer. Later, it fuses the derained detail layer and the base layer to get a preliminary derained image. Finally, it superimposes the details extracted from the dehazed image with some transparency on the preliminary result, obtaining the final result. Experimental results based on both real and synthetic rainy images demonstrate that our proposed solution can outperform several state-of-the-art algorithms, while it consumes much less running time and training time, compared against the competitors.},
  archive      = {J_TMM},
  author       = {Xiao Lin and Lizhuang Ma and Bin Sheng and Zhi-Jie Wang and Wansheng Chen},
  doi          = {10.1109/TMM.2020.2987703},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {664-676},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Utilizing two-phase processing with FBLS for single image deraining},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TCLiVi: Transmission control in live video streaming based
on deep reinforcement learning. <em>TMM</em>, <em>23</em>, 651–663. (<a
href="https://doi.org/10.1109/TMM.2020.2985631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, video content accounts for the majority of network traffic. With increased live streaming, rigorous requirements have been introduced for better Quality of Experience (QoE). It is challenging to meet satisfactory QoE in live streaming, where the aim is to achieve a balance between 1) enhancing the video quality and stability and 2) reducing the rebuffering time and end-to-end delay, under different scenarios with various network conditions and user preferences, where the fluctuation in the network throughput degrades the QoE severely. In this paper, we propose an approach to improve the QoE for live video streaming based on Deep Reinforcement Learning (DRL). The new approach jointly adjusts the streaming parameters, including the video bitrate and target buffer size. With the basic DRL framework, TCLiVi can automatically generate the inference model based on the playback information, to achieve the joint optimization of the video quality, stability, rebuffering time and latency parameters. We evaluate our framework on real-world data in different live streaming broadcast scenarios, such as a talent show and a sports competition under different network conditions. We compare TCLiVi with other algorithms, such as the Double DQN, MPC and Buffer-based algorithms. The simulation results show that TCLiVi significantly improves the video quality and decreases the rebuffering time, consequently increasing the QoE score by 40.84% in average. We also show that TCLiVi is self-adaptive in different scenarios.},
  archive      = {J_TMM},
  author       = {Laizhong Cui and Dongyuan Su and Shu Yang and Zhi Wang and Zhong Ming},
  doi          = {10.1109/TMM.2020.2985631},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {651-663},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TCLiVi: Transmission control in live video streaming based on deep reinforcement learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A recursive reversible data hiding in encrypted images
method with a very high payload. <em>TMM</em>, <em>23</em>, 636–650. (<a
href="https://doi.org/10.1109/TMM.2020.2985537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted images (RDHEI) can be used as an effective technique to embed additional data directly in the encrypted domain and therefore, without any invasion to privacy. In this way, RDHEI is especially useful for labeling encrypted images in cloud storage. In this paper, we propose a new method of data hiding in encrypted images, which is fully reversible and has a very high payload. All the bit-planes of an image are processed recursively, from the most significant one to the least significant by combining error prediction, reversible adaptation, encryption and embedding. For pixel prediction, the Median Edge Detector, also called LOCO-I and known to be efficient in JPEG-LS compression standard, is used for each bit-plane. Moreover, conversely to current state-of-the-art methods, in our proposed method, there is no pre-processing step to correct incorrectly predicted pixels and no flags to highlight them. Indeed, a reversible adaptation of the bit-planes is performed in order to make it possible to detect and correct all incorrectly predicted pixels during the decoding step. Thanks to the high correlation between pixels in the clear domain, a large part of the bits of an image can be substituted by bits of a secret message. Our experiments show that we can generally embed bits of the secret message until the fourth most-significant bit-plane of an image, this allows us to have an average payload value of 2.4586 bpp.},
  archive      = {J_TMM},
  author       = {Pauline Puteaux and William Puech},
  doi          = {10.1109/TMM.2020.2985537},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {636-650},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A recursive reversible data hiding in encrypted images method with a very high payload},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BR<span class="math inline"><sup>2</sup></span>net: Defocus
blur detection via a bidirectional channel attention residual refining
network. <em>TMM</em>, <em>23</em>, 624–635. (<a
href="https://doi.org/10.1109/TMM.2020.2985541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the remarkable potential applications, defocus blur detection, which aims to separate blurry regions from an image, has attracted much attention. Although significant progress has been made by many methods, there are still various challenges that hinder the results, e.g., confusing background areas, sensitivity to the scale and missing the boundary details of the defocus blur regions. To solve these issues, in this paper, we propose a deep convolutional neural network (CNN) for defocus blur detection via a Bi-directional Residual Refining network (BR 2 Net). Specifically, a residual learning and refining module (RLRM) is designed to correct the prediction errors in the intermediate defocus blur map. Then, we develop a bidirectional residual feature refining network with two branches by embedding multiple RLRMs into it for recurrently combining and refining the residual features. One branch of the network refines the residual features from the shallow layers to the deep layers, and the other branch refines the residual features from the deep layers to the shallow layers. In such a manner, both the low-level spatial details and highlevel semantic information can be encoded step by step in two directions to suppress background clutter and enhance the detected region details. The outputs of the two branches are fused to generate the final results. In addition, with the observation that different feature channels have different extents of discrimination for detecting blurred regions, we add a channel attention module to each feature extraction layer to select more discriminative features for residual learning. To promote further research on defocus blur detection, we create a new dataset with various challenging images and manually annotate their corresponding pixelwise ground truths. The proposed network is validated on two commonly used defocus blur detection datasets and our newly collected dataset by comparing it with 10 other state-of-the-art methods. Extensive experiments with ablation studies demonstrate that BR 2 Net consistently and significantly outperforms the competitors in terms of both the efficiency and accuracy.},
  archive      = {J_TMM},
  author       = {Chang Tang and Xinwang Liu and Shan An and Pichao Wang},
  doi          = {10.1109/TMM.2020.2985541},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {624-635},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BR$^2$Net: Defocus blur detection via a bidirectional channel attention residual refining network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond vision: A multimodal recurrent attention
convolutional neural network for unified image aesthetic prediction
tasks. <em>TMM</em>, <em>23</em>, 611–623. (<a
href="https://doi.org/10.1109/TMM.2020.2985526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, image aesthetic prediction has attracted increasing attention because of its wide applications, such as image retrieval, photo album management and aesthetic-driven image enhancement. However, previous studies in this area only achieve limited success because 1) they primarily depend on visual features and ignore textual information. 2) they tend to focus equally on to each part of images and ignore the selective attention mechanism. This paper overcomes these limitations by proposing a novel multimodal recurrent attention convolutional neural network (MRACNN). More specifically, the MRACNN consists of two streams: the vision stream and the language stream. The former employs the recurrent attention network to tune out irrelevant information and focuses on some key regions to extract visual features. The latter utilizes the Text-CNN to capture the high-level semantics of user comments. Finally, a multimodal factorized bilinear (MFB) pooling approach is used to achieve effective fusion of textual and visual features. Extensive experiments demonstrate that the proposed MRACNN significantly outperforms state-of-the-art methods for unified aesthetic prediction tasks: (i) aesthetic quality classification; (ii) aesthetic score regression; and (iii) aesthetic score distribution prediction.},
  archive      = {J_TMM},
  author       = {Xiaodan Zhang and Xinbo Gao and Wen Lu and Lihuo He and Jie Li},
  doi          = {10.1109/TMM.2020.2985526},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {611-623},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond vision: A multimodal recurrent attention convolutional neural network for unified image aesthetic prediction tasks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive unsupervised person re-identification by
tracklet association with spatio-temporal regularization. <em>TMM</em>,
<em>23</em>, 597–610. (<a
href="https://doi.org/10.1109/TMM.2020.2985525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for person re-identification (Re-ID) are mostly based on supervised learning which requires numerous manually labeled samples across all camera views for training. Such a paradigm suffers the scalability issue since in real-world Re-ID application, it is difficult to exhaustively label abundant identities over multiple disjoint camera views. To this end, we propose a progressive deep learning method for unsupervised person Re-ID in the wild by Tracklet Association with Spatio-Temporal Regularization (TASTR). In our approach, we first collect tracklet data within each camera by automatic person detection and tracking. Then, an initial Re-ID model is trained based on within-camera triplet construction for person representation learning. After that, based on the person visual feature and spatio-temporal constraint, we associate cross-camera tracklets to generate cross-camera triplets and update the Re-ID model. Lastly, with the refined Re-ID model, better visual feature of person can be extracted, which further promote the association of cross-camera tracklets. The last two steps are iterated multiple times to progressively upgrade the Re-ID model. To facilitate the study, we have collected a new 4K UHD video dataset named Campus4K with full frames and full spatio-temporal information. Experimental results show that with the spatio-temporal constraint in the training phase, the proposed approach outperforms the state-of-the-art unsupervised methods by notable margins on DukeMTMC-reID, and achieves competitive performance to fully supervised methods on both DukeMTMC-reID and Campus4K datasets.},
  archive      = {J_TMM},
  author       = {Qiaokang Xie and Wengang Zhou and Guo-Jun Qi and Qi Tian and Houqiang Li},
  doi          = {10.1109/TMM.2020.2985525},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {597-610},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive unsupervised person re-identification by tracklet association with spatio-temporal regularization},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learned resolution scaling powered gaming-as-a-service at
scale. <em>TMM</em>, <em>23</em>, 584–596. (<a
href="https://doi.org/10.1109/TMM.2020.2985538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Built on the explosive advancement of cloud and telecommunication technologies, Gaming-as-a-Service (GaaS) or cloud gaming system is expected to revolutionize the traditional multi-billion video game market in the near future. This wave is analogous to the rise of live-video-streaming-based-Netflix to replace conventional DVD rental business for movies and TVs. In practice, a successful GaaS platform need to operate in a transparent mode without requiring substantial efforts from both content providers and end users, and offer the pristine quality of experience (QoE) at an affordable cost. Our analysis suggests that GaaS provisioning cost can be reduced significantly by enforcing the game video rendering and streaming at a lower resolution (so as to increase the user concurrency in the cloud and reduce the streaming bandwidth over the network). However, streaming video at a lower resolution may deteriorate the QoE. To maintain the client QoE at the level using the default-native resolution for streaming or even enhance it, we introduce the learned resolution scaling (LRS), which leverages the computational capabilities at clients/edges to restore/improve the reconstructed image/video quality via stacked deep neural networks (DNN). We integrate this LRS into a commercialized GaaS platform - AnyGame, to study its efficiency and complexity quantitatively. Extensive real-life experiments have shown that LRS-powered AnyGame offers the state-of-the-art performance, and the lower operational cost, paving the road for a potential success of GaaS over the Internet. Additionally, we dive into proposed LRS via ablation studies to further demonstrate its consistent performance, including the discussions on trade-off between efficiency and complexity, alternative training sets, etc.},
  archive      = {J_TMM},
  author       = {Hao Chen and Ming Lu and Zhan Ma and Xu Zhang and Yiling Xu and Qiu Shen and Wenjun Zhang},
  doi          = {10.1109/TMM.2020.2985538},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {584-596},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learned resolution scaling powered gaming-as-a-service at scale},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Driver yawning detection based on subtle facial action
recognition. <em>TMM</em>, <em>23</em>, 572–583. (<a
href="https://doi.org/10.1109/TMM.2020.2985536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various investigations have shown that driver fatigue is the main cause of traffic accidents. Research on the use of computer vision techniques to detect signs of fatigue from facial actions, such as yawning, has demonstrated good potential. However, accurate and robust detection of yawning is difficult because of the complicated facial actions and expressions of drivers in the real driving environment. Several facial actions and expressions have the same mouth deformation as yawning. Thus, a novel approach to detecting yawning based on subtle facial action recognition is proposed in this study to alleviate the abovementioned problems. A 3D deep learning network with a low time sampling characteristic is proposed for subtle facial action recognition. This network uses 3D convolutional and bidirectional long short-term memory networks for spatiotemporal feature extraction and adopts SoftMax for classification. A keyframe selection algorithm is designed to select the most representative frame sequence from subtle facial actions. This algorithm rapidly eliminates redundant frames using image histograms with low computation cost and detects outliers by median absolute deviation. A series of experiments are also conducted on YawDD benchmark and self-collected datasets. Compared with several state-of-the-art methods, the proposed method has high yawning detection rates and can effectively distinguish yawning from similar facial actions.},
  archive      = {J_TMM},
  author       = {Hao Yang and Li Liu and Weidong Min and Xiaosong Yang and Xin Xiong},
  doi          = {10.1109/TMM.2020.2985536},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {572-583},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Driver yawning detection based on subtle facial action recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented adversarial training for cross-modal retrieval.
<em>TMM</em>, <em>23</em>, 559–571. (<a
href="https://doi.org/10.1109/TMM.2020.2985540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval has received considerable attention in recent years. The core of cross-modal retrieval is to find a representation space to align data from different modalities according to their semantics. In this paper, we propose a cross-modal retrieval method that aligns data from different modalities by transferring one source modality to another target modality with augmented adversarial training. To preserve the semantic meaning in the modality transfer process, we employ the idea of conditional GANs and augment it. The key idea is to incorporate semantic information from the label space into the adversarial training process by sampling more semantic relevant and irrelevant source-target sample pairs. The augmented sample pairs improve the alignment from two aspects. First, relevant source-target sample pairs provide more training samples, leading to a better guidance of the alignment of fake targets and true paired targets. Second, relevant and irrelevant source-target sample pairs teach the discriminator to better distinguish true relevant pairs from fake relevant pairs, which guides the generator to better transfer from the source modality to the target modality. Extensive experiments compared with state-of-the-art methods show the promising power of our approach.},
  archive      = {J_TMM},
  author       = {Yiling Wu and Shuhui Wang and Guoli Song and Qingming Huang},
  doi          = {10.1109/TMM.2020.2985540},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {559-571},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Augmented adversarial training for cross-modal retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mask cross-modal hashing networks. <em>TMM</em>,
<em>23</em>, 550–558. (<a
href="https://doi.org/10.1109/TMM.2020.2984081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of deep learning, cross-modal retrieval has achieved significant progress in recent years. Moreover, cross-modal hashing has recently attracted considerable attention to multi-modal retrieval applications due to its advantages of low storage costs and fast retrieval speed. However, it is still a challenging problem due to an existing semantic heterogeneity gap between different modalities. In order to further narrow the gap and obtain more effective hash codes, we put forward a novel mask deep cross-modal hashing (MDCH) approach to explore the similarity between inter-modal instances. The main contributions of this paper are that: (1) we attempt to introduce semantic mask information into cross-modal hashing retrieval, (2) we alternately train intra-modal and inter-modal networks to fully mine the semantic relationship between different modalities. The semantic mask can improve the semantic information of the image feature. While inter-modal similarity, explored by inter-modal networks, focuses on enforcing images and their corresponding text tags to have similar hash codes, intra-modal similarity, explored by intra-modal networks, can retain local structural information embedded in each modality to achieve internal similarity. A large number of experiments conducted on three datasets demonstrate that our proposed MDCH approach is superior to several state-of-the-art cross-modal hashing approaches.},
  archive      = {J_TMM},
  author       = {Qiubin Lin and Wenming Cao and Zhiquan He and Zhihai He},
  doi          = {10.1109/TMM.2020.2984081},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {550-558},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mask cross-modal hashing networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IPTV channel zapping recommendation with attention
mechanism. <em>TMM</em>, <em>23</em>, 538–549. (<a
href="https://doi.org/10.1109/TMM.2020.2984094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet Protocol TV (IPTV) normally has the advantage of providing far more TV channels than the traditional TV services, while as the other side of the coin it has the problem of information overload. Users of IPTV usually have difficulties finding channels matching their interests. In this paper, using a large IPTV dataset, we analyze channel zapping behaviors of IPTV users and discover various patterns that can be used to generate more accurate channel zapping recommendations. Based on user behavior analysis, we develop several base and fusion recommender systems that generate in real-time a short list of channels for users to consider whenever they want to switch channels. A deep neural network model that consists of a “Recommender System Attention (RS Attention)” module and a “Channel Attention” module capturing the static and dynamic user switching behaviors is also developed to further improve the recommendation accuracy. Evaluation on the IPTV dataset demonstrates that our fusion recommender can achieve 41% hit ratio with only three candidate channels, and our attention neural network model further pushes it up to 45%. Our recommender systems only take as input user channel zapping sequences, and can be easily adopted by IPTV systems with low data and computation overheads.},
  archive      = {J_TMM},
  author       = {Guangyu Li and Lina Qiu and Chenguang Yu and Houwei Cao and Yong Liu and Can Yang},
  doi          = {10.1109/TMM.2020.2984094},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {538-549},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IPTV channel zapping recommendation with attention mechanism},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel perspective to zero-shot learning: Towards an
alignment of manifold structures via semantic feature expansion.
<em>TMM</em>, <em>23</em>, 524–537. (<a
href="https://doi.org/10.1109/TMM.2020.2984091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning aims at recognizing unseen classes (no training example) with knowledge transferred from seen classes. This is typically achieved by exploiting a semantic feature space shared by both seen and unseen classes, i.e., attribute or word vector, as the bridge. One common practice in zero-shot learning is to train a projection between the visual and semantic feature spaces with labeled seen classes examples. When inferring, this learned projection is applied to unseen classes and recognizes the class labels by some metrics. However, the visual and semantic feature spaces are mutually independent and have quite different manifold structures. Under such a paradigm, most existing methods easily suffer from the domain shift problem and weaken the performance of zero-shot recognition. To address this issue, we propose a novel model called AMS-SFE. It considers the alignment of manifold structures by semantic feature expansion. Specifically, we build upon an autoencoder-based model to expand the semantic features from the visual inputs. Additionally, the expansion is jointly guided by an embedded manifold extracted from the visual feature space of the data. Our model is the first attempt to align both feature spaces by expanding semantic features and derives two benefits: first, we expand some auxiliary features that enhance the semantic feature space; second and more importantly, we implicitly align the manifold structures between the visual and semantic feature spaces; thus, the projection can be better trained and mitigate the domain shift problem. Extensive experiments show significant performance improvement, which verifies the effectiveness of our model.},
  archive      = {J_TMM},
  author       = {Jingcai Guo and Song Guo},
  doi          = {10.1109/TMM.2020.2984091},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {524-537},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A novel perspective to zero-shot learning: Towards an alignment of manifold structures via semantic feature expansion},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning face image super-resolution through facial semantic
attribute transformation and self-attentive structure enhancement.
<em>TMM</em>, <em>23</em>, 468–483. (<a
href="https://doi.org/10.1109/TMM.2020.2984092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face super-resolution is a domain-specific super-resolution (SR) problem of generating high-resolution (HR) face images from low-resolution (LR) inputs. Even though existing face SR methods have achieved great performance on the global region evaluation, most of them cannot restore local attributes and structure reasonably, especially to ultra-resolve tiny LR face images (16 × 16 pixels) to its larger version (8 × upscaling factor). In this paper, we propose an open source face SR framework based on facial semantic attribute transformation and self-attentive structure enhancement. Specifically, the proposed framework introduces face semantic information (i.e., face attributes) and face structure information (i.e., face boundaries) in a successive two-stage fashion. In the first stage, an Attribute Transformation Network (AT-Net) is established. It upsamples LR face images to HR feature maps and then combines facial attributes with these features to generate the intermediate HR results with rational attributes. In the second stage, a Structure Enhancement Network (SE-Net) is built. It simultaneously extracts face features and estimates facial boundary heatmaps from the inputs, and then fuses them to output the final HR face images. Extensive experiments demonstrate that our method achieves superior super-resolved results and outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Mengyan Li and Zhaoyu Zhang and Jun Yu and Chang Wen Chen},
  doi          = {10.1109/TMM.2020.2984092},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {468-483},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning face image super-resolution through facial semantic attribute transformation and self-attentive structure enhancement},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-temporal cascade autoencoder for video anomaly
detection in crowded scenes. <em>TMM</em>, <em>23</em>, 203–215. (<a
href="https://doi.org/10.1109/TMM.2020.2984093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-efficient anomaly detection and localization in video surveillance still remains challenging due to the complexity of “anomaly”. In this paper, we propose a cuboid-patch-based method characterized by a cascade of classifiers called a spatial-temporal cascade autoencoder (ST-CaAE), which makes full use of both spatial and temporal cues from video data. The ST-CaAE has two main stages, defined by two proposed neural networks: a spatial-temporal adversarial autoencoder (ST-AAE) and a spatial-temporal convolutional autoencoder (ST-CAE). First, the ST-AAE is used to preliminarily identify anomalous video cuboids and exclude normal cuboids. The key idea underlying ST-AAE is to obtain a Gaussian model to fit the distribution of the regular data. Then in the second stage, the ST-CAE classifies the specific abnormal patches in each anomalous cuboid with reconstruction error based strategy that takes advantage of the CAE and skip connection. A two-stream framework is utilized to fuse the appearance and motion cues to achieve more complete detection results, taking the gradient and optical flow cuboids as inputs for each stream. The proposed ST-CaAE is evaluated using three public datasets. The experimental results verify that our framework outperforms other state-of-the-art works.},
  archive      = {J_TMM},
  author       = {Nanjun Li and Faliang Chang and Chunsheng Liu},
  doi          = {10.1109/TMM.2020.2984093},
  journal      = {IEEE Transactions on Multimedia},
  month        = {4},
  pages        = {203-215},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal cascade autoencoder for video anomaly detection in crowded scenes},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial pyramid attention for deep convolutional neural
networks. <em>TMM</em>, <em>23</em>, 3048–3058. (<a
href="https://doi.org/10.1109/TMM.2021.3068576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms have shown great success in computer vision. However, the commonly used global average pooling in some implementations aggregates a three-dimensional feature map to a one-dimensional attention map, leading a significant loss of structural information in the attention learning. In this article, we present a novel Spatial Pyramid Attention Network (SPANet), which exploits the structural information and channel relationships for better feature representation. SPANet enhances a base network by adding Spatial Pyramid Attention (SPA) blocks laterally. By rethinking the self-attention mechanism design, we further present three topology structures of attention path connection for our SPANet. They can be flexibly applied to various CNN architectures. SPANet is conceptually simple but practically powerful. It uses both structural regularization and structural information to achieve better learning capability. We have comprehensively evaluated the performance of SPANet on four benchmark datasets for different visual tasks. The experimental results show that SPANet significantly improves the recognition accuracy without adding much computation overhead. Using SPANet, we achieve an improvement of 1.6% top-1 classification accuracy on the ImageNet 2012 benchmark based on ResNet50, and SPANet outperforms SENet and other attention methods. SPANet also significantly improves the object detection performance by a clear margin with negligible additional computation overhead. When applying SPANet to RetinaNet based on the ResNet50 backbone, we improve the performance of the baseline model by 2.3 mAP and the enhanced model outperforms SENet and GCNet by 1.1 mAP and 1.7 mAP respectively. The code of SPANet is made publicly available. 1 1[Online]. Available: https://github.com/13952522076/SPANet_TMM},
  archive      = {J_TMM},
  author       = {Xu Ma and Jingda Guo and Andrew Sansom and Mara McGuire and Andrew Kalaani and Qi Chen and Sihai Tang and Qing Yang and Song Fu},
  doi          = {10.1109/TMM.2021.3068576},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {3048-3058},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial pyramid attention for deep convolutional neural networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Intelligibility enhancement via normal-to-lombard speech
conversion with long short-term memory network and bayesian gaussian
mixture model. <em>TMM</em>, <em>23</em>, 3035–3047. (<a
href="https://doi.org/10.1109/TMM.2021.3068565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech communications and interactions frequently occur in a variety of environments. Noise in the environment significantly degrades speech intelligibility when speaking and listening. Especially in the listening stage, even if the multimedia terminal outputs clean speech, it is still difficult for listeners to obtain information. Intelligibility enhancement (IENH) of speech is a technique for overcoming the environmental noise in the listening stage. It implements a perceptual enhancement of non-noisy speech. This study focuses on IENH via normal-to-Lombard speech conversion, inspired by a well known acoustic mechanism named the Lombard effect. Our method combines the long short-term memory (LSTM) network and Bayesian Gaussian mixture model (BGMM) to build a conversion architecture. Compared with baselines, it has three main advantages: 1) an LSTM network is used for spectral tilt mapping with fully considering short-term correlations and high-dimensional expression abilities; 2) the aperiodicity (AP) is mapped together with the fundamental frequency ( $F_0$ ) by a BGMM, which considers their relevance constraints and the importance of APs; 3) the gender-dependent mapping is used for $F_0$ and APs to consider distribution differences between genders. Experiments indicate that our method gets better performance in both objective and subjective tests.},
  archive      = {J_TMM},
  author       = {Gang Li and Xiaochen Wang and Ruimin Hu and Huyin Zhang and Shanfa Ke},
  doi          = {10.1109/TMM.2021.3068565},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {3035-3047},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Intelligibility enhancement via normal-to-lombard speech conversion with long short-term memory network and bayesian gaussian mixture model},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic point cloud inpainting via spatial-temporal graph
learning. <em>TMM</em>, <em>23</em>, 3022–3034. (<a
href="https://doi.org/10.1109/TMM.2021.3068606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maturity of depth sensors and laser scanning techniques has enabled the convenient acquisition of 3D dynamic point clouds—one natural representation of 3D objects/scenes in motion, leading to a wide range of applications such as immersive tele-presence, autonomous driving, augmented and virtual reality. Nevertheless, dynamic point clouds usually exhibit holes of missing data, thus inpainting is crucial to the subsequent rendering or downstream understanding tasks. Dynamic point cloud inpainting has been largely overlooked so far, which is also quite challenging due to the irregular sampling patterns both in the spatial domain and temporal domain. To this end, we propose an efficient dynamic point cloud inpainting method based on a learnable spatial-temporal graph representation, exploiting both the second-order inter-frame coherence and the intra-frame self-similarity. The key is the second-order inter-frame coherence that enforces the consistent flow in 3D motion over time, for which we search the temporal correspondence in consecutive frames for the same underlying surface by the point-to-plane distance and represent the correlation between them via temporal edge weights in the graph. Based on the second-order inter-frame coherence and intra-frame self-similarity, we formulate dynamic point cloud inpainting as a joint optimization problem of the desired point cloud and underlying spatial-temporal graph, which is regularized by consistency in the temporal edge weights and smoothness in the spatial domain. We analyze and reformulate the optimization, leading to an efficient alternating minimization algorithm. Experimental results show that the proposed approach outperforms several competing methods significantly, both on synthetic holes and real holes.},
  archive      = {J_TMM},
  author       = {Zeqing Fu and Wei Hu},
  doi          = {10.1109/TMM.2021.3068606},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {3022-3034},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic point cloud inpainting via spatial-temporal graph learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learned multi-resolution variable-rate image compression
with octave-based residual blocks. <em>TMM</em>, <em>23</em>, 3013–3021.
(<a href="https://doi.org/10.1109/TMM.2021.3068523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently deep learning-based image compression has shown the potential to outperform traditional codecs. However, most existing methods train multiple networks for multiple bit rates, which increase the implementation complexity. In this paper, we propose a new variable-rate image compression framework, which employs generalized octave convolutions (GoConv) and generalized octave transposed-convolutions (GoTConv) with built-in generalized divisive normalization (GDN) and inverse GDN (IGDN) layers. Novel GoConv- and GoTConv-based residual blocks are also developed in the encoder and decoder networks. Our scheme also uses a stochastic rounding-based scalar quantization. To further improve the performance, we encode the residual between the input and the reconstructed image from the decoder network as an enhancement layer. To enable a single model to operate with different bit rates and to learn multi-rate image features, a new objective function is introduced. Experimental results show that the proposed framework trained with variable-rate objective function outperforms the standard codecs such as H.265/HEVC-based BPG and state-of-the-art learning-based variable-rate methods.},
  archive      = {J_TMM},
  author       = {Mohammad Akbari and Jie Liang and Jingning Han and Chengjie Tu},
  doi          = {10.1109/TMM.2021.3068523},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {3013-3021},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learned multi-resolution variable-rate image compression with octave-based residual blocks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expression-aware face reconstruction via a dual-stream
network. <em>TMM</em>, <em>23</em>, 2998–3012. (<a
href="https://doi.org/10.1109/TMM.2021.3068567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D face reconstruction from a single image has achieved promising progress by adopting the 3D Morphable Model (3DMM). However, face images taken in-the-wild usually involve expressions with a large range of variety. This poses difficulty to use 3DMM to represent such various facial expressions owing to the limited expressive ability of its linear model, thereby resulting in distortion and ambiguity in local facial regions. To tackle this problem, we present a novel dual-stream network composed of a geometry stream and a texture stream to deal with expression variations. Specifically, in the geometry stream, we propose novel Attribute Spatial Maps (ASMs) to decompose a face into the identity and expression attributes and then separately record the essential spatial information of the two facial attributes in the 2D image space. This avoids the interaction between the two attributes, thus preserving the identity information and further improving the ability of coping with expression variations. In the texture stream, we propose to generate facial appearance with realistic texture and canonical layout by our Semantic Region Stylization Mechanism (SRSM), that transfers the style from an input face to a 3DMM albedo map in a region-adaptive manner. Moreover, we also propose a Shared Semantic Region Prediction Module (SSRPM) to explore the common correspondence of semantic regions between the above two face texture representations. Both quantitative and qualitative evaluations on public datasets demonstrate the effectiveness of our approach in face reconstruction under expression variations.},
  archive      = {J_TMM},
  author       = {Xiaoyu Chai and Jun Chen and Chao Liang and Dongshu Xu and Chia-Wen Lin},
  doi          = {10.1109/TMM.2021.3068567},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {2998-3012},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Expression-aware face reconstruction via a dual-stream network},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion blur removal with quality assessment guidance.
<em>TMM</em>, <em>23</em>, 2986–2997. (<a
href="https://doi.org/10.1109/TMM.2021.3068561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-uniform blind motion deblurring is a challenging yet fundamental task in the computer vision field, which aims to restore the latent sharp image from the blurry input. Recently, deep-learning-based methods have made significant improvement and progress, on the metric of PSNR. They achieve good results mainly because they adopt Mean Squared Error (MSE) as the optimization objective, in addition to their good model design. However, simple adoption of the PSNR metric and the MSE loss, has non-ignorable disadvantages. PSNR cannot always succeed in assessing the deblurred quality in accordance with the human visual system (HVS), and MSE guides the network to generate over-smoothed images. To address these problems, we are the first to propose the deep-learning-based multi-scale non-reference quality assessment network (Deep DEBLUR-IQA) for assessing the quality of deblurred results. Moreover, a deblurring network of high efficiency is presented. It is more than 50 times faster than other SOTA multi-scale Convolution Neural Network (CNN) methods, with the newly propose Residual Dilated Block (RDB) and Light ResBlock (LRB). The deblurring network&#39;s performance can be further boosted with Multiple Dilation Block (MDB), with an acceptable speed decrease. Finally, and most importantly, we are the first to let Deep DEBLUR-IQA guide the deblurring network&#39;s optimization. This IQA-guided enhancement paradigm can significantly improve the deblurring results’ subjective quality while achieving excellent PSNR. Experimental results demonstrate that the proposed method performs favorably against state-of-the-art methods quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Jichun Li and Bo Yan and Qing Lin and Ang Li and Chenxi Ma},
  doi          = {10.1109/TMM.2021.3068561},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {2986-2997},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Motion blur removal with quality assessment guidance},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards coding for human and machine vision: Scalable face
image coding. <em>TMM</em>, <em>23</em>, 2957–2971. (<a
href="https://doi.org/10.1109/TMM.2021.3068580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decades have witnessed the rapid development of image and video coding techniques in the era of big data. However, the signal fidelity-driven coding pipeline design limits the capability of the existing image/video coding frameworks to fulfill the needs of both machine and human vision. In this paper, we come up with a novel face image coding framework by leveraging both the compressive and the generative models, to support machine vision and human perception tasks jointly. Given an input image, the feature analysis is first applied, and then the generative model is employed to reconstruct image with compact structure and color features, where sparse edges are extracted to connect both kinds of vision and a key reference pixel selection method is proposed to determine the priorities of the reference color pixels for scalable coding. The compact edge map serves as the basic layer for machine vision tasks, and the reference pixels act as an enhanced layer to guarantee signal fidelity for human vision. By introducing advanced generative models, we train a decoding network to reconstruct images from compact structure and color representations, which is flexible to accept inputs in a scalable way and to control the imagery effect of the outputs between signal fidelity and visual realism. Experimental results and comprehensive performance analysis over the face image dataset demonstrate the superiority of our framework in both human vision tasks and machine vision tasks, which provide useful evidence on the emerging standardization efforts on MPEG VCM (Video Coding for Machine).},
  archive      = {J_TMM},
  author       = {Shuai Yang and Yueyu Hu and Wenhan Yang and Ling-Yu Duan and Jiaying Liu},
  doi          = {10.1109/TMM.2021.3068580},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {2957-2971},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards coding for human and machine vision: Scalable face image coding},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge-cloud collaboration enabled video service enhancement:
A hybrid human-artificial intelligence scheme. <em>TMM</em>,
<em>23</em>, 2208–2221. (<a
href="https://doi.org/10.1109/TMM.2021.3066050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a video service enhancement strategy is investigated under an edge-cloud collaboration framework, where video caching and delivery decisions are made at the cloud and edge respectively. We aim to guarantee the user fairness in terms of video coding rate under statistical delay constraint and edge caching capacity constraint. A hybrid human-artificial intelligence approach is developed to improve the user hit rate for video caching. Specifically, individual user interest is first characterized by merging factorization machine (FM) model and multi-layer perceptron (MLP) model, where both low-order and high-order features can be well learned simultaneously. Thereafter, a social aware similarity model is constructed to transfer individual user interest to group interest, based on which, videos can be selected to cache at the network edge. Furthermore, a dual bisection exploration scheme is proposed to optimize wireless resource allocation and video coding rate. The effectiveness of the proposed video caching and delivery scheme is finally validated by extensive experiments with a real-world dataset.},
  archive      = {J_TMM},
  author       = {Dapeng Wu and Ruili Bao and Zhidu Li and Honggang Wang and Hong Zhang and Ruyan Wang},
  doi          = {10.1109/TMM.2021.3066050},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {2208-2221},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Edge-cloud collaboration enabled video service enhancement: A hybrid human-artificial intelligence scheme},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised adversarial instance-level image retrieval.
<em>TMM</em>, <em>23</em>, 2199–2207. (<a
href="https://doi.org/10.1109/TMM.2021.3065578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide use of visual sensors in the Internet of Things (IoT) in the past decades, huge amounts of images are captured in people&#39;s daily lives, which poses challenges to traditional deep-learning-based image retrieval frameworks. Most such frameworks need a large amount of annotated training data, which are expensive. Moreover, machines still lack human intelligence, as illustrated by the fact that they pay less attention to the interesting regions that humans generally focus on when searching for images. Hence, this paper proposes a novel unsupervised framework that focuses on the instance object in the image and integrates human intelligence into the deep-learning-based image retrieval. This framework is called adversarial instance-level image retrieval (AILIR). We incorporate adversarial training and an attention mechanism into this framework that considers human intelligence with artificial intelligence. The generator and discriminator are redesigned to guarantee that the generator retrieves similar images while the discriminator selects unmatched images and creates an adversarial reward for the generator. A minimax game is conducted by the adversarial reward retrieval mechanism until the discriminator is unable to judge whether the image sequence retrieved matches the query. Comparison and ablation experiments on four benchmark datasets prove that the proposed adversarial training framework indeed improves instance retrieval and outperforms the state-of-the-art methods focused on instance retrieval.},
  archive      = {J_TMM},
  author       = {Cong Bai and Hongkai Li and Jinglin Zhang and Ling Huang and Lu Zhang},
  doi          = {10.1109/TMM.2021.3065578},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {2199-2207},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised adversarial instance-level image retrieval},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human memory update strategy: A multi-layer template update
mechanism for remote visual monitoring. <em>TMM</em>, <em>23</em>,
2188–2198. (<a href="https://doi.org/10.1109/TMM.2021.3065580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of rapid development of artificial intelligence, the integration of multimedia and human-artificial intelligence has become an important research hotspot. Especially in the multimedia environment, effective remote visual monitoring has become the exploration direction of many scholars. The use of traditional correlation filtering (CF) algorithm for real-time monitoring in the context of multimedia is a practical strategy. However, most existing filtering-based visual monitoring algorithms still have the problem of insufficient robustness and effectiveness. Therefore, by considering the strategy of updating human memory, this paper proposes a multi-layer template update mechanism to achieve effective monitoring in a multimedia environment. In this strategy, the weighted template of the high-confidence matching memory is used as the confidence memory, and the unweighted template of the low-confidence matching memory is used as the cognitive memory. Through the alternate use of confidence memory, matching memory, and cognitive memory, it is ensured that the target will not be lost during the monitoring process. Experimental results show that this strategy does not affect the speed (still real-time) and improves the robustness in the multimedia background.},
  archive      = {J_TMM},
  author       = {Shuai Liu and Shuai Wang and Xinyu Liu and Amir H. Gandomi and Mahmoud Daneshmand and Khan Muhammad and Victor Hugo C. De Albuquerque},
  doi          = {10.1109/TMM.2021.3065580},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {2188-2198},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Human memory update strategy: A multi-layer template update mechanism for remote visual monitoring},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Metadata connector: Exploiting hashtag and tag for cross-OSN
event search. <em>TMM</em>, <em>23</em>, 510–523. (<a
href="https://doi.org/10.1109/TMM.2020.2982047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media has revolutionized the way people understand and keep track of real-world events. Various related multimedia information in different modalities such as texts, images and videos is updated on social media and reflects the events. These quantities of information distributes on different Online Social Networks (OSNs), which provides rich, wide coverage, comprehensive information about the trending events. Faced with such large amounts of data, searching has become a handy tool for event understanding and tracking on social media. However, existing single-OSN search mainly involves with single modality on single platform. Moreover, most OSNs usually focus on biased perspective of events, which significantly limits the coverage and diversity of single-OSN based event search. In this paper, we introduce a novel cross-OSN framework to help integrate these cross-OSN information regarding the same event and provide an immersive experience for information retrieval. Since social media information is widely distributed in different OSNs where semantic gap exists among these heterogeneous spaces, we propose to utilize hashtag and tag, which are user-generated metadata for organizing and labeling in many OSNs, as bridges to connect between different OSNs. In our four-stage solution framework, various methods are adopted for hashtag and tag filtering, search results representation, clustering and demonstration. Given an event query, in the first stage we generate related items with corresponding tags and hashtags from OSNs and filter the hashtags and tags we need. Then, topical representation is generated for hashtag and tag. The third stage leverages the derived representation for cross-OSN hashtag and tag clustering. Finally, demonstration for each query is produced and the results are organized hierarchically. Experiments on a dataset containing hundreds of search queries and related items demonstrate the effectiveness of our cross-OSN event search framework.},
  archive      = {J_TMM},
  author       = {Yuqi Gao and Jitao Sang and Chengpeng Fu and Zhengjia Wang and Tongwei Ren and Changsheng Xu},
  doi          = {10.1109/TMM.2020.2982047},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {510-523},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Metadata connector: Exploiting hashtag and tag for cross-OSN event search},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepDance: Music-to-dance motion choreography with
adversarial learning. <em>TMM</em>, <em>23</em>, 497–509. (<a
href="https://doi.org/10.1109/TMM.2020.2981989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of improvised dancing choreographies is an important research field of cross-modal analysis. A key point of this task is how to effectively create and correlate music and dance with a probabilistic one-to-many mapping, which is essential to create realistic dances of various genres. To address this issue, we propose a GAN-based cross-modal association framework, DeepDance, which correlates two different modalities (dance motion and music) together, aiming at creating the desired dance sequence in terms of the input music. Its generator is to predictively produce the dance movements best-fit to current music piece by learning from examples. In another hand, its discriminator acts as an external evaluation from the audience and judges the whole performance. The generated dance movements and the corresponding input music are considered to be well-matched if the discriminator cannot distinguish the generated movements from the training samples according to the estimated probability. By adding motion consistency constraints in our loss function, the proposed framework is able to create long realistic dance sequences. To alleviate the problem of expensive and inefficient data collection, we propose an effective approach to create a large-scale dataset, YouTube-Dance3D, from open data source. Extensive experiments on currently available music-dance datasets and our YouTube-Dance3D dataset demonstrate that our approach effectively captures the correlation between music and dance and can be used to choreograph appropriate dance sequences.},
  archive      = {J_TMM},
  author       = {Guofei Sun and Yongkang Wong and Zhiyong Cheng and Mohan S. Kankanhalli and Weidong Geng and Xiangdong Li},
  doi          = {10.1109/TMM.2020.2981989},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {497-509},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DeepDance: Music-to-dance motion choreography with adversarial learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning and fusing multiple user interest representations
for micro-video and movie recommendations. <em>TMM</em>, <em>23</em>,
484–496. (<a href="https://doi.org/10.1109/TMM.2020.2978618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is known to be effective at automating the generation of representations, which eliminates the need for handcrafted features. For the task of personalized recommendation, deep learning-based methods have achieved great success by learning efficient representations of multimedia items, especially images and videos. Previous works usually adopt simple, single-modality representations of user interest, such as user embeddings, which cannot fully characterize the diversity and volatility of user interest. To address this problem, in this paper we focus on learning and fusing multiple kinds of user interest representations by leveraging deep networks. Specifically, we consider efficient representations of four aspects of user interest: first, we use latent representation, i.e. user embedding, to profile the overall interest; second, we propose item-level representation, which is learned from and integrates the features of a user&#39;s historical items; third, we investigate neighbor-assisted representation, i.e. using neighboring users&#39; information to characterize user interest collaboratively; fourth, we propose category-level representation, which is learned from the categorical attributes of a user&#39;s historical items. In order to integrate these multiple user interest representations, we study both early fusion and late fusion; where for early fusion, we study different fusion functions. We validate the proposed method on two real-world video recommendation datasets for micro-video and movie recommendations, respectively. Experimental results demonstrate that our method outperforms existing state-of-the-arts by a significant margin. Our code is publicly available.},
  archive      = {J_TMM},
  author       = {Xusong Chen and Dong Liu and Zhiwei Xiong and Zheng-Jun Zha},
  doi          = {10.1109/TMM.2020.2978618},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {484-496},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning and fusing multiple user interest representations for micro-video and movie recommendations},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OPMP: An omnidirectional pyramid mask proposal network for
arbitrary-shape scene text detection. <em>TMM</em>, <em>23</em>,
454–467. (<a href="https://doi.org/10.1109/TMM.2020.2978630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text detection methods have achieved significant progresses. However, stack-omnidirectional text dilemma, under-segmentation of very close text words, and over-segmentation of arbitrary-shape long text lines, are still main challenges. Motivated by these problems, we proposed a two stage method called omnidirectional pyramid mask proposal text detector (OPMP). OPMP removes anchor mechanism that requires heuristic non-maximum suppress processing. Instead, it uses an effective pyramid lengthwise and sidewise residual sequence modeling method to produce arbitrary-shape proposals. To accurately extract the features of text shape, OPMP enhances the backbone layers by a multiple arbitrary-shape fitting mechanism. Finally, a multi-grain text classification module is proposed, which reclassifies each text region robustly. Comprehensive ablation studies demonstrate the effectiveness of each proposed component. In addition, experiments on various benchmarks, including ICDAR2015, MLT, MSRA-TD500, CTW1500, and Total-text, show that our method outperforms previous state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Sheng Zhang and Yuliang Liu and Lianwen Jin and Zhongrong Wei and Chunhua Shen},
  doi          = {10.1109/TMM.2020.2978630},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {454-467},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OPMP: An omnidirectional pyramid mask proposal network for arbitrary-shape scene text detection},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Density-aware multi-task learning for crowd counting.
<em>TMM</em>, <em>23</em>, 443–453. (<a
href="https://doi.org/10.1109/TMM.2020.2980945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method called density-aware convolutional neural network (DensityCNN) to perform the crowd counting task in various crowded scenes. The key idea of the DensityCNN is to utilize high-level semantic information to provide guidance and constraint when generating density maps. To this end, we implement the DensityCNN by adopting a multi-task CNN structure to jointly learn density-level classification and density map estimation. The density-level classification task learns multi-channel semantic features that are aware of the density distributions of the input image. This task is accomplished via our specially designed group-based convolutional structure in a supervised learning manner. In the density map estimation task, these semantic features are deployed together with high-dimension convolutional features to generate density maps with lower count errors. Extensive experiments on four challenging crowd datasets (ShanghaiTech, UCF_CC_50, UCF-QNCF, and WorldExpo&#39;10) and one vehicle dataset TRANCOS demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Xiaoheng Jiang and Li Zhang and Tianzhu Zhang and Pei Lv and Bing Zhou and Yanwei Pang and Mingliang Xu and Changsheng Xu},
  doi          = {10.1109/TMM.2020.2980945},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {443-453},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Density-aware multi-task learning for crowd counting},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent generative adversarial network for face
completion. <em>TMM</em>, <em>23</em>, 429–442. (<a
href="https://doi.org/10.1109/TMM.2020.2978633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recently-proposed face completion algorithms use high-level features extracted from convolutional neural networks (CNNs) to recover semantic texture content. Although the completed face is natural-looking, the synthesized content still lacks lots of high-frequency details, since the high-level features cannot supply sufficient spatial information for details recovery. To tackle this limitation, in this paper, we propose a R ecurrent G enerative A dversarial N etwork (RGAN) for face completion. Unlike previous algorithms, RGAN can take full advantage of multi-level features, and further provide advanced representations from multiple perspectives, which can well restore spatial information and details in face completion. Specifically, our RGAN model is composed of a CompletionNet and a DisctiminationNet, where the CompletionNet consists of two deep CNNs and a recurrent neural network (RNN). The first deep CNN is presented to learn the internal regulations of a masked image and represent it with multi-level features. The RNN model then exploits the relationships among the multi-level features and transfers these features in another domain, which can be used to complete the face image. Benefiting from bidirectional short links, another CNN is used to fuse multi-level features transferred from RNN and reconstruct the face image in different scales. Meanwhile, two context discrimination networks in the DisctiminationNet are adopted to ensure the completed image consistency globally and locally. Experimental results on benchmark datasets demonstrate qualitatively and quantitatively that our model performs better than the state-of-the-art face completion models, and simultaneously generates realistic image content and high-frequency details. The code will be released available soon.},
  archive      = {J_TMM},
  author       = {Qiang Wang and Huijie Fan and Gan Sun and Weihong Ren and Yandong Tang},
  doi          = {10.1109/TMM.2020.2978633},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {429-442},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Recurrent generative adversarial network for face completion},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3-d human behavior understanding using generalized TS-LSTM
networks. <em>TMM</em>, <em>23</em>, 415–428. (<a
href="https://doi.org/10.1109/TMM.2020.2978637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problems of skeleton feature representation and the modeling of temporal dynamics to recognize human actions consisting of poses. In contrast to traditional methods which generally used relative coordinate systems dependent on some joints, or modeled only the long-term dependency, we attempt to understand 3D human behavior with observation by taking temporally different windows. Instead of taking raw skeletons as the input, we transform the skeletons into another coordinate system to obtain the robustness to scale, rotation and translation, and extract motion features between adjacent skeletons, which finally constructs an efficient hybrid-stream combining both pose and motion streams. We propose novel generalized Temporal Sliding Long Short-term Memory (TS-LSTM) networks. The proposed networks are composed of multiple TS-LSTM networks with various hyper-parameters, which can capture various temporal dynamics of actions. We also propose a novel hyper-parameter searching method, which finds decent hyper-parameters of generalized TS-LSTM to handle temporal dynamics of actions. In the experiment, we evaluate the proposed networks to verify the effectiveness of the proposed methods, and compare them with the other methods on three challenging datasets. Additionally, we analyze a relation between the recognized actions and the hyper-parameters, and visualize the layers of the proposed models.},
  archive      = {J_TMM},
  author       = {Inwoong Lee and Doyoung Kim and Sanghoon Lee},
  doi          = {10.1109/TMM.2020.2978637},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {415-428},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3-D human behavior understanding using generalized TS-LSTM networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LAST: Location-appearance-semantic-temporal clustering based
POI summarization. <em>TMM</em>, <em>23</em>, 378–390. (<a
href="https://doi.org/10.1109/TMM.2020.2977478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning a trip, users tend to browse Place-of-Interest (POI) information on the Internet and then depart. Many works aimed at summarizing POIs by visual and textual analysis, while many of them ignored the inter-relationship between different views offered by the community-contributed information. In this paper, we propose a City-POI-LOI (CPL) summarization method to automatically mine POIs from the city-level landmark images. And a Location-Appearance-Semantic-Temporal (LAST) clustering method is proposed to mine the popular viewpoints termed Location-Of-Interest (LOI) in each POI by taking location, appearance, semantic, and temporal feature into consideration. We perform text and image summarization for each LOI, and we further summarize the POIs based on season. We conduct a series of experiments based on DIV400 and ATCF Dataset. Experimental results show the effectiveness of the proposed POI summarization approach.},
  archive      = {J_TMM},
  author       = {Xueming Qian and Yuxia Wu and Mingdi Li and Yayun Ren and Shuhui Jiang and Zhetao Li},
  doi          = {10.1109/TMM.2020.2977478},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {378-390},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LAST: Location-appearance-semantic-temporal clustering based POI summarization},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dress with style: Learning style from joint deep embedding
of clothing styles and body shapes. <em>TMM</em>, <em>23</em>, 365–377.
(<a href="https://doi.org/10.1109/TMM.2020.2980195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body shape is about proportion, and fashion style is all about dressing those proportions to look their very best. Figuring out the styles to suit a body shape can be a daunting task for many people. It is, therefore, essential to develop a framework for learning the compatibility of body shapes and clothing styles. Though fashion designers and fashion stylists have analyzed the correlation between human body shapes and fashion styles for a long time, this issue did not receive much attention in multimedia science. In this paper, we present a novel style recommender, on the basis of the user&#39;s body attributes. The rich amount of fashion styling knowledge from social big data is exploited for this purpose. We first construct a joint embedding of clothing styles and human body measurements with deep multimodal representation learning on a reference dataset that has been sorted to meet the fashion rules. We then discover the relevant semantic features by propagation and selection in clothing style and body shape graphs. Experiments demonstrate the effectiveness of the proposed framework when compared with several baseline methods.},
  archive      = {J_TMM},
  author       = {Shintami Chusnul Hidayati and Ting Wei Goh and Ji-Sheng Gary Chan and Cheng-Chun Hsu and John See and Lai-Kuan Wong and Kai-Lung Hua and Yu Tsao and Wen-Huang Cheng},
  doi          = {10.1109/TMM.2020.2980195},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {365-377},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dress with style: Learning style from joint deep embedding of clothing styles and body shapes},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting temporal binary coding for large-scale video
search. <em>TMM</em>, <em>23</em>, 353–364. (<a
href="https://doi.org/10.1109/TMM.2020.2978593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an explosive increase in the amount of existing visual data. Hashing techniques have been successfully applied to deal with the large-scale nearest neighbor search problem among data on this massive scale. However, existing hashing methods usually learn a single hash code for each data point, and only by taking the content correlations among them into account. In practice, however, when handling complex visual data such as video, strong temporal relations exist among the successive frames. Moreover, if the preferred performance for large-scale video search is to be delivered, multiple hash codes are required for each data point in order to build multiple hash table indices. To address these problems, in this paper, we first study the multi-table learning problem for video search and attempt to learn binary codes by capturing the intrinsic video similarities from both the visual and the temporal aspects. By regarding the search over multiple tables as an ensemble prediction, the whole multi-table learning problem can be solved in a boosting learning manner to complementarily cover the nearest neighbors. For each table, a temporal binary coding solution is devised that thinks over the intrinsic relations among the visual content and the temporal consistency among the successive frames simultaneously. More specifically, we approximate the intrinsic visual similarities using a low-rank matrix based on sparse, non-negative feature expression. Furthermore, to essentially preserve the temporal consistency, we introduce a subspace rotation to model the variation among the successive frames. Under the boosting learning framework, the binary codes, hash functions and temporal variation of each table can be efficiently and jointly optimized. Extensive experiments on three large video datasets demonstrate that the proposed approach significantly outperforms a number of state-of-the-art hashing methods.},
  archive      = {J_TMM},
  author       = {Yan Wu and Xianglong Liu and Haotong Qin and Ke Xia and Sheng Hu and Yuqing Ma and Meng Wang},
  doi          = {10.1109/TMM.2020.2978593},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {353-364},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting temporal binary coding for large-scale video search},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast nearest subspace search via random angular hashing.
<em>TMM</em>, <em>23</em>, 342–352. (<a
href="https://doi.org/10.1109/TMM.2020.2977459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspaces frequently offer powerful representation in many tasks including recognition, retrieval, and optimization. In these tasks, the nearest subspaces (i.e., subspace-to-subspace search) often inevitably arise. Several studies in the literature have attempted to address this hard problem using techniques such as locality-sensitive hashing. Unfortunately, these subspace hashing methods are severely affected by poor scaling, with consequently high computational cost or unsatisfying accuracy, when the subspaces originally distribute with arbitrary dimensions. Accordingly, in this paper, we propose random angular hashing, a new and efficient type of locality-sensitive hashing, for linear subspaces of arbitrary dimension. The method we proposed preserves the angular distances among subspaces by randomly projecting their orthonormal basis and then encoding them with binary codes, meanwhile not only achieving fast computation but also maintaining a powerful collision probability. Moreover, its flexibility to easily get a balance between efficiency and accuracy in terms of performance. The extensive experimental results on tasks of face recognition, video de-duplication, and gesture recognition demonstrate that the proposed approach performs better than the state-of-the-art methods heavily, in terms of both accuracy and efficiency (up to 16× speedup).},
  archive      = {J_TMM},
  author       = {Yi Xu and Xianglong Liu and Binshuai Wang and Renshuai Tao and Ke Xia and Xianbin Cao},
  doi          = {10.1109/TMM.2020.2977459},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {342-352},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast nearest subspace search via random angular hashing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A physiology-based QoE comparison of interactive augmented
reality, virtual reality and tablet-based applications. <em>TMM</em>,
<em>23</em>, 333–341. (<a
href="https://doi.org/10.1109/TMM.2020.2982046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of affordable head-mounted display technology has facilitated new, potentially more immersive, interactive multimedia experiences. These technologies were traditionally focused on entertainment; however, academia and industry are now exploring applications in other domains such as health, learning and training. Key to the success of these new multimedia experiences is the understanding of a user&#39;s perceived quality of experience (QoE). Subjective user ratings have been the primary mechanism to capture insights into a user&#39;s experience. Such ratings have generally been captured post experience and reflected using a mean opinion score (MOS). However, user perception is multifactorial and subjective ratings alone do not express the true measure of an experience. As a result, recent efforts to capture QoE have included exploring the use of implicit metrics (e.g., physiological measures). This article presents the results of an experimental QoE evaluation and comparison of immersive applications delivered across three multimedia platforms. The platforms compared were augmented reality, tablet and virtual reality. The QoE methodology employed considered explicit (post-test questionnaire) and implicit (heart rate and electrodermal activity) assessment methods. The results indicate comparatively higher levels of QoE for users of the augmented reality and tablet platforms.},
  archive      = {J_TMM},
  author       = {Conor Keighrey and Ronan Flynn and Siobhan Murray and Niall Murray},
  doi          = {10.1109/TMM.2020.2982046},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {333-341},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A physiology-based QoE comparison of interactive augmented reality, virtual reality and tablet-based applications},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quality index for view synthesis by measuring instance
degradation and global appearance. <em>TMM</em>, <em>23</em>, 320–332.
(<a href="https://doi.org/10.1109/TMM.2020.2980185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual view synthesis plays a vital role in the application of multi-view and free-viewpoint videos. Depth-image-based rendering (DIBR) is the most commonly used approach in view synthesis, and many DIBR algorithms have been proposed. However, how to evaluate the quality of DIBR-synthesized images and benchmark the DIBR algorithms are still very challenging, which may hinder the further development of the view synthesis technique. Hence, an effective quality metric for evaluating the distortions in view synthesis is urgently needed. With this motivation, this paper presents a quality index for view synthesis by simultaneously measuring local Instance DEgradation and global Appearance (IDEA). Due to the imperfection of rendering algorithms, local geometric distortions are easily introduced around instance contours, causing instance degradation, which is the dominant distortion in synthesized views. In this work, image instances are first detected and local instance degradation is measured based on discrete orthogonal moments. Meantime, we propose to measure the global appearance of synthesized images based on the superpixel representation. By integrating both local and global aspects of the distortions, a more accurate quality model is built for view synthesis. Extensive experiments and comparisons have demonstrated the superiority of the proposed method in evaluating the quality of DIBR-synthesized images and benchmarking the performance of view synthesis algorithms.},
  archive      = {J_TMM},
  author       = {Leida Li and Yu Zhou and Jinjian Wu and Fu Li and Guangming Shi},
  doi          = {10.1109/TMM.2020.2980185},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {320-332},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Quality index for view synthesis by measuring instance degradation and global appearance},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectrum characteristics preserved visible and near-infrared
image fusion algorithm. <em>TMM</em>, <em>23</em>, 306–319. (<a
href="https://doi.org/10.1109/TMM.2020.2978640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visible and near-infrared images fusion aims at utilizing their spectrum characteristics to enhance visibility. However, the current visible and near-infrared fusion algorithms cannot well preserve spectrum characteristics, which results in color distortion and halo artifacts. Therefore, this paper proposes a new visible and near infrared images fusion algorithm by fully considering their different reflection and scattering characteristics. According to image degradation model, the reflection weight model and the transmission weight model are established, respectively. The reflection weight model is established by calculating the difference between the visible (red, green, and blue) spectra and the near-infrared spectrum, while maintaining the correlation of the visible spectra. The proposed reflection weight model can preserve the original reflection characteristic of objects in natural scenes. On the other hand, the transmission weight model is explicitly proposed by calculating the gradient ratio of the visible spectra to the near-infrared spectrum. The proposed transmission weight model intends to make full use of the strong transmission performance of the near-infrared spectrum, which can complement the details loss of the visible spectra caused by light scattering. Moreover, the fused image based on two models is further enhanced according to the reflection characteristics of near-infrared spectrum in case of the non-uniform illumination. The experimental results demonstrate that the proposed algorithm can not only well preserve spectrum characteristics, but also avoid color distortion while maintaining the naturalness, which outperforms the state-of-the-art.},
  archive      = {J_TMM},
  author       = {Zhuo Li and Hai-Miao Hu and Wei Zhang and Shiliang Pu and Bo Li},
  doi          = {10.1109/TMM.2020.2978640},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {306-319},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spectrum characteristics preserved visible and near-infrared image fusion algorithm},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person re-identification in aerial imagery. <em>TMM</em>,
<em>23</em>, 281–291. (<a
href="https://doi.org/10.1109/TMM.2020.2977528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles (UAVs), visual surveillance by utilizing the UAV platform has been very attractive. Most of the research works for UAV captured visual data are mainly focused on the tasks of object detection and tracking. However, limited attention has been paid to the task of person Re-identification (ReID) which has been widely studied in ordinary surveillance cameras with fixed emplacements. In this paper, to facilitate the research of person ReID in aerial imagery, we collect a large scale airborne person ReID dataset named as Person ReID in Aerial Imagery (PRAI-1581), which consists of 39,461 images of 1581 person identities. The images of the dataset are shot by two DJI consumer UAVs flying at an altitude ranging from 20 to 60 meters above the ground, which covers most of the real UAV surveillance scenarios. In addition, we propose to utilize subspace pooling of convolution feature maps to represent the input person images. Our method can learn a discriminative and compact feature representation for ReID in aerial imagery and can be trained in an end-to-end fashion efficiently. We conduct extensive experiments on the proposed dataset and the experimental results demonstrate that re-identifying persons in aerial imagery is a challenging problem, where our method performs favorably against state of the arts.},
  archive      = {J_TMM},
  author       = {Shizhou Zhang and Qi Zhang and Yifei Yang and Xing Wei and Peng Wang and Bingliang Jiao and Yanning Zhang},
  doi          = {10.1109/TMM.2020.2977528},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {281-291},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Person re-identification in aerial imagery},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewpoint recommendation based on object-oriented 3D scene
reconstruction. <em>TMM</em>, <em>23</em>, 257–267. (<a
href="https://doi.org/10.1109/TMM.2020.2981237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viewpoint recommendation can recommend several viewpoints for taking aesthetic photographs of a place-of-interest (POI) and is of great importance for photography assistance. In this paper, we propose a system that can assist a user in choosing good viewpoints for taking high-quality photographs. Our system is based on social media and 3D reconstruction. To reduce the time cost and improve the quality of 3D reconstruction, we propose a weakly supervised object detection method that is used before 3D reconstruction. The camera pose of images is recovered by the subsequent 3D reconstruction pipeline. We use a convolutional neural network (CNN) to extract 2D image features, and we fuse them with 3D camera pose features to learn their relationships to image aesthetics. The trained model is utilized to evaluate the aesthetics of images. Finally, the 3D space of all possible camera poses is divided into 3D grids, and the aesthetics score of each grid is evaluated. We combine the aesthetics and diversity of all viewpoints and recommend several high-quality viewpoints. Experimental results indicate that our approach can help users choose viewpoints that will result in high-quality photographs while maintaining diversity.},
  archive      = {J_TMM},
  author       = {Ke Li and Yuxia Wu and Yao Xue and Xueming Qian},
  doi          = {10.1109/TMM.2020.2981237},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {257-267},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Viewpoint recommendation based on object-oriented 3D scene reconstruction},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive video retrieval in the age of deep learning –
detailed evaluation of VBS 2019. <em>TMM</em>, <em>23</em>, 243–256. (<a
href="https://doi.org/10.1109/TMM.2020.2980944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the fact that automatic content analysis has made remarkable progress over the last decade - mainly due to significant advances in machine learning - interactive video retrieval is still a very challenging problem, with an increasing relevance in practical applications. The Video Browser Showdown (VBS) is an annual evaluation competition that pushes the limits of interactive video retrieval with state-of-the-art tools, tasks, data, and evaluation metrics. In this paper, we analyse the results and outcome of the 8th iteration of the VBS in detail. We first give an overview of the novel and considerably larger V3C1 dataset and the tasks that were performed during VBS 2019. We then go on to describe the search systems of the six international teams in terms of features and performance. And finally, we perform an in-depth analysis of the per-team success ratio and relate this to the search strategies that were applied, the most popular features, and problems that were experienced. A large part of this analysis was conducted based on logs that were collected during the competition itself. This analysis gives further insights into the typical search behavior and differences between expert and novice users. Our evaluation shows that textual search and content browsing are the most important aspects in terms of logged user interactions. Furthermore, we observe a trend towards deep learning based features, especially in the form of labels generated by artificial neural networks. But nevertheless, for some tasks, very specific content-based search features are still being used. We expect these findings to contribute to future improvements of interactive video search systems.},
  archive      = {J_TMM},
  author       = {Luca Rossetto and Ralph Gasser and Jakub Lokoč and Werner Bailer and Klaus Schoeffmann and Bernd Muenzer and Tomáš Souček and Phuong Anh Nguyen and Paolo Bolettieri and Andreas Leibetseder and Stefanos Vrochidis},
  doi          = {10.1109/TMM.2020.2980944},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {243-256},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Interactive video retrieval in the age of deep learning – detailed evaluation of VBS 2019},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph embedding multi-kernel metric learning for image set
classification with grassmannian manifold-valued features. <em>TMM</em>,
<em>23</em>, 228–242. (<a
href="https://doi.org/10.1109/TMM.2020.2981189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of video-based image set classification, a considerable advance has been made by modeling a sequence of video frames (image set) as a linear subspace, which typically resides on a Grassmannian manifold. As a consequence of the large intra-class variations of the video data, there are two open challenges for the modeling task: how to establish appropriate image set models to encode these variations, and how to effectively measure the similarity between any two image sets. As a possible way to tackle these issues, this paper presents a graph embedding multi-kernel metric learning (GEMKML) algorithm for image set classification. The proposed GEMKML implements set modeling, feature extraction, and classification in two steps. Firstly, the proposed framework constructs a novel cascaded feature learning architecture on Grassmannian manifold with the aim of producing more effective Grassmannian manifold-valued feature representations. To make a better use of these learned features, a graph embedding multi-kernel metric learning scheme is then devised to map them into a lower-dimensional Euclidean space, where the inter-class distances are maximized and the intra-class distances are minimized. We evaluate the proposed GEMKML on five different visual classification tasks using widely adopted datasets. The extensive classification results confirm its superiority over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Rui Wang and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1109/TMM.2020.2981189},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {228-242},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph embedding multi-kernel metric learning for image set classification with grassmannian manifold-valued features},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing underexposed photos using perceptually
bidirectional similarity. <em>TMM</em>, <em>23</em>, 189–202. (<a
href="https://doi.org/10.1109/TMM.2020.2982045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although remarkable progress has been made, existing methods for enhancing underexposed photos tend to produce visually unpleasing results due to the existence of visual artifacts ( e . g ., color distortion, loss of details and uneven exposure). We observed that this is because they fail to ensure the perceptual consistency of visual information between the source underexposed image and its enhanced output. To obtain high-quality results free of these artifacts, we present a novel underexposed photo enhancement approach that is able to maintain the perceptual consistency. We achieve this by proposing an effective criterion, referred to as perceptually bidirectional similarity, which explicitly describes how to ensure the perceptual consistency. Particularly, we adopt the Retinex theory and cast the enhancement problem as a constrained illumination estimation optimization, where we formulate perceptually bidirectional similarity as constraints on illumination and solve for the illumination which can recover the desired artifact-free enhancement results. In addition, we describe a video enhancement framework that adopts the presented illumination estimation for handling underexposed videos. To this end, a probabilistic approach is introduced to propagate illuminations of sampled keyframes to the entire video by tackling a Bayesian Maximum A Posteriori problem. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Qing Zhang and Yongwei Nie and Lei Zhu and Chunxia Xiao and Wei-Shi Zheng},
  doi          = {10.1109/TMM.2020.2982045},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {189-202},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing underexposed photos using perceptually bidirectional similarity},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning representations for high-dynamic-range image color
transfer in a self-supervised way. <em>TMM</em>, <em>23</em>, 176–188.
(<a href="https://doi.org/10.1109/TMM.2020.2981994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference-based color transfer between images has been a fundamental function in image editing. However, existing approaches pay less attention to high-dynamic-range (HDR) images. It is worth noting that designing an appropriate representation for HDR images to achieve satisfying color transfer is challenging. In this paper, we propose an innovative high-dynamic-range image color transfer generative adversarial network (HDRCTGAN) to encode the original image into fine representations that allow transfer of the color of the reference image to the target image. We propose to learn fine representations through a generative adversarial network (GAN) in a self-supervised way. Particularly, the proposed method is self-supervised learning that requires only unlabeled HDR images instead of supervised learning that requires lots of ground truth pairs. HDRCTGAN consists of a generator to transfer the color of the reference image to the target image over the feature domain and a discriminator to suppress the artifacts caused by the generator. We also design a loss function to ensure that HDRCTGAN possesses two required properties: (a) high fidelity and (b) self-identity. The proposed approach yields a pleasing visual result. We have carried out HDR specific evaluations including both objective quantitative experiments with HDR metrics and subjective user studies operated on HDR display devices to demonstrate the effectiveness of our method. Furthermore, we have verified the applicability of the proposed approach to several applications, such as color transfer of HDR images captured by smartphones, color transfer of fabric images, and reference-based grayscale image colorization.},
  archive      = {J_TMM},
  author       = {Yifei Huang and Sheng Qiu and Changbo Wang and Chenhui Li},
  doi          = {10.1109/TMM.2020.2981994},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {176-188},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning representations for high-dynamic-range image color transfer in a self-supervised way},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose-guided tracking-by-detection: Robust multi-person pose
tracking. <em>TMM</em>, <em>23</em>, 161–175. (<a
href="https://doi.org/10.1109/TMM.2020.2980194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person pose tracking task aims to estimate and track person keypoints in videos. Most of the previous methods follow the general track-by-detection strategy that ignores the consistent pose information during the whole framework. Thus, they often suffer from missing detections or inaccurate human association in challenging scenes with motion blur or person occlusion. To handle those problems, we propose a pose-guided tracking-by-detection framework that fuses pose information into both video human detection and human association procedures. In the video human detection stage, we adopt the pose-guided person location prediction exploiting the temporal information to make up missing detections. Technically, pose heatmaps are utilized to cope with the person-specific intra-class distractors. Furthermore, in the human association stage, we propose an appearance discriminative model based on the hierarchical pose-guided graph convolutional networks (PoseGCN). The PoseGCN-based model exploits human structural relations to boost person representation. Extensive experiments show the superiority of our method on the challenging pose tracking benchmark. Our proposed method ranks first on the PoseTrack leaderboard. 1 1http://posetrack.net/leaderboard.php till the submission date (22-Aug-2019) of this paper. Our code has been publicly available at https://github.com/human-centric982/PGPT .},
  archive      = {J_TMM},
  author       = {Qian Bao and Wu Liu and Yuhao Cheng and Boyan Zhou and Tao Mei},
  doi          = {10.1109/TMM.2020.2980194},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {161-175},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pose-guided tracking-by-detection: Robust multi-person pose tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial disentanglement spectrum variations and
cross-modality attention networks for NIR-VIS face recognition.
<em>TMM</em>, <em>23</em>, 145–160. (<a
href="https://doi.org/10.1109/TMM.2020.2980201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-infrared and visual (NIR-VIS) matching task refers to the face recognition between the two images of different modalities, which remains a challenging task in the field of machine vision. The main problems of NIR-VIS Heterogeneous Face Recognition (HFR) tasks include two aspects: large intra-class differences caused by cross-modal data, and insufficient paired training samples. In this paper, an effective Adversarial Disentanglement spectrum variations and Cross-modality Attention Networks (ADCANs) is proposed for VIS-NIR matching task. Three key components are introduced to the ADCANs for reducing the gap of cross-modal images: Advanced Scatter Loss (ASL), Modality-adversarial Feature Learning (MaFL) and Cross-modality Attention Block (CmAB). The proposed ASL loss captures between- and within-class information of the data and embeds them to the network for more effective training, and it focuses on categories with small between-class distance and increases the distance between them. The MaFL consists of an Identity-Discriminative Feature Learning Network (IDFLN) and a Modality-Adversarial Disentanglement Network (MADN), which can enhance the identity-discriminative feature representations as well as disentangling spectrum variations via an adversarial learning. The IDFLN built by an end-to-end CNNs aims at learning identity-discriminative feature. While the MADN built by a discriminator $D$ and a generator $G$ focuses on removing modality-related information. Furthermore, to increase representation power as well as disentangling spectrum variations effectively, a CmAB block is introduced to the network, which sequentially applies spatial and channel attention modules to both the IDFLN and MADN. Since the channel attention module focuses on ‘what’ features to suppress or emphasize, an orthogonality constraint is introduced to the two channel attention modules, which allows MADN and IDFLN to focus on learning modality-related features and identity-related features, respectively. In particular, the ADCANs consists of multiple CmAB blocks to learn discriminative features and disentangle spectrum variations. A large number of experiments on three challenging HFR datasets indicate that the proposed ADCANs is effective for VIS-NIR HFR task.},
  archive      = {J_TMM},
  author       = {Weipeng Hu and Haifeng Hu},
  doi          = {10.1109/TMM.2020.2980201},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {145-160},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial disentanglement spectrum variations and cross-modality attention networks for NIR-VIS face recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose-normalized and appearance-preserved street-to-shop
clothing image generation and feature learning. <em>TMM</em>,
<em>23</em>, 133–144. (<a
href="https://doi.org/10.1109/TMM.2020.2978669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the task of street-to-shop clothing image synthesis. Given a daily person image with a particular clothing item captured in the street scenario, we aim to synthesize the frontal facing view of that item in the shop scenario. This problem has the following challenges: 1) the distinct visual discrepancy between the street and shop scenario; 2) the severe shape deformation of clothing in the presence of an arbitrary human pose; 3) the preservation of fine-grained details during the process of clothing image generation. In this paper, we jointly solve these difficulties by proposing a Pose-Normalized and Appearance-Preserved Generative Adversarial Network (PNAP-GAN). More specifically, conditioned on the clothing-agnostic representation (i.e., clothing landmarks and semantic parsing map), we disentangle the shape and appearance synthesis in a coarse-to-fine framework. Moreover, a semantic embedding loss is introduced to guide the domain transfer in the semantic level (i.e., keeping the clothing attributes). With the synthesized frontal shop image, a pose-normalized representation in complementary to the domain-invariant feature learnt from the original street image are integrated to facilitate the problem of street-to-shop clothing retrieval. Extensive experiments conducted demonstrate the effectiveness of the proposed PNAP-GAN on generating high quality frontal-view images and the excellence of the learnt pose-normalized features on the retrieval task than existing methods. In addition, we demonstrate that the pose-normalized retrieval feature benefits the cross-scenario (i.e., street-to-shop) clothing image generation in a semantic-preserved manner.},
  archive      = {J_TMM},
  author       = {Huijing Zhan and Chenyu Yi and Boxin Shi and Jie Lin and Ling-Yu Duan and Alex C. Kot},
  doi          = {10.1109/TMM.2020.2978669},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {133-144},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pose-normalized and appearance-preserved street-to-shop clothing image generation and feature learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Siamese tracking network with informative enhanced loss.
<em>TMM</em>, <em>23</em>, 120–132. (<a
href="https://doi.org/10.1109/TMM.2020.2978636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing an effective and uniform framework to meliorate tracking performance is very meaningful and essential. However, existing methods merely focus on single positive and negative instances corresponding to the exemplar, thoroughly ignoring the effective information hidden in other instances. To tackle this issue, in this paper, we present an informative enhanced loss based Siamese tracking network. Specifically, we introduce an informative enhanced loss to enable the network to capture information from an overall perspective. In other words, we construct dense connections among instances and exemplar. More importantly, we prove that our proposed loss can be transformed into the logistic loss and the triplet loss under particular parameter settings. Experiments on prevalent benchmarks demonstrate that the Siamese frameworks trained with our proposed loss indeed obtain better tracking results than original ones, and achieve promising performance against several state-of-the-art trackers on the real-time challenge.},
  archive      = {J_TMM},
  author       = {Shengjing Tian and Xiuping Liu and Meng Liu and Shuhua Li and Baocai Yin},
  doi          = {10.1109/TMM.2020.2978636},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {120-132},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Siamese tracking network with informative enhanced loss},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bottom-up and top-down integration framework for online
object tracking. <em>TMM</em>, <em>23</em>, 105–119. (<a
href="https://doi.org/10.1109/TMM.2020.2978623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust online object tracking entails integrating short-term memory based trackers and long-term memory based trackers in an elegant framework to handle structural and appearance variations of unknown objects in an online manner. The integration and synergy between short-term and long-term memory based trackers have yet studied well in the literature, especially in pre-training free settings. To address this issue, this paper presents a bottom-up and top-down integration framework. The bottom-up component realizes a data-driven approach for particle generation. It exploits a short-term memory based tracker to generate bounding box proposals in a new frame. In the top-down component, this paper presents a graph regularized sparse coding scheme as the long-term memory based tracker. The over-complete bases for sparse coding are composed of part-based representations learned from earlier tracking results and new observations to form a space with rich temporal context information. A particle graph is computed whose nodes are the bottom-up discriminative particles and edges are formed on-the-fly in terms of appearance and spatial-temporal similarities between particles. The particle graph induces a regularization term in optimizing the sparse coding coefficients for bottom-up particles. In experiments, the proposed method is tested on the widely used OTB-100 benchmark and the VOT2016 benchmark with better performance obtained than baselines including deep learning based trackers. In addition, the outputs from the top-down sparse coding are potentially useful for downstream tasks such as action recognition, multiple-object tracking, and object re-identification.},
  archive      = {J_TMM},
  author       = {Meihui Li and Lingbing Peng and Tianfu Wu and Zhenming Peng},
  doi          = {10.1109/TMM.2020.2978623},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {105-119},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A bottom-up and top-down integration framework for online object tracking},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating part of speech guidance for image captioning.
<em>TMM</em>, <em>23</em>, 92–104. (<a
href="https://doi.org/10.1109/TMM.2020.2976552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To generate an image caption, firstly, the content of the image should be fully understood; and then the semantic information contained in the image should be described using a phrase or statement that conforms to certain grammatical rules. Thus, it requires techniques from both computer vision and natural language processing to connect the two different media forms together, which is highly challenging. To adaptively adjust the effect of visual information and language information on the captioning process, in this paper, the part of speech information is proposed to novelly integrate with image captioning models based on the encoder-decoder framework. First, a part of speech prediction network is proposed to analyze and model the part of speech sequences for the words in natural language sentences; then, different mechanisms are proposed to integrate the part of speech guidance information with merge-based and inject-based image captioning models, respectively; finally, according to the integrated frameworks, a multi-task learning paradigm is proposed to facilitate model training. Experiments are conducted on two widely used image captioning datasets, Flickr30 k and COCO, and the results have validated that the image captions generated by the proposed method contain more accurate visual information and comply with language habits and grammar rules better.},
  archive      = {J_TMM},
  author       = {Ji Zhang and Kuizhi Mei and Yu Zheng and Jianping Fan},
  doi          = {10.1109/TMM.2020.2976552},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {92-104},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Integrating part of speech guidance for image captioning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-fidelity reversible image watermarking based on
effective prediction error-pairs modification. <em>TMM</em>,
<em>23</em>, 52–63. (<a
href="https://doi.org/10.1109/TMM.2020.2982042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reversible watermarking for image authentication, less degradation of the marked image is always desirable. For minimum distortion, the pairwise prediction-error expansion (PEE) technique was recently proposed to modify errors jointly. Although its superiority over conventional PEE has been verified, its potential has not been fully exploited yet. In this paper, we focus on optimal modification and propose an enhanced pairwise PEE. First, it is observed in PVO-based pairwise PEE that the histogram peak varies with relative location when predicting the largest/smallest two pixels. Then, a more effective 2D mapping is proposed by content-dependently selecting the expansion bin after introducing spatial location into prediction. Next, the 2D mapping is further extended considering prediction in non-smooth region tends to produce errors with large magnitude. Finally, we also propose to flexibly define the spatial location to achieve content-dependent prediction and further enhancement. Experimental results demonstrate that the proposed scheme achieves better capacity-distortion trade-off and outperforms several state-of-the-art schemes.},
  archive      = {J_TMM},
  author       = {Wenguang He and Zhanchuan Cai and Yaomin Wang},
  doi          = {10.1109/TMM.2020.2982042},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {52-63},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {High-fidelity reversible image watermarking based on effective prediction error-pairs modification},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast non-local adaptive in-loop filter optimization on GPU.
<em>TMM</em>, <em>23</em>, 39–51. (<a
href="https://doi.org/10.1109/TMM.2020.2981185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-local adaptive in-loop filter (NALF) for video coding has achieved significant coding gain by exploiting image non-local self-similarity (NSS) to efficiently reduce the compression artifacts. However, the intensive computation of NALF hinders its practical deployment in video standardizations. In this paper, we propose a fast NALF optimization algorithm in parallel-computing framework by leveraging the massive parallel execution resources of GPU. First, the computational complexity of original NALF is analyzed in depth, then the pipelines of computational-intensive modules are re-designed to adapt to the general-purpose GPU with more parallel-friendly consideration. Specifically, we speed up the NALF by optimizing thread allocation to maximize the parallelism degree and elaborately designing the GPU block dimension to avoid access conflict. The group-level and pixel-level parallelization for collaboratively filtering and patch matching modules are designed respectively. To reduce the cost in data transmission, the whole filtering process is implemented on GPU by taking the advantage of low data dependency in NALF. Extensive experimental results show that the proposed fast NALF optimization using GPU architecture achieves high-speeed processing while maintaining the significant coding performance of original NALF, which shows the potential of NALF in the future video coding standard.},
  archive      = {J_TMM},
  author       = {Chuanmin Jia and Falei Luo and Xinfeng Zhang and Shiqi Wang and Shanshe Wang and Siwei Ma},
  doi          = {10.1109/TMM.2020.2981185},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {39-51},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Fast non-local adaptive in-loop filter optimization on GPU},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning localized representations of point clouds with
graph-convolutional generative adversarial networks. <em>TMM</em>,
<em>23</em>, 402–414. (<a
href="https://doi.org/10.1109/TMM.2020.2976627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds are an important type of geometric data generated by 3D acquisition devices, and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Recently, supervised and semisupervised problems for point clouds leveraged graph convolution, a generalization of the convolution operation for data defined over graphs. This operation has been shown to be very successful at extracting localized features from point clouds. In this paper, we study the unsupervised problem of a generative model exploiting graph convolution. Employing graph convolution operations in generative models is not straightforward and it poses some unique challenges. In particular, we focus on the generator of a GAN, where the graph is not known in advance as it is the very output of the generator. We show that the proposed architecture can learn to generate the graph and the features simultaneously. We also study the problem of defining an upsampling layer in the graph-convolutional generator, proposing two methods that respectively learn to exploit a multi-resolution or self-similarity prior to sample the data distribution.},
  archive      = {J_TMM},
  author       = {Diego Valsesia and Giulia Fracastoro and Enrico Magli},
  doi          = {10.1109/TMM.2020.2976627},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {402-414},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning localized representations of point clouds with graph-convolutional generative adversarial networks},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPA-GAN: Spatial attention GAN for image-to-image
translation. <em>TMM</em>, <em>23</em>, 391–401. (<a
href="https://doi.org/10.1109/TMM.2020.2975961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation is to learn a mapping between images from a source domain and images from a target domain. In this paper, we introduce the attention mechanism directly to the generative adversarial network (GAN) architecture and propose a novel spatial attention GAN model (SPA-GAN) for image-to-image translation tasks. SPA-GAN computes the attention in its discriminator and use it to help the generator focus more on the most discriminative regions between the source and target domains, leading to more realistic output images. We also find it helpful to introduce an additional feature map loss in SPA-GAN training to preserve domain specific features during translation. Compared with existing attention-guided GAN models, SPA-GAN is a lightweight model that does not need additional attention networks or supervision. Qualitative and quantitative comparison against state-of-the-art methods on benchmark datasets demonstrates the superior performance of SPA-GAN.},
  archive      = {J_TMM},
  author       = {Hajar Emami and Majid Moradi Aliabadi and Ming Dong and Ratna Babu Chinnam},
  doi          = {10.1109/TMM.2020.2975961},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {391-401},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPA-GAN: Spatial attention GAN for image-to-image translation},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-synchronization using the hand preceding model for
multi-modal fusion in automatic continuous cued speech recognition.
<em>TMM</em>, <em>23</em>, 292–305. (<a
href="https://doi.org/10.1109/TMM.2020.2976493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cued Speech (CS) is an augmented lip reading system complemented by hand coding, and it is very helpful to the deaf people. Automatic CS recognition can help communications between the deaf people and others. Due to the asynchronous nature of lips and hand movements, fusion of them in automatic CS recognition is a challenging problem. In this work, we propose a novel re-synchronization procedure for multi-modal fusion, which aligns the hand features with lips feature. It is realized by delaying hand position and hand shape with their optimal hand preceding time which is derived by investigating the temporal organizations of hand position and hand shape movements in CS. This re-synchronization procedure is incorporated into a practical continuous CS recognition system that combines convolutional neural network (CNN) with multi-stream hidden markov model (MSHMM). A significant improvement of about 4.6% has been achieved retaining 76.6% CS phoneme recognition correctness compared with the state-of-the-art architecture (72.04%), which did not take into account the asynchrony issue of multi-modal fusion in CS. To our knowledge, this is the first work to tackle the asynchronous multi-modal fusion in the automatic continuous CS recognition.},
  archive      = {J_TMM},
  author       = {Li Liu and Gang Feng and Denis Beautemps and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2020.2976493},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {292-305},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Re-synchronization using the hand preceding model for multi-modal fusion in automatic continuous cued speech recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical reasoning network for pedestrian attribute
recognition. <em>TMM</em>, <em>23</em>, 268–280. (<a
href="https://doi.org/10.1109/TMM.2020.2975417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian attribute recognition, which can benefit other tasks such as person re-identification and pedestrian retrieval, is very important in video surveillance related tasks. In this paper, we observe that the existing methods tackle this problem from the perspective of multi-label classification without considering the hierarchical relationships among the attributes. In human cognition, the attributes can be categorized according to their semantic/abstraction levels. The high-level attributes can be predicted by reasoning from the low-level and medium-level attributes, while the recognition of the low-level and medium-level attributes can be guided by the high-level attributes. Based on this attribute categorization, we propose a novel Hierarchical Reasoning Network (HR-Net), which can hierarchically predict the attributes at different abstraction levels in different stages of the network. We also propose an attribute reasoning structure to exploit the relationships among the attributes at different semantic levels. Experimental results demonstrate that the proposed network gives superior performances compared to the state-of-the-art techniques.},
  archive      = {J_TMM},
  author       = {Haoran An and Hai-Miao Hu and Yuanfang Guo and Qianli Zhou and Bo Li},
  doi          = {10.1109/TMM.2020.2975417},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {268-280},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical reasoning network for pedestrian attribute recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning adaptive neighborhood graph on grassmann manifolds
for video/image-set subspace clustering. <em>TMM</em>, <em>23</em>,
216–227. (<a href="https://doi.org/10.1109/TMM.2020.2975394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of self-expression based spectral clustering is to learn an affinity matrix which accurately reflects the similarity among data, and the Laplacian constraint is usually exploited to make the affinity matrix preserve the global structure of raw data. However, there exist two drawbacks: firstly, these methods are mostly designed for vectorial data in Euclidean spaces, which are not suitable for multidimensional data with nonlinear manifold structure, e.g., videos and image-sets. Secondly, the clustering performance heavily relies on the quality of a pre-learned Laplacian matrix in which the global structure may be mis-interpreted without considering manifold structures. In this paper, we firstly provide a unified framework about self-expression learning on Grassmann manifolds, which implements the clustering tasks for multidimensional data under subspace views. Then, to assign optimal neighbors to each data depending on the local distance, we adaptively learn the neighborhood relationship from the obtained self-expression coefficient matrix, referred to Learning Adaptive Neighborhood Graph on Grassmann manifolds (GMAN). In the optimization process, the neighborhood relationship can be adaptively learned and updated with the coefficient matrix. The experimental results on five public datasets show that the proposed method is obviously better than many related clustering methods based on Grassmann manifolds, proving the effectiveness of GMAN in multidimensional data clustering.},
  archive      = {J_TMM},
  author       = {Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Fujiao Ju and Baocai Yin},
  doi          = {10.1109/TMM.2020.2975394},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {216-227},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning adaptive neighborhood graph on grassmann manifolds for Video/Image-set subspace clustering},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constant size point cloud clustering: A compact,
non-overlapping solution. <em>TMM</em>, <em>23</em>, 77–91. (<a
href="https://doi.org/10.1109/TMM.2020.2974325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds have recently become a popular 3D representation model for many application domains, notably virtual and augmented reality. Since point cloud data is often very large, processing a point cloud may require that it be segmented into smaller clusters. For example, the input to deep learning-based methods like auto-encoders should be constant size point cloud clusters, which are ideally compact and non-overlapping. However, given the unorganized nature of point clouds, defining the specific data segments to code is not always trivial. This paper proposes a point cloud clustering algorithm which targets five main goals: i) clusters with a constant number of points; ii) compact clusters, i.e., with low dispersion; iii) non-overlapping clusters, i.e., not intersecting each other; iv) ability to scale with the number of points; and v) low complexity. After appropriate initialization, the proposed algorithm transfers points between neighboring clusters as a propagation wave, filling or emptying clusters until they achieve the same size. The proposed algorithm is unique since there is no other point cloud clustering method available in the literature offering the same clustering features for large point clouds at such low complexity.},
  archive      = {J_TMM},
  author       = {André F. R. Guarda and Nuno M. M. Rodrigues and Fernando Pereira},
  doi          = {10.1109/TMM.2020.2974325},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {77-91},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Constant size point cloud clustering: A compact, non-overlapping solution},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-stream graph convolutional networks-hidden
conditional random field model for skeleton-based action recognition.
<em>TMM</em>, <em>23</em>, 64–76. (<a
href="https://doi.org/10.1109/TMM.2020.2974323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Graph Convolutional Network(GCN) methods for skeleton-based action recognition have achieved great success due to their ability to preserve structural information of the skeleton. However, these methods abandon the structural information in the classification stage by employing traditional fully-connected layers and softmax classifier, leading to sub-optimal performance. In this work, a novel Graph Convolutional Networks-Hidden conditional Random Field (GCN-HCRF) model is proposed to solve this problem. The proposed method combines GCN with HCRF to retain the human skeleton structure information even during the classification stage. Our model is trained end-to-end by utilizing the message passing from the belief propagation algorithm on the human structure graph. To further capture spatial and temporal information, we propose a multi-stream framework which takes the relative coordinate of the joints and bone direction as two static feature streams, and the temporal displacements between two consecutive frames as the dynamic feature stream. Experimental results on three challenging benchmarks (NTU RGB+D, N-UCLA, SYSU) show the superior performance of the proposed model over state-of-the-art models.},
  archive      = {J_TMM},
  author       = {Kai Liu and Lei Gao and Naimul Mefraz Khan and Lin Qi and Ling Guan},
  doi          = {10.1109/TMM.2020.2974323},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {64-76},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A multi-stream graph convolutional networks-hidden conditional random field model for skeleton-based action recognition},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BVI-SynTex: A synthetic video texture dataset for video
compression and quality assessment. <em>TMM</em>, <em>23</em>, 26–38.
(<a href="https://doi.org/10.1109/TMM.2020.2976591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly textured video content is challenging to compress since the bit-rate to video quality trade-off is high and complex perceptual masking influences performance. Test datasets that cover a wide range of texture types are thus important for codec evaluation, but few exist. In order to study the properties of video texture, this paper introduces a Synthetic video Texture dataset (BVI-SynTex) that was generated using a Computer-Generated Imagery (CGI) environment. It contains 196 sequences clustered in three different texture types and offers the capability of being able to generate many versions of the same scene with different video parameters. It therefore provides a flexible basis for studying the influence of texture type and parameters on video compression and perceived video quality. A thorough validation and comparison of BVI-SynTex with similarly textured natural video content is performed. The comparisons show that BVI-SynTex exhibits a comparable coverage over the spatial and temporal domain and that it produces similar encoding statistics to real video datasets. A subset of the BVI-SynTex dataset was selected to perform a subjective evaluation of compression using the MPEG HEVC codec.The results show the impact of the content parameters to both the compression efficiency and the perceived quality. The publicly available BVI-SynTex dataset contains all source sequences, the objective and subjective analysis results, providing a valuable resource for the research community.},
  archive      = {J_TMM},
  author       = {Angeliki V. Katsenou and Goce Dimitrov and Di Ma and David R. Bull},
  doi          = {10.1109/TMM.2020.2976591},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {26-38},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BVI-SynTex: A synthetic video texture dataset for video compression and quality assessment},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft video multicasting using adaptive compressed sensing.
<em>TMM</em>, <em>23</em>, 12–25. (<a
href="https://doi.org/10.1109/TMM.2020.2975420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, soft video multicasting has gained a lot of attention, especially in broadcast and mobile scenarios where the bit rate supported by the channel may differ across receivers, and may vary quickly over time. Unlike the conventional designs that force the source to use a single bit rate according to the receiver with the worst channel quality, soft video delivery schemes transmit the video such that the video quality at each receiver is commensurate with its specific instantaneous channel quality. In this paper, we present a soft video multicasting system using an adaptive block-based compressed sensing (BCS) method. The proposed system consists of an encoder, a transmission system, and a decoder. At the encoder side, each block in each frame of the input video is adaptively sampled with a rate that depends on the texture complexity and visual saliency of the block. The obtained BCS samples are then placed into several packets, and the packets are transmitted via a channel-aware OFDM (orthogonal frequency division multiplexing) transmission system with a number of subchannels. At the decoder side, the received BCS samples are first used to build an initial approximation of the transmitted frame. To further improve the reconstruction quality, an iterative BCS reconstruction algorithm is then proposed that uses an adaptive transform and an adaptive soft-thresholding operator, which exploits the temporal similarity between adjacent frames to achieve better reconstruction quality. The extensive objective and subjective experimental results indicate the superiority of the proposed system over the state-of-the-art soft video multicasting systems.},
  archive      = {J_TMM},
  author       = {Hadi Hadizadeh and Ivan V. Bajić},
  doi          = {10.1109/TMM.2020.2975420},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {12-25},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Soft video multicasting using adaptive compressed sensing},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end audiovisual speech recognition system with
multitask learning. <em>TMM</em>, <em>23</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2020.2975922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An automatic speech recognition (ASR) system is a key component in current speech-based systems. However, the surrounding acoustic noise can severely degrade the performance of an ASR system. An appealing solution to address this problem is to augment conventional audio-based ASR systems with visual features describing lip activity. This paper proposes a novel end-to-end, multitask learning (MTL), audiovisual ASR (AV-ASR) system. A key novelty of the approach is the use of MTL, where the primary task is AV-ASR, and the secondary task is audiovisual voice activity detection (AV-VAD). We obtain a robust and accurate audiovisual system that generalizes across conditions. By detecting segments with speech activity, the AV-ASR performance improves as its connectionist temporal classification (CTC) loss function can leverage from the AV-VAD alignment information. Furthermore, the end-to-end system learns from the raw audiovisual inputs a discriminative high-level representation for both speech tasks, providing the flexibility to mine information directly from the data. The proposed architecture considers the temporal dynamics within and across modalities, providing an appealing and practical fusion scheme. We evaluate the proposed approach on a large audiovisual corpus (over 60 hours), which contains different channel and environmental conditions, comparing the results with competitive single task learning (STL) and MTL baselines. Although our main goal is to improve the performance of our ASR task, the experimental results show that the proposed approach can achieve the best performance across all conditions for both speech tasks. In addition to state-of-the-art performance in AV-ASR, the proposed solution can also provide valuable information about speech activity, solving two of the most important tasks in speech-based applications.},
  archive      = {J_TMM},
  author       = {Fei Tao and Carlos Busso},
  doi          = {10.1109/TMM.2020.2975922},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {End-to-end audiovisual speech recognition system with multitask learning},
  volume       = {23},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
