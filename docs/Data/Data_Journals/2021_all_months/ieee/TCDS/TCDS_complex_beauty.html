<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds---92">TCDS - 92</h2>
<ul>
<li><details>
<summary>
(2021a). IEEE computational intelligence society information.
<em>TCDS</em>, <em>13</em>(4), C3. (<a
href="https://doi.org/10.1109/TCDS.2021.3107175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2021.3107175},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Present a world of opportunity. <em>TCDS</em>,
<em>13</em>(4), 1034. (<a
href="https://doi.org/10.1109/TCDS.2021.3109688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2021.3109688},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1034},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Present a world of opportunity},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Off-policy deep reinforcement learning based on steffensen
value iteration. <em>TCDS</em>, <em>13</em>(4), 1023–1032. (<a
href="https://doi.org/10.1109/TCDS.2020.3034452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important machine learning method, deep reinforcement learning (DRL) has been rapidly developed in recent years and has achieved breakthrough results in many fields, such as video games, natural language processing, and robot control. However, due to the inherit trial-and-error learning mechanism of reinforcement learning and the time-consuming training of deep neural network itself, the convergence speed of DRL is very slow and consequently limits the real applications of DRL. In this article, aiming to improve the convergence speed of DRL, we proposed a novel Steffensen value iteration (SVI) method by applying the Steffensen iteration to the value function iteration of off-policy DRL from the perspective of fixed-point iteration. The proposed SVI is theoretically proved to be convergent and have a faster convergence speed than Bellman value iteration. The proposed SVI has versatility, which can be easily combined with existing off-policy RL algorithms. In this article, we proposed two speedy off-policy DRLs by combining SVI with DDQN and TD3, respectively, namely, SVI-DDQN and SVI-TD3. Experiments on several discrete-action and continuous-action tasks from the Atari 2600 and MuJoCo platforms demonstrated that our proposed SVI-based DRLs can achieve higher average reward in a shorter time than the comparative algorithm.},
  archive      = {J_TCDS},
  author       = {Yuhu Cheng and Lin Chen and C. L. Philip Chen and Xuesong Wang},
  doi          = {10.1109/TCDS.2020.3034452},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1023-1032},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Off-policy deep reinforcement learning based on steffensen value iteration},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete interactions in decentralized multiagent
coordination: A probabilistic perspective. <em>TCDS</em>,
<em>13</em>(4), 1010–1022. (<a
href="https://doi.org/10.1109/TCDS.2020.3040769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While current work has proved that the interactions between tightly coupled multiagent is one of the effective means to overcome the information partially observable constraints, creating coordinative policies among loose coupled agents under uncertainty is still a big challenge. In this article, we explored how the discrete interactions affect the decentralized multiagent coordination, in which the coordinative relationship changes dynamically with randomly arrived tasks. We first proposed a decision model, DDI-MDPs, which supports both the independence and the interactive decision making, that is, interactions are spatiotemporal discrete occurred. On this basis, we contributed a heuristic imprecise probabilistic-based interaction algorithm, HDLI, which utilizes the multidimensional semantic relevance among agents, information and tasks, so that agents can continuously improve their cognition and optimize the decision-making efficiency by learning the interaction records. In addition, we dissected the transformation conditions between DDI-MDPs and the classic models, and modeled the dynamic relationships into a dynamic graph, and then analyzed its evolution processes. In the simulation, we evaluated the efficiency of the proposed work in several typical coordination scenarios, the results reveal that the interactive-growth relationship between agents has typical social network characteristics. Finally, some possible challenges in the follow-up research work and applications are discussed.},
  archive      = {J_TCDS},
  author       = {Ming Liu and Weiling Chang and Chao Li and Yuchun Ji and Ruiguang Li and Minyu Feng},
  doi          = {10.1109/TCDS.2020.3040769},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {1010-1022},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Discrete interactions in decentralized multiagent coordination: A probabilistic perspective},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OnionNet: Single-view depth prediction and camera pose
estimation for unlabeled video. <em>TCDS</em>, <em>13</em>(4), 995–1009.
(<a href="https://doi.org/10.1109/TCDS.2020.3042521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real scenes, humans can easily infer their positions and distances from other objects with their own eyes. To make the robots have the same visual ability, this article presents an unsupervised OnionNet framework, including LeafNet and ParachuteNet, for single-view depth prediction and camera pose estimation. In OnionNet, for speeding up OnionNet’s convergence and concretizing objects against the gradient locality and moving objects in videos, LeafNet adopts two decoders and enhanced upconvolution modules. Meanwhile, for improving the robustness of fast camera movement and rotation, ParachuteNet uses and integrates three pose networks to estimate multiview camera pose parameters by combining with the modified image preprocess. Different from existing methods, single-view depth prediction and camera pose estimation are trained view by view, where the variations between views is gradual reduction of view range and outer pixels disappear in next view, similar to onion peeling. Moreover, the LeafNet is optimized with pose parameter from each pose network in turn. Experimental results on the KITTI data set show the outstanding effectiveness of our method: single-view depth performs better than most supervised and unsupervised methods which contain two same subtasks, and pose estimation gets the state-of-the-art performance compared with existing methods under the comparable input settings.},
  archive      = {J_TCDS},
  author       = {Tianhao Gu and Zhe Wang and Dongdong Li and Hai Yang and Wenli Du and Yangming Zhou},
  doi          = {10.1109/TCDS.2020.3042521},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {995-1009},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {OnionNet: Single-view depth prediction and camera pose estimation for unlabeled video},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A study of joint effect on denoising techniques and visual
cues to improve speech intelligibility in cochlear implant simulation.
<em>TCDS</em>, <em>13</em>(4), 984–994. (<a
href="https://doi.org/10.1109/TCDS.2020.3017042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech perception is the key to verbal communication. For people with hearing loss, the capability to recognize speech is restricted, particularly in a noisy environment or the situations without visual cues, such as lip-reading unavailable via phone call. This study aimed to understand the improvement of vocoded speech intelligibility in cochlear implant (CI) simulation through two potential methods: 1) speech enhancement (SE) and 2) audiovisual integration. A fully convolutional neural network (FCN) using an intelligibility-oriented objective function was recently proposed and proven to effectively facilitate the speech intelligibility as an advanced denoising SE approach. Furthermore, audiovisual integration is reported to supply better speech comprehension compared to audio-only information. An experiment was designed to test speech intelligibility using tone-vocoded speech in CI simulation with a group of normal-hearing listeners. The experimental results confirmed the effectiveness of the FCN-based denoising SE and audiovisual integration on vocoded speech. Also, it positively recommended that these two methods could become a blended feature in a CI processor to improve the speech intelligibility for CI users under noisy conditions.},
  archive      = {J_TCDS},
  author       = {Rung-Yu Tseng and Tao-Wei Wang and Szu-Wei Fu and Chia-Ying Lee and Yu Tsao},
  doi          = {10.1109/TCDS.2020.3017042},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {984-994},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A study of joint effect on denoising techniques and visual cues to improve speech intelligibility in cochlear implant simulation},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Color facial expression recognition by quaternion
convolutional neural network with gabor attention. <em>TCDS</em>,
<em>13</em>(4), 969–983. (<a
href="https://doi.org/10.1109/TCDS.2020.3041642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) plays an important role in cognitive psychology research. In FER studies, deep convolutional neural networks (CNNs) and attention mechanisms have shown great advantages over traditional techniques. Although many attention-based CNN models have been developed, most of them are proposed to deal with grayscale images. For color images, these network models usually take their grayscale versions as the input or convert them to grayscale data by the first hidden layer. Furthermore, their attention modules focus on only grayscale information. This mechanism does not fully consider the correlation between color channels, which will degrade the performance of color FER. To alleviate these shortcomings, we introduce quaternion techniques to CNNs and propose a quaternion CNN integrated with an attention mechanism (QA-CNN) for color FER. The proposed QA-CNN takes into account not only the correlation between color channels but also expressional attention. The new attention mechanism is based on a multidirectional quaternion Gabor filter. Besides, the proposed QA-CNN reduces the number of network parameters by 75% compared with the real-valued CNN with the same structure. Experimental results on three widely used data sets demonstrate the effectiveness of the proposed QA-CNN by showing clear performance improvements over other state-of-the-art FER methods.},
  archive      = {J_TCDS},
  author       = {Yu Zhou and Lianghai Jin and Hong Liu and Enmin Song},
  doi          = {10.1109/TCDS.2020.3041642},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {969-983},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Color facial expression recognition by quaternion convolutional neural network with gabor attention},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchy graph convolution network and tree classification
for epileptic detection on electroencephalography signals.
<em>TCDS</em>, <em>13</em>(4), 955–968. (<a
href="https://doi.org/10.1109/TCDS.2020.3012278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The epileptic detection with electroencephalography (EEG) has been deeply studied and developed. However, previous research gave little attention to the physical appearance and early onset warnings of seizure. When a seizure occurs, electrodes near the epileptic foci will exhibit significantly fluctuating and inconsistent voltages. In this article, a novel approach to epileptic detection based on the hierarchy graph convolution network (HGCN) structure is proposed. Multiple features of time or frequency domains extracted from the raw EEG signals are taken as the input of HGCN. The topological relationship between every single electrode is utilized by HGCN. The tree classification (TC) and preictal fuzzification (PF) are proposed to adapt both multiclassification tasks and refine-classification tasks. Experiments are performed on the CHB-MIT and TUH data sets. Compared with the state of the art, our proposed model achieves a 5.77% improvement of accuracy on the CHB-MIT data set, and an improvement of 2.43% and 19.7% for sensitivity and specificity on the TUH data set, respectively.},
  archive      = {J_TCDS},
  author       = {Difei Zeng and Kejie Huang and Cenglin Xu and Haibin Shen and Zhong Chen},
  doi          = {10.1109/TCDS.2020.3012278},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {955-968},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Hierarchy graph convolution network and tree classification for epileptic detection on electroencephalography signals},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A channel-fused dense convolutional network for EEG-based
emotion recognition. <em>TCDS</em>, <em>13</em>(4), 945–954. (<a
href="https://doi.org/10.1109/TCDS.2020.2976112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotion recognition could greatly contribute to human–computer interaction with promising applications in artificial intelligence. One of the challenges in recognition tasks is learning effective representations with stable performances from electroencephalogram (EEG) signals. In this article, we propose a novel deep-learning framework, named channel-fused dense convolutional network, for EEG-based emotion recognition. First, we use a 1-D convolution layer to receive weighted combinations of contextual features along the temporal dimension from EEG signals. Next, inspired by state-of-the-art object classification techniques, we employ 1-D dense structures to capture electrode correlations along the spatial dimension. The developed algorithm is capable of handling temporal dependencies and electrode correlations with the effective feature extraction from noisy EEG signals. Finally, we perform extensive experiments based on two popular EEG emotion datasets. Results indicate that our framework achieves prominent average accuracies of 90.63% and 92.58% on the SEED and DEAP datasets, respectively, which both receive better performances than most of the compared studies. The novel model provides an interpretable solution with excellent generalization capacity for broader EEG-based classification tasks.},
  archive      = {J_TCDS},
  author       = {Zhongke Gao and Xinmin Wang and Yuxuan Yang and Yanli Li and Kai Ma and Guanrong Chen},
  doi          = {10.1109/TCDS.2020.2976112},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {945-954},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A channel-fused dense convolutional network for EEG-based emotion recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tutor-guided interior navigation with deep reinforcement
learning. <em>TCDS</em>, <em>13</em>(4), 934–944. (<a
href="https://doi.org/10.1109/TCDS.2020.3039859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional reinforcement learning makes policy based on the current system state. However, insufficient system information and few rewards lead to its limited applicability, especially in a partially observed environment with sparse rewards. In this work, we propose a tutor–student network (TSN) for improving an agent’s performance with additional auxiliary information. In the tutor–student framework, a tutor module generates auxiliary information, while a student module refers to the tutor’s suggestion during training. The key of our proposed approach is that tutor provides prior knowledge that does not correspond to a specified environment to help the student module accelerate the learning procedure. We build 12 indoor mazes in ViZDoom, including empty mazes and mazes with obstacles, evaluate the performance of TSN compared with advantage actor–critic (A2C) and show that the proposed network learned navigation faster and obtained higher accumulated rewards. More importantly, our approach could generalize well to new and unseen domains.},
  archive      = {J_TCDS},
  author       = {Fanyu Zeng and Chen Wang and Shuzhi Sam Ge},
  doi          = {10.1109/TCDS.2020.3039859},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {934-944},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Tutor-guided interior navigation with deep reinforcement learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling multiple language learning in a developmental
cognitive architecture. <em>TCDS</em>, <em>13</em>(4), 922–933. (<a
href="https://doi.org/10.1109/TCDS.2020.3033963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we model multiple natural language learning in a developmental neuroscience-inspired architecture. The artificial neural network with adaptive behavior exploited for language learning (ANNABELL) model, is a large-scale neural network, however, unlike most deep learning methods that solve natural language processing (NLP) tasks, it does not represent an empirical engineering solution for specific NLP problems; rather, its organization complies with findings from cognitive neuroscience, particularly the multicompartment working memory models. The system is appropriately trained to understand the level of cognitive development required for language acquisition and the robustness achieved in learning simultaneously four languages, using a corpus of text-based exchanges of developmental complexity. The selected languages, Greek, Italian and Albanian, besides English, differ significantly in structure and complexity. Initially, the system was validated in each language alone and was then compared with the open-ended cumulative training, in which languages are learned jointly, prior to querying with random language at random order. We aimed to assess if the model could learn the languages together to the same degree of skill as learning each apart. Moreover, we explored the generalization skill in multilingual context questions and the ability to elaborate a short text of preschool literature. We verified if the system could follow a dialogue coherently and cohesively, keeping track of its previous answers and recalling them in subsequent queries. The results show that the architecture developed broad language processing functionalities, with satisfactory performances in each language trained singularly, maintaining high accuracies when they are acquired cumulatively.},
  archive      = {J_TCDS},
  author       = {Ioanna Giorgi and Bruno Golosio and Massimo Esposito and Angelo Cangelosi and Giovanni L. Masala},
  doi          = {10.1109/TCDS.2020.3033963},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {922-933},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Modeling multiple language learning in a developmental cognitive architecture},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VRDriving: A virtual-to-real autonomous driving framework
based on adversarial learning. <em>TCDS</em>, <em>13</em>(4), 912–921.
(<a href="https://doi.org/10.1109/TCDS.2020.3006621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high cost of driving testing and the data collection for autonomous cars in the real world, transferring driving models from the simulator to the real world is significant and urgent. This article proposes a novel virtual-to-real driving (VRDriving) framework based on adversarial learning, which is able to transfer the ability of end-to-end learning models trained by simulation virtual data sets to the real world. The framework consists of three parts: 1) an end-to-end learning-based estimator with the cost-sensitive loss function for driving commands estimation; 2) an encoder for extracting the driving domain features from unlabeled real driving images; and 3) a discriminator for distinguishing whether the virtual driving domain features are real or fake. The above three modules can be trained simultaneously. With such a novel learning framework, the driving domain features encoded from the real images are closer to the driving domain features inside the end-to-end estimator via adversarial learning mechanism. Three experiments show the proposed VRDriving framework performs the state-of-the-art estimation accuracy of the steering command than several baselines and other typical frameworks. Therefore, the VRDriving framework is promising to improve the development efficiency of future autonomous driving.},
  archive      = {J_TCDS},
  author       = {Wei Yuan and Ming Yang and Chunxiang Wang and Bing Wang},
  doi          = {10.1109/TCDS.2020.3006621},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {912-921},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {VRDriving: A virtual-to-real autonomous driving framework based on adversarial learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identity–expression dual branch network for facial
expression recognition. <em>TCDS</em>, <em>13</em>(4), 898–911. (<a
href="https://doi.org/10.1109/TCDS.2020.3034807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate facial expression recognition is challenging because identity biases introduce large intraclass variations and high interclass similarities. Most existing facial expression recognition approaches are devoted to alleviate the effects of identity. However, based on the theories of cognitive science, psychology, and physiology, this article argues that the identity information is important and can promote expression recognition. Motivated by our investigation of the influences of identity on facial expression recognition, this article proposes an identity–expression dual branch network (IE-DBN) for facial expression recognition. First, identity-related features and expression-related features are learnt from the same input facial expression image by two branches respectively. Then, those two features are aggregated with our bilinear module. The bilinear aggregation not only emphasizes the impact of identity but also improves interclass variations and intraclass similarities. The joint training strategy is proposed to regularize the identity-related features. It can force our network to generate expression-guided identity-related features and suppresses negative identity factors in the same time. Experiments on three popular facial expression databases, including two popular posed facial expression databases and one spontaneous facial expression database, show that our IE-DBN outperforms most of the state of the arts, which demonstrates our superiority in facial expression recognition.},
  archive      = {J_TCDS},
  author       = {Haifeng Zhang and Wen Su and Jun Yu and Zengfu Wang},
  doi          = {10.1109/TCDS.2020.3034807},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {898-911},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Identity–Expression dual branch network for facial expression recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EEG emotion recognition based on 3-d feature representation
and dilated fully convolutional networks. <em>TCDS</em>, <em>13</em>(4),
885–897. (<a href="https://doi.org/10.1109/TCDS.2021.3051465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition involving high-dimensional electroencephalogram (EEG) data demands urgently for a way to learn robust and representative EEG features for final classification. In this article, a novel framework combining 3-D feature representation and dilated fully convolutional network (3DFR-DFCN) is proposed for EEG emotion recognition (EER). To excavate the prior knowledge, such as interchannel and interfrequency-band correlation information, 1-D feature sequences are extended into 2-D electrode meshes of different frequency bands. Then, the acquired electrode meshes under multiple activation patterns are further constructed into 3-D EEG arrays to capture their complementary information. To realize cross-band and cross-channel feature learning, a dilated fully convolutional network (DFCN) is built to process the input feature array, then the spectral norm regularization (SNR) item is introduced to reduce the sensitivity to the disturbed EEG features. Both subject-dependent and subject-independent experiments have conducted on DEAP and DREAMER data sets. An average accuracy of 94.59%/81.03%, 95.32%/79.91%, 94.78%/80.23% are, respectively, obtained for valence, arousal, and dominance classifications for two kinds of experiments on the DEAP data set. The integration of spatial information and frequency-band information is meaningful for assessment of human emotional states in practical or clinical applications.},
  archive      = {J_TCDS},
  author       = {Dongdong Li and Bing Chai and Zhe Wang and Hai Yang and Wenli Du},
  doi          = {10.1109/TCDS.2021.3051465},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {885-897},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG emotion recognition based on 3-D feature representation and dilated fully convolutional networks},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic assessment for severe self-reported depressive
symptoms using speech cues. <em>TCDS</em>, <em>13</em>(4), 875–884. (<a
href="https://doi.org/10.1109/TCDS.2020.3002512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with severe depressive symptoms are exposed to high suicidal risk. Since assessing depressive symptoms is either implicated by variability among subjects and clinicians or by time-consuming procedures, it is difficult to do objectively and effortlessly. There is a need for automatic approaches to estimate them. Previous investigations on speech-based severity’s categorical assignment predominantly focused on distinguishing depressive disorders from healthy. This article presents an automatic assessment system of severe self-reported depressive symptoms (SSDSs) to classify speakers with severe depressive symptoms from normal and not so severe ones. First, modulation-domain spectral centroid mean (MSCM) features characterizing severe depressive symptoms are extracted. Subsequently, aiming at the problems of limited and imbalanced training data, we establish variance-weighted sum-to- ${H}$ constraint ( ${H}$ is an adjustable parameter) collaborative representation-based classification (VWSC-CRC) method to exploit interclass separability between SSDS and non-SSDS. The proposed system (MSCM–VWSC-CRC) was evaluated on the small and imbalanced data set from AVEC2013 by dominant metrics, such as F1-score and area under the precision–recall curve (AU-PRC), as well as auxiliary indices, including specificity, recall (sensitivity), precision, accuracy, and area under the receiver operating characteristic curve (AU-ROC) as needed. The results exhibit a clear advantage over three important systems in the literature. The gains of this article are likely to lay the substantive groundwork to assist clinicians in automatically screening subjects with severe depressive symptoms so as to facilitate the diagnosis of the corresponding psychological disorders.},
  archive      = {J_TCDS},
  author       = {Jinfang Wang and Ke Lv and Chang Liu and Xinli Nie and Dhananjaya Gowda and Shuxin Luan},
  doi          = {10.1109/TCDS.2020.3002512},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {875-884},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automatic assessment for severe self-reported depressive symptoms using speech cues},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Person reidentification by multiscale feature representation
learning with random batch feature mask. <em>TCDS</em>, <em>13</em>(4),
865–874. (<a href="https://doi.org/10.1109/TCDS.2020.3003674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person reidentification (PReID) has received increasing attention due to its significant importance in intelligent video surveillance. However, most existing multiscale feature learning methods embed the multiscale feature extraction modules for PReID, which increases the complexity of the inference network and reduces the timeliness. Moreover, jointly using the small-scale and large-scale features to learn feature representations may weaken the local detailed features extraction and spatial information learning. Besides, some attentive local features are often suppressed when introducing the attention mechanisms for deep PReID models. To address these issues, a deep model with multiscale feature representation learning (MFRL) and random batch feature mask (RBFM) is proposed for PReID in this study. To ensure the feature representations discriminability and spatial information learning, two identity losses are adopted to supervise the small-scale and large-scale features learning in the MFRL module, respectively. To alleviate the situation of local attentive features being suppressed by using attention mechanisms, RBFM branch with random feature block dropping strategy which can learn the attentive local feature representations. The proposed methods are only performed in the training phase and discarded in the testing phase, thus, enhancing the effectiveness of the model. Our model achieves the state-of-the-art on the popular benchmark data sets, including Market-1501, DukeMTMC-reID, and CUHK03. Besides, we conduct a set of ablation experiments to verify the effectiveness of the proposed methods.},
  archive      = {J_TCDS},
  author       = {Yong Wu and Kun Zhang and Di Wu and Chao Wang and Chang-An Yuan and Xiao Qin and Tao Zhu and Yu-Chuan Du and Han-Li Wang and De-Shuang Huang},
  doi          = {10.1109/TCDS.2020.3003674},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {865-874},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Person reidentification by multiscale feature representation learning with random batch feature mask},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rapid learning of complex sequences with time constraints: A
dynamic neural field model. <em>TCDS</em>, <em>13</em>(4), 853–864. (<a
href="https://doi.org/10.1109/TCDS.2020.2991789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many of our sequential activities require that behaviors must be both precisely timed and put in the proper order. This article presents a neurocomputational model based on the theoretical framework of dynamic neural fields that supports the rapid learning and flexible adaptation of coupled order–timing representations of sequential events. A key assumption is that elapsed time is encoded in the monotonic buildup of self-stabilized neural population activity representing event memory. A stable activation gradient over subpopulations carries the information of an entire sequence. With robotics applications in mind, we test the model in simulations of learning by observation paradigm, in which the cognitive agent first memorizes the order and relative timing of observed events and, subsequently, recalls the information from memory taking potential speed constraints into account. Model robustness is tested by systematically varying sequence complexity along the temporal and the ordinal dimensions. Furthermore, an adaptation rule is proposed that allows the agent to adjust in a single trial a learned timing pattern to a changing temporal context. The simulation results are discussed with respect to our goal to endow autonomous robots with the capacity to efficiently learn complex sequences with time constraints, supporting more natural human–robot interactions.},
  archive      = {J_TCDS},
  author       = {Flora Ferreira and Weronika Wojtak and Emanuel Sousa and Luís Louro and Estela Bicho and Wolfram Erlhagen},
  doi          = {10.1109/TCDS.2020.2991789},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {853-864},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Rapid learning of complex sequences with time constraints: A dynamic neural field model},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep variational autoencoder for mapping functional brain
networks. <em>TCDS</em>, <em>13</em>(4), 841–852. (<a
href="https://doi.org/10.1109/TCDS.2020.3025137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the neuroimaging and brain mapping communities, researchers have proposed a variety of computational methods to map functional brain networks (FBNs). Recently, it has been proven that deep learning (DL) can be applied on functional magnetic resonance image (fMRI) data with superb representation power over the traditional machine learning methods. However, due to the lack of labeled data and the high dimension of fMRI volume images, DL suffers from overfitting in both supervised and unsupervised training processes. In this work, we proposed a novel generative model: deep variational autoencoder (DVAE) to tackle the challenge of insufficient data and incomplete supervision. The experimental results showed that the representations learned by DVAE are interpretable and meaningful compared to those learned from well-known sparse dictionary learning (SDL). Besides, the organization of some FBN patterns derived from different layers in DVAE was observed in a hierarchical fashion. Furthermore, we showed that DVAE has better performance on small dataset over autoencoder (AE). By using attention deficit hyperactivity disorder (ADHD)-200 dataset as our test bed, we constructed a DVAE-based modeling and classification pipeline in which all subjects’ functional connectivities estimated by FBNs were taken as input features to train a classifier. Finally, the results achieved by our pipeline reached state-of-the-art classification accuracies on three ADHD-200 sites compared with other fMRI-based methods.},
  archive      = {J_TCDS},
  author       = {Ning Qiang and Qinglin Dong and Fangfei Ge and Hongtao Liang and Bao Ge and Shu Zhang and Yifei Sun and Jie Gao and Tianming Liu},
  doi          = {10.1109/TCDS.2020.3025137},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {841-852},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep variational autoencoder for mapping functional brain networks},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural encoding for human visual cortex with deep neural
networks learning “what” and “where.” <em>TCDS</em>, <em>13</em>(4),
827–840. (<a href="https://doi.org/10.1109/TCDS.2020.3007761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural encoding, a crucial aspect to understand the human brain information processing system, aims to establish a quantitative relationship between the stimuli and the evoked brain activities. In the field of visual neuroscience, with the ability to explain how neurons in the primary visual cortex work, population receptive field (pRF) models have enjoyed high popularity and made reliable progress in recent years. However, existing models rely on either the inflexible prior assumptions about pRF or the clumsy parameter estimation methods, severely limiting the expressiveness and interpretability. In this article, we propose a novel neural encoding framework by learning “what” and “where” with deep neural networks. It involves two separate aspects: 1) the spatial characteristic (“where”) and 2) feature selection (“what”) of neuron populations in the visual cortex. Specifically, our approach first encodes visual stimuli into hierarchically intermediate features through a pretrained deep neural network (DNN), then converts DNN features into refined features with the channel attention and spatial receptive field (RF) to learn “where”, and finally regresses refined features simultaneously onto voxel activities to learn “what”. The sparsity regularization and smoothness regularization are adopted in our modeling approach so that the crucial RF can be estimated automatically without prior assumptions about shapes. Furthermore, an attempt is made to extend the voxel-wise modeling approach to multi-voxel joint encoding models, and we show that it is conducive to rescuing voxels with poor signal-to-noise characteristics. Extensive empirical results demonstrate that the method developed herein provides an effective strategy to establish neural encoding for the human visual cortex, with the weaker prior constraints but the higher encoding performance.},
  archive      = {J_TCDS},
  author       = {Haibao Wang and Lijie Huang and Changde Du and Dan Li and Bo Wang and Huiguang He},
  doi          = {10.1109/TCDS.2020.3007761},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {827-840},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neural encoding for human visual cortex with deep neural networks learning “What” and “Where”},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regulated morphogen gradients for target surrounding and
adaptive shape formation. <em>TCDS</em>, <em>13</em>(4), 818–826. (<a
href="https://doi.org/10.1109/TCDS.2020.2984087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In swarm robotics, developing algorithms for self-organizing minimalistic robots has become a popular research topic. Unlike others, minimalistic robots may not be able to self-localize themselves, making it very challenging to accomplish missions such as surrounding a target, whose position is typically unknown. In target surroundings, reaching a target and joining the swarm do not always lead to a satisfactory enclosure of the target. Furthermore, it is impossible for individual minimalistic robots to figure out what a global shape of the swarm should be without a collective decision making. In this article, we make use of diffusion and reaction of two morphogens for target surrounding and formation of a circular shape swarm. We show that the proposed method is able to adaptively form shapes surrounding multiple targets. Computer simulations and physical experiments using Kilobots are performed to assess the performance of the proposed algorithm.},
  archive      = {J_TCDS},
  author       = {Ataollah Ramezan Shirazi and Yaochu Jin},
  doi          = {10.1109/TCDS.2020.2984087},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {818-826},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Regulated morphogen gradients for target surrounding and adaptive shape formation},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimalistic attacks: How little it takes to fool deep
reinforcement learning policies. <em>TCDS</em>, <em>13</em>(4), 806–817.
(<a href="https://doi.org/10.1109/TCDS.2020.2974509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have revealed that neural-network-based policies can be easily fooled by adversarial examples. However, while most prior works analyze the effects of perturbing every pixel of every frame assuming white-box policy access, in this article, we take a more restrictive view toward adversary generation—with the goal of unveiling the limits of a model’s vulnerability. In particular, we explore minimalistic attacks by defining three key settings : 1) Black-Box Policy Access : where the attacker only has access to the input (state) and output (action probability) of an RL policy; 2) Fractional-State Adversary : where only several pixels are perturbed, with the extreme case being a single-pixel adversary; and 3) Tactically Chanced Attack : where only significant frames are tactically chosen to be attacked. We formulate the adversarial attack by accommodating the three key settings, and explore their potency on six Atari games by examining four fully trained state-of-the-art policies. In Breakout, for example, we surprisingly find that: 1) all policies showcase significant performance degradation by merely modifying 0.01% of the input state and 2) the policy trained by DQN is totally deceived by perturbing only 1% frames.},
  archive      = {J_TCDS},
  author       = {Xinghua Qu and Zhu Sun and Yew-Soon Ong and Abhishek Gupta and Pengfei Wei},
  doi          = {10.1109/TCDS.2020.2974509},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {806-817},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Minimalistic attacks: How little it takes to fool deep reinforcement learning policies},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot navigation in unseen spaces using an abstract map.
<em>TCDS</em>, <em>13</em>(4), 791–805. (<a
href="https://doi.org/10.1109/TCDS.2020.2993855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human navigation in built environments depends on symbolic spatial information which has unrealized potential to enhance robot navigation capabilities. Information sources, such as labels, signs, maps, planners, spoken directions, and navigational gestures communicate a wealth of spatial information to the navigators of built environments; a wealth of information that robots typically ignore. We present a robot navigation system that uses the same symbolic spatial information employed by humans to purposefully navigate in unseen built environments with a level of performance comparable to humans. The navigation system uses a novel data structure called the abstract map to imagine malleable spatial models for unseen spaces from spatial symbols. Sensorimotor perceptions from a robot are then employed to provide purposeful navigation to symbolic goal locations in the unseen environment. We show how a dynamic system can be used to create malleable spatial models for the abstract map, and provide an open-source implementation to encourage future work in the area of symbolic navigation. The symbolic navigation performance of humans and a robot is evaluated in a real-world built environment. This article concludes with a qualitative analysis of human navigation strategies, providing further insights into how the symbolic navigation capabilities of robots in unseen built environments can be improved in the future.},
  archive      = {J_TCDS},
  author       = {Ben Talbot and Feras Dayoub and Peter Corke and Gordon Wyeth},
  doi          = {10.1109/TCDS.2020.2993855},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {791-805},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robot navigation in unseen spaces using an abstract map},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semiautomatic behavioral change-point detection: A case
study analyzing children interactions with a social agent.
<em>TCDS</em>, <em>13</em>(4), 779–790. (<a
href="https://doi.org/10.1109/TCDS.2020.3023196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of human behaviors in cognitive sciences provides clues to understand and describe people’s personal and interpersonal functioning. In particular, the temporal analysis of behavioral dynamics can be a powerful tool to reveal events, correlations, and causalities but also to discover abnormal behaviors. However, the annotation of these dynamics can be expensive in terms of temporal and human resources. To tackle this challenge, this article proposes a methodology to semiautomatically annotate behavioral data. Behavioral dynamics can be expressed as sequences of simple dynamical processes: transitions between such processes are generally known as change points. This article describes the necessary steps to detect and classify change points in behavioral data by using a data set collected in a real use-case scenario. This data set includes motor observations from children with typical development and with neurodevelopmental disorders. Abnormal movements that are present in such disorders are useful to validate the system in conditions that are challenging even for experienced annotators. Results show that the system: can be effective in the semiautomated annotation task; can be efficient in presence of abnormal behaviors; may achieve good performance when trained with limited manually annotated data.},
  archive      = {J_TCDS},
  author       = {Vito Monteleone and Liliana Lo Presti and Marco La Cascia and Soizic Gauthier and Jean Xavier and Mohamed Zaoui and Alain Berthoz and David Cohen and Mohamed Chetouani and Salvatore Maria Anzalone},
  doi          = {10.1109/TCDS.2020.3023196},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {779-790},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Semiautomatic behavioral change-point detection: A case study analyzing children interactions with a social agent},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing EEG large-scale cortical functional network
connectivity based on brain atlas by s estimator. <em>TCDS</em>,
<em>13</em>(4), 769–778. (<a
href="https://doi.org/10.1109/TCDS.2020.2991414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale functional network connectivity (FNC) reveals the neural substrate of the cognitive process on a large-scale level. Electroencephalogram (EEG) is cost effective, portable, and noninvasive and is capable of capturing brain activities at a millisecond scale. Brain atlas derived from the anatomy clearly defines reliable functional subnetworks. In this article, we proposed to construct robust EEG FNC based on brain atlas by combining EEG source imaging with multivariate synchronization analysis to mine the brain’s large-scale information exchange. We evaluated the performances of two typical methods, canonical correlation analysis (CCA) and ${S}$ estimator, in quantifying the couplings among subnetworks by both simulation and application to real EEG data set. Simulation demonstrated that, compared to CCA, ${S}$ estimator shows high robustness and adaptability to low signal-to-noise ratio (SNR) and short length data. According to the FNC of P300, we further found that the FNC network constructed by ${S}$ estimator may be more consistent with the physiological mechanisms of P300 generation, where the ${S}$ estimator-based approach emphasizes the important role of the cerebellar network that has been proved to be involved in attention-related cognition tasks. This article provides a new tool to probe information processing during the cognition process at a higher hierarchal level.},
  archive      = {J_TCDS},
  author       = {Chanlin Yi and Chunli Chen and Lin Jiang and Qin Tao and Fali Li and Yajing Si and Tao Zhang and Dezhong Yao and Peng Xu},
  doi          = {10.1109/TCDS.2020.2991414},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {769-778},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Constructing EEG large-scale cortical functional network connectivity based on brain atlas by s estimator},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Morphological development in robotic learning: A survey.
<em>TCDS</em>, <em>13</em>(4), 750–768. (<a
href="https://doi.org/10.1109/TCDS.2021.3052548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans and animals undergo morphological development (MD) processes from infancy to adulthood that have been shown to facilitate learning. However, most of the work on developmental robotics (DRs) considers fixed morphologies, addressing only the development of the cognitive system of the robots. This article aims to provide a survey of the work that is being carried out within the relatively new field of MD in robots. In particular, it contemplates MD as the changes that occur in the properties of the joints, links and sensors of a robot during its lifetime and focuses on the work carried out by different authors to try to determine their influence on robot learning. To this end, walking, reaching, grasping and vocalization have been identified as the four most representative tasks addressed in the field, clustering the work of the different authors around them. The approach followed is multidisciplinary, discussing the relationships among DRs, embodied artificial intelligence and developmental psychology in humans in general, as well as for each of the tasks, and providing an overview of the many avenues of research that are still open in this field.},
  archive      = {J_TCDS},
  author       = {Martín Naya-Varela and Andrés Faíña and Richard J. Duro},
  doi          = {10.1109/TCDS.2021.3052548},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {750-768},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Morphological development in robotic learning: A survey},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey on neuromarketing using EEG signals. <em>TCDS</em>,
<em>13</em>(4), 732–749. (<a
href="https://doi.org/10.1109/TCDS.2021.3065200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromarketing is the application of neuroscience to the understanding of consumer preferences toward products and services. As such, it studies the neural activity associated with preference and purchase intent. Neuromarketing is considered an emerging area of research, driven in part by the approximately 400 billion dollars spent annually on advertisement and promotion. Given the size of this market, even a slight improvement in performance can have an immense impact. Traditional approaches to marketing consider a posteriori user feedback in the form of questionnaires, product ratings, or review comments, but these approaches do not fully capture or explain the real-time decision-making process of consumers. Various physiological measurement techniques have been proposed to facilitate the recording of this crucial aspect of the decision-making process, including brain imaging techniques [functional magnetic resonance imaging (fMRI), electroencephalography (EEG), steady state topography (SST)], and various biometric sensors. The use of EEG in neuromarketing is especially promising. EEG detects the sequential changes of brain activity, without appreciable time delay, needed to assess both the unconscious reaction and sensory reaction of the customer. Several types of EEG devices are now available in the market, each with its own advantages and disadvantages. Researchers have conducted experiments using many of these devices, across different age groups and different categories of products. Because of the deep insights that can be gained, the field of neuromarketing research is carefully monitored by consumer and research protection groups to ensure that subjects are properly protected. This article surveys a range of considerations for EEG-based neuromarketing strategies, including the types of information that can be gathered, how marketing stimuli are presented to consumers, how such strategies may affect the consumer in terms of appeal and memory, machine learning techniques applied in the field, and the variety of challenges faced, including ethics, in this emerging field.},
  archive      = {J_TCDS},
  author       = {Vaishali Khurana and Monika Gahalawat and Pradeep Kumar and Partha Pratim Roy and Debi Prosad Dogra and Erik Scheme and Mohammad Soleymani},
  doi          = {10.1109/TCDS.2021.3065200},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {12},
  number       = {4},
  pages        = {732-749},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A survey on neuromarketing using EEG signals},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). IEEE computational intelligence society information.
<em>TCDS</em>, <em>13</em>(3), C3. (<a
href="https://doi.org/10.1109/TCDS.2021.3107131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2021.3107131},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explanation as a social practice: Toward a conceptual
framework for the social design of AI systems. <em>TCDS</em>,
<em>13</em>(3), 717–728. (<a
href="https://doi.org/10.1109/TCDS.2020.3044366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent surge of interest in explainability in artificial intelligence (XAI) is propelled by not only technological advancements in machine learning but also by regulatory initiatives to foster transparency in algorithmic decision making. In this article, we revise the current concept of explainability and identify three limitations: passive explainee, narrow view on the social process, and undifferentiated assessment of explainee’s understanding. In order to overcome these limitations, we present explanation as a social practice in which explainer and explainee co-construct understanding on the microlevel. We view the co-construction on a microlevel as embedded into a macrolevel, yielding expectations concerning, e.g., social roles or partner models: typically, the role of the explainer is to provide an explanation and to adapt it to the current level of explainee’s understanding; the explainee, in turn, is expected to provide cues that direct the explainer. Building on explanations being a social practice, we present a conceptual framework that aims to guide future research in XAI. The framework relies on the key concepts of monitoring and scaffolding to capture the development of interaction. We relate our conceptual framework and our new perspective on explaining to transparency and autonomy as objectives considered for XAI.},
  archive      = {J_TCDS},
  author       = {Katharina J. Rohlfing and Philipp Cimiano and Ingrid Scharlau and Tobias Matzner and Heike M. Buhl and Hendrik Buschmeier and Elena Esposito and Angela Grimminger and Barbara Hammer and Reinhold Häb-Umbach and Ilona Horwath and Eyke Hüllermeier and Friederike Kern and Stefan Kopp and Kirsten Thommes and Axel-Cyrille Ngonga Ngomo and Carsten Schulte and Henning Wachsmuth and Petra Wagner and Britta Wrede},
  doi          = {10.1109/TCDS.2020.3044366},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {717-728},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Explanation as a social practice: Toward a conceptual framework for the social design of AI systems},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep multibranch fusion residual network for insect pest
recognition. <em>TCDS</em>, <em>13</em>(3), 705–716. (<a
href="https://doi.org/10.1109/TCDS.2020.2993060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earlier insect pest recognition is one of the critical factors for agricultural yield. Thus, an effective method to recognize the category of insect pests has become significant issues in the agricultural field. In this article, we proposed a new residual block to learn multiscale representation. In each block, it contains three branches: one is parameter-free, and the others contain several successive convolution layers. Moreover, we proposed a module and embedded it into the new residual block to recalibrate the channelwise feature response and to model the relationship of the three branches. By stacking this kind of block, we constructed the deep multibranch fusion residual network (DMF-ResNet). For evaluating the model performance, we first test our model on CIFAR-10 and CIFAR-100 benchmark data sets. The experimental results show that DMF-ResNet outperforms the baseline models significantly. Then, we construct DMF-ResNet with different depths for high-resolution image classification tasks and apply it to recognize insect pests. We evaluate the model performance on the IP102 data set, and the experimental results show that DMF-ResNet could achieve the best accuracy performance than the baseline models and other state-of-the-art methods. Based on these empirical experiments, we demonstrate the effectiveness of our approach.},
  archive      = {J_TCDS},
  author       = {Wenjie Liu and Guoqing Wu and Fuji Ren},
  doi          = {10.1109/TCDS.2020.2993060},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {705-716},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep multibranch fusion residual network for insect pest recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep echo state network with multiple adaptive reservoirs
for time series prediction. <em>TCDS</em>, <em>13</em>(3), 693–704. (<a
href="https://doi.org/10.1109/TCDS.2021.3062177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, considering the plasticity of reservoir, a deep echo state network with multiple adaptive reservoirs in series configuration, called MAR-DESN, is proposed for time series prediction. First, according to the characteristics of input signals and reservoir states, the number of reservoirs of MAR-DESN and each reservoir size can be automatically determined by using the principal component analysis. Second, a parameter optimization method based on Broyden–Fletcher–Goldfarb–Shanno quasi-Newton algorithm is given to optimize the reservoir parameters of MAR-DESN. Third, a sufficient condition for the uniform echo state property of MAR-DESN is given, such that the MAR-DESN can be stably applied in different applications. Finally, three examples are used to verify the effectiveness of MAR-DESN.},
  archive      = {J_TCDS},
  author       = {Zhanshan Wang and Xianshuang Yao and Zhanjun Huang and Lei Liu},
  doi          = {10.1109/TCDS.2021.3062177},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {693-704},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep echo state network with multiple adaptive reservoirs for time series prediction},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal deep learning framework for image popularity
prediction on social media. <em>TCDS</em>, <em>13</em>(3), 679–692. (<a
href="https://doi.org/10.1109/TCDS.2020.3036690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Billions of photos are uploaded to the Web daily through various types of social networks. Some of these images receive millions of views and become popular, whereas others remain completely unnoticed. This raises the problem of predicting image popularity on social media. The popularity of an image can be affected by several factors, such as visual content, aesthetic quality, user, post metadata, and time. Thus, considering all these factors is essential for accurately predicting image popularity. In addition, the efficiency of the predictive model also plays a crucial role. In this study, motivated by multimodal learning, which uses information from various modalities, and the current success of convolutional neural networks (CNNs) in various fields, we propose a deep learning model, called visual-social CNN (VSCNN), which predicts the popularity of a posted image by incorporating various types of visual and social features into a unified network model. VSCNN first learns to extract high-level representations from the input visual and social features by utilizing two individual CNNs. The outputs of these two networks are then fused into a joint network to estimate the popularity score in the output layer. We assess the performance of the proposed method by conducting extensive experiments on a data set of approximately 432K images posted on Flickr. The simulation results demonstrate that the proposed VSCNN model significantly outperforms state-of-the-art models, with a relative improvement of greater than 2.33%, 7.59%, and 14.16% in terms of Spearman&#39;s Rho, mean absolute error, and mean-squared error, respectively.},
  archive      = {J_TCDS},
  author       = {Fatma S. Abousaleh and Wen-Huang Cheng and Neng-Hao Yu and Yu Tsao},
  doi          = {10.1109/TCDS.2020.3036690},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {679-692},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal deep learning framework for image popularity prediction on social media},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Driving fatigue recognition with functional connectivity
based on phase synchronization. <em>TCDS</em>, <em>13</em>(3), 668–678.
(<a href="https://doi.org/10.1109/TCDS.2020.2985539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accumulating evidences showed that the optimal brain network topology was altered with the progression of fatigue during car driving. However, the extent of the discriminative power of functional connectivity that contributes to driving fatigue detection is still unclear. In this article, we extracted two types of features (network properties and critical connections) to explore their usefulness in driving fatigue detection. EEG data were recorded twice from twenty healthy subjects during a simulated driving experiment. Multiband functional connectivity matrices were established using the phase lag index, which serve as input for the following graph theoretical analysis and critical connections determination between the most vigilant and fatigued states. We found a reorganization of a brain network toward less efficient architecture in fatigue state across all frequency bands. Further interrogations showed that the discriminative connections were mainly connected to frontal areas, i.e., most of the increased connections are from frontal pole to parietal or occipital regions. Moreover, we achieved a satisfactory classification accuracy (96.76%) using the discriminative connection features in β band. This article demonstrated that graph theoretical properties and critical connections are of discriminative power for manifesting fatigue alterations and the critical connection is an efficient feature for driving fatigue detection.},
  archive      = {J_TCDS},
  author       = {Hongtao Wang and Xucheng Liu and Junhua Li and Tao Xu and Anastasios Bezerianos and Yu Sun and Feng Wan},
  doi          = {10.1109/TCDS.2020.2985539},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {668-678},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Driving fatigue recognition with functional connectivity based on phase synchronization},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time multifunctional framework for guidewire
morphological and positional analysis in interventional x-ray
fluoroscopy. <em>TCDS</em>, <em>13</em>(3), 657–667. (<a
href="https://doi.org/10.1109/TCDS.2020.3023952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In endovascular and cardiovascular surgery, real-time guidewire morphological and positional analysis is an important prerequisite for robot-assisted intervention, which can aid in reducing the radiation dose, contrast agent, and procedure time. Nevertheless, this task often comes with the challenge of the deformable elongated structure with low contrast in noisy X-ray fluoroscopy. In this article, a real-time multifunctional framework is proposed for fully automatic guidewire morphological and positional analysis, namely, guidewire segmentation, endpoint localization, and angle measurement. In the first stage, the proposed fast attention recurrent network (FAR-Net) achieves real-time and accurate guidewire segmentation. In the second stage, the endpoint localization and angle measurement algorithm robustly obtain subpixel-level endpoint and angle of the guidewire tip. Quantitative and qualitative evaluations on the MSGSeg data set consisting of 180 X-ray sequences from 30 patients demonstrate that the proposed framework significantly outperforms simpler baselines as well as the best previously published result for this task. The proposed approach reached F 1 -Score of 0.938, mean distance error of 0.596 pixels, endpoint localization and angle measurement accuracy of 97.8% and 95.3%, and an inference rate of approximately 13 FPS. The proposed framework not only addresses the issues of extreme class imbalance and misclassified examples but also meets the real-time requirements, achieving state-of-the-art performance. The proposed approach is promising for integration into robotic navigation frameworks to various intravascular applications, enabling robotic-assisted intervention.},
  archive      = {J_TCDS},
  author       = {Yan-Jie Zhou and Xiao-Liang Xie and Xiao-Hu Zhou and Shi-Qi Liu and Gui-Bin Bian and Zeng-Guang Hou},
  doi          = {10.1109/TCDS.2020.3023952},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {657-667},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A real-time multifunctional framework for guidewire morphological and positional analysis in interventional X-ray fluoroscopy},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multilayer neural network merging image preprocessing and
pattern recognition by integrating diffusion and drift memristors.
<em>TCDS</em>, <em>13</em>(3), 645–656. (<a
href="https://doi.org/10.1109/TCDS.2020.3003377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of research on novel memristor model and device, neural networks by integrating various memristor models have become a hot research topic recently. However, state-of-the-art works still build such neural networks using drift memristor only. Furthermore, some other related works are only applied to a few individual applications, including pattern recognition and edge detection. In this article, a novel kind of multilayer neural network is proposed, in which diffusion and drift memristor models are applied to construct a system merging image preprocessing and pattern recognition. Specifically, the entire network consists of two diffusion memristive cellular layers for image preprocessing and one drift memristive feedforward layer for pattern recognition. The experimental results show that good recognition accuracy of noisy MNIST is obtained due to the fusion of image preprocessing and pattern recognition. Moreover, owing to high-efficiency in-memory computing and brief spiking encoding methods, high processing speed, high throughput, and few hardware resources of the entire network are achieved.},
  archive      = {J_TCDS},
  author       = {Zhiri Tang and Ruohua Zhu and Ruihan Hu and Yanhua Chen and Edmond Q. Wu and Hao Wang and Jin He and Qijun Huang and Sheng Chang},
  doi          = {10.1109/TCDS.2020.3003377},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {645-656},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A multilayer neural network merging image preprocessing and pattern recognition by integrating diffusion and drift memristors},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN-g: Convolutional neural network combined with graph for
image segmentation with theoretical analysis. <em>TCDS</em>,
<em>13</em>(3), 631–644. (<a
href="https://doi.org/10.1109/TCDS.2020.2998497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network (CNN), although recognized to be considerably successful in its application to semantic segmentation, is inadequate for extracting the overall structure information, for its representing images with the data in the Euclidean space. To improve this inadequacy, a new model in the graph domain that transforms semantic segmentation into graph node classification is proposed for semantic segmentation. In this model, the image is represented by a graph, with its nodes initialized by the feature map obtained by a CNN, and its edges reflecting the relationships of the nodes. The node relationships that are taken into consideration include distance-based ones and semantic ones, respectively, calculated with the Gauss kernel function and attention mechanism. The graph neural network is also introduced in this model for the classification of graph nodes, which can expand the receptive field without the loss of location information and combine the structure with the feature extraction. Most importantly, it is theoretically concluded that the proposed graph model takes the same role as a Laplace regularization term in image segmentation, which has been proven by multiple comparative experiments that show the effectiveness of the model in image semantic segmentation. The learned attention is visualized by the heatmap to validate the structure learning ability of our model. The results of these experiments show the importance of structural information in image segmentation. Hence, an idea of deep learning combined with graph structural information is provided in theory and method.},
  archive      = {J_TCDS},
  author       = {Yi Lu and Yaran Chen and Dongbin Zhao and Bao Liu and Zhichao Lai and Jianxin Chen},
  doi          = {10.1109/TCDS.2020.2998497},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {631-644},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CNN-G: Convolutional neural network combined with graph for image segmentation with theoretical analysis},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Faster single model vigilance detection based on deep
learning. <em>TCDS</em>, <em>13</em>(3), 621–630. (<a
href="https://doi.org/10.1109/TCDS.2019.2963073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various reports have shown that the rate of road traffic accidents has increased due to reduced driver vigilance. Therefore, an accurate estimation of the driver&#39;s alertness status plays an important part. To estimate vigilance, we adopt a novel strategy that is a deep autoencoder with subnetwork nodes (DAE SN ). The proposed network model is designed not only for sparse representation but also for dimension reduction. Some hidden layers are not calculated by randomly acquired, but by replacement technologies. Unlike the traditional electrooculogram (EOG) signals, the forehead EOG (EOG F ) signals are collected through forehead electrodes that do not have to surround the eyes, which has a convenient and effective practical application. The root-mean-square error (RMSE) and correlation coefficient (COR) while separately using three EOG F features improved to 0.11/0.79, 0.10/0.83, and 0.11/0.80, respectively. Implemented in an experimental environment, percentage of eye closure over time is calculated in real time through SMI eye-tracking-glasses, up to 120 frames/s. In addition, the time to extract features from the raw signal and display the prediction is only 34 ms, that is the level of the driver&#39;s fatigue can be detected quickly. The experimental study shows that the proposed model for vigilance analysis has better robustness and learning capability.},
  archive      = {J_TCDS},
  author       = {Wei Wu and Wei Sun and Q. M. Jonathan Wu and Cheng Zhang and Yimin Yang and Hongshan Yu and Bao-Liang Lu},
  doi          = {10.1109/TCDS.2019.2963073},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {621-630},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Faster single model vigilance detection based on deep learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the generalization ability of deep neural networks
for cross-domain visual recognition. <em>TCDS</em>, <em>13</em>(3),
607–620. (<a href="https://doi.org/10.1109/TCDS.2020.2965166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature learning with deep neural networks (DNNs) has made remarkable progress in recent years. However, its data-driven nature makes the collection of labeled training data expensive or impossible when the testing domain changes. Here, we propose a method of transferable feature learning and instance-level adaptation to improve the generalization ability of DNNs so as to mitigate the domain shift challenge for cross-domain visual recognition. When less labeled information is available, our proposed method shows attractive results in the new target domain and outperforms the typical fine-tuning method. Two DNNs are chosen as the representatives working with our proposed method, to do a comprehensive study about the generalization ability on the tasks of image-to-image transfer, image-to-video transfer, multidomain image classification, and weakly supervised detection. The experimental results show that our proposed method is superior to other existing works in the literature. In addition, a large scale of cross-domain database is merged from three different domains, providing a quantitative platform to evaluate different approaches in the field of cross-domain object detection.},
  archive      = {J_TCDS},
  author       = {Jianwei Zheng and Chao Lu and Cong Hao and Deming Chen and Donghui Guo},
  doi          = {10.1109/TCDS.2020.2965166},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {607-620},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Improving the generalization ability of deep neural networks for cross-domain visual recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatiotemporal dynamical analysis of brain activity during
mental fatigue process. <em>TCDS</em>, <em>13</em>(3), 593–606. (<a
href="https://doi.org/10.1109/TCDS.2020.2976610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental fatigue is a common phenomenon with implicit and multidimensional properties. It brings dynamic changes in functional brain networks. However, the challenging problem of false positives appears when the connectivity is estimated by electroencephalography (EEG). In this article, we propose a novel framework based on spatial clustering to explore the sources of mental fatigue and functional activity changes caused by them. To suppress false positive observations, spatial clustering is implemented in brain networks. The nodes extracted by spatial clustering are registered back to the functional magnetic resonance imaging (fMRI) source space to determine the sources of mental fatigue. The wavelet entropy of EEG in a sliding window is calculated to find the temporal features of mental fatigue. Our experimental results show that the extracted nodes correspond to the fMRI sources across different subjects and different tasks. The entropy values on the extracted nodes demonstrate clearer staged decreasing changes (deactivation). Additionally, the synchronization among the extracted nodes is stronger than that among all the nodes in the deactivation stage. The initial time of the strong synchronized deactivation is consistent with the subjective fatigue time reported by the subjects themselves. It means the synchronization and deactivation correspond to the subjective feelings of fatigue. Therefore, this functional activity pattern may be caused by sources of mental fatigue. The proposed framework is useful for a wide range of prolonged functional imaging and fatigue detection studies.},
  archive      = {J_TCDS},
  author       = {Chi Zhang and Lina Sun and Fengyu Cong and Tapani Ristaniemi},
  doi          = {10.1109/TCDS.2020.2976610},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {593-606},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatiotemporal dynamical analysis of brain activity during mental fatigue process},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The growing from adolescence to adulthood influences the
decision strategy to unfair situations. <em>TCDS</em>, <em>13</em>(3),
586–592. (<a href="https://doi.org/10.1109/TCDS.2020.2981512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerged evidence has uncovered the differences in cognition and decision characteristics between adolescents and adults. Nevertheless, the neural mechanism that accounts for the differences between the two groups is still left unveiled. Using the event-related potential (ERP) and functional brain networks, the present study investigated the neural mechanism underpinning decision-making differences between adults and adolescents in the ultimatum game (UG). The results revealed that despite a higher medial frontal negativity (MFN) in response to unfair offers as compared to fair ones in adults, no difference was found in adolescents. Comparing the MFN with N1, one of the early ERP components, we found a larger MFN in adults but an opposite tendency in adolescents. The network analysis revealed that when responding to the unfair offers, the enhanced linkages are observed to be locally confined in the occipital cortex for adolescents, while the increased long-range linkages between the prefrontal and occipital areas could be found for adults. Taken together, our findings suggested that adolescents tend to the interest of offers while the adults prefer the rule of fairness in the context of unfairness. The social experience for the fairness establishment reshapes the two groups with different brain activities.},
  archive      = {J_TCDS},
  author       = {Yajing Si and Fali Li and Fang Li and Jingwei Tu and Chanlin Yi and Qin Tao and Xiabing Zhang and Changfu Pei and Shan Gao and Dezhong Yao and Peng Xu},
  doi          = {10.1109/TCDS.2020.2981512},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {586-592},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The growing from adolescence to adulthood influences the decision strategy to unfair situations},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting fatigue status of pilots based on deep learning
network using EEG signals. <em>TCDS</em>, <em>13</em>(3), 575–585. (<a
href="https://doi.org/10.1109/TCDS.2019.2963476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a solution for fatigue recognition through a new deep learning model that has a characteristic input of the power spectrum of an electroencephalogram (EEG) signal. First, four rhythms are obtained through the designed FIR filters, and the curve areas of their power spectrum density are coupled into four fatigue indicators. Second, a deep sparse contractive autoencoder network is proposed to learn more local fatigue characteristics, and the recognition results of pilots mental fatigue status are given. Compared with the state-of-the-art models, the results show that our model has good learning performance in extracting local features and fatigue status detection.},
  archive      = {J_TCDS},
  author       = {Edmond Q. wu and Ping-Yu Deng and Xu-Yi Qiu and Zhiri Tang and Wen-Ming Zhang and Li-Min Zhu and He Ren and Gui-Rong Zhou and Richard S. F. Sheng},
  doi          = {10.1109/TCDS.2019.2963476},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {575-585},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Detecting fatigue status of pilots based on deep learning network using EEG signals},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Increase in brain effective connectivity in multitasking but
not in a high-fatigue state. <em>TCDS</em>, <em>13</em>(3), 566–574. (<a
href="https://doi.org/10.1109/TCDS.2020.2990898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitasking has become omnipresent in daily activities and increased brain connectivity under high workload conditions has been reported. Moreover, the effect of fatigue on neural activity has been shown in participants performing cognitive tasks, but the effect of fatigue on different cognitive workload conditions is unclear. In this article, we investigated the effect of fatigue on changes in effective connectivity (EC) across the brain network under distinctive workload conditions. There were 133 electroencephalography (EEG) data sets collected from 16 participants over a five-month study, in which high-risk, reduced, and normal states of real-world fatigue were identified through a daily sampling system. The participants were required to perform a lane-keeping task (LKT) with/without multimodal dynamic attention-shifting (DAS) tasks. The results show that the EC magnitude is positively correlated with the increased workload in normal and reduced states. However, low EC was discovered in the high-risk state under high workload conditions. To the best of our knowledge, this investigation is the first EEG-based longitudinal study of real-world fatigue under multitasking conditions. These results could be beneficial for real-life applications, and adaptive models are essential for monitoring important brain patterns under varying workload demands and fatigue states.},
  archive      = {J_TCDS},
  author       = {Tien-Thong Nguyen Do and Yu-Kai Wang and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2020.2990898},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {566-574},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Increase in brain effective connectivity in multitasking but not in a high-fatigue state},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep convolutional network based on interleaved fusion
group. <em>TCDS</em>, <em>13</em>(3), 555–565. (<a
href="https://doi.org/10.1109/TCDS.2020.2974322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that the classification accuracy of the deep convolutional network can be remarkably improved by increasing its depth and width. However, as the network size increases, the number of network parameters will increase significantly, which results in network redundancy and performance degradation. In order to reduce network redundancy and improve classification accuracy by means of reducing the number of parameters, a highly modularized and lightweight deep interleaved fusion group convolutional network is proposed. The main idea is to design a more efficient “network computing manner,” so as to simplify the network model, reduce redundancy, and improve computational efficiency without the loss of classification accuracy. First, a template block is constructed by using the stacking and split-transform-merge strategies, which simplifies the network model by reducing the number of parameters. Then, the introduced group convolution and structured sparse convolution further simplify the network model and improve the computational efficiency. Experiments on the standard image recognition tasks have shown that the proposed convolutional network achieves better classification performance and has superior generalization ability compared with state-of-the-art deep networks.},
  archive      = {J_TCDS},
  author       = {Enhui Lv and Yuhu Cheng and Xuesong Wang and C. L. Philip Chen},
  doi          = {10.1109/TCDS.2020.2974322},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {555-565},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep convolutional network based on interleaved fusion group},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural correlates of interobserver visual congruency in
free-viewing condition. <em>TCDS</em>, <em>13</em>(3), 546–554. (<a
href="https://doi.org/10.1109/TCDS.2020.3002765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic visual scenes lead to varying consistency of observers’ eye movements, reflecting one intrinsic characteristic of visual scenes, termed interobserver visual congruency (IOVC). However, the neural correlates underlying IOVC are largely unknown especially in the free-viewing condition. In this study, we explored the neural correlates of IOVC using functional magnetic resonance imaging (fMRI) and eye-tracking data acquired in a naturalistic paradigm. Specifically, we estimated IOVC of movie shots from eye-tracking data and then conducted two statistical analyses for functional inference, including a hypothesis-driven analyses [general linear model (GLM)] and a data-driven approach [intersubject correlation (ISC)]. The GLM analysis demonstrated that IOVC recruited two distinctive streams of neural systems. Specifically, neural activities in superior temporal gyrus, default mode network, and hippocampus were positively correlated with IOVC, whereas those in the primary and secondary visual cortices, as well as the dorsal attention network exhibited negative correlations. Further ISC analysis revealed that movie shots with higher IOVC evoked more synchronous brain activities in the primary auditory cortex, primary visual cortex, and superior parietal lobule compared to those shots with lower IOVC. This study provides some novel evidence of visual processing in the human brain in the free-viewing condition.},
  archive      = {J_TCDS},
  author       = {Huan Liu and Xintao Hu and Yudan Ren and Liting Wang and Lei Guo and Christine Cong Guo and Junwei Han},
  doi          = {10.1109/TCDS.2020.3002765},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {546-554},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neural correlates of interobserver visual congruency in free-viewing condition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end mammogram diagnosis: A new multi-instance and
multiscale method based on single-image feature. <em>TCDS</em>,
<em>13</em>(3), 535–545. (<a
href="https://doi.org/10.1109/TCDS.2019.2963682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography is the most common modality used in breast cancer detection. Most diagnostic mammography studies, however, are based on single-image training with little attention to the fact that the size of breast lesions varies significantly and the overall condition of the breast from different views. Therefore, the methodology is not in accord with the clinical requirement. To overcome this problem, we propose a new end-to-end method for mammographic diagnosis. As part of this process, we construct a data set of patients from West China Hospital to validate the new method. Furthermore, a multiscale module is proposed for the acquisition of complex breast features in a single image, enabling the screening of unique features in variably sized lesions. Finally, a multi-instance module is proposed for realistic hospital requirements to calculate the contribution of each mammogram in reaching the final diagnosis. Guidance by the single-image features can ameliorate the problem of weak one-case labeling. The new method yielded both a public data set and a realistic hospital data set.},
  archive      = {J_TCDS},
  author       = {Zizhou Wang and Lei Zhang and Xin Shu and Qing Lv and Zhang Yi},
  doi          = {10.1109/TCDS.2019.2963682},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {535-545},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An end-to-end mammogram diagnosis: A new multi-instance and multiscale method based on single-image feature},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding the role of objects in joint attention task
framework for children with autism. <em>TCDS</em>, <em>13</em>(3),
524–534. (<a href="https://doi.org/10.1109/TCDS.2020.2983333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint Attention (JA) refers to the triadic relationship between two individuals with a common target of interest in their shared visual space. JA is one of the core deficits in children with autism spectrum disorder (ASD), which adversely affects their overall socio-communicative development. Researchers have focused on the JA skill training using various approaches where the mediator/facilitator provides the JA cues toward a target of interest. Research studies have used different objects as targets of interest in JA skill training. However, the role of the objects for JA skill training has often been neglected. Objects in our environment aid in mediating the social relationships between children and their social partners and establishing socio-communicative relations with the environment. In this study, we have designed a virtual environment with virtual objects to understand the role of objects in JA skill training for individuals with ASD. We have created a database of virtual objects and projected these objects in a computer-based JA task framework. Based on a survey, we selected objects that were preferred by these children, with the rest being comparatively less preferred. Our experimental study involves presenting the designed virtual objects in a randomized manner in the computer-based JA task trials. Results of this study with a group of 15 children with ASD were promising. The results of this study indicate differentiated implications of preferred objects presented as target and non-target on the task performance and looking pattern of these children.},
  archive      = {J_TCDS},
  author       = {Vishav Jyoti and Sanika Gupta and Uttama Lahiri},
  doi          = {10.1109/TCDS.2020.2983333},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {524-534},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Understanding the role of objects in joint attention task framework for children with autism},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep spiking neural networks with binary weights for object
recognition. <em>TCDS</em>, <em>13</em>(3), 514–523. (<a
href="https://doi.org/10.1109/TCDS.2020.2971655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have shown great potential as a solution for realizing ultralow-power consumption on neuromorphic hardware, but obtaining deep SNNs is still a challenging problem. Existing network conversion methods can effectively obtain SNNs from the trained convolutional neural networks (CNNs) with little performance loss, however, high-precision weights in the converted SNNs would take up high-storage space nonamicable to limited memory resources. To tackle this problem, we analyze the relationship between weights and thresholds of spiking neurons and propose an efficient weights-threshold balance conversion method to obtain SNNs with binary weights, resulting in a significant memory storage reduction. The experimental results evaluated with various network structures on benchmark data sets show that the binary SNN not only needs much less memory resources compared to its high-precision counterpart but also achieves the high-recognition accuracy comparable to other state-of-the-art SNNs.},
  archive      = {J_TCDS},
  author       = {Yixuan Wang and Yang Xu and Rui Yan and Huajin Tang},
  doi          = {10.1109/TCDS.2020.2971655},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {514-523},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Deep spiking neural networks with binary weights for object recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bioinspired visual-integrated model for multilabel
classification of textile defect images. <em>TCDS</em>, <em>13</em>(3),
503–513. (<a href="https://doi.org/10.1109/TCDS.2020.2977974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern textile industrial processes, textile defect recognition and classification are of vital importance for textile quality control. Recently, as the critical machine-learning method, the deep convolutional neural network (CNN) has shown superior performance in the single-label classification of textile defect images. However, accurately recognizing and classifying multilabel textile defect images are still an unsolved issue due to the complexity of intersected defects, as well as difficulty in distinguishing small-size defects and concerning the correlations amongst labels. To address these challenges, we propose a bioinspired visual-integrated model for multilabel classification of textile defects called BIVI-ML. Three bioinspired visual mechanisms (the visual gain mechanism, the visual attention mechanism, and the visual memory mechanism) are proposed and built within the BIVI-ML to: 1) enhance the resolution and feature discrimination; 2) attend to the textile defect; and 3) associate relevant labels. To evaluate the proposed method, a unique multilabel textile defect database is built as the benchmark for the multilabel classification of textile defect images. Compared with both the traditional and the state-of-the-art deep-learning classification methods, the proposed BIVI-ML achieves the best performance in both single-label and multilabel image classification.},
  archive      = {J_TCDS},
  author       = {Bing Wei and Kuangrong Hao and Lei Gao and Xue-Song Tang},
  doi          = {10.1109/TCDS.2020.2977974},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {503-513},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bioinspired visual-integrated model for multilabel classification of textile defect images},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based video hashing for large-scale video
retrieval. <em>TCDS</em>, <em>13</em>(3), 491–502. (<a
href="https://doi.org/10.1109/TCDS.2019.2963339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale video retrieval is a challenging problem because of the exponential growth of video collections on the Internet. To address this challenge, we propose an attention-based video hashing (AVH) method for large-scale video retrieval. Unlike most of the existing video hashing methods, which consider different frames within a video separately for hash learning, we use a convolutional neural network and long short-term memory (LSTM) network as the backbone to learn compact and discriminative hash codes by exploiting the structural information among different frames. To better capture informative clues in the video, an attention mechanism is added into the backbone, which can assign different weights to different LSTM time steps. Experiments were conducted to evaluate the proposed AVH method in comparison with existing methods. The experimental results on two widely used data sets show that our method outperforms existing state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Yingxin Wang and Xiushan Nie and Yang Shi and Xin Zhou and Yilong Yin},
  doi          = {10.1109/TCDS.2019.2963339},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {491-502},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Attention-based video hashing for large-scale video retrieval},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised hyperalignment for multisubject fMRI data
alignment. <em>TCDS</em>, <em>13</em>(3), 475–490. (<a
href="https://doi.org/10.1109/TCDS.2020.2965981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperalignment (HA) has been widely employed in multivariate pattern (MVP) analysis to discover the cognitive states in the human brains based on multisubject functional magnetic resonance imaging (fMRI) data sets. Most of the existing HA methods utilized unsupervised approaches, where they only maximized the correlation between the voxels with the same position in the time series. However, these unsupervised solutions may not be optimum for handling the functional alignment in the supervised MVP problems. This article proposes a supervised HA (SHA) method to ensure better functional alignment for MVP analysis, where the proposed method provides a supervised shared space that can maximize the correlation among the stimuli belonging to the same category and minimize the correlation between distinct categories of stimuli. Furthermore, SHA employs a generalized optimization solution, which generates the shared space and calculates the mapped features in a single iteration, hence with optimum time and space complexities for large data sets. Experiments on multisubject data sets demonstrate that the SHA method achieves up to 19% better performance for multiclass problems over the state-of-the-art HA algorithms.},
  archive      = {J_TCDS},
  author       = {Muhammad Yousefnezhad and Alessandro Selvitella and Liangxiu Han and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2020.2965981},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {475-490},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Supervised hyperalignment for multisubject fMRI data alignment},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A vector-based constrained obstacle avoidance scheme for
wheeled mobile redundant robot manipulator. <em>TCDS</em>,
<em>13</em>(3), 465–474. (<a
href="https://doi.org/10.1109/TCDS.2020.2979340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obstacle avoidance is an important issue when a wheeled mobile redundant robot manipulator (WMRRM) completes given tasks in complex environment. In this article, a novel vector-based constrained obstacle avoidance (VOA) scheme is designed and analyzed for motion planning of the WMRRM. Specifically, the VOA scheme is first described as a quadratic programming (QP) problem subject to an equality constraint, an inequality constraint, and a bound constraint. Compared with the traditional obstacle avoidance scheme, the proposed VOA scheme can not only achieve subtasks, such as obstacle avoidance and physical limit avoidance when the robot completes the main end-effector task, but also greatly expands the range of feasible spaces. In addition, the VOA scheme is transformed into piecewise linear projection equations (PLPEs) and solved by a linear-variational-inequality-based primal-dual neural network (LVI-PDNN), which can efficiently obtain the optimal solutions to the VOA scheme. Computer simulations demonstrate the reliability, accuracy, and effectiveness of the proposed VOA scheme.},
  archive      = {J_TCDS},
  author       = {Zhijun Zhang and Song Yang and Siyuan Chen and Yamei Luo and Hui Yang and Yu Liu},
  doi          = {10.1109/TCDS.2020.2979340},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {465-474},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A vector-based constrained obstacle avoidance scheme for wheeled mobile redundant robot manipulator},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An encoding framework with brain inner state for natural
image identification. <em>TCDS</em>, <em>13</em>(3), 453–464. (<a
href="https://doi.org/10.1109/TCDS.2020.2987352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural encoding and decoding, which aim to characterize the relationship between stimuli and brain activities, have emerged as an important area in cognitive neuroscience. Traditional encoding models, which focus on feature extraction and mapping, consider the brain as an input-output mapper without inner states. In this article, inspired by the fact that the human brain acts like a state machine, we proposed a novel encoding framework that combines information from both the external world and the inner state to predict brain activity. The framework comprises two parts: 1) forward encoding model that deals with visual stimuli and 2) inner state model that captures influence from intrinsic connections in the brain. The forward model can be any traditional encoding model, making the framework flexible. The inner state model is a linear model to utilize information in the prediction residuals of the forward model. The proposed encoding framework achieved much better performance on natural image identification than forward-only models, with a maximum identification accuracy of 100%. The identification accuracy decreased slightly with the data set size increasing, but remained relatively stable with different identification methods. The results confirm that the new encoding framework is effective and robust when used for brain decoding.},
  archive      = {J_TCDS},
  author       = {Hao Wu and Ziyu Zhu and Jiayi Wang and Nanning Zheng and Badong Chen},
  doi          = {10.1109/TCDS.2020.2987352},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {453-464},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An encoding framework with brain inner state for natural image identification},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient unified approach using demonstrations for
inverse reinforcement learning. <em>TCDS</em>, <em>13</em>(3), 444–452.
(<a href="https://doi.org/10.1109/TCDS.2019.2957831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reinforcement learning (RF) agent is always equipped with a designed reward function to correct policies for optimal decision making through interactions with an environment. However, it is difficult to design a reward function appropriate for complex RF problems. To solve this difficulty, the inverse RF (IRL) is introduced to provide an efficient way to design a reward function based on input derived from knowledgeable experts. In the IRL, experts provide demonstrations so that the agents can imitate the behaviors accordingly. However, even incorrect demonstrations have merits, some of which are similar to correct ones, so as that the agents with these clues can endeavor to avoid the occurrence of that behavior. This article introduces an IRL method which considers two types of demonstrations, correct and incorrect, in function approximation of a reward function. Given the clues from two opposite demonstrations, agents can iteratively approximate a reward function that can guide them to like expert’s correct demonstrations and also, prevent them from making the same mistakes as the expert did. These incorrect demonstrations provide agents with some guidelines to avoid erroneous motions in the initial phase. Two simulated tasks, a labyrinth and robot soccer games are conducted to validate the proposed method. The simulation results show that the proposed method can achieve the objectives of generating an appropriate reward function to accomplish apprentice learning with an efficient learning time in IRL.},
  archive      = {J_TCDS},
  author       = {Maxwell Hwang and Wei-Cheng Jiang and Yu-Jen Chen and Kao-Shing Hwang and Yi-Chia Tseng},
  doi          = {10.1109/TCDS.2019.2957831},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {9},
  number       = {3},
  pages        = {444-452},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An efficient unified approach using demonstrations for inverse reinforcement learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). IEEE computational intelligence society information.
<em>TCDS</em>, <em>13</em>(2), C3. (<a
href="https://doi.org/10.1109/TCDS.2021.3082968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2021.3082968},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hippocampal–entorhinal system inspired model for visual
concept representation. <em>TCDS</em>, <em>13</em>(2), 429–441. (<a
href="https://doi.org/10.1109/TCDS.2020.2978918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual concepts play a significant role in human cognition. The question of how past experience guides the formation of the visual concept system has been contentious for decades. There have been exciting recent progresses on concept learning and visual understanding. Despite the great advances, the previous models ignored the manner of visual concepts organization and representation in the brain. The semantic concepts in these models are not grounded on concrete visual representations. In this article, we propose a novel framework named visual concept space model (VCSM) by drawing inspiration from the hippocampal-entorhinal system. Specifically, we extend the role of the hippocampal-entorhinal system from spatial navigation to visual concept space. The proposed model provides a spatial representation format for visual concepts. The semantic concepts in the VCSM are explicitly built on visual representations. Once trained, VCSM can infer the visual attributes of input images and reconstruct diverse images from given concepts. Comprehensive experimental results on 3-D Chairs and Extended Yale Face Database B demonstrate the effectiveness of the proposed model.},
  archive      = {J_TCDS},
  author       = {Huankun Sheng and Hongwei Mo and Christian-Marie Moanda Ndeko Mosengo},
  doi          = {10.1109/TCDS.2020.2978918},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {429-441},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A Hippocampal–Entorhinal system inspired model for visual concept representation},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot multimodal object perception and recognition:
Synthetic maturation of sensorimotor learning in embodied systems.
<em>TCDS</em>, <em>13</em>(2), 416–428. (<a
href="https://doi.org/10.1109/TCDS.2020.2965985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that during early infancy, humans experience many physical and cognitive changes that shape their learning and refine their understanding of objects in the world. With the extended arm being one of the very first objects they familiarize, infants undergo a series of developmental stages that progressively facilitate physical interactions, enrich sensory information, and develop the skills to learn and recognize. Drawing inspiration from infancy, this article deals with the modeling of an open-ended learning mechanism for embodied agents that considers the cumulative and increasing complexity of physical interactions with the world. The proposed system achieves object perception and recognition as the agent (i.e., a humanoid robot) matures, experiences changes to its visual capabilities, develops sensorimotor control, and interacts with objects within its reach. The reported findings demonstrate the critical role of developing vision on the effectiveness of object learning and recognition and the importance of reaching and grasping in solving visually elicited ambiguities. Impediments caused by the interdependency of parallel components responsible for the agent&#39;s physical and cognitive functionalities are exposed, demonstrating an interesting phase transition in utilizing object perceptions for recognition.},
  archive      = {J_TCDS},
  author       = {Raphaël Braud and Alexandros Giagkos and Patricia Shaw and Mark Lee and Qiang Shen},
  doi          = {10.1109/TCDS.2020.2965985},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {416-428},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robot multimodal object perception and recognition: Synthetic maturation of sensorimotor learning in embodied systems},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigation of local white matter properties in
professional chess player: A diffusion magnetic resonance imaging study
based on automatic annotation fiber clustering. <em>TCDS</em>,
<em>13</em>(2), 403–415. (<a
href="https://doi.org/10.1109/TCDS.2020.2968116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates whether the white matter (WM) anatomical changes exist in the brain of professional chess players. Diffusion magnetic resonance imaging (dMRI) and fiber tracking provide a unique, noninvasive technique to study brain WM microstructures in vivo. However, existing methods focused on the entire tracts without detailed anatomical annotation, which depended on expert neuroanatomical knowledge to subdivide and annotate fiber tracts. This article used an automatically annotated fiber clustering method to identify anatomically meaningful WM structures from the whole brain&#39;s tractography. Each fiber was resampled to 200 equally spaced nodes along the whole tract to detect local differences. We calculated diffusion characteristics for 4 regions and 64 fiber clusters of professional chess players ( n = 28) and matched controls ( n = 29). The experimental results showed that there were significant differences in the diffusion characteristics of continuous locations in the thalamofrontal tracts and left superior longitudinal fasciculus. Pearson correlation analysis was also performed to prove our results. Professional chess players are significantly correlated with training duration and training frequency. Their brain structures have changed due to cognitive aspects, such as learning and memory. It is very meaningful to study the cognitive mechanism of change, which can improve high-level cognitive ability.},
  archive      = {J_TCDS},
  author       = {Yuanjing Feng and Jiahao Song and Wenxuan Yan and Jingqiang Wang and Changchen Zhao and Qingrun Zeng},
  doi          = {10.1109/TCDS.2020.2968116},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {403-415},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Investigation of local white matter properties in professional chess player: A diffusion magnetic resonance imaging study based on automatic annotation fiber clustering},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel grip force cognition scheme for robot-assisted
minimally invasive surgery. <em>TCDS</em>, <em>13</em>(2), 391–402. (<a
href="https://doi.org/10.1109/TCDS.2020.2981876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining a grip force in robot-assisted minimally invasive surgery (RMIS) has always been a crucial research topic. A novel grip force cognition scheme is proposed in this article. We utilize the dynamic analysis of the cable-driven system to conduct feature engineering and add prior knowledge to the Gaussian process regression (GPR). Specifically, the feature and GPR model candidates are acquired by analyzing the dynamic characteristics first. After that, a wrapper searching method combined with exhaustive search (ES) and sequential backward selection (SBS) is proposed to determine the features and GPR model. The hyperparameters are optimized by differential evolution (DE) by minimizing the negative log marginal likelihood (NLML). Besides, the training set is extended by including several different objects to avoid the algorithm “learning” the object property. Our method is verified on a cable-driven platform to ensure the errors are all derived from the algorithm rather than the cable tension loss. Our method reduces the errors on our testing set by about 64% and outperforms several popular model-based and learning-based methods. This article illustrates the significance to integrate the system&#39;s characteristics when introducing machine learning techniques into robot systems.},
  archive      = {J_TCDS},
  author       = {Yongchen Guo and Bo Pan and Yili Fu and Max Q.-H. Meng},
  doi          = {10.1109/TCDS.2020.2981876},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {391-402},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel grip force cognition scheme for robot-assisted minimally invasive surgery},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconfiguration of brain network between resting state and
p300 task. <em>TCDS</em>, <em>13</em>(2), 383–390. (<a
href="https://doi.org/10.1109/TCDS.2020.2965135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have explored the power spectra from the resting-state condition to the oddball task, but whether significant difference in the brain network exists is still unclear. This article aims to address how the brain reconfigures its architecture from a resting-state condition (i.e., baseline) to the P300 task in the visual oddball task. In this article, electroencephalograms (EEGs) were collected from 24 subjects, who were required to only mentally count the number of target stimulus; afterward, EEG networks constructed in different bands were compared between baseline and task to evaluate the reconfiguration of functional connectivity. Compared with the baseline, our results showed the significantly enhanced delta/theta functional connectivity and decreased alpha default mode network in the progress of brain reconfiguration to the task. Furthermore, the reconfigured coupling strengths were found to relate to P300 amplitudes, which were then regarded as features to train a classifier to differentiate the brain states and the high and low P300 groups with an accuracy of 100% and 77.78%, respectively. The findings of this article help us understand the updates in functional connectivity from resting state to the oddball task, and the reconfigured network structure has the potential for the selection of good subjects for P300-based brain-computer interface.},
  archive      = {J_TCDS},
  author       = {Fali Li and Chanlin Yi and Yuanyuan Liao and Yuanling Jiang and Yajing Si and Limeng Song and Tao Zhang and Dezhong Yao and Yangsong Zhang and Zehong Cao and Peng Xu},
  doi          = {10.1109/TCDS.2020.2965135},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {383-390},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reconfiguration of brain network between resting state and p300 task},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Epileptic signal classification based on synthetic minority
oversampling and blending algorithm. <em>TCDS</em>, <em>13</em>(2),
368–382. (<a href="https://doi.org/10.1109/TCDS.2020.3009020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scalp electroencephalogram (EEG) has been extensively studied for epileptic signal classification in the past, but little attention has been paid to the data imbalance among different epileptic states. It is well known that, in general, the duration of seizure onset is less than several minutes or even shorter. This will result in an imbalance problem when comparing to the durations of the preictal and interictal states. In this article, a novel epileptic classification and seizure detection algorithm for imbalanced data is proposed. The wavelet packet decomposition (WPD)-based statistical features (SFs) of multichannel EEGs are first extracted for representation. Then, the $K$ -means synthetic minority oversampling technique ( $K$ -means SMOTE) is applied for data balancing. A blending algorithm that consists of random forests (RFs), extremely randomized trees (Extra-Trees), and gradient boosting decision trees (GBDTs) is finally adopted for feature learning and epileptic signal classification. The developed algorithm provides an average accuracy of 89.49% and 83.90% on the Children’s Hospital Boston (CHB)-MIT and iNeuro databases, respectively. For the patient-specific classification experiment on the iNeuro database, the proposed algorithm achieves the highest average accuracy of 92.68%.},
  archive      = {J_TCDS},
  author       = {Dinghan Hu and Jiuwen Cao and Xiaoping Lai and Junbiao Liu and Shuang Wang and Yao Ding},
  doi          = {10.1109/TCDS.2020.3009020},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {368-382},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Epileptic signal classification based on synthetic minority oversampling and blending algorithm},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel bi-hemispheric discrepancy model for EEG emotion
recognition. <em>TCDS</em>, <em>13</em>(2), 354–367. (<a
href="https://doi.org/10.1109/TCDS.2020.2999337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscience study has revealed the discrepancy of emotion expression between the left and right hemispheres of human brain. Inspired by this study, in this article, we propose a novel bi-hemispheric discrepancy model (BiHDM) to learn this discrepancy information between the two hemispheres to improve electroencephalograph (EEG) emotion recognition. Concretely, we first employ four directed recurrent neural networks (RNNs) based on two spatial orientations to traverse electrode signals on two separate brain regions. This enables the proposed model to obtain the deep representations of all the EEG electrodes’ signals that keep their intrinsic spatial dependence. Upon this representation, a pairwise subnetwork is designed to explicitly capture the discrepancy information between the two hemispheres and extract higher level features for final classification. Furthermore, considering the presence of the domain shift between training and testing data, we incorporate a domain discriminator that adversarially induces the overall feature learning module to generate emotion related but domain-invariant feature representation so as to further promote EEG emotion recognition. Experiments are conducted on three public EEG emotional data sets, in which we evaluate the performance of the proposed BiHDM as well as investigated the important brain areas in emotion expression and explore to use less electrodes to achieve comparable results. These experimental results jointly demonstrate the effectiveness and advantage of the proposed BiHDM model in solving the EEG emotion recognition problem.},
  archive      = {J_TCDS},
  author       = {Yang Li and Lei Wang and Wenming Zheng and Yuan Zong and Lei Qi and Zhen Cui and Tong Zhang and Tengfei Song},
  doi          = {10.1109/TCDS.2020.2999337},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {354-367},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel bi-hemispheric discrepancy model for EEG emotion recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speech emotion recognition based on robust discriminative
sparse regression. <em>TCDS</em>, <em>13</em>(2), 343–353. (<a
href="https://doi.org/10.1109/TCDS.2020.2990928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition has recently attracted much interest due to the widespread of multimedia data. It generally involves two basic problems: 1) feature extraction and 2) emotion classification. Most previous algorithms just focus on solving one of these two problems. In this article, we aim to deal with these two problems in a joint learning framework, and present a novel regression algorithm, namely, robust discriminative sparse regression (RDSR). In RDSR, we propose a sparse regression algorithm to make our model robust to outliers and noises, and introduce a feature selection regularization constraint simultaneously to select the most discriminative and relevant features. In addition, to well predict the labels, we exploit the local and global consistency over labels, and incorporate it into the proposed framework. To solve the objective function of RDSR, we design an efficient alternative optimization algorithm. Finally, experimental results on several public emotion data sets verify the effectiveness and the superiority of our proposed method.},
  archive      = {J_TCDS},
  author       = {Peng Song and Wenming Zheng and Yanwei Yu and Shifeng Ou},
  doi          = {10.1109/TCDS.2020.2990928},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {343-353},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Speech emotion recognition based on robust discriminative sparse regression},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vocal imitation in sensorimotor learning models: A
comparative review. <em>TCDS</em>, <em>13</em>(2), 326–342. (<a
href="https://doi.org/10.1109/TCDS.2020.3041179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensorimotor learning represents a challenging problem for natural and artificial systems. Several computational models have been proposed to explain the neural and cognitive mechanisms at play in the brain. In general, these models can be decomposed in three common components: 1) a sensory system; 2) a motor control device; and 3) a learning framework. The latter includes the architecture, the learning rule or optimization method, and the exploration strategy used to guide learning. In this review, we focus on imitative vocal learning, that is, exemplified in song learning in birds and speech acquisition in humans. We aim to synthesize, analyze, and compare the various models of vocal learning that have been proposed, highlighting their common points and differences. We first introduce the biological context, including the behavioral and physiological hallmarks of vocal learning and sketch the neural circuits involved. Then, we detail the different components of a vocal learning model and how they are implemented in the reviewed models.},
  archive      = {J_TCDS},
  author       = {Silvia Pagliarini and Arthur Leblois and Xavier Hinaut},
  doi          = {10.1109/TCDS.2020.3041179},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {326-342},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Vocal imitation in sensorimotor learning models: A comparative review},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Where do i move my sensors? Emergence of a topological
representation of sensors poses from the sensorimotor flow.
<em>TCDS</em>, <em>13</em>(2), 312–325. (<a
href="https://doi.org/10.1109/TCDS.2019.2959915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the perception of mobile robotic systems within the framework of interactive perception, and inspired by the sensorimotor contingencies (SMCs) theory. These approaches state that perception arises from the active exploration of an environment. In the SMC theory, it is postulated that information about the structure of space could be recovered from a quasi-uninterpreted sensorimotor flow. In a recent article, the authors have provided a mathematical framework for the construction of a sensorimotor representation of the interaction between the sensors and the body of a naive agent, provided that the sensory inputs come from the agent&#39;s own body. An extension of these results, with stimulations coming from an unknown changing environment, is proposed in this article. More precisely, it is demonstrated that, through repeated explorations of its motor configurations, the perceived sensory invariants can be exploited to build a topologically accurate internal representation of the relative poses of the agent&#39;s sensors in the physical world. Precise theoretical considerations are provided as well as an experimental framework assessed in simulated but challenging environments.},
  archive      = {J_TCDS},
  author       = {Valentin Marcel and Sylvain Argentieri and Bruno Gas},
  doi          = {10.1109/TCDS.2019.2959915},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {312-325},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Where do i move my sensors? emergence of a topological representation of sensors poses from the sensorimotor flow},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous identification and goal-directed invocation of
event-predictive behavioral primitives. <em>TCDS</em>, <em>13</em>(2),
298–311. (<a href="https://doi.org/10.1109/TCDS.2019.2925890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voluntary behavior of humans appears to be composed of small, elementary building blocks, or behavioral primitives. While this modular organization seems crucial for the learning of complex motor skills and the flexible adaption of behavior to new circumstances, the problem of learning meaningful, compositional abstractions from sensorimotor experiences remains an open challenge. Here, we introduce a computational learning architecture, termed as surprise-based behavioral modularization into event-predictive structures (SUBMODES) that explores behavior and identifies the underlying behavioral units completely from scratch. The SUBMODES architecture bootstraps sensorimotor exploration using a self-organizing neural controller. While exploring the behavioral capabilities of its own body, the system learns modular structures that predict the sensorimotor dynamics and generate the associated behavior. In line with recent theories of event perception, the system uses unexpected prediction error signals, i.e., surprise, to detect transitions between successive behavioral primitives. We show that, when applied to two robotic systems with completely different body kinematics, the system manages to learn a variety of complex behavioral primitives. Moreover, after initial self exploration the system can use its learned predictive models progressively more effectively for invoking model predictive planning and goal-directed control in different tasks and environments.},
  archive      = {J_TCDS},
  author       = {Christian Gumbsch and Martin V. Butz and Georg Martius},
  doi          = {10.1109/TCDS.2019.2925890},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {298-311},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Autonomous identification and goal-directed invocation of event-predictive behavioral primitives},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effect regulated projection of robot’s action space for
production and prediction of manipulation primitives through learning
progress and predictability-based exploration. <em>TCDS</em>,
<em>13</em>(2), 286–297. (<a
href="https://doi.org/10.1109/TCDS.2019.2933900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an effective action parameter exploration mechanism that enables efficient discovery of robot actions through interacting with objects in a simulated table-top environment. For this, the robot organizes its action parameter space based on the generated effects in the environment and learns forward models for predicting consequences of its actions. Following the intrinsic motivation approach, the robot samples the action parameters from the regions that are expected to yield high learning progress (LP). In addition to the LP-based action sampling, our method uses a novel parameter space organization scheme to form regions that naturally correspond to qualitatively different action classes, which might be also called action primitives. The proposed method enabled the robot to discover a number of lateralized movement primitives and to acquire the capability of predicting the consequences of these primitives. Furthermore, our results suggest the reasons behind the earlier development of grasp compared to push action in infants. Finally, our findings show some parallels with data from infant development where correspondence between action production and prediction is observed.},
  archive      = {J_TCDS},
  author       = {Serkan Bugur and Erhan Oztop and Yukie Nagai and Emre Ugur},
  doi          = {10.1109/TCDS.2019.2933900},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {286-297},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Effect regulated projection of robot’s action space for production and prediction of manipulation primitives through learning progress and predictability-based exploration},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Open-ended continuous learning of compound goals.
<em>TCDS</em>, <em>13</em>(2), 274–285. (<a
href="https://doi.org/10.1109/TCDS.2019.2947896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous learning of increasingly difficult tasks is envisaged to provide learning autonomy to robots. The tasks, however, are often human-designed or require significant external intervention. This article proposes a domain-independent goal generation mechanism to generate goals at different levels of complexity. Using a mobile robot application, we demonstrate how an agent generates compound goals by combining the state space attributes from the states that it has experienced during exploration and uses task-independent reward functions to learn the solutions to those goals. Finally, the whole process is repeated when its environment changes, thus, forming a continuous learning architecture. Results from the experiments show how the agent can combine complementary and contradictory groups of state attributes to form expressive goals and learn behaviors akin to wall following, avoiding obstacle and lane following without any previous knowledge of its environment.},
  archive      = {J_TCDS},
  author       = {Paresh Dhakan and Kathryn Kasmarik and Iñaki Rañó and Nazmul Siddique},
  doi          = {10.1109/TCDS.2019.2947896},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {274-285},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Open-ended continuous learning of compound goals},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intrinsically motivated hierarchical policy learning in
multiobjective markov decision processes. <em>TCDS</em>, <em>13</em>(2),
262–273. (<a href="https://doi.org/10.1109/TCDS.2019.2948025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiobjective Markov decision processes (MOMDPs) are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problem cannot be solved by a single optimal policy as in the conventional case. Alternatively, multiobjective reinforcement learning (RL) methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in the nonstationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skillset that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics, therefore, it can facilitate a continuous learning process. In this article, intrinsically motivated RL (IMRL) has been successfully deployed to evolve generic skillsets for learning hierarchical policy to solve the MOMDPs. We propose a novel dual-phase IMRL method to address this limitation. In the first phase, a generic set of skills is learned, while in the second phase, this set is used to bootstrap policy coverage sets for each shift in the environment dynamics. We show experimentally that the proposed method significantly outperforms the state-of-the-art multiobjective reinforcement methods on a dynamic robotics environment.},
  archive      = {J_TCDS},
  author       = {Sherif Abdelfattah and Kathryn Merrick and Jiankun Hu},
  doi          = {10.1109/TCDS.2019.2948025},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {262-273},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Intrinsically motivated hierarchical policy learning in multiobjective markov decision processes},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BND*-DDQN: Learn to steer autonomously through deep
reinforcement learning. <em>TCDS</em>, <em>13</em>(2), 249–261. (<a
href="https://doi.org/10.1109/TCDS.2019.2928820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is vital for mobile robots to achieve safe autonomous steering in various changing environments. In this paper, a novel end-to-end network architecture is proposed for mobile robots to learn steering autonomously through deep reinforcement learning. Specifically, two sets of feature representations are first extracted from the depth inputs through two different input streams. The acquired features are then merged together to derive both linear and angular actions simultaneously. Moreover, a new action selection strategy is also introduced to achieve motion filtering by taking the consistency in angular velocity into account. Besides, in addition to the extrinsic rewards, the intrinsic bonuses are also adopted during training to improve the exploration capability. Furthermore, it is worth noting the proposed model is readily transferable from the simple virtual training environment to much more complicated real-world scenarios so that no further fine-tuning is required for real deployment. Compared to the existing methods, the proposed method demonstrates significant superiority in terms of average reward, convergence speed, success rate, and generalization capability. In addition, it exhibits outstanding performance in various cluttered real-world environments containing both static and dynamic obstacles. A video of our experiments can be found at https://youtu.be/19jrQGG1oCU.},
  archive      = {J_TCDS},
  author       = {Keyu Wu and Han Wang and Mahdi Abolfazli Esfahani and Shenghai Yuan},
  doi          = {10.1109/TCDS.2019.2928820},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {249-261},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {BND*-DDQN: Learn to steer autonomously through deep reinforcement learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLIC: Curriculum learning and imitation for object control
in nonrewarding environments. <em>TCDS</em>, <em>13</em>(2), 239–248.
(<a href="https://doi.org/10.1109/TCDS.2019.2933371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a new reinforcement learning (RL) setting where the environment is nonrewarding, contains several possibly related objects of various controllability, where an apt agent Bob acts following its own goals, without necessarily providing helpful demonstrations, and where the objective of an agent is to learn to control objects individually. We present a generic discrete-state discrete-action model of such environments, and an unsupervised RL agent called CLIC for curriculum learning and imitation for control to achieve the desired objective. CLIC selects objects to focus on when training and imitating by maximizing its learning progress. We show that CLIC can effectively observe Bob to gain control of objects faster, even if Bob is not explicitly teaching. Despite choosing what it imitates in a principled way, CLIC retains the natural ability to follow Bob when he provides ordered demonstrations. Finally, we show that compared with a noncurriculum-based agent, when Bob controls objects that the agent cannot, or in presence of a hierarchy between objects in the environment, CLIC achieves faster mastery of the environment by ignoring nonreproducible and already mastered interactions with objects when imitating.},
  archive      = {J_TCDS},
  author       = {Pierre Fournier and Cédric Colas and Mohamed Chetouani and Olivier Sigaud},
  doi          = {10.1109/TCDS.2019.2933371},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {239-248},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CLIC: Curriculum learning and imitation for object control in nonrewarding environments},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial special issue on continual unsupervised
sensorimotor learning. <em>TCDS</em>, <em>13</em>(2), 234–238. (<a
href="https://doi.org/10.1109/TCDS.2021.3082880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of higher levels of autonomy and versatility in robotics is arguably led by two main factors. First, as we push robots out of the labs and production lines, it becomes increasingly challenging to design for all possible scenarios that a particular robot might encounter. Second, the cost of designing, manufacturing, and maintaining such systems becomes prohibitive.},
  archive      = {J_TCDS},
  author       = {Nicolás Navarro-Guerrero and Sao Mai Nguyen and Erhan Oztop and Junpei Zhong},
  doi          = {10.1109/TCDS.2021.3082880},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {2},
  pages        = {234-238},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial special issue on continual unsupervised sensorimotor learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). IEEE computational intelligence society information.
<em>TCDS</em>, <em>13</em>(1), C3. (<a
href="https://doi.org/10.1109/TCDS.2020.3044738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TCDS},
  doi          = {10.1109/TCDS.2020.3044738},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised temporospatial neural architecture for
sensorimotor map learning. <em>TCDS</em>, <em>13</em>(1), 223–230. (<a
href="https://doi.org/10.1109/TCDS.2019.2934643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to learn the sensorimotor maps of unknown environments without supervision is a vital capability of any autonomous agent, be it biological or artificial. An accurate sensorimotor map should be able to encode the agent&#39;s world and equip it with the capability to anticipate or predict the results of its actions. However, to design a robust autonomous learning technique for an unknown, dynamic, partially observable, or noisy environment remains a daunting task. This article proposes a temporospatial merge grow when required (TMGWR) network for continuous self-organization of an agent&#39;s sensorimotor awareness in noisy environments. TMGWR is an adaptive neural algorithm that learns the sensorimotor map of an agent&#39;s world using a time series self-organizing strategy and the grow when required (GWR) algorithm. The algorithm is compared with growing neural gas (GNG), GWR, and time GNG in terms of their disambiguation performance, sensorial representation accuracy, and sensorimotor-link error, a new metric that is developed in this article to evaluate how well a sensorimotor map represents causality in the agent&#39;s world. The outcomes of the experiments show that TMGWR is more efficient and suitable for sensorimotor map learning in noisy environments than the competing algorithms.},
  archive      = {J_TCDS},
  author       = {Chinedu Pascal Ezenkwu and Andrew Starkey},
  doi          = {10.1109/TCDS.2019.2934643},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {223-230},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Unsupervised temporospatial neural architecture for sensorimotor map learning},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A regression method with subnetwork neurons for vigilance
estimation using EOG and EEG. <em>TCDS</em>, <em>13</em>(1), 209–222.
(<a href="https://doi.org/10.1109/TCDS.2018.2889223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, it has been observed that there is an increasing rate of road accidents due to the low vigilance of drivers. Thus, the estimation of drivers&#39; vigilance state plays a significant role in public transportation safety. We have adopted a feature fusion strategy that combines the electroencephalogram (EEG) signals collected from various sites of the human brain, including forehead, temporal, and posterior and forehead electrooculogram (forehead-EOG) signals, to address this factor. The level of vigilance is predicted through a new learning model known as double-layered neural network with subnetwork nodes (DNNSNs), which comprises several subnetwork nodes, and each node in turn is composed of many hidden nodes that have various capabilities of feature selection (dimension reduced), feature learning, etc. The proposed single modality that uses only forehead-EOG signal exhibits a mean root-mean-square error (RMSE) of 0.12 and a mean Pearson product-moment correlation coefficient (COR) of 0.78. On one hand, an EEG signal achieved a mean RMSE of 0.13 and a mean COR of 0.72. Whereas, on the other, the proposed multimodality achieved values of 0.09 and 0.85 for the mean RMSE and the mean COR, respectively. Experimental results show that the proposed DNNSN with multimodality fusion outperforms the model with single modality for vigilance estimation due to the complementary information between forehead-EOG and EEG. After a favorable learning rate was applied to the input layer, the mean RMSE/COR improved to 0.11/0.79, 0.12/0.74, and 0.08/0.86, respectively. Hence, this quantitative analysis proves that the proposed method provides better feasibility and efficiency learning capability and surmounts other state-of-the-art techniques.},
  archive      = {J_TCDS},
  author       = {Wei Wu and Q. M. Jonathan Wu and Wei Sun and Yimin Yang and Xiaofang Yuan and Wei-Long Zheng and Bao-Liang Lu},
  doi          = {10.1109/TCDS.2018.2889223},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {209-222},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A regression method with subnetwork neurons for vigilance estimation using EOG and EEG},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A gesture recognition system based on time domain features
and linear discriminant analysis. <em>TCDS</em>, <em>13</em>(1),
200–208. (<a href="https://doi.org/10.1109/TCDS.2018.2884942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface electromyogram (sEMG) signals have been used to control multifunctional prosthetic hands. Researchers usually focused on the use of several channels with sEMG signals to identify more gestures without limiting the number of sEMG sensors. However, the residual muscles of an amputee are limited. Therefore, the point of a successful recognition system is to decrease the channels of sEMG signals to classify more gestures. To achieve this goal, we proposed a novel gesture recognition system, in which three channels of sEMG signals can classify nine gestures. In this recognition system, the time domain features, root mean square ratio, and autoregressive model, were selected to extract the features of the sEMG signals as compared with the time-frequency domain features. Furthermore, the linear discriminant analysis was adopted as the classifier. Consequently, the average accuracy rate of the presented system was 91.7%. Therefore, the proposed gesture recognition system is feasible to identify more gestures with less sensors.},
  archive      = {J_TCDS},
  author       = {Feng Duan and Xina Ren and Yikang Yang},
  doi          = {10.1109/TCDS.2018.2884942},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {200-208},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A gesture recognition system based on time domain features and linear discriminant analysis},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An ensemble net of convolutional auto-encoder and graph
auto-encoder for auto-diagnosis. <em>TCDS</em>, <em>13</em>(1), 189–199.
(<a href="https://doi.org/10.1109/TCDS.2020.2984335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective auto-diagnosis assistants can benefit our healthcare system in various aspects, such as, saving labor cost, sharing knowledge among the crowd, and timely supporting the patients. However, the existing auto-diagnosis models are ineffective due to issues caused by information island, poor information coding, and inefficient informative retrieval. To address these issues, this article presents a diagnosis assistant that is designed and implemented to manage abundant historical inquiries between patients and doctors. The core of the auto-diagnosis system is a novel model called ensemble net of convolutional auto-encoder and graph auto-encoder (EN-C+GAE) which can be trained using historical data and generate a list of candidate diagnoses for a doctor to select. The experimental results show that the proposed approach outperforms the counterparts in generating more fluent and relevant diagnoses. The proposed system also shows its potential in real-world deployment in healthcare scenarios.},
  archive      = {J_TCDS},
  author       = {Jianqiang Li and Changping Ji and Guokai Yan and Linlin You and Jie Chen},
  doi          = {10.1109/TCDS.2020.2984335},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {189-199},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An ensemble net of convolutional auto-encoder and graph auto-encoder for auto-diagnosis},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel robotic guidance system with eye-gaze tracking
control for needle-based interventions. <em>TCDS</em>, <em>13</em>(1),
179–188. (<a href="https://doi.org/10.1109/TCDS.2019.2959071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robotic technologies have been widely used in the operating room for decades. Among them, needle-based percutaneous interventions have attracted much attention from engineering and medical communities. However, the currently used robotic systems for interventional procedures are too cumbersome, requiring a large footprint in the operating room. Recently developed light-weight puncture robotic systems for needle positioning are able to reduce the size, but has the limitation of awkward ergonomics. In this article, we design a compact robotic guidance system that could accurately realize the needle position and orientation within the operating room. The eye-gaze tracking-based approach is proposed to control the position and orientation of the needle toward the desired location in a more intuitive manner, and the forward and inverse kinematics of this 4-DoF robot are analyzed. The robot operating system (ROS)-based experimental studies are performed to evaluate the needle placement accuracy during interventional therapy. The result indicated the effectiveness of the proposed robotic hardware and the eye-gaze-based control framework, which can achieve a distance error of the robot’s end effector to the target point within 1 mm.},
  archive      = {J_TCDS},
  author       = {Jing Guo and Yi Liu and Qing Qiu and Jie Huang and Chao Liu and Zhiguang Cao and Yue Chen},
  doi          = {10.1109/TCDS.2019.2959071},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {179-188},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A novel robotic guidance system with eye-gaze tracking control for needle-based interventions},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal continual learning using online dictionary
updating. <em>TCDS</em>, <em>13</em>(1), 171–178. (<a
href="https://doi.org/10.1109/TCDS.2020.2973280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the continual learning task from the perspective of multimodal fusion. In the multimodal fusion problem, the unified representations of heterogeneous modalities are continuously updated, while the classifier is also constantly refreshed for each multimodal learning task. To tackle this point, we establish a continual learning framework for multimodal learning and design an effective online dictionary updating method. Finally, we experimentally verify a complex material identification task and obtain promising results.},
  archive      = {J_TCDS},
  author       = {Fuchun Sun and Huaping Liu and Chao Yang and Bin Fang},
  doi          = {10.1109/TCDS.2020.2973280},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {171-178},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal continual learning using online dictionary updating},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework of hybrid force/motion skills learning for
robots. <em>TCDS</em>, <em>13</em>(1), 162–170. (<a
href="https://doi.org/10.1109/TCDS.2020.2968056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human factors and human-centered design philosophy are highly desired in today&#39;s robotics applications such as human-robot interaction (HRI). Several studies showed that endowing robots of human-like interaction skills can not only make them more likeable but also improve their performance. In particular, skill transfer by imitation learning can increase the usability and acceptability of robots by users without computer programming skills. In fact, besides positional information, muscle stiffness of the human arm and contact force with the environment also play important roles in understanding and generating human-like manipulation behaviors for robots, e.g., in physical HRI and teleoperation. To this end, we present a novel robot learning framework based on dynamic movement primitives (DMPs), taking into consideration both the positional and contact force profiles for human-robot skills transferring. Distinguished from the conventional method involving only the motion information, the proposed framework combines two sets of DMPs, which are built to model the motion trajectory and the force variation of the robot manipulator, respectively. Thus, a hybrid force/motion control approach is taken to ensure the accurate tracking and reproduction of the desired positional and force motor skills. Meanwhile, in order to simplify the control system, a momentum-based force observer is applied to estimate the contact force instead of employing force sensors. To deploy the learned motion-force robot manipulation skills to a broader variety of tasks, the generalization of these DMP models in actual situations is also considered. Comparative experiments have been conducted using a Baxter robot to verify the effectiveness of the proposed learning framework on real-world scenarios like cleaning a table.},
  archive      = {J_TCDS},
  author       = {Ning Wang and Chuize Chen and Alessandro Di Nuovo},
  doi          = {10.1109/TCDS.2020.2968056},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {162-170},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A framework of hybrid Force/Motion skills learning for robots},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A rapid spiking neural network approach with an application
on hand gesture recognition. <em>TCDS</em>, <em>13</em>(1), 151–161. (<a
href="https://doi.org/10.1109/TCDS.2019.2918228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spiking neural network (SNN) is considered to be the third generation of neural networks featured by its low power consumption and high computing capability, which has great application potential in robotics. However, the present SNN has two limitations: 1) the neuron’s spike firing time is calculated based on the iterative approach, which dramatically slows down the calculation rate of the SNN and 2) the existing learning algorithm is more suitable for the single-layer structure, which can hardly train the network with “deep structure.” To this end, this paper proposes a novel spike firing time search algorithm that can narrow the search interval. In addition, a pretrained subnet SNN is designed, which makes the SNN have more hidden layers. This setting of the SNN can effectively improve its performance in pattern recognition tasks. Furthermore, by using the surface electromyography signal (sEMG), the proposed SNN is used to recognize the hand gestures. The experimental results show that: 1) the spike firing time search algorithm can significantly increase the forward propagation rate of the SNN and 2) the proposed SNN can reach a satisfactory recognition accuracy ratio 97.4%, which is 0.9% higher than that of the fully connected SNN.},
  archive      = {J_TCDS},
  author       = {Long Cheng and Yang Liu and Zeng-Guang Hou and Min Tan and Dajun Du and Minrui Fei},
  doi          = {10.1109/TCDS.2019.2918228},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {151-161},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A rapid spiking neural network approach with an application on hand gesture recognition},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Performance comparison of gesture recognition system based
on different classifiers. <em>TCDS</em>, <em>13</em>(1), 141–150. (<a
href="https://doi.org/10.1109/TCDS.2020.2969297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hand plays a very important role in our daily life, and the amputees suffer a lot from the loss of hands or upper limbs. Hence, assisting devices are desired urgently. Today, the prosthetic hands based on surface electromyography (sEMG) signals can recognize many hand gestures, but some problems still exist. To identify more gestures, some recognition systems require multiple electrodes, which are unable to be applied to the amputees with less residual muscles. Meanwhile, better computing performance is required as the number of electrodes increases, which is difficult to be applied to the real-time embedded systems. In this article, we aim to recognize six hand gestures by using sEMG sensors as little as possible. To realize this goal, we compare the accuracy and processing time of different feature extraction and classification methods offline, and the results indicate that the combination of time-domain features and backpropagation neural network has better performance. In total, nine subjects participated in the offline experiments, and the accuracy is up to 95.46% by employing two sEMG sensors to recognize six hand gestures.},
  archive      = {J_TCDS},
  author       = {Yikang Yang and Feng Duan and Jia Ren and Jianing Xue and Yizhi Lv and Chi Zhu and Hiroshi Yokoi},
  doi          = {10.1109/TCDS.2020.2969297},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {141-150},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Performance comparison of gesture recognition system based on different classifiers},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A reinforcement learning method using multifunctional
principal component analysis for human-like grasping. <em>TCDS</em>,
<em>13</em>(1), 132–140. (<a
href="https://doi.org/10.1109/TCDS.2020.2988641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Postural synergies allow a rich set of hand configurations to be represented in lower dimension space compared to the original joint space. In our previous works, we have shown that this can be extended to trajectories thanks to the multivariate functional principal component analysis, obtaining a set of basis functions able to represent grasping movements learned from human demonstration. In this article, we introduce a human cognition-inspired approach for generalizing and improving robot grasping skills in this motion synergies subspace. The use of a reinforcement learning (RL) algorithm allows the robot to explore the surrounding space and improve its capability to reach and grasp objects. The learning method is the policy improvement with path integrals, running in the policy space. Bootstrapped with synergy coefficients obtained from neural networks, the policy reward is based on a force closure grasp quality index computed at the end of the task, measuring how a firm is the grip. We finally show that combining neural networks and RL allows the robot manipulator to have a good initial estimate of the grasping configuration and faster convergence to an optimal grasp with respect to a database approach, the latter a less general solution in presence of new objects.},
  archive      = {J_TCDS},
  author       = {Marco Monforte and Fanny Ficuciello},
  doi          = {10.1109/TCDS.2020.2988641},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {132-140},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A reinforcement learning method using multifunctional principal component analysis for human-like grasping},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maxwell-model-based compliance control for human–robot
friendly interaction. <em>TCDS</em>, <em>13</em>(1), 118–131. (<a
href="https://doi.org/10.1109/TCDS.2020.2992538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores an alternative design philosophy of compliance control for safe and friendly human–robot interaction. Conventional approaches are based on the Voigt model with spring and damper connected in parallel. The theoretical and experimental results of disciplines other than robotics (e.g., mechanics of materials and elastoplastic mechanics) show that the Maxwell model has the merit of removing the elastic return force and reducing the effect of collisions with respect to the Voigt model. Therefore, the authors are motivated to exploit the advantages of the Maxwell model in reactions to human interactions and develop a series of Maxwell model-based compliance control methodologies, which only require kinematic control interfaces. First, a Maxwell model-based Cartesian admittance control scheme is presented to produce a novel plastic compliance of robot end effector. Moreover, we present a Maxwell model-based null space admittance control approach for manipulators with redundancy to achieve a novel plastic whole body compliance. We have evaluated the effectiveness and generalizability of the proposed methods by extensive comparative experiments on human–robot cooperative tasks in various sectors, including domestic, part assembly, and robotic puncture surgery. Our approaches can be viewed as a new and comfortable interface of physical human–robot interaction and collaboration.},
  archive      = {J_TCDS},
  author       = {Le Fu and Jie Zhao},
  doi          = {10.1109/TCDS.2020.2992538},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {118-131},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Maxwell-model-based compliance control for Human–Robot friendly interaction},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skill learning strategy based on dynamic motion primitives
for human–robot cooperative manipulation. <em>TCDS</em>, <em>13</em>(1),
105–117. (<a href="https://doi.org/10.1109/TCDS.2020.3021762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a skill learning-based hierarchical control strategy for human-robot cooperative manipulation, which constitutes a novel learning-control system. The high-level learning strategy aims to learn the motor skills from human demonstrations by fusion with dynamic motion primitives (DMPs) and the Gaussian mixture model (GMM). The lower level control strategy guarantees the compliance of the robot movement under human interaction using admittance control and integral barrier Lyapunov function (IBLF)-based adaptive neural controller. First, the robot learns the motor skills from observing the successful execution of tasks by a demonstrator through DMP-GMM methods. Then, the robot reproduces the complex skills and executes the interactive task by demonstrations. Finally, the effectiveness of the proposed learning-control strategy is demonstrated with experimental results. The results show that the developed hierarchical strategy has good performance in cooperation by learning and control that reacts compliantly to robot interaction with human subjects.},
  archive      = {J_TCDS},
  author       = {Junjun Li and Zhijun Li and Xinde Li and Ying Feng and Yingbai Hu and Bugong Xu},
  doi          = {10.1109/TCDS.2020.3021762},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {105-117},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Skill learning strategy based on dynamic motion primitives for Human–Robot cooperative manipulation},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BCI-controlled assistive manipulator: Developed architecture
and experimental results. <em>TCDS</em>, <em>13</em>(1), 91–104. (<a
href="https://doi.org/10.1109/TCDS.2020.2979375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a control architecture for a robotic manipulator finally aimed at helping people with severe motion disabilities in performing daily life operations, such as manipulating objects or drinking. The proposed solution allows the user to focus the attention only on the operational tasks, while all the safety-related issues are automatically handled by the developed control architecture. The user commands the manipulator sending high-level commands via a P300-based brain–computer interface. A perception module, relying on an RGB-D sensor, continuously detects and localizes the objects in the scene, tracking the position of the user and monitoring the environment for identifying static and dynamic obstacles, e.g., a person entering in the scene. A lightweight manipulator is controlled relying on a task-priority inverse kinematics algorithm that handles task hierarchies composed of equality-based and set-based tasks, including obstacle avoidance and joint mechanical limits. This article describes the overall architecture and the integration of the implemented software modules, that are based on common frameworks and software libraries, such as the robotic operating system (ROS), BCI2000, OpenCV, and PCL. The experimental results on a use case scenario using a Kinova 7DOFs Jaco 2 robot helping a user to perform drinking and manipulation tasks show the effectiveness of the developed control architecture.},
  archive      = {J_TCDS},
  author       = {Paolo Di Lillo and Filippo Arrichiello and Daniele Di Vito and Gianluca Antonelli},
  doi          = {10.1109/TCDS.2020.2979375},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {91-104},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {BCI-controlled assistive manipulator: Developed architecture and experimental results},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-layer learning-based predictive control with echo
state network for pneumatic-muscle-actuators-driven exoskeleton.
<em>TCDS</em>, <em>13</em>(1), 80–90. (<a
href="https://doi.org/10.1109/TCDS.2020.2968733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a single-layer learning-based predictive control strategy for pneumatic muscle actuators (PMAs)-driven lower limb exoskeleton. Although PMAs are promising for rehabilitation robots, they suffer from nonlinearities, unmodeled uncertainties, hysteresis, etc. As a consequence, the mechanism actuated by PMAs rarely involves complex dynamics, and the related precise control remains a challenging problem. Hence, considering the global approximation capability of neural networks, we use an echo state network (ESN) to approximate the dynamics of the PMAs-driven exoskeleton with a nonlinear autoregressive exogenous model and forecast its behaviors by constructing training and testing data sets. Through the model predictions, the idea of single-layer learning solves a quadratic programming problem based on the principle of predictive control over a finite future horizon. After that, the control strategy turns out to be asymptotically stable when the ESN is capable of approximating the dynamics of the exoskeleton. Passive gait training experiments are conducted with six healthy subjects to verify the effectiveness of the proposed control strategy. Compared with the traditional strategies, the proposed control strategy achieves higher tracking accuracy for passive gait training tasks.},
  archive      = {J_TCDS},
  author       = {Yu Cao and Jian Huang and Caihua Xiong},
  doi          = {10.1109/TCDS.2020.2968733},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {80-90},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Single-layer learning-based predictive control with echo state network for pneumatic-muscle-actuators-driven exoskeleton},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exoskeleton online learning and estimation of human walking
intention based on dynamical movement primitives. <em>TCDS</em>,
<em>13</em>(1), 67–79. (<a
href="https://doi.org/10.1109/TCDS.2020.2968845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human walking intention estimation is a critical step for the active assistance control of lower limb exoskeleton, because the purpose of active assistance control is human motion assistance rather than human motion tracking. Complying with human walking intention is the basic requirement of human walking assistance. Hence, the human walking intention must be estimated first to ensure the exoskeleton will not impede human motion. Actually, estimating human walking intention is to estimate human joint torque during walking. In order to estimate a smooth personalized human joint torque profile, an online learning and prediction algorithm of human joint trajectory and joint torque is proposed in this article. The algorithm is based on the dynamical movement primitives model which is used for online learning and predicting human joint trajectory which is substituted into the human dynamics model to estimate the human joint torque. The results of human walking experiments demonstrate that the proposed algorithm can not only predict a smooth human joint trajectory and joint torque profile in real time but also compensate the phase delay caused by sensor signal filtering. Hence, the proposed algorithm is suitable for the active walking assistance control of exoskeleton.},
  archive      = {J_TCDS},
  author       = {Shiyin Qiu and Wei Guo and Darwin Caldwell and Fei Chen},
  doi          = {10.1109/TCDS.2020.2968845},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {67-79},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Exoskeleton online learning and estimation of human walking intention based on dynamical movement primitives},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-in-the-loop control strategy of unilateral exoskeleton
robots for gait rehabilitation. <em>TCDS</em>, <em>13</em>(1), 57–66.
(<a href="https://doi.org/10.1109/TCDS.2019.2954289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a human-in-the-loop control methodology is proposed for the gait rehabilitation of patients with hemiplegia. It utilizes a unilateral exoskeleton system consisting of a unilateral lower limb exoskeleton and a real-time robot follower, such that the affected legs can be coordinated with the healthy legs with the assistance of the exoskeleton robot. In order to achieve immersive training during the physical therapy, the human-in-the-loop controller is developed. Furthermore, a region-based barrier Lyapunov function (BLF) is designed to separate the task workspace of the exoskeleton into a human region and a robot region, enabling the human leg to follow the desired motion trajectory in a compliant region, and the motion control of the exoskeleton is determined by humans; while in the robot region, the exoskeleton dominates the movement of human subjects. In order to make the motion control transit smoothly between the robot region and the human region, an adaptive controller is exploited to counteract the system’s nonlinear uncertainties. Both the theoretical analysis and experimental results support the effectiveness and practicability on hemiplegic patients of our control strategy.},
  archive      = {J_TCDS},
  author       = {Dong Wei and Zhijun Li and Qiang Wei and Hang Su and Bo Song and Wei He and Jianqiang Li},
  doi          = {10.1109/TCDS.2019.2954289},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {57-66},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Human-in-the-loop control strategy of unilateral exoskeleton robots for gait rehabilitation},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locomotion mode identification and gait phase estimation for
exoskeletons during continuous multilocomotion tasks. <em>TCDS</em>,
<em>13</em>(1), 45–56. (<a
href="https://doi.org/10.1109/TCDS.2019.2933648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait phase estimation is important technology in controlling the exoskeleton robot to assist elderly walking. Several kinds of Gait estimation methods have been proposed, however, the previously proposed methods were mainly aiming at one kind of walking task, e.g., level ground walking. There are only a few studies aiming at continuous gait phase estimation during continuous multilocomotion tasks. In this article, we design a continuous gait phase estimator based on adaptive oscillator (AO) network. In order to overcome the problem that the traditional AO does not converge or converges slowly when the gait task is switching, a new structure of gait phase estimator, including a gait tasks classifier, an AO reset, a peak detector, and a model-based (MB) transition gait phase estimator is designed to improve the performance of AOs network. The switching unit is designed to reorganize the output gait phase. Considering the stabilization of the sensors in continuous multilocomotion tasks, the gait tasks classifier only utilizes the angle of hip joints. The results show that the constructed classifier has similar performance to other gait tasks classifiers and requires minimum sensing sources. The continuous gait phase estimation results during continuous multilocomotion tasks show that the proposed method has better performance than the traditional AO and the AO network with self-designed reset.},
  archive      = {J_TCDS},
  author       = {Xinyu Wu and Yue Ma and Xu Yong and Chao Wang and Yong He and Nan Li},
  doi          = {10.1109/TCDS.2019.2933648},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {45-56},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Locomotion mode identification and gait phase estimation for exoskeletons during continuous multilocomotion tasks},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational modeling of emotion-motivated decisions for
continuous control of mobile robots. <em>TCDS</em>, <em>13</em>(1),
31–44. (<a href="https://doi.org/10.1109/TCDS.2019.2963545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immediate rewards are usually very sparse in the real world, which brings a great challenge to plain learning methods. Inspired by the fact that emotional reactions are incorporated into the computation of subjective value during decision-making in humans, an emotion-motivated decision-making framework is proposed in this article. Specifically, we first build a brain-inspired computational model of amygdala–hippocampus interaction to generate emotional reactions. The intrinsic emotion derives from the external reward and episodic memory and represents three psychological states: 1) valence; 2) novelty; and 3) motivational relevance. Then, a model-based (MB) decision-making approach with emotional intrinsic rewards is proposed to solve the continuous control problem of mobile robots. This method can execute online MB control with the constraint of the model-free policy and global value function, which is conducive to getting a better solution with a faster policy search. The simulation results demonstrate that the proposed approach has higher learning efficiency and maintains a higher level of exploration, especially, in some very sparse-reward environments.},
  archive      = {J_TCDS},
  author       = {Xiao Huang and Wei Wu and Hong Qiao},
  doi          = {10.1109/TCDS.2019.2963545},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {31-44},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Computational modeling of emotion-motivated decisions for continuous control of mobile robots},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning bodily expression of emotion for social robots
through human interaction. <em>TCDS</em>, <em>13</em>(1), 16–30. (<a
href="https://doi.org/10.1109/TCDS.2020.3005907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human facial and bodily expressions play a crucial role in human–human interaction to convey the communicator’s feelings. Being echoed by the influence of human social behavior, recent studies in human–robot interaction (HRI) have investigated how to generate emotional behaviors for social robots. Emotional behaviors can enhance user engagement, allowing the user to interact with robots in a transparent manner. However, they are ambiguous and affected by many factors, such as personality traits, cultures, and environments. This article focuses on developing the robot’s emotional bodily expressions adopting the user’s affective gestures. We propose the behavior selection and transformation model, enabling the robots to incrementally learn from the user’s gestures, to select the user’s habitual behaviors, and to transform the selected behaviors into robot motions. The experimental results under several scenarios showed that the proposed incremental learning model endows a social robot with the capability of entering into a positive, long-lasting HRI. We have also confirmed that the robot can express emotions through the imitated motions of the user. The robot’s emotional gestures that reflected the interacting partner’s traits were widely accepted within the same cultural group, and perceptible across different cultural groups in different ways.},
  archive      = {J_TCDS},
  author       = {Nguyen Tan Viet Tuyen and Armagan Elibol and Nak Young Chong},
  doi          = {10.1109/TCDS.2020.3005907},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {16-30},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Learning bodily expression of emotion for social robots through human interaction},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An unsupervised approach for knowledge construction applied
to personal robots. <em>TCDS</em>, <em>13</em>(1), 6–15. (<a
href="https://doi.org/10.1109/TCDS.2020.2983406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The employment of personal robots or service robots has aroused much interest in recent years with an amazing growth of robotics in different domains. Although sophisticated humanoid robots have been developed, much more effort is needed for improving their cognitive capabilities. Interactions with humans and/or with other agents are still limited and not considered satisfactory. So, the way we store and represent knowledge in a cognitive architecture is fundamental in order to overcome these limitations and improve the human-machine and machine-machine interactions. In this article, we propose an unsupervised approach for knowledge construction based on the robot&#39;s perception. Our approach makes use of Kohonen maps as an unsupervised machine learning technique and allows the definition of semantic clusters from visual features perceived by the robot. Besides, a multimedia graph knowledge base using a pure formalism is presented, which can be actively used by personal robots in their classic activities, such as environment exploration or information gathering, to represent and share the acquired knowledge, linking it to abstract concepts gifted with semantic relations.},
  archive      = {J_TCDS},
  author       = {Cristiano Russo and Kurosh Madani and Antonio Maria Rinaldi},
  doi          = {10.1109/TCDS.2020.2983406},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {6-15},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An unsupervised approach for knowledge construction applied to personal robots},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: Special issue on human-friendly cognitive
robotics. <em>TCDS</em>, <em>13</em>(1), 2–5. (<a
href="https://doi.org/10.1109/TCDS.2020.3044691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in robotics and information technology have been applied to robots to improve their perception, cognition, learning, and control abilities to enhance the performance [items 1)–6) in the Appendix]. A challenging issue on robotics research is to make the robots working around with human more flexibly and safely. To address the challenge, this special issue focuses on the latest advances in the area of human-friendly cognitive robotics, particularly, the research concentrating on methodologies, computational models, and technologies to enhance the cognitive functionalities and intelligence of human–robot systems for better interaction performance. The development of these technologies aims to bring innovations into human–robot systems.},
  archive      = {J_TCDS},
  author       = {Chenguang Yang and Yan Wu and Fanny Ficuciello and Xin Wang and Angelo Cangelosi},
  doi          = {10.1109/TCDS.2020.3044691},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  number       = {1},
  pages        = {2-5},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial: Special issue on human-friendly cognitive robotics},
  volume       = {13},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
