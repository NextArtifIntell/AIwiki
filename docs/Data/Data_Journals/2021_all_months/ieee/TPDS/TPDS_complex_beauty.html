<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpds---226">TPDS - 226</h2>
<ul>
<li><details>
<summary>
<p>(2021). Modeling and analyzing waiting policies for cloud-enabled
schedulers. <em>TPDS</em>, <em>32</em>(12), 3081–3100. (<a
href="https://doi.org/10.1109/TPDS.2021.3086270">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud platforms have popularized the Infrastructure-as-a-Service (IaaS) purchasing model, which enables users to rent computing resources on demand to execute their jobs. However, buying fixed resources is still much cheaper than renting if their resource utilization is high. Thus, to optimize cost, users must decide how many fixed resources to provision versus rent “on demand” based on their workload. In this article, we introduce the concept of a waiting policy for cloud-enabled schedulers and show that the optimal cost depends on it. The waiting policy explicitly controls how long jobs wait for resources, as jobs never need to wait, since cloud platforms provide the illusion of infinite scalability. A waiting policy is the dual of a scheduling policy: while a scheduling policy determines which jobs should run when fixed resources are available, a waiting policy determines which jobs should wait when fixed resources are not available. We define multiple waiting policies and develop simple and general analytical models to reveal their tradeoff between fixed resource provisioning, cost, and job waiting time. We evaluate the impact of different waiting policies on a real year-long batch workload consisting of 14M jobs run on a 14.3k-core cluster. We show that a compound waiting policy, which forces jobs with long running times or short waiting times to wait for fixed resources, offers the best tradeoff. The policy decreases both the cost (by 5 percent) and mean job waiting time (by 7×) compared to the current cluster, and also decreases the cost (by 43 percent) compared to renting on-demand resources for a modest increase in mean job waiting time (at 1.74 hours).},
  archive      = {J_TPDS},
  author       = {Pradeep Ambati and Noman Bashir and David Irwin and Prashant Shenoy},
  doi          = {10.1109/TPDS.2021.3086270},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3081-3100},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Modeling and analyzing waiting policies for cloud-enabled schedulers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). An elastic task scheduling scheme on coarse-grained
reconfigurable architectures. <em>TPDS</em>, <em>32</em>(12), 3066–3080.
(<a href="https://doi.org/10.1109/TPDS.2021.3084804">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-grained reconfigurable architectures (CGRAs) are increasingly employed as domain-specific accelerators due to their efficiency and flexibility. A CGRA typically relies on compilers to perform task scheduling. The longstanding problem of static scheduling is that it suffers from insufficient parallelism in handling irregularities due to over-serialization and workload imbalance, which leads to severe resource underutilization and performance loss. To counteract the limitations of static scheduling in CGRAs, it is essential to exploit dynamic parallelism automatically and manage hardware resources adaptively. However, existing dynamic scheduling mechanisms, e.g., work stealing, often reschedule aggressively for instant performance but sacrifice efficiency, which is unfavorable to CGRAs that emphasize efficiency and fewer reconfigurations. This article proposes an elastic task scheduling scheme that enables lightweight dynamic scheduling in CGRAs. Tasks are rescheduled at runtime according to the classic tagged-token dataflow paradigm to enable dynamic task-level parallelism. Meanwhile, tasks are dynamically resized according to run-time throughputs via duplication, combination, and substitution operators for balanced multitask execution. We implement the elastic task scheduling scheme on a well-known reconfigurable architecture - triggered instruction architecture (TIA). Evaluation on the MachSuite benchmarks shows that the proposed scheme is effective in improving performance and energy efficiency. The average speedup is 2× over the baseline. Also, our design attains a 57 percent improvement in the area-normalized performance and a 49 percent better energy efficiency. Compared with a state-of-the-art dynamic scheduling method, our scheme achieves 1.6× speedup and 1.6× energy efficiency than work-stealing mechanism on the same substrate.},
  archive      = {J_TPDS},
  author       = {Longlong Chen and Jianfeng Zhu and Yangdong Deng and Zhaoshi Li and Jian Chen and Xiaowei Jiang and Shouyi Yin and Shaojun Wei and Leibo Liu},
  doi          = {10.1109/TPDS.2021.3084804},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3066-3080},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An elastic task scheduling scheme on coarse-grained reconfigurable architectures},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Parallel fine-grained comparison of long DNA sequences in
homogeneous and heterogeneous GPU platforms with pruning. <em>TPDS</em>,
<em>32</em>(12), 3053–3065. (<a
href="https://doi.org/10.1109/TPDS.2021.3084069">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallelization of Smith-Waterman (SW) sequence comparison tools for long DNA sequences has been a big challenge over the years, requesting the use of several devices and sophisticated optimizations. Pruning is one of these optimizations, which can reduce considerably the amount of computation. This article proposes MultiBP, a sequence comparison solution in multiple GPUs with block pruning. Two MultiBP strategies are proposed. In static score-sharing, workload is statically distributed to the GPUs, and the best score is sent to neighbor GPUs to simulate a global view. In the dynamic strategy, execution is divided into cycles and workload is dynamically assigned, according to the GPUs processing rate. MultiBP was integrated to MASA-CUDAlign and tested in homogeneous and heterogeneous platforms, with different NVidia GPU architectures. The best results in our homogeneous and heterogeneous platforms were mostly obtained by the static and dynamic approaches, respectively. We also show that our decision module is able to select the best strategy in most cases. Finally, the comparison of the human and chimpanzee chromosomes 1 in a cluster with 512 V100 NVidia GPUs took 11 minutes and obtained the impressive rate of 82,822 GCUPS (Billions of Cells Updated per Second) which is, to our knowledge, the best performance for SW tools in GPUs.},
  archive      = {J_TPDS},
  author       = {Marco Figueiredo and Joao Paulo Navarro and Edans F. O. Sandes and George Teodoro and Alba C. M. A. Melo},
  doi          = {10.1109/TPDS.2021.3084069},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3053-3065},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel fine-grained comparison of long DNA sequences in homogeneous and heterogeneous GPU platforms with pruning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Logically parallel communication for fast MPI+threads
applications. <em>TPDS</em>, <em>32</em>(12), 3038–3052. (<a
href="https://doi.org/10.1109/TPDS.2021.3075157">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supercomputing applications are increasingly adopting the MPI+threads programming model over the traditional “MPI everywhere” approach to better handle the disproportionate increase in the number of cores compared with other on-node resources. In practice, however, most applications observe a slower performance with MPI+threads primarily because of poor communication performance. Recent research efforts on MPI libraries address this bottleneck by mapping logically parallel communication, that is, operations that are not subject to MPI&#39;s ordering constraints to the underlying network parallelism. Domain scientists, however, typically do not expose such communication independence information because the existing MPI-3.1 standard&#39;s semantics can be limiting. Researchers had initially proposed user-visible endpoints to combat this issue, but such a solution requires intrusive changes to the standard (new APIs). The upcoming MPI-4.0 standard, on the other hand, allows applications to relax unneeded semantics and provides them with many opportunities to express logical communication parallelism. In this article, we show how MPI+threads applications can achieve high performance with logically parallel communication. Through application case studies, we compare the capabilities of the new MPI-4.0 standard with those of the existing one and user-visible endpoints (upper bound). Logical communication parallelism can boost the overall performance of an application by over 2×.},
  archive      = {J_TPDS},
  author       = {Rohit Zambre and Damodar Sahasrabudhe and Hui Zhou and Martin Berzins and Aparna Chandramowlishwaran and Pavan Balaji},
  doi          = {10.1109/TPDS.2021.3075157},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3038-3052},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Logically parallel communication for fast MPI+Threads applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A unified framework for flexible playback latency control
in live video streaming. <em>TPDS</em>, <em>32</em>(12), 3024–3037. (<a
href="https://doi.org/10.1109/TPDS.2021.3083202">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live video streaming has seen tremendous growth in the past decade. An important fact in live streaming is that the demand for low playback-latency inherently conflicts with the desire for high QoE. This requires different types of live services to seek different latency-QoE tradeoffs according to their service-requirements. However, our investigations revealed that it is fundamentally difficult for existing streaming algorithms to keep consistent latency in changing network conditions, let alone achieve the service-desired latency-QoE tradeoff. To tackle the challenge, this article develops a novel framework called Flexible Latency Aware Streaming (FLAS) that not only can achieve consistent low latency, but also control the latency-QoE tradeoff flexibly. Specifically, FLAS generates a set of adaptation logics offline, each optimized for a candidate tradeoff point, then selects the most appropriate one to run online. We first show how FLAS can be applied to optimizing the existing algorithms, then developed a novel Genetic Programming approach to fully exploit FLAS&#39;s potential. Extensive evaluations show that FLAS can precisely control latency all the way down to 1s and achieve substantially higher QoE than state-of-the-arts. FLAS can be readily implemented into real streaming platforms, offering a practical and reliable solution for live-streaming services.},
  archive      = {J_TPDS},
  author       = {Guanghui Zhang and Jack Y. B. Lee and Ke Liu and Haibo Hu and Vaneet Aggarwal},
  doi          = {10.1109/TPDS.2021.3083202},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3024-3037},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A unified framework for flexible playback latency control in live video streaming},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Self-stabilizing population protocols with global
knowledge. <em>TPDS</em>, <em>32</em>(12), 3011–3023. (<a
href="https://doi.org/10.1109/TPDS.2021.3076769">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the population protocol model, many problems cannot be solved in a self-stabilizing manner. However, global knowledge, such as the number of nodes in a network, sometimes enables the design of a self-stabilizing protocol for such problems. For example, it is known that we can solve the self-stabilizing leader election in complete graphs if and only if every node knows the exact number of nodes. In this article, we investigate the effect of global knowledge on the possibility of self-stabilizing population protocols in arbitrary graphs. Specifically, we clarify the solvability of the leader election problem, the ranking problem, the degree recognition problem, and the neighbor recognition problem by self-stabilizing population protocols with knowledge of the number of nodes and/or the number of edges in a network.},
  archive      = {J_TPDS},
  author       = {Yuichi Sudo and Masahiro Shibata and Junya Nakamura and Yonghwan Kim and Toshimitsu Masuzawa},
  doi          = {10.1109/TPDS.2021.3076769},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {3011-3023},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Self-stabilizing population protocols with global knowledge},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). ReliableBox: Secure and verifiable cloud storage with
location-aware backup. <em>TPDS</em>, <em>32</em>(12), 2996–3010. (<a
href="https://doi.org/10.1109/TPDS.2021.3080594">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the prevalent cloud storage platforms are offering convenient services in support of diverse data-driven applications for clients, various security concerns raise in terms of data confidentiality, availability, and retrievability. Among them, servers&#39; dishonesty on the location-specific data backup becomes a serious concern when the data stands out clients&#39; control, considering the strict regulations imposed by many governments and organizations on data storage location. This article studies location-aware data backup verification for the data stored in clouds and aims to design a secure framework, named as ReliableBox, enabling the clients to verify if their data have been backed up on the remote servers with specific geolocation. In the design of ReliableBox, we leverage the prominent proof-of-storage techniques for data possession proof, and take advantage of multilateration geolocation and Intel SGX for the precise communication delay measurement and trust computing delay measurement, respectively. In ReliableBox, a client first computes integrity tags for the files and then outsources both the files and tags to the cloud storage server. In the later attestation, with the precise network delay and distance measurement from location-known verifiers, the client verifies that the outsourced files are intact and backed-up to hosts at the specific geolocation. With the customized design, ReliableBox can support the security needs in terms of both data integrity and backup location verification for clients, even when there exists potential dishonest cloud service providers who may manipulate the network delays or forge verification proofs. We provide security analysis to show the security property of ReliableBox in terms of data access, confidentiality, and verifications. In the end, we implement the system prototype and deploy it into several prevalent and commercial cloud platforms for performance evaluation. The experimental results demonstrate that ReliableBox is secure in support of data integrity checking and location-aware backup auditing, while it is robust to the data possession and location spoofing attacks.},
  archive      = {J_TPDS},
  author       = {Tao Jiang and Wenjuan Meng and Xu Yuan and Liangmin Wang and Jianhua Ge and Jianfeng Ma},
  doi          = {10.1109/TPDS.2021.3080594},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2996-3010},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ReliableBox: Secure and verifiable cloud storage with location-aware backup},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Fast, accurate processor evaluation through
heterogeneous, sample-based benchmarking. <em>TPDS</em>,
<em>32</em>(12), 2983–2995. (<a
href="https://doi.org/10.1109/TPDS.2021.3080702">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance evaluation is a key task in computing and communication systems. Benchmarking is one of the most common techniques for evaluation purposes, where the performance of a set of representative applications is used to infer system responsiveness in a general usage scenario. Unfortunately, most benchmarking suites are limited to a reduced number of applications, and in some cases, rigid execution configurations. This makes it hard to extrapolate performance metrics for a general-purpose architecture, supposed to have a multi-year lifecycle, running dissimilar applications concurrently. The main culprit of this situation is that current benchmark-derived metrics lack generality, statistical soundness and fail to represent general-purpose environments. Previous attempts to overcome these limitations through random app mixes significantly increase computational cost (workload population shoots up), making the evaluation process barely affordable. To circumvent this problem, in this article we present a more elaborate performance evaluation methodology named BenchCast. Our proposal provides more representative performance metrics, but with a drastic reduction of computational cost, limiting app execution to a small and representative fraction marked through code annotation. Thanks to this labeling and making use of synchronization techniques, we generate heterogeneous workloads where every app runs simultaneously inside its Region Of Interest, making a few execution seconds highly representative of full application execution.},
  archive      = {J_TPDS},
  author       = {Pablo Prieto and Pablo Abad and Jose Angel Gregorio and Valentin Puente},
  doi          = {10.1109/TPDS.2021.3080702},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2983-2995},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast, accurate processor evaluation through heterogeneous, sample-based benchmarking},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Scalable energy games solvers on GPUs. <em>TPDS</em>,
<em>32</em>(12), 2970–2982. (<a
href="https://doi.org/10.1109/TPDS.2021.3080925">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the consumption of limited resources, e.g., time or energy, plays a central role on the design of reactive systems such as embedded controllers. To this aim, quantitative objectives are defined on game arenas that can be easily modeled as weighted graphs. Instances of these games, called energy games , can be solved in ${\mathcal {O}(\vert {E}\vert {\cdot }\vert {V}\vert {\cdot }W)}$ where $W$ is the maximum weight. Recent work has demonstrated that sequential implementations hardly solve practical instances due to their size and the number of interactions required to converge to a solution. Recent work has demonstrated that sequential implementations hardly solve practical instances. Furthermore, emerging approaches, that have investigated the parallelism of CPUs multi-core and GPU for solving the initial credit problem for energy games, still perform poorly due to the non-trivial characteristics of these graphs. In this article we first describe a revised version of the algorithm on multi-core CPU that obtains a faster convergence time on real-world graphs with up to 30x against the serial implementation by showing good scalability overall. Second, we provide a new GPU-based parallel implementation based on warp-level primitives that allows to reduce the time-to-solution on several instances with up to 3.6x of speed-up against traditional parallel vertex-based approaches. We also discuss a methodology to build synthetic energy games to validate the scalability of parallel algorithms on two totally different settings.},
  archive      = {J_TPDS},
  author       = {Andrea Formisano and Raffaella Gentilini and Flavio Vella},
  doi          = {10.1109/TPDS.2021.3080925},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2970-2982},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Scalable energy games solvers on GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Towards efficient distributed subgraph enumeration via
backtracking-based framework. <em>TPDS</em>, <em>32</em>(12), 2953–2969.
(<a href="https://doi.org/10.1109/TPDS.2021.3076246">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding or monitoring subgraph instances that are isomorphic to a given pattern graph in a data graph is a fundamental query operation in many graph analytic applications, such as network motif mining and fraud detection. Existing distributed methods are inefficient in communication. They have to shuffle partial matching results during the distributed multiway join. The partial matching results may be much larger than the data graph itself. To overcome the drawback, we develop the Batch-BENU framework for distributed subgraph enumeration on static data graphs. Batch-BENU executes a group of local search tasks in parallel. Each task enumerates subgraphs around a vertex in the data graph, guided by a backtracking-based execution plan. To handle large-scale data graphs that may exceed the memory capacity of a single machine, Batch-BENU stores the data graph in a distributed database. Each task queries adjacency sets of the data graph on demand, shuffling the data graph instead of partial matching results. To support incremental subgraph enumeration on dynamic data graphs, we propose the Streaming-BENU framework. Streaming-BENU turns the problem of enumerating incremental matching results into enumerating all matching results of incremental pattern graphs at each time step. We implement Batch-BENU and Streaming-BENU with the local database cache and the load balance optimization to improve their efficiency. Extensive experiments show that Batch-BENU and Streaming-BENU can scale to big graphs and complex pattern graphs. They outperform the state-of-the-art distributed methods by up to one and two orders of magnitude, respectively.},
  archive      = {J_TPDS},
  author       = {Zhaokang Wang and Weiwei Hu and Guowang Chen and Chunfeng Yuan and Rong Gu and Yihua Huang},
  doi          = {10.1109/TPDS.2021.3076246},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2953-2969},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficient distributed subgraph enumeration via backtracking-based framework},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Identifying degree and sources of non-determinism in MPI
applications via graph kernels. <em>TPDS</em>, <em>32</em>(12),
2936–2952. (<a
href="https://doi.org/10.1109/TPDS.2021.3081530">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the scientific community prepares to deploy an increasingly complex and diverse set of applications on exascale platforms, the need to assess reproducibility of simulations and identify the root causes of reproducibility failures increases correspondingly. One of the greatest challenges facing reproducibility issues at exascale is the inherent non-determinism at the level of inter-process communication. The use of non-deterministic communication constructs is necessary to boost performance, but communication non-determinism can also hamper software correctness and result reproducibility. To address this challenge, we propose a software framework for identifying the percentage and sources of communication non-determinism. We model parallel executions as directed graphs and leverage graph kernels to characterize run-to-run variations in inter-process communication. We demonstrate the effectiveness of graph kernel similarity as a proxy for non-determinism, by showing that these kernels can quantify the type and degree of non-determinism present in communication patterns. To demonstrate our framework’s ability to link and quantify runtime non-determinism to root sources, demonstrate with present for an adaptive mesh refinement application, where our framework automatically quantifies the impact of function calls on non-determinism, and a Monte Carlo application, where our framework automatically quantifies the impact of parameter configurations on non-determinism.},
  archive      = {J_TPDS},
  author       = {Dylan Chapp and Nigel Tan and Sanjukta Bhowmick and Michela Taufer},
  doi          = {10.1109/TPDS.2021.3081530},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2936-2952},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Identifying degree and sources of non-determinism in MPI applications via graph kernels},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). GML: Efficiently auto-tuning flink’s configurations via
guided machine learning. <em>TPDS</em>, <em>32</em>(12), 2921–2935. (<a
href="https://doi.org/10.1109/TPDS.2021.3081600">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly popular fused batch-streaming big data framework, Apache Flink, has many performance-critical as well as untamed configuration parameters. However, how to tune them for optimal performance has not yet been explored. Machine learning (ML) has been chosen to tune the configurations for other big data frameworks (e.g., Apache Spark), showing significant performance improvements. However, it needs a long time to collect a large amount of training data by nature. In this article, we propose a guided machine learning (GML) approach to tune the configurations of Flink with significantly shorter time for collecting training data compared to traditional ML approaches. GML innovates two techniques. First, it leverages generative adversarial networks (GANs) to generate a part of training data, reducing the time needed for training data collection. Second, GML guides a ML algorithm to select configurations that the corresponding performance is higher than the average performance of random configurations. We evaluate GML on a lab cluster with 4 servers and a real production cluster in an internet company. The results show that GML significantly outperforms the state-of-the-art, DAC (Datasize-Aware-Configuration) (Z. Yu et al. 2018) for tuning the configurations of Spark, with 2.4× of reduced data collection time but with 30 percent reduced 99th percentile latency. When GML is used in the internet company, it reduces the latency by up to 57.8× compared to the configurations made by the company.},
  archive      = {J_TPDS},
  author       = {Yijin Guo and Huasong Shan and Shixin Huang and Kai Hwang and Jianping Fan and Zhibin Yu},
  doi          = {10.1109/TPDS.2021.3081600},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2921-2935},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GML: Efficiently auto-tuning flink&#39;s configurations via guided machine learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). RENDA: Resource and network aware data placement
algorithm for periodic workloads in cloud. <em>TPDS</em>,
<em>32</em>(12), 2906–2920. (<a
href="https://doi.org/10.1109/TPDS.2021.3080582">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hadoop enabled cloud platforms are gradually becoming preferred computational environment to execute scientific big data workloads in a periodic manner. However, it is observed that the default data placement approach of such cloud platforms is not the efficient one and often ends up with significant data transfer overhead leading to degradation of the overall job completion time. In this article, a Resource and Network-aware Data Placement Algorithm (RENDA) is proposed to reduce the non-local executions and thereby reduce the overall job completion time for periodic workloads in the cloud environment. The entire job execution is modeled as a two-stage execution characterized as data distribution and data processing. The RENDA reduces the time of the stages as mentioned above by estimating the heterogeneous performance of the nodes on a real-time basis followed by careful allocation of data in several installments to participating nodes. The experimental results show that the proposed RENDA algorithm consistently outperforms over the recent state-of-the-art alternatives with as much as 28 percent reduction in data transfer overhead leading to 16 percent reduction in average job completion time with 27 percent average speedup on average job execution.},
  archive      = {J_TPDS},
  author       = {Hiren Kumar Thakkar and Prasan Kumar Sahoo and Bharadwaj Veeravalli},
  doi          = {10.1109/TPDS.2021.3080582},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2906-2920},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {RENDA: Resource and network aware data placement algorithm for periodic workloads in cloud},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A quantum approach towards the adaptive prediction of
cloud workloads. <em>TPDS</em>, <em>32</em>(12), 2893–2905. (<a
href="https://doi.org/10.1109/TPDS.2021.3079341">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a novel Evolutionary Quantum Neural Network (EQNN) based workload prediction model for Cloud datacenter. It exploits the computational efficiency of quantum computing by encoding workload information into qubits and propagating this information through the network to estimate the workload or resource demands with enhanced accuracy proactively. The rotation and reverse rotation effects of the Controlled-NOT (C-NOT) gate serve activation function at the hidden and output layers to adjust the qubit weights. In addition, a Self Balanced Adaptive Differential Evolution (SB-ADE) algorithm is developed to optimize qubit network weights. The accuracy of the EQNN prediction model is extensively evaluated and compared with seven state-of-the-art methods using eight real world benchmark datasets of three different categories. Experimental results reveal that the use of the quantum approach to evolutionary neural network substantially improves the prediction accuracy up to 91.6 percent over the existing approaches.},
  archive      = {J_TPDS},
  author       = {Ashutosh Kumar Singh and Deepika Saxena and Jitendra Kumar and Vrinda Gupta},
  doi          = {10.1109/TPDS.2021.3079341},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2893-2905},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A quantum approach towards the adaptive prediction of cloud workloads},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). ARENA: Asynchronous reconfigurable accelerator ring to
enable data-centric parallel computing. <em>TPDS</em>, <em>32</em>(12),
2880–2892. (<a
href="https://doi.org/10.1109/TPDS.2021.3081074">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next generation HPC and data centers are likely to be reconfigurable and data-centric due to the trend of hardware specialization and the emergence of data-driven applications. In this article, we propose ARENA - an asynchronous reconfigurable accelerator ring architecture as a potential scenario on how the future HPC and data centers will be like. Despite using the coarse-grained reconfigurable arrays (CGRAs) as the substrate platform, our key contribution is not only the CGRA-cluster design itself, but also the ensemble of a new architecture and programming model that enables asynchronous tasking across a cluster of reconfigurable nodes, so as to bring specialized computation to the data rather than the reverse. We presume distributed data storage without asserting any prior knowledge on the data distribution. Hardware specialization occurs at runtime when a task finds the majority of data it requires are available at the present node. In other words, we dynamically generate specialized CGRA accelerators where the data reside. The asynchronous tasking for bringing computation to data is achieved by circulating the task token, which describes the dataflow graphs to be executed for a task, among the CGRA cluster connected by a fast ring network. Evaluations on a set of HPC and data-driven applications across different domains show that ARENA can provide better parallel scalability with reduced data movement (53.9 percent). Compared with contemporary compute-centric parallel models, ARENA can bring on average 4.37× speedup. The synthesized CGRAs and their task-dispatchers only occupy 2.93mm$^2$2 chip area under 45nm process technology and can run at 800MHz with on average 759.8mW power consumption. ARENA also supports the concurrent execution of multi-applications, offering ideal architectural support for future high-performance parallel computing and data analytics systems.},
  archive      = {J_TPDS},
  author       = {Cheng Tan and Chenhao Xie and Tong Geng and Andres Marquez and Antonino Tumeo and Kevin Barker and Ang Li},
  doi          = {10.1109/TPDS.2021.3081074},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2880-2892},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ARENA: Asynchronous reconfigurable accelerator ring to enable data-centric parallel computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Tardiness bounds for sporadic gang tasks under preemptive
global EDF scheduling. <em>TPDS</em>, <em>32</em>(12), 2867–2879. (<a
href="https://doi.org/10.1109/TPDS.2021.3081019">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the trend of increasing autonomy in cyber-physical systems, parallel embedded architectures have enabled devices to better handle the large streams of data and intensive computation required by such autonomous systems. However, while the explosion of highly-parallel platforms has seen a proportional growth in the number of applications/devices that utilize these platforms, the embedded systems community&#39;s understanding of how to build time-predictable, safety-critical systems with parallel platforms has not kept pace. As a well-motivated but challenging parallel scheduling model, gang scheduling requires all parallel threads of each parallel task to simultaneously execute in unison, which is in contrast to traditional, multi-threaded parallel scheduling, where a parallel task may spawn multiple threads, and each thread will be scheduled independently of other threads of the same task. While increasing research efforts on hard real-time (HRT) gang scheduling have recently been seen, the problem of gang scheduling in the context of soft real-time (SRT) systems, where provably bounded deadline tardiness can be tolerated, has hardly been studied yet. In this article, we derive and prove the first tardiness bounds for sporadic gang task systems under preemptive GEDF scheduling. A total utilization bound for SRT-schedulability is required for ensuring such tardiness bounds but it is shown to be tight with respect to the platform capacity and maximum parallelism-induced idleness. Furthermore, we also empirically evaluate the effects of different degrees of task parallelism upon the SRT-schedulability.},
  archive      = {J_TPDS},
  author       = {Zheng Dong and Kecheng Yang and Nathan Fisher and Cong Liu},
  doi          = {10.1109/TPDS.2021.3081019},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2867-2879},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Tardiness bounds for sporadic gang tasks under preemptive global EDF scheduling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Architectural adaptation and performance-energy
optimization for CFD application on AMD EPYC rome. <em>TPDS</em>,
<em>32</em>(12), 2852–2866. (<a
href="https://doi.org/10.1109/TPDS.2021.3078153">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advantages of the second-generation AMD EPYC Rome processors can be successfully used in the race to Exascale. However, the novel architecture&#39;s complexity makes it challenging to adapt demanding scientific codes - like stencil ones - to platforms with Rome CPUs. This article tackles this challenge by exploring the adaptation of the stencil-based CFD (computational fluid dynamics) application called MPDATA to these processors&#39; influential features. We show that the previously proposed parametric adaptation methodology can be profitably applied to extend the performance portability of the memory-bound MPDATA on the AMD EPYC architecture. The extension of the parametric adaptation on the novel architecture requires careful consideration of two relevant aspects that reflect splitting the Rome architecture into multiple dies - features of the cache hierarchy and partitioning cores into work teams. The article also investigates the correlation between the performance optimizations and energy efficiency for a ccNUMA platform powered by top-of-the-line 64-core AMD Rome 7742 CPUs, comparing the results against two servers with Intel Xeon Scalable processors of different generations. Even without appealing to prices, the achieved performance and energy efficiency results are a solid argument confirming the competitiveness of AMD Rome processors against Intel Xeon CPUs in scientific applications.},
  archive      = {J_TPDS},
  author       = {Lukasz Szustak and Roman Wyrzykowski and Lukasz Kuczynski and Tomasz Olas},
  doi          = {10.1109/TPDS.2021.3078153},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {12},
  pages        = {2852-2866},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Architectural adaptation and performance-energy optimization for CFD application on AMD EPYC rome},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Multi-queue request scheduling for profit maximization in
IaaS clouds. <em>TPDS</em>, <em>32</em>(11), 2838–2851. (<a
href="https://doi.org/10.1109/TPDS.2021.3075254">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, service providers rent heterogeneous servers from cloud providers, i.e., Infrastructure as a Service (IaaS), to meet requests of consumers. The heterogeneity of servers and impatience of consumers pose great challenges to service providers for profit maximization. In this article, we transform this problem into a multi-queue model where the optimal expected response time of each queue is theoretically analyzed. A multi-queue request scheduling algorithm framework is proposed to maximize the total profit of service providers, which consists of three components: request stream splitting, requests allocation, and server assignment. A request stream splitting algorithm is designed to split the arriving requests to minimize the response time in the multi-queue system. An allocation algorithm, which adopts a one-step improvement strategy, is developed to further optimize the response time of the requests. Furthermore, an algorithm is developed to determine the appropriate number of required servers of each queue. After statistically calibrating parameters and algorithm components over a comprehensive set of random instances, the proposed algorithms are compared with the state-of-the-art over both simulated and real-world instances. The results indicate that the proposed multi-queue request scheduling algorithm outperforms the other algorithms with acceptable computational time.},
  archive      = {J_TPDS},
  author       = {Shuang Wang and Xiaoping Li and Quan Z. Sheng and Rubén Ruiz and Jinquan Zhang and Amin Beheshti},
  doi          = {10.1109/TPDS.2021.3075254},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2838-2851},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-queue request scheduling for profit maximization in IaaS clouds},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). An incremental iterative acceleration architecture in
distributed heterogeneous environments with GPUs for deep learning.
<em>TPDS</em>, <em>32</em>(11), 2823–2837. (<a
href="https://doi.org/10.1109/TPDS.2021.3078254">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallel computing capabilities of GPUs have a significant impact on computationally intensive iterative tasks. Offloading part or all of the deep learning tasks from the CPU to the GPU for execution is mainstream. However, a large number of redundant iterative calculations exist in the iterative process of computing tasks. Therefore, we propose a GPU-based distributed incremental iterative computing architecture that can make full use of distributed parallel computing and GPU memory structure. The architecture supports deep learning and other computationally intensive iterative applications by optimizing data placement and reducing redundant iterative calculations. To support block-based data partitioning and coalesced memory access on GPUs, we propose GDataSet, an abstract data set. The GPU incremental iteration manager called GTracker is designed to be responsible for GDataSet cache management on the GPU. In order to solve the limitation of on-chip memory size, we propose a variable sliding window mechanism. It improves the hit rate of cache access and the speed of data access by realizing the best block arrangement between on-chip memory and off-chip memory. Besides, a communication channel based on an incremental iterative model is designed to support data transmission and task communication in cluster computing. Finally, we implement the proposed architecture based on Spark 2.4.1 and CUDA 10.0. Comparative experiments with widely used computationally intensive iterative applications (K-means, LSTM, etc.) show that the incremental iterative acceleration architecture can significantly improve the efficiency of iterative computing.},
  archive      = {J_TPDS},
  author       = {Xuedong Zhang and Zhuo Tang and Lifan Du and Li Yang},
  doi          = {10.1109/TPDS.2021.3078254},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2823-2837},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An incremental iterative acceleration architecture in distributed heterogeneous environments with GPUs for deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A split execution model for SpTRSV. <em>TPDS</em>,
<em>32</em>(11), 2809–2822. (<a
href="https://doi.org/10.1109/TPDS.2021.3074501">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Triangular Solve (SpTRSV) is an important and extensively used kernel in scientific computing. Parallelism within SpTRSV depends upon matrix sparsity pattern and, in many cases, is non-uniform from one computational step to the next. In cases where the SpTRSV computational steps have contrasting parallelism characteristics- some steps are more parallel, others more sequential in nature, the performance of an SpTRSV algorithm may be limited by the contrasting parallelism characteristics. In this work, we propose a split-execution model for SpTRSV to automatically divide SpTRSV computation into two sub-SpTRSV systems and an SpMV, such that one of the sub-SpTRSVs has more parallelism than the other. Each sub-SpTRSV is then computed using different SpTRSV algorithms, which are possibly executed on different platforms (CPU or GPU). By analyzing the SpTRSV Directed Acyclic Graph (DAG) and matrix sparsity features, we use a heuristics-based approach to (i) automatically determine the suitability of an SpTRSV for split-execution, (ii) find the appropriate split-point, and (iii) execute SpTRSV in a split fashion using two SpTRSV algorithms while managing any required inter-platform communication. Experimental evaluation of the execution model on two CPU-GPU machines with a matrix dataset of 327 matrices from the SuiteSparse Matrix Collection shows that our approach correctly selects the fastest SpTRSV method (split or unsplit) for 88 percent of matrices on the Intel Xeon Gold (6148) + NVIDIA Tesla V100 and 83 percent on the Intel Core I7 + NVIDIA G1080 Ti platform achieving speedups up to 10x and 6.36x respectively.},
  archive      = {J_TPDS},
  author       = {Najeeb Ahmad and Buse Yilmaz and Didem Unat},
  doi          = {10.1109/TPDS.2021.3074501},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2809-2822},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A split execution model for SpTRSV},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Efficient virtual network embedding of cloud-based data
center networks into optical networks. <em>TPDS</em>, <em>32</em>(11),
2793–2808. (<a
href="https://doi.org/10.1109/TPDS.2021.3075296">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for data center bandwidth has exploded due to the continuous development of cloud computing, causing the use of network resources close to saturation. Optical network has become an encouraging technology for many burgeoning networks and parallel/distributed computing applications because of its huge bandwidth. This article focuses on efficient embedding of data centers into optical networks, which aims to reduce complexity of the network topology by using the parallel transmission characteristics of optical fiber. We first present a novel virtual network embedding (VNE) mathematical model used for optical data center networks. Then we derive a priority of location VNE algorithm according to node proximity sensing and path comprehensive evaluation. Furthermore, we propose routing and wavelength assignment for DCNs into optical networks, and identify the lower bound of the required number of wavelengths. Extensive evaluations show that the proposed embedding algorithm can reduce the average waiting time of virtual network requests by 20 percent, increase the request acceptance rate and revenue-overhead ratio by 13 percent, as compared to the latest VNE algorithm.},
  archive      = {J_TPDS},
  author       = {Weibei Fan and Fu Xiao and Xiaobai Chen and Lei Cui and Shui Yu},
  doi          = {10.1109/TPDS.2021.3075296},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2793-2808},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient virtual network embedding of cloud-based data center networks into optical networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Offloading tasks with dependency and service caching in
mobile edge computing. <em>TPDS</em>, <em>32</em>(11), 2777–2792. (<a
href="https://doi.org/10.1109/TPDS.2021.3076687">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mobile Edge Computing (MEC), many tasks require specific service support for execution and in addition, have a dependent order of execution among the tasks. However, previous works often ignore the impact of having limited services cached at the edge nodes on (dependent) task offloading, thus may lead to an infeasible offloading decision or a longer completion time. To bridge the gap, this article studies how to efficiently offload dependent tasks to edge nodes with limited (and predetermined) service caching. We formally define the problem of offloading dependent tasks with service caching (ODT-SC), and prove that there exists no algorithm with constant approximation for this hard problem. Then, we design an efficient convex programming based algorithm (CP) to solve this problem. Moreover, we study a special case with a homogeneous MEC and propose a favorite successor based algorithm (FS) to solve this special case with a competitive ratio of O(1)O(1). Extensive simulation results using Google data traces show that our proposed algorithms can significantly reduce applications&#39; completion time by about 21-47 percent compared with other alternatives.},
  archive      = {J_TPDS},
  author       = {Gongming Zhao and Hongli Xu and Yangming Zhao and Chunming Qiao and Liusheng Huang},
  doi          = {10.1109/TPDS.2021.3076687},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2777-2792},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Offloading tasks with dependency and service caching in mobile edge computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Improved MPC algorithms for edit distance and ulam
distance. <em>TPDS</em>, <em>32</em>(11), 2764–2776. (<a
href="https://doi.org/10.1109/TPDS.2021.3076534">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edit distance is one of the most fundamental problems in combinatorial optimization to measure the similarity between strings. Ulam distance is a special case of edit distance where no character is allowed to appear more than once in a string. Recent developments have been very fruitful for obtaining fast and parallel algorithms for both edit distance and Ulam distance. In this work, we present an almost optimal MPC (massively parallel computation) algorithm for Ulam distance and improve MPC algorithms for edit distance. Our algorithm for Ulam distance is almost optimal in the sense that (1) the approximation factor of our algorithm is 1+ε1+ε, (2) the round complexity of our algorithm is constant, (3) the total memory of our algorithm is almost linear (~O ε (n)Õε(n)), and (4) the overall running time of our algorithm is almost linear which is the best known for Ulam distance. We also improve the work of Hajiaghayi et al. for edit distance in terms of total memory. The best previously known MPC algorithm for edit distance requires ~O(n 2x )Õ(n2x) machines when the memory of each machine is bounded by ~O(n 1-x )Õ(n1-x). In this work, we improve the number of machines to ~O(n (9/5)x )Õ(n(9/5)x) while keeping the memory limit intact. Moreover, the round complexity of our algorithm is constant and the total running time of our algorithm is truly subquadratic. However, our improvement comes at the expense of a constant factor in the approximation guarantee of the algorithm. This improvement is inspired by the recent techniques of Boroujeni et al. and Chakraborty et al. for obtaining truly subquadratic time algorithms for edit distance.},
  archive      = {J_TPDS},
  author       = {Mahdi Boroujeni and Mohammad Ghodsi and Saeed Seddighin},
  doi          = {10.1109/TPDS.2021.3076534},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2764-2776},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improved MPC algorithms for edit distance and ulam distance},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). WindFlow: High-speed continuous stream processing with
parallel building blocks. <em>TPDS</em>, <em>32</em>(11), 2748–2763. (<a
href="https://doi.org/10.1109/TPDS.2021.3073970">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, we are witnessing the diffusion of Stream Processing Systems (SPSs) able to analyze data streams in near realtime. Traditional SPSs like Storm and Flink target distributed clusters and adopt the continuous streaming model , where inputs are processed as soon as they are available while outputs are continuously emitted. Recently, there has been a great focus on SPSs for scale-up machines. Some of them (e.g., BriskStream ) still use the continuous model to achieve low latency. Others optimize throughput with batching approaches that are, however, often inadequate to minimize latency for live-streaming applications. Our contribution is to show a novel software engineering approach to design the runtime system of SPSs targeting multicores, with the aim of providing a uniform solution able to optimize throughput and latency. The approach has a formal nature based on the assembly of components called building blocks , whose composition allows optimizations to be easily expressed in a compositional manner. We use this methodology to build a new SPS called WindFlow . Our evaluation showcases the benefits of WindFlow : it provides lower latency than SPSs for continuous streaming, and can be configured to optimize throughput, to perform similarly and even better than batch-based scale-up SPSs.},
  archive      = {J_TPDS},
  author       = {Gabriele Mencagli and Massimo Torquati and Andrea Cardaci and Alessandra Fais and Luca Rinaldi and Marco Danelutto},
  doi          = {10.1109/TPDS.2021.3073970},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2748-2763},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {WindFlow: High-speed continuous stream processing with parallel building blocks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Memory-side prefetching scheme incorporating dynamic page
mode in 3D-stacked DRAM. <em>TPDS</em>, <em>32</em>(11), 2734–2747. (<a
href="https://doi.org/10.1109/TPDS.2020.3044856">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern multiprocessor systems running multiple applications concurrently exhibit irregular memory access pattern during different phases of execution. The principle of locality is hard to exploit in the presence of such irregular memory requests and may result in additional delays due to resource conflicts throughout memory hierarchy. Prefetching is a promising technique to reduce the memory access latency where data is speculatively fetched ahead of time and stored in a faster memory structure like cache or dedicated prefetch buffer. The emergence of 3D-stacked DRAM provides huge internal bandwidth that makes memory-side prefetching an effective approach to improving system performance. Leveraging the unique architecture of 3D-stacked DRAM, we introduce a memory-side prefetching scheme that works in conjunction with dynamic page mode to reduce memory access latency. We introduce a novel prefetch buffer management scheme that makes intelligent replacement decision based on the utilization and recency of the prefetched data, which also serves as a guidance for future prefetching. Simulation results indicate that our approach improves performance by 21.8 percent on average, compared to a baseline scheme that prefetches a whole row on consecutive hits and implements static open page policy. Our scheme also outperforms an existing memory-side prefetching scheme by 13.2 percent on average, which dynamically adjusts the prefetch degree based on the usefulness of prefetched data.},
  archive      = {J_TPDS},
  author       = {Muhammad M. Rafique and Zhichun Zhu},
  doi          = {10.1109/TPDS.2020.3044856},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2734-2747},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Memory-side prefetching scheme incorporating dynamic page mode in 3D-stacked DRAM},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). High performance multivariate geospatial statistics on
manycore systems. <em>TPDS</em>, <em>32</em>(11), 2719–2733. (<a
href="https://doi.org/10.1109/TPDS.2021.3071423">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and inferring spatial relationships and predicting missing values of environmental data are some of the main tasks of geospatial statisticians. These routine tasks are accomplished using multivariate geospatial models and the cokriging technique. The latter requires the evaluation of the expensive Gaussian log-likelihood function, which has impeded the adoption of multivariate geospatial models for large multivariate spatial datasets. However, this large-scale cokriging challenge provides a fertile ground for supercomputing implementations for the geospatial statistics community as it is paramount to scale computational capability to match the growth in environmental data coming from the widespread use of different data collection technologies. In this article, we develop and deploy large-scale multivariate spatial modeling and inference on parallel hardware architectures. To tackle the increasing complexity in matrix operations and the massive concurrency in parallel systems, we leverage low-rank matrix approximation techniques with task-based programming models and schedule the asynchronous computational tasks using a dynamic runtime system. The proposed framework provides both the dense and the approximated computations of the Gaussian log-likelihood function. It demonstrates accuracy robustness and performance scalability on a variety of computer systems. Using both synthetic and real datasets, the low-rank matrix approximation shows better performance compared to exact computation, while preserving the application requirements in both parameter estimation and prediction accuracy. We also propose a novel algorithm to assess the prediction accuracy after the online parameter estimation. The algorithm quantifies prediction performance and provides a benchmark for measuring the efficiency and accuracy of several approximation techniques in multivariate spatial modeling.},
  archive      = {J_TPDS},
  author       = {Mary Lai O. Salvaña and Sameh Abdulah and Huang Huang and Hatem Ltaief and Ying Sun and Marc G. Genton and David E. Keyes},
  doi          = {10.1109/TPDS.2021.3071423},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2719-2733},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High performance multivariate geospatial statistics on manycore systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). MCFsyn: A multi-party set reconciliation protocol with
the marked cuckoo filter. <em>TPDS</em>, <em>32</em>(11), 2705–2718. (<a
href="https://doi.org/10.1109/TPDS.2021.3074440">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-party set reconciliation is a key component in distributed and networking systems. It naturally contains two dimensions, i.e., set representation and reconciliation protocol. However, existing sketch data structures are insufficient to satisfy the new needs brought by the multi-party scenario simultaneously, including space-efficiency, mergeability, and completeness. The current reconciliation protocols, on the other hand, fail to achieve the global optimization of communication cost. To this end, in this article, we propose the marked cuckoo filter (MCF), a data structure for representing set members. Grounded on MCF, we implement the MCFsyn protocol to reconcile multiple sets. MCFsyn aggregates and distributes sets information represented by MCFs along with an underlying minimum spanning tree among the participants. The participants then identify the different elements by traversing the overall MCF which contains the information of all elements in the union set. For the identified missing elements, MCFsyn helps the participants to choose the optimal senders to fetch with the minimum communication cost. Comprehensive evaluations indicate that MCFsyn significantly outperforms existing alternatives in terms of both reconciliation accuracy and communication cost.},
  archive      = {J_TPDS},
  author       = {Lailong Luo and Deke Guo and Yawei Zhao and Ori Rottenstreich and Richard T. B. Ma and Xueshan Luo},
  doi          = {10.1109/TPDS.2021.3074440},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2705-2718},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MCFsyn: A multi-party set reconciliation protocol with the marked cuckoo filter},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Timestamped state sharing for stream analytics.
<em>TPDS</em>, <em>32</em>(11), 2691–2704. (<a
href="https://doi.org/10.1109/TPDS.2021.3073253">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State access in existing distributed stream processing systems is restricted locally within each operator. However, in advanced stream analytics such as online learning and dynamic graph analytics, enabling state sharing across different operators makes application development easier and stream processing more efficient. In addition, when stream records are timestamped, proper time semantics should be defined for both state updates and fetches. We propose a new state abstraction to address the limitations of existing systems and develop a distributed stream processing system, Nova, with native support for timestamped state sharing. We validate the expressiveness and efficiency of Nova with extensive experiments.},
  archive      = {J_TPDS},
  author       = {Yunjian Zhao and Zhi Liu and Yidi Wu and Guanxian Jiang and James Cheng and Kunlong Liu and Xiao Yan},
  doi          = {10.1109/TPDS.2021.3073253},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2691-2704},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Timestamped state sharing for stream analytics},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021b). Efficient forwarding anomaly detection in
software-defined networks. <em>TPDS</em>, <em>32</em>(11), 2676–2690.
(<a href="https://doi.org/10.1109/TPDS.2021.3068135">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data centers, the critical infrastructure underpinning Cloud computing, often employ Software-Defined Networks (SDN) to manage cluster, wide-area and enterprise networks. As the network forwarding in SDN is dynamically programmed by controllers, it is crucial to ensure that the controller intent is correctly translated into underlying forwarding rules. Therefore, detecting and locating forwarding anomalies in SDN is a fundamental problem in production networks. Existing research proposals, roughly categorized into probing-based, packet piggybacking-based, and flow statistics analysis-based, either impose significant overhead or do not provide sufficient coverage for certain forwarding anomalies. In this article, we propose ${\sf FADE}$ , a controllable and passive measuring scheme to simultaneously deliver detection efficiency and accuracy. ${\sf FADE}$ first analyzes the entire network topology and flow rules, and then computes a minimal set of flows that can cover all forwarding rules. For each selected network flow, ${\sf FADE}$ decides the optimal number of monitoring positions on its path (much less than total number of hops), and installs dedicated rules to collect flow statistics. ${\sf FADE}$ controls the installation and expiration of these rules, along with unique flow labels, to guarantee the accuracy of collected statistics, based on which ${\sf FADE}$ algorithmically decides whether a forwarding anomaly is detected, and if so it further locates the anomaly. On top of ${\sf FADE}$ , we propose ${\sf iFADE}$ (a more scalable version of ${\sf FADE}$ ) to further optimize the usage and deployment of dedicated measurement rules. ${\sf iFADE}$ achieves over 40 percent rule reduction compared with ${\sf FADE}$ . We implement a prototype of both ${\sf FADE}$ and ${\sf iFADE}$ in about 12000 lines of code and evaluate the prototype extensively. The experiment results demonstrate ${\sf (i)}$ ${\sf FADE}$ and ${\sf iFADE}$ are accurate, e.g., they achieve over 95 percent true positive rate and 99 percent true negative rate in anomaly detection; ${\sf (ii)}$ ${\sf FADE}$ and ${\sf iFADE}$ are lightweight, e.g., they reduce the overhead of control messages compared with state-of-the-art by about 50 and 90 percent, respectively.},
  archive      = {J_TPDS},
  author       = {Qi Li and Yunpeng Liu and Zhuotao Liu and Peng Zhang and Chunhui Pang},
  doi          = {10.1109/TPDS.2021.3068135},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2676-2690},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient forwarding anomaly detection in software-defined networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Coflow scheduling in data centers: Routing and bandwidth
allocation. <em>TPDS</em>, <em>32</em>(11), 2661–2675. (<a
href="https://doi.org/10.1109/TPDS.2021.3068424">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed computing frameworks like MapReduce, Spark, and Dyrad, a coflow is a set of flows transferring data between two stages of a job. The job cannot start its next stage unless all flows in the coflow finish. To improve the execution performance of such a job, it is crucial to reduce the completion time of a coflow, as it can contribute more than 50 percent of the job completion time. While several coflow schedulers have been proposed, we observe that routing, as a factor greatly impacting the Coflow Completion Time (CCT), has not been well considered. In this article, we focus on the coflow scheduling problem and jointly consider routing and bandwidth allocation. We begin by providing an analytical solution to the problem of optimal bandwidth allocation with pre-determined routes. In the following, we formulate the problem of scheduling a single coflow as a Non-linear Mixed Integer Programming problem and present its relaxed convex optimization problem. We further propose two algorithms, CoRBA and its simplified version: CoRBA-fast that solve the single coflow scheduling problem with a joint consideration of routing and bandwidth allocation. Lastly, to address multiple coflows in online scheduling, we propose an online scheduler named OnCoRBA. By comparing with the start-of-the-art algorithms and schedulers via simulations, we demonstrate that CoRBA and CoRBA-fast reduce the CCT by 30-400 percent and the OnCoRBA scheduler reduces the average online CCT by 20-230 percent. In addition, CoRBA-fast can be hundreds times faster than CoRBA with around 8 percent performance degradation compared to CoRBA, which makes the use of CoRBA-fast very appropriate in practice.},
  archive      = {J_TPDS},
  author       = {Li Shi and Yang Liu and Junwei Zhang and Thomas Robertazzi},
  doi          = {10.1109/TPDS.2021.3068424},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2661-2675},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coflow scheduling in data centers: Routing and bandwidth allocation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Trust: Triangle counting reloaded on GPUs. <em>TPDS</em>,
<em>32</em>(11), 2646–2660. (<a
href="https://doi.org/10.1109/TPDS.2021.3064892">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangle counting is a building block for a wide range of graph applications. Traditional wisdom suggests that i) hashing is not suitable for triangle counting, ii) edge-centric triangle counting beats vertex-centric design, and iii) communication-free and workload balanced graph partitioning is a grand challenge for triangle counting. On the contrary, we advocate that i) hashing can help the key operations for scalable triangle counting on Graphics Processing Units (GPUs), i.e., list intersection and graph partitioning, ii) vertex-centric design reduces both hash table construction cost and memory consumption, which is limited on GPUs. In addition, iii) we exploit graph and workload collaborative, and hashing-based 2D partitioning to scale vertex-centric triangle counting over 1000 GPUs with sustained scalability. In this article, we present Trust which performs triangle counting with the hash operation and vertex-centric mechanism at the core. To the best of our knowledge, Trust is the first work that achieves over one trillion Traversed Edges Per Second (TEPS) rate for triangle counting.},
  archive      = {J_TPDS},
  author       = {Santosh Pandey and Zhibin Wang and Sheng Zhong and Chen Tian and Bolong Zheng and Xiaoye Li and Lingda Li and Adolfy Hoisie and Caiwen Ding and Dong Li and Hang Liu},
  doi          = {10.1109/TPDS.2021.3064892},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2646-2660},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Trust: Triangle counting reloaded on GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Critique of “planetary normal mode computation: Parallel
algorithms, performance, and reproducibility” by SCC team from peking
university. <em>TPDS</em>, <em>32</em>(11), 2643–2645. (<a
href="https://doi.org/10.1109/TPDS.2020.3049050">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shi et al. (2018) proposed a highly parallel polynomial filtering eigensolver for the computation of planetary normal modes. As a challenge at the Student Cluster Competition in The International Conference for High Performance Computing, Networking, Storage and Analysis (SC19), we reproduce the computational efficiency of the polynomial filtering eigensolver on our Intel Xeon machine. We present the weak scalability, scaling of runtime with model size (in a fixed interval) and the strong scalability results in this report.},
  archive      = {J_TPDS},
  author       = {Yihua Cheng and Zejia Fan and Jing Mai and Yifan Wu and Pengcheng Xu and Yuxuan Yan and Zhenxin Fu and Yun Liang},
  doi          = {10.1109/TPDS.2020.3049050},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2643-2645},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Planetary normal mode computation: Parallel algorithms, performance, and reproducibility” by SCC team from peking university},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Critique of “planetary normal mode computation: Parallel
algorithms, performance, and reproducibility” by SCC team from
university of washington. <em>TPDS</em>, <em>32</em>(11), 2639–2642. (<a
href="https://doi.org/10.1109/TPDS.2021.3051943">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the tasks for the SC19 Student Cluster Competition is to reproduce the results in the reproducibility challenge article “Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver”, by J. Shi et al., which describes a highly parallel algorithm for computing planetary normal modes. In running experiments from the article, we study the weak and strong scalability of the algorithm, as well as the relationship between model size, degree of polynomial filter, and execution time. We investigate these findings on a two-node, 64-core Intel Skylake-based Xeon cluster. Unfortunately, we are able to confirm some, but not all, of the original findings, with discrepancies possibly due to a low number of experimental runs due to competition time limits as well as nonuniform scaling of compute resources.},
  archive      = {J_TPDS},
  author       = {David Liu and Matthew Cinnamon and Thorne Garvin and Andrei Karavanov and Sungchan Park and Darius Strobeck and Andrew Lumsdaine},
  doi          = {10.1109/TPDS.2021.3051943},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2639-2642},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Planetary normal mode computation: Parallel algorithms, performance, and reproducibility” by SCC team from university of washington},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Critique of “planetary normal mode computation: Parallel
algorithms, performance, and reproducibility” by SCC team from
university of warsaw. <em>TPDS</em>, <em>32</em>(11), 2635–2638. (<a
href="https://doi.org/10.1109/TPDS.2021.3050997">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Supercomputing conference holds a student cluster competition - A competition that aims to introduce undergraduate students to the world of High Performance Computing. One of the tasks at the SC19 conference was to reproduce the scaling results obtained in the article titled Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver by Jia Shi et al. They introduced a new approach to the problem of calculating planetary normal modes. The developed method is a highly parallel algorithm that approximates the results via the mixed finite element method on unstructured tetrahedral meshes. The cluster used consisted of five 40-core Intel Xeon Cascade Lake nodes, equipped with 384 GB of RAM each. The original study was demonstrated on Stampede2 system on partitions ranging from 2 to 256 nodes with 48-core Intel Xeon Skylake and 192 GB of RAM each. Our cluster was equipped with Infiniband interconnect while Stampede2 used Intel Omni-Path network. We decided to work with HPC-X MPI library in place of Intel MPI used the reference study. The design - sizes and schedule - of experimental runs was the most challenging part of our study. We run both strong and weak scalability experiments in a one set of runs in order to fit in limited computing capacity and very tight time schedule. Due to limitations of our hardware, we had to split the weak scaling study into two separate smaller studies. We didn&#39;t manage to reproduce the exact results, however, we achieved similar scalability trend.},
  archive      = {J_TPDS},
  author       = {Marek Masiak and Iwona Kotlarska and Ukasz Kondraciuk and Maciej Szpindler},
  doi          = {10.1109/TPDS.2021.3050997},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2635-2638},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Planetary normal mode computation: Parallel algorithms, performance, and reproducibility” by SCC team from university of warsaw},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Critique of “planetary normal mode computation: Parallel
algorithms, performance, and reproducibility” by SCC team from tsinghua
university. <em>TPDS</em>, <em>32</em>(11), 2631–2634. (<a
href="https://doi.org/10.1109/TPDS.2020.3049025">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we present our results from the SC19 Student Cluster Competition Reproducibility Challenge. The challenge entails reproducing the article entitled “Computing Planetary Interior Normal Modes with A Highly Parallel Polynomial Filtering Eigensolver” presented at SC&#39;18, which proposes a parallel polynomial filtered Lanczos algorithm to directly calculate the planetary normal modes of heterogeneous planets. The proposed algorithm showed excellent performance with relatively low memory consumption and high parallel efficiency. In this work, we reproduce the scaling tests in that article on a cluster using Intel Cascade Lake architecture and use the proposed algorithm to illustrate specific normal modes of Mars. We compare the results obtained on our cluster with those in the original article. We also design a new metric to better analyze the results. In addition, we use the profiling tool Intel VTune Amplifier to explain our discoveries. Our results demonstrate that the given models show great scalability, which is similar to the original article. The required normal modes of Mars are also successfully calculated and visualized.},
  archive      = {J_TPDS},
  author       = {Chen Zhang and Chenggang Zhao and Jiaao He and Shengqi Chen and Liyan Zheng and Kezhao Huang and Wentao Han and Jidong Zhai},
  doi          = {10.1109/TPDS.2020.3049025},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2631-2634},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Planetary normal mode computation: Parallel algorithms, performance, and reproducibility” by SCC team from tsinghua university},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Critique of “planetary normal mode computation: Parallel
algorithms, performance, and reproducibility” by SCC team from ETH
zurich. <em>TPDS</em>, <em>32</em>(11), 2627–2630. (<a
href="https://doi.org/10.1109/TPDS.2021.3050500">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This report analyzes the reproducibility of the article “Computing Planetary Interior Normal Modes with A Highly Parallel Polynomial Filtering Eigensolver” by Jia Shi et al. (Shi, 2018). To reproduce the results we perform different weak and strong scaling studies using a series of Mars models. All experimental runs were performed during the SC19 Student Cluster Competition on a four node Intel Skylake cluster. We show that the findings of the original article can be reproduced in a different environment.},
  archive      = {J_TPDS},
  author       = {Manuel Burger and Jan Kleine},
  doi          = {10.1109/TPDS.2021.3050500},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2627-2630},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Planetary normal mode computation: Parallel algorithms, performance, and reproducibility” by SCC team from ETH zurich},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Critique of “planetary normal mode computation: Parallel
algorithms, performance, and reproducibility” by SCC team from national
tsing hua university. <em>TPDS</em>, <em>32</em>(11), 2623–2626. (<a
href="https://doi.org/10.1109/TPDS.2021.3051725">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special activity of the Student Cluster Competition at SC19 conference, we made an attempt to reproduce the scalability evaluations of a highly paralleled polynomial filtering eigensolver for computing planetary interior normal modes. Our experiments were conducted on a Mars dataset using a small scale 4-node cluster with Intel Skylake CPU architecture, while the original article’s were conducted on a Moon dataset using a large scale 256-node supercomputer with Intel CPU Skylake and KNL architectures. This article shares our experiences and observations from our reproducibility activity and discusses our findings on three main sections: the weak scalability, the strong scalability, and the relationships between variables. The results of weak scalability and strong scalability were successfully reproduced. But due to the differences on the problem scale, input dataset, and system architecture, different behaviors regarding the polynomial degree were observed.},
  archive      = {J_TPDS},
  author       = {Wei-Fang Sun and Hung-Hsin Chen and Shao-Fu Lin and Yuan-Ching Lin and Jing-Wei Wu and En-Te Lin and Jerry Chou},
  doi          = {10.1109/TPDS.2021.3051725},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2623-2626},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Critique of “Planetary normal mode computation: Parallel algorithms, performance, and reproducibility” by SCC team from national tsing hua university},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Planetary normal mode computation: Parallel algorithms,
performance, and reproducibility. <em>TPDS</em>, <em>32</em>(11),
2609–2622. (<a
href="https://doi.org/10.1109/TPDS.2021.3050448">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is an extension of work entitled “Computing planetary interior normal modes with a highly parallel polynomial filtering eigensolver.” by Shi et al. , [1] originally presented at the SC18 conference. A highly parallel polynomial filtered eigensolver was developed and exploited to calculate the planetary normal modes. The proposed method is ideally suited for computing interior eigenpairs for large-scale eigenvalue problems as it greatly enhances memory and computational efficiency. In this article, the second-order finite element method is used to further improve the accuracy as only the first-order finite element method was deployed in the previous work. The parallel algorithm, its parallel performance up to 20k processors, and the great computational accuracy are illustrated. The reproducibility of the previous work was successfully performed on the Student Cluster Competition at the SC19 conference by several participant teams using a completely different Mars-model dataset on different clusters. Both weak and strong scaling performances of the reproducibility by the participant teams were impressive and encouraging. The analysis and reflection of their results are demonstrated and future direction is discussed.},
  archive      = {J_TPDS},
  author       = {Jia Shi and Ruipeng Li and Yuanzhe Xi and Yousef Saad and Maarten V. de Hoop},
  doi          = {10.1109/TPDS.2021.3050448},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2609-2622},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Planetary normal mode computation: Parallel algorithms, performance, and reproducibility},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Transparency and reproducibility practice in large-scale
computational science: A preface to the special section. <em>TPDS</em>,
<em>32</em>(11), 2607–2608. (<a
href="https://doi.org/10.1109/TPDS.2021.3058393">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With this special section we bring you a practice and experience effort in transparency and reproducibility for large-scale computational science. A unique section, it consists of a research work plus six critques, each by a student team that reproduced the work. The original research work has been expanded in its science and also in its contribution to open science with a discussion of the student effort. Our letter contemplates implications as well.},
  archive      = {J_TPDS},
  author       = {Beth Plale and Stephen Lien Harrell},
  doi          = {10.1109/TPDS.2021.3058393},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2607-2608},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Transparency and reproducibility practice in large-scale computational science: A preface to the special section},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021c). Guest editorial: Special section on SC19 student cluster
competition. <em>TPDS</em>, <em>32</em>(11), 2606. (<a
href="https://doi.org/10.1109/TPDS.2021.3053641">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reproducibility is foundational to solid scientific and technical research, and the ability to repeat the research that produced is a key approach for confirming the validity of a new scientific discovery. The IEEE Transactions on Parallel and Distributed Systems (TPDS) is committed to enabling reproducible research through transparency and the availability and potential reuse of code. As part of its Reproducibility Initiative, TPDS is exploring post-publication peer review of code associated with articles published in TPDS. Authors who have published in TPDS can make their published article more reproducible and earn a reproducibility badge by submitting their associated code for post-publication peer review. However, the nature of the parallel and distributed systems research covered by TPDS makes it challenging to evaluate code and data challenging for reproducibility. This is because such an evaluation may require access to specific hardware, system architectures and scales, OS configurations, and so on, which may not be feasible or practical. Consequently, TPDS is exploring an alternate approach where members of the community can submit short, supplemental ‘critique’ papers that present their experiences in reproducing published results using the artifacts and/or evaluations or experiences with published artifacts. These supplemental paper submissions are reviewed and, if accepted, are linked to the original publication and are citable, serving to validate the reproducibility of the original publication. This special section, consisting of a primary paper and 6 critique papers that reproduce the results of the primary paper, is the initial pilot of this approach. The special section builds on the efforts of the SC19 Student Cluster Competition, which was part of the SC19 conference (https://sc19.supercomputing.org/) and represents a step forward in enabling reproducible research publications in parallel and distributed systems.},
  archive      = {J_TPDS},
  author       = {Manish Parashar},
  doi          = {10.1109/TPDS.2021.3053641},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {11},
  pages        = {2606},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Guest editorial: Special section on SC19 student cluster competition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Decentralized dual proximal gradient algorithms for
non-smooth constrained composite optimization problems. <em>TPDS</em>,
<em>32</em>(10), 2594–2605. (<a
href="https://doi.org/10.1109/TPDS.2021.3072373">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized dual methods play significant roles in large-scale optimization, which effectively resolve many constrained optimization problems in machine learning and power systems. In this article, we focus on studying a class of totally non-smooth constrained composite optimization problems over multi-agent systems, where the mutual goal of agents in the system is to optimize a sum of two separable non-smooth functions consisting of a strongly-convex function and another convex (not necessarily strongly-convex) function. Agents in the system conduct parallel local computation and communication in the overall process without leaking their private information. In order to resolve the totally non-smooth constrained composite optimization problem in a fully decentralized manner, we devise a synchronous decentralized dual proximal (SynDe-DuPro) gradient algorithm and its asynchronous version (AsynDe-DuPro) based on the randomized block-coordinate method. Both SynDe-DuPro and AsynDe-DuPro algorithms are theoretically proved to achieve the globally optimal solution to the totally non-smooth constrained composite optimization problem relied on the quasi-Fejér monotone theorem. As a main result, AsynDe-DuPro algorithm attains the globally optimal solution without requiring all agents to be activated at each iteration and thus is more robust than most existing synchronous algorithms. The practicability of the proposed algorithms and correctness of the theoretical findings are demonstrated by the experiments on a constrained Decentralized Sparse Logistic Regression (DSLR) problem in machine learning and a Decentralized Energy Resources Coordination (DERC) problem in power systems.},
  archive      = {J_TPDS},
  author       = {Huaqing Li and Jinhui Hu and Liang Ran and Zheng Wang and Qingguo Lü and Zhenyuan Du and Tingwen Huang},
  doi          = {10.1109/TPDS.2021.3072373},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2594-2605},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Decentralized dual proximal gradient algorithms for non-smooth constrained composite optimization problems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). LightChain: Scalable DHT-based blockchain. <em>TPDS</em>,
<em>32</em>(10), 2582–2593. (<a
href="https://doi.org/10.1109/TPDS.2021.3071176">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an append-only distributed database, blockchain is utilized in a vast variety of applications including the cryptocurrency and Internet-of-Things (IoT). The existing blockchain solutions show downsides in communication and storage scalability, as well as decentralization. In this article, we propose LightChain, which is the first blockchain architecture that operates over a Distributed Hash Table (DHT) of participating peers. LightChain is a permissionless blockchain that provides addressable blocks and transactions within the network, which makes them efficiently accessible by all peers. Each block and transaction is replicated within the DHT of peers and is retrieved in an on-demand manner. Hence, peers in LightChain are not required to retrieve or keep the entire ledger. LightChain is fair as all of the participating peers have a uniform chance of being involved in the consensus regardless of their influence such as hashing power or stake. We provide formal mathematical analysis and experimental results (simulations and cloud deployment) to demonstrate the security, efficiency, and fairness of LightChain, and show that LightChain is the only existing blockchain that can provide integrity under the corrupted majority power of peers. As we experimentally demonstrate, compared to the mainstream blockchains such as Bitcoin and Ethereum, LightChain requires around 66 times smaller per node storage, and is around 380 times faster on bootstrapping a new node to the system, and each LightChain node is rewarded equally likely for participating in the protocol.},
  archive      = {J_TPDS},
  author       = {Yahya Hassanzadeh-Nazarabadi and Alptekin Küpçü and Öznur Özkasap},
  doi          = {10.1109/TPDS.2021.3071176},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2582-2593},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {LightChain: Scalable DHT-based blockchain},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Data life aware model updating strategy for stream-based
online deep learning. <em>TPDS</em>, <em>32</em>(10), 2571–2581. (<a
href="https://doi.org/10.1109/TPDS.2021.3071939">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many deep learning applications deployed in dynamic environments change over time, in which the training models are supposed to be continuously updated with streaming data to guarantee better descriptions of data trends. However, most state-of-the-art learning frameworks support well in offline training methods while omitting online model updating strategies. In this work, we propose and implement iDlaLayer , a thin middleware layer on top of existing training frameworks that streamlines the support and implementation of online deep learning applications. In pursuit of good model quality and fast data incorporation, we design a Data Life Aware model updating strategy (DLA), which builds training data samples according to contributions of data from different life stages, and considers the training cost consumed in model updating. We evaluate iDlaLayer&#39;s performance through simulations and experiments based on TensorflowOnSpark with three representative online learning workloads. Our experimental results demonstrate that iDlaLayer reduces the overall elapsed time of ResNet, DeepFM and PageRank by 11.3, 28.2, and 15.2 percent compared to the periodic update strategy, respectively. It further achieves an average 20 percent decrease in training cost and brings about a 5 percent improvement in model quality against the traditional continuous training method.},
  archive      = {J_TPDS},
  author       = {Wei Rang and Donglin Yang and Dazhao Cheng and Yu Wang},
  doi          = {10.1109/TPDS.2021.3071939},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2571-2581},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Data life aware model updating strategy for stream-based online deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Virtualization overhead of multithreading in x86
state-of-the-art &amp; remaining challenges. <em>TPDS</em>,
<em>32</em>(10), 2557–2570. (<a
href="https://doi.org/10.1109/TPDS.2021.3064709">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite great advancements in hardware-assisted virtualization of the x86 architecture, certain workloads still suffer significant overhead. This article dissects said overhead in the context of multi-threading. We describe the state-of-the-art, pinpoint challenges, and suggest improvements, aiming to provide a valuable reference to developers and users of virtualization systems alike. We study the virtualization overhead of the PARSEC and SPLASH2X multithreaded benchmarks in a variety of scenarios using a state-of-the-art system. Through controlled experiments, source code analysis and literature review, we quantify the virtualization overhead multithreading still induces and link it to its root causes, after which we suggest possible mitigation strategies. Multithreading still induces high virtualization overhead, mainly caused by synchronization, spinning at user level and NUMA management. The overhead is diverse in nature and embodiment as it is a function of many system and workload properties. System-level solutions are feasible, but often imply difficult trade-offs. Systematic workload optimization is a promising alternative.},
  archive      = {J_TPDS},
  author       = {Stijn Schildermans and Jianchen Shan and Kris Aerts and Jason Jackrel and Xiaoning Ding},
  doi          = {10.1109/TPDS.2021.3064709},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2557-2570},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Virtualization overhead of multithreading in x86 state-of-the-art &amp; remaining challenges},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Efficient data loader for fast sampling-based GNN
training on large graphs. <em>TPDS</em>, <em>32</em>(10), 2541–2556. (<a
href="https://doi.org/10.1109/TPDS.2021.3065737">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging graph neural networks (GNNs) have extended the successes of deep learning techniques against datasets like images and texts to more complex graph-structured data. By leveraging GPU accelerators, existing frameworks combine mini-batch and sampling for effective and efficient model training on large graphs. However, this setup faces a scalability issue since loading rich vertex features from CPU to GPU through a limited bandwidth link usually dominates the training cycle. In this article, we propose PaGraph, a novel, efficient data loader that supports general and efficient sampling-based GNN training on single-server with multi-GPU. PaGraph significantly reduces the data loading time by exploiting available GPU resources to keep frequently-accessed graph data with a cache. It also embodies a lightweight yet effective caching policy that takes into account graph structural information and data access patterns of sampling-based GNN training simultaneously. Furthermore, to scale out on multiple GPUs, PaGraph develops a fast GNN-computation-aware partition algorithm to avoid cross-partition access during data-parallel training and achieves better cache efficiency. Finally, it overlaps data loading and GNN computation for further hiding loading costs. Evaluations on two representative GNN models, GCN and GraphSAGE, using two sampling methods, Neighbor and Layer-wise, show that PaGraph could eliminate the data loading time from the GNN training pipeline, and achieve up to 4.8× performance speedup over the state-of-the-art baselines. Together with preprocessing optimization, PaGraph further delivers up to 16.0× end-to-end speedup.},
  archive      = {J_TPDS},
  author       = {Youhui Bai and Cheng Li and Zhiqi Lin and Yufei Wu and Youshan Miao and Yunxin Liu and Yinlong Xu},
  doi          = {10.1109/TPDS.2021.3065737},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2541-2556},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient data loader for fast sampling-based GNN training on large graphs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). VeriML: Enabling integrity assurances and fair payments
for machine learning as a service. <em>TPDS</em>, <em>32</em>(10),
2524–2540. (<a
href="https://doi.org/10.1109/TPDS.2021.3068195">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning as a Service (MLaaS) allows clients with limited resources to outsource their expensive ML tasks to powerful servers. Despite the huge benefits, current MLaaS solutions still lack strong assurances on: 1) service correctness (i.e., whether the MLaaS works as expected); 2) trustworthy accounting (i.e., whether the bill for the MLaaS resource consumption is correctly accounted); 3) fair payment (i.e., whether a client gets the entire MLaaS result before making the payment). Without these assurances, unfaithful service providers can return improperly-executed ML task results or partially-trained ML models while asking for over-claimed rewards. Moreover, it is hard to argue for wide adoption of MLaaS to both the client and the service provider, especially in the open market without a trusted third party. In this article, we present VeriML, a novel and efficient framework to bring integrity assurances and fair payments to MLaaS. With VeriML, clients can be assured that ML tasks are correctly executed on an untrusted server, and the resource consumption claimed by the service provider equals to the actual workload. We strategically use succinct non-interactive arguments of knowledge (SNARK) on randomly-selected iterations during the ML training phase for efficiency with tunable probabilistic assurance. We also develop multiple ML-specific optimizations to the arithmetic circuit required by SNARK. Our system implements six common algorithms: linear regression, logistic regression, neural network, support vector machine, K-means and decision tree. The experimental results have validated the practical performance of VeriML.},
  archive      = {J_TPDS},
  author       = {Lingchen Zhao and Qian Wang and Cong Wang and Qi Li and Chao Shen and Bo Feng},
  doi          = {10.1109/TPDS.2021.3068195},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2524-2540},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {VeriML: Enabling integrity assurances and fair payments for machine learning as a service},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). DTransE: Distributed translating embedding for knowledge
graph. <em>TPDS</em>, <em>32</em>(10), 2509–2523. (<a
href="https://doi.org/10.1109/TPDS.2021.3066442">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs play an important role in many applications, such as link prediction and question answering. Translating embedding for knowledge graphs is done with the aim of encoding structured information on entities and their rich relations in a low-dimensional embedding space. TransE is one of the most important methods in translation-based models, and uses translation invariance to implement translating embedding for knowledge graphs. In this line of work, translating embedding models represent the relation as a translation from the head entity to the tail entity and have achieved impressive results. Currently, the TransE model is only developed on single-node machines. Unfortunately, the computing and storage capacities of a single machine can easily reach their limits as knowledge graphs become larger and more complex, which limits the application scope of TransE. In order to solve this problem, we propose a distributed TransE method, known as DTransE, which can utilize distributed computing resources to calculate knowledge graph embeddings. However, building a distributed TransE is complicated and involves challenges of knowledge graph partitioning and computation. To solve these challenges, we provide a high-quality edge partitioning algorithm for the power-law graph by considering the high-degree and low-degree vertices with adaptive weights, which can balance the workload. By using the unactivated Gather-Apply-Scatter model on TransE, the processes periodically exchange messages in a loop. The irregular data distribution among the processes is also optimized to further accelerate communication. As far as we know, this is the first work on a distributed TransE method. We use link prediction to evaluate the DTransE in a distributed environment. Experiments show that, compared to the original TransE method, our proposed DTransE is, on average, 24.5 times faster with a minimum loss of accuracy; compared to the state-of-the-art parallel TransE implementation, DTransE is two times faster on average.},
  archive      = {J_TPDS},
  author       = {Dandan Song and Feng Zhang and Meiyan Lu and Sicheng Yang and Heyan Huang},
  doi          = {10.1109/TPDS.2021.3066442},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2509-2523},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DTransE: Distributed translating embedding for knowledge graph},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). FRATO: Fog resource based adaptive task offloading for
delay-minimizing IoT service provisioning. <em>TPDS</em>,
<em>32</em>(10), 2491–2508. (<a
href="https://doi.org/10.1109/TPDS.2021.3067654">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the IoT-based systems, the fog computing allows the fog nodes to offload and process tasks requested from IoT-enabled devices in a distributed manner instead of the centralized cloud servers to reduce the response delay. However, achieving such a benefit is still challenging in the systems with high rate of requests, which imply long queues of tasks in the fog nodes, thus exposing probably an inefficiency in terms of latency to offload the tasks. In addition, a complicated heterogeneous degree in the fog environment introduces an additional issue that many of single fogs can not process heavy tasks due to lack of available resources or limited computing capabilities. To cope with the situation, this article introduces FRATO (Fog Resource aware Adaptive Task Offloading) - a framework for the IoT-fog-cloud systems to offer the minimal service provisioning delay through an adaptive task offloading mechanism. Fundamentally, FRATO is based on the fog resource to select flexibly the optimal offloading policy, which in particular includes a collaborative task offloading solution based on the data fragment concept. In addition, two distributed fog resource allocation algorithms, namely TPRA and MaxRU are developed to deploy the optimized offloading solutions efficiently in cases of resource competition. Through the extensive simulation analysis, the FRATO-based service provisioning approaches show potential advantages in reducing the average delay significantly in the systems with high rate of service requests and heterogeneous fog environment compared with the existing solutions.},
  archive      = {J_TPDS},
  author       = {Hoa Tran-Dang and Dong-Seong Kim},
  doi          = {10.1109/TPDS.2021.3067654},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2491-2508},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FRATO: Fog resource based adaptive task offloading for delay-minimizing IoT service provisioning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Group reassignment for dynamic edge partitioning.
<em>TPDS</em>, <em>32</em>(10), 2477–2490. (<a
href="https://doi.org/10.1109/TPDS.2021.3069292">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph partitioning is a mandatory step in large-scale distributed graph processing. When partitioning real-world power-law graphs, the edge partitioning algorithm performs better than the traditional vertex partitioning algorithm, because it can cut a single vertex into multiple replicas to apportion the computation. Many advanced edge partitioning methods are designed for partitioning a static graph from scratch. However, the real-world graph structure changes continuously, which leads to a decrease in partition quality and affects the performance of the graph applications. Some studies are devoted to offline repartitioning or batch incremental partitioning, but how to deal with dynamics in real-time is still worthy of in-depth study. In this article, we discuss the impact of dynamic change on partition and discover that both insertion and deletion will lead to local suboptimal partitioning, which is the reason for the degradation of partition quality. As a solution, a dynamic edge partitioning algorithm is proposed to partition dynamics in real-time. Specifically, we deal with dynamics by a distributed stream and improve partition quality by reassigning some closely connected edges. Experiments show that it is robust to initial partition quality, dynamic scale and type, and distributed scale. Compared with the state-of-the-art dynamic partitioner, it can reduce vertex-cuts by 29.5 percent. Compared with the repartitioning algorithms, it can save the partitioning time by 91.0 percent. Applied on the graph task, it can reduce the increase of communication cost and the increase of the total time of task by 41.5 and 71.4 percent.},
  archive      = {J_TPDS},
  author       = {He Li and Hang Yuan and Jianbin Huang and Jiangtao Cui and Xiaoke Ma and Senzhang Wang and Jaesoo Yoo and Philip S. Yu},
  doi          = {10.1109/TPDS.2021.3069292},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2477-2490},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Group reassignment for dynamic edge partitioning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). The case for cross-component power coordination on power
bounded systems. <em>TPDS</em>, <em>32</em>(10), 2464–2476. (<a
href="https://doi.org/10.1109/TPDS.2021.3068235">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computer systems are increasingly bounded by the available or permissible power at multiple layers from components to systems. To cope with this reality, it is necessary to understand how power bounds impact the design and performance of emergent computer systems. Prior work mainly focuses on power capping and budgeting on individual components without coordinating them to achieve the best possible performance. In this article, we study the problem of power bounded computing and power allocation across computer components on CPU and GPU-accelerated systems. We investigate the dynamics between cross-component power allocation and generalize the performance impacts, and propose lightweight heuristics to maximize performance. We draw multiple insights: (1) for a given application and power bound, there exists a maximum achievable performance which requires coordinated power allocation among components for balanced computation and memory access; (2) the max performance increases with the total power bound but only in a definite range specific to applications; (3) the dynamics of power allocations has categorical patterns with regard to performance trends and actual power use; and (4) the categorical patterns can be leveraged to design coordinated power allocations. These findings suggest the promises of cross-component coordination in forthcoming power bounded high performance computing.},
  archive      = {J_TPDS},
  author       = {Rong Ge and Xizhou Feng and Tyler Allen and Pengfei Zou},
  doi          = {10.1109/TPDS.2021.3068235},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2464-2476},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The case for cross-component power coordination on power bounded systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Spartan: A sparsity-adaptive framework to accelerate deep
neural network training on GPUs. <em>TPDS</em>, <em>32</em>(10),
2448–2463. (<a
href="https://doi.org/10.1109/TPDS.2021.3067825">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have emerged as an important class of machine learning algorithms, providing accurate solutions to a broad range of applications. Sparsity in activation maps in DNN training presents an opportunity to reduce computations. However, exploiting activation sparsity presents two major challenges: i) profiling activation sparsity during training comes with significant overhead due to computing the degree of sparsity and the data movement; ii) the dynamic nature of activation maps requires dynamic dense-to-sparse conversion during training, leading to significant overhead. In this article, we present Spartan, a lightweight hardware/software framework to accelerate DNN training on a GPU. Spartan provides a cost-effective and programmer-transparent microarchitectural solution to exploit activation sparsity detected during training. Spartan provides an efficient sparsity monitor, a tile-based sparse GEMM algorithm, and a novel compaction engine designed for GPU workloads. Spartan can reduce sparsity profiling overhead by 52.5× on average. For the most compute-intensive layers, i.e., convolutional layers, we can speedup AlexNet by 3.4×, VGGNet-16 by 2.14×, and ResNet-18 by 2.02×, when training on the ImageNet dataset.},
  archive      = {J_TPDS},
  author       = {Shi Dong and Yifan Sun and Nicolas Bohm Agostini and Elmira Karimi and Daniel Lowell and Jing Zhou and José Cano and José L. Abellán and David Kaeli},
  doi          = {10.1109/TPDS.2021.3067825},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2448-2463},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Spartan: A sparsity-adaptive framework to accelerate deep neural network training on GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). 3D perception with slanted stixels on GPU. <em>TPDS</em>,
<em>32</em>(10), 2434–2447. (<a
href="https://doi.org/10.1109/TPDS.2021.3067836">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a GPU-accelerated software design of the recently proposed model of Slanted Stixels, which represents the geometric and semantic information of a scene in a compact and accurate way. We reformulate the measurement depth model to reduce the computational complexity of the algorithm, relying on the confidence of the depth estimation and the identification of invalid values to handle outliers. The proposed massively parallel scheme and data layout for the irregular computation pattern that corresponds to a Dynamic Programming paradigm is described and carefully analyzed in performance terms. Performance is shown to scale gracefully on current generation embedded GPUs. We assess the proposed methods in terms of semantic and geometric accuracy as well as run-time performance on three publicly available benchmark datasets. Our approach achieves real-time performance with high accuracy for 2048 × 1024 image sizes and 4 × 4 Stixel resolution on the low-power embedded GPU of an NVIDIA Tegra Xavier.},
  archive      = {J_TPDS},
  author       = {Daniel Hernandez-Juarez and Antonio Espinosa and David Vazquez and Antonio M. Lopez and Juan C. Moure},
  doi          = {10.1109/TPDS.2021.3067836},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2434-2447},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {3D perception with slanted stixels on GPU},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). ETICA: Efficient two-level i/o caching architecture for
virtualized platforms. <em>TPDS</em>, <em>32</em>(10), 2415–2433. (<a
href="https://doi.org/10.1109/TPDS.2021.3066308">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, increased I/O demand of Virtual Machines (VMs) in large-scale data centers and cloud computing has encouraged system architects to design high-performance storage systems. One common approach to improving performance is to employ fast storage devices such as Solid-State Drives (SSDs) as an I/O caching layer for slower storage devices. SSDs provide high performance, especially on random requests, but they also have limited endurance: they support only a limited number of write operations and can therefore wear out relatively fast due to write operations. In addition to the write requests generated by the applications, each read miss in the SSD cache is served at the cost of imposing a write operation to the SSD (to copy the data block into the cache), resulting in an even larger number of writes into the SSD. Previous I/O caching schemes on virtualized platforms only partially mitigate the endurance limitations of SSD-based I/O caches; they mainly focus on assigning efficient cache write policies and cache space to the VMs. Moreover, existing cache space allocation schemes have inefficiencies: they do not take into account the impact of cache write policy in reuse distance calculation of the running workloads and hence, reserve cache blocks for accesses that would not be served by cache. In this article, we propose an Efficient Two-Level I/O Caching Architecture (ETICA) for virtualized platforms that can significantly improve I/O latency, endurance, and cost (in terms of cache size) while preserving the reliability of write-pending data blocks. As opposed to previous one-level I/O caching schemes in virtualized platforms, our proposed architecture 1) provides two levels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD in the I/O caching layer of virtualized platforms and 2) effectively partitions the cache space between running VMs to achieve maximum performance and minimum cache size. To manage the two-level cache, unlike the previous reuse distance calculation schemes such as Useful Reuse Distance (URD), which only consider the request type and neglect the impact of cache write policy, we propose a new metric, Policy Optimized reuse Distance (POD). The key idea of POD is to effectively calculate the reuse distance and estimate the amount of two-level DRAM+SSD cache space to allocate by considering both 1) the request type and 2) the cache write policy. Doing so results in enhanced performance and reduced cache size due to the allocation of cache blocks only for the requests that would be served by the I/O cache. ETICA maintains the reliability of write-pending data blocks and improves performance by 1) assigning an effective and fixed write policy at each level of the I/O cache hierarchy and 2) employing effective promotion and eviction methods between cache levels. Our extensive experiments conducted with a real implementation of the proposed two-level storage caching architecture show that ETICA provides 45 percent higher performance, compared to the state-of-the-art caching schemes in virtualized platforms, while improving both cache size and SSD endurance by 51.7 and 33.8 percent, respectively.},
  archive      = {J_TPDS},
  author       = {Saba Ahmadian and Reza Salkhordeh and Onur Mutlu and Hossein Asadi},
  doi          = {10.1109/TPDS.2021.3066308},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2415-2433},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ETICA: Efficient two-level I/O caching architecture for virtualized platforms},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Analysis of GPU data access patterns on complex
geometries for the D3Q19 lattice boltzmann algorithm. <em>TPDS</em>,
<em>32</em>(10), 2400–2414. (<a
href="https://doi.org/10.1109/TPDS.2021.3061895">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU performance of the lattice Boltzmann method (LBM) depends heavily on memory access patterns. When implemented with GPUs on complex domains, typically, geometric data is accessed indirectly and lattice data is accessed lexicographically. Although there are a variety of other options, no study has examined the relative efficacy between them. Here, we examine a suite of memory access schemes via empirical testing and performance modeling. We find strong evidence that semi-direct is often better suited than the more common indirect addressing, providing increased computational speed and reducing memory consumption. For the layout, we find that the Collected Structure of Arrays (CSoA) and bundling layouts outperform the common Structure of Array layout; on V100 and P100 devices, CSoA consistently outperforms bundling, however the relationship is more complicated on K40 devices. When compared to state-of-the-art practices, our recommendations lead to speedups of 10-40 percent and reduce memory consumption up to 17 percent. Using performance modeling and computational experimentation, we determine the mechanisms behind the accelerations. We demonstrate that our results hold across multiple GPUs on two leadership class systems, and present the first near-optimal strong results for LBM with arterial geometries run on GPUs.},
  archive      = {J_TPDS},
  author       = {Gregory Herschlag and Seyong Lee and Jeffrey S. Vetter and Amanda Randles},
  doi          = {10.1109/TPDS.2021.3061895},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2400-2414},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Analysis of GPU data access patterns on complex geometries for the D3Q19 lattice boltzmann algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). GIM: GPU accelerated RIS-based influence maximization
algorithm. <em>TPDS</em>, <em>32</em>(10), 2386–2399. (<a
href="https://doi.org/10.1109/TPDS.2021.3066215">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a social network modeled as a weighted graph GG, the influence maximization problem seeks kk vertices to become initially influenced, to maximize the expected number of influenced nodes under a particular diffusion model. The influence maximization problem has been proven to be NP-hard, and most proposed solutions to the problem are approximate greedy algorithms, which can guarantee a tunable approximation ratio for their results with respect to the optimal solution. The state-of-the-art algorithms are based on Reverse Influence Sampling (RIS) technique, which can offer both computational efficiency and non-trivial (1-1/e-ε)(1-1/e-ε)-approximation ratio guarantee for any ε &gt; 0ε&gt;0. RIS-based algorithms, despite their lower computational cost compared to other methods, still require long running times to solve the problem in large-scale graphs with low values of ε. In this article, we present a novel and efficient parallel implementation of a RIS-based algorithm, namely IMM, on GPU. The proposed GPU-accelerated influence maximization algorithm, named gIM, can significantly reduce the running time on large-scale graphs with low values of ε. Furthermore, we show that gIM algorithm can solve other variations of the IM problem, only by applying minor modifications. Experimental results show that the proposed solution reduces the runtime by a factor up to 220 ×. The source code of gIM is publicly available online.},
  archive      = {J_TPDS},
  author       = {Soheil Shahrouz and Saber Salehkaleybar and Matin Hashemi},
  doi          = {10.1109/TPDS.2021.3066215},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2386-2399},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GIM: GPU accelerated RIS-based influence maximization algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021a). Editor’s note. <em>TPDS</em>, <em>32</em>(10),
2381–2385. (<a
href="https://doi.org/10.1109/TPDS.2021.3066313">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TPDS},
  author       = {Manish Parashar},
  doi          = {10.1109/TPDS.2021.3066313},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {10},
  pages        = {2381-2385},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Editor&#39;s note},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Optimizing the LINPACK algorithm for large-scale
PCIe-based CPU-GPU heterogeneous systems. <em>TPDS</em>, <em>32</em>(9),
2367–2380. (<a
href="https://doi.org/10.1109/TPDS.2021.3067731">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a widening gap between GPU and other components (CPU, PCIe bus and communication network) in heterogeneous parallel system. The gap forces us to orchestrate cooperative execution among these components much more carefully than ever before. By taking the LINPACK benchmark as a case study, this article proposes a fine-grained pipelining algorithm on large-scale CPU-GPU heterogeneous cluster systems. First, we build an algorithmic model that reveals a new approach to GPU-centric and fine-grained pipelining algorithm design. Then, we present four model-driven pipelining algorithms that incrementally squeeze bubbles in the pipeline so that it is occupied by more useful floating-point calculations. The algorithms are implemented on both the AMD and NVIDIA GPU platforms. The finally optimized LINPACK program achieves 107 PFlops on 25, 600 GPUs (70 percent floating-point efficiency). Several insights have been drawn to suggest tradeoff of algorithm design, programming support, and architecture design.},
  archive      = {J_TPDS},
  author       = {Guangming Tan and Chaoyang Shui and Yinshan Wang and Xianzhi Yu and Yujin Yan},
  doi          = {10.1109/TPDS.2021.3067731},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2367-2380},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing the LINPACK algorithm for large-scale PCIe-based CPU-GPU heterogeneous systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accelerating the bron-kerbosch algorithm for maximal
clique enumeration using GPUs. <em>TPDS</em>, <em>32</em>(9), 2352–2366.
(<a href="https://doi.org/10.1109/TPDS.2021.3067053">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximal clique enumeration (MCE) is a classic problem in graph theory to identify all complete subgraphs in a graph. In prior MCE work, the Bron-Kerbosch algorithm is one of the most popular solutions, and there are several improved algorithms proposed on CPU platforms. However, while few studies have focused on the related issue of parallel implementation, recently, there have been numerous explorations of the acceleration of general purpose applications using a graphics processing unit (GPU) to reduce the computing power consumption. In this article, we develop a GPU-based Bron-Kerbosch algorithm that efficiently solves the MCE problem in parallel by optimizing the process of subproblem decomposition and computing resource usage. To speed up the computations, we use coalesced memory accesses and warp reductions to increase bandwidth and reduce memory latency. Our experimental results show that the proposed algorithm can fully exploit the resources of GPU architectures, allowing for the vast acceleration of operations to solve the MCE problem.},
  archive      = {J_TPDS},
  author       = {Yi-Wen Wei and Wei-Mei Chen and Hsin-Hung Tsai},
  doi          = {10.1109/TPDS.2021.3067053},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2352-2366},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating the bron-kerbosch algorithm for maximal clique enumeration using GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). OWebSync: Seamless synchronization of distributed web
clients. <em>TPDS</em>, <em>32</em>(9), 2338–2351. (<a
href="https://doi.org/10.1109/TPDS.2021.3066276">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many enterprise software services are adopting a fully web-based architecture for both internal line-of-business applications and for online customer-facing applications. Although wireless connections are becoming more ubiquitous and faster, mobile employees and customers are often offline due to expected or unexpected network disruptions. Nevertheless, continuous operation of the software is expected. This article presents OWebSync: a web-based middleware for data synchronization in interactive groupware with fast resynchronization of offline clients and continuous, interactive synchronization of online clients. To automatically resolve conflicts, OWebSync implements a fine-grained data synchronization model and leverages state-based Conflict-free Replicated Data Types. This middleware uses Merkle-trees embedded in the tree-structured data and virtual Merkle-tree levels to achieve the required interactive performance. Our comparative evaluation with available operation-based and delta-state-based middleware solutions shows that OWebSync is especially better in operating in and recovering from offline settings and network disruptions. In addition, OWebSync scales more efficiently over time, as it does not store version vectors or other meta-data for all past clients.},
  archive      = {J_TPDS},
  author       = {Kristof Jannes and Bert Lagaisse and Wouter Joosen},
  doi          = {10.1109/TPDS.2021.3066276},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2338-2351},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {OWebSync: Seamless synchronization of distributed web clients},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). YuenyeungSpTRSV: A thread-level and warp-level fusion
synchronization-free sparse triangular solve. <em>TPDS</em>,
<em>32</em>(9), 2321–2337. (<a
href="https://doi.org/10.1109/TPDS.2021.3066635">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse triangular solves (SpTRSVs) are widely used in linear algebra domains, and several GPU-based SpTRSV algorithms have been developed. Synchronization-free SpTRSVs, due to their short preprocessing time and high performance, are currently the most popular SpTRSV algorithms. However, we observe that the performance of those SpTRSV algorithms on different matrices can vary greatly by 845 times. Our further studies show that when the average number of components per level is high and the average number of nonzero elements per row is low, those SpTRSVs exhibit extremely low performance. The reason is that, they use a warp on the GPU to process a row in sparse matrices, and such warp-level designs have severe underutilization of the GPU. To solve this problem, we propose YuenyeungSpTRSV, a thread-level and wrap-level fusion synchronization-free SpTRSV algorithm, which handles the rows with a large number of nonzero elements at warp-level while the rows with a low number of nonzero elements at thread-level. Particularly, YuenyeungSpTRSV has three novel features. First, unlike the previous studies, YuenyeungSpTRSV does not need long preprocessing time to calculate levels. Second, YuenyeungSpTRSV exhibits high performance on matrices that previous SpTRSVs cannot handle efficiently. Third, YuenyeungSpTRSV&#39;s optimization does not rely on the specific sparse matrix storage format. Instead, it can achieve very good performance on the most popular sparse matrix storage, compressed sparse row (CSR) format, and thus users do not need to conduct format conversion. We evaluate YuenyeungSpTRSV with 245 matrices from the Florida Sparse Matrix Collection on four GPU platforms, and experiments show that our YuenyeungSpTRSV exhibits 7.14 GFLOPS/s, which is 5.98x speedup over the state-of-the-art synchronization-free SpTRSV algorithm, and 4.83x speedup over the SpTRSV in cuSPARSE.},
  archive      = {J_TPDS},
  author       = {Feng Zhang and Jiya Su and Weifeng Liu and Bingsheng He and Ruofan Wu and Xiaoyong Du and Rujia Wang},
  doi          = {10.1109/TPDS.2021.3066635},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2321-2337},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {YuenyeungSpTRSV: A thread-level and warp-level fusion synchronization-free sparse triangular solve},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Fine-grained multi-query stream processing on integrated
architectures. <em>TPDS</em>, <em>32</em>(9), 2303–2320. (<a
href="https://doi.org/10.1109/TPDS.2021.3066407">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the sharing opportunities among multiple stream queries is crucial for high-performance stream processing. Modern stream processing necessitates accelerating multiple queries by utilizing heterogeneous coprocessors, such as GPUs, and this has shown to be an effective method. Emerging CPU-GPU integrated architectures 6integrate CPU and GPU on the same chip and eliminate PCI-e bandwidth bottleneck. Such a novel architecture provides new opportunities for improving multi-query performance in stream processing but has not been fully explored by existing systems. We introduce a stream processing engine, called FineStream, for efficient multi-query window-based stream processing on CPU-GPU integrated architectures. FineStream&#39;s key contribution is a novel fine-grained workload scheduling mechanism between CPU and GPU to take advantage of both architectures. Particularly, FineStream is able to efficiently handle multiple queries in both static and dynamic streams. Our experimental results show that 1) on integrated architectures, FineStream achieves an average 52 percent throughput improvement and 36 percent lower latency over the state-of-the-art stream processing engine; 2) compared to the coarse-grained strategy of applying different devices for multiple queries, FineStream achieves 32 percent throughput improvement; 3) compared to the stream processing engine on the discrete architecture, FineStream on the integrated architecture achieves 10.4× price-throughput ratio, 1.8× energy efficiency, and can enjoy lower latency benefits.},
  archive      = {J_TPDS},
  author       = {Feng Zhang and Chenyang Zhang and Lin Yang and Shuhao Zhang and Bingsheng He and Wei Lu and Xiaoyong Du},
  doi          = {10.1109/TPDS.2021.3066407},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2303-2320},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fine-grained multi-query stream processing on integrated architectures},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). BALS: Blocked alternating least squares for parallel
sparse matrix factorization on GPUs. <em>TPDS</em>, <em>32</em>(9),
2291–2302. (<a
href="https://doi.org/10.1109/TPDS.2021.3064942">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization on sparse matrices has been proven to be an effective approach for data mining and machine learning. However, the prior parallel implementations for matrix factorization fail to capture the internal social property embedded in real-world use cases. This article presents an efficient implementation of the alternative least squares (ALS) algorithm called BALS built on top of a new sparse matrix format for parallel matrix factorization. The BALS storage format organizes the sparse matrix into 2D tiles to avoid repeated data loads and improve data reuses. We further propose a data reordering technique to sort sparse matrices according to nonzeros. The experimental results show that BALS can yield a superior performance than state-of-the-art implementations, i.e., our BALS generally runs faster than Gates’ implementation over different latent feature sizes, with a speedup of up to 2.08× on K20C, 3.72× on TITAN X and 3.13× on TITAN RTX. When compared with alternative matrix factorization algorithms, our BALS consistently outperforms CDMF, cuMF_CCD, and cuMF_SGD over various latent feature sizes and datasets. The reordering technique can provide an extra improvement of up to 23.68 percent on K20C, 19.87 percent on TITAN X and 20.38 percent on TITAN RTX.},
  archive      = {J_TPDS},
  author       = {Jing Chen and Jianbin Fang and Weifeng Liu and Canqun Yang},
  doi          = {10.1109/TPDS.2021.3064942},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2291-2302},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BALS: Blocked alternating least squares for parallel sparse matrix factorization on GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). PISTIS: An event-triggered real-time byzantine-resilient
protocol suite. <em>TPDS</em>, <em>32</em>(9), 2277–2290. (<a
href="https://doi.org/10.1109/TPDS.2021.3056718">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated digitalisation of society along with technological evolution have extended the geographical span of cyber-physical systems. Two main threats have made the reliable and real-time control of these systems challenging: (i) uncertainty in the communication infrastructure induced by scale, and heterogeneity of the environment and devices; and (ii) targeted attacks maliciously worsening the impact of the above-mentioned communication uncertainties, disrupting the correctness of real-time applications. This article addresses those challenges by showing how to build distributed protocols that provide both real-time with practical performance, and scalability in the presence of network faults and attacks, in probabilistic synchronous environments. We provide a suite of real-time Byzantine protocols, which we prove correct, starting from a reliable broadcast protocol, called PISTIS, up to atomic broadcast and consensus. This suite simplifies the construction of powerful distributed and decentralized monitoring and control applications, including state-machine replication. Extensive empirical simulations showcase PISTIS&#39;s robustness, latency, and scalability. For example, PISTIS can withstand message loss (and delay) rates up to 50 percent in systems with 49 nodes and provides bounded delivery latencies in the order of a few milliseconds.},
  archive      = {J_TPDS},
  author       = {David Kozhaya and Jérémie Decouchant and Vincent Rahli and Paulo Esteves-Verissimo},
  doi          = {10.1109/TPDS.2021.3056718},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2277-2290},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PISTIS: An event-triggered real-time byzantine-resilient protocol suite},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). An efficient parallel secure machine learning framework
on GPUs. <em>TPDS</em>, <em>32</em>(9), 2262–2276. (<a
href="https://doi.org/10.1109/TPDS.2021.3059108">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is widely used in our daily lives. Large amounts of data have been continuously produced and transmitted to the cloud for model training and data processing, which raises a problem: how to preserve the security of the data. Recently, a secure machine learning system named SecureML has been proposed to solve this issue using two-party computation. However, due to the excessive computation expenses of two-party computation, the secure machine learning is about 2× slower than the original machine learning methods. Previous work on secure machine learning mostly focused on novel protocols or improving accuracy, while the performance metric has been ignored. In this article, we propose a GPU-based framework ParSecureML to improve the performance of secure machine learning algorithms based on two-party computation. The main challenges of developing ParSecureML lie in the complex computation patterns, frequent intra-node data transmission between CPU and GPU, and complicated inter-node data dependence. To handle these challenges, we propose a series of novel solutions, including profiling-guided adaptive GPU utilization, fine-grained double pipeline for intra-node CPU-GPU cooperation, and compressed transmission for inter-node communication. Moreover, we integrate architecture specific optimizations, such as Tensor Cores, into ParSecureML. As far as we know, this is the first GPU-based secure machine learning framework. Compared to the state-of-the-art framework, ParSecureML achieves an average of 33.8× speedup. ParSecureML can also be applied to inferences, which achieves 31.7× speedup on average.},
  archive      = {J_TPDS},
  author       = {Feng Zhang and Zheng Chen and Chenyang Zhang and Amelie Chi Zhou and Jidong Zhai and Xiaoyong Du},
  doi          = {10.1109/TPDS.2021.3059108},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2262-2276},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficient parallel secure machine learning framework on GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Structured allocation-based consistent hashing with
improved balancing for cloud infrastructure. <em>TPDS</em>,
<em>32</em>(9), 2248–2261. (<a
href="https://doi.org/10.1109/TPDS.2021.3058963">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistent hashing has played an indispensable role in cloud infrastructure, although its load balancing performance is not necessarily perfect. Consistent hashing has long remained the most widely used method despite many methods being proposed to improve load balancing because these methods trade off load balancing against consistency, memory usage, lookup performance, and/or fault-tolerance. This article presents Structured Allocation-based Consistent Hashing (SACH), a cloud-optimized consistent hashing algorithm that overcomes the trade-offs by taking advantage of the characteristics of cloud environments: scaling management and auto-healing. Since scaling can be distinguished from failures, SACH applies two different algorithms to update hashing functions: a fast-update algorithm for unmanaged backend failures to satisfy fault-tolerance with quick response and a slow-update algorithm for managed scaling. Hashing functions are initialized or slow-updated considering the characteristics of the fast-update algorithm to satisfy load balancing and the other properties as far as the number of failed backends is kept small by auto-healing. The experimental results show that SACH outperforms existing algorithms in each aspect. SACH will improve the load balancing of cloud infrastructure components, where the trade-offs have prevented the renewal of hashing functions.},
  archive      = {J_TPDS},
  author       = {Yuichi Nakatani},
  doi          = {10.1109/TPDS.2021.3058963},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2248-2261},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Structured allocation-based consistent hashing with improved balancing for cloud infrastructure},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accurate differentially private deep learning on the
edge. <em>TPDS</em>, <em>32</em>(9), 2231–2247. (<a
href="https://doi.org/10.1109/TPDS.2021.3064345">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) models are increasingly built on federated edge participants holding local data. To enable insight extractions without the risk of information leakage, DL training is usually combined with differential privacy (DP). The core theme is to tradeoff learning accuracy by adding statistically calibrated noises, particularly to local gradients of edge learners, during model training. However, this privacy guarantee unfortunately degrades model accuracy due to edge learners&#39; local noises, and the global noise aggregated at the central server. Existing DP frameworks for edge focus on local noise calibration via gradient clipping techniques, overlooking the heterogeneity and dynamic changes of local gradients, and their aggregated impact on accuracy. In this article, we present a systematical analysis that unveils the influential factors capable of mitigating local and aggregated noises, and design PrivateDL to leverage these factors in noise calibration so as to improve model accuracy while fulfilling privacy guarantee. PrivateDL features on: (i) sampling-based sensitivity estimation for local noise calibration and (ii) combining large batch sizes and critical data identification in global training. We implement PrivateDL on the popular Laplace/Gaussian DP mechanisms and demonstrate its effectiveness using Intel BigDL workloads, i.e., considerably improving model accuracy by up to 5X when comparing against existing DP frameworks.},
  archive      = {J_TPDS},
  author       = {Rui Han and Dong Li and Junyan Ouyang and Chi Harold Liu and Guoren Wang and Dapeng Wu and Lydia Y. Chen},
  doi          = {10.1109/TPDS.2021.3064345},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2231-2247},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accurate differentially private deep learning on the edge},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A survey of system architectures and techniques for FPGA
virtualization. <em>TPDS</em>, <em>32</em>(9), 2216–2230. (<a
href="https://doi.org/10.1109/TPDS.2021.3063670">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FPGA accelerators are gaining increasing attention in both cloud and edge computing because of their hardware flexibility, high computational throughput, and low power consumption. However, the design flow of FPGAs often requires specific knowledge of the underlying hardware, which hinders the wide adoption of FPGAs by application developers. Therefore, the virtualization of FPGAs becomes extremely important to create a useful abstraction of the hardware suitable for application developers. Such abstraction also enables the sharing of FPGA resources among multiple users and accelerator applications, which is important because, traditionally, FPGAs have been mostly used in single-user, single-embedded-application scenarios. There are many works in the field of FPGA virtualization covering different aspects and targeting different application areas. In this article, we review the system architectures used in the literature for FPGA virtualization. In addition, we identify the primary objectives of FPGA virtualization, based on which we summarize the techniques for realizing FPGA virtualization. This article helps researchers to efficiently learn about FPGA virtualization research by providing a comprehensive review of the existing literature.},
  archive      = {J_TPDS},
  author       = {Masudul Hassan Quraishi and Erfan Bank Tavakoli and Fengbo Ren},
  doi          = {10.1109/TPDS.2021.3063670},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2216-2230},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A survey of system architectures and techniques for FPGA virtualization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Octans: Optimal placement of service function chains in
many-core systems. <em>TPDS</em>, <em>32</em>(9), 2202–2215. (<a
href="https://doi.org/10.1109/TPDS.2021.3063613">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Function Virtualization (NFV) offers service delivery flexibility and reduces overall costs by running service function chains (SFCs) on commodity servers with many cores. Existing solutions for placing SFCs in one server treat all CPU cores as equal and allocate isolated CPU cores to network functions (NFs). However, advanced servers often adopt Non-Uniform Memory Access (NUMA) architecture to improve the scalability of many-core systems. CPU cores are grouped into nodes, incurring performance degradation due to cross-node memory access and intra-node resource contention. Our evaluation shows that randomly selecting cores to place NFs in an SFC could suffer from 39.2 percent lower throughput comparing to an optimal placement solution. In this article, we propose Octans, an NFV orchestrator to achieve maximum aggregate throughput of all SFCs in many-core systems. Octans first formulates the optimization problem as a Non-Linear Integer Programming (NLIP) Model. Then we identify the key factor for problem solving as evaluating the throughput drop of an NF caused by other NFs in the same SFC or different SFCs, i.e., performance drop index, and propose a formal and accurate prediction model based on system level performance metrics. Finally, we propose two online algorithms to quickly find near-optimal placement solutions for one-time and incremental deployment. Extensive evaluation on a prototype implementation shows that Octans significantly improves the aggregate throughput comparing to two state-of-the-art placement solutions by 27.1 ~ 45.2 percent for one-time deployment and by 20.9 ~ 38.1 percent for incremental deployment, with very low prediction errors. Moreover, Octans could quickly find a near-optimal placement solution with tiny optimality gap.},
  archive      = {J_TPDS},
  author       = {Heng Yu and Zhilong Zheng and Junxian Shen and Congcong Miao and Chen Sun and Hongxin Hu and Jun Bi and Jianping Wu and Jilong Wang},
  doi          = {10.1109/TPDS.2021.3063613},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2202-2215},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Octans: Optimal placement of service function chains in many-core systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Optimizing resource allocation for data-parallel jobs via
GCN-based prediction. <em>TPDS</em>, <em>32</em>(9), 2188–2201. (<a
href="https://doi.org/10.1109/TPDS.2021.3055019">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under-allocating or over-allocating computation resources (e.g., CPU cores) can prolong the completion time of data-parallel jobs in a distributed system. We present a predictor, ReLocag, to find the near-optimal number of CPU cores to minimize job completion time (JCT). ReLocag includes a graph convolutional network (GCN) and a fully-connected network (FCNN). The GCN learns the dependency between operations from the workflow of a job, and then the FCNN takes the workflow dependency together with other features (e.g., the input size, the number of CPU cores, the amount of memory, and the number of computation tasks) as input for JCT prediction. The prediction result can guide the user to determine the near-optimal number of CPU cores. Besides, we propose two effective strategies to overcome the time-consuming issue of training sample collection in big data applications. First, we develop an adaptive sampling method to collect essential samples judiciously. Second, we further design a cross-application transfer learning model to exploit the training samples collected from other applications. We conduct extensive experiments in a Spark cluster for 7 types of exemplary Spark applications. Results show that ReLocag improves the JCT prediction accuracy by 4-14 percent. Moreover, the CPU core consumption decreases by 58.2 percent.},
  archive      = {J_TPDS},
  author       = {Zhiyao Hu and Dongsheng Li and Dongxiang Zhang and Yiming Zhang and Baoyun Peng},
  doi          = {10.1109/TPDS.2021.3055019},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2188-2201},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimizing resource allocation for data-parallel jobs via GCN-based prediction},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). DeepSlicing: Collaborative and adaptive CNN inference
with low latency. <em>TPDS</em>, <em>32</em>(9), 2175–2187. (<a
href="https://doi.org/10.1109/TPDS.2021.3058532">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The booming of Convolutional Neural Networks (CNNs) has empowered lots of computer-vision applications. Due to its stringent requirement for computing resources, substantial research has been conducted on how to optimize its deployment and execution on resource-constrained devices. However, previous works have several weaknesses, including limited support for various CNN structures, fixed scheduling strategies, overlapped computations, high synchronization overheads, etc. In this article, we present DeepSlicing, a collaborative and adaptive inference system that adapts to various CNNs and supports customized flexible fine-grained scheduling. As a built-in functionality, DeepSlicing has supported typical CNNs including GoogLeNet, ResNet, etc. By partitioning both model and data, we also design an efficient scheduler, Proportional Synchronized Scheduler (PSS), which achieves the trade-off between computation and synchronization. Based on PyTorch, we have implemented DeepSlicing on the testbed with real-world edge settings that consists of 8 heterogeneous Raspberry Pi&#39;s. The results indicate that DeepSlicing with PSS outperforms the existing systems dramatically, e.g., the inference latency and memory footprint are reduced up to 5.79× and 14.72×, respectively.},
  archive      = {J_TPDS},
  author       = {Shuai Zhang and Sheng Zhang and Zhuzhong Qian and Jie Wu and Yibo Jin and Sanglu Lu},
  doi          = {10.1109/TPDS.2021.3058532},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2175-2187},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DeepSlicing: Collaborative and adaptive CNN inference with low latency},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Retargeting tensor accelerators for epistasis detection.
<em>TPDS</em>, <em>32</em>(9), 2160–2174. (<a
href="https://doi.org/10.1109/TPDS.2021.3060322">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The substitution of nucleotides at specific positions in the genome of a population, known as single-nucleotide polymorphisms (SNPs), has been correlated with a number of important diseases. Complex conditions such as Alzheimer&#39;s disease or Crohn&#39;s disease are significantly linked to genetics when the impact of multiple SNPs is considered. SNPs often interact in an epistatic manner, where the joint effect of multiple SNPs may not be simply mapped to a linear additive combination of individual effects. Genome-wide association studies considering epistasis are computationally challenging, especially when performing triplet searches is required. Some contemporary computer architectures support fused XOR and population count as the highest throughput operations as part of tensor operations. This article presents a new approach for efficiently repurposing this capability to accelerate 2-way (pairs) and 3-way (triplets) epistasis detection searches. Experimental evaluation targeting the Turing GPU architecture resulted in previously unattainable levels of performance, with the proposal being able to evaluate up to 108.1 and 54.5 tera unique sets of SNPs per second, scaled to the sample size, in 2-way and 3-way searches, respectively.},
  archive      = {J_TPDS},
  author       = {Ricardo Nobre and Aleksandar Ilic and Sergio Santander-Jiménez and Leonel Sousa},
  doi          = {10.1109/TPDS.2021.3060322},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2160-2174},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Retargeting tensor accelerators for epistasis detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Overlapping communication with computation in parameter
server for scalable DL training. <em>TPDS</em>, <em>32</em>(9),
2144–2159. (<a
href="https://doi.org/10.1109/TPDS.2021.3062721">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scalability of distributed deep learning (DL) training with parameter server (PS) architecture is often communication constrained in large clusters. There are recent efforts that use a layer by layer strategy to overlap gradient communication with backward computation so as to reduce the impact of communication constraint on the scalability. However, the approaches could bring significant overhead in gradient communication. Meanwhile, they cannot be effectively applied to the overlap between parameter communication and forward computation. In this article, we propose and develop iPart, a novel approach that partitions communication and computation in various partition sizes to overlap gradient communication with backward computation and parameter communication with forward computation. iPart formulates the partitioning decision as an optimization problem and solves it based on a greedy algorithm to derive communication and computation partitions. We implement iPart in the open-source DL framework BigDL and perform evaluations with various DL workloads. Experimental results show that iPart improves the scalability of a cluster of 72 nodes by up to 94 percent over the default PS and 52 percent over the layer by layer strategy.},
  archive      = {J_TPDS},
  author       = {Shaoqi Wang and Aidi Pi and Xiaobo Zhou and Jun Wang and Cheng-Zhong Xu},
  doi          = {10.1109/TPDS.2021.3062721},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {9},
  pages        = {2144-2159},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Overlapping communication with computation in parameter server for scalable DL training},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Joint SFC deployment and resource management in
heterogeneous edge for latency minimization. <em>TPDS</em>,
<em>32</em>(8), 2131–2143. (<a
href="https://doi.org/10.1109/TPDS.2021.3062341">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of edge computing and network function virtualization, it is promising to provide flexible and low-latency network services at the network edge. However, due to resource limitation and heterogeneity of servers at the edge, it is unlikely to achieve an efficient service function chain deployment without considering the resource management of edge servers jointly. In this article, we consider the Joint Service function chain Deployment and Resource Management problem (JSDRM) in heterogeneous edge environments with the goal of minimizing the total system latency. We prove the NP-hardness of JSDRM and propose a scheme called JOint service function chain deployment and resource management Scheme (JOS) based on a game-theoretic approach to deploy service function chains and manage resources. We prove that JOS has a constant approximation ratio of 2.62 Extensive simulation results show that our scheme performs comparably to the optimal solution and much better than the baselines. The simulation results also show that the proposed scheme is time-efficient.},
  archive      = {J_TPDS},
  author       = {Yu Liu and Xiaojun Shang and Yuanyuan Yang},
  doi          = {10.1109/TPDS.2021.3062341},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2131-2143},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint SFC deployment and resource management in heterogeneous edge for latency minimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Online scheduling technique to handle data velocity
changes in stream workflows. <em>TPDS</em>, <em>32</em>(8), 2115–2130.
(<a href="https://doi.org/10.1109/TPDS.2021.3059480">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many IoT applications and services such as smart parking and smart traffic control contain a network of different analytical components, which are composed in the form of a workflow to make better decisions. These workflows are also known as stream workflows. The focus of existing research works is on the streaming operator graph, which differs from stream workflow application as it involves heterogeneity, multiple data sources and multiple outputs. Considering the complexity and dynamism of stream workflow, meeting real-time data analysis requirements at deployment time is not the whole story as the velocity of data changes over time. This change is the most dynamic form of stream workflow that occurs frequently during the execution of this application. In this article, we propose a new dynamic scheduling technique that manages cloud resources over time to handle data velocity changes in stream workflow while maintaining user-defined real-time data analysis requirements and minimising execution cost. The efficiency of the proposed technique is evaluated, and experimental results showed that this technique outperformed its competitors and is close to the lower bound.},
  archive      = {J_TPDS},
  author       = {Mutaz Barika and Saurabh Garg and Albert Y. Zomaya and Rajiv Ranjan},
  doi          = {10.1109/TPDS.2021.3059480},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2115-2130},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online scheduling technique to handle data velocity changes in stream workflows},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). High-performance computing implementations of agent-based
economic models for realizing 1: 1 scale simulations of large economies.
<em>TPDS</em>, <em>32</em>(8), 2101–2114. (<a
href="https://doi.org/10.1109/TPDS.2021.3060462">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a scalable high-performance computing implementation of an agent-based economic model using distributed + shared-memory hybrid parallelization paradigms, capable of simulating 1:1 scale models of large economies like the eurozone. Agent-based economic models consist of millions of agents interacting over several graphs, which are either centralized or scale-free in nature. While most of the interactions are bi-directional, the interaction graphs are dense and random and keep evolving as the simulation progresses. These characteristics cause a very large and unknown number of random communications among MPI processes, posing challenges to developing scalable parallel extensions. Further, random access to large volume of data makes the algorithms highly memory-bound, severely degrading computational performance. Adopting various strategies inspired by the real-world functioning of economies, we reduce the large unknown number of communications to a known handful number. Memory-intensive algorithms are improved to make these cache-efficient, and advanced MPI functions are used to minimize communication overhead, thereby attaining higher performance and scalability. Further, an MPI + OpenMP hybrid model is developed to best utilize modern many-core computing nodes with low per-core memory capacity. It is demonstrated that our implementation can simulate a full fledged economic model with 331 million agents within 108 seconds using 128 CPU cores attaining 70 percent strong scalability.},
  archive      = {J_TPDS},
  author       = {Amit Gill and Madegedara Lalith and Sebastian Poledna and Muneo Hori and Kohei Fujita and Tsuyoshi Ichimura},
  doi          = {10.1109/TPDS.2021.3060462},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2101-2114},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High-performance computing implementations of agent-based economic models for realizing 1: 1 scale simulations of large economies},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Joint task scheduling and containerizing for efficient
edge computing. <em>TPDS</em>, <em>32</em>(8), 2086–2100. (<a
href="https://doi.org/10.1109/TPDS.2021.3059447">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container-based operation system (OS) level virtualization has been adopted by many edge-computing platforms. However, for an edge server, inter-container communications, and container management consume significant CPU resources. Given an application composed of interdependent tasks, the number of such operations is closely related to the dependency between the scheduled tasks. Thus, to improve the execution efficiency of an application in an edge server, task scheduling and task containerizing need to be considered together. To this end, a joint task scheduling and containerizing (JTSC) scheme is developed in this article. Experiments are first carried out to quantify the resource utilization of container operations. System models are then built to capture the features of task execution in containers in an edge server with multiple processors. With these models, joint task scheduling and containerizing is conducted as follows. First, tasks are scheduled without considering containerization, which results in initial schedules. Second, based on system models and guidelines gained from the initial schedules, several containerization algorithms are designed to map tasks to containers. Third, task execution durations are updated by adding the time for inter-container communications, and then the task schedules are updated accordingly. The JTSC scheme is evaluated through extensive simulations. The results show that it reduces inefficient container operations and enhances the execution efficiency of applications by 60 percent.},
  archive      = {J_TPDS},
  author       = {Jiawei Zhang and Xiaochen Zhou and Tianyi Ge and Xudong Wang and Taewon Hwang},
  doi          = {10.1109/TPDS.2021.3059447},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2086-2100},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Joint task scheduling and containerizing for efficient edge computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Proof of federated learning: A novel energy-recycling
consensus algorithm. <em>TPDS</em>, <em>32</em>(8), 2074–2085. (<a
href="https://doi.org/10.1109/TPDS.2021.3056773">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proof of work (PoW), the most popular consensus mechanism for blockchain, requires ridiculously large amounts of energy but without any useful outcome beyond determining accounting rights among miners. To tackle the drawback of PoW, we propose a novel energy-recycling consensus algorithm, namely proof of federated learning (PoFL), where the energy originally wasted to solve difficult but meaningless puzzles in PoW is reinvested to federated learning. Federated learning and pooled-mining, a trend of PoW, have a natural fit in terms of organization structure. However, the separation between the data usufruct and ownership in blockchain lead to data privacy leakage in model training and verification, deviating from the original intention of federal learning. To address the challenge, a reverse game-based data trading mechanism and a privacy-preserving model verification mechanism are proposed. The former can guard against training data leakage while the latter verifies the accuracy of a trained model with privacy preservation of the task requester&#39;s test data as well as the pool&#39;s submitted model. To the best of our knowledge, our article is the first work to employ federal learning as the proof of work for blockchain. Extensive simulations based on synthetic and real-world data demonstrate the effectiveness and efficiency of our proposed mechanisms.},
  archive      = {J_TPDS},
  author       = {Xidi Qu and Shengling Wang and Qin Hu and Xiuzhen Cheng},
  doi          = {10.1109/TPDS.2021.3056773},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2074-2085},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Proof of federated learning: A novel energy-recycling consensus algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A fault-tolerant distributed framework for asynchronous
iterative computations. <em>TPDS</em>, <em>32</em>(8), 2062–2073. (<a
href="https://doi.org/10.1109/TPDS.2021.3059420">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous iterative computations (AIC) are common in machine learning and data mining systems. However, the lack of synchronization barriers in asynchronous processing brings challenges for continuous processing while workers might fail. There is no global synchronization point that all workers can roll back to. In this article, we propose a fault-tolerant framework for asynchronous iterative computations (FAIC). Our framework takes a virtual snapshot of the AIC system without halting the computation of any worker. We prove that the virtual snapshot capture by FAIC can recover the AIC system correctly. We evaluate our FAIC framework on two existing AIC systems, Maiter and NOMAD. Our experiment result shows that the checkpoint overhead of FAIC is more than 50 percent shorter than the synchronous checkpoint method. FAIC is around 10 percent faster than other asynchronous snapshot algorithms, such as the Chandy-Lamport algorithm. Our experiments on a large cluster demonstrate that FAIC scales with the number of workers.},
  archive      = {J_TPDS},
  author       = {Tian Zhou and Lixin Gao and Xiaohong Guan},
  doi          = {10.1109/TPDS.2021.3059420},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2062-2073},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A fault-tolerant distributed framework for asynchronous iterative computations},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Silhouette: Efficient cloud configuration exploration for
large-scale analytics. <em>TPDS</em>, <em>32</em>(8), 2049–2061. (<a
href="https://doi.org/10.1109/TPDS.2021.3058165">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing the best cloud configuration for large-scale data analytics jobs deployed in the cloud can substantially improve their performance and reduce costs. However, current cloud providers offer a wide variety of instance types and customized cluster sizes, making it both time-consuming and costly to pinpoint the optimal cloud configuration. This article presents the design, implementation, and evaluation of Silhouette, a cloud configuration selection framework based on performance models for various large-scale analytics jobs with minimal training overhead. The essence of Silhouette is to build performance prediction models with carefully selected small-scale experiments on small subsets of input data to estimate the performance with entire input data on larger cluster sizes. To reduce the training time and cost, Silhouette incorporates new statistical techniques to select those experiments that yield the best possible information for performance prediction. Moreover, we develop a novel model transformer to convert a prediction model built on one instance type to a different instance type with only one extra experiment, which significantly reduces the training overhead. We evaluate Silhouette with an extensive array of large-scale data analytics jobs on Amazon EC2. Our experimental results have shown convincing evidence that Silhouette is effective in optimizing cloud configuration while saving both training time and costs compared with existing solutions.},
  archive      = {J_TPDS},
  author       = {Yanjiao Chen and Long Lin and Baochun Li and Qian Wang and Qian Zhang},
  doi          = {10.1109/TPDS.2021.3058165},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2049-2061},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Silhouette: Efficient cloud configuration exploration for large-scale analytics},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Hardware accelerator integration tradeoffs for
high-performance computing: A case study of GEMM acceleration in n-body
methods. <em>TPDS</em>, <em>32</em>(8), 2035–2048. (<a
href="https://doi.org/10.1109/TPDS.2021.3056045">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study performance and energy saving benefits of hardware acceleration under different hardware configurations and usage scenarios for a state-of-the-art Fast Multipole Method (FMM), which is a popular N-body method. We use a dedicated Application Specific Integrated Circuit (ASIC) to accelerate General Matrix-Matrix Multiply (GEMM) operations. FMM is widely used in applications and is representative example of the workload for many HPC applications. We compare architectures that integrate the GEMM ASIC next to, in or near main memory with an on-chip coupling aimed at minimizing or avoiding repeated round-trip transfers through DRAM for communication between accelerator and CPU. We study tradeoffs using detailed and accurately calibrated x86 CPU, accelerator and DRAM simulations. Our results show that simply moving accelerators closer to the chip does not necessarily lead to performance/energy gains. We demonstrate that, while careful software blocking and on-chip placement optimizations can reduce DRAM accesses by 2X over a naive on-chip integration, these dramatic savings in DRAM traffic do not automatically translate into significant total energy or runtime savings. This is chiefly due to the application characteristics, the high idle power and effective hiding of memory latencies in modern systems. Only when more aggressive co-optimizations such as software pipelining and overlapping are applied, additional performance and energy savings can be unlocked by 37 and 35 percent respectively over baseline acceleration. When similar optimizations (pipelining and overlapping) are applied with an off-chip integration, on-chip integration delivers up to 20 percent better performance and 17 percent less total energy consumption than off-chip integration.},
  archive      = {J_TPDS},
  author       = {Mochamad Asri and Dhairya Malhotra and Jiajun Wang and George Biros and Lizy K. John and Andreas Gerstlauer},
  doi          = {10.1109/TPDS.2021.3056045},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2035-2048},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hardware accelerator integration tradeoffs for high-performance computing: A case study of GEMM acceleration in N-body methods},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021c). Hone: Mitigating stragglers in distributed stream
processing with tuple scheduling. <em>TPDS</em>, <em>32</em>(8),
2021–2034. (<a
href="https://doi.org/10.1109/TPDS.2021.3051059">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low latency stream processing on large clusters consisting of hundreds to thousands of servers is an increasingly important challenge. A crucial barrier to tackling this challenge is stragglers, i.e., tasks that are significantly straggling behind others in processing the stream data. However, prior straggler mitigation solutions have significant limitations. They balance streaming workloads among tasks but may incur imbalanced backlogs when the workloads exhibit variance, causing stragglers as well. Fortunately, we observe that carefully scheduling the outgoing tuples of different tasks can yield benefits for balancing backlogs, and thus avoids stragglers. To this end, we present Hone, a tuple scheduler that aims to minimize the maximum queue backlog of all tasks over time. Hone leverages an online Largest-Backlog-First (LBF) algorithm with a provable good competitive ratio to perform efficient tuple scheduling. We have implemented Hone based on Apache Storm and evaluated it extensively via both simulations and testbed experiments. Our results show that under the same workload balancing strategy-shuffle grouping, Hone outperforms the original Storm significantly, with the end-to-end tuple processing latency reduced by 78.7 percent on average.},
  archive      = {J_TPDS},
  author       = {Wenxin Li and Duowen Liu and Kai Chen and Keqiu Li and Heng Qi},
  doi          = {10.1109/TPDS.2021.3051059},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2021-2034},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hone: Mitigating stragglers in distributed stream processing with tuple scheduling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Pebbles: Leveraging sketches for processing voluminous,
high velocity data streams. <em>TPDS</em>, <em>32</em>(8), 2005–2020.
(<a href="https://doi.org/10.1109/TPDS.2021.3055265">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voluminous, time-series data streams originating in continuous sensing environments pose data ingestion and processing challenges. We present a holistic methodology centered around data sketching to address both challenges. We introduce an order-preserving sketching algorithm that we have designed for space-efficient representation of multi-feature streams with native support for stream processing related operations. Observational streams are preprocessed at the edges of the network generating sketched streams to reduce data transfer costs and energy consumption. Ingested sketched streams are then processed using sketch-aware extensions to existing stream processing APIs delivering improved performance. Our benchmarks with real-world datasets show up to a ~8× reduction in data volumes transferred and a ~27× improvement in throughput.},
  archive      = {J_TPDS},
  author       = {Thilina Buddhika and Sangmi Lee Pallickara and Shrideep Pallickara},
  doi          = {10.1109/TPDS.2021.3055265},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {2005-2020},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Pebbles: Leveraging sketches for processing voluminous, high velocity data streams},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A GPU acceleration framework for motif and discord based
pattern mining. <em>TPDS</em>, <em>32</em>(8), 1987–2004. (<a
href="https://doi.org/10.1109/TPDS.2021.3055765">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast digitalization of our society, mining patterns from large time series data is increasingly becoming a critical problem for a wide range of big data applications. Motif and discord discovery algorithms, which offer effective solutions to identify repeatedly appearing and abnormal patterns, respectively, are fundamental building blocks for time series processing. Both approaches, however, can be time extremely consuming when handling large time series due to the subsequence-based computations of distance similarity metrics. In this article, we show that the highly involved subsequence-based computations can actually be decomposed into a few fine-grained computing patterns for efficient data parallel computing. By developing highly efficient GPU algorithms for such basic patterns and effectively composing such patterns, we are able to solve both motif and discord discovery problems under euclidean and DTW distance metrics in a unified GPU acceleration framework. Extensive experiments prove that the proposed framework outperforms pruned CPU algorithms by up to three orders of magnitude. Our work paves the foundation of building GPU acceleration frameworks for large-scale time series datasets.},
  archive      = {J_TPDS},
  author       = {Biru Zhu and Youyou Jiang and Ming Gu and Yangdong Deng},
  doi          = {10.1109/TPDS.2021.3055765},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1987-2004},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A GPU acceleration framework for motif and discord based pattern mining},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). True load balancing for matricized tensor times
khatri-rao product. <em>TPDS</em>, <em>32</em>(8), 1974–1986. (<a
href="https://doi.org/10.1109/TPDS.2021.3053836">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MTTKRP is the bottleneck operation in algorithms used to compute the CP tensor decomposition. For sparse tensors, utilizing the compressed sparse fibers (CSF) storage format and the CSF-oriented MTTKRP algorithms is important for both memory and computational efficiency on distributed-memory architectures. Existing intelligent tensor partitioning models assume the computational cost of MTTKRP to be proportional to the total number of nonzeros in the tensor. However, this is not the case for the CSF-oriented MTTKRP on distributed-memory architectures. We outline two deficiencies of nonzero-based intelligent partitioning models when CSF-oriented MTTKRP operations are performed locally: failure to encode processors&#39; computational loads and increase in total computation due to fiber fragmentation. We focus on existing fine-grain hypergraph model and propose a novel vertex weighting scheme that enables this model encode correct computational loads of processors. We also propose to augment the fine-grain model by fiber nets for reducing the increase in total computational load via minimizing fiber fragmentation. In this way, the proposed model encodes minimizing the load of the bottleneck processor. Parallel experiments with real-world sparse tensors on up to 1024 processors prove the validity of the outlined deficiencies and demonstrate the merit of our proposed improvements in terms of parallel runtimes.},
  archive      = {J_TPDS},
  author       = {Nabil Abubaker and Seher Acer and Cevdet Aykanat},
  doi          = {10.1109/TPDS.2021.3053836},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1974-1986},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {True load balancing for matricized tensor times khatri-rao product},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). E-PoS: Making proof-of-stake decentralized and fair.
<em>TPDS</em>, <em>32</em>(8), 1961–1973. (<a
href="https://doi.org/10.1109/TPDS.2020.3048853">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain applications that rely on the Proof-of-Work (PoW) have increasingly become energy inefficient with a staggering carbon footprint. In contrast, energy efficient alternative consensus protocols such as Proof-of-Stake (PoS) may cause centralization and unfairness in the blockchain system. To address these challenges, we propose a modular version of PoS-based blockchain systems called e-PoS that resists the centralization of network resources by extending mining opportunities to a wider set of stakeholders. Moreover, e-PoS leverages the in-built system operations to promote fair mining practices by penalizing malicious entities. We validate e-PoS &#39;s achievable objectives through theoretical analysis and simulations. Our results show that e-PoS ensures fairness and decentralization, and can be applied to existing blockchain applications.},
  archive      = {J_TPDS},
  author       = {Muhammad Saad and Zhan Qin and Kui Ren and DaeHun Nyang and David Mohaisen},
  doi          = {10.1109/TPDS.2020.3048853},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1961-1973},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {E-PoS: Making proof-of-stake decentralized and fair},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). DL2: A deep learning-driven scheduler for deep learning
clusters. <em>TPDS</em>, <em>32</em>(8), 1947–1960. (<a
href="https://doi.org/10.1109/TPDS.2021.3052895">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource scheduling is essential for maximal utilization of expensive deep learning (DL) clusters. Existing cluster schedulers either are agnostic to machine learning (ML) workload characteristics, or use scheduling heuristics based on operators&#39; understanding of particular ML framework and workload, which are less efficient or not general enough. In this article, we show that DL techniques can be adopted to design a generic and efficient scheduler. Specifically, we propose DL2, a DL-driven scheduler for DL clusters, targeting global training job expedition by dynamically resizing resources allocated to jobs. DL2 advocates a joint supervised learning and reinforcement learning approach: a neural network is warmed up via offline supervised learning based on job traces produced by the existing cluster scheduler; then the neural network is plugged into the live DL cluster, fine-tuned by reinforcement learning carried out throughout the training progress of the DL jobs, and used for deciding job resource allocation in an online fashion. We implement DL2 on Kubernetes and enable dynamic resource scaling in DL jobs on MXNet. Extensive evaluation shows that DL2 outperforms fairness scheduler (i.e., DRF) by 44.1 percent and expert heuristic scheduler (i.e., Optimus) by 17.5 percent in terms of average job completion time.},
  archive      = {J_TPDS},
  author       = {Yanghua Peng and Yixin Bao and Yangrui Chen and Chuan Wu and Chen Meng and Wei Lin},
  doi          = {10.1109/TPDS.2021.3052895},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1947-1960},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {DL2: A deep learning-driven scheduler for deep learning clusters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). An optimized weighted average makespan in fault-tolerant
heterogeneous MPSoCs. <em>TPDS</em>, <em>32</em>(8), 1933–1946. (<a
href="https://doi.org/10.1109/TPDS.2021.3053150">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiprocessor system on chips (MPSoCs) are considered today the core of most modern systems. Most of the applications of these heterogeneous MPSoCs include critical systems and hence terms of fault tolerance and reliability have become essential. Task replication is a technique to carry out fault tolerance and can help for reducing the schedule length by increasing locality. It introduces an upper and lower bound for the makespan of each schedule while each task is replicated more than once. If a fault occurs during execution, the expected makespan will be some value between the upper bound and the lower bound based on when and where the fault has occurred. In this research a new performance parameter namely the weighted average makespan is introduced. It is calculated as the average of the lower and upper bounds of makespan using the probability of occurrence of each. Two scheduling algorithms are presented for fault tolerant scheduling based on directed acyclic graphs. These algorithms are the list scheduling algorithm and the optimizing of the weighted average makespan based on simulated annealing method. The simulation results show that the techniques can improve the schedule length and increase the system reliability without compromising the performance.},
  archive      = {J_TPDS},
  author       = {Hassan Youness and Aly Omar and Mohamed Moness},
  doi          = {10.1109/TPDS.2021.3053150},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1933-1946},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An optimized weighted average makespan in fault-tolerant heterogeneous MPSoCs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Burst load evacuation based on dispatching and scheduling
in distributed edge networks. <em>TPDS</em>, <em>32</em>(8), 1918–1932.
(<a href="https://doi.org/10.1109/TPDS.2021.3052236">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing, a fast evolving computing paradigm, has spawned a variety of new system architectures and computing methods discussed in both academia and industry. Edge servers are directly deployed near users’ equipment or devices owned by telecommunications companies. This allows for offloading computing tasks of various devices nearby to edge servers. Due to the shortage of computing resources in edge computing networks, they are often not as sufficient as the computing resources in a cloud computing center. This leads to the problem of service load imbalance once the load in the edge computing network increases suddenly. To solve the problem of “load evacuation” in edge environments, we introduce a strategy when the number of service requests for mobile devices or IoT devices increases rapidly within a short period of time. Therefore, to prevent poor QoS in edge computing, service load should be migrated to other edge servers to reduce the overall delay of these service requests. In this article, we have introduced a strategy with two stages during the burst load evacuation. Based on an optimal routing search at the dispatching stage, tasks will be migrated from the server in which the burst load occurs to other servers as soon as possible. Subsequently, with the assistance of the remote server and edge servers, these tasks are processed with the highest efficiency through the proposed parallel structure at the scheduling stage. Finally, we conduct numerical experiments to clarify the superiority of our algorithm in an edge environment simulation.},
  archive      = {J_TPDS},
  author       = {Shuiguang Deng and Cheng Zhang and Chang Li and Jianwei Yin and Schahram Dustdar and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2021.3052236},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1918-1932},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Burst load evacuation based on dispatching and scheduling in distributed edge networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). MG-WFBP: Merging gradients wisely for efficient
communication in distributed deep learning. <em>TPDS</em>,
<em>32</em>(8), 1903–1917. (<a
href="https://doi.org/10.1109/TPDS.2021.3052862">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed synchronous stochastic gradient descent has been widely used to train deep neural networks (DNNs) on computer clusters. With the increase of computational power, network communications generally limit the system scalability. Wait-free backpropagation (WFBP) is a popular solution to overlap communications with computations during the training process. In this article, we observe that many DNNs have a large number of layers with only a small amount of data to be communicated at each layer in distributed training, which could make WFBP inefficient. Based on the fact that merging some short communication tasks into a single one can reduce the overall communication time, we formulate an optimization problem to minimize the training time in pipelining communications and computations. We derive an optimal solution that can be solved efficiently without affecting the training performance. We then apply the solution to propose a distributed training algorithm named merged-gradient WFBP (MG-WFBP) and implement it in two platforms Caffe and PyTorch. Extensive experiments in three GPU clusters are conducted to verify the effectiveness of MG-WFBP. We further exploit trace-based simulations of 4 to 2048 GPUs to explore the potential scaling efficiency of MG-WFBP. Experimental results show that MG-WFBP achieves much better scaling performance than existing methods.},
  archive      = {J_TPDS},
  author       = {Shaohuai Shi and Xiaowen Chu and Bo Li},
  doi          = {10.1109/TPDS.2021.3052862},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {8},
  pages        = {1903-1917},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MG-WFBP: Merging gradients wisely for efficient communication in distributed deep learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). EDGES: An efficient distributed graph embedding system on
GPU clusters. <em>TPDS</em>, <em>32</em>(7), 1892–1902. (<a
href="https://doi.org/10.1109/TPDS.2020.3041219">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding training models access parameters sparsely in a “one-hot” manner. Currently, the distributed graph embedding neural network is learned by data parallel with the parameter server, which suffers significant performance and scalability problems. In this article, we analyze the problems and characteristics of training this kind of models on distributed GPU clusters for the first time, and find that fixed model parameters scattered among different machine nodes are a major limiting factor for efficiency. Based on our observation, we develop an efficient distributed graph embedding system called EDGES, which can utilize GPU clusters to train large graph models with billions of nodes and trillions of edges using data and model parallelism. Within the system, we propose a novel dynamic partition architecture for training these models, achieving at least one half of communication reduction compared to existing training systems. According to our evaluations on real-world networks, our system delivers a competitive accuracy for the trained embeddings, and significantly accelerates the training process of the graph node embedding neural network, achieving a speedup of 7.23x and 18.6x over the existing fastest training system on single node and multi-node, respectively. As for the scalability, our experiments show that EDGES obtains a nearly linear speedup.},
  archive      = {J_TPDS},
  author       = {Dongxu Yang and Junhong Liu and Junjie Lai},
  doi          = {10.1109/TPDS.2020.3041219},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1892-1902},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {EDGES: An efficient distributed graph embedding system on GPU clusters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accelerating binarized neural networks via
bit-tensor-cores in turing GPUs. <em>TPDS</em>, <em>32</em>(7),
1878–1891. (<a
href="https://doi.org/10.1109/TPDS.2020.3045828">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite foreseeing tremendous speedups over conventional deep neural networks, the performance advantage of binarized neural networks (BNNs) has merely been showcased on general-purpose processors such as CPUs and GPUs. In fact, due to being unable to leverage bit-level-parallelism with a word-based architecture, GPUs have been criticized for extremely low utilization (1 percent) when executing BNNs. Consequently, the latest tensorcores in NVIDIA Turing GPUs start to experimentally support bit computation. In this article, we look into this brand new bit computation capability and characterize its unique features. We show that the stride of memory access can significantly affect performance delivery and a data-format co-design is highly desired to support the tensorcores for achieving superior performance than existing software solutions without tensorcores. We realize the tensorcore-accelerated BNN design, particularly the major functions for fully-connect and convolution layers - bit matrix multiplication and bit convolution. Evaluations on two NVIDIA Turing GPUs show that, with ResNet-18, our BTC-BNN design can process ImageNet at a rate of 5.6K images per second, 77 percent faster than state-of-the-art. Our BNN approach is released on https://github.com/pnnl/TCBNN.},
  archive      = {J_TPDS},
  author       = {Ang Li and Simon Su},
  doi          = {10.1109/TPDS.2020.3045828},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1878-1891},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating binarized neural networks via bit-tensor-cores in turing GPUs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Efficient methods for mapping neural machine translator
on FPGAs. <em>TPDS</em>, <em>32</em>(7), 1866–1877. (<a
href="https://doi.org/10.1109/TPDS.2020.3047371">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural machine translation (NMT) is one of the most critical applications in natural language processing (NLP) with the main idea of converting text in one language to another using deep neural networks. In recent year, we have seen continuous development of NMT by integrating more emerging technologies, such as bidirectional gated recurrent units (GRU), attention mechanisms, and beam-search algorithms, for improved translation quality. However, with the increasing problem size, the real-life NMT models have become much more complicated and difficult to implement on hardware for acceleration opportunities. In this article, we aim to exploit the capability of FPGAs to deliver highly efficient implementations for real-life NMT applications. We map the inference of a large-scale NMT model with total computation of 172 GFLOP to a highly optimized high-level synthesis (HLS) IP and integrate the IP into Xilinx VCU118 FPGA platform. The model has widely used key features for NMTs, including the bidirectional GRU layer, attention mechanism, and beam search. We quantize the model to mixed-precision representation in which parameters and portions of calculations are in 16-bit half precision, and others remain as 32-bit floating-point. Compared to the float NMT implementation on FPGA, we achieve 13.1× speedup with an end-to-end performance of 22.0 GFLOPS without any accuracy degradation. Based on our knowledge, this is the first work that successfully implements a real-life end-to-end NMT model to an FPGA on board.},
  archive      = {J_TPDS},
  author       = {Qin Li and Xiaofan Zhang and Jinjun Xiong and Wen-Mei Hwu and Deming Chen},
  doi          = {10.1109/TPDS.2020.3047371},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1866-1877},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient methods for mapping neural machine translator on FPGAs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Improving HW/SW adaptability for accelerating CNNs on
FPGAs through a dynamic/static co-reconfiguration approach.
<em>TPDS</em>, <em>32</em>(7), 1854–1865. (<a
href="https://doi.org/10.1109/TPDS.2020.3046762">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous evolution of Convolutional Neural Networks (CNNs) and the improvement of the computing capability of FPGAs, the deployment of CNN accelerator based on FPGA has become more and more popular in various computing scenarios. The key element of implementing these accelerators is to take full advantage of underlying hardware characteristics to adapt to the computational features of the software-level CNN model. To achieve this goal, however, previous designs mainly focus on the static hardware reconfiguration pattern, which is not flexible enough and can hardly make the accelerator architecture and the CNN features fully fit, resulting in inefficient computations and data communications. By leveraging the dynamic partial reconfiguration technology equipped in the modern FPGA devices, in this article, we propose a new accelerator architecture for implementing CNNs on FPGAs in which static and dynamic reconfigurabilities of the hardware are cooperatively utilized to maximize the acceleration efficiency. Based on this architecture, we further present a systematic design and optimization methodology for implementing the specific CNN model in the particular computing scenario, in which a static design space exploration method and a reinforcement learning-based decision method are proposed to obtain the optimal static hardware configuration and run-time reconfiguration strategy respectively. We evaluate our proposal by implementing three widely used CNN models, AlexNet, VGG16C, and ResNet34, on the Xilinx ZCU102 FPGA platform. Experimental results show that our implementations on average can achieve 683 GOPS under 16-bit fixed data type and 1.37 TOPS under 8-bit fixed data type for three targeted CNN models, and improve the computational density from 1.1× to 1.91× compared with previous implementations on the same type of FPGA platform.},
  archive      = {J_TPDS},
  author       = {Lei Gong and Chao Wang and Xi Li and Xuehai Zhou},
  doi          = {10.1109/TPDS.2020.3046762},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1854-1865},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving HW/SW adaptability for accelerating CNNs on FPGAs through a Dynamic/Static co-reconfiguration approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Adaptive SpMV/SpMSpV on GPUs for input vectors of varied
sparsity. <em>TPDS</em>, <em>32</em>(7), 1842–1853. (<a
href="https://doi.org/10.1109/TPDS.2020.3040150">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite numerous efforts for optimizing the performance of Sparse Matrix and Vector Multiplication (SpMV) on modern hardware architectures, few works are done to its sparse counterpart, Sparse Matrix and Sparse Vector Multiplication (SpMSpV), not to mention dealing with input vectors of varied sparsity. The key challenge is that depending on the sparsity levels, distribution of data, and compute platform, the optimal choice of SpMV/SpMSpV kernel can vary, and a static choice does not suffice. In this article, we propose an adaptive SpMV/SpMSpV framework, which can automatically select the appropriate SpMV/SpMSpV kernel on GPUs for any sparse matrix and vector at the runtime. Based on systematic analysis on key factors such as computing pattern, workload distribution and write-back strategy, eight candidate SpMV/SpMSpV kernels are encapsulated into the framework to achieve high performance in a seamless manner. A comprehensive study on machine learning-based kernel selector is performed to choose the kernel and adapt with the varieties of both the input and hardware from both accuracy and overhead perspectives. Experiments demonstrate that the adaptive framework can substantially outperform the previous state-of-the-art in real-world applications on NVIDIA Tesla K40m, P100, and V100 GPUs.},
  archive      = {J_TPDS},
  author       = {Min Li and Yulong Ao and Chao Yang},
  doi          = {10.1109/TPDS.2020.3040150},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1842-1853},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive SpMV/SpMSpV on GPUs for input vectors of varied sparsity},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>[art_2021_101109tpds20203047460_haoliandzixuanli_sgd__tuckeranovelstochasticoptim]</p></li>
<li><p>(2021). A probabilistic machine learning approach to scheduling
parallel loops with bayesian optimization. <em>TPDS</em>,
<em>32</em>(7), 1815–1827. (<a
href="https://doi.org/10.1109/TPDS.2020.3046461">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes Bayesian optimization augmented factoring self-scheduling (BO FSS), a new parallel loop scheduling strategy. BO FSS is an automatic tuning variant of the factoring self-scheduling (FSS) algorithm and is based on Bayesian optimization (BO), a black-box optimization algorithm. Its core idea is to automatically tune the internal parameter of FSS by solving an optimization problem using BO. The tuning procedure only requires online execution time measurement of the target loop. In order to apply BO, we model the execution time using two Gaussian process (GP) probabilistic machine learning models. Notably, we propose a locality-aware GP model, which assumes that the temporal locality effect resembles an exponentially decreasing function. By accurately modeling the temporal locality effect, our locality-aware GP model accelerates the convergence of BO. We implemented BO FSS on the GCC implementation of the OpenMP standard and evaluated its performance against other scheduling algorithms. Also, to quantify our method&#39;s performance variation on different workloads, or workload-robustness in our terms, we measure the minimax regret. According to the minimax regret, BO FSS shows more consistent performance than other algorithms. Within the considered workloads, BO FSS improves the execution time of FSS by as much as 22\% and 5\% on average.},
  archive      = {J_TPDS},
  author       = {Khu-Rai Kim and Youngjae Kim and Sungyong Park},
  doi          = {10.1109/TPDS.2020.3046461},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1815-1827},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A probabilistic machine learning approach to scheduling parallel loops with bayesian optimization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accelerating end-to-end deep learning workflow with
codesign of data preprocessing and scheduling. <em>TPDS</em>,
<em>32</em>(7), 1802–1814. (<a
href="https://doi.org/10.1109/TPDS.2020.3047966">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the performance bottleneck of existing deep learning (DL) systems and propose DLBooster to improve the running efficiency of deploying DL applications on GPU clusters. At its core, DLBooster leverages two-level optimizations to boost the end-to-end DL workflow. On the one hand, DLBooster selectively offloads some key decoding workloads to FPGAs to provide high-performance online data preprocessing services to the computing engine. On the other hand, DLBooster reorganizes the computational workloads of training neural networks with the backpropagation algorithm and schedules them according to their dependencies to improve the utilization of GPUs at runtime. Based on our experiments, we demonstrate that compared with baselines, DLBooster can improve the image processing throughput by 1.4× - 2.5× and reduce the processing latency by 1/3 in several real-world DL applications and datasets. Moreover, DLBooster consumes less than 1 CPU core to manage FPGA devices at runtime, which is at least 90 percent less than the baselines in some cases. DLBooster shows its potential to accelerate DL workflows in the cloud.},
  archive      = {J_TPDS},
  author       = {Yang Cheng and Dan Li and Zhiyuan Guo and Binyao Jiang and Jinkun Geng and Wei Bai and Jianping Wu and Yongqiang Xiong},
  doi          = {10.1109/TPDS.2020.3047966},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1802-1814},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating end-to-end deep learning workflow with codesign of data preprocessing and scheduling},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Fine-grained powercap allocation for power-constrained
systems based on multi-objective machine learning. <em>TPDS</em>,
<em>32</em>(7), 1789–1801. (<a
href="https://doi.org/10.1109/TPDS.2020.3045983">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power capping is an important solution to keep the system within a fixed power constraint. However, for the over-provisioned and power-constrained systems, especially the future exascale supercomputers, powercap needs to be reasonably allocated according to the workloads of compute nodes to achieve trade-offs among performance, energy and powercap. Thus it is necessary to model performance and energy and to predict the optimal powercap allocation strategies. Existing power allocation approaches have insufficient granularity within nodes. Modeling approaches usually model performance and energy separately, ignoring the correlation between objectives, and do not expose the Pareto-optimal powercap configurations. Therefore, this article combines the powercap with uncore frequency scaling and proposes an approach to predict the Pareto-optimal powercap configurations on the power-constrained system for input MPI and OpenMP parallel applications. Our approach first uses the elaborately designed micro-benchmarks and a small number of existing benchmarks to build the training set, and then applies a multi-objective machine learning algorithm which combines the stacked single-target method with extreme gradient boosting to build multi-objective models of performance and energy. The models can be used to predict the optimal processor and memory powercap settings, helping compute nodes perform fine-grained powercap allocation. When the optimal powercap configuration is determined, the uncore frequency scaling is used to further optimize the energy consumption. Compared with the reference powercap configuration, the predicted optimal configurations predicted by our method can achieve an average powercap reduction of 31.35 percent, an average energy reduction of 12.32 percent, and average performance degradation of only 2.43 percent.},
  archive      = {J_TPDS},
  author       = {Meng Hao and Weizhe Zhang and Yiming Wang and Gangzhao Lu and Farui Wang and Athanasios V. Vasilakos},
  doi          = {10.1109/TPDS.2020.3045983},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1789-1801},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fine-grained powercap allocation for power-constrained systems based on multi-objective machine learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Privacy-preserving computation offloading for parallel
deep neural networks training. <em>TPDS</em>, <em>32</em>(7), 1777–1788.
(<a href="https://doi.org/10.1109/TPDS.2020.3040734">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have brought significant performance improvements to various real-life applications. However, a DNN training task commonly requires intensive computing resources and a huge data collection, which makes it hard for personal devices to carry out the entire training, especially for mobile devices. The federated learning concept has eased this situation. However, it is still an open problem for individuals to train their own DNN models at an affordable price. In this article, we propose an alternative DNN training strategy for resource-limited users. With the help of an untrusted server, end users can offload their DNN training tasks to the server in a privacy-preserving manner. To this end, we study the possibility of the separation of a DNN. Then we design a differentially private activation algorithm for end users to ensure the privacy of the offloading after model separation. Furthermore, to meet the rising demand for federated learning, we extend the offloading solution to parallel DNN models training with a secure model weights aggregation scheme for the privacy concern. Experimental results prove the feasibility of computation offloading solutions for DNN models in both solo and parallel modes.},
  archive      = {J_TPDS},
  author       = {Yunlong Mao and Wenbo Hong and Heng Wang and Qun Li and Sheng Zhong},
  doi          = {10.1109/TPDS.2020.3040734},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1777-1788},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy-preserving computation offloading for parallel deep neural networks training},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Parallel blockwise knowledge distillation for deep neural
network compression. <em>TPDS</em>, <em>32</em>(7), 1765–1776. (<a
href="https://doi.org/10.1109/TPDS.2020.3047003">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been extremely successful in solving many challenging AI tasks in natural language processing, speech recognition, and computer vision nowadays. However, DNNs are typically computation intensive, memory demanding, and power hungry, which significantly limits their usage on platforms with constrained resources. Therefore, a variety of compression techniques (e.g., quantization, pruning, and knowledge distillation) have been proposed to reduce the size and power consumption of DNNs. Blockwise knowledge distillation is one of the compression techniques that can effectively reduce the size of a highly complex DNN. However, it is not widely adopted due to its long training time. In this article, we propose a novel parallel blockwise distillation algorithm to accelerate the distillation process of sophisticated DNNs. Our algorithm leverages local information to conduct independent blockwise distillation, utilizes depthwise separable layers as the efficient replacement block architecture, and properly addresses limiting factors (e.g., dependency, synchronization, and load balancing) that affect parallelism. The experimental results running on an AMD server with four Geforce RTX 2080Ti GPUs show that our algorithm can achieve 3x speedup plus 19 percent energy savings on VGG distillation, and 3.5x speedup plus 29 percent energy savings on ResNet distillation, both with negligible accuracy loss. The speedup of ResNet distillation can be further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster.},
  archive      = {J_TPDS},
  author       = {Cody Blakeney and Xiaomin Li and Yan Yan and Ziliang Zong},
  doi          = {10.1109/TPDS.2020.3047003},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1765-1776},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallel blockwise knowledge distillation for deep neural network compression},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A distributed framework for EA-based NAS. <em>TPDS</em>,
<em>32</em>(7), 1753–1764. (<a
href="https://doi.org/10.1109/TPDS.2020.3046774">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary Algorithms (EA) are widely applied in Neural Architecture Search (NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow. Specifically, the first step is the initialization of the population with different encoding schemes, and the second step is the evaluation of the individuals by the fitness function. Then, the EA-based NAS algorithm executes evolution operations, e.g., selection, mutation, and crossover, to eliminate weak individuals and generate more competitive ones. Lastly, evolution continues until the max generation and the best neural architectures will be chosen. Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this article proposes a distributed framework to boost the computation of the EA-based NAS algorithm. This framework is a server/worker model where the server distributes individuals requested by the computing nodes and collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) of the EA-based NAS is allocated to the computing nodes, which send requests asynchronously to the server and evaluate the fitness values of the individuals. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representations and support different EA-based NAS algorithms. We design an EA-based NAS algorithm as a case to investigate the efficiency of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the search time with the increase of the computational nodes. Furthermore, the length of the exchanged messages among the cluster is tiny, which benefits the framework expansion.},
  archive      = {J_TPDS},
  author       = {Qing Ye and Yanan Sun and Jixin Zhang and Jiancheng Lv},
  doi          = {10.1109/TPDS.2020.3046774},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1753-1764},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A distributed framework for EA-based NAS},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). IMLBench: A machine learning benchmark suite for CPU-GPU
integrated architectures. <em>TPDS</em>, <em>32</em>(7), 1740–1752. (<a
href="https://doi.org/10.1109/TPDS.2020.3046870">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing heterogeneous accelerators, especially GPUs, to accelerate machine learning tasks has shown to be a great success in recent years. GPUs bring huge performance improvements to machine learning and greatly promote the widespread adoption of machine learning. However, the discrete CPU-GPU architecture design with high PCIe transmission overhead decreases the GPU computing benefits in machine learning training tasks. To overcome such limitations, hardware vendors release CPU-GPU integrated architectures with shared unified memory. In this article, we design a benchmark suite for machine learning training on CPU-GPU integrated architectures, called iMLBench, covering a wide range of machine learning applications and kernels. We mainly explore two features on integrated architectures: 1) zero-copy, which means that the PCIe overhead has been eliminated for machine learning tasks and 2) co-running, which means that the CPU and the GPU co-run together to process a single machine learning task. Our experimental results on iMLBench show that the integrated architecture brings an average 7.1× performance improvement over the original implementations. Specifically, the zero-copy design brings 4.65× performance improvement, and co-running brings 1.78× improvement. Moreover, integrated architectures exhibit promising results from both performance-per-dollar and energy perspectives, achieving 6.50× performance-price ratio while 4.06× energy efficiency over discrete GPUs. The benchmark is open-sourced at https://github.com/ChenyangZhang-cs/iMLBench.},
  archive      = {J_TPDS},
  author       = {Chenyang Zhang and Feng Zhang and Xiaoguang Guo and Bingsheng He and Xiao Zhang and Xiaoyong Du},
  doi          = {10.1109/TPDS.2020.3046870},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1740-1752},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IMLBench: A machine learning benchmark suite for CPU-GPU integrated architectures},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Breaking (global) barriers in parallel stochastic
optimization with wait-avoiding group averaging. <em>TPDS</em>,
<em>32</em>(7), 1725–1739. (<a
href="https://doi.org/10.1109/TPDS.2020.3040606">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning at scale is dominated by communication time. Distributing samples across nodes usually yields the best performance, but poses scaling challenges due to global information dissemination and load imbalance across uneven sample lengths. State-of-the-art decentralized optimizers mitigate the problem, but require more iterations to achieve the same accuracy as their globally-communicating counterparts. We present Wait-Avoiding Group Model Averaging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces global communication via subgroup weight exchange. The key insight is a combination of algorithmic changes to the averaging scheme and the use of a group allreduce operation. We prove the convergence of WAGMA-SGD, and empirically show that it retains convergence rates similar to Allreduce-SGD. For evaluation, we train ResNet-50 on ImageNet; Transformer for machine translation; and deep reinforcement learning for navigation at scale. Compared with state-of-the-art decentralized SGD variants, WAGMA-SGD significantly improves training throughput (e.g., 2.1× on 1,024 GPUs for reinforcement learning), and achieves the fastest time-to-solution (e.g., the highest score using the shortest training time for Transformer).},
  archive      = {J_TPDS},
  author       = {Shigang Li and Tal Ben-Nun and Giorgi Nadiradze and Salvatore Di Girolamo and Nikoli Dryden and Dan Alistarh and Torsten Hoefler},
  doi          = {10.1109/TPDS.2020.3040606},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1725-1739},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Breaking (Global) barriers in parallel stochastic optimization with wait-avoiding group averaging},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A runtime and non-intrusive approach to optimize EDP by
tuning threads and CPU frequency for OpenMP applications. <em>TPDS</em>,
<em>32</em>(7), 1713–1724. (<a
href="https://doi.org/10.1109/TPDS.2020.3046537">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently exploiting thread-level parallelism has been challenging. Many parallel applications are not sufficiently balanced or CPU-bound to take advantage of the increasing number of cores and the highest possible operating frequency. Moreover, many variables may change according to the system (input set, microarchitecture, and number of cores) or during execution, influencing each parallel region in different ways. Therefore, the task of rightly choosing the ideal configuration (number of threads and DVFS) for each parallel region to deliver the best Energy-Delay Product (EDP) is not straightforward. While the significant number of variables prevents the use of exhaustive search methods, the changing nature of the problem precludes offline strategies. Few solutions are online and synergistically consider thread throttling and DVFS. However, they lack transparency (demand changes in the original code) and/or adaptability (do not automatically adjust to applications at run-time). Our proposed Hoder covers all the characteristics above, optimizing at run-time any dynamically linked OpenMP application, without requiring any code transformation or recompilation. We show Hoder&#39;s efficiency by comparing it to two exhaustive offline and two online search approaches, three state-of-the-art techniques, and regular OpenMP execution, considering different setups (Intel 44-, 16- and 12-core; AMD 8- and 12-core).},
  archive      = {J_TPDS},
  author       = {Janaina Schwarzrock and Charles C. de Oliveira and Marcus Ritt and Arthur F. Lorenzon and Antonio Carlos S. Beck},
  doi          = {10.1109/TPDS.2020.3046537},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1713-1724},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A runtime and non-intrusive approach to optimize EDP by tuning threads and CPU frequency for OpenMP applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Why dataset properties bound the scalability of parallel
machine learning training algorithms. <em>TPDS</em>, <em>32</em>(7),
1702–1712. (<a
href="https://doi.org/10.1109/TPDS.2020.3048836">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the training dataset size and the model size of machine learning increase rapidly, more computing resources are consumed to speedup the training process. However, the scalability and performance reproducibility of parallel machine learning training, which mainly uses stochastic optimization algorithms, are limited. In this paper, we demonstrate that the sample difference in the dataset plays a prominent role in the scalability of parallel machine learning algorithms. We propose to use statistical properties of dataset to measure sample differences. These properties include the variance of sample features, sample sparsity, sample diversity, and similarity in sampling sequences. We choose four types of parallel training algorithms as our research objects: (1) the asynchronous parallel SGD algorithm (Hogwild! algorithm), (2) the parallel model average SGD algorithm (minibatch SGD algorithm), (3) the decentralization optimization algorithm, and (4) the dual coordinate optimization (DADM algorithm). Our results show that the statistical properties of training datasets determine the scalability upper bound of these parallel training algorithms.},
  archive      = {J_TPDS},
  author       = {Daning Cheng and Shigang Li and Hanping Zhang and Fen Xia and Yunquan Zhang},
  doi          = {10.1109/TPDS.2020.3048836},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1702-1712},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Why dataset properties bound the scalability of parallel machine learning training algorithms},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). SmartTuning: Selecting hyper-parameters of a ConvNet
system for fast training and small working memory. <em>TPDS</em>,
<em>32</em>(7), 1690–1701. (<a
href="https://doi.org/10.1109/TPDS.2020.3040723">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is desirable to deploy a ConvNet system with high inference accuracy, as well as fast training and small inference memory. However, existing approaches to hyper-parameter tuning only focus on high accuracy. Although achieving high accuracy, tuning poorly can significantly increase the performance burden, and thus degrade the overall performance of a ConvNet system. In this article, we propose SmartTuning, an approach to identifying the hyper-parameters of a ConvNet system for high training speed and small working memory, with the restriction of high inference accuracy. The key idea of SmartTuning is to build a new performance model for a ConvNet system, and to integrate Bayesian Optimization to learn the relationship between the overall performance and the hyper-parameters of a ConvNet system. In this way, SmartTuning can balance inference accuracy, training speed and inference memory usage during the tuning process, and thus maximizes the overall performance of a ConvNet system. Our experiments show that SmartTuning can stably identify the hyper-parameter sets that offer very close accuracy with faster training speed (i.e., 7×-11× over MNIST and 2×-3× over CIFAR-10) and much less inference memory usage (i.e., 17×-23× over MNIST and 4×-9× over CIFAR-10), compared with existing tuning approaches.},
  archive      = {J_TPDS},
  author       = {Xiaqing Li and Guangyan Zhang and Weimin Zheng},
  doi          = {10.1109/TPDS.2020.3040723},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1690-1701},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SmartTuning: Selecting hyper-parameters of a ConvNet system for fast training and small working memory},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). FT-CNN: Algorithm-based fault tolerance for convolutional
neural networks. <em>TPDS</em>, <em>32</em>(7), 1677–1689. (<a
href="https://doi.org/10.1109/TPDS.2020.3043449">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are becoming more and more important for solving challenging and critical problems in many fields. CNN inference applications have been deployed in safety-critical systems, which may suffer from soft errors caused by high-energy particles, high temperature, or abnormal voltage. Of critical importance is ensuring the stability of the CNN inference process against soft errors. Traditional fault tolerance methods are not suitable for CNN inference because error-correcting code is unable to protect computational components, instruction duplication techniques incur high overhead, and existing algorithm-based fault tolerance (ABFT) techniques cannot protect all convolution implementations. In this article, we focus on how to protect the CNN inference process against soft errors as efficiently as possible, with the following three contributions. (1) We propose several systematic ABFTschemes based on checksum techniques and analyze their fault protection ability and runtime thoroughly. Unlike traditional ABFT based on matrix-matrix multiplication, our schemes support any convolution implementations. (2) We design a novel workflow integrating all the proposed schemes to obtain a high detection/correction ability with limited total runtime overhead. (3) We perform our evaluation using ImageNet with well-known CNN models including AlexNet, VGG-19, ResNet-18, and YOLOv2. Experimental results demonstrate that our implementation can handle soft errors with very limited runtime overhead (4\%~8\% in both error-free and error-injected situations).},
  archive      = {J_TPDS},
  author       = {Kai Zhao and Sheng Di and Sihuan Li and Xin Liang and Yujia Zhai and Jieyang Chen and Kaiming Ouyang and Franck Cappello and Zizhong Chen},
  doi          = {10.1109/TPDS.2020.3043449},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1677-1689},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FT-CNN: Algorithm-based fault tolerance for convolutional neural networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Model parallelism optimization for distributed inference
via decoupled CNN structure. <em>TPDS</em>, <em>32</em>(7), 1665–1676.
(<a href="https://doi.org/10.1109/TPDS.2020.3041474">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is promising to deploy CNN inference on local end-user devices for high-accuracy and time-sensitive applications. Model parallelism has the potential to provide high throughput and low latency in distributed CNN inference. However, it is non-trivial to use model parallelism as the original CNN model is inherently tightly-coupled structure. In this article, we propose DeCNN, a more effective inference approach that uses decoupled CNN structure to optimize model parallelism for distributed inference on end-user devices. DeCNN is novel consisting of three schemes. Scheme-1 is structure-level optimization. It exploits group convolution and channel shuffle to decouple the original CNN structure for model parallelism. Scheme-2 is partition-level optimization. It is based on channel group to partition the convolutional layers, and then leverages input-based method to partition the fully connected layers, further exposing high degree of parallelism. Scheme-3 is communication-level optimization. It uses inter-sample parallelism to hide communications for better performance and robustness, especially in the weak network connections. We use ImageNet classification task to evaluate the effectiveness of DeCNN on a distributed multi-ARM platform. Notably, when using the number of devices from 1 to 4, DeCNN can accelerate the inference of large-scale ResNet-50 by 3.21×, and reduce 65.3 percent memory footprint, with 1.29 percent accuracy improvement.},
  archive      = {J_TPDS},
  author       = {Jiangsu Du and Xin Zhu and Minghua Shen and Yunfei Du and Yutong Lu and Nong Xiao and Xiangke Liao},
  doi          = {10.1109/TPDS.2020.3041474},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1665-1676},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Model parallelism optimization for distributed inference via decoupled CNN structure},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A hybrid fuzzy convolutional neural network based
mechanism for photovoltaic cell defect detection with
electroluminescence images. <em>TPDS</em>, <em>32</em>(7), 1653–1664.
(<a href="https://doi.org/10.1109/TPDS.2020.3046018">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the intelligent manufacturing process of solar photovoltaic (PV) cells, the automatic defect detection system using the Industrial Internet of Things (IIoT) smart cameras and sensors cooperated in IIoT has become a promising solution. Many works have been devoted to defect detection of PV cells in a data-driven way. However, because of the subjectivity and fuzziness of human annotation, the data contains a high quantity of noise and unpredictable uncertainties, which creates great difficulties in automatic defect detection. To address this problem, we propose a novel architecture named fuzzy convolution, which integrates fuzzy logic and convolution operations at microscopic level. Combining the proposed fuzzy convolution with the regular convolution, we build a network called Hybrid Fuzzy Convolutional Neural Network (HFCNN). Compared with convolutional neural networks (CNNs), HFCNN can address the uncertainties of PV cell data to improve the accuracy with fewer parameters, making it possible to apply our method in smart cameras. Experimental results on a public dataset show the superiority of our proposed method compared with CNNs.},
  archive      = {J_TPDS},
  author       = {Chunpeng Ge and Zhe Liu and Liming Fang and Huading Ling and Aiping Zhang and Changchun Yin},
  doi          = {10.1109/TPDS.2020.3046018},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1653-1664},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A hybrid fuzzy convolutional neural network based mechanism for photovoltaic cell defect detection with electroluminescence images},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). The case for strong scaling in deep learning: Training
large 3D CNNs with hybrid parallelism. <em>TPDS</em>, <em>32</em>(7),
1641–1652. (<a
href="https://doi.org/10.1109/TPDS.2020.3047974">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present scalable hybrid-parallel algorithms for training large-scale 3D convolutional neural networks. Deep learning-based emerging scientific workflows often require model training with large, high-dimensional samples, which can make training much more costly and even infeasible due to excessive memory usage. We solve these challenges by extensively applying hybrid parallelism throughout the end-to-end training pipeline, including both computations and I/O. Our hybrid-parallel algorithm extends the standard data parallelism with spatial parallelism, which partitions a single sample in the spatial domain, realizing strong scaling beyond the mini-batch dimension with a larger aggregated memory capacity. We evaluate our proposed training algorithms with two challenging 3D CNNs, CosmoFlow and 3D U-Net. Our comprehensive performance studies show that good weak and strong scaling can be achieved for both networks using up to 2K GPUs. More importantly, we enable training of CosmoFlow with much larger samples than previously possible, realizing an order-of-magnitude improvement in prediction accuracy.},
  archive      = {J_TPDS},
  author       = {Yosuke Oyama and Naoya Maruyama and Nikoli Dryden and Erin McCarthy and Peter Harrington and Jan Balewski and Satoshi Matsuoka and Peter Nugent and Brian Van Essen},
  doi          = {10.1109/TPDS.2020.3047974},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1641-1652},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The case for strong scaling in deep learning: Training large 3D CNNs with hybrid parallelism},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A game-based approach for cost-aware task assignment with
QoS constraint in collaborative edge and cloud environments.
<em>TPDS</em>, <em>32</em>(7), 1629–1640. (<a
href="https://doi.org/10.1109/TPDS.2020.3041029">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the Internet of Things, the data that needs to be processed is increasing rapidly. Therefore, the collaboration of cloud and edge emerges as the times require. Edge nodes are mainly responsible for collecting data, and decide to process the data locally or offload to cloud data centers. Cloud data centers are suitable for data analysis, model training, and managing edge nodes. In this article, we focus on the task assignment problems in collaborative edge and cloud environments and study it in a distributed, non-cooperative environment. An M/M/1 queueing model is established to characterize the task transmission. Because of the multi-core processors, we set an M/M/C queueing model to characterize the task computation. We consider the problem from the perspective of game theory and formulate it into a non-cooperative game among multi-agents (multiple edge data centers) in which each agent is informed with incomplete information (allocation strategies) of others. For each agent, we define a function of the expected cost of tasks as the disutility function, and minimize it subject to the QoS constraint. We analyze the existence of Nash equilibrium and develop a Greedy Energy-aware Algorithm (GEA) to choose active servers using the Limit Searching Algorithm (LSA) to find the ceiling utilization. Then we propose the Best Response Algorithm (BRA) to optimize the utility function. The convergence of the BRA algorithm has been discussed. Finally, the results demonstrate that the BRA algorithm can get a solution close to Nash equilibrium and reach it quickly.},
  archive      = {J_TPDS},
  author       = {Saiqin Long and Weifan Long and Zhetao Li and Kenli Li and Yuanqing Xia and Zhuo Tang},
  doi          = {10.1109/TPDS.2020.3041029},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1629-1640},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A game-based approach for cost-aware task assignment with QoS constraint in collaborative edge and cloud environments},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Systematically landing machine learning onto market-scale
mobile malware detection. <em>TPDS</em>, <em>32</em>(7), 1615–1628. (<a
href="https://doi.org/10.1109/TPDS.2020.3046092">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite being crucial to today&#39;s mobile ecosystem, app markets have meanwhile become a natural, convenient malware delivery channel as they actually “lend credibility” to malicious apps. In the past few years, machine learning (ML) techniques have been widely explored for automated, robust malware detection, but till now we have not seen an ML-based malware detection solution applied at market scales. To systematically understand the real-world challenges, we conduct a collaborative study with T-Market, a popular Android app market that offers us large-scale ground-truth data. Our study illustrates that the key to successfully developing such systems is multifold, including feature selection and encoding, feature engineering and exposure, app analysis speed and efficacy, developer and user engagement, as well as ML model evolution. Failure in any of the above aspects could lead to the “wooden barrel effect” of the whole system. This article presents our judicious design choices and first-hand deployment experiences in building a practical ML-powered malware detection system. It has been operational at T-Market, using a single commodity server to check ~12K apps every day, and has achieved an overall precision of 98.9 percent and recall of 98.1 percent with an average per-app scan time of 0.9 minutes.},
  archive      = {J_TPDS},
  author       = {Liangyi Gong and Hao Lin and Zhenhua Li and Feng Qian and Yang Li and Xiaobo Ma and Yunhao Liu},
  doi          = {10.1109/TPDS.2020.3046092},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1615-1628},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Systematically landing machine learning onto market-scale mobile malware detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Distributed task migration optimization in MEC by
extending multi-agent deep reinforcement learning approach.
<em>TPDS</em>, <em>32</em>(7), 1603–1614. (<a
href="https://doi.org/10.1109/TPDS.2020.3046737">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Closer to mobile users geographically, mobile edge computing (MEC) can provide some cloud-like capabilities to users more efficiently. This enables it possible for resource-limited mobile users to offload their computation-intensive and latency-sensitive tasks to MEC nodes. For its great benefits, MEC has drawn wide attention and extensive works have been done. However, few of them address task migration problem caused by distributed user mobility, which can&#39;t be ignored with quality of service (QoS) consideration. In this article, we study task migration problem and try to minimize the average completion time of tasks under migration energy budget. There are multiple independent users and the movement of each mobile user is memoryless with a sequential decision-making process, thus reinforcement learning algorithm based on Markov chain model is applied with low computation complexity. To further facilitate cooperation among users, we devise a distributed task migration algorithm based on counterfactual multi-agent (COMA) reinforcement learning approach to solve this problem. Extensive experiments are carried out to assess the performance of this distributed task migration algorithm. Compared with no migrating (NM) and single-agent actor-critic (AC) algorithms, the proposed distributed task migration algorithm can achieve up 30-50 percent reduction about average completion time.},
  archive      = {J_TPDS},
  author       = {Chubo Liu and Fan Tang and Yikun Hu and Kenli Li and Zhuo Tang and Keqin Li},
  doi          = {10.1109/TPDS.2020.3046737},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1603-1614},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed task migration optimization in MEC by extending multi-agent deep reinforcement learning approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accelerating gossip-based deep learning in heterogeneous
edge computing platforms. <em>TPDS</em>, <em>32</em>(7), 1591–1602. (<a
href="https://doi.org/10.1109/TPDS.2020.3046440">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of data created at the network edge, decentralized and Gossip-based training of deep learning (DL) models on edge computing (EC) gains tremendous research momentum, owing to its capability to learn from resource-strenuous edge nodes with limited network connectivity. Today&#39;s edge devices are extremely heterogeneous, e.g., hardware and software stacks, and result in high performance variation of training time and inducing extra delay to synchronize and converge. The large body of prior art accelerates DL, being data or model parallelization, via a centralized server, e.g., parameter server scheme, which may easily turn into the system bottleneck or single point of failure. In this artice, we propose EdgeGossip, a framework specifically designed to accelerate the training process of decentralized and Gossip-based DL training for heterogeneous EC platforms. EdgeGossip features on: (i) low performance variation among multiple EC platforms during iterative training, and (ii) accuracy-aware training to fastly obtain best possible model accuracy. We implement EdgeGossip based on popular Gossip algorithms and demonstrate its effectiveness using real-world DL workloads, i.e., considerably reducing model training time by an average of 2.70 times while only incurring accuracy losses of 0.78 percent.},
  archive      = {J_TPDS},
  author       = {Rui Han and Shilin Li and Xiangwei Wang and Chi Harold Liu and Gaofeng Xin and Lydia Y. Chen},
  doi          = {10.1109/TPDS.2020.3046440},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1591-1602},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating gossip-based deep learning in heterogeneous edge computing platforms},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Learning spatiotemporal failure dependencies for
resilient edge computing services. <em>TPDS</em>, <em>32</em>(7),
1578–1590. (<a
href="https://doi.org/10.1109/TPDS.2020.3046188">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing services are exposed to infrastructural failures due to geographical dispersion, ad hoc deployment, and rudimentary support systems. Two unique characteristics of the edge computing paradigm necessitate a novel failure resilience approach. First, edge servers, contrary to cloud counterparts with reliable data center networks, are typically connected via ad hoc networks. Thus, link failures need more attention to ensure truly resilient services. Second, network delay is a critical factor for the deployment of edge computing services. This restricts replication decisions to geographical proximity and necessitates joint consideration of delay and resilience. In this article, we propose a novel machine learning based mechanism that evaluates the failure resilience of a service deployed redundantly on the edge infrastructure. Our approach learns the spatiotemporal dependencies between edge server failures and combines them with the topological information to incorporate link failures. Ultimately, we infer the probability that a certain set of servers fails or disconnects concurrently during service runtime. Furthermore, we introduce Dependency- and Topology-aware Failure Resilience (DTFR), a two-stage scheduler that minimizes either failure probability or redundancy cost, while maintaining low network delay. Extensive evaluation with various real-world failure traces and workload configurations demonstrate superior performance in terms of availability, number of failures, network delay, and cost with respect to the state-of-the-art schedulers.},
  archive      = {J_TPDS},
  author       = {Atakan Aral and Ivona Brandić},
  doi          = {10.1109/TPDS.2020.3046188},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1578-1590},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Learning spatiotemporal failure dependencies for resilient edge computing services},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). FedSCR: Structure-based communication reduction for
federated learning. <em>TPDS</em>, <em>32</em>(7), 1565–1577. (<a
href="https://doi.org/10.1109/TPDS.2020.3046250">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning allows edge devices to collaboratively train a shared model on their local data without leaking user privacy. The non-independent-and-identically-distributed (Non-IID) property of data distribution, which leads to severe accuracy degradation, and enormous communication overhead for aggregating parameters should be tackled in federated learning. In this article, we conduct a detailed analysis of parameter updates on the Non-IID datasets and compare the difference with the IID setting. Experimental results exhibit that parameter update matrices are structure-sparse and show that more gradients could be identified as negligible updates on the Non-IID data. As a result, we propose a structure-based communication reduction algorithm, called FedSCR, that reduces the number of parameters transported through the network while maintaining the model accuracy. FedSCR aggregates the parameter updates over channels and filters, identifies and removes the redundant updates by comparing the aggregated values with a threshold. Unlike the traditional structured pruning methods, FedSCR retains the complete model that does not require to be retrained and fine-tuned. The local loss and weight divergence on each device vary a lot because of the unbalanced data distribution. We further propose an adaptive FedSCR, that dynamically changes the bounded threshold, to enhance the model robustness on the Non-IID data. Evaluation results show that our proposed strategies achieve almost 50 percent upstream communication reduction without loss of accuracy. FedSCR can be integrated into state-of-the-art federated learning algorithms to dramatically reduce the number of parameters pushed to the global server with a tolerable accuracy reduction.},
  archive      = {J_TPDS},
  author       = {Xueyu Wu and Xin Yao and Cho-Li Wang},
  doi          = {10.1109/TPDS.2020.3046250},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1565-1577},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {FedSCR: Structure-based communication reduction for federated learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). An efficiency-boosting client selection scheme for
federated learning with fairness guarantee. <em>TPDS</em>,
<em>32</em>(7), 1552–1564. (<a
href="https://doi.org/10.1109/TPDS.2020.3040887">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of potential privacy leakage during centralized AI&#39;s model training has drawn intensive concern from the public. A Parallel and Distributed Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new paradigm to cope with the privacy issue by allowing clients to perform model training locally, without the necessity to upload their personal sensitive data. In FL, the number of clients could be sufficiently large, but the bandwidth available for model distribution and re-upload is quite limited, making it sensible to only involve part of the volunteers to participate in the training process. The client selection policy is critical to an FL process in terms of training efficiency, the final model&#39;s quality as well as fairness. In this article, we will model the fairness guaranteed client selection as a Lyapunov optimization problem and then a C 2 MAB-based method is proposed for estimation of the model exchange time between each client and the server, based on which we design a fairness guaranteed algorithm termed RBCS-F for problem-solving. The regret of RBCS-F is strictly bounded by a finite constant, justifying its theoretical feasibility. Barring the theoretical results, more empirical data can be derived from our real training experiments on public datasets.},
  archive      = {J_TPDS},
  author       = {Tiansheng Huang and Weiwei Lin and Wentai Wu and Ligang He and Keqin Li and Albert Y. Zomaya},
  doi          = {10.1109/TPDS.2020.3040887},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1552-1564},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An efficiency-boosting client selection scheme for federated learning with fairness guarantee},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accelerating federated learning over reliability-agnostic
clients in mobile edge computing systems. <em>TPDS</em>, <em>32</em>(7),
1539–1551. (<a
href="https://doi.org/10.1109/TPDS.2020.3040867">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC), which incorporates the Cloud, edge nodes, and end devices, has shown great potential in bringing data processing closer to the data sources. Meanwhile, Federated learning (FL) has emerged as a promising privacy-preserving approach to facilitating AI applications. However, it remains a big challenge to optimize the efficiency and effectiveness of FL when it is integrated with the MEC architecture. Moreover, the unreliable nature (e.g., stragglers and intermittent drop-out) of end devices significantly slows down the FL process and affects the global model&#39;s quality in such circumstances. In this article, a multi-layer federated learning protocol called HybridFL is designed for the MEC architecture. HybridFL adopts two levels (the edge level and the cloud level) of model aggregation enacting different aggregation strategies. Moreover, in order to mitigate stragglers and end device drop-out, we introduce regional slack factors into the stage of client selection performed at the edge nodes using a probabilistic approach without identifying or probing the state of end devices (whose reliability is agnostic). We demonstrate the effectiveness of our method in modulating the proportion of clients selected and present the convergence analysis for our protocol. We have conducted extensive experiments with machine learning tasks in different scales of MEC system. The results show that HybridFL improves the FL training process significantly in terms of shortening the federated round length, speeding up the global model&#39;s convergence (by up to 12×) and reducing end device energy consumption (by up to 58 percent).},
  archive      = {J_TPDS},
  author       = {Wentai Wu and Ligang He and Weiwei Lin and Rui Mao},
  doi          = {10.1109/TPDS.2020.3040867},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1539-1551},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating federated learning over reliability-agnostic clients in mobile edge computing systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Mutual information driven federated learning.
<em>TPDS</em>, <em>32</em>(7), 1526–1538. (<a
href="https://doi.org/10.1109/TPDS.2020.3040981">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is an emerging research field that yields a global trained model from different local clients without violating data privacy. Existing FL techniques often ignore the effective distinction between local models and the aggregated global model when doing the client-side weight update, as well as the distinction of local models for the server-side aggregation. In this article, we propose a novel FL approach with resorting to mutual information (MI). Specifically, in client-side, the weight update is reformulated through minimizing the MI between local and aggregated models and employing Negative Correlation Learning (NCL) strategy. In server-side, we select top effective models for aggregation based on the MI between an individual local model and its previous aggregated model. We also theoretically prove the convergence of our algorithm. Experiments conducted on MNIST, CIFAR-10, ImageNet, and the clinical MIMIC-III datasets manifest that our method outperforms the state-of-the-art techniques in terms of both communication and testing performance.},
  archive      = {J_TPDS},
  author       = {Md Palash Uddin and Yong Xiang and Xuequan Lu and John Yearwood and Longxiang Gao},
  doi          = {10.1109/TPDS.2020.3040981},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1526-1538},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Mutual information driven federated learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Biscotti: A blockchain system for private and secure
federated learning. <em>TPDS</em>, <em>32</em>(7), 1513–1525. (<a
href="https://doi.org/10.1109/TPDS.2020.3044223">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning is the current state-of-the-art in supporting secure multi-party machine learning (ML): data is maintained on the owner&#39;s device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to both protect the privacy of an individual client&#39;s update and maintain the performance of the global model at scale when 30 percent adversaries are present in the system.},
  archive      = {J_TPDS},
  author       = {Muhammad Shayan and Clement Fung and Chris J. M. Yoon and Ivan Beschastnikh},
  doi          = {10.1109/TPDS.2020.3044223},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1513-1525},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Biscotti: A blockchain system for private and secure federated learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Guest editorial. <em>TPDS</em>, <em>32</em>(7),
1511–1512. (<a
href="https://doi.org/10.1109/TPDS.2020.3047357">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section present the state-of-the-art technologies and the challenges of parallel and distributed computing techniques for artificial intelligence (AI), machine learning (ML), and deep learning (DL). AI, ML, and DL have established themselves in a multitude of domains because of their ability to process and model unstructured input data.},
  archive      = {J_TPDS},
  author       = {Pavan Balaji and Jidong Zhai and Min Si},
  doi          = {10.1109/TPDS.2020.3047357},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {7},
  pages        = {1511-1512},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Guest editorial},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Network-aware locality scheduling for distributed data
operators in data centers. <em>TPDS</em>, <em>32</em>(6), 1494–1510. (<a
href="https://doi.org/10.1109/TPDS.2021.3053241">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large data centers are currently the mainstream infrastructures for big data processing. As one of the most fundamental tasks in these environments, the efficient execution of distributed data operators (e.g., join and aggregation) are still challenging current data systems, and one of the key performance issues is network communication time. State-of-the-art methods trying to improve that problem focus on either application-layer data locality optimization to reduce network traffic or on network-layer data flow optimization to increase bandwidth utilization. However, the techniques in the two layers are totally independent from each other, and performance gains from a joint optimization perspective have not yet been explored. In this article, we propose a novel approach called NEAL (NEtwork-Aware Locality scheduling) to bridge this gap, and consequently to further reduce communication time for distributed big data operators. We present the detailed design and implementation of NEAL, and our experimental results demonstrate that NEAL always performs better than current approaches for different workloads and network bandwidth configurations.},
  archive      = {J_TPDS},
  author       = {Long Cheng and Ying Wang and Qingzhi Liu and Dick H.J. Epema and Cheng Liu and Ying Mao and John Murphy},
  doi          = {10.1109/TPDS.2021.3053241},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1494-1510},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Network-aware locality scheduling for distributed data operators in data centers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A scalable platform for distributed object tracking
across a many-camera network. <em>TPDS</em>, <em>32</em>(6), 1479–1493.
(<a href="https://doi.org/10.1109/TPDS.2021.3049450">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in deep neural networks (DNN) and computer vision (CV) algorithms have made it feasible to extract meaningful insights from large-scale deployments of urban cameras. Tracking an object of interest across the camera network in near real-time is a canonical problem. However, current tracking platforms have two key limitations: 1) They are monolithic, proprietary and lack the ability to rapidly incorporate sophisticated tracking models, and 2) They are less responsive to dynamism across wide-area computing resources that include edge, fog, and cloud abstractions. We address these gaps using Anveshak, a runtime platform for composing and coordinating distributed tracking applications. It provides a domain-specific dataflow programming model to intuitively compose a tracking application, supporting contemporary CV advances like query fusion and re-identification, and enabling dynamic scoping of the camera network&#39;s search space to avoid wasted computation. We also offer tunable batching and data-dropping strategies for dataflow blocks deployed on distributed resources to respond to network and compute variability. These balance the tracking accuracy, its real-time performance, and the active camera-set size. We illustrate the concise expressiveness of the programming model for four tracking applications. Our detailed experiments for a network of 1000 camera-feeds on modest resources exhibit the tunable scalability, performance, and quality trade-offs enabled by our dynamic tracking, batching, and dropping strategies.},
  archive      = {J_TPDS},
  author       = {Aakash Khochare and Aravindhan Krishnan and Yogesh Simmhan},
  doi          = {10.1109/TPDS.2021.3049450},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1479-1493},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A scalable platform for distributed object tracking across a many-camera network},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A high-throughput FPGA accelerator for short-read mapping
of the whole human genome. <em>TPDS</em>, <em>32</em>(6), 1465–1478. (<a
href="https://doi.org/10.1109/TPDS.2021.3051011">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mapping of DNA subsequences to a known reference genome, referred to as “short-read mapping”, is essential for next-generation sequencing. Hundreds of millions of short reads need to be aligned to a tremendously long reference sequence, making short-read mapping very time consuming. In this article, a high-throughput hardware accelerator is proposed so as to accelerate this task. A Bloom filter-based candidate mapping location (CML) generator and a folded processing element (PE) array are proposed to address CML selection and the Smith-Waterman (SW) alignment algorithm, respectively. It is shown that the proposed CML generator reduces the required memory access by 40 percent by employing a down-sampling scheme when compared to the Ferragina-Manzini index (FM-index) solution. The proposed hierarchical Bloom filter (HBF) that includes optimized parameters achieves a 1.5×10 4 times acceleration over the conventional Bloom filter. The proposed memory re-allocation scheme further reduces the memory access time for the HBF by a factor of 256. The proposed folded PE array delivers a 1.2-to-3.2 times higher giga cell updates per second (GCUPS). The processing time can be further reduced by 53-to-72 percent by employing a fully pipelined PE array that allows for a tailored shift amount for seeding. The accelerator is realized on a Stratix V GX FPGA with 16GB external SDRAM. Operated at 200MHz, the proposed FPGA accelerator delivers a 2.1-to-11 times higher throughput with the highest 99 percent accuracy and 98 percent sensitivity compared to the state-of-the-art FPGA-based solutions.},
  archive      = {J_TPDS},
  author       = {Yen-Lung Chen and Bo-Yi Chang and Chia-Hsiang Yang and Tzi-Dar Chiueh},
  doi          = {10.1109/TPDS.2021.3051011},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1465-1478},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A high-throughput FPGA accelerator for short-read mapping of the whole human genome},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A parallel jacobi-embedded gauss-seidel method.
<em>TPDS</em>, <em>32</em>(6), 1452–1464. (<a
href="https://doi.org/10.1109/TPDS.2021.3052091">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A broad range of scientific simulations involve solving large-scale computationally expensive linear systems of equations. Iterative solvers are typically preferred over direct methods when it comes to large systems due to their lower memory requirements and shorter execution times. However, selecting the appropriate iterative solver is problem-specific and dependent on the type and symmetry of the coefficient matrix. Gauss-Seidel (GS) is an iterative method for solving linear systems that are either strictly diagonally dominant or symmetric positive definite. This technique is an improved version of Jacobi and typically converges in fewer iterations. However, the sequential nature of this algorithm complicates the parallel extraction. In fact, most parallel derivatives of GS rely on the sparsity pattern of the coefficient matrix and require matrix reordering or domain decomposition. In this article, we introduce a new algorithm that exploits the convergence property of GS and adapts the parallel structure of Jacobi. The proposed method works for both dense and sparse systems and is straightforward to implement. We have examined the performance of our method on multicore and many-core architectures. Experimental results demonstrate the superior performance of the proposed algorithm compared with GS and Jacobi. Additionally, performance comparison with built-in Krylov solvers in MATLAB showed that in terms of time per iteration, Krylov methods perform faster on CPUs, but our approach is significantly better when executed on GPUs. Lastly, we apply our method to solve the power flow problem, and the results indicate a significant improvement in runtime, reaching up to 87 times faster speed compared with GS.},
  archive      = {J_TPDS},
  author       = {Afshin Ahmadi and Felice Manganiello and Amin Khademi and Melissa C. Smith},
  doi          = {10.1109/TPDS.2021.3052091},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1452-1464},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A parallel jacobi-embedded gauss-seidel method},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Co-active: A workload-aware collaborative cache
management scheme for NVMe SSDs. <em>TPDS</em>, <em>32</em>(6),
1437–1451. (<a
href="https://doi.org/10.1109/TPDS.2021.3052028">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When it comes to NAND Flash-based solid-state disks (SSDs), cache can narrow the performance gap between user-level I/Os and flash memory. Cache management schemes impose relentless impacts on the endurance and performance of flash memory. A vast majority of existing cache management techniques adopt a passive data-update style (e.g., GCaR, LCR), thereby undermining response times in burst I/O requests-based applications 1 1.Burst I/O requests must be served in a real-time manner. This type of I/O access pattern is prevalent in data-intensive workloads.. To address this issue, we propose a collaborative active write-back cache management scheme, called Co-Active, customized for I/O access patterns and the usage status of a flash chip. We design a hot/cold separation module to determine whether data is cold or hot in workload. When a flash chip is idle, cold and dirty data in the cache is flushed into the idle flash chip to produce clean data. To curtail cache replacement cost, clean data are preferentially evicted amid the procedure of cache replacement. A maximum write-back threshold is configured according to the level of burst I/O requests in workload. This threshold is intended to avert redundant write I/Os flushing into flash memory, thereby boosting the endurance of flash memory. The experiments are conducted to validate the advantages of Co-Active in terms of average response time, write amplification, and erase count. The findings unveil that compared with the six popular cache management schemes (LRU, CFLRU, GCaR_CFLRU, LCR, and MQSim), Co-Active (1) slashes the average response time by up to 83.89 percent with an average of 32.7 percent; (2) drives up the performance cliff degree by up to 76.4 percent with an average of 42.3 percent; and (3) improves write amplification rate by up to 60.5 percent with an average of 5.4 percent.},
  archive      = {J_TPDS},
  author       = {Hui Sun and Shangshang Dai and Jianzhong Huang and Xiao Qin},
  doi          = {10.1109/TPDS.2021.3052028},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1437-1451},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Co-active: A workload-aware collaborative cache management scheme for NVMe SSDs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Reversible CSP computations. <em>TPDS</em>,
<em>32</em>(6), 1425–1436. (<a
href="https://doi.org/10.1109/TPDS.2021.3051747">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversibility enables a program to be executed both forwards and backwards. This ability allows programmers to backtrack the execution to a previous state. This is essential if the computation is not deterministic because re-running the program forwards may not lead to that state of interest. Reversibility of sequential programs has been well studied and a strong theoretical basis exists. Contrarily, reversibility of concurrent programs is still very young, especially in the practical side. For instance, in the particular case of the Communicating Sequential Processes (CSP) language, reversibility is practically missing. In this article, we present a new technique, including its formal definition and its implementation, to reverse CSP computations. Most of the ideas presented can be directly applied to other concurrent specification languages such as Promela or CCS, but we center the discussion and the implementation on CSP. The technique proposes different forms of reversibility, including strict reversibility and causal-consistent reversibility. On the practical side, we provide an implementation of a system to reverse CSP computations that is able to highlight the source code that is being executed in each forwards/backwards computation step, and that has been optimized to be scalable to real systems.},
  archive      = {J_TPDS},
  author       = {Carlos Galindo and Naoki Nishida and Josep Silva and Salvador Tamarit},
  doi          = {10.1109/TPDS.2021.3051747},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1425-1436},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reversible CSP computations},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A machine-learning-based framework for productive
locality exploitation. <em>TPDS</em>, <em>32</em>(6), 1409–1424. (<a
href="https://doi.org/10.1109/TPDS.2021.3051348">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data locality is of extreme importance in programming distributed-memory architectures due to its implications on latency and energy consumption. Automated compiler and runtime system optimization studies have attempted to improve data locality exploitation without burdening the programmer. However, due to the difficulty of static code analysis, conservatism in compiler optimizations to avoid errors, and cost of dynamic analysis, the efficacy of automated optimizations is limited. Therefore, programmers need to spend significant effort in optimizing locality while creating applications for distributed memory parallel systems. We present a machine-learning based framework to automatically exploit locality in distributed memory applications. This framework takes application source whose time-critical blocks are marked by pragmas, and produces optimized source code that uses a regressor for efficient data movement. The regressor is trained with automatically-collected application profiles with very small input data sizes. We integrate our prototype in the Chapel language stack. In our experiments, we show that the Elastic Net model is the ideal regressor for our case and applications that utilize Elastic Net can perform very similarly to programmer-optimized versions. We also show that such regressors can be trained within few minutes on a cluster or within 30 minutes on a workstation, including data collection.},
  archive      = {J_TPDS},
  author       = {Engin Kayraklioglu and Erwan Favry and Tarek El-Ghazawi},
  doi          = {10.1109/TPDS.2021.3051348},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1409-1424},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A machine-learning-based framework for productive locality exploitation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Optimised lambda architecture for monitoring scientific
infrastructure. <em>TPDS</em>, <em>32</em>(6), 1395–1408. (<a
href="https://doi.org/10.1109/TPDS.2017.2772241">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within scientific infrastructuscientists execute millions of computational jobs daily, resulting in the movement of petabytes of data over the heterogeneous infrastructure. Monitoring the computing and user activities over such a complex infrastructure is incredibly demanding. Whereas present solutions are traditionally based on a Relational Database Management System (RDBMS) for data storage and processing, recent developments evaluate the Lambda Architecture (LA). In particular these studies have evaluated data storage and batch processing for processing large-scale monitoring datasets using Hadoop and its MapReduce framework. Although LA performed better than the RDBMS following evaluation, it was fairly complex to implement and maintain. This paper presents an Optimised Lambda Architecture (OLA) using the Apache Spark ecosystem, which involves modelling an efficient way of joining batch computation and real-time computation transparently without the need to add complexity. A few models were explored: pure streaming, pure batch computation, and the combination of both batch and streaming. An evaluation of the OLA on the CERN IT on-premises Hadoop cluster and the public Amazon cloud infrastructure for the monitoring WLCG Data acTivities (WDT) use case are both presented, demonstrating how the new architecture can offer benefits by combining both batch and real-time processing to compensate for batch-processing latency.},
  archive      = {J_TPDS},
  author       = {Uthayanath Suthakar and Luca Magnoni and David Ryan Smith and Akram Khan},
  doi          = {10.1109/TPDS.2017.2772241},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1395-1408},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimised lambda architecture for monitoring scientific infrastructure},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A scalable stateful approach for virtual security
functions orchestration. <em>TPDS</em>, <em>32</em>(6), 1383–1394. (<a
href="https://doi.org/10.1109/TPDS.2021.3049804">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works suggested different approaches to implementing service chaining. Their goal is to enhance the performance of the middleboxes and satisfy the expectations of the cloud providers and users. To meet these expectations, the delay factor, i.e., flow through the low-cost paths, as well as the best node processing factor, are considered. Achieving these two goals simultaneously turns the middlebox optimal placement into an NP-hard problem. Therefore, when the problem size is large, it is infeasible to obtain an optimal solution at a reasonable time. One of the important issues which has not been considered in the previous works is stateful optimal placement when receiving a new request. Due to resource constraints as well as financial costs for the customers, it is not possible to create functions for all requests. Therefore, not only it is possible to integrate the same network functions between new flows, but it will also be examined between new on-demand network functions as well as existing ones. Our proposed approach not only reduces the creation of network functions that can be cost-effective for the customer but also because of the migration of previous network functions (integration with on-demand network functions) to optimize new requests, overall, it will optimize the entire network cost over time. We formulated the problem as 0-1 programming problem. The results of this article are based on a fat-tree data center. To show that our stateful solution is scalable in large networks, we use network zoning and topology partitioning heuristics. Our simulations show that we were able to scale our placement model to a network with 54K nodes and 1.5M edges.},
  archive      = {J_TPDS},
  author       = {Niloofar Moradi and Alireza Shameli-Sendi and Alireza Khajouei},
  doi          = {10.1109/TPDS.2021.3049804},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1383-1394},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A scalable stateful approach for virtual security functions orchestration},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). On consortium blockchain consistency: A queueing network
model approach. <em>TPDS</em>, <em>32</em>(6), 1369–1382. (<a
href="https://doi.org/10.1109/TPDS.2021.3049915">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing blockchain protocols is a notoriously difficult task due to the underlying large scale distributed networks. To address this problem, stochastic model-based approaches are often utilized. However, the abstract models in prior work turn out not to be adoptable to consortium blockchains as the consensus of such a blockchain often consists of multiple processes. To address the lack of efficient analysis tools, we propose a queueing network-based method for analyzing consistency properties of consortium blockchain protocols in this article. Our method provides a way to evaluate the performance of the main stages in blockchain consensus. We apply our framework to the Hyperledger Fabric system and recover key properties of the blockchain network. Using our method, we analyze the security properties of the ordering mechanism and the impact of delaying endorsement messages in consortium blockchain protocols. Then an upper bound is derived of the damage an attacker could cause who is capable of delaying the honest players&#39; messages. Based on the proposed method, we employ analytical derivations to investigate both the security and performance features, and corroborate close agreement with measurements on a wide-area network testbed running the Hyperledger Fabric blockchain. With the proposed method, designers of future blockchains can provide a more rigorous analysis of their consortium blockchain schemes.},
  archive      = {J_TPDS},
  author       = {Tianhui Meng and Yubin Zhao and Katinka Wolter and Cheng-Zhong Xu},
  doi          = {10.1109/TPDS.2021.3049915},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1369-1382},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On consortium blockchain consistency: A queueing network model approach},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Reliability and confidentiality co-verification for
parallel applications in distributed systems. <em>TPDS</em>,
<em>32</em>(6), 1353–1368. (<a
href="https://doi.org/10.1109/TPDS.2021.3049780">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-verification of reliability and confidentiality is a necessary process for safety- and security-critical applications. While these two objectives are conflicting, preassignment has emerged as an effective and efficient verification solution. In this article, we propose two preassignment-based co-verification techniques, namely, Blocks-based Vulnerability Preassignment (BVP) and Reversed Blocks-based Time Preassignment (RBTP) for a parallel application in distributed CAN FD systems. BVP can significantly improve reliability under a vulnerability bound, while RBTP can reduce vulnerability over a reliability goal. Real case study with the parallel automotive application and parallelism study with two structures of high-parallelism and low-parallelism applications are demonstrated; the proposed BVP and RBTP can improve the verification acceptance ratio by 19 and 10 percent compared to the state-of-the-art Average Vulnerability Preassignment (AVP) and Average Time Preassignment (ATP) techniques, respectively.},
  archive      = {J_TPDS},
  author       = {Guoqi Xie and Kehua Yang and Haibo Luo and Renfa Li and Shiyan Hu},
  doi          = {10.1109/TPDS.2021.3049780},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1353-1368},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Reliability and confidentiality co-verification for parallel applications in distributed systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Rings for privacy: An architecture for large scale
privacy-preserving data mining. <em>TPDS</em>, <em>32</em>(6),
1340–1352. (<a
href="https://doi.org/10.1109/TPDS.2021.3049286">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new architecture for privacy-preserving data mining based on Multi Party Computation (MPC) and secure sums. While traditional MPC approaches rely on a small number of aggregation peers replacing a centralized trusted entity, the current study puts forth a distributed solution that involves all data sources in the aggregation process, with the help of a single server for storing intermediate results. A large-scale scenario is examined and the possibility that data become inaccessible during the aggregation process is considered, a possibility that traditional schemes often neglect. Here, it is explicitly examined, as it might be provoked by intermittent network connectivity or sudden user departures. For increasing system reliability, data sources are organized in multiple sets, called rings, which independently work on the aggregation process. Two different protocol schemes are proposed and their failure probability, i.e., the probability that the data mining output cannot guarantee the desired level of accuracy, is analytically modeled. The privacy degree, the communication cost and the computational complexity that the schemes exhibit are also characterized. Finally, the new protocols are applied to some specific use cases, demonstrating their feasibility and attractiveness.},
  archive      = {J_TPDS},
  author       = {Maria Luisa Merani and Daniele Croce and Ilenia Tinnirello},
  doi          = {10.1109/TPDS.2021.3049286},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1340-1352},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Rings for privacy: An architecture for large scale privacy-preserving data mining},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Partitioning-based scheduling of OpenMP task systems with
tied tasks. <em>TPDS</em>, <em>32</em>(6), 1322–1339. (<a
href="https://doi.org/10.1109/TPDS.2020.3048373">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OpenMP is a popular programming framework in both general and high-performance computing and has recently drawn much interest in embedded and real-time computing. Although the execution semantics of OpenMP are similar to the DAG task model, the constraints posed by the OpenMP specification make them significantly more challenging to analyze. A tied task is an important feature in OpenMP that must execute on the same thread throughout its entire life cycle. A previous work [1] succeeded in analyzing the real-time scheduling of tied tasks by modifying the Task Scheduling Constraints (TSCs) in OpenMP specification. In this article, we also study the real-time scheduling of OpenMP task systems with tied tasks but without changing the original TSCs. In particular, we propose a partitioning-based algorithm, P-EDF-omp, by which the tied constraint can be automatically guaranteed as long as an OpenMP task system can be successfully partitioned to a multiprocessor platform. Furthermore, we conduct comprehensive experiments with both synthetic workloads and established OpenMP benchmarks to show that our approach consistently outperforms the work in [1] -even without modifying the TSCs.},
  archive      = {J_TPDS},
  author       = {Yang Wang and Xu Jiang and Nan Guan and Zhishan Guo and Xue Liu and Wang Yi},
  doi          = {10.1109/TPDS.2020.3048373},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1322-1339},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Partitioning-based scheduling of OpenMP task systems with tied tasks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). E2bird: Enhanced elastic batch for improving
responsiveness and throughput of deep learning services. <em>TPDS</em>,
<em>32</em>(6), 1307–1321. (<a
href="https://doi.org/10.1109/TPDS.2020.3047638">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to tackle existing problems about deep learning serving on GPUs in the view of the system. GPUs have been widely adopted to serve online deep learning-based services that have stringent QoS(Quality-of-Service) requirements. However, emerging deep learning serving systems often result in poor responsiveness and low throughput of the inferences that damage user experience and increase the number of GPUs required to host an online service. Our investigation shows that the poor batching operation and the lack of data transfer-computation overlap are the root causes of the poor responsiveness and low throughput. To this end, we propose E 2 bird, a deep learning serving system that is comprised of a GPU-resident memory pool, a multi-granularity inference engine, and an elastic batch scheduler. The memory pool eliminates the unnecessary waiting of the batching operation and enables data transfer-computation overlap. The inference engine enables concurrent execution of different batches, improving the GPU resource utilization. The batch scheduler organizes inferences elasticallyto guarantee the QoS. Our experimental results on an Nvidia Titan RTXGPU show that E 2 bird reduces the response latency of inferences by up to 82.4 percent and improves the throughput by up to 62.8 percent while guaranteeing the QoS target compared with TensorFlow Serving.},
  archive      = {J_TPDS},
  author       = {Weihao Cui and Quan Chen and Han Zhao and Mengze Wei and Xiaoxin Tang and Minyi Guo},
  doi          = {10.1109/TPDS.2020.3047638},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1307-1321},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {E2bird: Enhanced elastic batch for improving responsiveness and throughput of deep learning services},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Distributed adaptive consensus tracking control for
multi-agent system with communication constraints. <em>TPDS</em>,
<em>32</em>(6), 1293–1306. (<a
href="https://doi.org/10.1109/TPDS.2020.3048383">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at a class of high-order strict feedback nonlinear multi-agent systems with communication constraints, a novel distributed adaptive back-stepping control method is proposed to cooperatively track the moving targets. First, five agents are used as controlled objects, and all five agents form a “leader-follower” mode with a distributed control structure. Meanwhile, the leader&#39;s moving velocity is considered the forward velocity of the whole formation system, and remaining agents follow the leader&#39;s movement. Then, each agent tracks the desired formation with a time-varying reference trajectory, thereby achieving the consensus tracking purpose of multi-agent system. Moreover, the multi-agent formation system avoids obstacles with the optimal trajectory and maintains the desired formation movement. Finally, the simulation results show that the designed controller can achieve the lateral and horizontal tracking errors of the multi-agent system to converge quickly, and then keep the system asymptotically stable during tracking process.},
  archive      = {J_TPDS},
  author       = {Pu Zhang and Huifeng Xue and Shan Gao and Jialong Zhang},
  doi          = {10.1109/TPDS.2020.3048383},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1293-1306},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed adaptive consensus tracking control for multi-agent system with communication constraints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Distributed and dynamic service placement in pervasive
edge computing networks. <em>TPDS</em>, <em>32</em>(6), 1277–1292. (<a
href="https://doi.org/10.1109/TPDS.2020.3046000">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth of mobile devices promotes the prosperity of novel mobile applications, which can be realized by service offloading with the assistance of edge computing servers. However, due to limited computation and storage capabilities of a single server, long service latency hinders the continuous development of service offloading in mobile networks. By supporting multi-server cooperation, Pervasive Edge Computing (PEC) is promising to enable service migration in highly dynamic mobile networks. With the objective of maximizing the system utility, we formulate the optimization problem by jointly considering the constraints of server storage capability and service execution latency. To enable dynamic service placement, we first utilize Lyapunov optimization method to decompose the long-term optimization problem into a series of instant optimization problems. Then, a sample average approximation-based stochastic algorithm is proposed to approximate the future expected system utility. Afterwards, a distributed Markov approximation algorithm is utilized to determine the service placement configurations. Through theoretical analysis, the time complexity of our proposed algorithm is linear to the number of users, and the backlog queue of PEC servers is stable. Performance evaluations are conducted based on both synthetic and real trace-driven scenarios, with numerical results demonstrating the effectiveness of our proposed algorithm from various aspects.},
  archive      = {J_TPDS},
  author       = {Zhaolong Ning and Peiran Dong and Xiaojie Wang and Shupeng Wang and Xiping Hu and Song Guo and Tie Qiu and Bin Hu and Ricky Y. K. Kwok},
  doi          = {10.1109/TPDS.2020.3046000},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {6},
  pages        = {1277-1292},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed and dynamic service placement in pervasive edge computing networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A case for pricing bandwidth: Sharing datacenter networks
with cost dominant fairness. <em>TPDS</em>, <em>32</em>(5), 1256–1269.
(<a href="https://doi.org/10.1109/TPDS.2020.3045709">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike other resources such as CPU or memory in a virtual machine, inter-virtual-machine (inter-VM) bandwidth has not been explicitly priced in datacenter networks. In this article, we argue that tenants of an IaaS cloud computing platform should be given the flexibility to pay more for explicitly priced datacenter bandwidth beyond traditional virtual machines, in order to achieve better (or more predictable) application performance. We show that a much simpler design principle can be followed to allocate bandwidth fairly, and desirable properties related to fairness can be more easily achieved, compared with state-of-the-art proposals. We call such a design principle cost dominant fairness, which stipulates that bandwidth should be allocated based on the total cost that a tenant incurs for running its applications in the cloud. Guided by the principle of cost dominant fairness, we explore the design space of pricing inter-VM bandwidth, as well as achieving fair bandwidth sharing among multiple tenants. Through our study, we believe that it is best to assign per-VM-pair weights based on individualized prices. We present a distributed bandwidth allocation algorithm that is theoretically supported by a network utility maximization formulation, and practically implemented as a shim layer at each virtual machine. We are also concerned with practical issues of billing, where discounts are needed to ensure that a tenant only pays for the bandwidth share that it is allocated. Finally, we have evaluated our pricing framework and per-VM-pair weighted fair bandwidth allocation in the Mininet emulation testbed and simulations.},
  archive      = {J_TPDS},
  author       = {Li Chen and Yuan Feng and Baochun Li and Bo Li},
  doi          = {10.1109/TPDS.2020.3045709},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1256-1269},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A case for pricing bandwidth: Sharing datacenter networks with cost dominant fairness},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). On the effective parallelization and near-optimal
deployment of service function chains. <em>TPDS</em>, <em>32</em>(5),
1238–1255. (<a
href="https://doi.org/10.1109/TPDS.2020.3043768">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network operators compose Service Function Chains (SFCs) by tying different network functions (e.g., packet inspection, flow shaping, network address translation) together and process traffic flows in the order the network functions are chained. Leveraging the technique of Network Function Virtualization (NFV), each network function can be “virtualized” and decoupled from its dedicated hardware, and therefore can be deployed flexibly for better performance at any appropriate location of the underlying network infrastructure. However, an SFC often incurs high latency as traffic goes through the virtual network functions one after another. In this article, we first design an algorithm that leverages virtual network function dependency to convert an original SFC into a parallelized SFC (p-SFC). Then, to deploy multiple p-SFCs over the network for serving a large number of users, we model the deployment problem as an Integer Linear Program and propose a heuristic, ParaSFC, based on the Viterbi dynamic programming algorithm to estimate each p-SFC’s occupation of the bottleneck resources and adjust the processing order of the p-SFCs in order to approximate the optimal solution. Finally, we conduct extensive trace-driven evaluations and exhibit that, compared to the Greedy method and the state-of-the-art CoordVNF method, ParaSFC reduces the average service latency of all the deployed p-SFCs by about 15 percent through parallelization while accommodating more SFC deployment requests over resource-limited networks.},
  archive      = {J_TPDS},
  author       = {Jianzhen Luo and Jun Li and Lei Jiao and Jun Cai},
  doi          = {10.1109/TPDS.2020.3043768},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1238-1255},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {On the effective parallelization and near-optimal deployment of service function chains},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Collaborative heterogeneity-aware OS scheduler for
asymmetric multicore processors. <em>TPDS</em>, <em>32</em>(5),
1224–1237. (<a
href="https://doi.org/10.1109/TPDS.2020.3045279">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric multicore processors (AMP) offer multiple types of cores under the same programming interface. Extracting the full potential of AMPs requires intelligent scheduling decisions, matching each thread with the right kind of core, the core that will maximize performance or minimize wasted energy for this thread. Existing OS schedulers are not up to this task. While they may handle certain aspects of asymmetry in the system, none can handle all runtime factors affecting AMPs for the general case of multi-threaded multi-programmed workloads. We address this problem by introducing COLAB, a general purpose asymmetry-aware scheduler targeting multi-threaded multi-programmed workloads. It estimates the performance and power of each thread on each type of core and identifies communication patterns and bottleneck threads. With this information, the scheduler makes coordinated core assignment and thread selection decisions that still provide each application its fair share of the processor&#39;s time. We evaluate our approach using both the GEM5 simulator on four distinct big.LITTLE configurations and a development board with ARM Cortex-A73/A53 processors and mixed workloads composed of PARSEC and SPLASH2 benchmarks. Compared to the state-of-the art Linux CFS and AMP-aware schedulers, we demonstrate performance gains of up to 25 and 5 to 15 percent on average, together with an average 5 percent energy saving depending on the hardware setup.},
  archive      = {J_TPDS},
  author       = {Teng Yu and Runxin Zhong and Vladimir Janjic and Pavlos Petoumenos and Jidong Zhai and Hugh Leather and John Thomson},
  doi          = {10.1109/TPDS.2020.3045279},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1224-1237},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Collaborative heterogeneity-aware OS scheduler for asymmetric multicore processors},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Auditing cache data integrity in the edge computing
environment. <em>TPDS</em>, <em>32</em>(5), 1210–1223. (<a
href="https://doi.org/10.1109/TPDS.2020.3043755">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing allows app vendors to deploy their applications and relevant data on distributed edge servers to serve nearby users. Caching data on edge servers can minimize users&#39; data retrieval latency. However, such cache data are subject to both intentional and accidental corruption in the highly distributed, dynamic, and volatile edge computing environment. Given a large number of edge servers and their limited computing resources, how to effectively and efficiently audit the integrity of app vendors&#39; cache data is a critical and challenging problem. This article makes the first attempt to tackle this Edge Data Integrity (EDI) problem. We first analyze the threat model and the audit objectives, then propose a lightweight sampling-based probabilistic approach, namely EDI-V, to help app vendors audit the integrity of their data cached on a large scale of edge servers. We propose a new data structure named variable Merkle hash tree (VMHT) for generating the integrity proofs of those data replicas during the audit. VMHT can ensure the audit accuracy of EDI-V by maintaining sampling uniformity. EDI-V allows app vendors to inspect their cache data and locate the corrupted ones efficiently and effectively. Both theoretical analysis and comprehensively experimental evaluation demonstrate the efficiency and effectiveness of EDI-V.},
  archive      = {J_TPDS},
  author       = {Bo Li and Qiang He and Feifei Chen and Hai Jin and Yang Xiang and Yun Yang},
  doi          = {10.1109/TPDS.2020.3043755},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1210-1223},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Auditing cache data integrity in the edge computing environment},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). PaKman: A scalable algorithm for generating genomic
contigs on distributed memory machines. <em>TPDS</em>, <em>32</em>(5),
1191–1209. (<a
href="https://doi.org/10.1109/TPDS.2020.3043241">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {De novo genome assembly is a fundamental problem in the field of bioinformatics, that aims to assemble the DNA sequence of an unknown genome from numerous short DNA fragments (aka reads) obtained from it. With the advent of high-throughput sequencing technologies, billions of reads can be generated in a matter of hours, necessitating efficient parallelization of the assembly process. While multiple parallel solutions have been proposed in the past, conducting a large-scale assembly at scale remains a challenging problem because of the inherent complexities associated with data movement, and irregular access footprints of memory and I/O operations. In this article, we present a novel algorithm, called PaKman, to address the problem of performing large-scale genome assemblies on a distributed memory parallel computer. Our approach focuses on improving performance through a combination of novel data structures and algorithmic strategies for reducing the communication and I/O footprint during the assembly process. PaKman presents a solution for the two most time-consuming phases in the full genome assembly pipeline, namely, k-mer counting and contig generation. A key aspect of our algorithm is its graph data structure (PaK-Graph), which comprises fat nodes (or what we call “macro-nodes”) that reduce the communication burden during contig generation. We present an extensive performance and qualitative evaluation of our algorithm across a wide range of genomes (varying in both size and species group), including comparisons to other state-of-the-art parallel assemblers. Our results demonstrate the ability to achieve near-linear speedups on up to 16K cores (tested) on the NERSC Cori supercomputer; perform better than or comparable to other state-of-the-art distributed memory and shared memory tools in terms of performance while delivering comparable (if not better) quality; and reduce time to solution significantly. For instance, PaKman is able to generate a high-quality set of assembled contigs for complex genomes such as the human and bread wheat genomes in under a minute on 16K cores. In addition, PaKman was able to successfully process a 3.1 TB simulated dataset of one of the largest known genomes (to date)-Ambystoma mexicanum (the axolotl), in just over 200 seconds on 16K cores.},
  archive      = {J_TPDS},
  author       = {Priyanka Ghosh and Sriram Krishnamoorthy and Ananth Kalyanaraman},
  doi          = {10.1109/TPDS.2020.3043241},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1191-1209},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PaKman: A scalable algorithm for generating genomic contigs on distributed memory machines},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Profiles of upcoming HPC applications and their impact on
reservation strategies. <em>TPDS</em>, <em>32</em>(5), 1178–1190. (<a
href="https://doi.org/10.1109/TPDS.2020.3039728">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expected convergence between HPC, BigData and AI, new applications with different profiles are coming to HPC infrastructures. We aim at better understanding the features and needs of these applications in order to be able to run them efficiently on HPC platforms. The approach followed is bottom-up: we study thoroughly an emerging application, Spatially Localized Atlas Network Tiles (SLANT, originating from the neuroscience community) to understand its behavior. Based on these observations, we derive a generic, yet simple, application model (namely, a linear sequence of stochastic jobs). We expect this model to be representative for a large set of upcoming applications from emerging fields that start to require the computational power of HPC clusters without fitting the typical behavior of large-scale traditional applications. In a second step, we show how one can use this generic model in a scheduling framework. Specifically we consider the problem of making reservations (both time and memory) for an execution on an HPC platform based on the application expected resource requirements. We derive solutions using the model provided by the first step of this work. We experimentally show the robustness of the model, even with very few data points or using another application, to generate the model, and provide performance gains with regards to standard and more recent approaches used in the neuroscience community.},
  archive      = {J_TPDS},
  author       = {Ana Gainaru and Brice Goglin and Valentin Honoré and Guillaume Pallez},
  doi          = {10.1109/TPDS.2020.3039728},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1178-1190},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Profiles of upcoming HPC applications and their impact on reservation strategies},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Efficient buffer overflow detection on GPU.
<em>TPDS</em>, <em>32</em>(5), 1161–1177. (<a
href="https://doi.org/10.1109/TPDS.2020.3042965">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rich thread-level parallelism of GPU has motivated co-running GPU kernels on a single GPU. However, when GPU kernels co-run, it is possible that one kernel can leverage buffer overflow to attack another kernel running on the same GPU. There is very limited work aiming to detect buffer overflow for GPU. Existing work has either large performance overhead or limited capability in detecting buffer overflow. In this article, we introduce GMODx, a runtime software system that can detect GPU buffer overflow. GMODx performs always-on monitoring on allocated memory based on a canary-based design. First, for the fine-grained memory management, GMODx introduces a set of byte arrays to store buffer information for overflow detection. Techniques, such as lock-free accesses to the byte arrays, delayed memory free, efficient memory reallocation, and garbage collection for the byte arrays, are proposed to achieve high performance. Second, for the coarse-grained memory management, GMODx utilizes unified memory to delegate the always-on monitoring to the CPU. To reduce performance overhead, we propose several techniques, including customized list data structure and specific optimizations against the unified memory. For micro-benchmarking, our experiments show that GMODx is capable of detecting buffer overflow for the fine-grained memory management without performance loss, and that it incurs small runtime overhead (4.2 percent on average and up to 9.7 percent) for the coarse-grained memory management. For real workloads, we deploy GMODx on the TensorFlow framework, it only causes 0.8 percent overhead on average (up to 1.8 percent).},
  archive      = {J_TPDS},
  author       = {Bang Di and Jianhua Sun and Hao Chen and Dong Li},
  doi          = {10.1109/TPDS.2020.3042965},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1161-1177},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Efficient buffer overflow detection on GPU},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A scalable multi-layer PBFT consensus for blockchain.
<em>TPDS</em>, <em>32</em>(5), 1146–1160. (<a
href="https://doi.org/10.1109/TPDS.2020.3042392">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical Byzantine Fault Tolerance (PBFT) consensus mechanism shows a great potential to break the performance bottleneck of the Proof-of-Work (PoW)-based blockchain systems, which typically support only dozens of transactions per second and require minutes to hours for transaction confirmation. However, due to frequent inter-node communications, PBFT mechanism has a poor node scalability and thus it is typically adopted in small networks. To enable PBFT in large systems such as massive Internet of Things (IoT) ecosystems and blockchain, in this article, a scalable multi-layer PBFT-based consensus mechanism is proposed by hierarchically grouping nodes into different layers and limiting the communication within the group. We first propose an optimal double-layer PBFT and show that the communication complexity is significantly reduced. Specifically, we prove that when the nodes are evenly distributed within the sub-groups in the second layer, the communication complexity is minimized. The security threshold is analyzed based on faulty probability determined (FPD) and faulty number determined (FND) models, respectively. We also provide a practical protocol for the proposed double-layer PBFT system. Finally, the results are extended to arbitrary-layer PBFT systems with communication complexity and security analysis. Simulation results verify the effectiveness of the analytical results.},
  archive      = {J_TPDS},
  author       = {Wenyu Li and Chenglin Feng and Lei Zhang and Hao Xu and Bin Cao and Muhammad Ali Imran},
  doi          = {10.1109/TPDS.2020.3042392},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1146-1160},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A scalable multi-layer PBFT consensus for blockchain},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Multi-hop multi-task partial computation offloading in
collaborative edge computing. <em>TPDS</em>, <em>32</em>(5), 1133–1145.
(<a href="https://doi.org/10.1109/TPDS.2020.3042224">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing (CEC) is a recent popular paradigm where different edge devices collaborate by sharing data and computation resources. One of the fundamental issues in CEC is to make task offloading decision. However, it is a challenging problem to solve as tasks can be offloaded to a device at multi-hop distance leading to conflicting network flows due to limited bandwidth constraint. There are some works on multi-hop computation offloading problem in the literature. However, existing works have not jointly considered multi-hop partial computation offloading and network flow scheduling that can cause network congestion and inefficient performance in terms of completion time. This article formulates the joint multi-task partial computation offloading and network flow scheduling problem to minimize the average completion time of all tasks. The formulated problem optimizes several dependent decision variables including partial offloading ratio, remote offloading device, start time of tasks, routing path, and start time of network flows. The problem is formulated as an MINLP optimization problem and shown to be NP-hard. We propose a joint partial offloading and flow scheduling heuristic (JPOFH) that decides partial offloading ratio by considering both waiting times at the devices and start time of network flows. We also do the relaxation of formulated MINLP problem to an LP problem using McCormick envelope to give a lower bound solution. Performance comparison done using simulation shows that JPOFH leads to up to 32 percent improvement in average completion time compared to benchmark solutions which do not make a joint decision.},
  archive      = {J_TPDS},
  author       = {Yuvraj Sahni and Jiannong Cao and Lei Yang and Yusheng Ji},
  doi          = {10.1109/TPDS.2020.3042224},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1133-1145},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-hop multi-task partial computation offloading in collaborative edge computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Design and implementation of a criticality- and
heterogeneity-aware runtime system for task-parallel applications.
<em>TPDS</em>, <em>32</em>(5), 1117–1132. (<a
href="https://doi.org/10.1109/TPDS.2020.3031911">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multiprocessing (HMP) is an emerging technology for high-performance and energy-efficient computing. While task parallelism is widely used in various computing domains, such as embedded, big-data, and machine-learning computing domains, it still remains unexplored to investigate the efficient runtime support that effectively utilizes the criticality of the tasks of the target application and the heterogeneity of the underlying HMP system with full resource management. To bridge this gap, we propose CHRT, a criticality- and heterogeneity-aware runtime system for task-parallel applications. CHRT dynamically estimates the performance and power consumption of the target task-parallel application and robustly manages the full HMP system resources (i.e., core types, counts, and voltage/frequency levels) to maximize the overall efficiency. Our quantitative evaluation based on widely-used task parallel benchmarks and two full HMP systems (i.e., the XU3 and HiKey970 HMP systems) demonstrates the effectiveness of CHRT in that CHRT achieves significantly higher energy (e.g., 60.4 and 57.2 percent on average on the XU3 system) and energy-delay product (e.g., 52.2 and 44.0 percent on average on the HiKey970 system) efficiency than the baseline runtime system that employs the breadth-first scheduler and the state-of-the-art criticality-aware runtime system and incurs low performance overheads.},
  archive      = {J_TPDS},
  author       = {Myeonggyun Han and Jinsu Park and Woongki Baek},
  doi          = {10.1109/TPDS.2020.3031911},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1117-1132},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and implementation of a criticality- and heterogeneity-aware runtime system for task-parallel applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Subutai: Speeding up legacy parallel applications through
data synchronization. <em>TPDS</em>, <em>32</em>(5), 1102–1116. (<a
href="https://doi.org/10.1109/TPDS.2020.3040066">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decrease of the performance gain dictated by Moore&#39;s Law boosted the development of manycore architectures to replace single-core architectures. These new architectures must employ parallel applications and distribute its workload over a multitude of cores to reach the desired performance. Parallel applications are harder to develop than sequential ones since the developer must guarantee data integrity using synchronization primitives. While multiple novel solutions have been proposed to speed up parallel applications through handling one type of data synchronization primitive, exceptionally few works support multiple types of synchronization primitives and legacy code. This article proposes Subutai, a hardware/software co-design solution for accelerating multiple synchronization primitives without modifying the application source code. By providing a new user library, while retaining an existing synchronization API, legacy and novel applications can benefit from our solution. Our experimental evaluation, which provides a POSIX Threads implementation, demonstrates Subutai speeds up to 2.71× and 4.61× the execution of single- and multiple-application executions, respectively.},
  archive      = {J_TPDS},
  author       = {Rodrigo Cataldo and Ramon Fernandes and Kevin J. M. Martin and Jarbas Silveira and Gustavo Sanchez and Johanna Sepúlveda and César Marcon and Jean-Philippe Diguet},
  doi          = {10.1109/TPDS.2020.3040066},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1102-1116},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Subutai: Speeding up legacy parallel applications through data synchronization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Distributed and collective deep reinforcement learning
for computation offloading: A practical perspective. <em>TPDS</em>,
<em>32</em>(5), 1085–1101. (<a
href="https://doi.org/10.1109/TPDS.2020.3042599">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is a promising solution to support resource-constrained devices by offloading tasks to the edge servers. However, traditional approaches (e.g., linear programming and game-theory methods) for computation offloading mainly focus on the immediate performance, potentially leading to performance degradation in the long run. Recent breakthroughs regarding deep reinforcement learning (DRL) provide alternative methods, which focus on maximizing the cumulative reward. Nonetheless, there exists a large gap to deploy real DRL applications in MEC. This is because: 1) training a well-performed DRL agent typically requires data with large quantities and high diversity, and 2) DRL training is usually accompanied by huge costs caused by trial-and-error. To address this mismatch, we study the applications of DRL on the multi-user computation offloading problem from a more practical perspective. In particular, we propose a distributed and collective DRL algorithm called DC-DRL with several improvements: 1) a distributed and collective training scheme that assimilates knowledge from multiple MEC environments, which not only greatly increases data amount and diversity but also spreads the exploration costs, 2) an updating method called adaptive n-step learning, which can improve training efficiency without suffering from the high variance caused by distributed training, and 3) combining the advantages of deep neuroevolution and policy gradient to maximize the utilization of multiple environments and prevent the premature convergence. Lastly, evaluation results demonstrate the effectiveness of our proposed algorithm. Compared with the baselines, the exploration costs and final system costs are reduced by at least 43 and 9.4 percent, respectively.},
  archive      = {J_TPDS},
  author       = {Xiaoyu Qiu and Weikun Zhang and Wuhui Chen and Zibin Zheng},
  doi          = {10.1109/TPDS.2020.3042599},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1085-1101},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Distributed and collective deep reinforcement learning for computation offloading: A practical perspective},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Privacy-preserving similarity search with efficient
updates in distributed key-value stores. <em>TPDS</em>, <em>32</em>(5),
1072–1084. (<a
href="https://doi.org/10.1109/TPDS.2020.3042695">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving similarity search plays an essential role in data analytics, especially when very large encrypted datasets are stored in the cloud. Existing mechanisms on privacy-preserving similarity search were not able to support secure updates (addition and deletion) efficiently when frequent updates are needed. In this article, we propose a new mechanism to support parallel privacypreserving similarity search in a distributed key-value store in the cloud, with a focus on efficient addition and deletion operations, both executed with sublinear time complexity. If search accuracy is the top priority, we further leverage Yao&#39;s garbled circuits and the homomorphic property of Hash-ElGamal encryption to build a secure evaluation protocol, which can obtain the top-R most accurate results without extensive client-side post-processing. We have formally analyzed the security strength of our proposed approach, and performed an extensive array of experiments to show its superior performance as compared to existing mechanisms in the literature. In particular, we evaluate the performance of our proposed protocol with respect to the time it takes to build the index and perform similarity queries. Extensive experimental results demonstrated that our protocol can speedup the index building process by up to 800x with 2 threads and the similarity queries by up to -7x with comparable accuracy, as compared to the state-of-the-art in the literature.},
  archive      = {J_TPDS},
  author       = {Wanyu Lin and Helei Cui and Baochun Li and Cong Wang},
  doi          = {10.1109/TPDS.2020.3042695},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1072-1084},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy-preserving similarity search with efficient updates in distributed key-value stores},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). IPPTS: An efficient algorithm for scientific workflow
scheduling in heterogeneous computing systems. <em>TPDS</em>,
<em>32</em>(5), 1057–1071. (<a
href="https://doi.org/10.1109/TPDS.2020.3041829">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient scheduling algorithms are key for attaining high performance in heterogeneous computing systems. In this article, we propose a new list scheduling algorithm for assigning task graphs to fully connected heterogeneous processors with an aim to minimize the scheduling length. The proposed algorithm, called Improved Predict Priority Task Scheduling (IPPTS) algorithm has two phases: task prioritization phase, which gives priority to tasks, and processor selection phase, which selects a processor for a task. The IPPTS algorithm has a quadratic time complexity as the related algorithms for the same goal, that is $O(t^{2} \times p)$ , for $t$ tasks and $p$ processors. Our algorithm reduces the scheduling length significantly by looking ahead in both task prioritization phase and processor selection phase. In this way, the algorithm is looking ahead to schedule a task and its heaviest successor task to the optimistic processor, i.e., the processor that minimizes their computation and communication costs. The experiments based on both randomly generated graphs and graphs of real-world applications show that the IPPTS algorithm significantly outperforms previous list scheduling algorithms in terms of makespan, speedup, makespan standard deviation, efficiency, and frequency of best results.},
  archive      = {J_TPDS},
  author       = {Hamza Djigal and Jun Feng and Jiamin Lu and Jidong Ge},
  doi          = {10.1109/TPDS.2020.3041829},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1057-1071},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {IPPTS: An efficient algorithm for scientific workflow scheduling in heterogeneous computing systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Thermal prediction for efficient energy management of
clouds using machine learning. <em>TPDS</em>, <em>32</em>(5), 1044–1056.
(<a href="https://doi.org/10.1109/TPDS.2020.3040800">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal management in the hyper-scale cloud data centers is a critical problem. Increased host temperature creates hotspots which significantly increases cooling cost and affects reliability. Accurate prediction of host temperature is crucial for managing the resources effectively. Temperature estimation is a non-trivial problem due to thermal variations in the data center. Existing solutions for temperature estimation are inefficient due to their computational complexity and lack of accurate prediction. However, data-driven machine learning methods for temperature prediction is a promising approach. In this regard, we collect and study data from a private cloud and show the presence of thermal variations. We investigate several machine learning models to accurately predict the host temperature. Specifically, we propose a gradient boosting machine learning model for temperature prediction. The experiment results show that our model accurately predicts the temperature with the average RMSE value of 0.05 or an average prediction error of 2.38 °C, which is 6 °C less as compared to an existing theoretical model. In addition, we propose a dynamic scheduling algorithm to minimize the peak temperature of hosts. The results show that our algorithm reduces the peak temperature by 6.5 °C and consumes 34.5 percent less energy as compared to the baseline algorithm.},
  archive      = {J_TPDS},
  author       = {Shashikant Ilager and Kotagiri Ramamohanarao and Rajkumar Buyya},
  doi          = {10.1109/TPDS.2020.3040800},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1044-1056},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Thermal prediction for efficient energy management of clouds using machine learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Petrel: Heterogeneity-aware distributed deep learning via
hybrid synchronization. <em>TPDS</em>, <em>32</em>(5), 1030–1043. (<a
href="https://doi.org/10.1109/TPDS.2020.3040601">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parameter server (PS) paradigm has achieved great success in deploying large-scale distributed Deep Learning (DL) systems. However, these systems implicitly assume that the cluster is homogeneous and this assumption does not hold in many realworld cases. Although the previous efforts are paid to address heterogeneity, they mainly prioritize the contribution of fast workers and reduce the involvement of slow workers, resulting in the limitations of workload imbalance and computation inefficiency. We reveal that grouping workers into communities, an abstraction proposed by us, and handling parameter synchronization at the community level can conquer these limitations and accelerate the training convergence progress. The inspiration of community comes from our exploration of prior knowledge about the similarity between workers, which is often neglected by previous work. These observations motivate us to propose a new synchronization mechanism named Community-aware Synchronous Parallel (CASP), which uses the Asynchronous Advantage Actor-Critic (A3C)-based algorithm to intelligently determine community configuration and fully improve the synchronization performance. The whole idea has been implemented in a prototype system called Petrel that achieves a good balance between convergence efficiency and communication overhead. The evaluation under various benchmarks with multiple metrics and baseline comparison demonstrates the effectiveness of Petrel. Specifically, Petrel accelerates the training convergence speed by up to 1.87 x faster and reduces communication traffic by up to 26.85 percent, on average, over the non-community synchronization mechanisms.},
  archive      = {J_TPDS},
  author       = {Qihua Zhou and Song Guo and Zhihao Qu and Peng Li and Li Li and Minyi Guo and Kun Wang},
  doi          = {10.1109/TPDS.2020.3040601},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1030-1043},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Petrel: Heterogeneity-aware distributed deep learning via hybrid synchronization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Transformations of high-level synthesis codes for
high-performance computing. <em>TPDS</em>, <em>32</em>(5), 1014–1029.
(<a href="https://doi.org/10.1109/TPDS.2020.3039409">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial computing architectures promise a major stride in performance and energy efficiency over the traditional load/store devices currently employed in large scale computing systems. The adoption of high-level synthesis (HLS) from languages such as C++ and OpenCL has greatly increased programmer productivity when designing for such platforms. While this has enabled a wider audience to target spatial computing architectures, the optimization principles known from traditional software design are no longer sufficient to implement high-performance codes, due to fundamentally distinct aspects of hardware design, such as programming for deep pipelines, distributed memory resources, and scalable routing. To alleviate this, we present a collection of optimizing transformations for HLS, targeting scalable and efficient architectures for high-performance computing (HPC) applications. We systematically identify classes of transformations (pipelining, scalability, and memory), the characteristics of their effect on the HLS code and the resulting hardware (e.g., increasing data reuse or resource consumption), and the objectives that each transformation can target (e.g., resolve interface contention, or increase parallelism). We show how these can be used to efficiently exploit pipelining, on-chip distributed fast memory, and on-chip dataflow, allowing for massively parallel architectures. To quantify the effect of various transformations, we cover the optimization process of a sample set of HPC kernels, provided as open source reference codes. We aim to establish a common toolbox to guide both performance engineers and compiler engineers in tapping into the performance potential offered by spatial computing architectures using HLS.},
  archive      = {J_TPDS},
  author       = {Johannes de Fine Licht and Maciej Besta and Simon Meierhans and Torsten Hoefler},
  doi          = {10.1109/TPDS.2020.3039409},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1014-1029},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Transformations of high-level synthesis codes for high-performance computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Boosting parallel influence-maximization kernels for
undirected networks with fusing and vectorization. <em>TPDS</em>,
<em>32</em>(5), 1001–1013. (<a
href="https://doi.org/10.1109/TPDS.2020.3038376">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) is the problem of finding a seed vertex set which is expected to incur the maximum influence spread on a graph. It has various applications in practice such as devising an effective and efficient approach to disseminate information, news or ad within a social network. The problem is shown to be NP-hard and approximation algorithms with provable quality guarantees exist in the literature. However, these algorithms are computationally expensive even for medium-scaled graphs. Furthermore, graph algorithms usually suffer from spatial and temporal irregularities during memory accesses, and this adds an extra cost on top of the already expensive IM kernels. In this article we leverage fused sampling, memoization, and vectorization to restructure, parallelize and boost their performance on undirected networks. The proposed approach employs a pseudo-random function and performs multiple Monte-Carlo simulations in parallel to exploit the SIMD lanes effectively and efficiently. In addition, it significantly reduces the number of edge traversals, hence the amount of data brought from the memory, which is critical for almost all memory-bound graph kernels. We apply the proposed approach to the traditional MIXGREEDY algorithm and propose INFUSER-MG which is more than 3000χ fasterthan the greedy approaches and can run on large graphs that have been considered as too large in the literature. For instance, the new algorithm runs in 2.09, 0.08, 0.36 seconds on graphs Amazon, NetHEP, NetPhy with 16 threads where the sequential baseline takes 141.3, 259.1 and 1725.2 seconds, respectively. To compare INFUSER-MG with the state-of-the-art approximation algorithms, we conduct a thorough experimental analysis with various influence settings. The results on real-life, undirected networks show that on 16 threads, INFUSER-MG is 2:3χ-173:8χ faster than state-of-the-art while being superior in terms of influence scores, and using a comparable amount of memory.},
  archive      = {J_TPDS},
  author       = {Gökhan Göktürk and Kamer Kaya},
  doi          = {10.1109/TPDS.2020.3038376},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {1001-1013},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Boosting parallel influence-maximization kernels for undirected networks with fusing and vectorization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Analysis of global and local synchronization in parallel
computing. <em>TPDS</em>, <em>32</em>(5), 988–1000. (<a
href="https://doi.org/10.1109/TPDS.2020.3037469">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a parallel computing scenario, the synchronization overhead, needed to coordinate the execution on the parallel computing nodes, can significantly impair the overall execution performance. Typically, synchronization is achieved by adopting a global synchronization schema involving all the nodes. In many application domains, though, a looser synchronization schema, namely, local synchronization, can be exploited, in which each node needs to synchronize only with a subset of the other nodes. In this work, we compare the performance of global and local synchronization using the efficiency, i.e., the ratio between the useful computing time and the total computing time, including the synchronization overhead, as a key performance indicator. We present an analytical study of the asymptotic behavior of the efficiency when the number of nodes increases. As an original contribution, we prove, using the Max-Plus algebra, that there is a non-zero lower bound on the efficiency in the case of local synchronization and we present a statistical procedure to find a value of this bound. This outcome marks a significant advantage of local synchronization with respect to global synchronization, for which the efficiency tends to zero when increasing the number of nodes.},
  archive      = {J_TPDS},
  author       = {Franco Cicirelli and Andrea Giordano and Carlo Mastroianni},
  doi          = {10.1109/TPDS.2020.3037469},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {5},
  pages        = {988-1000},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Analysis of global and local synchronization in parallel computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Parallelization and optimization of NSGA-II on sunway
TaihuLight system. <em>TPDS</em>, <em>32</em>(4), 975–987. (<a
href="https://doi.org/10.1109/TPDS.2020.3037082">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sunway TaihuLight system is the first supercomputer offering a peak performance over 100 PFlops, which can be utilized to parallelize Non-dominated Sorting Genetic Algorithm II (NSGA-II), a standard approach to multi-objective optimization. However, insufficient off-chip memory bandwidth and limited scratchpad memory capacity of the supercomputer hinder the performance improvement of parallellizing NSGA-II. In this article, we propose an optimized parallel NSGA-II on Sunway TaihuLight system, called swNSGA-II, by utilizing process- and thread-level parallelism of the system based on an improved island/master-slave model. To overcome the hurdles of low memory bandwidth and capacity, we propose a data sharing scheme based on register-level communication that can efficiently parallelize non-dominated sorting and crowding-distance computation of NSGA-II. Several optimization techniques including vectorization, direct memory accessing, and double buffering are also adopted to further accelerate swNSGA-II. Experiment results show that the proposed swNSGA-II can achieve a speedup of 41284 on a use case of path planning, and a speedup of 62692 on ZDT1 as compared to conventional NSGA-II.},
  archive      = {J_TPDS},
  author       = {Xin Liu and Jun Sun and Lin Zheng and Su Wang and Yao Liu and Tongquan Wei},
  doi          = {10.1109/TPDS.2020.3037082},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {975-987},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Parallelization and optimization of NSGA-II on sunway TaihuLight system},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A generic stochastic model for resource availability in
fog computing environments. <em>TPDS</em>, <em>32</em>(4), 960–974. (<a
href="https://doi.org/10.1109/TPDS.2020.3037247">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is an increasingly popular method with which to process the huge amount of data generated by the Internet of Things (IoT) devices and applications at the edge-level, using the heterogeneous autonomous end-devices of the participating users. To meet the requirements of the IoT and time-sensitive applications, a Fog computing platform needs to select appropriate resources, the availability of which can be guaranteed during the execution of the application. For the proper selection of resources, the platform must be able to predict future availability. Hence, a proper resource availability model which provides knowledge about the future availability of resources in the Fog computing environment is required. However, designing an efficient resource availability model, in a highly distributed and mobile environment like the Fog, is a complex task due to the multidimensional characteristics of Fog devices, such as mobility, lack of centralised control, limited resources, and being battery powered. Existing resource availability models did not consider all the characteristics of a real Fog environment. Therefore, this study aims to provide a generic continuous-time Markov chain (CTMC), based resource availability model for Fog computing environments. The applicability of the model is shown by integrating the model input with the nearest-location best fit (NLBF) and Best-Fit resource selection policies.},
  archive      = {J_TPDS},
  author       = {Sudheer Kumar Battula and Małgorzata M. O&#39;Reilly and Saurabh Garg and James Montgomery},
  doi          = {10.1109/TPDS.2020.3037247},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {960-974},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A generic stochastic model for resource availability in fog computing environments},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). High-performance routing with multipathing and path
diversity in ethernet and HPC networks. <em>TPDS</em>, <em>32</em>(4),
943–959. (<a
href="https://doi.org/10.1109/TPDS.2020.3035761">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent line of research into topology design focuses on lowering network diameter. Many low-diameter topologies such as Slim Fly or Jellyfish that substantially reduce cost, power consumption, and latency have been proposed. A key challenge in realizing the benefits of these topologies is routing. On one hand, these networks provide shorter path lengths than established topologies such as Clos or torus, leading to performance improvements. On the other hand, the number of shortest paths between each pair of endpoints is much smaller than in Clos, but there is a large number of non-minimal paths between router pairs. This hampers or even makes it impossible to use established multipath routing schemes such as ECMP. In this article, to facilitate high-performance routing in modern networks, we analyze existing routing protocols and architectures, focusing on how well they exploit the diversity of minimal and non-minimal paths. We first develop a taxonomy of different forms of support for multipathing and overall path diversity. Then, we analyze how existing routing schemes support this diversity. Among others, we consider multipathing with both shortest and non-shortest paths, support for disjoint paths, or enabling adaptivity. To address the ongoing convergence of HPC and “Big Data” domains, we consider routing protocols developed for both HPC systems and for data centers as well as general clusters. Thus, we cover architectures and protocols based on Ethernet, InfiniBand, and other HPC networks such as Myrinet. Our review will foster developing future high-performance multipathing routing protocols in supercomputers and data centers.},
  archive      = {J_TPDS},
  author       = {Maciej Besta and Jens Domke and Marcel Schneider and Marek Konieczny and Salvatore Di Girolamo and Timo Schneider and Ankit Singla and Torsten Hoefler},
  doi          = {10.1109/TPDS.2020.3035761},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {943-959},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {High-performance routing with multipathing and path diversity in ethernet and HPC networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Towards greening MapReduce clusters considering both
computation energy and cooling energy. <em>TPDS</em>, <em>32</em>(4),
931–942. (<a
href="https://doi.org/10.1109/TPDS.2020.3029724">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increased processing power of MapReduce clusters generally enhances performance and availability at the cost of substantial energy consumption that often incurs higher operational costs (e.g., electricity bills) and negative environmental impacts (e.g., carbon dioxide emissions). There exist a few greening methods for computing clusters in the literature that focus mainly on computational energy consumption leaving cooling energy, which occupies a significant portion of the total energy consumed by the clusters. To this extent, in this article, we propose a machine learning-based approach that reduces the total energy consumption of a MapReduce cluster considering both computational energy and cooling energy. Our approach predicts the number of machines that results in minimum total energy consumption. We perform the prediction through applying different machine learning techniques over year-long data collected from a real setup. We evaluate performance of our approach through both real test-bed experimentation and simulation. Our evaluation reveals that our approach achieves substantial reduction in total energy consumption compared to other state-of-the-art alternatives while experiencing marginal performance degradation in a few cases.},
  archive      = {J_TPDS},
  author       = {Tarik Reza Toha and A. S. M. Rizvi and Jannatun Noor and Muhammad Abdullah Adnan and A. B. M. Alim Al Islam},
  doi          = {10.1109/TPDS.2020.3029724},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {931-942},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards greening MapReduce clusters considering both computation energy and cooling energy},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Large-scale analysis of docker images and performance
implications for container storage systems. <em>TPDS</em>,
<em>32</em>(4), 918–930. (<a
href="https://doi.org/10.1109/TPDS.2020.3034517">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Docker containers have become a prominent solution for supporting modern enterprise applications due to the highly desirable features of isolation, low overhead, and efficient packaging of the application’s execution environment. Containers are created from images which are shared between users via a registry. The amount of data registries store is massive. For example, Docker Hub, a popular public registry, stores at least half a million public images. In this article, we analyze over 167 TB of uncompressed Docker Hub images, characterize them using multiple metrics and evaluate the potential of file-level deduplication. Our analysis helps to make conscious decisions when designing storage for containers in general and Docker registries in particular. For example, only 3 percent of the files in images are unique while others are redundant file copies, which means file-level deduplication has a great potential to save storage space. Furthermore, we carry out a comprehensive analysis of both small I/O request performance and copy-on-write performance for multiple popular container storage drivers. Our findings can motivate and help improve the design of data reduction and caching methods for images, pulling optimizations for registries, and storage drivers.},
  archive      = {J_TPDS},
  author       = {Nannan Zhao and Vasily Tarasov and Hadeel Albahar and Ali Anwar and Lukas Rupprecht and Dimitrios Skourtis and Arnab K. Paul and Keren Chen and Ali R. Butt},
  doi          = {10.1109/TPDS.2020.3034517},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {918-930},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Large-scale analysis of docker images and performance implications for container storage systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Canary: Decentralized distributed deep learning via
gradient sketch and partition in multi-interface networks.
<em>TPDS</em>, <em>32</em>(4), 900–917. (<a
href="https://doi.org/10.1109/TPDS.2020.3036738">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-interface networks are efficient infrastructures to deploy distributed Deep Learning (DL) tasks as the model gradients generated by each worker can be exchanged to others via different links in parallel. Although this decentralized parameter synchronization mechanism can reduce the time of gradient exchange, building a high-performance distributed DL architecture still requires the balance of communication efficiency and computational utilization, i.e., addressing the issues of traffic burst, data consistency, and programming convenience. To achieve this goal, we intend to asynchronously exchange gradient pieces without the central control in multi-interface networks. We propose the Piece-level Gradient Exchange and Multi-interface Collective Communication to handle parameter synchronization and traffic transmission, respectively. Specifically, we design the gradient sketch approach based on 8-bit uniform quantization to compress gradient tensors and introduce the colayerabstraction to better handle gradient partition, exchange and pipelining. Also, we provide general programming interfaces to capture the synchronization semantics and build the Gradient Exchange Index (GEI) data structures to make our approach online applicable. We implement our algorithms into a prototype system called Canary by using PyTorch-1.4.0. Experiments conducted in Alibaba Cloud demonstrate that Canary reduces 56.28 percent traffic on average and completes the training by up to 1.61x, 2.28x, and 2.84x faster than BML, Ako on PyTorch, and PS on TensorFlow, respectively.},
  archive      = {J_TPDS},
  author       = {Qihua Zhou and Kun Wang and Haodong Lu and Wenyao Xu and Yanfei Sun and Song Guo},
  doi          = {10.1109/TPDS.2020.3036738},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {900-917},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Canary: Decentralized distributed deep learning via gradient sketch and partition in multi-interface networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Coarse-grained parallel routing with recursive
partitioning for FPGAs. <em>TPDS</em>, <em>32</em>(4), 884–899. (<a
href="https://doi.org/10.1109/TPDS.2020.3035787">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing is a very time-consuming stage in the FPGA design flow, significantly hindering the productivity. This article proposes CPRS, a coarse-grained parallel routing scheme in a distributed computing environment. First, we partition entire routing region to guide the assignment of nets for parallel processing. The partitioning is a recursive fashion, and at each recursive partitioning, the region is partitioned into two subregions forming three subsets of nets. The first subset consists of potentially dependent nets and they are distributed in different subregions. The remaining two subsets consist of potentially independent nets and they are distributed in their own subregions. Second, we route the nets of first subset in serial and process the remaining two subsets in parallel. The parallel processing is a coarse-grained fashion, which is implemented by MPI parallel programming model. Finally, we explore the optimization of both partitioning and parallel processing to further improve the overall speedup of parallel routing. In addition, we adopt MPI message to synchronize the intermediate results between different cores in parallel routing for a feasible solution. Experiments use a set of commonly used benchmarks to demonstrate the effectiveness of CPRS. Notably, CPRS achieves about 18× speedup on average using 32 processor cores with minor loss of quality, compared with the VTR 7.0 serial router. There is about 1.6× improvement over the state-of-the-art parallel router.},
  archive      = {J_TPDS},
  author       = {Minghua Shen and Guojie Luo and Nong Xiao},
  doi          = {10.1109/TPDS.2020.3035787},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {884-899},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Coarse-grained parallel routing with recursive partitioning for FPGAs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Towards efficient large-scale interprocedural program
static analysis on distributed data-parallel computation. <em>TPDS</em>,
<em>32</em>(4), 867–883. (<a
href="https://doi.org/10.1109/TPDS.2020.3036190">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static program analysis has been widely applied along the whole process of the program development for bug detection, code optimization, testing, etc. Although researchers have made significant work in static program analysis, it is still challenging to perform sophisticated interprocedural analysis on large-scale modern software. The underlying reason is that interprocedural analysis for large-scale modern software is highly computation- and memory-intensive, leading to poor efficiency and scalability. In this article, we introduce an efficient distributed and scalable solution for sophisticated static analysis. Specifically, we propose a data-parallel algorithm and a join-process-filter computation model for the CFL-reachability-based interprocedural analysis. Based on that, an efficient distributed static analysis engine called BigSpa is developed, which is composed of an offline batch static program analysis system and an online incremental static program analysis system. The BigSpa system has high generality and can support all kinds of static analysis tasks that can be expressed as CFL reachability problems. The performance of BigSpa is evaluated on real-world large-scale software datasets. Our experiments show that the offline batch system can exceed an order of magnitude compared with the most advanced analysis tools available on performance, and for incremental analysis with small batch updates on the same data sets, the online analysis system can achieve near real-time response, which is very fast and flexible.},
  archive      = {J_TPDS},
  author       = {Rong Gu and Zhiqiang Zuo and Xi Jiang and Han Yin and Zhaokang Wang and Linzhang Wang and Xuandong Li and Yihua Huang},
  doi          = {10.1109/TPDS.2020.3036190},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {867-883},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficient large-scale interprocedural program static analysis on distributed data-parallel computation},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). MO-tree: An efficient forwarding engine for
spatiotemporal-aware pub/sub systems. <em>TPDS</em>, <em>32</em>(4),
855–866. (<a
href="https://doi.org/10.1109/TPDS.2020.3036014">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For large-scale spatiotemporal-aware publish/subscribe systems, it is critical to design an efficient forwarding engine to achieve fast matching and maintenance of events and subscriptions. For this goal, we propose a novel data structure called MO-Tree to index both subscriptions and events in a unified way. The design philosophy behind MO-Tree is to keep the data structure concise, which manifests in three aspects: limiting the height of MO-Tree, trading space for time, and avoiding node merging and splitting. The difficulty in designing MO-Tree is how to efficiently index width-variable intervals. We present a multi-level cell-overlapping partition scheme and build a theoretical model to optimize the cell width in each level. To evaluate the performance of MO-Tree, a series of experiments is conducted on real-world trace datasets. The experiment results show MO-Tree significantly outperforms the state-of-the-art in terms of matching speed and maintenance cost.},
  archive      = {J_TPDS},
  author       = {Tianchen Ding and Shiyou Qian and Jian Cao and Guangtao Xue and Yanmin Zhu and Jiadi Yu and Minglu Li},
  doi          = {10.1109/TPDS.2020.3036014},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {855-866},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {MO-tree: An efficient forwarding engine for spatiotemporal-aware Pub/Sub systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). SEIZE: Runtime inspection for parallel dataflow systems.
<em>TPDS</em>, <em>32</em>(4), 842–854. (<a
href="https://doi.org/10.1109/TPDS.2020.3035170">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many Data-Intensive Scalable Computing (DISC) Systems provide easy-to-use functional APIs, and efficient scheduling and execution strategies allowing users to build concise data-parallel programs. In these systems, data transformations are concealed by exposed APIs, and intermediate execution states are masked under dataflow transitions. Consequently, many crucial features and optimizations (e.g., debugging, data provenance, runtime skew detection), which require runtime datafow states, are not well-supported. Inspired by our experience in implementing features and optimizations over DISC systems, we present SEIZE, a unified framework that enables dataflow inspection-wiretapping the data-path with listening logic-in MapReduce-style programming model. We generalize our lessons learned by providing a set of primitives defining dataflow inspection, orchestration options for different inspection granularities, and operator decomposition and dataflow punctuation strategy for dataflow intervention. We demonstrate the generality and flexibility of the approach by deploying SEIZE in both Apache Spark and Apache Flink, and by implementing a prototype runtime query optimizer for Spark. Our experiments show that, the overhead introduced by the inspection logic is most of the time negligible (less than 5 percent in Spark and 10 percent in Flink).},
  archive      = {J_TPDS},
  author       = {Youfu Li and Matteo Interlandi and Fotis Psallidas and Wei Wang and Carlo Zaniolo},
  doi          = {10.1109/TPDS.2020.3035170},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {842-854},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {SEIZE: Runtime inspection for parallel dataflow systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Cuttlefish: Neural configuration adaptation for video
analysis in live augmented reality. <em>TPDS</em>, <em>32</em>(4),
830–841. (<a
href="https://doi.org/10.1109/TPDS.2020.3035044">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instead of relying on remote clouds, today&#39;s Augmented Reality (AR) applications usually send videos to nearby edge servers for analysis (such as objection detection) so as to optimize the user&#39;s quality of experience (QoE), which is often determined by not only detection latency but also detection accuracy, playback fluency, etc. Therefore, many studies have been conducted to help adaptively choose best video configuration, e.g., resolution and frame per second (fps), based on network bandwidth to further improve QoE. However, we notice that the video content itself has significant impacts on the configuration selection, e.g., the videos with high-speed objects must be encoded with a high fps to meet the user&#39;s fluency requirement. In this article, we aim to adaptively select configurations that match the time-varying network condition as well as the video content. We design Cuttlefish, a system that generates video configuration decisions using reinforcement learning (RL). Cuttlefish trains a neural network model that picks a configuration for the next encoding slot based on observations collected by AR devices. Cuttlefish does not rely on any pre-programmed models or specific assumptions on the environments. Instead, it learns to make configuration decisions solely through observations of the resulting performance of historical decisions. Cuttlefish automatically learns the adaptive configuration policy for diverse AR video streams and obtains a gratifying QoE. We compared Cuttlefish to several state-of-the-art bandwidth-based and velocity-based methods using trace-driven and real world experiments. The results show that Cuttlefish achieves a 18.4-25.8 percent higher QoE than the others.},
  archive      = {J_TPDS},
  author       = {Ning Chen and Siyi Quan and Sheng Zhang and Zhuzhong Qian and Yibo Jin and Jie Wu and Wenzhong Li and Sanglu Lu},
  doi          = {10.1109/TPDS.2020.3035044},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {830-841},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cuttlefish: Neural configuration adaptation for video analysis in live augmented reality},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Achieving probabilistic atomicity with well-bounded
staleness and low read latency in distributed datastores. <em>TPDS</em>,
<em>32</em>(4), 815–829. (<a
href="https://doi.org/10.1109/TPDS.2020.3034328">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although it has been commercially successful to deploy weakly consistent but highly-responsive distributed datastores, the tension between developing complex applications and obtaining only weak consistency guarantees becomes more and more severe. The almost strong consistency tradeoff aims at achieving both strong consistency and low latency in the common case. In distributed storage systems, we investigate the generic notion of almost strong consistency in terms of designing fast read algorithms while guaranteeing Probabilistic Atomicity with well-Bounded staleness (PAB). This problem has been explored in the case where only one client can write the data. However, the more general case where multiple clients can write the data has not been studied. In this article, we study the fast read algorithm for PAB in the multi-writer case. We show the bound of data staleness and the probability of atomicity violation by decomposing inconsistent reads into the read inversion and the write inversion patterns. We implement the fast read algorithm and evaluate the consistency-latency tradeoffs based on the instrumentation of Cassandra and the YCSB benchmark framework. The theoretical analysis and the experimental evaluations show that our fast read algorithm guarantees PAB, even when faced with dynamic changes in the computing environment.},
  archive      = {J_TPDS},
  author       = {Lingzhi Ouyang and Yu Huang and Hengfeng Wei and Jian Lu},
  doi          = {10.1109/TPDS.2020.3034328},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {815-829},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Achieving probabilistic atomicity with well-bounded staleness and low read latency in distributed datastores},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Energy-aware inference offloading for DNN-driven
applications in mobile edge clouds. <em>TPDS</em>, <em>32</em>(4),
799–814. (<a
href="https://doi.org/10.1109/TPDS.2020.3032443">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing focus on Artificial Intelligence (AI) applications, Deep Neural Networks (DNNs) have been successfully used in a number of application areas. As the number of layers and neurons in DNNs increases rapidly, significant computational resources are needed to execute a learned DNN model. This ever-increasing resource demand of DNNs is currently met by large-scale data centers with state-of-the-art GPUs. However, increasing availability of mobile edge computing and 5G technologies provide new possibilities for DNN-driven AI applications, especially where these application make use of data sets that are distributed in different locations. One fundamental process of a DNN-driven application in mobile edge clouds is the adoption of “inferencing” - the process of executing a pre-trained DNN based on newly generated image and video data from mobile devices. We investigate offloading DNN inference requests in a 5G-enabled mobile edge cloud (MEC), with the aim to admit as many inference requests as possible. We propose exact and approximate solutions to the problem of inference offloading in MECs. We also consider dynamic task offloading for inference requests, and devise an online algorithm that can be adapted in real time. The proposed algorithms are evaluated through large-scale simulations and using a real world test-bed implementation. The experimental results demonstrate that the empirical performance of the proposed algorithms outperform their theoretical counterparts and other similar heuristics reported in literature.},
  archive      = {J_TPDS},
  author       = {Zichuan Xu and Liqian Zhao and Weifa Liang and Omer F. Rana and Pan Zhou and Qiufen Xia and Wenzheng Xu and Guowei Wu},
  doi          = {10.1109/TPDS.2020.3032443},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {799-814},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-aware inference offloading for DNN-driven applications in mobile edge clouds},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). BOSSA: A decentralized system for proofs of data
retrievability and replication. <em>TPDS</em>, <em>32</em>(4), 786–798.
(<a href="https://doi.org/10.1109/TPDS.2020.3030063">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proofs of retrievability and proofs of replication are two cryptographic tools that enable a remote server to prove that the users&#39; data has been correctly stored. Nevertheless, the literature either requires the users themselves to perform expensive verification jobs, or relies on a “fully trustworthy” third party auditor (TPA) to execute the public verification. In addition, none of existing solutions consider the underlying incentive issues behind a rational server who is motivated to collect users&#39; data but tries to evade the replication checking in order to save storage resources. In this article, we propose the first decentralized system for proofs of data retrievability and replication-BOSSA, which is incentive-compatible for each party and realizes automated auditing atop off-the-shelf blockchain platforms. We deal with issues such as proof enforcements to catch malicious behaviors, new metrics to measure the contributions, and reward distributions to create a fair reciprocal environment. BOSSA also incorporates privacy-enhancing techniques to prevent decentralized peers (including blockchain nodes) from inferring private information about the outsourced data. Security analysis is presented in the context of integrity, privacy, and reliability. We implement a prototype based on BOSSA leveraging the smart contracts of Ethereum blockchain. Our extensive experimental evaluations demonstrate the practicality of our proposal.},
  archive      = {J_TPDS},
  author       = {Dian Chen and Haobo Yuan and Shengshan Hu and Qian Wang and Cong Wang},
  doi          = {10.1109/TPDS.2020.3030063},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {786-798},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {BOSSA: A decentralized system for proofs of data retrievability and replication},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Resettable encoded vector clock for causality analysis
with an application to dynamic race detection. <em>TPDS</em>,
<em>32</em>(4), 772–785. (<a
href="https://doi.org/10.1109/TPDS.2020.3032293">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality tracking among events is a fundamental challenge in distributed environments. Much previous work on this subject has focused on designing an efficient and scalable protocol to represent logical time. Several implementations of logical clocks have been proposed, most recently the Encoded Vector Clock (EVC), a protocol to encode Vector Clocks (VC) in scalar numbers through the use of prime numbers, to improve performance and scalability. We propose and formalize the concept of Resettable Encoded Vector Clock (REVC), a new logical clock implementation, which builds on the EVC to tackle its very high growth rate issue. We show how our REVC can be applied in both shared memory systems and message passing systems to achieve a consistent logical clock. We show, through practical examples, the advantage of REVC&#39;s growth rate with respect to EVC&#39;s growth rate. Finally, we show a practical application of the REVC to the dynamic race detection problem in multi-threaded environments. We compare our tool to the currently existing VC-based tool DJIT + to show how the REVC can help in achieving higher performance with respect to the Vector Clock.},
  archive      = {J_TPDS},
  author       = {Tommaso Pozzetti and Ajay D. Kshemkalyani},
  doi          = {10.1109/TPDS.2020.3032293},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {772-785},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Resettable encoded vector clock for causality analysis with an application to dynamic race detection},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Homomorphic sorting with better scalability.
<em>TPDS</em>, <em>32</em>(4), 760–771. (<a
href="https://doi.org/10.1109/TPDS.2020.3030748">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homomorphic sorting is an operation that blindly sorts a given set of encrypted numbers without decrypting them (thus, there is no need for the secret key). In this article, we propose a new, efficient, and scalable method for homomorphic sorting of numbers: polynomial rank sort algorithm. To put the new algorithm in a comparative perspective, we provide an extensive survey of classical sorting algorithms and networks that are not directly suitable for homomorphic computation. We also include, in our discussions, two of our previous algorithms specifically designed for homomorphic sorting operation: direct and greedy sort, and explain how they evolve from classical sorting networks. We theoretically show that the new algorithm is superior in terms of multiplicative depth when compared with all other algorithms. When batched implementation is used, the number of comparisons is reduced from O(N 2 ) to O(N) provided that the number of slots is larger than or equal to the number of elements in the set. Our software implementation results confirm that the new algorithm is several orders of magnitude faster than many methods in the literature. Also, the polynomial sort algorithm scales better than the fastest algorithm in the literature to the best our knowledge although for small sets the execution times are comparable. The proposed algorithm is amenable to parallel implementation as most time consuming operations in the algorithm can naturally be performed concurrently.},
  archive      = {J_TPDS},
  author       = {Gizem S. Çetin and Erkay Savaş and Berk Sunar},
  doi          = {10.1109/TPDS.2020.3030748},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {760-771},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Homomorphic sorting with better scalability},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Accelerating large-scale prioritized graph computations
by hotness balanced partition. <em>TPDS</em>, <em>32</em>(4), 746–759.
(<a href="https://doi.org/10.1109/TPDS.2020.3032709">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prioritized computation is shown promising performance for a large class of graph algorithms. It prioritizes the execution of some vertices that play important roles in determining convergence. For large-scale distributed graph processing, graph partitioning is an important preprocessing step that aims to balance workload and to reduce communication costs between workers. However, existing graph partitioning methods are designed for round-robin synchronous distributed frameworks. They balance workload without distinction of vertex importance and fail to consider the characteristics of priority-based scheduling, which may limit the benefit of prioritized graph computation. In this article, to accelerate prioritized iterative graph computations, we propose Hotness Balanced Partition (HBP). In prioritized graph computation, high priority vertices are likely to be executed more frequently and are likely to pass more messages, which result in hot vertices. Based on this observation, we partition graph by distributing vertices with distinction according to their hotness rather than blindly distributing vertices with equal weights, aiming to evenly distribute the hot vertices among workers. We further provide two HBP algorithms: a streaming-based algorithm for efficient one-pass processing and a distributed algorithm for distributed processing. Our results show that our proposed partitioning methods outperform the state-of-the-art partitioning methods, Fennel, HotGraph, and SNE.},
  archive      = {J_TPDS},
  author       = {Shufeng Gong and Yanfeng Zhang and Ge Yu},
  doi          = {10.1109/TPDS.2020.3032709},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {746-759},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Accelerating large-scale prioritized graph computations by hotness balanced partition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021b). Editor’s note. <em>TPDS</em>, <em>32</em>(4), 743–745.
(<a href="https://doi.org/10.1109/TPDS.2020.3035244">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TPDS},
  author       = {Manish Parashar},
  doi          = {10.1109/TPDS.2020.3035244},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {4},
  pages        = {743-745},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Editor&#39;s note},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Achieving fine-grained flow management through hybrid
rule placement in SDNs. <em>TPDS</em>, <em>32</em>(3), 728–742. (<a
href="https://doi.org/10.1109/TPDS.2020.3030630">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained flow management is useful in many practical applications, e.g., resource allocation, anomaly detection and traffic engineering. However, it is difficult to provide fine-grained management for a large number of flows in SDNs due to switches&#39; limited flow table capacity. While using wildcard rules can reduce the number of flow entries needed, it cannot fully ensure fine-grained management for all the flows without degrading application performance. In this article, we design and implement hybrid rule placement for fine-grained flow management (to be referred to as HiFi here after). HiFi achieves fine-grained management with a minimal number of flow entries through taking a two-step approach: wildcard entry installment and application-specific exact-match entry installment. How to optimally install wildcard and exact-match flow entries, however, is intractable. Therefore, we design approximation algorithms with bounded factors to solve these problems. We consider how to achieve network-wide load balancing via fine-grained flow management as a case study. Both experiment on a testbed built with open virtual switches and extensive simulation show that HiFi can reduce the number of required flow entries by about 45-69 percent and reduce the control overhead by about 28-50 percent compared with the state-of-the-art approaches for achieving fine-grained flow management.},
  archive      = {J_TPDS},
  author       = {Gongming Zhao and Hongli Xu and Jingyuan Fan and Liusheng Huang and Chunming Qiao},
  doi          = {10.1109/TPDS.2020.3030630},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {728-742},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Achieving fine-grained flow management through hybrid rule placement in SDNs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021a). The deep learning compiler: A comprehensive survey.
<em>TPDS</em>, <em>32</em>(3), 708–727. (<a
href="https://doi.org/10.1109/TPDS.2020.3030548">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.},
  archive      = {J_TPDS},
  author       = {Mingzhen Li and Yi Liu and Xiaoyan Liu and Qingxiao Sun and Xin You and Hailong Yang and Zhongzhi Luan and Lin Gan and Guangwen Yang and Depei Qian},
  doi          = {10.1109/TPDS.2020.3030548},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {708-727},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {The deep learning compiler: A comprehensive survey},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Hierarchical multi-agent optimization for resource
allocation in cloud computing. <em>TPDS</em>, <em>32</em>(3), 692–707.
(<a href="https://doi.org/10.1109/TPDS.2020.3030920">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, an important concern is to allocate the available resources of service nodes to the requested tasks on demand and to make the objective function optimum, i.e., maximizing resource utilization, payoffs, and available bandwidth. This article proposes a hierarchical multi-agent optimization (HMAO) algorithm in order to maximize the resource utilization and make the bandwidth cost minimum for cloud computing. The proposed HMAO algorithm is a combination of the genetic algorithm (GA) and the multi-agent optimization (MAO) algorithm. With maximizing the resource utilization, an improved GA is implemented to find a set of service nodes that are used to deploy the requested tasks. A decentralized-based MAO algorithm is presented to minimize the bandwidth cost. We study the effect of key parameters of the HMAO algorithm by the Taguchi method and evaluate the performance results. The results demonstrate that the HMAO algorithm is more effective than two baseline algorithms of genetic algorithm (GA) and fast elitist non-dominated sorting genetic algorithm (NSGA-II) in solving the large-scale optimization problem of resource allocation. Furthermore, we provide the performance comparison of the HMAO algorithm with two heuristic Greedy and Viterbi algorithms in on-line resource allocation.},
  archive      = {J_TPDS},
  author       = {Xiangqiang Gao and Rongke Liu and Aryan Kaushik},
  doi          = {10.1109/TPDS.2020.3030920},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {692-707},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Hierarchical multi-agent optimization for resource allocation in cloud computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Cryptomining detection in container clouds using system
calls and explainable machine learning. <em>TPDS</em>, <em>32</em>(3),
674–691. (<a
href="https://doi.org/10.1109/TPDS.2020.3029088">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of containers in cloud computing has been steadily increasing. With the emergence of Kubernetes, the management of applications inside containers (or pods) is simplified. Kubernetes allows automated actions like self-healing, scaling, rolling back, and updates for the application management. At the same time, security threats have also evolved with attacks on pods to perform malicious actions. Out of several recent malware types, cryptomining has emerged as one of the most serious threats with its hijacking of server resources for cryptocurrency mining. During application deployment and execution in the pod, a cryptomining process, started by a hidden malware executable can be run in the background, and a method to detect malicious cryptomining software running inside Kubernetes pods is needed. One feasible strategy is to use machine learning (ML) to identify and classify pods based on whether or not they contain a running process of cryptomining. In addition to such detection, the system administrator will need an explanation as to the reason(s) of the ML&#39;s classification outcome. The explanation will justify and support disruptive administrative decisions such as pod removal or its restart with a new image. In this article, we describe the design and implementation of an ML-based detection system of anomalous pods in a Kubernetes cluster by monitoring Linux-kernel system calls (syscalls). Several types of cryptominers images are used as containers within an anomalous pod, and several ML models are built to detect such pods in the presence of numerous healthy cloud workloads. Explainability is provided using SHAP, LIME, and a novel auto-encoding-based scheme for LSTM models. Seven evaluation metrics are used to compare and contrast the explainable models of the proposed ML cryptomining detection engine.},
  archive      = {J_TPDS},
  author       = {Rupesh Raj Karn and Prabhakar Kudva and Hai Huang and Sahil Suneja and Ibrahim M. Elfadel},
  doi          = {10.1109/TPDS.2020.3029088},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {674-691},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cryptomining detection in container clouds using system calls and explainable machine learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Constructing completely independent spanning trees in
data center network based on augmented cube. <em>TPDS</em>,
<em>32</em>(3), 665–673. (<a
href="https://doi.org/10.1109/TPDS.2020.3029654">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A set of spanning trees T 1 ; T 2 ;. .. ; Tk in a network G are Completely Independent Spanning Trees (CISTs) if for any two nodes u and v in V (G), the paths between u and v in any two trees have no common edges and no common internal nodes. CISTs have important applications in data center networks, such as fault-tolerant multi-node broadcasting, fault-tolerant one-to-all broadcasting, reliable broadcasting, secure message distribution, and so on. The augmented cube AQn is a prominent variant of the well-known hypercube Qn, and having the important property of scalability, and both Qn and AQn have been proposed as the underlying structure for a data center network. The data center network based on AQn is denoted by AQDNn, and the logic graph of AQDNn is denoted by L-AQDNn. In this article, we study how to construct n - 1 CISTs in L-AQDNn. The constructed n - 1 CISTs are optimal in the sense that n - 1 is the maximally allowed CISTs in L-AQDNn. The correctness of our construction algorithm is proved. It is the first time a direct relationship is established between the dimension of a hypercube-family network and the number of CISTs it can host.},
  archive      = {J_TPDS},
  author       = {Guo Chen and Baolei Cheng and Dajin Wang},
  doi          = {10.1109/TPDS.2020.3029654},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {665-673},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Constructing completely independent spanning trees in data center network based on augmented cube},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Lewat: A lightweight, efficient, and wear-aware
transactional persistent memory system. <em>TPDS</em>, <em>32</em>(3),
649–664. (<a
href="https://doi.org/10.1109/TPDS.2020.3028385">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging non-volatile memory (also termed as persistent memory, PM) technologies promise persistence, byte-addressability, and DRAM-like read/write latency. A proliferation of persistent memory systems have been proposed to leverage PM for fast data persistence and expose malloc-like persistent APIs. By eliminating disk I/Os, these systems gain low-latency and high-throughput access performance for persistent data. However, there still exist non-negligible limitations in these systems, such as frequent context switches, inefficient allocation, heavy logging overhead, and lack of wear-leveling techniques. To solve these problems, we develop Lewat, a lightweight, efficient, and wear-aware transactional persistent memory system. Lewat is built in user-layer to avoid kernel/user layer context switches and enables lightweight persistent data access. We decouple the data space into slot zone and page zone. Based on this, we design different allocators in these two zones to achieve efficient allocation performance for both small-sized data and large-sized data. To minimize logging overhead, we propose an efficient adaptive logging framework. The main idea is to utilize different logging techniques for different workloads. We also propose a suite of system-coupled wear-leveling techniques that contain wear-aware allocation, wear-aware update, and write reduction. We evaluate Lewat on a real non-volatile memory platform and the experimental results show that compared with state-of-the-art persistent memory systems, Lewat has much lower latency and higher throughput.},
  archive      = {J_TPDS},
  author       = {Kaixin Huang and Sumin Li and Linpeng Huang and Kian-Lee Tan and Hong Mei},
  doi          = {10.1109/TPDS.2020.3028385},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {649-664},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Lewat: A lightweight, efficient, and wear-aware transactional persistent memory system},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Energy-efficient hardware-accelerated synchronization for
shared-l1-memory multiprocessor clusters. <em>TPDS</em>, <em>32</em>(3),
633–648. (<a
href="https://doi.org/10.1109/TPDS.2020.3028691">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The steeply growing performance demands for highly power- and energy-constrained processing systems such as end-nodes of the Internet-of-Things (IoT) have led to parallel near-threshold computing (NTC), joining the energy-efficiency benefits of low-voltage operation with the performance typical of parallel systems. Shared-L1-memory multiprocessor clusters are a promising architecture, delivering performance in the order of GOPS and over 100 GOPS/W of energy-efficiency. However, this level of computational efficiency can only be reached by maximizing the effective utilization of the processing elements (PEs) available in the clusters. Along with this effort, the optimization of PE-to-PE synchronization and communication is a critical factor for performance. In this article, we describe a light-weight hardware-accelerated synchronization and communication unit (SCU) for tightly-coupled clusters of processors. We detail the architecture, which enables fine-grain per-PE power management, and its integration into an eight-core cluster of RISC-V processors. To validate the effectiveness of the proposed solution, we implemented the eight-core cluster in advanced 22 nm FDX technology and evaluated performance and energy-efficiency with tunable microbenchmarks and a set of real-life applications and kernels. The proposed solution allows synchronization-free regions as small as 42 cycles, over 41× smaller than the baseline implementation based on fast test-and-set access to L1 memory when constraining the microbenchmarks to 10 percent synchronization overhead. When evaluated on the real-life DSP-applications, the proposed SCU improves performance by up to 92 and 23 percent on average and energy efficiency by up to 98 and 39 percent on average.},
  archive      = {J_TPDS},
  author       = {Florian Glaser and Giuseppe Tagliavini and Davide Rossi and Germain Haugou and Qiuting Huang and Luca Benini},
  doi          = {10.1109/TPDS.2020.3028691},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {633-648},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Energy-efficient hardware-accelerated synchronization for shared-l1-memory multiprocessor clusters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Modeling and optimization of performance and cost of
serverless applications. <em>TPDS</em>, <em>32</em>(3), 615–632. (<a
href="https://doi.org/10.1109/TPDS.2020.3028841">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function-as-a-Service (FaaS) and serverless applications have proliferated significantly in recent years because of their high scalability, ease of resource management, and pay-as-you-go pricing model. However, cloud users are facing practical problems when they migrate their applications to the serverless pattern, which are the lack of analytical performance and billing model and the trade-off between limited budget and the desired quality of service of serverless applications. In this article, we fill this gap by proposing and answering two research questions regarding the prediction and optimization of performance and cost of serverless applications. We propose a new construct to formally define a serverless application workflow, and then implement analytical models to predict the average end-to-end response time and the cost of the workflow. Consequently, we propose a heuristic algorithm named Probability Refined Critical Path Greedy algorithm (PRCP) with four greedy strategies to answer two fundamental optimization questions regarding the performance and the cost. We extensively evaluate the proposed models by conducting experimentation on AWS Lambda and Step Functions. Our analytical models can predict the performance and cost of serverless applications with more than 98 percent accuracy. The PRCP algorithms can achieve the optimal configurations of serverless applications with 97 percent accuracy on average.},
  archive      = {J_TPDS},
  author       = {Changyuan Lin and Hamzeh Khazaei},
  doi          = {10.1109/TPDS.2020.3028841},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {615-632},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Modeling and optimization of performance and cost of serverless applications},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Failure-atomic byte-addressable r-tree for persistent
memory. <em>TPDS</em>, <em>32</em>(3), 601–614. (<a
href="https://doi.org/10.1109/TPDS.2020.3028699">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose Failure-atomic Byte-addressable R-tree (FBR-tree) that leverages the byte-addressability, persistence, and high performance of persistent memory while guaranteeing the crash consistency. We carefully control the order of store and cacheline flush instructions and prevent any single store instruction from making an FBR-tree inconsistent and unrecoverable. We also develop a non-blocking lock-free range query algorithm for FBR-tree. Since FBR-tree allows read transactions to detect and ignore any transient inconsistent states, multiple read transactions can concurrently access tree nodes without using shared locks while other write transactions are making changes to them. Our performance study shows that FBR-tree successfully reduces the legacy logging overhead and the lock-free range query algorithm shows up to 2.6x higher query processing throughput than the shared lock-based crabbing concurrency protocol.},
  archive      = {J_TPDS},
  author       = {Soojeong Cho and Wonbae Kim and Sehyeon Oh and Changdae Kim and Kwangwon Koh and Beomseok Nam},
  doi          = {10.1109/TPDS.2020.3028699},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {601-614},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Failure-atomic byte-addressable R-tree for persistent memory},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Investigating the adoption of hybrid encrypted cloud data
deduplication with game theory. <em>TPDS</em>, <em>32</em>(3), 587–600.
(<a href="https://doi.org/10.1109/TPDS.2020.3028685">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted data deduplication, along with different preferences in data access control, brings the birth of hybrid encrypted cloud data deduplication (H-DEDU for short). However, whether H-DEDU can be successfully deployed in practice has not been seriously investigated. Obviously, the adoption of H-DEDU depends on whether it can bring economic benefits to all stakeholders. But existing economic models of cloud storage fail to support H-DEDU due to complicated interactions among stakeholders. In this article, we establish a formal economic model of H-DEDU by formulating the utilities of all involved stakeholders, i.e., data holders, data owners, and Cloud Storage Providers (CSPs). Then, we construct a multi-stage Stackelberg game, which consists of Holder Participation Game, Owner Online Game, and CSP Pricing Game, to capture the interactions among all system stakeholders. We further analyze the conditions of the existence of a sub-game perfect Nash Equilibrium and propose a gradient-based algorithm to help the stakeholders choose near-optimal strategies. Extensive experiments show the feasibility of the proposed algorithm in achieving the Nash Equilibrium of the Stackelberg game. Additionally, we investigate the effects of parameters related to CSP, data owners and data holders on H-DEDU adoption. Our study advises all stakeholders the best strategies to adopt H-DEDU.},
  archive      = {J_TPDS},
  author       = {Xueqin Liang and Zheng Yan and Robert H. Deng and Qinghua Zheng},
  doi          = {10.1109/TPDS.2020.3028685},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {587-600},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Investigating the adoption of hybrid encrypted cloud data deduplication with game theory},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). PQC acceleration using GPUs: FrodoKEM, NewHope, and
kyber. <em>TPDS</em>, <em>32</em>(3), 575–586. (<a
href="https://doi.org/10.1109/TPDS.2020.3025691">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present the first GPU implementation for FrodoKEM-976, NewHope-1024, and Kyber-1024. These algorithms belong to three different classes of post-quantum algorithms: Learning with errors (LWE), Ring-LWE, and Module-LWE. We show the practical applicability of the algorithms in different scenarios using two different implementation approaches. Moreover, we achieve highly efficient realization of computationally expensive operations such as NTT (Number Theoretic Transform), matrix multiplication, and Keccak. Since, these are the most common operations in lattice-based cryptographic algorithms, the techniques presented in this article will likely benefit other similar algorithms. Using a NVIDIA QUADRO GV100 graphics card, we undertook a detailed experimental study. For NewHope and Kyber we were able to perform approximately 504K and 473K key exchanges per second, demonstrating a speedup of almost 53.1× and 51.05× compared to the reference C implementation. Compared to the optimized AVX2 versions we obtain speedups of 25.7× and 14.6×, respectively. Further, implementation of FrodoKEM resulted in a speedup of 50.6×, 44.2×, and 36.9× for KeyGen, Encaps and Decaps operations. Compared to its AVX2 counterpart, we achieved a speedup of about 7.3×, 4.7× and 4.9×, respectively. We also show that using multiple streams resulted in further speedup of about 28-38 percent.},
  archive      = {J_TPDS},
  author       = {Naina Gupta and Arpan Jati and Amit Kumar Chauhan and Anupam Chattopadhyay},
  doi          = {10.1109/TPDS.2020.3025691},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {575-586},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PQC acceleration using GPUs: FrodoKEM, NewHope, and kyber},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Privacy-preserving multi-keyword searchable encryption
for distributed systems. <em>TPDS</em>, <em>32</em>(3), 561–574. (<a
href="https://doi.org/10.1109/TPDS.2020.3027003">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cloud storage has been widely adopted in various applications, how to protect data privacy while allowing efficient data search and retrieval in a distributed environment remains a challenging research problem. Existing searchable encryption schemes are still inadequate on desired functionality and security/privacy perspectives. Specifically, supporting multi-keyword search under the multi-user setting, hiding search pattern and access pattern, and resisting keyword guessing attacks (KGA) are the most challenging tasks. In this article, we present a new searchable encryption scheme that addresses the above problems simultaneously, which makes it practical to be adopted in distributed systems. It not only enables multi-keyword search over encrypted data under a multi-writer/multi-reader setting but also guarantees the data and search pattern privacy. To prevent KGA, our scheme adopts a multi-server architecture, which accelerates search response, shares the workload, and lowers the key leakage risk by allowing only authorized servers to jointly test whether a search token matches a stored ciphertext. A novel subset decision mechanism is also designed as the core technique underlying our scheme and can be further used in applications other than keyword search. Finally, we prove the security and evaluate the computational and communication efficiency of our scheme to demonstrate its practicality.},
  archive      = {J_TPDS},
  author       = {Xueqiao Liu and Guomin Yang and Willy Susilo and Joseph Tonien and Ximeng Liu and Jian Shen},
  doi          = {10.1109/TPDS.2020.3027003},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {561-574},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Privacy-preserving multi-keyword searchable encryption for distributed systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Bi-objective optimization of data-parallel applications
on heterogeneous HPC platforms for performance and energy through
workload distribution. <em>TPDS</em>, <em>32</em>(3), 543–560. (<a
href="https://doi.org/10.1109/TPDS.2020.3027338">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance and energy are the two most important objectives for optimization on modern parallel platforms. In this article, we show that moving from single-objective optimization for performance or energy to their bi-objective optimization on heterogeneous processors results in a tremendous increase in the number of optimal solutions (workload distributions) even for the simple case of linear performance and energy profiles. We then study full performance and energy profiles of two real-life data-parallel applications and find that they exhibit shapes that are non-linear and complex enough to prevent good approximation of them as analytical functions for input to exact algorithms or optimization software for determining the Pareto front. We, therefore, propose a solution method solving the bi-objective optimization problem on heterogeneous processors. The method&#39;s novel component is an efficient and exact global optimization algorithm that takes as an input performance and energy profiles as arbitrary discrete functions of workload size, which accurately and realistically take into account resource contention and NUMA inherent in modern parallel platforms, and returns the Pareto-optimal solutions (generally speaking, load imbalanced). To construct the input discrete energy functions, the method employs a methodology that accurately models the energy consumption by a hybrid data-parallel application executing on a heterogeneous HPC platform containing different computing devices using system-level power measurements provided by power meters. We experimentally analyse the proposed solution method using three data-parallel applications, matrix multiplication, 2D fast Fourier transform (2D-FFT), and gene sequencing, on two connected heterogeneous servers consisting of multicore CPUs, GPUs, and Intel Xeon Phi. We show that it determines a superior Pareto front containing the best load balanced solutions and all the load imbalanced solutions that are ignored by load balancing methods.},
  archive      = {J_TPDS},
  author       = {Hamidreza Khaleghzadeh and Muhammad Fahad and Arsalan Shahid and Ravi Reddy Manumachu and Alexey Lastovetsky},
  doi          = {10.1109/TPDS.2020.3027338},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {543-560},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Bi-objective optimization of data-parallel applications on heterogeneous HPC platforms for performance and energy through workload distribution},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Optimistic causal consistency for geo-replicated
key-value stores. <em>TPDS</em>, <em>32</em>(3), 527–542. (<a
href="https://doi.org/10.1109/TPDS.2020.3026778">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal consistency (CC) is an attractive consistency model for geo-replicated data stores because it hits a sweet spot in the ease-of-programming versus performance trade-off. We present a new approach for implementing CC in geo-replicated data stores, which we call Optimistic Causal Consistency (OCC). OCC&#39;s main design goal is to maximize data freshness. The optimism in our approach lies in the fact that the updates replicated to a remote data center are made visible immediately, without checking if their causal dependencies have been received. Servers perform the dependency check needed to enforce CC only upon serving a client operation, rather than on receipt of a replicated data item as in existing systems. OCC offers a significant gain in data freshness, which is of crucial importance for various types of applications, such as real-time systems. OCC&#39;s potentially blocking behavior makes it vulnerable to network partitions. We therefore propose a recovery mechanism that allows an OCC system to fall back on a pessimistic protocol to continue operating during network partitions. We implement POCC, the first causally consistent geo-replicated multi-master key-value data store designed to maximize data freshness. We show that POCC improves data freshness, while offering comparable or better performance than its pessimistic counterparts.},
  archive      = {J_TPDS},
  author       = {Kristina Spirovska and Diego Didona and Willy Zwaenepoel},
  doi          = {10.1109/TPDS.2020.3026778},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {527-542},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Optimistic causal consistency for geo-replicated key-value stores},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). ADRL: A hybrid anomaly-aware deep reinforcement
learning-based resource scaling in clouds. <em>TPDS</em>,
<em>32</em>(3), 514–526. (<a
href="https://doi.org/10.1109/TPDS.2020.3025914">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The virtualization concept and elasticity feature of cloud computing enable users to request resources on-demand and in the pay-as-you-go model. However, the high flexibility of the model makes the on-time resource scaling problem more complex. A variety of techniques such as threshold-based rules, time series analysis, or control theory are utilized to increase the efficiency of dynamic scaling of resources. However, the inherent dynamicity of cloud-hosted applications requires autonomic and adaptable systems that learn from the environment in real-time. Reinforcement Learning (RL) is a paradigm that requires some agents to monitor the surroundings and regularly perform an action based on the observed states. RL has a weakness to handle high dimensional state space problems. Deep-RL models are a recent breakthrough for modeling and learning in complex state space problems. In this article, we propose a Hybrid Anomaly-aware Deep Reinforcement Learning-based Resource Scaling (ADRL) for dynamic scaling of resources in the cloud. ADRL takes advantage of anomaly detection techniques to increase the stability of decision-makers by triggering actions in response to the identified anomalous states in the system. Two levels of global and local decision-makers are introduced to handle the required scaling actions. An extensive set of experiments for different types of anomaly problems shows that ADRL can significantly improve the quality of service with less number of actions and increased stability of the system.},
  archive      = {J_TPDS},
  author       = {Sara Kardani-Moghaddam and Rajkumar Buyya and Kotagiri Ramamohanarao},
  doi          = {10.1109/TPDS.2020.3025914},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {514-526},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {ADRL: A hybrid anomaly-aware deep reinforcement learning-based resource scaling in clouds},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A thread level SLO-aware i/o framework for embedded
virtualization. <em>TPDS</em>, <em>32</em>(3), 500–513. (<a
href="https://doi.org/10.1109/TPDS.2020.3026042">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of virtualization technology, it is practical and necessary to integrate virtual machine software into embedded systems. I/O scheduling is important for embedded systems, because embedded systems always face different situations and their requests have more diversity on the requirement of real-time and importance. However, the semantic information associated with the I/O data is completely lost when crossing the virtualized I/O software stack. Here, we present an I/O scheduling framework to connect the semantic gap between the application threads in virtual machines and hardware schedulers in the host machine. Therefore, the details for the I/O request can be passed through the layers of the software stack and each layer can get the specific information about the device environment. Also, various scheduling points have been provided to implement different I/O strategies. Our framework was implemented based on Linux operating system, KVM, QEMU and virtio protocol. A prototype scheduler, Orthrus, was implemented to evaluate the effectiveness of the framework. Comprehensive experiments were conducted and the results show that our framework can guarantee the real-time requirements, and reserve more system resources for critical tasks, with negligible memory consumption and throughput overhead.},
  archive      = {J_TPDS},
  author       = {Xiaoli Gong and Dingyuan Cao and Yuxuan Li and Ximing Liu and Yusen Li and Jin Zhang and Tao Li},
  doi          = {10.1109/TPDS.2020.3026042},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {3},
  pages        = {500-513},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A thread level SLO-aware I/O framework for embedded virtualization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). QShield: Protecting outsourced cloud data queries with
multi-user access control based on SGX. <em>TPDS</em>, <em>32</em>(2),
485–499. (<a
href="https://doi.org/10.1109/TPDS.2020.3024880">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the concern on cloud security, digital encryption is applied before outsourcing data to the cloud for utilization. This introduces a challenge about how to efficiently perform queries over ciphertexts. Crypto-based solutions currently suffer from limited operation support, high computational complexity, weak generality, and poor verifiability. An alternative method that utilizes hardware-assisted Trusted Execution Environment (TEE), i.e., Intel SGX, has emerged to offer high computational efficiency, generality and flexibility. However, SGX-based solutions lack support on multi-user query control and suffer from security compromises caused by untrustworthy TEE function invocation, e.g., key revocation failure, incorrect query results, and sensitive information leakage. In this article, we leverage SGX and propose a secure and efficient SQL-style query framework named QShield. Notably, we propose a novel lightweight secret sharing scheme in QShield to enable multi-user query control; it effectively circumvents key revocation and avoids cumbersome remote attestation for authentication. We further embed a trust-proof mechanism into QShield to guarantee the trustworthiness of TEE function invocation; it ensures the correctness of query results and alleviates side-channel attacks. Through formal security analysis, proof-of-concept implementation and performance evaluation, we show that QShield can securely query over outsourced data with high efficiency and scalable multi-user support.},
  archive      = {J_TPDS},
  author       = {Yaxing Chen and Qinghua Zheng and Zheng Yan and Dan Liu},
  doi          = {10.1109/TPDS.2020.3024880},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {485-499},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {QShield: Protecting outsourced cloud data queries with multi-user access control based on SGX},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Dynamic load balancing in parallel execution of cellular
automata. <em>TPDS</em>, <em>32</em>(2), 470–484. (<a
href="https://doi.org/10.1109/TPDS.2020.3025102">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The allocation of the computational load across different processing elements is an important issue in parallel computing. Indeed, an unbalanced load distribution can strongly affect the performances of a parallel system caused by an excess of synchronization idle times due to less loaded processes waiting for more loaded ones. In this article, we focus on the load balancing issues in the context of the parallel execution of spatial-related applications where the domain space is partitioned in regions that are assigned to different processing elements. In particular, without loss of generality, we consider the well-known spatial-related Cellular Automata computational paradigm for evaluating the proposed dynamic load balancing approach. The main contribution of this article is the derivation of simple closed-form expressions that allow to compute the optimal workload assignment in a dynamic fashion, with the goal of guaranteeing a fully balanced workload distribution during the parallel execution. Based on these expressions, an algorithm for balanced execution of cellular automata is presented and implemented using the MPI technology. Eventually, an experimental section practically shows the behaviour of the proposed dynamic load balancing approach and proves its performance improvement, compared to the not-balanced version, as witnessed by the appreciable reduction of execution times.},
  archive      = {J_TPDS},
  author       = {Andrea Giordano and Alessio De Rango and Rocco Rongo and Donato D&#39;Ambrosio and William Spataro},
  doi          = {10.1109/TPDS.2020.3025102},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {470-484},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Dynamic load balancing in parallel execution of cellular automata},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Minimizing coflow completion time in optical circuit
switched networks. <em>TPDS</em>, <em>32</em>(2), 457–469. (<a
href="https://doi.org/10.1109/TPDS.2020.3025145">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, optical circuit switching is becoming an increasingly favored technology in scaling data center networks for its definitive advantages in data rate, power consumption, and device cost. Concurrently, reducing coflow completion time (CCT) is of great significance for improving application-level performance. However, minimizing CCT in circuit switched networks is totally different from that in traditional packet switched networks due to port constraints and circuit reconfiguration delays. To address this issue, this article proposes Grouped Optimization-based Scheduling (GOS), a CCT minimization algorithm for circuit switched networks integrating circuit and coflow scheduling. We first formalize the CCT minimization problem into a 0-1 programming problem, then relax and solve the problem in 2 steps to obtain the coflow order and flow grouping decisions on each circuit. Thus intra-group reconfiguration delays are saved, and small coflows can be prioritized at the group level. Theoretical analysis proves GOS is a 4-approximation algorithm in average CCT. To reduce computing overheads, we further propose a heuristic approximation algorithm. Extensive simulations show that the heuristic algorithm has satisfactory CCT performance (0.12× Varys, 0.36× Sunflow) as well as high throughput (16.74× Varys, 1.32× Sunflow), and well adapts to a wide range of reconfiguration delays and algorithm decision time.},
  archive      = {J_TPDS},
  author       = {Tong Zhang and Fengyuan Ren and Jiakun Bao and Ran Shu and Wenxue Cheng},
  doi          = {10.1109/TPDS.2020.3025145},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {457-469},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Minimizing coflow completion time in optical circuit switched networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Adaptive preference-aware co-location for improving
resource utilization of power constrained datacenters. <em>TPDS</em>,
<em>32</em>(2), 441–456. (<a
href="https://doi.org/10.1109/TPDS.2020.3023997">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datacenters often host latency-sensitive services that have stringent Quality-of-Service requirement and experience diurnal load pattern. Co-locating best-effort applications that have no QoS requirement with the latency-sensitive services has been widely used to improve the resource utilization of datacenters with careful shared resource management. However, existing co-location techniques tend to result in the power overload problem on power constrained servers due to the ignorance of the power consumption. To this end, we propose Sturgeon, a runtime system proactively manages resources between co-located applications in a power constrained environment, to ensure the QoS of latency-sensitive services while maximizing the throughput of best-effort applications. Our investigation shows that, at a given load, there are multiple feasible resource configurations to meet both QoS requirement and power budget, while one of them yields the maximum throughput of best-effort applications. To find such a configuration, we establish models to accurately predict the performance and power consumption of the co-located applications. Sturgeon monitors the QoS of the services periodically, in order to eliminate the potential QoS violation caused by the unpredictable interference. Besides, when the datacenter hosts different types of applications to perform co-location, Sturgeon places applications with their preferable candidates to improve the overall throughput. The experimental results show that at server level Sturgeon improves the throughput of the best-effort application by 25.43 percent compared to the state-of-the-art technique, while guaranteeing the 95\%-ile latency within the QoS target; at cluster level, Sturgeon improves the overall throughput of best-effort applications by 13.74 percent compared to the baseline.},
  archive      = {J_TPDS},
  author       = {Pu Pang and Quan Chen and Deze Zeng and Minyi Guo},
  doi          = {10.1109/TPDS.2020.3023997},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {441-456},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Adaptive preference-aware co-location for improving resource utilization of power constrained datacenters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Towards minimizing resource usage with QoS guarantee in
cloud gaming. <em>TPDS</em>, <em>32</em>(2), 426–440. (<a
href="https://doi.org/10.1109/TPDS.2020.3024068">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud gaming has been very popular recently, but providing satisfactory gaming experiences to players at a modest cost is still challenging. Colocating several games onto one server could improve server utilization. However, prior work regarding colocating games either ignores the performance interference between games or uses simple performance model to charaterize it, which may make inefficient game colocation decisions and cause QoS violations. In this article, we address the resource allocation issues for colocating games in cloud gaming. We first propose a novel machine learning-based performance model, which is able to capture the complex relationship among the performance interference, the contention features of colocated games and resource partition. Guided by the performance model, we then propose efficient and effective algorithms for two resource allocation scenarios in cloud gaming. We evaluate the proposed solutions through extensive experiments using a large number of real popular games. The results show that our performance model is able to identify whether a colocated game satisfies QoS requirement within an average error of 5 percent, which significantly outperforms the alternatives. Our resource allocation algorithms are able to increase the resource utilization by up to 60 percent compared to the state-of-the-art solutions.},
  archive      = {J_TPDS},
  author       = {Yusen Li and Changjian Zhao and Xueyan Tang and Wentong Cai and Xiaoguang Liu and Gang Wang and Xiaoli Gong},
  doi          = {10.1109/TPDS.2020.3024068},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {426-440},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards minimizing resource usage with QoS guarantee in cloud gaming},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Multi-agent imitation learning for pervasive edge
computing: A decentralized computation offloading algorithm.
<em>TPDS</em>, <em>32</em>(2), 411–425. (<a
href="https://doi.org/10.1109/TPDS.2020.3023936">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive edge computing refers to one kind of edge computing that merely relies on edge devices with sensing, storage and communication abilities to realize peer-to-peer offloading without centralized management. Due to lack of unified coordination, users always pursue profits by maximizing their own utilities. However, on one hand, users may not make appropriate scheduling decisions based on their local observations. On the other hand, how to guarantee the fairness among different edge devices in the fully decentralized environment is rather challenging. To solve the above issues, we propose a decentrailized computation offloading algorithm with the purpose of minimizing average task completion time in the pervasive edge computing networks. We first derive a Nash equilibrium among devices by stochastic game theories based on the full observations of system states. After that, we design a traffic offloading algorithm based on partial observations by integrating general adversarial imitation learning. Multiple experts can provide demonstrations, so that devices can mimic the behaviors of corresponding experts by minimizing the gaps between the distributions of their observation-action pairs. At last, theoretical and performance results show that our solution has a significant advantage compared with other representative algorithms.},
  archive      = {J_TPDS},
  author       = {Xiaojie Wang and Zhaolong Ning and Song Guo},
  doi          = {10.1109/TPDS.2020.3023936},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {411-425},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-agent imitation learning for pervasive edge computing: A decentralized computation offloading algorithm},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Towards efficient scheduling of federated mobile devices
under computational and statistical heterogeneity. <em>TPDS</em>,
<em>32</em>(2), 394–410. (<a
href="https://doi.org/10.1109/TPDS.2020.3023905">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Originated from distributed learning, federated learning enables privacy-preserved collaboration on a new abstracted level by sharing the model parameters only. While the current research mainly focuses on optimizing learning algorithms and minimizing communication overhead left by distributed learning, there is still a considerable gap when it comes to the real implementation on mobile devices. In this article, we start with an empirical experiment to demonstrate computation heterogeneity is a more pronounced bottleneck than communication on the current generation of battery-powered mobile devices, and the existing methods are haunted by mobile stragglers. Further, non-identically distributed data across the mobile users makes the selection of participants critical to the accuracy and convergence. To tackle the computational and statistical heterogeneity, we utilize data as a tuning knob and propose two efficient polynomial-time algorithms to schedule different workloads on various mobile devices, when data is identically or non-identically distributed. For identically distributed data, we combine partitioning and linear bottleneck assignment to achieve near-optimal training time without accuracy loss. For non-identically distributed data, we convert it into an average cost minimization problem and propose a greedy algorithm to find a reasonable balance between computation time and accuracy. We also establish an offline profiler to quantify the runtime behavior of different devices, which serves as the input to the scheduling algorithms. We conduct extensive experiments on a mobile testbed with two datasets and up to 20 devices. Compared with the common benchmarks, the proposed algorithms achieve 2-100× speedup epoch-wise, 2–7 percent accuracy gain and boost the convergence rate by more than 100 percent on CIFAR10.},
  archive      = {J_TPDS},
  author       = {Cong Wang and Yuanyuan Yang and Pengzhan Zhou},
  doi          = {10.1109/TPDS.2020.3023905},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {394-410},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Towards efficient scheduling of federated mobile devices under computational and statistical heterogeneity},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Comment on “circuit ciphertext-policy attribute-based
hybrid encryption with verifiable delegation in cloud computing.”
<em>TPDS</em>, <em>32</em>(2), 392–393. (<a
href="https://doi.org/10.1109/TPDS.2020.3021683">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scheme [1] is flawed because: (1) its circuit access structure is confusingly described; (2) the cloud server cannot complete the related computations; (3) some users can conspire to generate new decryption keys, without the help of the key generation authority.},
  archive      = {J_TPDS},
  author       = {Zhengjun Cao and Olivier Markowitch},
  doi          = {10.1109/TPDS.2020.3021683},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {392-393},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Comment on “Circuit ciphertext-policy attribute-based hybrid encryption with verifiable delegation in cloud computing”},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Multi-GPU design and performance evaluation of
homomorphic encryption on GPU clusters. <em>TPDS</em>, <em>32</em>(2),
379–391. (<a
href="https://doi.org/10.1109/TPDS.2020.3021238">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-GPU design, implementation and performance evaluation of the Halevi-Polyakov-Shoup (HPS) variant of the Fan-Vercauteren (FV) levelled Fully Homomorphic Encryption (FHE) scheme. Our design follows a data parallelism approach and uses partitioning methods to distribute the workload in FV primitives evenly across available GPUs. The design is put to address space and runtime requirements of FHE computations. It is also suitable for distributed-memory architectures, and includes efficient GPU-to-GPU data exchange protocols. Moreover, it is user-friendly as user intervention is not required for task decomposition, scheduling or load balancing. We implement and evaluate the performance of our design on two homogeneous and heterogeneous NVIDIA GPU clusters: K80, and a customized P100. We also provide a comparison with a recent shared-memory-based multi-core CPU implementation using two homomorphic circuits as workloads: vector addition and multiplication. Moreover, we use our multi-GPU Levelled-FHE to implement the inference circuit of two Convolutional Neural Networks (CNNs) to perform homomorphically image classification on encrypted images from the MNIST and CIFAR - 10 datasets. Our implementation provides 1 to 3 orders of magnitude speedup compared with the CPU implementation on vector operations. In terms of scalability, our design shows reasonable scalability curves when the GPUs are fully connected.},
  archive      = {J_TPDS},
  author       = {Ahmad Al Badawi and Bharadwaj Veeravalli and Jie Lin and Nan Xiao and Matsumura Kazuaki and Aung Khin Mi Mi},
  doi          = {10.1109/TPDS.2020.3021238},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {379-391},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-GPU design and performance evaluation of homomorphic encryption on GPU clusters},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A parallel structured divide-and-conquer algorithm for
symmetric tridiagonal eigenvalue problems. <em>TPDS</em>,
<em>32</em>(2), 367–378. (<a
href="https://doi.org/10.1109/TPDS.2020.3019471">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a parallel structured divide-and-conquer (PSDC) eigensolver is proposed for symmetric tridiagonal matrices based on ScaLAPACK and a parallel structured matrix multiplication algorithm, called PSMMA. Computing the eigenvectors via matrix-matrix multiplications is the most computationally expensive part of the divide-and-conquer algorithm, and one of the matrices involved in such multiplications is a rank-structured Cauchy-like matrix. By exploiting this particular property, PSMMA constructs the local matrices by using generators of Cauchy-like matrices without any communication, and further reduces the computation costs by using a structured low-rank approximation algorithm. Thus, both the communication and computation costs are reduced. Experimental results show that both PSMMA and PSDC are highly scalable and scale to 4096 processes at least. PSDC has better scalability than PHDC that was proposed in [16] and only scaled to 300 processes for the same matrices. Comparing with PDSTEDC in ScaLAPACK, PSDC is always faster and achieves 1.4x-1.6x speedup for some matrices with few deflations. PSDC is also comparable with ELPA, with PSDC being faster than ELPA when using few processes and a little slower when using many processes.},
  archive      = {J_TPDS},
  author       = {Xia Liao and Shengguo Li and Yutong Lu and Jose E. Roman},
  doi          = {10.1109/TPDS.2020.3019471},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {367-378},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A parallel structured divide-and-conquer algorithm for symmetric tridiagonal eigenvalue problems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A resource and performance optimization reduction circuit
on FPGAs. <em>TPDS</em>, <em>32</em>(2), 355–366. (<a
href="https://doi.org/10.1109/TPDS.2020.3020117">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reduce is a fundamental computing pattern, which is widely involved in scientific and engineering applications. For example, accumulation, the most common example of reduce pattern, is the core of applications such as dot product, matrix multiplication, and finite impulse response (FIR) filter. However, there is a trade-off between performance and area in the hardware implementation of the reduce pattern. To solve this problem, we propose an optimized reduction method that can handle multiple arbitrary-length sets. The performance of the proposed method is evaluated for both a single data set and numerous data sets. Moreover, to quickly differentiate the data of different sets in the reduction circuit, individual modules are designed to manage the data. We implement the design on FPGAs and present the experimental results. The proposed design with high performance and low resource consumption can achieve at least 1.59 times improvement on area-time product compared with the reported methods.},
  archive      = {J_TPDS},
  author       = {Linhuai Tang and Gang Cai and Yong Zheng and Jiamin Chen},
  doi          = {10.1109/TPDS.2020.3020117},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {355-366},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A resource and performance optimization reduction circuit on FPGAs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). CPDE: A methodology for the transparent distribution of
centralized smart grid programs. <em>TPDS</em>, <em>32</em>(2), 342–354.
(<a href="https://doi.org/10.1109/TPDS.2020.3019759">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control and management in smart grids are facing many challenges such as scalability, heterogeneity and technology innovation. This requires a transformation from the traditional centralised paradigm into a distributed one. In this article, a new distributed programming methodology, called Centralised Programming and Distributed Execution (CPDE), is proposed. CPDE relies on (i) the abstraction of the whole system as a distributed database; (ii) the use of the Smartlog declarative and reactive rule based language for expressing data manipulation; and (iii) the automatic Smartlog rule distribution according to data distribution. It thus provides a simple and straightforward mean for distributed programming. A centralised algorithm of fair over-voltage regulation of PV systems is used as a typical smart grids study case to validate the methodology and to compare it with centralized implementations. The experiments are implemented in a real-time simulation platform with a network of Raspberry Pis. In addition to showing its correctnes and ease of use, the performance of the CPDE implementation is studied, as well as its sensitivity to the increasing number of computing units and the data distribution. Results are promising and show the clear benefits of this methodology compared to more classical implementations.},
  archive      = {J_TPDS},
  author       = {Thi Thanh Quynh Nguyen and Christophe Bobineau and Vincent Debusschere and Quang Huy Giap and Nouredine Hadjsaid},
  doi          = {10.1109/TPDS.2020.3019759},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {342-354},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CPDE: A methodology for the transparent distribution of centralized smart grid programs},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). An automatic synthesizer of advising tools for high
performance computing. <em>TPDS</em>, <em>32</em>(2), 330–341. (<a
href="https://doi.org/10.1109/TPDS.2020.3018636">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Egeria, the first automatic synthesizer of advising tools for High-Performance Computing (HPC). When one provides it with some HPC programming guides as inputs, Egeria automatically constructs a text retrieval tool that can advise on what to do to improve the performance of a given program. The advising tool provides a concise list of essential rules automatically extracted from the documents and can retrieve relevant optimization knowledge for optimization questions. Egeria is built based on a distinctive multi-layered design that leverages natural language processing (NLP) techniques and extends them with HPC-specific knowledge and considerations. This article presents the design, implementation, and both quantitative and qualitative evaluation results of Egeria.},
  archive      = {J_TPDS},
  author       = {Hui Guan and Xipeng Shen and Hamid Krim},
  doi          = {10.1109/TPDS.2020.3018636},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {330-341},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {An automatic synthesizer of advising tools for high performance computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Realizing best checkpointing control in computing
systems. <em>TPDS</em>, <em>32</em>(2), 315–329. (<a
href="https://doi.org/10.1109/TPDS.2020.3015805">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers best checkpointing control realizable in real-world systems, whose mean time between failures (MTBFs) often fluctuate. The considered control scheme is based on equating aggregate checkpointing overhead over an activity sequence of interest (θ) and the expected rework amount after a failure recovery for best checkpointing, called “CHORE” (i.e., checkpointing overhead and rework equated), where θ starts from execution resumption after failure recovery and ends after restore from the following failure. CHORE lets its inter-checkpoint intervals in θ follow a pre-determined sequence independent of MTBF to aim at performance optimality and is shown analytically to keep overall execution time overhead upper bounded. When failure occurrences are tracked during job execution for real-time MTBF estimation, an enhanced CHORE (dubbed En-CHORE) is obtained to lower checkpointing overhead by skipping certain checkpoints at the beginning of each θ before taking checkpoints with the most desirable inter-checkpoint intervals determined on-the-fly for best checkpointing control. En-CHORE can outperform optimal checkpointing (which follows a fixed inter-checkpoint interval optimized for one constant global MTBF known a prior) both under synthetic random failures with local MTBF fluctuating markedly and under real failure traces of 22 real HPC systems (whose failure rates actually fluctuate over their trace time spans).},
  archive      = {J_TPDS},
  author       = {Purushottam Sigdel and Xu Yuan and Nian-Feng Tzeng},
  doi          = {10.1109/TPDS.2020.3015805},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {315-329},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Realizing best checkpointing control in computing systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Recent advances of resource allocation in network
function virtualization. <em>TPDS</em>, <em>32</em>(2), 295–314. (<a
href="https://doi.org/10.1109/TPDS.2020.3017001">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Function Virtualization (NFV) has been emerging as an appealing solution that transforms complex network functions from dedicated hardware implementations to software instances running in a virtualized environment. Due to the numerous advantages such as flexibility, efficiency, scalability, short deployment cycles, and service upgrade, NFV has been widely recognized as the next-generation network service provisioning paradigm. In NFV, the requested service is implemented by a sequence of Virtual Network Functions (VNF) that can run on generic servers by leveraging the virtualization technology. These VNFs are pitched with a predefined order through which data flows traverse, and it is also known as the Service Function Chaining (SFC). In this article, we provide an overview of recent advances of resource allocation in NFV. We generalize and analyze four representative resource allocation problems, namely, (1) the VNF Placement and Traffic Routing problem, (2) VNF Placement problem, (3) Traffic Routing problem in NFV, and (4) the VNF Redeployment and Consolidation problem. After that, we study the delay calculation models and VNF protection (availability) models in NFV resource allocation, which are two important Quality of Service (QoS) parameters. Subsequently, we classify and summarize the representative work for solving the generalized problems by considering various QoS parameters (e.g., cost, delay, reliability, and energy) and different scenarios (e.g., edge cloud, online provisioning, and distributed provisioning). Finally, we conclude our article with a short discussion on the state-of-the-art and emerging topics in the related fields, and highlight areas where we expect high potential for future research.},
  archive      = {J_TPDS},
  author       = {Song Yang and Fan Li and Stojan Trajanovski and Ramin Yahyapour and Xiaoming Fu},
  doi          = {10.1109/TPDS.2020.3017001},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {295-314},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Recent advances of resource allocation in network function virtualization},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021b). Online collaborative data caching in edge computing.
<em>TPDS</em>, <em>32</em>(2), 281–294. (<a
href="https://doi.org/10.1109/TPDS.2020.3016344">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the edge computing (EC) environment, edge servers are deployed at base stations to offer highly accessible computing and storage resources to nearby app users. From the app vendor&#39;s perspective, caching data on edge servers can ensure low latency in app users&#39; retrieval of app data. However, an edge server normally owns limited resources due to its limited size. In this article, we investigate the collaborative caching problem in the EC environment with the aim to minimize the system cost including data caching cost, data migration cost, and quality-of-service (QoS) penalty. We model this collaborative edge data caching problem (CEDC) as a constrained optimization problem and prove that it is NP-complete. We propose an online algorithm, called CEDC-O, to solve this CEDC problem during all time slots. CEDC-O is developed based on Lyapunov optimization, works online without requiring future information, and achieves provable close-to-optimal performance. CEDC-O is evaluated on a real-world data set, and the results demonstrate that it significantly outperforms four representative approaches.},
  archive      = {J_TPDS},
  author       = {Xiaoyu Xia and Feifei Chen and Qiang He and John Grundy and Mohamed Abdelrazek and Hai Jin},
  doi          = {10.1109/TPDS.2020.3016344},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {281-294},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Online collaborative data caching in edge computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). A two-phase dynamic throughput optimization model for big
data transfers. <em>TPDS</em>, <em>32</em>(2), 269–280. (<a
href="https://doi.org/10.1109/TPDS.2020.3012929">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of data transferred over dedicated and non-dedicated network links has been increasing much faster than the increase in the network capacity. On the other hand, the current data transfer solutions fail to guarantee even the promised achievable transfer throughput. In this article, we propose a novel two-phase dynamic throughput optimization model based on mathematical modeling with offline knowledge discovery/analysis and adaptive online decision making. In the offline analysis, we mine historical transfer logs to perform knowledge discovery about the transfer characteristics. The online phase uses the discovered knowledge from the offline analysis along with the real-time investigation of the network condition to optimize the protocol parameters. As the real-time investigation is expensive and provides partial knowledge about the current network status, our model uses historical knowledge about the network and data characteristics to reduce the real-time investigation overhead while ensuring near-optimal throughput for each transfer. Our novel approach is tested over different networks with different datasets, and it has outperformed its closest competitor by 1.7x and the default case by 5x. It also achieved up to 93 percent accuracy compared to the optimal achievable throughput possible on those networks.},
  archive      = {J_TPDS},
  author       = {MD S Q Zulkar Nine and Tevfik Kosar},
  doi          = {10.1109/TPDS.2020.3012929},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {269-280},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {A two-phase dynamic throughput optimization model for big data transfers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Middleware to manage fault tolerance using
semi-coordinated checkpoints. <em>TPDS</em>, <em>32</em>(2), 254–268.
(<a href="https://doi.org/10.1109/TPDS.2020.3015615">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute node failures are becoming a normal event for many long-running and scalable MPI applications. Keeping within the MPI standards and applying some of the methods developed so far in terms of fault tolerance, we developed a methodology that allows applications to tolerate failures through the creation of semi-coordinated checkpoints within the RADIC architecture. To do this, we developed the ULSC2-RADIC middleware that divides the application into independent MPI worlds where each MPI world would correspond to a compute node and make use of the DMTCP checkpoint library in a semi-coordinated environment. We performed experimental results using scientific applications and the NAS Parallel Benchmarks to assess the overhead and also the functionality in case of a node failure. We evaluated the computational cost of the semi-coordinated checkpoints compared with the coordinated checkpoints.},
  archive      = {J_TPDS},
  author       = {Alvaro Wong and Elisa Heymann and Dolores Rexachs and Emilio Luque},
  doi          = {10.1109/TPDS.2020.3015615},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {2},
  pages        = {254-268},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Middleware to manage fault tolerance using semi-coordinated checkpoints},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Fast adaptive task offloading in edge computing based on
meta reinforcement learning. <em>TPDS</em>, <em>32</em>(1), 242–253. (<a
href="https://doi.org/10.1109/TPDS.2020.3014896">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) aims to extend cloud service to the network edge to reduce network traffic and service latency. A fundamental problem in MEC is how to efficiently offload heterogeneous tasks of mobile applications from user equipment (UE) to MEC hosts. Recently, many deep reinforcement learning (DRL)-based methods have been proposed to learn offloading policies through interacting with the MEC environment that consists of UE, wireless channels, and MEC hosts. However, these methods have weak adaptability to new environments because they have low sample efficiency and need full retraining to learn updated policies for new environments. To overcome this weakness, we propose a task offloading method based on meta reinforcement learning, which can adapt fast to new environments with a small number of gradient updates and samples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the offloading policy by a custom sequence-to-sequence (seq2seq) neural network. To efficiently train the seq2seq network, we propose a method that synergizes the first order approximation and clipped surrogate objective. The experimental results demonstrate that this new offloading method can reduce the latency by up to 25 percent compared to three baselines while being able to adapt fast to new environments.},
  archive      = {J_TPDS},
  author       = {Jin Wang and Jia Hu and Geyong Min and Albert Y. Zomaya and Nektarios Georgalas},
  doi          = {10.1109/TPDS.2020.3014896},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {242-253},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Fast adaptive task offloading in edge computing based on meta reinforcement learning},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Multi-GPU parallelization of the NAS multi-zone parallel
benchmarks. <em>TPDS</em>, <em>32</em>(1), 229–241. (<a
href="https://doi.org/10.1109/TPDS.2020.3015148">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPU-based computing systems have become a widely accepted solution for the high-performance-computing (HPC) domain. GPUs have shown highly competitive performance-per-watt ratios and can exploit an astonishing level of parallelism. However, exploiting the peak performance of such devices is a challenge, mainly due to the combination of two essential aspects of multi-GPU execution. On one hand, the workload should be distributed evenly among the GPUs. On the other hand, communications between GPU devices are costly and should be minimized. Therefore, a trade-of between work-distribution schemes and communication overheads will condition the overall performance of parallel applications run on multi-GPU systems. In this article we present a multi-GPU implementation of NAS Multi-Zone Parallel Benchmarks (which execution alternate communication and computational phases). We propose several work-distribution strategies that try to evenly distribute the workload among the GPUs. Our evaluations show that performance is highly sensitive to this distribution strategy, as the the communication phases of the applications are heavily affected by the work-distribution schemes applied in computational phases. In particular, we consider Static, Dynamic, and Guided schedulers to find a trade-off between both phases to maximize the overall performance. In addition, we compare those schedulers with an optimal scheduler computed offline using IBM CPLEX. On an evaluation environment composed of 2 x IBM Power9 8335-GTH and 4 x GPU NVIDIA V100 (Volta), our multi-GPU parallelization outperforms single-GPU execution from 1.48x to 1.86x (2 GPUs) and from 1.75x to 3.54x (4 GPUs). This article analyses these improvements in terms of the relationship between the computational and communication phases of the applications as the number of GPUs is increased. We prove that Guided schedulers perform at similar level as optimal schedulers.},
  archive      = {J_TPDS},
  author       = {Marc González and Enric Morancho},
  doi          = {10.1109/TPDS.2020.3015148},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {229-241},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Multi-GPU parallelization of the NAS multi-zone parallel benchmarks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Improving the performance of deduplication-based storage
cache via content-driven cache management methods. <em>TPDS</em>,
<em>32</em>(1), 214–228. (<a
href="https://doi.org/10.1109/TPDS.2020.3012704">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication, as a proven technology for effective data reduction in backup and archiving storage systems, is also showing promises in increasing the logical space capacity for storage caches by removing redundant data. However, our in-depth evaluation of the existing deduplication-aware caching algorithms reveals that they only work well when the cached block size is set to 4 KB. Unfortunately, modern storage systems often set the block size to be much larger than 4 KB, and in this scenario, the overall performance of these caching schemes drops below that of the conventional replacement algorithms without any deduplication. There are several reasons for this performance degradation. The first reason is the deduplication overhead, which is the time spent on generating the data fingerprints and their use to identify duplicate data. Such overhead offsets the benefits of deduplication. The second reason is the extremely low cache space utilization caused by read and write alignment. The third reason is that existing algorithms only exploit access locality to identify block replacement. There is a lost opportunity to effectively leverage the content usage patterns such as intensity of content redundancy and sharing in deduplication-based storage caches to further improve performance. We propose CDAC, a Content-driven Deduplication-Aware Cache, to address this problem. CDAC focuses on exploiting the content redundancy in blocks and intensity of content sharing among source addresses in cache management strategies. We have implemented CDAC based on LRU and ARC algorithms, called CDAC-LRU and CDAC-ARC respectively. Our extensive experimental results show that CDAC-LRU and CDAC-ARC outperform the state-of-the-art deduplication-aware caching algorithms, D-LRU, and D-ARC, by up to 23.83X in read cache hit ratio, with an average of 3.23X, and up to 53.3 percent in IOPS, with an average of 49.8 percent, under a real-world mixed workload when the cache size ranges from 20 to 50 percent of the workload size and the block size ranges from 4KB to 32 KB.},
  archive      = {J_TPDS},
  author       = {Yujuan Tan and Congcong Xu and Jing Xie and Zhichao Yan and Hong Jiang and Witawas Srisa-an and Xianzhang Chen and Duo Liu},
  doi          = {10.1109/TPDS.2020.3012704},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {214-228},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Improving the performance of deduplication-based storage cache via content-driven cache management methods},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). O3BNN-r: An out-of-order architecture for
high-performance and regularized BNN inference. <em>TPDS</em>,
<em>32</em>(1), 199–213. (<a
href="https://doi.org/10.1109/TPDS.2020.3013637">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binarized Neural Networks (BNN), which significantly reduce computational complexity and memory demand, have shown potential in cost- and power-restricted domains, such as IoT and smart edge-devices, where reaching certain accuracy bars is sufficient and real-time is highly desired. In this article, we demonstrate that the highly-condensed BNN model can be shrunk significantly by dynamically pruning irregular redundant edges. Based on two new observations on BNN-specific properties, an out-of-order (OoO) architecture, O3BNN-R, which can curtail edge evaluation in cases where the binary output of a neuron can be determined early at runtime during inference, is proposed. Similar to instruction level parallelism (ILP), fine-grained, irregular, and runtime pruning opportunities are traditionally presumed to be difficult to exploit. To further enhance the pruning opportunities, we conduct an algorithm/architecture co-design approach where we augment the loss function during the training stage with specialized regularization terms favoring edge pruning. We evaluate our design on an embedded FPGA using networks that include VGG-16, AlexNet for ImageNet, and a VGG-like network for Cifar-10. Results show that O3BNN-R without regularization can prune, on average, 30 percent of the operations, without any accuracy loss, bringing 2.2× inference-speedup, and on average 34× energy-efficiency improvement over state-of-the-art BNN implementations on FPGA/GPU/CPU. With regularization at training, the performance is further improved, on average, by 15 percent.},
  archive      = {J_TPDS},
  author       = {Tong Geng and Ang Li and Tianqi Wang and Chunshu Wu and Yanfei Li and Runbin Shi and Wei Wu and Martin Herbordt},
  doi          = {10.1109/TPDS.2020.3013637},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {199-213},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {O3BNN-R: An out-of-order architecture for high-performance and regularized BNN inference},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Rusty: Runtime interference-aware predictive monitoring
for modern multi-tenant systems. <em>TPDS</em>, <em>32</em>(1), 184–198.
(<a href="https://doi.org/10.1109/TPDS.2020.3013948">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern micro-service and container-based cloud-native applications have leveraged multi-tenancy as a first class system design concern. The increasing number of co-located services/workloads into server facilities stresses resource availability and system capability in an unconventional and unpredictable manner. To efficiently manage resources in such dynamic environments, run-time observability and forecasting are required to capture workload sensitivities under differing interference effects, according to applied co-location scenarios. While several research efforts have emerged on interference-aware performance modelling, they are usually applied at a very coarse-grained manner e.g., estimating the overall performance degradation of an application, thus failing to effectively quantify, predict or provide educated insights on the impact of continuous runtime interference on per-resource allocations. In this paper, we present Rusty, a predictive monitoring system that leverages the power of Long Short-Term Memory networks to enable fast and accurate runtime forecasting of key performance metrics and resource stresses of cloud-native applications under interference. We evaluate Rusty under a diverse set of interference scenarios for a plethora of representative cloud workloads, showing that Rusty i) achieves extremely high prediction accuracy, average R 2 value of 0.98, ii) enables very deep prediction horizons retaining high accuracy, e.g., R 2 of around 0.99 for a horizon of 1 sec ahead and around 0.94 for an horizon of 5 sec ahead, while iii) satisfying, at the same time, the strict latency constraints required to make Rusty practical for continuous predictive monitoring at runtime.},
  archive      = {J_TPDS},
  author       = {Dimosthenis Masouros and Sotirios Xydis and Dimitrios Soudris},
  doi          = {10.1109/TPDS.2020.3013948},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {184-198},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Rusty: Runtime interference-aware predictive monitoring for modern multi-tenant systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Blockchain at the edge: Performance of
resource-constrained IoT networks. <em>TPDS</em>, <em>32</em>(1),
174–183. (<a
href="https://doi.org/10.1109/TPDS.2020.3013892">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of IoT in various technological realms has resulted in the massive spurt of unsecured data. The use of complex security mechanisms for securing these data is highly restricted owing to the low-power and low-resource nature of most of the IoT devices, especially at the Edge. In this article, we propose to use blockchains for extending security to such IoT implementations. We deploy a Ethereum blockchain consisting of both regular and constrained devices connecting to the blockchain through wired and wireless heterogeneous networks. We additionally implement a secure and encrypted networked clock mechanism to synchronize the non-real-time IoT Edge nodes within the blockchain. Further, we experimentally study the feasibility of such a deployment and the bottlenecks associated with it by running necessary cryptographic operations for blockchains in IoT devices. We study the effects of network latency, increase in constrained blockchain nodes, data size, Ether, and blockchain node mobility during transaction and mining of data within our deployed blockchain. This study serves as a guideline for designing secured solutions for IoT implementations under various operating conditions such as those encountered for static IoT nodes and mobile IoT devices.},
  archive      = {J_TPDS},
  author       = {Sudip Misra and Anandarup Mukherjee and Arijit Roy and Nishant Saurabh and Yogachandran Rahulamathavan and Muttukrishnan Rajarajan},
  doi          = {10.1109/TPDS.2020.3013892},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {174-183},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Blockchain at the edge: Performance of resource-constrained IoT networks},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Feluca: A two-stage graph coloring algorithm with
color-centric paradigm on GPU. <em>TPDS</em>, <em>32</em>(1), 160–173.
(<a href="https://doi.org/10.1109/TPDS.2020.3014173">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are great challenges in performing graph coloring on GPU in general. First, the long-tail problem exists in the recursion algorithm because the conflict (i.e., different threads assign the adjacent nodes to the same color) becomes more likely to occur as the number of iterations increases. Second, it is hard to parallelize the sequential spread algorithm because the color allocation depends on the adjoining iteration. Third, the atomic operation is widely used on GPU to maintain the color list, which can greatly reduce the efficiency of GPU threads. In this article, we propose a two-stage high-performance graph coloring algorithm, called Feluca, aiming to address the above challenges. Feluca combines the recursion-based method with the sequential spread-based method. In the first stage, Feluca uses a recursive routine to color a majority of vertices in the graph. Then, it switches to the sequential spread method to color the remaining vertices in order to avoid the conflicts of the recursive algorithm. Moreover, the following techniques are proposed to further improve the graph coloring performance. i) A new method is proposed to eliminate the cycles in the graph; ii) a top-down scheme is developed to avoid the atomic operation originally required for color selection; and iii) a novel color-centric coloring paradigm is designed to improve the degree of parallelism for the sequential spread part. All these newly developed techniques, together with further GPU-specific optimizations such as coalesced memory access, comprise an efficient parallel graph coloring solution in Feluca. We have conducted extensive experiments on NVIDIA GPU. The results show that Feluca can achieve 1.19 - 8.39× speedup over the state-of-the-art algorithms.},
  archive      = {J_TPDS},
  author       = {Zhigao Zheng and Xuanhua Shi and Ligang He and Hai Jin and Shuo Wei and Hulin Dai and Xuan Peng},
  doi          = {10.1109/TPDS.2020.3014173},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {160-173},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Feluca: A two-stage graph coloring algorithm with color-centric paradigm on GPU},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Partitioning models for general medium-grain parallel
sparse tensor decomposition. <em>TPDS</em>, <em>32</em>(1), 147–159. (<a
href="https://doi.org/10.1109/TPDS.2020.3012624">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus of this article is efficient parallelization of the canonical polyadic decomposition algorithm utilizing the alternating least squares method for sparse tensors on distributed-memory architectures. We propose a hypergraph model for general medium-grain partitioning which does not enforce any topological constraint on the partitioning. The proposed model is based on splitting the given tensor into nonzero-disjoint component tensors. Then a mode-dependent coarse-grain hypergraph is constructed for each component tensor. A net amalgamation operation is proposed to form a composite medium-grain hypergraph from these mode-dependent coarse-grain hypergraphs to correctly encapsulate the minimization of the communication volume. We propose a heuristic which splits the nonzeros of dense slices to obtain sparse slices in component tensors. So we partially attain slice coherency at (sub)slice level since partitioning is performed on (sub)slices instead of individual nonzeros. We also utilize the well-known recursive-bipartitioning framework to improve the quality of the splitting heuristic. Finally, we propose a medium-grain tripartite graph model with the aim of a faster partitioning at the expense of increasing the total communication volume. Parallel experiments conducted on 10 real-world tensors on up to 1024 processors confirm the validity of the proposed hypergraph and graph models.},
  archive      = {J_TPDS},
  author       = {M. Ozan Karsavuran and Seher Acer and Cevdet Aykanat},
  doi          = {10.1109/TPDS.2020.3012624},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {147-159},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Partitioning models for general medium-grain parallel sparse tensor decomposition},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). CASpMV: A customized and accelerative SpMV framework for
the sunway TaihuLight. <em>TPDS</em>, <em>32</em>(1), 131–146. (<a
href="https://doi.org/10.1109/TPDS.2019.2907537">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sunway TaihuLight, equipped with 10 million cores, is currently the world&#39;s third fastest supercomputer. SpMV is one of core algorithms in many high-performance computing applications. This paper implements a fine-grained design for generic parallel SpMV based on the special Sunway architecture and finds three main performance limitations, i.e., storage limitation, load imbalance, and huge overhead of irregular memory accesses. To address these problems, this paper introduces a customized and accelerative framework for SpMV (CASpMV) on the Sunway. The CASpMV customizes an auto-tuning four-way partition scheme for SpMV based on the proposed statistical model, which describes the sparse matrix structure characteristics, to make it better fit in with the computing architecture and memory hierarchy of the Sunway. Moreover, the CASpMV provides an accelerative method and customized optimizations to avoid irregular memory accesses and further improve its performance on the Sunway. Our CASpMV achieves a performance improvement that ranges from 588.05 to 2118.62 percent over the generic parallel SpMV on a CG (which corresponds to an MPI process) of the Sunway on average and has good scalability on multiple CGs. The performance comparisons of the CASpMV with state-of-the-art methods on the Sunway indicate that the sparsity and irregularity of data structures have less impact on CASpMV.},
  archive      = {J_TPDS},
  author       = {Guoqing Xiao and Kenli Li and Yuedan Chen and Wangquan He and Albert Y. Zomaya and Tao Li},
  doi          = {10.1109/TPDS.2019.2907537},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {131-146},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {CASpMV: A customized and accelerative SpMV framework for the sunway TaihuLight},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Sova: A software-defined autonomic framework for virtual
network allocations. <em>TPDS</em>, <em>32</em>(1), 116–130. (<a
href="https://doi.org/10.1109/TPDS.2020.3012146">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of network virtualization, the workloads deployed on data center are dramatically changed to support diverse service-oriented applications, which are in general characterized by the time-bounded service response that in turn puts great burden on the data-center networks. Although there have been numerous techniques proposed to optimize the virtual network allocation in data center, the research on coordinating them in a flexible and effective way to autonomically adapt to the workloads for service time reduction is few and far between. To address these issues, in this article we propose Sova, an autonomic framework that can combine the virtual dynamic SR-IOV (DSR-IOV) and the virtual machine live migration (VLM) for virtual network allocations in data centers. DSR-IOV is a SR-IOV-based virtual network allocation technology, but its operation scope is very limited to a single physical machine, which could lead to the local hotspot issue in the course of computation and communication, likely increasing the service response time. In contrast, VLM is an often-used virtualization technique to optimize global network traffic via VM migration. Sova exploits the software-defined approach to combine these two technologies with reducing the service response time as a goal. To realize the autonomic coordination, the architecture of Sova is designed based on the MAPE-K loop in autonomic computing. With this design, Sova can adaptively optimize the network allocation between different services by coordinating DSR-IOV and VLM in autonomic way, depending on the resource usages of physical servers and the network characteristics of VMs. To this end, Sova needs to monitor the network traffic as well as the workload characteristics in the cluster, whereby the network properties are derived on the fly to direct the coordination between these two technologies. Our experiments show that Sova can exploit the advantages of both techniques to match and even beat the better performance of each individual technology by adapting to the VM workload changes.},
  archive      = {J_TPDS},
  author       = {Zhiyong Ye and Yang Wang and Shuibing He and Chengzhong Xu and Xian-He Sun},
  doi          = {10.1109/TPDS.2020.3012146},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {116-130},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Sova: A software-defined autonomic framework for virtual network allocations},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Elastic scheduling for microservice applications in
clouds. <em>TPDS</em>, <em>32</em>(1), 98–115. (<a
href="https://doi.org/10.1109/TPDS.2020.3011979">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservices are widely used for flexible software development. Recently, containers have become the preferred deployment technology for microservices because of fast start-up and low overhead. However, the container layer complicates task scheduling and auto-scaling in clouds. Existing algorithms do not adapt to the two-layer structure composed of virtual machines and containers, and they often ignore streaming workloads. To this end, this article proposes an Elastic Scheduling for Microservices (ESMS) that integrates task scheduling with auto-scaling. ESMS aims to minimize the cost of virtual machines while meeting deadline constraints. Specifically, we define the task scheduling problem of microservices as a cost optimization problem with deadline constraints and propose a statistics-based strategy to determine the configuration of containers under a streaming workload. Then, we propose an urgency-based workflow scheduling algorithm that assigns tasks and determines the type and quantity of instances for scale-up. Finally, we model the mapping of new containers to virtual machines as a variable-sized bin-packing problem and solve it to achieve integrated scaling of the virtual machines and containers. Via simulation-based experiments with well-known workflow applications, the ability of ESMS to improve the success ratio of meeting deadlines and reduce the cost is verified through comparison with existing algorithms.},
  archive      = {J_TPDS},
  author       = {Sheng Wang and Zhijun Ding and Changjun Jiang},
  doi          = {10.1109/TPDS.2020.3011979},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {98-115},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Elastic scheduling for microservice applications in clouds},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). K-athena: A performance portable structured grid finite
volume magnetohydrodynamics code. <em>TPDS</em>, <em>32</em>(1), 85–97.
(<a href="https://doi.org/10.1109/TPDS.2020.3010016">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale simulations are a key pillar of modern research and require ever-increasing computational resources. Different novel manycore architectures have emerged in recent years on the way towards the exascale era. Performance portability is required to prevent repeated non-trivial refactoring of a code for different architectures. We combine ATHENA++, an existing magnetohydrodynamics (MHD) CPU code, with KOKKOS, a performance portable on-node parallel programming paradigm, into K-ATHENA to allow efficient simulations on multiple architectures using a single codebase. We present profiling and scaling results for different platforms including Intel Skylake CPUs, Intel Xeon Phis, and NVIDIA GPUs. K-ATHENA achieves &gt; 10 8 cell-updates/s on a single V100 GPU for second-order double precision MHD calculations, and a speedup of 30 on up to 24 576 GPUs on Summit (compared to 172,032 CPU cores), reaching 1:94 × 10 12 total cell-updates/s at 76 percent parallel efficiency. Using a roofline analysis we demonstrate that the overall performance is currently limited by DRAM bandwidth and calculate a performance portability metric of 62.8 percent. Finally, we present the implementation strategies used and the challenges encountered in maximizing performance. This will provide other research groups with a straightforward approach to prepare their own codes for the exascale era. K-ATHENA is available at https://gitlab.com/pgrete/kathena.},
  archive      = {J_TPDS},
  author       = {Philipp Grete and Forrest W. Glines and Brian W. O&#39;Shea},
  doi          = {10.1109/TPDS.2020.3010016},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {85-97},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {K-athena: A performance portable structured grid finite volume magnetohydrodynamics code},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). GPU tensor cores for fast arithmetic reductions.
<em>TPDS</em>, <em>32</em>(1), 72–84. (<a
href="https://doi.org/10.1109/TPDS.2020.3011893">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a parallel algorithm for computing the arithmetic reduction of n numbers as a set of matrix-multiply accumulate (MMA) operations that are executed simultaneously by GPU tensor cores. The analysis, assuming tensors of size m x m, shows that the proposed algorithm has a parallel running time of T(n) = 5log m2 n and a speedup of S = 45log 2m 2 over a canonical parallel reduction. Experimental performance results on a Tesla V100 GPU show that the tensor-core based approach is energy efficient and runs up to ~ 3:2× and 2× faster than a standard GPU-based reduction and Nvidia&#39;s CUB library, respectively, while keeping the numerical error below 1 percent with respect to a double precision CPU reduction. The chained design of the algorithm allows a flexible configuration of GPU thread-blocks and the optimal values found through experimentation agree with the theoretical ones. The results obtained in this work show that GPU tensor cores are relevant not only for Deep Learning or Linear Algebra computations, but also for applications that require the acceleration of large summations.},
  archive      = {J_TPDS},
  author       = {Cristóbal A. Navarro and Roberto Carrasco and Ricardo J. Barrientos and Javier A. Riquelme and Raimundo Vega},
  doi          = {10.1109/TPDS.2020.3011893},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {72-84},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {GPU tensor cores for fast arithmetic reductions},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Self-balancing federated learning with global imbalanced
data in mobile systems. <em>TPDS</em>, <em>32</em>(1), 59–71. (<a
href="https://doi.org/10.1109/TPDS.2020.3009406">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a distributed deep learning method that enables multiple participants, such as mobile and IoT devices, to contribute a neural network while their private training data remains in local devices. This distributed approach is promising in the mobile systems where have a large corpus of decentralized data and require high privacy. However, unlike the common datasets, the data distribution of the mobile systems is imbalanced which will increase the bias of model. In this article, we demonstrate that the imbalanced distributed training data will cause an accuracy degradation of FL applications. To counter this problem, we build a self-balancing FL framework named Astraea, which alleviates the imbalances by 1) Z-score-based data augmentation, and 2) Mediator-based multi-client rescheduling. The proposed framework relieves global imbalance by adaptive data augmentation and downsampling, and for averaging the local imbalance, it creates the mediator to reschedule the training of clients based on Kullback-Leibler divergence (KLD) of their data distribution. Compared with FedAvg, the vanilla FL algorithm, Astraea shows +4.39 and +6.51 percent improvement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10 datasets, respectively. Meanwhile, the communication traffic of Astraea is reduced by 75 percent compared to FedAvg.},
  archive      = {J_TPDS},
  author       = {Moming Duan and Duo Liu and Xianzhang Chen and Renping Liu and Yujuan Tan and Liang Liang},
  doi          = {10.1109/TPDS.2020.3009406},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {59-71},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Self-balancing federated learning with global imbalanced data in mobile systems},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). PredCom: A predictive approach to collecting approximated
communication traces. <em>TPDS</em>, <em>32</em>(1), 45–58. (<a
href="https://doi.org/10.1109/TPDS.2020.3011121">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication traces collected from MPI applications are an important source of information for performance optimization as they can help analysts determine communication patterns and identify inefficiencies. However, their collection, especially at scale, is time consuming, since it usually requires running the complete target application on a large number of nodes. In this work, we present PredCom, a tool-chain to generate a predictive communication proxy based on information gathered from a few small scale runs, which allows us to extract approximate communication traces with an accuracy high enough for most analysis goals. For this, we combine LLVM passes on the original source code (to capture static program structure) with parameter prediction (to capture dynamic and scaling behavior). This approach drastically reduces the time needed for collecting the communication traces, even for traces on large numbers of MPI processes. We demonstrate that PredCom generates communication traces of various applications up to 1612x faster with an accuracy loss of 0.11 on average compared to the original large-scale traces, and we show that the generated traces can be used to optimize process placement.},
  archive      = {J_TPDS},
  author       = {Shinobu Miwa and Ignacio Laguna and Martin Schulz},
  doi          = {10.1109/TPDS.2020.3011121},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {45-58},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {PredCom: A predictive approach to collecting approximated communication traces},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021a). Cost-effective app data distribution in edge computing.
<em>TPDS</em>, <em>32</em>(1), 31–44. (<a
href="https://doi.org/10.1109/TPDS.2020.3010521">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing, as an extension of cloud computing, distributes computing and storage resources from centralized cloud to distributed edge servers, to power a variety of applications demanding low latency, e.g., IoT services, virtual reality, real-time navigation, etc. From an app vendor&#39;s perspective, app data needs to be transferred from the cloud to specific edge servers in an area to serve the app users in the area. However, according to the pay-as-you-go business model, distributing a large amount of data from the cloud to edge servers can be expensive. The optimal data distribution strategy must minimize the cost incurred, which includes two major components, the cost of data transmission between the cloud to edge servers and the cost of data transmission between edge servers. In the meantime, the delay constraint must be fulfilled - the data distribution must not take too long. In this article, we make the first attempt to formulate this Edge Data Distribution (EDD) problem as a constrained optimization problem from the app vendor&#39;s perspective and prove its NP-hardness. We propose an optimal approach named EDD-IP to solve this problem exactly with the Integer Programming technique. Then, we propose an O(k)-approximation algorithm named EDD-A for finding approximate solutions to largescale EDD problems efficiently. EDD-IP and EDD-A are evaluated on a real-world dataset and the results demonstrate that they significantly outperform three representative approaches.},
  archive      = {J_TPDS},
  author       = {Xiaoyu Xia and Feifei Chen and Qiang He and John C. Grundy and Mohamed Abdelrazek and Hai Jin},
  doi          = {10.1109/TPDS.2020.3010521},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {31-44},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Cost-effective app data distribution in edge computing},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Design and evaluation of a risk-aware failure
identification scheme for improved RAS in erasure-coded data centers.
<em>TPDS</em>, <em>32</em>(1), 16–30. (<a
href="https://doi.org/10.1109/TPDS.2020.3010048">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data reliability and availability, and serviceability (RAS) of erasure-coded data centers are highly affected by data repair induced by node failures. In a traditional failure identification scheme, all chunks share the same identification time threshold, thus losing opportunities to further improve the RAS. To solve this problem, we propose RAFI, a novel risk-aware failure identification scheme. In RAFI, chunk failures in stripes experiencing different numbers of failed chunks are identified using different time thresholds. For those chunks in a high-risk stripe, a shorter identification time is adopted, thus improving the overall data reliability and availability. For those chunks in a low-risk stripe, a longer identification time is adopted, thus reducing the repair network traffic. Therefore, RAS can be improved simultaneously. We also propose three optimization techniques to reduce the additional overhead that RAFI imposes on management nodes and to ensure that RAFI can work properly under large-scale clusters. We use simulation, emulation, and prototyping implementation to evaluate RAFI from multiple aspects. Simulation and prototype results prove the effectiveness and correctness of RAFI, and the performance improvement of the optimization techniques on RAFI is demonstrated by running the emulator.},
  archive      = {J_TPDS},
  author       = {Weichen Huang and Juntao Fang and Shenggang Wan and Changsheng Xie and Xubin He},
  doi          = {10.1109/TPDS.2020.3010048},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {16-30},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Design and evaluation of a risk-aware failure identification scheme for improved RAS in erasure-coded data centers},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
<p>(2021). Learning-driven interference-aware workload
parallelization for streaming applications in heterogeneous cluster.
<em>TPDS</em>, <em>32</em>(1), 1–15. (<a
href="https://doi.org/10.1109/TPDS.2020.3008725">www</a>)</p>
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, with the rapid development of CPU-GPU heterogeneous computing, the issue of task scheduling in the heterogeneous cluster has attracted a great deal of attention. This problem becomes more challenging with the need for efficient co-execution of tasks on the GPUs. However, the uncertainty of heterogeneous cluster and the interference caused by resource contention among co-executing tasks can lead to the unbalanced use of computing resource and further cause the degradation in performance of computing platform. In this article, we propose a two-stage task scheduling approach for streaming applications based on deep reinforcement learning and neural collaborative filtering, which considers fine-grained task division and task interference on the GPU. Specifically, the Learning-Driven Workload Parallelization (LDWP) method selects an appropriate execution node for the mutually independent tasks. By using the deep Q-network, the cluster-level scheduling model is online learned to perform the current optimal scheduling actions according to the runtime status of cluster environments and characteristics of tasks. The Interference-Aware Workload Parallelization (IAWP) method assigns subtasks with dependencies to the appropriate computing units, taking into account the interference of subtasks on the GPU by using neural collaborative filtering. For making the learning of neural network more efficient, we use pre-training in the two-stage scheduler. Besides, we use transfer learning technology to efficiently rebuild task scheduling model referring to the existing model. We evaluate our learning-driven and interference-aware task scheduling approach on a prototype platform with other widely used methods. The experimental results show that the proposed strategy can averagely improve the throughout for distributed computing system by 26.9 percent and improve the GPU resource utilization by around 14.7 percent.},
  archive      = {J_TPDS},
  author       = {Haitao Zhang and Xin Geng and Huadong Ma},
  doi          = {10.1109/TPDS.2020.3008725},
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  number       = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  title        = {Learning-driven interference-aware workload parallelization for streaming applications in heterogeneous cluster},
  volume       = {32},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
