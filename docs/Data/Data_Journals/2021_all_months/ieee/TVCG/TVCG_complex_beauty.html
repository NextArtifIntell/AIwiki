<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---383">TVCG - 383</h2>
<ul>
<li><details>
<summary>
(2021). Towards automatic skeleton extraction with skeleton
grafting. <em>TVCG</em>, <em>27</em>(12), 4520–4532. (<a
href="https://doi.org/10.1109/TVCG.2020.3003994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel approach to generate visually promising skeletons automatically without any manual tuning. In practice, it is challenging to extract promising skeletons directly using existing approaches. This is because they either cannot fully preserve shape features, or require manual intervention, such as boundary smoothing and skeleton pruning, to justify the eye-level view assumption. We propose an approach here that generates backbone and dense skeletons by shape input, and then extends the backbone branches via skeleton grafting from the dense skeleton to ensure a well-integrated output. Based on our evaluation, the generated skeletons best depict the shapes at levels that are similar to human perception. To evaluate and fully express the properties of the extracted skeletons, we introduce two potential functions within the high-order matching protocol to improve the accuracy of skeleton-based matching. These two functions fuse the similarities between skeleton graphs and geometrical relations characterized by multiple skeleton endpoints. Experiments on three high-order matching protocols show that the proposed potential functions can effectively reduce the number of incorrect matches.},
  archive      = {J_TVCG},
  author       = {Cong Yang and Bipin Indurkhya and John See and Marcin Grzegorzek},
  doi          = {10.1109/TVCG.2020.3003994},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4520-4532},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards automatic skeleton extraction with skeleton grafting},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tilt map: Interactive transitions between choropleth map,
prism map and bar chart in immersive environments. <em>TVCG</em>,
<em>27</em>(12), 4507–4519. (<a
href="https://doi.org/10.1109/TVCG.2020.3004137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Tilt Map , a novel interaction technique for intuitively transitioning between 2D and 3D map visualisations in immersive environments. Our focus is visualising data associated with areal features on maps, for example, population density by state. Tilt Map transitions from 2D choropleth maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our article includes two user studies. The first study compares subjects’ task performance interpreting population density data using 2D choropleth maps and 3D prism maps in virtual reality (VR). We observed greater task accuracy with prism maps, but faster response times with choropleth maps. The complementarity of these views inspired our hybrid Tilt Map design. Our second study compares Tilt Map to: a side-by-side arrangement of the various views; and interactive toggling between views. The results indicate benefits for Tilt Map in user preference; and accuracy (versus side-by-side) and time (versus toggle).},
  archive      = {J_TVCG},
  author       = {Yalong Yang and Tim Dwyer and Kim Marriott and Bernhard Jenny and Sarah Goodwin},
  doi          = {10.1109/TVCG.2020.3004137},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4507-4519},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tilt map: Interactive transitions between choropleth map, prism map and bar chart in immersive environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sketch-based fast and accurate querying of time series using
parameter-sharing LSTM networks. <em>TVCG</em>, <em>27</em>(12),
4495–4506. (<a href="https://doi.org/10.1109/TVCG.2020.3002950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketching is one common approach to query time series data for patterns of interest. Most existing solutions for matching the data with the interaction are based on an empirically modeled similarity function between the user’s sketch and the time series data with limited efficiency and accuracy. In this article, we introduce a machine learning based solution for fast and accurate querying of time series data based on a swift sketching interaction. We build on existing LSTM technology (long short-term memory) to encode both the sketch and the time series data in a network with shared parameters. We use data from a user study to let the network learn a proper similarity function. We focus our approach on perceived similarities and achieve that the learned model also includes a user-side aspect. To the best of our knowledge, this is the first data-driven solution for querying time series data in visual analytics. Besides evaluating the accuracy and efficiency directly in a quantitative way, we also compare our solution to the recently published Qetch algorithm as well as the commonly used dynamic time warping (DTW) algorithm.},
  archive      = {J_TVCG},
  author       = {Chaoran Fan and Krešimir Matković and Helwig Hauser},
  doi          = {10.1109/TVCG.2020.3002950},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4495-4506},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch-based fast and accurate querying of time series using parameter-sharing LSTM networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulating multi-scale, granular materials and their
transitions with a hybrid euler-lagrange solver. <em>TVCG</em>,
<em>27</em>(12), 4483–4494. (<a
href="https://doi.org/10.1109/TVCG.2021.3107597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale granular materials, such as powdered materials and mudslides, are pretty common in nature. Modeling such materials and their phase transitions remains challenging since this task involves the delicate representations of various ranges of particles with multiple scales that cause their property variations among liquid, granular solid (i.e., particles), and smoke-like materials. To effectively animate the complicated yet intriguing natural phenomena involving multi-scale granular materials and their phase transitions in graphics with high fidelity, this article advocates a hybrid Euler-Lagrange solver to handle the behaviors of involved discontinuous fluid-like materials faithfully. At the algorithmic level, we present a unified framework that tightly couples the affine particle-in-cell (APIC) solver with density field to achieve the transformation spanning across granular particles, dust cloud, powders, and their natural mixtures. For example, a part of the granular particles could be transformed into dust cloud while interacting with air and being represented by density field. Meanwhile, the velocity decrease of the involved materials could also result in the transit from the density-field-driven dust to powder particles. Besides, to further enhance our modeling and simulation power to broaden the range of multi-scale materials, we introduce a moisture property for granular particles to control the transitions between particles and viscous liquid. At the geometric level, we devise an additional surface-tracking procedure to simulate the viscous liquid phase. We can arrive at delicate viscous behaviors by controlling the corresponding yield conditions. Through various experiments with the different scenes design being conducted in our unified framework, we can validate the mixed multi-scale materials’ mutual transformation processes. Our unified framework furnished with a hybrid solver can significantly enhance the modeling flexibility and the animation potential of the particle-grid hybrid materials in graphics.},
  archive      = {J_TVCG},
  author       = {Yang Gao and Shuai Li and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2021.3107597},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4483-4494},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simulating multi-scale, granular materials and their transitions with a hybrid euler-lagrange solver},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selective guidance normal filter for geometric texture
removal. <em>TVCG</em>, <em>27</em>(12), 4469–4482. (<a
href="https://doi.org/10.1109/TVCG.2020.3005424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is typically a trade-off between removing the detailed appearance (i.e., geometric textures ) and preserving the intrinsic properties (i.e., geometric structures ) of 3D surfaces. The conventional use of mesh vertex/facet-centered patches in many filters leads to side-effects including remnant textures, improperly filtered structures, and distorted shapes. We propose a selective guidance normal filter (SGNF) which adapts the Relative Total Variation (RTV) to a maximal/minimal scheme (mmRTV). The mmRTV measures the geometric flatness of surface patches, which helps in finding adaptive patches whose boundaries are aligned with the facet being processed. The adaptive patches provide selective guidance normals, which are subsequently used for normal filtering. The filtering smooths out the geometric textures by using guidance normals estimated from patches with maximal RTV (the least flatness), and preserves the geometric structures by using normals estimated from patches with minimal RTV (the most flatness). This simple yet effective modification of the RTV makes our SGNF specialized rather than trade off between texture removal and structure preservation, which is distinct from existing mesh filters. Experiments show that our approach is visually and numerically comparable to the state-of-the-art mesh filters, in most cases. In addition, the mmRTV is generally applicable to bas-relief modeling and image texture removal.},
  archive      = {J_TVCG},
  author       = {Mingqiang Wei and Yidan Feng and Honghua Chen},
  doi          = {10.1109/TVCG.2020.3005424},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4469-4482},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Selective guidance normal filter for geometric texture removal},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PyramidTags: Context-, time- and word order-aware tag maps
to explore large document collections. <em>TVCG</em>, <em>27</em>(12),
4455–4468. (<a href="https://doi.org/10.1109/TVCG.2020.3010095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult to explore large text collections if no or little information is available on the contained documents. Hence, starting analytic tasks on such corpora is challenging for many stakeholders from various domains. As a remedy, recent visualization research suggests to use visual spatializations of representative text documents or tags to explore text collections. With PyramidTags, we introduce a novel approach for summarizing large text collections visually. In contrast to previous work, PyramidTags in particular aims at creating an improved representation that incorporates both temporal evolution and semantic relationship of visualized tags within the summarized document collection. As a result, it equips analysts with a visual starting point for interactive exploration to not only get an overview of the main terms and phrases of the corpus, but also to grasp important ideas and stories. Analysts can hover and select multiple tags to explore relationships and retrieve the most relevant documents. In this work, we apply PyramidTags to hundreds of thousands of web-crawled news reports. Our benchmarks suggest that PyramidTags creates time- and context-aware layouts, while preserving the inherent word order of important pairs.},
  archive      = {J_TVCG},
  author       = {Johannes Knittel and Steffen Koch and Thomas Ertl},
  doi          = {10.1109/TVCG.2020.3010095},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4455-4468},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PyramidTags: Context-, time- and word order-aware tag maps to explore large document collections},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic data-driven sampling via multi-criteria
importance analysis. <em>TVCG</em>, <em>27</em>(12), 4439–4454. (<a
href="https://doi.org/10.1109/TVCG.2020.3006426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although supercomputers are becoming increasingly powerful, their components have thus far not scaled proportionately. Compute power is growing enormously and is enabling finely resolved simulations that produce never-before-seen features. However, I/O capabilities lag by orders of magnitude, which means only a fraction of the simulation data can be stored for post hoc analysis. Prespecified plans for saving features and quantities of interest do not work for features that have not been seen before. Data-driven intelligent sampling schemes are needed to detect and save important parts of the simulation while it is running. Here, we propose a novel sampling scheme that reduces the size of the data by orders-of-magnitude while still preserving important regions. The approach we develop selects points with unusual data values and high gradients. We demonstrate that our approach outperforms traditional sampling schemes on a number of tasks.},
  archive      = {J_TVCG},
  author       = {Ayan Biswas and Soumya Dutta and Earl Lawrence and John Patchett and Jon C. Calhoun and James Ahrens},
  doi          = {10.1109/TVCG.2020.3006426},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4439-4454},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Probabilistic data-driven sampling via multi-criteria importance analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OctoPocus in VR: Using a dynamic guide for 3D mid-air
gestures in virtual reality. <em>TVCG</em>, <em>27</em>(12), 4425–4438.
(<a href="https://doi.org/10.1109/TVCG.2021.3101854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bau and Mackays OctoPocus dynamic guide helps novices learn, execute, and remember 2D surface gestures. We adapt OctoPocus to 3D mid-air gestures in Virtual Reality (VR) using an optimization-based recognizer, and by introducing an optional exploration mode to help visualize the spatial complexity of guides in a 3D gesture set. A replication of the original experiment protocol is used to compare OctoPocus in VR with a VR implementation of a crib-sheet. Results show that despite requiring 0.9s more reaction time than crib-sheet, OctoPocus enables participants to execute gestures 1.8s faster with 13.8 percent more accuracy during training, while remembering a comparable number of gestures. Subjective ratings support these results, 75 percent of participants found OctoPocus easier to learn and 83 percent found it more accurate. We contribute an implementation and empirical evidence demonstrating that an adaptation of the OctoPocus guide to VR is feasible and beneficial.},
  archive      = {J_TVCG},
  author       = {Katherine Fennedy and Jeremy Hartmann and Quentin Roy and Simon Tangi Perrault and Daniel Vogel},
  doi          = {10.1109/TVCG.2021.3101854},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4425-4438},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OctoPocus in VR: Using a dynamic guide for 3D mid-air gestures in virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion planning for convertible indoor scene layout design.
<em>TVCG</em>, <em>27</em>(12), 4413–4424. (<a
href="https://doi.org/10.1109/TVCG.2020.3005680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system for designing indoor scenes with convertible furniture layouts. Such layouts are useful for scenarios where an indoor scene has multiple purposes and requires layout conversion, such as merging multiple small furniture objects into a larger one or changing the locus of the furniture. We aim at planning the motion for the convertible layouts of a scene with the most efficient conversion process. To achieve this, our system first establishes object-level correspondences between the layout of a given source and that of a reference to compute a target layout, where the objects are re-arranged in the source layout with respect to the reference layout. After that, our system initializes the movement paths of objects between the source and target layouts based on various mechanical constraints. A joint space-time optimization is then performed to program a control stream of object translations, rotations, and stops, under which the movements of all objects are efficient and the potential object collisions are avoided. We demonstrate the effectiveness of our system through various design examples of multi-purpose, indoor scenes with convertible layouts.},
  archive      = {J_TVCG},
  author       = {Guoming Xiong and Qiang Fu and Hongbo Fu and Bin Zhou and Guoliang Luo and Zhigang Deng},
  doi          = {10.1109/TVCG.2020.3005680},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4413-4424},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Motion planning for convertible indoor scene layout design},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geono-cluster: Interactive visual cluster analysis for
biologists. <em>TVCG</em>, <em>27</em>(12), 4401–4412. (<a
href="https://doi.org/10.1109/TVCG.2020.3002166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biologists often perform clustering analysis to derive meaningful patterns, relationships, and structures from data instances and attributes. Though clustering plays a pivotal role in biologists’ data exploration, it takes non-trivial efforts for biologists to find the best grouping in their data using existing tools. Visual cluster analysis is currently performed either programmatically or through menus and dialogues in many tools, which require parameter adjustments over several steps of trial-and-error. In this article, we introduce Geono-Cluster, a novel visual analysis tool designed to support cluster analysis for biologists who do not have formal data science training. Geono-Cluster enables biologists to apply their domain expertise into clustering results by visually demonstrating how their expected clustering outputs should look like with a small sample of data instances. The system then predicts users’ intentions and generates potential clustering results. Our study follows the design study protocol to derive biologists’ tasks and requirements, design the system, and evaluate the system with experts on their own dataset. Results of our study with six biologists provide initial evidence that Geono-Cluster enables biologists to create, refine, and evaluate clustering results to effectively analyze their data and gain data-driven insights. At the end, we discuss lessons learned and implications of our study.},
  archive      = {J_TVCG},
  author       = {Subhajit Das and Bahador Saket and Bum Chul Kwon and Alex Endert},
  doi          = {10.1109/TVCG.2020.3002166},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4401-4412},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Geono-cluster: Interactive visual cluster analysis for biologists},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmenting perceived softness of haptic proxy objects
through transient vibration and visuo-haptic illusion in virtual
reality. <em>TVCG</em>, <em>27</em>(12), 4387–4400. (<a
href="https://doi.org/10.1109/TVCG.2020.3002245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the effects of active transient vibration and visuo-haptic illusion to augment the perceived softness of haptic proxy objects. We introduce a system combining active transient vibration at the fingertip with visuo-haptic illusions. In our hand-held device, a voice coil actuator transmits active transient vibrations to the index fingertip, while a force sensor measures the force applied on passive proxy objects to create visuo-haptic illusions in virtual reality. We conducted three user studies to understand both the vibrotactile effect and its combined effect with visuo-haptic illusions. A preliminary study confirmed that active transient vibrations can intuitively alter the perceived softness of a proxy object. Our first study demonstrated that those same active transient vibrations can generate different perceptions of softness depending on the material of the proxy object used. In our second study, we evaluated the combination of active transient vibration and visuo-haptic illusion, and found that both significantly influence perceived softness, with with the visuo-haptic effect being dominant. Our third study further investigated the vibrotactile effect while controlling for the visuo-haptic illusion. The combination of these two methods allows users to effectively perceive various levels of softness when interacting with haptic proxy objects.},
  archive      = {J_TVCG},
  author       = {Inrak Choi and Yiwei Zhao and Eric J. Gonzalez and Sean Follmer},
  doi          = {10.1109/TVCG.2020.3002245},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4387-4400},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmenting perceived softness of haptic proxy objects through transient vibration and visuo-haptic illusion in virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved augmented-reality framework for differential
rendering beyond the lambertian-world assumption. <em>TVCG</em>,
<em>27</em>(12), 4374–4386. (<a
href="https://doi.org/10.1109/TVCG.2020.3004195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In augmented reality, it is important to achieve visual consistency between inserted virtual objects and the real scene. As specular and transparent objects can produce caustics, which affect the appearance of inserted virtual objects, we herein propose a framework for differential rendering beyond the Lambertian-world assumption. Our key idea is to jointly optimize illumination and parameters of specular and transparent objects. To estimate the parameters of transparent objects efficiently, the psychophysical scaling method is introduced while considering visual characteristics of the human eye to obtain the step size for estimating the refractive index. We verify our technique on multiple real scenes, and the experimental results show that the fusion effects are visually consistent.},
  archive      = {J_TVCG},
  author       = {Aijia Zhang and Yan Zhao and Shigang Wang},
  doi          = {10.1109/TVCG.2020.3004195},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4374-4386},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An improved augmented-reality framework for differential rendering beyond the lambertian-world assumption},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A virtual reality memory palace variant aids knowledge
retrieval from scholarly articles. <em>TVCG</em>, <em>27</em>(12),
4359–4373. (<a href="https://doi.org/10.1109/TVCG.2020.3009003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present exploratory research of virtual reality techniques and mnemonic devices to assist in retrieving knowledge from scholarly articles. We used abstracts of scientific publications to represent knowledge in scholarly articles; participants were asked to read, remember, and retrieve knowledge from a set of abstracts. We conducted an experiment to compare participants’ recall and recognition performance in three different conditions: a control condition without a pre-specified strategy to test baseline individual memory ability, a condition using an image-based variant of a mnemonic called a “memory palace,” and a condition using a virtual reality-based variant of a memory palace. Our analyses show that using a virtual reality-based memory palace variant greatly increased the amount of knowledge retrieved and retained over the baseline, and it shows a moderate improvement over the other image-based memory palace variant. Anecdotal feedback from participants suggested that personalizing a memory palace variant would be appreciated. Our results support the value of virtual reality for some high-level cognitive tasks and help improve future applications of virtual reality and visualization.},
  archive      = {J_TVCG},
  author       = {Fumeng Yang and Jing Qian and Johannes Novotny and David Badre and Cullen D. Jackson and David H. Laidlaw},
  doi          = {10.1109/TVCG.2020.3009003},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4359-4373},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A virtual reality memory palace variant aids knowledge retrieval from scholarly articles},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A discrete probabilistic approach to dense flow
visualization. <em>TVCG</em>, <em>27</em>(12), 4347–4358. (<a
href="https://doi.org/10.1109/TVCG.2020.3006995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense flow visualization is a popular visualization paradigm. Traditionally, the various models and methods in this area use a continuous formulation, resting upon the solid foundation of functional analysis. In this work, we examine a discrete formulation of dense flow visualization. From probability theory, we derive a similarity matrix that measures the similarity between different points in the flow domain, leading to the discovery of a whole new class of visualization models. Using this matrix, we propose a novel visualization approach consisting of the computation of spectral embeddings, i.e., characteristic domain maps, defined by particle mixture probabilities. These embeddings are scalar fields that give insight into the mixing processes of the flow on different scales. The approach of spectral embeddings is already well studied in image segmentation, and we see that spectral embeddings are connected to Fourier expansions and frequencies. We showcase the utility of our method using different 2D and 3D flows.},
  archive      = {J_TVCG},
  author       = {Daniel Preuß and Tino Weinkauf and Jens Krüger},
  doi          = {10.1109/TVCG.2020.3006995},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4347-4358},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A discrete probabilistic approach to dense flow visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Editor’s note. <em>TVCG</em>, <em>27</em>(12), 4342–4346.
(<a href="https://doi.org/10.1109/TVCG.2021.3112912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2021.3112912},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {12},
  pages        = {4342-4346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editor&#39;s note},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual extensions improve perception-based instrument
alignment using optical see-through devices. <em>TVCG</em>,
<em>27</em>(11), 4332–4341. (<a
href="https://doi.org/10.1109/TVCG.2021.3106506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrument alignment is a common task in various surgical interventions using navigation. The goal of the task is to position and orient an instrument as it has been planned preoperatively. To this end, surgeons rely on patient-specific data visualized on screens alongside preplanned trajectories. The purpose of this manuscript is to investigate the effect of instrument visualization/non visualization on alignment tasks, and to compare it with virtual extensions approach which augments the realistic representation of the instrument with simple 3D objects. 18 volunteers performed six alignment tasks under each of the following conditions: no visualization on the instrument; realistic visualization of the instrument; realistic visualization extended with virtual elements (Virtual extensions). The first condition represents an egocentric-based alignment while the two other conditions additionally make use of exocentric depth estimation to perform the alignment. The device used was a see-through device (Microsoft HoloLens 2). The positions of the head and the instrument were acquired during the experiment. Additionally, the users were asked to fill NASA-TLX and SUS forms for each condition. The results show that instrument visualization is essential for a good alignment using see-through devices. Moreover, virtual extensions helped achieve the best performance compared to the other conditions with medians of 2 mm and 2° positional and angular error respectively. Furthermore, the virtual extensions decreased the average head velocity while similarly reducing the frustration levels. Therefore, making use of virtual extensions could facilitate alignment tasks in augmented and virtual reality (AR/VR) environments, specifically in AR navigated surgical procedures when using optical see-through devices.},
  archive      = {J_TVCG},
  author       = {Mohamed Benmahdjoub and Wiro J. Niessen and Eppo B. Wolvius and Theo van Walsum},
  doi          = {10.1109/TVCG.2021.3106506},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4332-4341},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual extensions improve perception-based instrument alignment using optical see-through devices},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual animals as diegetic attention guidance mechanisms in
360-degree experiences. <em>TVCG</em>, <em>27</em>(11), 4321–4331. (<a
href="https://doi.org/10.1109/TVCG.2021.3106490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360-degree experiences such as cinematic virtual reality and 360-degree videos are becoming increasingly popular. In most examples, viewers can freely explore the content by changing their orientation. However, in some cases, this increased freedom may lead to viewers missing important events within such experiences. Thus, a recent research thrust has focused on studying mechanisms for guiding viewers&#39; attention while maintaining their sense of presence and fostering a positive user experience. One approach is the utilization of diegetic mechanisms, characterized by an internal consistency with respect to the narrative and the environment, for attention guidance. While such mechanisms are highly attractive, their uses and potential implementations are still not well understood. Additionally, acknowledging the user in 360-degree experiences has been linked to a higher sense of presence and connection. However, less is known when acknowledging behaviors are carried out by attention guiding mechanisms. To close these gaps, we conducted a within-subjects user study with five conditions of no guide and virtual arrows, birds, dogs, and dogs that acknowledge the user and the environment. Through our mixed-methods analysis, we found that the diegetic virtual animals resulted in a more positive user experience, all of which were at least as effective as the non-diegetic arrow in guiding users towards target events. The acknowledging dog received the most positive responses from our participants in terms of preference and user experience and significantly improved their sense of presence compared to the non-diegetic arrow. Lastly, three themes emerged from a qualitative analysis of our participants&#39; feedback, indicating the importance of the guide&#39;s blending in, its acknowledging behavior, and participants&#39; positive associations as the main factors for our participants&#39; preferences.},
  archive      = {J_TVCG},
  author       = {Nahal Norouzi and Gerd Bruder and Austin Erickson and Kangsoo Kim and Jeremy Bailenson and Pamela Wisniewski and Charlie Hughes and Greg Welch},
  doi          = {10.1109/TVCG.2021.3106490},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4321-4331},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual animals as diegetic attention guidance mechanisms in 360-degree experiences},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using multi-level precueing to improve performance in
path-following tasks in virtual reality. <em>TVCG</em>, <em>27</em>(11),
4311–4320. (<a href="https://doi.org/10.1109/TVCG.2021.3106476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Work on VR and AR task interaction and visualization paradigms has typically focused on providing information about the current step (a cue) immediately before or during its performance. Some research has also shown benefits to simultaneously providing information about the next step (a precue). We explore whether it would be possible to improve efficiency by precueing information about multiple upcoming steps before completing the current step. To accomplish this, we developed a remote VR user study comparing task completion time and subjective metrics for different levels and styles of precueing in a path-following task. Our visualizations vary the precueing level (number of steps precued in advance) and style (whether the path to a target is communicated through a line to the target, and whether the place of a target is communicated through graphics at the target). Participants in our study performed best when given two to three precues for visualizations using lines to show the path to targets. However, performance degraded when four precues were used. On the other hand, participants performed best with only one precue for visualizations without lines, showing only the places of targets, and performance degraded when a second precue was given. In addition, participants performed better using visualizations with lines than ones without lines.},
  archive      = {J_TVCG},
  author       = {Jen-Shuo Liu and Carmine Elvezio and Barbara Tversky and Steven Feiner},
  doi          = {10.1109/TVCG.2021.3106476},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4311-4320},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Using multi-level precueing to improve performance in path-following tasks in virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding, modeling and simulating unintended positional
drift during repetitive steering navigation tasks in virtual reality.
<em>TVCG</em>, <em>27</em>(11), 4300–4310. (<a
href="https://doi.org/10.1109/TVCG.2021.3106504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual steering techniques enable users to navigate in larger Virtual Environments (VEs) than the physical workspace available. Even though these techniques do not require physical movement of the users (e.g. using a joystick and the head orientation to steer towards a virtual direction), recent work observed that users might unintentionally move in the physical workspace while navigating, resulting in Unintended Positional Drift (UPD). This phenomenon can be a safety issue since users may unintentionally reach the physical boundaries of the workspace while using a steering technique. In this context, as a necessary first step to improve the design of navigation techniques minimizing the UPD, this paper aims at analyzing and modeling the UPD during a virtual navigation task. In particular, we characterize and analyze the UPD for a dataset containing the positions and orientations of eighteen users performing a virtual slalom task using virtual steering techniques. Participants wore a head-mounted display and had to follow three different sinusoidal-like trajectories (with low, medium and high curvature) using a torso-steering navigation technique. We analyzed the performed motions and proposed two UPD models: the first based on a linear regression analysis and the second based on a Gaussian Mixture Model (GMM) analysis. Then, we assessed both models through a simulation-based evaluation where we reproduced the same navigation task using virtual agents. Our results indicate the feasibility of using simulation-based evaluations to study UPD. The paper concludes with a discussion of potential applications of the results in order to gain a better understanding of UPD during steering and therefore improve the design of navigation techniques by compensating for UPD.},
  archive      = {J_TVCG},
  author       = {Hugo Brument and Gerd Bruder and Maud Marchal and Anne Hélène Olivier and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2021.3106504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4300-4310},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Understanding, modeling and simulating unintended positional drift during repetitive steering navigation tasks in virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The cognitive loads and usability of target-based and
steering-based travel techniques. <em>TVCG</em>, <em>27</em>(11),
4289–4299. (<a href="https://doi.org/10.1109/TVCG.2021.3106507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target and steering-based techniques are two common approaches to travel in consumer VR applications. In this paper, we present two within-subject studies that employ a prior dual-task methodology to evaluate and compare the cognitive loads, travel performances, and simulator sickness of three common target-based travel techniques and three common steering-based travel techniques. We also present visual meta-analyses comparing our results to prior results using the same dual-task methodology. Based on our results and meta-analyses, we present several design suggestions for travel techniques based on various aspects of user experiences.},
  archive      = {J_TVCG},
  author       = {Chengyuan Lai and Xinyu Hu and Afham Ahmed Aiyaz and Ann Segismundo and Ananya Phadke and Ryan P. McMahan},
  doi          = {10.1109/TVCG.2021.3106507},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4289-4299},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The cognitive loads and usability of target-based and steering-based travel techniques},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Redirected walking using continuous curvature manipulation.
<em>TVCG</em>, <em>27</em>(11), 4278–4288. (<a
href="https://doi.org/10.1109/TVCG.2021.3106501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel redirected walking (RDW) technique that applies dynamic bending and curvature gains so that users perceive less discomfort than existing techniques that apply constant gains. Humans are less likely to notice continuous changes than those that are sudden. Therefore, instead of applying constant bending or curvature gains to users, we propose a dynamic method that continuously changes the gains. We conduct experiments to investigate the effect of dynamic gains in bending and curvature manipulation with regards to discomfort. The experimental results show that the proposed method significantly suppresses discomfort by up to 16 and 9\% for bending and curvature manipulations, respectively.},
  archive      = {J_TVCG},
  author       = {Hiroaki Sakono and Keigo Matsumoto and Takuji Narumi and Hideaki Kuzuoka},
  doi          = {10.1109/TVCG.2021.3106501},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4278-4288},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Redirected walking using continuous curvature manipulation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Redirected walking in static and dynamic scenes using
visibility polygons. <em>TVCG</em>, <em>27</em>(11), 4267–4277. (<a
href="https://doi.org/10.1109/TVCG.2021.3106432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach for redirected walking in static and dynamic scenes that uses techniques from robot motion planning to compute the redirection gains that steer the user on collision-free paths in the physical space. Our first contribution is a mathematical framework for redirected walking using concepts from motion planning and configuration spaces. This framework highlights various geometric and perceptual constraints that tend to make collision-free redirected walking difficult. We use our framework to propose an efficient solution to the redirection problem that uses the notion of visibility polygons to compute the free spaces in the physical environment and the virtual environment. The visibility polygon provides a concise representation of the entire space that is visible, and therefore walkable, to the user from their position within an environment. Using this representation of walkable space, we apply redirected walking to steer the user to regions of the visibility polygon in the physical environment that closely match the region that the user occupies in the visibility polygon in the virtual environment. We show that our algorithm is able to steer the user along paths that result in significantly fewer resets than existing state-of-the-art algorithms in both static and dynamic scenes. Our project website is available at https://ganuna.umd.edu/vis.poly/ .},
  archive      = {J_TVCG},
  author       = {Niall L. Williams and Aniket Bera and Dinesh Manocha},
  doi          = {10.1109/TVCG.2021.3106432},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4267-4277},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Redirected walking in static and dynamic scenes using visibility polygons},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multifocal stereoscopic projection mapping. <em>TVCG</em>,
<em>27</em>(11), 4256–4266. (<a
href="https://doi.org/10.1109/TVCG.2021.3106486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereoscopic projection mapping (PM) allows a user to see a three-dimensional (3D) computer-generated (CG) object floating over physical surfaces of arbitrary shapes around us using projected imagery. However, the current stereoscopic PM technology only satisfies binocular cues and is not capable of providing correct focus cues, which causes a vergence-accommodation conflict (VAC). Therefore, we propose a multifocal approach to mitigate VAC in stereoscopic PM. Our primary technical contribution is to attach electrically focus-tunable lenses (ETLs) to active shutter glasses to control both vergence and accommodation. Specifically, we apply fast and periodical focal sweeps to the ETLs, which causes the “virtual image” (as an optical term) of a scene observed through the ETLs to move back and forth during each sweep period. A 3D CG object is projected from a synchronized high-speed projector only when the virtual image of the projected imagery is located at a desired distance. This provides an observer with the correct focus cues required. In this study, we solve three technical issues that are unique to stereoscopic PM: (1) The 3D CG object is displayed on non-planar and even moving surfaces; (2) the physical surfaces need to be shown without the focus modulation; (3) the shutter glasses additionally need to be synchronized with the ETLs and the projector. We also develop a novel compensation technique to deal with the “lens breathing” artifact that varies the retinal size of the virtual image through focal length modulation. Further, using a proof-of-concept prototype, we demonstrate that our technique can present the virtual image of a target 3D CG object at the correct depth. Finally, we validate the advantage provided by our technique by comparing it with conventional stereoscopic PM using a user study on a depth-matching task.},
  archive      = {J_TVCG},
  author       = {Sorashi Kimura and Daisuke Iwai and Parinya Punpongsanon and Kosuke Sato},
  doi          = {10.1109/TVCG.2021.3106486},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4256-4266},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multifocal stereoscopic projection mapping},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile3DScanner: An online 3D scanner for high-quality
object reconstruction with a mobile device. <em>TVCG</em>,
<em>27</em>(11), 4245–4255. (<a
href="https://doi.org/10.1109/TVCG.2021.3106491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel online 3D scanning system for high-quality object reconstruction with a mobile device, called Mobile3DScanner. Using a mobile device equipped with an embedded RGBD camera, our system provides online 3D object reconstruction capability for users to acquire high-quality textured 3D object models. Starting with a simultaneous pose tracking and TSDF fusion module, our system allows users to scan an object with a mobile device to get a 3D model for real-time preview. After the real-time scanning process is completed, the scanned 3D model is globally optimized and mapped with multi-view textures as an efficient postprocess to get the final textured 3D model on the mobile device. Unlike most existing state-of-the-art systems which can only scan homeware objects such as toys with small dimensions due to the limited computation and memory resources of mobile platforms, our system can reconstruct objects with large dimensions such as statues. We propose a novel visual-inertial ICP approach to achieve real-time accurate 6DoF pose tracking of each incoming frame on the front end, while maintaining a keyframe pool on the back end where the keyframe poses are optimized by local BA. Simultaneously, the keyframe depth maps are fused by the optimized poses to a TSDF model in real-time. Especially, we propose a novel adaptive voxel resizing strategy to solve the out-of-memory problem of large dimension TSDF fusion on mobile platforms. In the post-process, the keyframe poses are globally optimized and the keyframe depth maps are optimized and fused to obtain a final object model with more accurate geometry. The experiments with quantitative and qualitative evaluation demonstrate the effectiveness of the proposed 3D scanning system based on a mobile device, which can successfully achieve online high-quality 3D reconstruction of natural objects with larger dimensions for efficient AR content creation.},
  archive      = {J_TVCG},
  author       = {Xiaojun Xiang and Hanqing Jiang and Guofeng Zhang and Yihao Yu and Chenchen Li and Xingbin Yang and Danpeng Chen and Hujun Bao},
  doi          = {10.1109/TVCG.2021.3106491},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4245-4255},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mobile3DScanner: An online 3D scanner for high-quality object reconstruction with a mobile device},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long-range augmented reality with dynamic occlusion
rendering. <em>TVCG</em>, <em>27</em>(11), 4236–4244. (<a
href="https://doi.org/10.1109/TVCG.2021.3106434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper occlusion based rendering is very important to achieve realism in all indoor and outdoor Augmented Reality (AR) applications. This paper addresses the problem of fast and accurate dynamic occlusion reasoning by real objects in the scene for large scale outdoor AR applications. Conceptually, proper occlusion reasoning requires an estimate of depth for every point in augmented scene which is technically hard to achieve for outdoor scenarios, especially in the presence of moving objects. We propose a method to detect and automatically infer the depth for real objects in the scene without explicit detailed scene modeling and depth sensing (e.g. without using sensors such as 3D-LiDAR). Specifically, we employ instance segmentation of color image data to detect real dynamic objects in the scene and use either a top-down terrain elevation model or deep learning based monocular depth estimation model to infer their metric distance from the camera for proper occlusion reasoning in real time. The realized solution is implemented in a low latency real-time framework for video-see-though AR and is directly extendable to optical-see-through AR. We minimize latency in depth reasoning and occlusion rendering by doing semantic object tracking and prediction in video frames.},
  archive      = {J_TVCG},
  author       = {Mikhail Sizintsev and Niluthpol Chowdhury Mithun and Han-Pang Chiu and Supun Samarasekera and Rakesh Kumar},
  doi          = {10.1109/TVCG.2021.3106434},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4236-4244},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Long-range augmented reality with dynamic occlusion rendering},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instant visual odometry initialization for mobile AR.
<em>TVCG</em>, <em>27</em>(11), 4226–4235. (<a
href="https://doi.org/10.1109/TVCG.2021.3106505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile AR applications benefit from fast initialization to display world-locked effects instantly. However, standard visual odometry or SLAM algorithms require motion parallax to initialize (see Figure 1 ) and, therefore, suffer from delayed initialization. In this paper, we present a 6-DoF monocular visual odometry that initializes instantly and without motion parallax. Our main contribution is a pose estimator that decouples estimating the 5-DoF relative rotation and translation direction from the 1-DoF translation magnitude. While scale is not observable in a monocular vision-only setting, it is still paramount to estimate a consistent scale over the whole trajectory (even if not physically accurate) to avoid AR effects moving erroneously along depth. In our approach, we leverage the fact that depth errors are not perceivable to the user during rotation-only motion. However, as the user starts translating the device, depth becomes perceivable and so does the capability to estimate consistent scale. Our proposed algorithm naturally transitions between these two modes. Our second contribution is a novel residual in the relative pose problem to further improve the results. The residual combines the Jacobians of the functional and the functional itself and is minimized using a Levenberg–Marquardt optimizer on the 5-DoF manifold. We perform extensive validations of our contributions with both a publicly available dataset and synthetic data. We show that the proposed pose estimator outperforms the classical approaches for 6-DoF pose estimation used in the literature in low-parallax configurations. Likewise, we show our relative pose estimator outperforms state-of-the-art approaches in an odometry pipeline configuration where we can leverage initial guesses. We release a dataset for the relative pose problem using real data to facilitate the comparison with future solutions for the relative pose problem. Our solution is either used as a full odometry or as a pre-SLAM component of any supported SLAM system (ARKit, ARCore) in world-locked AR effects on platforms such as Instagram and Facebook.},
  archive      = {J_TVCG},
  author       = {Alejo Concha and Michael Burri and Jesús Briales and Christian Forster and Luc Oth},
  doi          = {10.1109/TVCG.2021.3106505},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4226-4235},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Instant visual odometry initialization for mobile AR},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HPUI: Hand proximate user interfaces for one-handed
interactions on head mounted displays. <em>TVCG</em>, <em>27</em>(11),
4215–4225. (<a href="https://doi.org/10.1109/TVCG.2021.3106493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the design of Hand Proximate User Interfaces (HPUIs) for head-mounted displays (HMDs) to facilitate near-body interactions with the display directly projected on, or around the user&#39;s hand. We focus on single-handed input, while taking into consideration the hand anatomy which distorts naturally when the user interacts with the display. Through two user studies, we explore the potential for discrete as well as continuous input. For discrete input, HPUIs favor targets that are directly on the fingers (as opposed to off-finger) as they offer tactile feedback. We demonstrate that continuous interaction is also possible, and is as effective on the fingers as in the off-finger space between the index finger and thumb. We also find that with continuous input, content is more easily controlled when the interaction occurs in the vertical or horizontal axes, and less with diagonal movements. We conclude with applications and recommendations for the design of future HPUIs.},
  archive      = {J_TVCG},
  author       = {Shariff AM Faleel and Michael Gammon and Kevin Fan and Da-Yuan Huang and Wei Li and Pourang Irani},
  doi          = {10.1109/TVCG.2021.3106493},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4215-4225},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HPUI: Hand proximate user interfaces for one-handed interactions on head mounted displays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Head-mounted display with increased downward field of view
improves presence and sense of self-location. <em>TVCG</em>,
<em>27</em>(11), 4204–4214. (<a
href="https://doi.org/10.1109/TVCG.2021.3106513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common existing head-mounted displays (HMDs) for virtual reality (VR) provide users with a high presence and embodiment. However, the field of view (FoV) of a typical HMD for VR is about 90 to 110 [deg] in the diagonal direction and about 70 to 90 [deg] in the vertical direction, which is narrower than that of humans. Specifically, the downward FoV of conventional HMDs is too narrow to present the user avatar&#39;s body and feet. To address this problem, we have developed a novel HMD with a pair of additional display units to increase the downward FoV by approximately 60 ( $10+50$ ) [deg]. We comprehensively investigated the effects of the increased downward FoV on the sense of immersion that includes presence, sense of self-location (SoSL), sense of agency (SoA), and sense of body ownership (SoBO) during VR experience and on patterns of head movements and cybersickness as its secondary effects. As a result, it was clarified that the HMD with an increased FoV improved presence and SoSL. Also, it was confirmed that the user could see the object below with a head movement pattern close to the real behavior, and did not suffer from cybersickness. Moreover, the effect of the increased downward FoV on SoBO and SoA was limited since it was easier to perceive the misalignment between the real and virtual bodies.},
  archive      = {J_TVCG},
  author       = {Kizashi Nakano and Naoya Isoyama and Diego Monteiro and Nobuchika Sakata and Kiyoshi Kiyokawa and Takuji Narumi},
  doi          = {10.1109/TVCG.2021.3106513},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4204-4214},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Head-mounted display with increased downward field of view improves presence and sense of self-location},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaze-contingent retinal speckle suppression for
perceptually-matched foveated holographic displays. <em>TVCG</em>,
<em>27</em>(11), 4194–4203. (<a
href="https://doi.org/10.1109/TVCG.2021.3106433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-generated holographic (CGH) displays show great potential and are emerging as the next-generation displays for augmented and virtual reality, and automotive heads-up displays. One of the critical problems harming the wide adoption of such displays is the presence of speckle noise inherent to holography, that compromises its quality by introducing perceptible artifacts. Although speckle noise suppression has been an active research area, the previous works have not considered the perceptual characteristics of the Human Visual System (HVS), which receives the final displayed imagery. However, it is well studied that the sensitivity of the HVS is not uniform across the visual field, which has led to gaze-contingent rendering schemes for maximizing the perceptual quality in various computer-generated imagery. Inspired by this, we present the first method that reduces the “perceived speckle noise” by integrating foveal and peripheral vision characteristics of the HVS, along with the retinal point spread function, into the phase hologram computation. Specifically, we introduce the anatomical and statistical retinal receptor distribution into our computational hologram optimization, which places a higher priority on reducing the perceived foveal speckle noise while being adaptable to any individual&#39;s optical aberration on the retina. Our method demonstrates superior perceptual quality on our emulated holographic display. Our evaluations with objective measurements and subjective studies demonstrate a significant reduction of the human perceived noise.},
  archive      = {J_TVCG},
  author       = {Praneeth Chakravarthula and Zhan Zhang and Okan Tursun and Piotr Didyk and Qi Sun and Henry Fuchs},
  doi          = {10.1109/TVCG.2021.3106433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4194-4203},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gaze-contingent retinal speckle suppression for perceptually-matched foveated holographic displays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foveated photon mapping. <em>TVCG</em>, <em>27</em>(11),
4183–4193. (<a href="https://doi.org/10.1109/TVCG.2021.3106488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) applications require high-performance rendering algorithms to efficiently render 3D scenes on the VR head-mounted display, to provide users with an immersive and interactive virtual environment. Foveated rendering provides a solution to improve the performance of rendering algorithms by allocating computing resources to different regions based on the human visual acuity, and renders images of different qualities in different regions. Rasterization-based methods and ray tracing methods can be directly applied to foveated rendering, but rasterization-based methods are difficult to estimate global illumination (GI), and ray tracing methods are inefficient for rendering scenes that contain paths with low probability. Photon mapping is an efficient GI rendering method for scenes with different materials. However, since photon mapping cannot dynamically adjust the rendering quality of GI according to the human acuity, it cannot be directly applied to foveated rendering. In this paper, we propose a foveated photon mapping method to render realistic GI effects in the foveal region. We use the foveated photon tracing method to generate photons with high density in the foveal region, and these photons are used to render high-quality images in the foveal region. We further propose a temporal photon management to select and update the valid foveated photons of the previous frame for improving our method&#39;s performance. Our method can render diffuse, specular, glossy and transparent materials to achieve effects specifically related to GI, such as color bleeding, specular reflection, glossy reflection and caustics. Our method supports dynamic scenes and renders high-quality GI in the foveal region at interactive rates.},
  archive      = {J_TVCG},
  author       = {Xuehuai Shi and Lili Wang and Xiaoheng Wei and Ling-Qi Yan},
  doi          = {10.1109/TVCG.2021.3106488},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4183-4193},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Foveated photon mapping},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Directions for 3D user interface research from consumer VR
games. <em>TVCG</em>, <em>27</em>(11), 4171–4182. (<a
href="https://doi.org/10.1109/TVCG.2021.3106431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuing development of affordable immersive virtual reality (VR) systems, there is now a growing market for consumer content. The current form of consumer systems is not dissimilar to the lab-based VR systems of the past 30 years: the primary input mechanism is a head-tracked display and one or two tracked hands with buttons and joysticks on hand-held controllers. Over those 30 years, a very diverse academic literature has emerged that covers design and ergonomics of 3D user interfaces (3DUIs). However, the growing consumer market has engaged a very broad range of creatives that have built a very diverse set of designs. Sometimes these designs adopt findings from the academic literature, but other times they experiment with completely novel or counter-intuitive mechanisms. In this paper and its online adjunct, we report on novel 3DUI design patterns that are interesting from both design and research perspectives: they are highly novel, potentially broadly re-usable and/or suggest interesting avenues for evaluation. The supplemental material, which is a living document, is a crowd-sourced repository of interesting patterns. This paper is a curated snapshot of those patterns that were considered to be the most fruitful for further elaboration.},
  archive      = {J_TVCG},
  author       = {Anthony Steed and Tuukka M. Takala and Daniel Archer and Wallace Lages and Robert W. Lindeman},
  doi          = {10.1109/TVCG.2021.3106431},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4171-4182},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Directions for 3D user interface research from consumer VR games},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Directionally decomposing structured light for projector
calibration. <em>TVCG</em>, <em>27</em>(11), 4161–4170. (<a
href="https://doi.org/10.1109/TVCG.2021.3106511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic projector calibration is essential in projection mapping (PM) applications, especially in dynamic PM. However, due to the shallow depth-of-field (DOF) of a projector, more work is needed to ensure accurate calibration. We aim to estimate the intrinsic parameters of a projector while avoiding the limitation of shallow DOF. As the core of our technique, we present a practical calibration device that requires a minimal working volume directly in front of the projector lens regardless of the projector&#39;s focusing distance and aperture size. The device consists of a flat-bed scanner and pinhole-array masks. For calibration, a projector projects a series of structured light patterns in the device. The pinholes directionally decompose the structured light, and only the projected rays that pass through the pinholes hit the scanner plane. For each pinhole, we extract a ray passing through the optical center of the projector. Consequently, we regard the projector as a pinhole projector that projects the extracted rays only, and we calibrate the projector by applying the standard camera calibration technique, which assumes a pinhole camera model. Using a proof-of-concept prototype, we demonstrate that our technique can calibrate projectors with different focusing distances and aperture sizes at the same accuracy as a conventional method. Finally, we confirm that our technique can provide intrinsic parameters accurate enough for a dynamic PM application, even when a projector is placed too far from a projection target for a conventional method to calibrate the projector using a fiducial object of reasonable size.},
  archive      = {J_TVCG},
  author       = {Masatoki Sugimoto and Daisuke Iwai and Koki Ishida and Parinya Punpongsanon and Kosuke Sato},
  doi          = {10.1109/TVCG.2021.3106511},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4161-4170},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Directionally decomposing structured light for projector calibration},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and evaluation of personalized percutaneous coronary
intervention surgery simulation system. <em>TVCG</em>, <em>27</em>(11),
4150–4160. (<a href="https://doi.org/10.1109/TVCG.2021.3106478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, medical simulators have been widely applied to a broad range of surgery training tasks. However, most of the existing surgery simulators can only provide limited immersive environments with a few pre-processed organ models, while ignoring the instant modeling of various personalized clinical cases, which brings substantive differences between training experiences and real surgery situations. To this end, we present a virtual reality (VR) based surgery simulation system for personalized percutaneous coronary intervention (PCI). The simulation system can directly take patient-specific clinical data as input and generate virtual 3D intervention scenarios. Specially, we introduce a fiber-based patient-specific cardiac dynamic model to simulate the nonlinear deformation among the multiple layers of the cardiac structure, which can well respect and correlate the atriums, ventricles and vessels, and thus gives rise to more effective visualization and interaction. Meanwhile, we design a tracking and haptic feedback hardware, which can enable users to manipulate physical intervention instruments and interact with virtual scenarios. We conduct quantitative analysis on deformation precision and modeling efficiency, and evaluate the simulation system based on the user studies from 16 cardiologists and 20 intervention trainees, comparing it to traditional desktop intervention simulators. The results confirm that our simulation system can provide a better user experience, and is a suitable platform for PCI surgery training and rehearsal.},
  archive      = {J_TVCG},
  author       = {Shuai Li and Jiahao Cui and Aimin Hao and Shuyang Zhang and Qinping Zhao},
  doi          = {10.1109/TVCG.2021.3106478},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4150-4160},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design and evaluation of personalized percutaneous coronary intervention surgery simulation system},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complex interaction as emergent behaviour: Simulating
mid-air virtual keyboard typing using reinforcement learning.
<em>TVCG</em>, <em>27</em>(11), 4140–4149. (<a
href="https://doi.org/10.1109/TVCG.2021.3106494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately modelling user behaviour has the potential to significantly improve the quality of human-computer interaction. Traditionally, these models are carefully hand-crafted to approximate specific aspects of well-documented user behaviour. This limits their availability in virtual and augmented reality where user behaviour is often not yet well understood. Recent efforts have demonstrated that reinforcement learning can approximate human behaviour during simple goal-oriented reaching tasks. We build on these efforts and demonstrate that reinforcement learning can also approximate user behaviour in a complex mid-air interaction task: typing on a virtual keyboard. We present the first reinforcement learning-based user model for mid-air and surface-aligned typing on a virtual keyboard. Our model is shown to replicate high-level human typing behaviour. We demonstrate that this approach may be used to augment or replace human testing during the validation and development of virtual keyboards.},
  archive      = {J_TVCG},
  author       = {Lorenz Hetzel and John Dudley and Anna Maria Feit and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2021.3106494},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4140-4149},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Complex interaction as emergent behaviour: Simulating mid-air virtual keyboard typing using reinforcement learning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Avatars for teleconsultation: Effects of avatar embodiment
techniques on user perception in 3D asymmetric telepresence.
<em>TVCG</em>, <em>27</em>(11), 4129–4139. (<a
href="https://doi.org/10.1109/TVCG.2021.3106480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3D Telepresence system allows users to interact with each other in a virtual, mixed, or augmented reality (VR, MR, AR) environment, creating a shared space for collaboration and communication. There are two main methods for representing users within these 3D environments. Users can be represented either as point cloud reconstruction-based avatars that resemble a physical user or as virtual character-based avatars controlled by tracking the users&#39; body motion. This work compares both techniques to identify the differences between user representations and their fit in the reconstructed environments regarding the perceived presence, uncanny valley factors, and behavior impression. Our study uses an asymmetric VR/AR teleconsultation system that allows a remote user to join a local scene using VR. The local user observes the remote user with an AR head-mounted display, leading to facial occlusions in the 3D reconstruction. Participants perform a warm-up interaction task followed by a goal-directed collaborative puzzle task, pursuing a common goal. The local user was represented either as a point cloud reconstruction or as a virtual character-based avatar, in which case the point cloud reconstruction of the local user was masked. Our results show that the point cloud reconstruction-based avatar was superior to the virtual character avatar regarding perceived co-presence, social presence, behavioral impression, and humanness. Further, we found that the task type partly affected the perception. The point cloud reconstruction-based approach led to higher usability ratings, while objective performance measures showed no significant difference. We conclude that despite partly missing facial information, the point cloud-based reconstruction resulted in better conveyance of the user behavior and a more coherent fit into the simulation context.},
  archive      = {J_TVCG},
  author       = {Kevin Yu and Gleb Gorbachev and Ulrich Eck and Frieder Pankratz and Nassir Navab and Daniel Roth},
  doi          = {10.1109/TVCG.2021.3106480},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4129-4139},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Avatars for teleconsultation: Effects of avatar embodiment techniques on user perception in 3D asymmetric telepresence},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented reality for subsurface utility engineering,
revisited. <em>TVCG</em>, <em>27</em>(11), 4119–4128. (<a
href="https://doi.org/10.1109/TVCG.2021.3106479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Civil engineering is a primary domain for new augmented reality technologies. In this work, the area of subsurface utility engineering is revisited, and new methods tackling well-known, yet unsolved problems are presented. We describe our solution to the outdoor localization problem, which is deemed one of the most critical issues in outdoor augmented reality, proposing a novel, lightweight hardware platform to generate highly accurate position and orientation estimates in a global context. Furthermore, we present new approaches to drastically improve realism of outdoor data visualizations. First, a novel method to replace physical spray markings by indistinguishable virtual counterparts is described. Second, the visualization of 3D reconstructions of real excavations is presented, fusing seamlessly with the view onto the real environment. We demonstrate the power of these new methods on a set of different outdoor scenarios.},
  archive      = {J_TVCG},
  author       = {Lasse H. Hansen and Philipp Fleck and Marco Stranner and Dieter Schmalstieg and Clemens Arth},
  doi          = {10.1109/TVCG.2021.3106479},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4119-4128},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented reality for subsurface utility engineering, revisited},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AgentDress: Realtime clothing synthesis for virtual agents
using plausible deformations. <em>TVCG</em>, <em>27</em>(11), 4107–4118.
(<a href="https://doi.org/10.1109/TVCG.2021.3106429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a CPU-based real-time cloth animation method for dressing virtual humans of various shapes and poses. Our approach formulates the clothing deformation as a high-dimensional function of body shape parameters and pose parameters. In order to accelerate the computation, our formulation factorizes the clothing deformation into two independent components: the deformation introduced by body pose variation (Clothing Pose Model) and the deformation from body shape variation (Clothing Shape Model). Furthermore, we sample and cluster the poses spanning the entire pose space and use those clusters to efficiently calculate the anchoring points. We also introduce a sensitivity-based distance measurement to both find nearby anchoring points and evaluate their contributions to the final animation. Given a query shape and pose of the virtual agent, we synthesize the resulting clothing deformation by blending the Taylor expansion results of nearby anchoring points. Compared to previous methods, our approach is general and able to add the shape dimension to any clothing pose model. Furthermore, we can animate clothing represented with tens of thousands of vertices at 50+ FPS on a CPU. We also conduct a user evaluation and show that our method can improve a user&#39;s perception of dressed virtual agents in an immersive virtual environment (IVE) compared to a realtime linear blend skinning method.},
  archive      = {J_TVCG},
  author       = {Nannan Wu and Qianwen Chao and Yanzhen Chen and Weiwei Xu and Chen Liu and Dinesh Manocha and Wenxin Sun and Yi Han and Xinran Yao and Xiaogang Jin},
  doi          = {10.1109/TVCG.2021.3106429},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4107-4118},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AgentDress: Realtime clothing synthesis for virtual agents using plausible deformations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive light estimation using dynamic filtering for
diverse lighting conditions. <em>TVCG</em>, <em>27</em>(11), 4097–4106.
(<a href="https://doi.org/10.1109/TVCG.2021.3106497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dynamic range (HDR) panoramic environment maps are widely used to illuminate virtual objects to blend with real-world scenes. However, in common applications for augmented and mixed-reality (AR/MR), capturing 360° surroundings to obtain an HDR environment map is often not possible using consumer-level devices. We present a novel light estimation method to predict 360° HDR environment maps from a single photograph with a limited field-of-view (FOV). We introduce the Dynamic Lighting network (DLNet), a convolutional neural network that dynamically generates the convolution filters based on the input photograph sample to adaptively learn the lighting cues within each photograph. We propose novel Spherical Multi-Scale Dynamic (SMD) convolutional modules to dynamically generate sample-specific kernels for decoding features in the spherical domain to predict 360° environment maps. Using DLNet and data augmentations with respect to FOV, an exposure multiplier, and color temperature, our model shows the capability of estimating lighting under diverse input variations. Compared with prior work that fixes the network filters once trained, our method maintains lighting consistency across different exposure multipliers and color temperature, and maintains robust light estimation accuracy as FOV increases. The surrounding lighting information estimated by our method ensures coherent illumination of 3D objects blended with the input photograph, enabling high fidelity augmented and mixed reality supporting a wide range of environmental lighting conditions and device sensors.},
  archive      = {J_TVCG},
  author       = {Junhong Zhao and Andrew Chalmers and Taehyun Rhee},
  doi          = {10.1109/TVCG.2021.3106497},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4097-4106},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive light estimation using dynamic filtering for diverse lighting conditions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A partially-sorted concentric layout for efficient label
localization in augmented reality. <em>TVCG</em>, <em>27</em>(11),
4087–4096. (<a href="https://doi.org/10.1109/TVCG.2021.3106492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common approach for Augmented Reality labeling is to display the label text on a flag planted into the real world element at a 3D anchor point. When there are more than just a few labels, the efficiency of the interface decreases as the user has to search for a given label sequentially. The search can be accelerated by sorting the labels alphabetically, but sorting all labels results in long and intersecting leader lines from the anchor points to the labels. This paper proposes a partially-sorted concentric label layout that leverages the search efficiency of sorting while avoiding the label display problems of long or intersecting leader lines. The labels are partitioned into a small number of sorted sequences displayed on circles of increasing radii. Since the labels on a circle are sorted, the user can quickly search each circle. A tight upper bound derived from circular permutation theory limits the number of circles and thereby the complexity of the label layout. For example, 12 labels require at most three circles. When the application allows it, the labels are presorted to further reduce the number of circles in the layout. The layout was tested in a user study where it significantly reduced the label searching time compared to a conventional single-circle layout.},
  archive      = {J_TVCG},
  author       = {Zijing Zhou and Lili Wang and Voicu Popescu},
  doi          = {10.1109/TVCG.2021.3106492},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4087-4096},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A partially-sorted concentric layout for efficient label localization in augmented reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Message from the ISMAR 2021 science and technology journal
program chairs and TVCG guest editors. <em>TVCG</em>, <em>27</em>(11),
4086. (<a href="https://doi.org/10.1109/TVCG.2021.3110544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG) , we are pleased to present the journal papers from the 20th IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2021), which will be held as a virtual conference between October 4 and 8, 2021. ISMAR continues the over twenty year long tradition of IWAR, ISMR, and ISAR, and is the premier conference for Mixed and Augmented Reality in the world.},
  archive      = {J_TVCG},
  author       = {Daisuke Iwai and Guillaume Moreau and Denis Kalkofen and Tabitha Peck},
  doi          = {10.1109/TVCG.2021.3110544},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4086},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the ISMAR 2021 science and technology journal program chairs and TVCG guest editors},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Message from the editor-in-chief and from the associate
editor-in-chief. <em>TVCG</em>, <em>27</em>(11), 4085. (<a
href="https://doi.org/10.1109/TVCG.2021.3110543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the November 2021 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) . This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR). The conference took place in Bari, Italy from October 4–8, 2021 in virtual mode due to the COVID-19 pandemic.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller and Doug Bowman},
  doi          = {10.1109/TVCG.2021.3110543},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {11},
  pages        = {4085},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief and from the associate editor-in-chief},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstructing reflection maps using a stacked-CNN for mixed
reality rendering. <em>TVCG</em>, <em>27</em>(10), 4073–4084. (<a
href="https://doi.org/10.1109/TVCG.2020.3001917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corresponding lighting and reflectance between real and virtual objects is important for spatial presence in augmented and mixed reality (AR and MR) applications. We present a method to reconstruct real-world environmental lighting, encoded as a reflection map (RM), from a conventional photograph. To achieve this, we propose a stacked convolutional neural network (SCNN) that predicts high dynamic range (HDR) 360° RMs with varying roughness from a limited field of view, low dynamic range photograph. The SCNN is progressively trained from high to low roughness to predict RMs at varying roughness levels, where each roughness level corresponds to a virtual object&#39;s roughness (from diffuse to glossy) for rendering. The predicted RM provides high-fidelity rendering of virtual objects to match with the background photograph. We illustrate the use of our method with indoor and outdoor scenes trained on separate indoor/outdoor SCNNs showing plausible rendering and composition of virtual objects in AR/MR. We show that our method has improved quality over previous methods with a comparative user study and error metrics.},
  archive      = {J_TVCG},
  author       = {Andrew Chalmers and Junhong Zhao and Daniel Medeiros and Taehyun Rhee},
  doi          = {10.1109/TVCG.2020.3001917},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4073-4084},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reconstructing reflection maps using a stacked-CNN for mixed reality rendering},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DNF-net: A deep normal filtering network for mesh denoising.
<em>TVCG</em>, <em>27</em>(10), 4060–4072. (<a
href="https://doi.org/10.1109/TVCG.2020.3001681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a deep normal filtering network, called DNF-Net, for mesh denoising. To better capture local geometry, our network processes the mesh in terms of local patches extracted from the mesh. Overall, DNF-Net is an end-to-end network that takes patches of facet normals as inputs and directly outputs the corresponding denoised facet normals of the patches. In this way, we can reconstruct the geometry from the denoised normals with feature preservation. Besides the overall network architecture, our contributions include a novel multi-scale feature embedding unit, a residual learning strategy to remove noise, and a deeply-supervised joint loss function. Compared with the recent data-driven works on mesh denoising, DNF-Net does not require manual input to extract features and better utilizes the training data to enhance its denoising performance. Finally, we present comprehensive experiments to evaluate our method and demonstrate its superiority over the state of the art on both synthetic and real-scanned meshes.},
  archive      = {J_TVCG},
  author       = {Xianzhi Li and Ruihui Li and Lei Zhu and Chi-Wing Fu and Pheng-Ann Heng},
  doi          = {10.1109/TVCG.2020.3001681},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4060-4072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DNF-net: A deep normal filtering network for mesh denoising},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic lightcuts for sampling many lights.
<em>TVCG</em>, <em>27</em>(10), 4049–4059. (<a
href="https://doi.org/10.1109/TVCG.2020.3001271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce stochastic lightcuts by combining the lighting approximation of lightcuts with stochastic sampling for efficiently rendering scenes with a large number of light sources. Our stochastic lightcuts method entirely eliminates the sampling correlation of lightcuts and replaces it with noise. To minimize this noise, we present a robust hierarchical sampling strategy, combining the benefits of importance sampling, adaptive sampling, and stratified sampling. Our approach also provides temporally stable results and lifts any restrictions on the light types that can be approximated with lightcuts. We present examples of using stochastic lightcuts with path tracing and indirect illumination with virtual lights, achieving more than an order of magnitude faster render times than lightcuts by effectively approximating direct illumination using a small number of light samples, in addition to providing temporal stability. Our comparisons to other stochastic sampling techniques demonstrate that we provide superior sampling quality that matches and improves the excellent convergence rates of the lightcuts approach.},
  archive      = {J_TVCG},
  author       = {Cem Yuksel},
  doi          = {10.1109/TVCG.2020.3001271},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4049-4059},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Stochastic lightcuts for sampling many lights},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attribute-conditioned layout GAN for automatic graphic
design. <em>TVCG</em>, <em>27</em>(10), 4039–4048. (<a
href="https://doi.org/10.1109/TVCG.2020.2999335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling layout is an important first step for graphic design. Recently, methods for generating graphic layouts have progressed, particularly with Generative Adversarial Networks (GANs). However, the problem of specifying the locations and sizes of design elements usually involves constraints with respect to element attributes, such as area, aspect ratio and reading-order. Automating attribute conditional graphic layouts remains a complex and unsolved problem. In this article, we introduce Attribute-conditioned Layout GAN to incorporate the attributes of design elements for graphic layout generation by forcing both the generator and the discriminator to meet attribute conditions. Due to the complexity of graphic designs, we further propose an element dropout method to make the discriminator look at partial lists of elements and learn their local patterns. In addition, we introduce various loss designs following different design principles for layout optimization. We demonstrate that the proposed method can synthesize graphic layouts conditioned on different element attributes. It can also adjust well-designed layouts to new sizes while retaining elements&#39; original reading-orders. The effectiveness of our method is validated through a user study.},
  archive      = {J_TVCG},
  author       = {Jianan Li and Jimei Yang and Jianming Zhang and Chang Liu and Christina Wang and Tingfa Xu},
  doi          = {10.1109/TVCG.2020.2999335},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4039-4048},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Attribute-conditioned layout GAN for automatic graphic design},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual co-embodiment: Evaluation of the sense of agency
while sharing the control of a virtual body among two individuals.
<em>TVCG</em>, <em>27</em>(10), 4023–4038. (<a
href="https://doi.org/10.1109/TVCG.2020.2999197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a concept called “virtual co-embodiment”, which enables a user to share their virtual avatar with another entity (e.g., another user, robot, or autonomous agent). We describe a proof-of-concept in which two users can be immersed from a first-person perspective in a virtual environment and can have complementary levels of control (total, partial, or none) over a shared avatar. In addition, we conducted an experiment to investigate the influence of users’ level of control over the shared avatar and prior knowledge of their actions on the users’ sense of agency and motor actions. The results showed that participants are good at estimating their real level of control but significantly overestimate their sense of agency when they can anticipate the motion of the avatar. Moreover, participants performed similar body motions regardless of their real control over the avatar. The results also revealed that the internal dimension of the locus of control, which is a personality trait, is negatively correlated with the user’s perceived level of control. The combined results unfold a new range of applications in the fields of virtual-reality-based training and collaborative teleoperation, where users would be able to share their virtual body.},
  archive      = {J_TVCG},
  author       = {Rebecca Fribourg and Nami Ogawa and Ludovic Hoyet and Ferran Argelaguet and Takuji Narumi and Michitaka Hirose and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2020.2999197},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4023-4038},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual co-embodiment: Evaluation of the sense of agency while sharing the control of a virtual body among two individuals},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dynamic textures for neural rendering of human
actors. <em>TVCG</em>, <em>27</em>(10), 4009–4022. (<a
href="https://doi.org/10.1109/TVCG.2020.2996594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing realistic videos of humans using neural networks has been a popular alternative to the conventional graphics-based rendering pipeline due to its high efficiency. Existing works typically formulate this as an image-to-image translation problem in 2D screen space, which leads to artifacts such as over-smoothing, missing body parts, and temporal instability of fine-scale detail, such as pose-dependent wrinkles in the clothing. In this article, we propose a novel human video synthesis method that approaches these limiting factors by explicitly disentangling the learning of time-coherent fine-scale details from the embedding of the human in 2D screen space. More specifically, our method relies on the combination of two convolutional neural networks (CNNs). Given the pose information, the first CNN predicts a dynamic texture map that contains time-coherent high-frequency details, and the second CNN conditions the generation of the final video on the temporally coherent output of the first CNN. We demonstrate several applications of our approach, such as human reenactment and novel view synthesis from monocular video, where we show significant improvement over the state of the art both qualitatively and quantitatively.},
  archive      = {J_TVCG},
  author       = {Lingjie Liu and Weipeng Xu and Marc Habermann and Michael Zollhöfer and Florian Bernard and Hyeongwoo Kim and Wenping Wang and Christian Theobalt},
  doi          = {10.1109/TVCG.2020.2996594},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {4009-4022},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning dynamic textures for neural rendering of human actors},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spanning trees as approximation of data structures.
<em>TVCG</em>, <em>27</em>(10), 3994–4008. (<a
href="https://doi.org/10.1109/TVCG.2020.2995465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The connections in a graph generate a structure that is independent of a coordinate system. This visual metaphor allows creating a more flexible representation of data than a two-dimensional scatterplot. In this article, we present STAD (Simplified Topological Abstraction of Data), a parameter-free dimensionality reduction method that projects high-dimensional data into a graph. STAD generates an abstract representation of high-dimensional data by giving each data point a location in a graph which preserves the approximate distances in the original high-dimensional space. The STAD graph is built upon the Minimum Spanning Tree (MST) to which new edges are added until the correlation between the distances from the graph and the original dataset is maximized. Additionally, STAD supports the inclusion of additional functions to focus the exploration and allow the analysis of data from new perspectives, emphasizing traits in data which otherwise would remain hidden. We demonstrate the effectiveness of our method by applying it to two real-world datasets: traffic density in Barcelona and temporal measurements of air quality in Castile and León in Spain.},
  archive      = {J_TVCG},
  author       = {Daniel Alcaide and Jan Aerts},
  doi          = {10.1109/TVCG.2020.2995465},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3994-4008},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spanning trees as approximation of data structures},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Top-down shape abstraction based on greedy pole selection.
<em>TVCG</em>, <em>27</em>(10), 3982–3993. (<a
href="https://doi.org/10.1109/TVCG.2020.2995495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the fact that the medial axis transform is able to encode the shape completely, we propose to use as few medial balls as possible to approximate the original enclosed volume by the boundary surface. We progressively select new medial balls, in a top-down style, to enlarge the region spanned by the existing medial balls. The key spirit of the selection strategy is to encourage large medial balls while imposing given geometric constraints. We further propose a speedup technique based on a provable observation that the intersection of medial balls implies the adjacency of power cells (in the sense of the power crust). We further elaborate the selection rules in combination with two closely related applications. One application is to develop an easy-to-use ball-stick modeling system that helps non-professional users to quickly build a shape with only balls and wires, but any penetration between two medial balls must be suppressed. The other application is to generate porous structures with convex, compact (with a high isoperimetric quotient) and shape-aware pores where two adjacent spherical pores may have penetration as long as the mechanical rigidity can be well preserved.},
  archive      = {J_TVCG},
  author       = {Zhiyang Dou and Shiqing Xin and Rui Xu and Jian Xu and Yuanfeng Zhou and Shuangmin Chen and Wenping Wang and Xiuyang Zhao and Changhe Tu},
  doi          = {10.1109/TVCG.2020.2995495},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3982-3993},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Top-down shape abstraction based on greedy pole selection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PICO: Procedural iterative constrained optimizer for
geometric modeling. <em>TVCG</em>, <em>27</em>(10), 3968–3981. (<a
href="https://doi.org/10.1109/TVCG.2020.2995556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural modeling has produced amazing results, yet fundamental issues such as controllability and limited user guidance persist. We introduce a novel procedural model called PICO (Procedural Iterative Constrained Optimizer) and PICO-Graph that is the underlying procedural model designed with optimization in mind. The key novelty of PICO is that it enables the exploration of generative designs by combining both user and environmental constraints into a single framework by using optimization without the need to write procedural rules. The PICO-Graph procedural model consists of a set of geometry generating operations and a set of axioms connected in a directed cyclic graph. The forward generation is initiated by a set of axioms that use the connections to send coordinate systems and geometric objects through the PICO-Graph, which in turn generates more objects. This allows for fast generation of complex and varied geometries. Moreover, we combine PICO-Graph with efficient optimization that allows for quick exploration of the generated models and the generation of variants. The user defines the rules, the axioms, and the set of constraints; for example, whether an existing object should be supported by the generated model, whether symmetries exist, whether the object should spin, etc. PICO then generates a class of geometric models and optimizes them so that they fulfill the constraints. The generation and the optimization in our implementation provides interactive user control during model execution providing continuous feedback. For example, the user can sketch the constraints and guide the geometry to meet these specified goals. We show PICO on a variety of examples such as the generation of procedural chairs with multiple supports, generation of support structures for 3D printing, generation of spinning objects, or generation of procedural terrains matching a given input. Our framework could be used as a component in a larger design workflow; its strongest application is in the early rapid ideation and prototyping phases.},
  archive      = {J_TVCG},
  author       = {Vojtěch Krs and Radomír Měch and Mathieu Gaillard and Nathan Carr and Bedrich Benes},
  doi          = {10.1109/TVCG.2020.2995556},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3968-3981},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PICO: Procedural iterative constrained optimizer for geometric modeling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive steering of hierarchical clustering.
<em>TVCG</em>, <em>27</em>(10), 3953–3967. (<a
href="https://doi.org/10.1109/TVCG.2020.2995100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical clustering is an important technique to organize big data for exploratory data analysis. However, existing one-size-fits-all hierarchical clustering methods often fail to meet the diverse needs of different users. To address this challenge, we present an interactive steering method to visually supervise constrained hierarchical clustering by utilizing both public knowledge (e.g., Wikipedia) and private knowledge from users. The novelty of our approach includes 1) automatically constructing constraints for hierarchical clustering using knowledge (knowledge-driven) and intrinsic data distribution (data-driven), and 2) enabling the interactive steering of clustering through a visual interface (user-driven). Our method first maps each data item to the most relevant items in a knowledge base. An initial constraint tree is then extracted using the ant colony optimization algorithm. The algorithm balances the tree width and depth and covers the data items with high confidence. Given the constraint tree, the data items are hierarchically clustered using evolutionary Bayesian rose tree. To clearly convey the hierarchical clustering results, an uncertainty-aware tree visualization has been developed to enable users to quickly locate the most uncertain sub-hierarchies and interactively improve them. The quantitative evaluation and case study demonstrate that the proposed approach facilitates the building of customized clustering trees in an efficient and effective manner.},
  archive      = {J_TVCG},
  author       = {Weikai Yang and Xiting Wang and Jie Lu and Wenwen Dou and Shixia Liu},
  doi          = {10.1109/TVCG.2020.2995100},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3953-3967},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive steering of hierarchical clustering},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SpotSDC: Revealing the silent data corruption propagation in
high-performance computing systems. <em>TVCG</em>, <em>27</em>(10),
3938–3952. (<a href="https://doi.org/10.1109/TVCG.2020.2994954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trend of rapid technology scaling is expected to make the hardware of high-performance computing (HPC) systems more susceptible to computational errors due to random bit flips. Some bit flips may cause a program to crash or have a minimal effect on the output, but others may lead to silent data corruption (SDC), i.e., undetected yet significant output errors. Classical fault injection analysis methods employ uniform sampling of random bit flips during program execution to derive a statistical resiliency profile. However, summarizing such fault injection result with sufficient detail is difficult, and understanding the behavior of the fault-corrupted program is still a challenge. In this article, we introduce SpotSDC, a visualization system to facilitate the analysis of a program&#39;s resilience to SDC. SpotSDC provides multiple perspectives at various levels of detail of the impact on the output relative to where in the source code the flipped bit occurs, which bit is flipped, and when during the execution it happens. SpotSDC also enables users to study the code protection and provide new insights to understand the behavior of a fault-injected program. Based on lessons learned, we demonstrate how what we found can improve the fault injection campaign method.},
  archive      = {J_TVCG},
  author       = {Zhimin Li and Harshitha Menon and Dan Maljovec and Yarden Livnat and Shusen Liu and Kathryn Mohror and Peer-Timo Bremer and Valerio Pascucci},
  doi          = {10.1109/TVCG.2020.2994954},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3938-3952},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SpotSDC: Revealing the silent data corruption propagation in high-performance computing systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology constrained shape correspondence. <em>TVCG</em>,
<em>27</em>(10), 3926–3937. (<a
href="https://doi.org/10.1109/TVCG.2020.2994013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better address the deformation and structural variation challenges inherently present in 3D shapes, researchers have shifted their focus from designing handcrafted point descriptors to learning point descriptors and their correspondences in a data-driven manner. Recent studies have developed deep neural networks for robust point descriptor and shape correspondence learning in consideration of local structural information. In this article, we developed a novel shape correspondence learning network, called TC-NET, which further enhances performance by encouraging the topological consistency between the embedding feature space and the input shape space. Specifically, in this article, we first calculate the topology-associated edge weights to represent the topological structure of each point. Then, in order to preserve this topological structure in high-dimensional feature space, a structural regularization term is defined to minimize the topology-consistent feature reconstruction loss (Topo-Loss) during the correspondence learning process. Our proposed method achieved state-of-the-art performance on three shape correspondence benchmark datasets. In addition, the proposed topology preservation concept can be easily generalized to other learning-based shape analysis tasks to regularize the topological structure of high-dimensional feature spaces.},
  archive      = {J_TVCG},
  author       = {Xiang Li and Congcong Wen and Lingjing Wang and Yi Fang},
  doi          = {10.1109/TVCG.2020.2994013},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3926-3937},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Topology constrained shape correspondence},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Void space surfaces to convey depth in vessel
visualizations. <em>TVCG</em>, <em>27</em>(10), 3913–3925. (<a
href="https://doi.org/10.1109/TVCG.2020.2993992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance depth perception and thus data comprehension, additional depth cues are often used in 3D visualizations of complex vascular structures. There is a variety of different approaches described in the literature, ranging from chromadepth color coding over depth of field to glyph-based encodings. Unfortunately, the majority of existing approaches suffers from the same problem: As these cues are directly applied to the geometry&#39;s surface, the display of additional information on the vessel wall, such as other modalities or derived attributes, is impaired. To overcome this limitation we propose Void Space Surfaces which utilizes empty space in between vessel branches to communicate depth and their relative positioning. This allows us to enhance the depth perception of vascular structures without interfering with the spatial data and potentially superimposed parameter information. With this article, we introduce Void Space Surfaces, describe their technical realization, and show their application to various vessel trees. Moreover, we report the outcome of two user studies which we have conducted in order to evaluate the perceptual impact of Void Space Surfaces compared to existing vessel visualization techniques and discuss expert feedback.},
  archive      = {J_TVCG},
  author       = {Julian Kreiser and Pedro Hermosilla and Timo Ropinski},
  doi          = {10.1109/TVCG.2020.2993992},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3913-3925},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Void space surfaces to convey depth in vessel visualizations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video folding: Increased framerate for semi-repetitive
sequences. <em>TVCG</em>, <em>27</em>(10), 3900–3912. (<a
href="https://doi.org/10.1109/TVCG.2020.2992670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a technique to synthetically increase the framerate of semi-repetitive videos (i.e., videos of motion that repeats but not in an identical fashion) to aid in visualization. By reordering and combining frames from all repetitions, we produce a single non-repetitive sequence with much higher temporal resolution. Then, we use a novel frame warping technique based on a dense corrective flow to counteract differences between repetitions. The resulting video maintains smoothness of motion and additionally allows for seamless, infinite looping. We demonstrate the effectiveness of the proposed solution both quantitatively, by measuring the improvement over existing methods, and qualitatively, by performing a user evaluation and providing several examples in the article and accompanying video.},
  archive      = {J_TVCG},
  author       = {Chris May and Manuel M. Oliveira and Daniel Aliaga},
  doi          = {10.1109/TVCG.2020.2992670},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3900-3912},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Video folding: Increased framerate for semi-repetitive sequences},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UrbanMotion: Visual analysis of metropolitan-scale sparse
trajectories. <em>TVCG</em>, <em>27</em>(10), 3881–3899. (<a
href="https://doi.org/10.1109/TVCG.2020.2992200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing massive scale human movement in cities plays an important role in solving many of the problems that modern cities face (e.g., traffic optimization, business site configuration). In this article, we study a big mobile location dataset that covers millions of city residents, but is temporally sparse on the trajectory of individual user. Mapping sparse trajectories to illustrate population movement poses several challenges from both analysis and visualization perspectives. In the literature, there are a few techniques designed for sparse trajectory visualization; yet they do not consider trajectories collected from mobile apps that possess long-tailed sparsity with record intervals as long as hours. This article introduces UrbanMotion, a visual analytics system that extends the original wind map design by supporting map-matched local movements, multi-directional population flows, and population distributions. Effective methods are proposed to extract and aggregate population movements from dense parts of the trajectories leveraging their long-tailed sparsity. Both characteristic and anomalous patterns are discovered and visualized. We conducted three case studies, one comparative experiment, and collected expert feedback in the application domains of commuting analysis, event detection, and business site configuration. The study result demonstrates the significance and effectiveness of our system in helping to complete key analytics tasks for urban users.},
  archive      = {J_TVCG},
  author       = {Lei Shi and Congcong Huang and Meijun Liu and Jia Yan and Tao Jiang and Zhihao Tan and Yifan Hu and Wei Chen and Xiatian Zhang},
  doi          = {10.1109/TVCG.2020.2992200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3881-3899},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {UrbanMotion: Visual analysis of metropolitan-scale sparse trajectories},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning physical parameters and detail enhancement for
gaseous scene design based on data guidance. <em>TVCG</em>,
<em>27</em>(10), 3867–3880. (<a
href="https://doi.org/10.1109/TVCG.2020.2991217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article articulates a novel learning framework for both parameter estimation and detail enhancement for Eulerian gas based on data guidance. The key motivation of this article is to devise a new hybrid, grid-based simulation that could inherit modeling and simulation advantages from both physically-correct simulation methods and powerful data-driven methods, while combating existing difficulties exhibited in both approaches. We first employ a convolutional neural network (CNN) to estimate the physical parameters of gaseous phenomena in Eulerian settings, then we can use the just-learnt parameters to re-simulate (with or without artists&#39; guidance) for specific scenes with flexible coupling effects. Next, a second CNN is adopted to reconstruct the high-resolution velocity field to guide a fast re-simulation on the finer grid, achieving richer and more realistic details with little extra computational expense. From the perspective of physics-based simulation, our trained networks respect temporal coherence and physical constraints. From the perspective of the data-driven machine-learning approaches, our network design aims at extracting a meaningful parameters and reconstructing visually realistic details. Additionally, our implementation based on parallel acceleration could significantly enhance the computational performance of every involved module. Our comprehensive experiments confirm the controllability, effectiveness, and accuracy of our novel approach when producing various gaseous scenes with rich details for widespread graphics applications.},
  archive      = {J_TVCG},
  author       = {Chen Li and Sheng Qiu and Changbo Wang and Hong Qin},
  doi          = {10.1109/TVCG.2020.2991217},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3867-3880},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Learning physical parameters and detail enhancement for gaseous scene design based on data guidance},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analytics for hypothesis-driven exploration in
computational pathology. <em>TVCG</em>, <em>27</em>(10), 3851–3866. (<a
href="https://doi.org/10.1109/TVCG.2020.2990336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in computational and algorithmic power are evolving the field of medical imaging rapidly. In cancer research, many new directions are sought to characterize patients with additional imaging features derived from radiology and pathology images. The emerging field of Computational Pathology targets the high-throughput extraction and analysis of the spatial distribution of cells from digital histopathology images. The associated morphological and architectural features allow researchers to quantify and characterize new imaging biomarkers for cancer diagnosis, prognosis, and treatment decisions. However, while the image feature space grows, exploration and analysis become more difficult and ineffective. There is a need for dedicated interfaces for interactive data manipulation and visual analysis of computational pathology and clinical data. For this purpose, we present IIComPath , a visual analytics approach that enables clinical researchers to formulate hypotheses and create computational pathology pipelines involving cohort construction, spatial analysis of image-derived features, and cohort analysis. We demonstrate our approach through use cases that investigate the prognostic value of current diagnostic features and new computational pathology biomarkers.},
  archive      = {J_TVCG},
  author       = {A. Corvò and H. S. Garcia Caballero and M. A. Westenberg and M. A. van Driel and J. J. van Wijk},
  doi          = {10.1109/TVCG.2020.2990336},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3851-3866},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for hypothesis-driven exploration in computational pathology},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Immersion and coherence: Research agenda and early results.
<em>TVCG</em>, <em>27</em>(10), 3839–3850. (<a
href="https://doi.org/10.1109/TVCG.2020.2983701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presence has been studied in the context of virtual environments for nearly thirty years, but the field has yet to reach consensus on even basic issues of definition and measurement, and there are many open research questions. We gather many of these open research questions and systematically group them according to what we believe are five key constructs that inform user experience in virtual environments: immersion, coherence, Place Illusion, Plausibility Illusion, and presence. We also report on the design and results of a study that investigated the effects of immersion and coherence on user experience in a stressful virtual visual cliff environment. In this article, each participant experienced a given VE in one of four conditions chosen from a 2x2 design: high or low levels of immersion and high or low levels of coherence. We collected both questionnaire-based and physiological metrics. Several existing presence questionnaires could not reliably distinguish the effects of immersion from those of coherence. They did, however, indicate that high levels of both together result in higher presence, compared any of the other three conditions. This suggests that “breaks in PI” and “breaks in Psi” belong to a broader category of “breaks in experience,” any of which result in a degraded user experience. Participants&#39; heart rates responded markedly differently in the two coherence conditions; no such difference was observed across the immersion conditions. This indicates that a VE that exhibits unusual or confusing behavior can cause stress in a user that affects physiological responses, and that one must take care to eliminate such confusing behaviors if one is using physiological measurement as a proxy for subjective experience in a VE.},
  archive      = {J_TVCG},
  author       = {Richard Skarbez and Frederick P. Brooks and Mary C. Whitton},
  doi          = {10.1109/TVCG.2020.2983701},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {10},
  pages        = {3839-3850},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersion and coherence: Research agenda and early results},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual model fit estimation in scatterplots: Influence of
amount and decentering of noise. <em>TVCG</em>, <em>27</em>(9),
3834–3838. (<a href="https://doi.org/10.1109/TVCG.2021.3051853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots with a model enable visual estimation of model-data fit. In Experiment 1 (N = 62) we quantified the influence of noise-level on subjective misfit and found a negatively accelerated relationship. Experiment 2 showed that decentering of noise only mildly reduced fit ratings. The results have consequences for model-evaluation.},
  archive      = {J_TVCG},
  author       = {Daniel Reimann and Christine Blech and Nilam Ram and Robert Gaschler},
  doi          = {10.1109/TVCG.2021.3051853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3834-3838},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual model fit estimation in scatterplots: Influence of amount and decentering of noise},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VIS30K: A collection of figures and tables from IEEE
visualization conference publications. <em>TVCG</em>, <em>27</em>(9),
3826–3833. (<a href="https://doi.org/10.1109/TVCG.2021.3054916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K&#39;s comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN, visimagenavigator.github.io ), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.},
  archive      = {J_TVCG},
  author       = {Jian Chen and Meng Ling and Rui Li and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Torsten Möller and Robert S. Laramee and Han-Wei Shen and Katharina Wünsche and Qiru Wang},
  doi          = {10.1109/TVCG.2021.3054916},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3826-3833},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VIS30K: A collection of figures and tables from IEEE visualization conference publications},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualization techniques in augmented reality: A taxonomy,
methods and patterns. <em>TVCG</em>, <em>27</em>(9), 3808–3825. (<a
href="https://doi.org/10.1109/TVCG.2020.2986247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the development of Augmented Reality (AR) frameworks made AR application development widely accessible to developers without AR expert background. With this development, new application fields for AR are on the rise. This comes with an increased need for visualization techniques that are suitable for a wide range of application areas. It becomes more important for a wider audience to gain a better understanding of existing AR visualization techniques. In this article we provide a taxonomy of existing works on visualization techniques in AR. The taxonomy aims to give researchers and developers without an in-depth background in Augmented Reality the information to successively apply visualization techniques in Augmented Reality environments. We also describe required components and methods and analyze common patterns.},
  archive      = {J_TVCG},
  author       = {Stefanie Zollmann and Tobias Langlotz and Raphael Grasset and Wei Hong Lo and Shohei Mori and Holger Regenbrecht},
  doi          = {10.1109/TVCG.2020.2986247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3808-3825},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization techniques in augmented reality: A taxonomy, methods and patterns},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vectorizing quantum turbulence vortex-core lines for
real-time visualization. <em>TVCG</em>, <em>27</em>(9), 3794–3807. (<a
href="https://doi.org/10.1109/TVCG.2020.2981460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vectorizing vortex-core lines is crucial for high-quality visualization and analysis of turbulence. While several techniques exist in the literature, they can only be applied to classical fluids. As quantum fluids with turbulence are gaining attention in physics, extracting and visualizing vortex-core lines for quantum fluids is increasingly desirable. In this article, we develop an efficient vortex-core line vectorization method for quantum fluids enabling real-time visualization of high-resolution quantum turbulence structure. From a dataset obtained through simulation, our technique first identifies vortex nodes based on the circulation field. To vectorize the vortex-core lines interpolating these vortex nodes, we propose a novel graph-based data structure, with iterative graph reduction and density-guided local optimization, to locate sub-grid-scale vortex-core line samples more precisely, which are then vectorized by continuous curves. This vortex-core representation naturally captures complex topology, such as branching during reconnection. Our vectorization approach reduces memory consumption by orders of magnitude, enabling real-time visualization performance. Different types of interactive visualizations are demonstrated to show the effectiveness of our technique, which could help further research on quantum turbulence.},
  archive      = {J_TVCG},
  author       = {Daoming Liu and Chi Xiong and Xiaopei Liu},
  doi          = {10.1109/TVCG.2020.2981460},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3794-3807},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vectorizing quantum turbulence vortex-core lines for real-time visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector field decompositions using multiscale poisson kernel.
<em>TVCG</em>, <em>27</em>(9), 3781–3793. (<a
href="https://doi.org/10.1109/TVCG.2020.2984413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extraction of multiscale features using scale-space is one of the fundamental approaches to analyze scalar fields. However, similar techniques for vector fields are much less common, even though it is well known that, for example, turbulent flows contain cascades of nested vortices at different scales. The challenge is that the ideas related to scale-space are based upon iteratively smoothing the data to extract features at progressively larger scale, making it difficult to extract overlapping features. Instead, we consider spatial regions of influence in vector fields as scale, and introduce a new approach for the multiscale analysis of vector fields. Rather than smoothing the flow, we use the natural Helmholtz-Hodge decomposition to split it into small-scale and large-scale components using progressively larger neighborhoods. Our approach creates a natural separation of features by extracting local flow behavior, for example, a small vortex, from large-scale effects, for example, a background flow. We demonstrate our technique on large-scale, turbulent flows, and show multiscale features that cannot be extracted using state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Harsh Bhatia and Robert M. Kirby and Valerio Pascucci and Peer-Timo Bremer},
  doi          = {10.1109/TVCG.2020.2984413},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3781-3793},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vector field decompositions using multiscale poisson kernel},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TopoTag: A robust and scalable topological fiducial marker
system. <em>TVCG</em>, <em>27</em>(9), 3769–3780. (<a
href="https://doi.org/10.1109/TVCG.2020.2988466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fiducial markers have been playing an important role in augmented reality (AR), robot navigation, and general applications where the relative pose between a camera and an object is required. Here we introduce TopoTag, a robust and scalable topological fiducial marker system, which supports reliable and accurate pose estimation from a single image. TopoTag uses topological and geometrical information in marker detection to achieve higher robustness. Topological information is extensively used for 2D marker detection, and further corresponding geometrical information for ID decoding. Robust 3D pose estimation is achieved by taking advantage of all TopoTag vertices. Without sacrificing bits for higher recall and precision like previous systems, TopoTag can use full bits for ID encoding. TopoTag supports tens of thousands unique IDs and easily extends to millions of unique tags resulting in massive scalability. We collected a large test dataset including in total 169,713 images for evaluation, involving in-plane and out-of-plane rotation, image blur, different distances, and various backgrounds, etc. Experiments on the dataset and real indoor and outdoor scene tests with a rolling shutter camera both show that TopoTag significantly outperforms previous fiducial marker systems in terms of various metrics, including detection accuracy, vertex jitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion as long as the main tag topological structure is maintained and allows for flexible shape design where users can customize internal and external marker shapes. Code for our marker design/generation, marker detection, and dataset are available at http://herohuyongtao.github.io/research/publications/topo-tag/.},
  archive      = {J_TVCG},
  author       = {Guoxing Yu and Yongtao Hu and Jingwen Dai},
  doi          = {10.1109/TVCG.2020.2988466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3769-3780},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TopoTag: A robust and scalable topological fiducial marker system},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The importance of phase to texture discrimination and
similarity. <em>TVCG</em>, <em>27</em>(9), 3755–3768. (<a
href="https://doi.org/10.1109/TVCG.2020.2981063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the importance of phase for texture discrimination and similarity estimation tasks. We first use two psychophysical experiments to investigate the relative importance of phase and magnitude spectra for human texture discrimination and similarity estimation. The results show that phase is more important to humans for both tasks. We further examine the ability of 51 computational feature sets to perform these two tasks. In contrast with the psychophysical experiments, it is observed that the magnitude data is more important to these computational feature sets than the phase data. We hypothesise that this inconsistency is due to the difference between the abilities of humans and the computational feature sets to utilise phase data. This motivates us to investigate the application of the 51 feature sets to phase-only images in addition to their use on the original data set. This investigation is extended to exploit Convolutional Neural Network (CNN) features. The results show that our feature fusion scheme improves the average performance of those feature sets for estimating humans&#39; perceptual texture similarity. The superior performance should be attributed to the importance of phase to texture similarity.},
  archive      = {J_TVCG},
  author       = {Xinghui Dong and Ying Gao and Junyu Dong and Mike J. Chantler},
  doi          = {10.1109/TVCG.2020.2981063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3755-3768},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The importance of phase to texture discrimination and similarity},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sketch-R2CNN: An RNN-rasterization-CNN architecture for
vector sketch recognition. <em>TVCG</em>, <em>27</em>(9), 3745–3754. (<a
href="https://doi.org/10.1109/TVCG.2020.2987626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches in existing large-scale datasets like the recent QuickDraw collection are often stored in a vector format, with strokes consisting of sequentially sampled points. However, most existing sketch recognition methods rasterize vector sketches as binary images and then adopt image classification techniques. In this article, we propose a novel end-to-end single-branch network architecture RNN-Rasterization-CNN (Sketch-R2CNN for short) to fully leverage the vector format of sketches for recognition. Sketch-R2CNN takes a vector sketch as input and uses an RNN for extracting per-point features in the vector space. We then develop a neural line rasterization module to convert the vector sketch and the per-point features to multi-channel point feature maps, which are subsequently fed to a CNN for extracting convolutional features in the pixel space. Our neural line rasterization module is designed in a differentiable way for end-to-end learning. We perform experiments on existing large-scale sketch recognition datasets and show that the RNN-Rasterization design brings consistent improvement over CNN baselines and that Sketch-R2CNN substantially outperforms the state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Lei Li and Changqing Zou and Youyi Zheng and Qingkun Su and Hongbo Fu and Chiew-Lan Tai},
  doi          = {10.1109/TVCG.2020.2987626},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3745-3754},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch-R2CNN: An RNN-rasterization-CNN architecture for vector sketch recognition},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QuadStack: An efficient representation and direct rendering
of layered datasets. <em>TVCG</em>, <em>27</em>(9), 3733–3744. (<a
href="https://doi.org/10.1109/TVCG.2020.2981565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce QuadStack, a novel algorithm for volumetric data compression and direct rendering. Our algorithm exploits the data redundancy often found in layered datasets which are common in science and engineering fields such as geology, biology, mechanical engineering, medicine, etc. QuadStack first compresses the volumetric data into vertical stacks which are then compressed into a quadtree that identifies and represents the layered structures at the internal nodes. The associated data (color, material, density, etc.) and shape of these layer structures are decoupled and encoded independently, leading to high compression rates (4× to 54× of the original voxel model memory footprint in our experiments). We also introduce an algorithm for value retrieving from the QuadStack representation and we show that the access has logarithmic complexity. Because of the fast access, QuadStack is suitable for efficient data representation and direct rendering. We show that our GPU implementation performs comparably in speed with the state-of-the-art algorithms (18-79 MRays/s in our implementation), while maintaining a significantly smaller memory footprint.},
  archive      = {J_TVCG},
  author       = {Alejandro Graciano and Antonio J. Rueda and Adam Pospíšil and Jiří Bittner and Bedrich Benes},
  doi          = {10.1109/TVCG.2020.2981565},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3733-3744},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {QuadStack: An efficient representation and direct rendering of layered datasets},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LADV: Deep learning assisted authoring of dashboard
visualizations from images and sketches. <em>TVCG</em>, <em>27</em>(9),
3717–3732. (<a href="https://doi.org/10.1109/TVCG.2020.2980227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dashboard visualizations are widely used in data-intensive applications such as business intelligence, operation monitoring, and urban planning. However, existing visualization authoring tools are inefficient in the rapid prototyping of dashboards because visualization expertise and user intention need to be integrated. We propose a novel approach to rapid conceptualization that can construct dashboard templates from exemplars to mitigate the burden of designing, implementing, and evaluating dashboard visualizations. The kernel of our approach is a novel deep learning-based model that can identify and locate charts of various categories and extract colors from an input image or sketch. We design and implement a web-based authoring tool for learning, composing, and customizing dashboard visualizations in a cloud computing environment. Examples, user studies, and user feedback from real scenarios in Alibaba Cloud verify the usability and efficiency of the proposed approach.},
  archive      = {J_TVCG},
  author       = {Ruixian Ma and Honghui Mei and Huihua Guan and Wei Huang and Fan Zhang and Chengye Xin and Wenzhuo Dai and Xiao Wen and Wei Chen},
  doi          = {10.1109/TVCG.2020.2980227},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3717-3732},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LADV: Deep learning assisted authoring of dashboard visualizations from images and sketches},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive graph construction for graph-based
semi-supervised learning. <em>TVCG</em>, <em>27</em>(9), 3701–3716. (<a
href="https://doi.org/10.1109/TVCG.2021.3084694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) provides a way to improve the performance of prediction models (e.g., classifier) via the usage of unlabeled samples. An effective and widely used method is to construct a graph that describes the relationship between labeled and unlabeled samples. Practical experience indicates that graph quality significantly affects the model performance. In this paper, we present a visual analysis method that interactively constructs a high-quality graph for better model performance. In particular, we propose an interactive graph construction method based on the large margin principle. We have developed a river visualization and a hybrid visualization that combines a scatterplot, a node-link diagram, and a bar chart to convey the label propagation of graph-based SSL. Based on the understanding of the propagation, a user can select regions of interest to inspect and modify the graph. We conducted two case studies to showcase how our method facilitates the exploitation of labeled and unlabeled samples for improving model performance.},
  archive      = {J_TVCG},
  author       = {Changjian Chen and Zhaowei Wang and Jing Wu and Xiting Wang and Lan-Zhe Guo and Yu-Feng Li and Shixia Liu},
  doi          = {10.1109/TVCG.2021.3084694},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3701-3716},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive graph construction for graph-based semi-supervised learning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DPVis: Visual analytics with hidden markov models for
disease progression pathways. <em>TVCG</em>, <em>27</em>(9), 3685–3700.
(<a href="https://doi.org/10.1109/TVCG.2020.2985689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical researchers use disease progression models to understand patient status and characterize progression patterns from longitudinal health records. One approach for disease progression modeling is to describe patient status using a small number of states that represent distinctive distributions over a set of observed measures. Hidden Markov models (HMMs) and its variants are a class of models that both discover these states and make inferences of health states for patients. Despite the advantages of using the algorithms for discovering interesting patterns, it still remains challenging for medical experts to interpret model outputs, understand complex modeling parameters, and clinically make sense of the patterns. To tackle these problems, we conducted a design study with clinical scientists, statisticians, and visualization experts, with the goal to investigate disease progression pathways of chronic diseases, namely type 1 diabetes (T1D), Huntington&#39;s disease, Parkinson&#39;s disease, and chronic obstructive pulmonary disease (COPD). As a result, we introduce DPVis which seamlessly integrates model parameters and outcomes of HMMs into interpretable and interactive visualizations. In this article, we demonstrate that DPVis is successful in evaluating disease progression models, visually summarizing disease states, interactively exploring disease progression patterns, and building, analyzing, and comparing clinically relevant patient subgroups.},
  archive      = {J_TVCG},
  author       = {Bum Chul Kwon and Vibha Anand and Kristen A. Severson and Soumya Ghosh and Zhaonan Sun and Brigitte I. Frohnert and Markus Lundgren and Kenney Ng},
  doi          = {10.1109/TVCG.2020.2985689},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3685-3700},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DPVis: Visual analytics with hidden markov models for disease progression pathways},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed interactive visualization using GPU-optimized
spark. <em>TVCG</em>, <em>27</em>(9), 3670–3684. (<a
href="https://doi.org/10.1109/TVCG.2020.2990894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of advances in imaging and computing technologies, large-scale data acquisition and processing have become commonplace in many science and engineering disciplines. Conventional workflows for large-scale data processing usually rely on in-house or commercial software that are designed for domain-specific computing tasks. Recent advances in MapReduce, which was originally developed for batch processing textual data via a simplified programming model of the map and reduce functions, have expanded its applications to more general tasks in big-data processing, such as scientific computing, and biomedical image processing. However, as shown in previous work, volume rendering and visualization using MapReduce is still considered challenging and impractical owing to the disk-based, batch-processing nature of its computing model. In this article, contrary to this common belief, we show that the MapReduce computing model can be effectively used for interactive visualization. Our proposed system is a novel extension of Spark, one of the most popular open-source MapReduce frameworks, which offers GPU-accelerated MapReduce computing. To minimize CPU-GPU communication and overcome slow, disk-based shuffle performance, the proposed system supports GPU in-memory caching and MPI-based direct communication between compute nodes. To allow for GPU-accelerated in-situ visualization using raster graphics in Spark, we leveraged the CUDA-OpenGL interoperability, resulting in faster processing speeds by several orders of magnitude compared to conventional MapReduce systems. We demonstrate the performance of our system via several volume processing and visualization tasks, such as direct volume rendering, iso-surface extraction, and numerical simulations with in-situ visualization.},
  archive      = {J_TVCG},
  author       = {Sumin Hong and Junyoung Choi and Won-Ki Jeong},
  doi          = {10.1109/TVCG.2020.2990894},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3670-3684},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Distributed interactive visualization using GPU-optimized spark},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dataless sharing of interactive visualization.
<em>TVCG</em>, <em>27</em>(9), 3656–3669. (<a
href="https://doi.org/10.1109/TVCG.2020.2984708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive visualization has become a powerful insight-revealing medium. However, the close dependency of interactive visualization on its data inhibits its shareability. Users have to choose between the two extremes of (i) sharing non-interactive dataless formats such as images and videos, or (ii) giving access to their data and software to others with no control over how the data will be used. In this work, we fill the gap between the two extremes and present a new system, called Loom. Loom captures interactive visualizations as standalone dataless objects. Users can interact with Loom objects as if they still have the original software and data that created those visualizations. Yet, Loom objects are completely independent and can therefore be shared online without requiring the data or the visualization software. Loom objects are efficient to store and use, and provide privacy preserving mechanisms. We demonstrate Loom&#39;s efficacy with examples of scientific visualization using Paraview, information visualization using Tableau, and journalistic visualization from New York Times.},
  archive      = {J_TVCG},
  author       = {Mohammad Raji and Jeremiah Duncan and Tanner Hobson and Jian Huang},
  doi          = {10.1109/TVCG.2020.2984708},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3656-3669},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dataless sharing of interactive visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ArchiText: Interactive hierarchical topic modeling.
<em>TVCG</em>, <em>27</em>(9), 3644–3655. (<a
href="https://doi.org/10.1109/TVCG.2020.2981456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-in-the-loop topic modeling allows users to explore and steer the process to produce better quality topics that align with their needs. When integrated into visual analytic systems, many existing automated topic modeling algorithms are given interactive parameters to allow users to tune or adjust them. However, this has limitations when the algorithms cannot be easily adapted to changes, and it is difficult to realize interactivity closely supported by underlying algorithms. Instead, we emphasize the concept of tight integration, which advocates for the need to co-develop interactive algorithms and interactive visual analytic systems in parallel to allow flexibility and scalability. In this article, we describe design goals for efficiently and effectively executing the concept of tight integration among computation, visualization, and interaction for hierarchical topic modeling of text data. We propose computational base operations for interactive tasks to achieve the design goals. To instantiate our concept, we present ArchiText, a prototype system for interactive hierarchical topic modeling, which offers fast, flexible, and algorithmically valid analysis via tight integration. Utilizing interactive hierarchical topic modeling, our technique lets users generate, explore, and flexibly steer hierarchical topics to discover more informed topics and their document memberships.},
  archive      = {J_TVCG},
  author       = {Hannah Kim and Barry Drake and Alex Endert and Haesun Park},
  doi          = {10.1109/TVCG.2020.2981456},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3644-3655},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ArchiText: Interactive hierarchical topic modeling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AgentVis: Visual analysis of agent behavior with
hierarchical glyphs. <em>TVCG</em>, <em>27</em>(9), 3626–3643. (<a
href="https://doi.org/10.1109/TVCG.2020.2985923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glyphs representing complex behavior provide a useful and common means of visualizing multivariate data. However, due to their complex shape, overlapping, and occlusion of glyphs is a common and prominent limitation. This limits the number of discreet data tuples that can be displayed in a given image. Using a real-world application, glyphs are used to depict agent behavior in a call center. However, many call centers feature thousands of agents. A standard approach representing thousands of agents with glyphs does not scale. To accommodate the visualization incorporating thousands of glyphs we develop clustering of overlapping glyphs into a single parent glyph. This hierarchical glyph represents the mean value of all child agent glyphs, removing overlap and reduTcing visual clutter. Multi-variate clustering techniques are explored and developed in collaboration with domain experts in the call center industry. We implement dynamic control of glyph clusters according to zoom level and customized distance metrics, to utilize image space with reduced overplotting and cluttering. We demonstrate our technique with examples and a usage scenario using real-world call-center data to visualize thousands of call center agents, revealing insight into their behavior and reporting feedback from expert call-center analysts.},
  archive      = {J_TVCG},
  author       = {Dylan Rees and Robert S. Laramee and Paul Brookes and Tony D&#39;Cruze and Gary A. Smith and Aslam Miah},
  doi          = {10.1109/TVCG.2020.2985923},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3626-3643},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AgentVis: Visual analysis of agent behavior with hierarchical glyphs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple video-based technique for measuring latency in
virtual reality or teleoperation. <em>TVCG</em>, <em>27</em>(9),
3611–3625. (<a href="https://doi.org/10.1109/TVCG.2020.2980527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designers of virtual reality (VR) systems are aware of the need to minimize delays between the user&#39;s tracked physical actions and the consequent displayed actions in the virtual environment. Such delays, also referred to as end-to-end latency, are known to degrade user performance and even cause simulator sickness. Though a wide variety of hardware and software design strategies have been used to reduce delays, techniques for measuring and minimizing latency continue to be needed since transmission and switching delays are likely to continue to introduce new sources of latency, especially in wireless mobile environments. This article describes a convenient low-cost technique for measuring end-to-end latencies using a human evaluator and an ordinary consumer camera (e.g., cell phone camera). Since the technique does not depend upon the use of specialized hardware and software, it differs from other methods in that it can easily be used to measure latencies of systems in the specific hardware and software configuration and the relevant performance environments. The achievable measurement accuracy was assessed in an experimental trial. Results indicate a measurement uncertainty below 10 ms. Some refinements to the technique are discussed, which may further reduce the measurement uncertainty to approximately 1 ms.},
  archive      = {J_TVCG},
  author       = {Ilja T. Feldstein and Stephen R. Ellis},
  doi          = {10.1109/TVCG.2020.2980527},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3611-3625},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A simple video-based technique for measuring latency in virtual reality or teleoperation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 4D light field segmentation from light field super-pixel
hypergraph representation. <em>TVCG</em>, <em>27</em>(9), 3597–3610. (<a
href="https://doi.org/10.1109/TVCG.2020.2982158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate segmentation of full 4D light fields is an important task in computer vision and computer graphics. The massive volume and the redundancy of light fields make it an open challenge. In this article, we propose a novel light field hypergraph (LFHG) representation using the light field super-pixel (LFSP) for interactive light field segmentation. The LFSPs not only maintain the light field spatio-angular consistency, but also greatly contribute to the hypergraph coarsening. These advantages make LFSPs useful to improve segmentation performance. Based on the LFHG representation, we present an efficient light field segmentation algorithm via graph-cut optimization. Experimental results on both synthetic and real scene data demonstrate that our method outperforms state-of-the-art methods on the light field segmentation task with respect to both accuracy and efficiency.},
  archive      = {J_TVCG},
  author       = {Xianqiang Lv and Xue Wang and Qing Wang and Jingyi Yu},
  doi          = {10.1109/TVCG.2020.2982158},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {9},
  pages        = {3597-3610},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {4D light field segmentation from light field super-pixel hypergraph representation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unordered task-parallel augmented merge tree construction.
<em>TVCG</em>, <em>27</em>(8), 3585–3596. (<a
href="https://doi.org/10.1109/TVCG.2021.3076875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary scientific data sets require fast and scalable topological analysis to enable visualization, simplification and interaction. Within this field, parallel merge tree construction has seen abundant recent contributions, with a trend of decentralized, task-parallel or SMP-oriented algorithms dominating in terms of total runtime. However, none of these recent approaches computed complete merge trees on distributed systems, leaving this field to traditional divide &amp; conquer approaches. This article introduces a scalable, parallel and distributed algorithm for merge tree construction outperforming the previously fastest distributed solution by a factor of around three. This is achieved by a task-parallel identification of individual merge tree arcs by growing regions around critical points in the data, without any need for ordered progression or global data structures, based on a novel insight introducing a sufficient local boundary for region growth.},
  archive      = {J_TVCG},
  author       = {Kilian Werner and Christoph Garth},
  doi          = {10.1109/TVCG.2021.3076875},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3585-3596},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unordered task-parallel augmented merge tree construction},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SplitStreams: A visual metaphor for evolving hierarchies.
<em>TVCG</em>, <em>27</em>(8), 3571–3584. (<a
href="https://doi.org/10.1109/TVCG.2020.2973564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visualization of hierarchically structured data over time is an ongoing challenge and several approaches exist trying to solve it. Techniques such as animated or juxtaposed tree visualizations are not capable of providing a good overview of the time series and lack expressiveness in conveying changes over time. Nested streamgraphs provide a better understanding of the data evolution, but lack the clear outline of hierarchical structures at a given timestep. Furthermore, these approaches are often limited to static hierarchies or exclude complex hierarchical changes in the data, limiting their use cases. We propose a novel visual metaphor capable of providing a static overview of all hierarchical changes over time, as well as clearly outlining the hierarchical structure at each individual time step. Our method allows for smooth transitions between treemaps and nested streamgraphs, enabling the exploration of the trade-off between dynamic behavior and hierarchical structure. As our technique handles topological changes of all types, it is suitable for a wide range of applications. We demonstrate the utility of our method on several use cases, evaluate it with a user study, and provide its full source code.},
  archive      = {J_TVCG},
  author       = {Fabian Bolte and Mahsan Nourani and Eric D. Ragan and Stefan Bruckner},
  doi          = {10.1109/TVCG.2020.2973564},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3571-3584},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SplitStreams: A visual metaphor for evolving hierarchies},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sketch augmentation-driven shape retrieval learning
framework based on convolutional neural networks. <em>TVCG</em>,
<em>27</em>(8), 3558–3570. (<a
href="https://doi.org/10.1109/TVCG.2020.2975504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a deep learning approach to sketch-based shape retrieval that incorporates a few novel techniques to improve the quality of the retrieval results. First, to address the problem of scarcity of training sketch data, we present a sketch augmentation method that more closely mimics human sketches compared to simple image transformation. Our method generates more sketches from the existing training data by (i) removing a stroke, (ii) adjusting a stroke, and (iii) rotating the sketch. As such, we generate a large number of sketch samples for training our neural network. Second, we obtain the 2D renderings of each 3D model in the shape database by determining the view positions that best depict the 3D shape: i.e., avoiding self-occlusion, showing the most salient features, and following how a human would normally sketch the model. We use a convolutional neural network (CNN) to learn the best viewing positions of each 3D model and generates their 2D images for the next step. Third, our method uses a cross-domain learning strategy based on two Siamese CNNs that pair up sketches and the 2D shape images. A joint Bayesian measure is used to measure the output similarity from these CNNs to maximize inter-class similarity and minimize intra-class similarity. Extensive experiments show that our proposed approach comprehensively outperforms many existing state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Wen Zhou and Jinyuan Jia and Wenying Jiang and Chenxi Huang},
  doi          = {10.1109/TVCG.2020.2975504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3558-3570},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sketch augmentation-driven shape retrieval learning framework based on convolutional neural networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RIAS: Repeated invertible averaging for surface
multiresolution of arbitrary degree. <em>TVCG</em>, <em>27</em>(8),
3546–3557. (<a href="https://doi.org/10.1109/TVCG.2020.2972877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce two local surface averaging operators with local inverses and use them to devise a method for surface multiresolution (subdivision and reverse subdivision) of arbitrary degree. Similar to previous works by Stam, Zorin, and Schröder that achieved forward subdivision only, our averaging operators involve only direct neighbours of a vertex, and can be configured to generalize B-Spline multiresolution to arbitrary topology surfaces. Our subdivision surfaces are hence able to exhibit C d continuity at regular vertices (for arbitrary values of d ) and appear to exhibit C 1 continuity at extraordinary vertices. Smooth reverse and non-uniform subdivisions are additionally supported.},
  archive      = {J_TVCG},
  author       = {Troy Alderson and Ali Mahdavi-Amiri and Faramarz Samavati},
  doi          = {10.1109/TVCG.2020.2972877},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3546-3557},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RIAS: Repeated invertible averaging for surface multiresolution of arbitrary degree},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed reality tabletop gameplay: Social interaction with a
virtual human capable of physical influence. <em>TVCG</em>,
<em>27</em>(8), 3534–3545. (<a
href="https://doi.org/10.1109/TVCG.2019.2959575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the effects of the physical influence of a virtual human (VH) in the context of face-to-face interaction in a mixed reality environment. In Experiment 1, participants played a tabletop game with a VH, in which each player takes a turn and moves their own token along the designated spots on the shared table. We compared two conditions as follows: the VH in the virtual condition moves a virtual token that can only be seen through augmented reality (AR) glasses, while the VH in the physical condition moves a physical token as the participants do; therefore the VH’s token can be seen even in the periphery of the AR glasses. For the physical condition, we designed an actuator system underneath the table. The actuator moves a magnet under the table which then moves the VH’s physical token over the surface of the table. Our results indicate that participants felt higher co-presence with the VH in the physical condition, and participants assessed the VH as a more physical entity compared to the VH in the virtual condition. We further observed transference effects when participants attributed the VH’s ability to move physical objects to other elements in the real world. Also, the VH’s physical influence improved participants’ overall experience with the VH. In Experiment 2, we further looked into the question how the physical-virtual latency in movements affected the perceived plausibility of the VH’s interaction with the real world. Our results indicate that a slight temporal difference between the physical token reacting to the virtual hand’s movement increased the perceived realism and causality of the mixed reality interaction. We discuss potential explanations for the findings and implications for future shared mixed reality tabletop setups.},
  archive      = {J_TVCG},
  author       = {Myungho Lee and Nahal Norouzi and Gerd Bruder and Pamela J. Wisniewski and Gregory F. Welch},
  doi          = {10.1109/TVCG.2019.2959575},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3534-3545},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mixed reality tabletop gameplay: Social interaction with a virtual human capable of physical influence},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interweaving multimodal interaction with flexible unit
visualizations for data exploration. <em>TVCG</em>, <em>27</em>(8),
3519–3533. (<a href="https://doi.org/10.1109/TVCG.2020.2978050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal interfaces that combine direct manipulation and natural language have shown great promise for data visualization. Such multimodal interfaces allow people to stay in the flow of their visual exploration by leveraging the strengths of one modality to complement the weaknesses of others. In this article, we introduce an approach that interweaves multimodal interaction combining direct manipulation and natural language with flexible unit visualizations. We employ the proposed approach in a proof-of-concept system, DataBreeze. Coupling pen, touch, and speech-based multimodal interaction with flexible unit visualizations, DataBreeze allows people to create and interact with both systematically bound (e.g., scatterplots, unit column charts) and manually customized views, enabling a novel visual data exploration experience. We describe our design process along with DataBreeze&#39;s interface and interactions, delineating specific aspects of the design that empower the synergistic use of multiple modalities. We also present a preliminary user study with DataBreeze, highlighting the data exploration patterns that participants employed. Finally, reflecting on our design process and preliminary user study, we discuss future research directions.},
  archive      = {J_TVCG},
  author       = {Arjun Srinivasan and Bongshin Lee and John Stasko},
  doi          = {10.1109/TVCG.2020.2978050},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3519-3533},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interweaving multimodal interaction with flexible unit visualizations for data exploration},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive focus+context rendering for hexahedral mesh
inspection. <em>TVCG</em>, <em>27</em>(8), 3505–3518. (<a
href="https://doi.org/10.1109/TVCG.2021.3074607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual inspection of a hexahedral mesh with respect to element quality is difficult due to clutter and occlusions that are produced when rendering all element faces or their edges simultaneously. Current approaches overcome this problem by using focus on specific elements that are then rendered opaque, and carving away all elements occluding their view. In this work, we make use of advanced GPU shader functionality to generate a focus+context rendering that highlights the elements in a selected region and simultaneously conveys the global mesh structure and deformation field. To achieve this, we propose a gradual transition from edge-based focus rendering to volumetric context rendering, by combining fragment shader-based edge and face rendering with per-pixel fragment lists. A fragment shader smoothly transitions between wireframe and face-based rendering, including focus-dependent rendering style and depth-dependent edge thickness and halos, and per-pixel fragment lists are used to blend fragments in correct visibility order. To maintain the global mesh structure in the context regions, we propose a new method to construct a sheet-based level-of-detail hierarchy and smoothly blend it with volumetric information. The user guides the exploration process by moving a lens-like hotspot. Since all operations are performed on the GPU, interactive frame rates are achieved even for large meshes.},
  archive      = {J_TVCG},
  author       = {Christoph Neuhauser and Junpeng Wang and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2021.3074607},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3505-3518},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive Focus+Context rendering for hexahedral mesh inspection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HyperLabels: Browsing of dense and hierarchical molecular 3D
models. <em>TVCG</em>, <em>27</em>(8), 3493–3504. (<a
href="https://doi.org/10.1109/TVCG.2020.2975583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for the browsing of hierarchical 3D models in which we combine the typical navigation of hierarchical structures in a 2D environment—using clicks on nodes, links, or icons—with a 3D spatial data visualization. Our approach is motivated by large molecular models, for which the traditional single-scale navigational metaphors are not suitable. Multi-scale phenomena, e. g., in astronomy or geography, are complex to navigate due to their large data spaces and multi-level organization. Models from structural biology are in addition also densely crowded in space and scale. Cutaways are needed to show individual model subparts. The camera has to support exploration on the level of a whole virus, as well as on the level of a small molecule. We address these challenges by employing HyperLabels: active labels that—in addition to their annotational role—also support user interaction. Clicks on HyperLabels select the next structure to be explored. Then, we adjust the visualization to showcase the inner composition of the selected subpart and enable further exploration. Finally, we use a breadcrumbs panel for orientation and as a mechanism to traverse upwards in the model hierarchy. We demonstrate our concept of hierarchical 3D model browsing using two exemplary models from meso-scale biology.},
  archive      = {J_TVCG},
  author       = {David Kouřil and Tobias Isenberg and Barbora Kozlíková and Miriah Meyer and M. Eduard Gröller and Ivan Viola},
  doi          = {10.1109/TVCG.2020.2975583},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3493-3504},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HyperLabels: Browsing of dense and hierarchical molecular 3D models},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GLoG: Laplacian of gaussian for spatial pattern detection in
spatio-temporal data. <em>TVCG</em>, <em>27</em>(8), 3481–3492. (<a
href="https://doi.org/10.1109/TVCG.2020.2978847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundary detection has long been a fundamental tool for image processing and computer vision, supporting the analysis of static and time-varying data. In this work, we built upon the theory of Graph Signal Processing to propose a novel boundary detection filter in the context of graphs, having as main application scenario the visual analysis of spatio-temporal data. More specifically, we propose the equivalent for graphs of the so-called Laplacian of Gaussian edge detection filter, which is widely used in image processing. The proposed filter is able to reveal interesting spatial patterns while still enabling the definition of entropy of time slices. The entropy reveals the degree of randomness of a time slice, helping users to identify expected and unexpected phenomena over time. The effectiveness of our approach appears in applications involving synthetic and real data sets, which show that the proposed methodology is able to uncover interesting spatial and temporal phenomena. The provided examples and case studies make clear the usefulness of our approach as a mechanism to support visual analytic tasks involving spatio-temporal data.},
  archive      = {J_TVCG},
  author       = {Luis Gustavo Nonato and Fabiano Petronetto do Carmo and Claudio T. Silva},
  doi          = {10.1109/TVCG.2020.2978847},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3481-3492},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GLoG: Laplacian of gaussian for spatial pattern detection in spatio-temporal data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FTK: A simplicial spacetime meshing framework for robust and
scalable feature tracking. <em>TVCG</em>, <em>27</em>(8), 3463–3480. (<a
href="https://doi.org/10.1109/TVCG.2021.3073399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Feature Tracking Kit (FTK), a framework that simplifies, scales, and delivers various feature-tracking algorithms for scientific data. The key of FTK is our simplicial spacetime meshing scheme that generalizes both regular and unstructured spatial meshes to spacetime while tessellating spacetime mesh elements into simplices. The benefits of using simplicial spacetime meshes include (1) reducing ambiguity cases for feature extraction and tracking, (2) simplifying the handling of degeneracies using symbolic perturbations, and (3) enabling scalable and parallel processing. The use of simplicial spacetime meshing simplifies and improves the implementation of several feature-tracking algorithms for critical points, quantum vortices, and isosurfaces. As a software framework, FTK provides end users with VTK/ParaView filters, Python bindings, a command line interface, and programming interfaces for feature-tracking applications. We demonstrate use cases as well as scalability studies through both synthetic data and scientific applications including tokamak, fluid dynamics, and superconductivity simulations. We also conduct end-to-end performance studies on the Summit supercomputer. FTK is open sourced under the MIT license: https://github.com/hguo/ftk.},
  archive      = {J_TVCG},
  author       = {Hanqi Guo and David Lenz and Jiayi Xu and Xin Liang and Wenbin He and Iulian R. Grindeanu and Han-Wei Shen and Tom Peterka and Todd Munson and Ian Foster},
  doi          = {10.1109/TVCG.2021.3073399},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3463-3480},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FTK: A simplicial spacetime meshing framework for robust and scalable feature tracking},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design exposition discussion documents for rich design
discourse in applied visualization. <em>TVCG</em>, <em>27</em>(8),
3451–3462. (<a href="https://doi.org/10.1109/TVCG.2020.2979433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present and report on Design Exposition Discussion Documents (DExDs), a new means of fostering collaboration between visualization designers and domain experts in applied visualization research. DExDs are a collection of semi-interactive web-based documents used to promote design discourse: to communicate new visualization designs, and their underlying rationale, and to elicit feedback and new design ideas. Developed and applied during a four-year visual data analysis project in criminal intelligence, these documents enabled a series of visualization re-designs to be explored by crime analysts remotely - in a flexible and authentic way. The DExDs were found to engender a level of engagement that is qualitatively distinct from more traditional methods of feedback elicitation, supporting the kind of informed, iterative and design-led feedback that is core to applied visualization research. They also offered a solution to limited and intermittent contact between analyst and visualization researcher and began to address more intractable deficiencies, such as social desirability-bias, common to applied visualization projects. Crucially, DExDs conferred to domain experts greater agency over the design process - collaborators proposed design suggestions, justified with design knowledge, that directly influenced the re-redesigns. We provide context that allows the contributions to be transferred to a range of settings.},
  archive      = {J_TVCG},
  author       = {Roger Beecham and Jason Dykes and Chris Rooney and William Wong},
  doi          = {10.1109/TVCG.2020.2979433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3451-3462},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design exposition discussion documents for rich design discourse in applied visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DecorIn: An automatic method for plane-based decorating.
<em>TVCG</em>, <em>27</em>(8), 3438–3450. (<a
href="https://doi.org/10.1109/TVCG.2020.2972897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing demand for interior design and decorating. The main challenges are where to put the objects and how to put them plausibly in the given domain. In this article, we propose an automatic method for decorating the planes in a given image. We call it Decoration In (DecorIn for short). Given an image, we first extract planes as decorating candidates according to the estimated geometric features. Then we parameterize the planes with an orthogonal and semantically consistent grid. Finally, we compute the position for the decoration, i.e., a decoration box, on the plane by an example-based decorating method which can describe the partial image and compute the similarity between partial scenes. We have conducted comprehensive evaluations and demonstrate our method on a number of applications. Our method is more efficient both in time and economic than generating a layout from scratch.},
  archive      = {J_TVCG},
  author       = {Yuan Liang and Lubin Fan and Peiran Ren and Xuansong Xie and Xian-Sheng Hua},
  doi          = {10.1109/TVCG.2020.2972897},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3438-3450},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DecorIn: An automatic method for plane-based decorating},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conceptual model of visual analytics for hands-on
cybersecurity training. <em>TVCG</em>, <em>27</em>(8), 3425–3437. (<a
href="https://doi.org/10.1109/TVCG.2020.2977336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hands-on training is an effective way to practice theoretical cybersecurity concepts and increase participants&#39; skills. In this article, we discuss the application of visual analytics principles to the design, execution, and evaluation of training sessions. We propose a conceptual model employing visual analytics that supports the sensemaking activities of users involved in various phases of the training life cycle. The model emerged from our long-term experience in designing and organizing diverse hands-on cybersecurity training sessions. It provides a classification of visualizations and can be used as a framework for developing novel visualization tools supporting phases of the training life-cycle. We demonstrate the model application on examples covering two types of cybersecurity training programs.},
  archive      = {J_TVCG},
  author       = {Radek Ošlejšek and Vít Rusňák and Karolína Burská and Valdemar Švábenský and Jan Vykopal and Jakub Čegan},
  doi          = {10.1109/TVCG.2020.2977336},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3425-3437},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Conceptual model of visual analytics for hands-on cybersecurity training},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Characterizing the quality of insight by interactions: A
case study. <em>TVCG</em>, <em>27</em>(8), 3410–3424. (<a
href="https://doi.org/10.1109/TVCG.2020.2977634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the quality of insight has become increasingly important with the trend of allowing users to post comments during visual exploration, yet approaches for qualifying insight are rare. This article presents a case study to investigate the possibility of characterizing the quality of insight via the interactions performed. To do this, we devised the interaction of a visualization tool-MediSyn-for insight generation. MediSyn supports five types of interactions: selecting, connecting, elaborating, exploring, and sharing. We evaluated MediSyn with 14 participants by allowing them to freely explore the data and generate insights. We then extracted seven interaction patterns from their interaction logs and correlated the patterns to four aspects of insight quality. The results show the possibility of qualifying insights via interactions. Among other findings, exploration actions can lead to unexpected insights; the drill-down pattern tends to increase the domain values of insights. A qualitative analysis shows that using domain knowledge to guide exploration can positively affect the domain value of derived insights. We discuss the study&#39;s implications, lessons learned, and future research opportunities.},
  archive      = {J_TVCG},
  author       = {Chen He and Luana Micallef and Liye He and Gopal Peddinti and Tero Aittokallio and Giulio Jacucci},
  doi          = {10.1109/TVCG.2020.2977634},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3410-3424},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Characterizing the quality of insight by interactions: A case study},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Can visualization alleviate dichotomous thinking? Effects of
visual representations on the cliff effect. <em>TVCG</em>,
<em>27</em>(8), 3397–3409. (<a
href="https://doi.org/10.1109/TVCG.2021.3073466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common reporting styles for statistical results in scientific articles, such as p -values and confidence intervals (CI), have been reported to be prone to dichotomous interpretations, especially with respect to the null hypothesis significance testing framework. For example when the p -value is small enough or the CIs of the mean effects of a studied drug and a placebo are not overlapping, scientists tend to claim significant differences while often disregarding the magnitudes and absolute differences in the effect sizes. This type of reasoning has been shown to be potentially harmful to science. Techniques relying on the visual estimation of the strength of evidence have been recommended to reduce such dichotomous interpretations but their effectiveness has also been challenged. We ran two experiments on researchers with expertise in statistical analysis to compare several alternative representations of confidence intervals and used Bayesian multilevel models to estimate the effects of the representation styles on differences in researchers&#39; subjective confidence in the results. We also asked the respondents&#39; opinions and preferences in representation styles. Our results suggest that adding visual information to classic CI representation can decrease the tendency towards dichotomous interpretations - measured as the `cliff effect&#39;: the sudden drop in confidence around p-value 0.05 - compared with classic CI visualization and textual representation of the CI with p -values. All data and analyses are publicly available at https://github.com/helske/statvis.},
  archive      = {J_TVCG},
  author       = {Jouni Helske and Satu Helske and Matthew Cooper and Anders Ynnerman and Lonni Besançon},
  doi          = {10.1109/TVCG.2021.3073466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3397-3409},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Can visualization alleviate dichotomous thinking? effects of visual representations on the cliff effect},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of methods to compute minkowski operations for
geometric overlap detection. <em>TVCG</em>, <em>27</em>(8), 3377–3396.
(<a href="https://doi.org/10.1109/TVCG.2020.2976922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides an extensive review of algorithms for constructing Minkowski sums and differences of polygons and polyhedra, both convex and non-convex, commonly known as no-fit polygons and configuration space obstacles. The Minkowski difference is a set operation, which when applied to shapes defines a method for efficient overlap detection, providing an important tool in packing and motion-planning problems. This is the first complete review on this specific topic, and aims to unify algorithms spread over the literature of separate disciplines.},
  archive      = {J_TVCG},
  author       = {Wesley Cox and Lyndon While and Mark Reynolds},
  doi          = {10.1109/TVCG.2020.2976922},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3377-3396},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A review of methods to compute minkowski operations for geometric overlap detection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of rendering techniques for 3D line sets with
transparency. <em>TVCG</em>, <em>27</em>(8), 3361–3376. (<a
href="https://doi.org/10.1109/TVCG.2020.2975795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive study of rendering techniques for 3D line sets with transparency. The rendering of transparent lines is widely used for visualizing trajectories of tracer particles in flow fields. Transparency is then used to fade out lines deemed unimportant, based on, for instance, geometric properties or attributes defined along with them. Accurate blending of transparent lines requires rendering the lines in back-to-front or front-to-back order, yet enforcing this order for space-filling 3D line sets with extremely high-depth complexity becomes challenging. In this article, we study CPU and GPU rendering techniques for transparent 3D line sets. We compare accurate and approximate techniques using optimized implementations and several benchmark data sets. We discuss the effects of data size and transparency on quality, performance, and memory consumption. Based on our study, we propose two improvements to per-pixel fragment lists and multi-layer alpha blending. The first improves the rendering speed via an improved GPU sorting operation, and the second improves rendering quality via transparency-based bucketing.},
  archive      = {J_TVCG},
  author       = {Michael Kern and Christoph Neuhauser and Torben Maack and Mengjiao Han and Will Usher and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2020.2975795},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3361-3376},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparison of rendering techniques for 3D line sets with transparency},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D-kernel foveated rendering for light fields.
<em>TVCG</em>, <em>27</em>(8), 3350–3360. (<a
href="https://doi.org/10.1109/TVCG.2020.2975801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light fields capture both the spatial and angular rays, thus enabling free-viewpoint rendering and custom selection of the focal plane. Scientists can interactively explore pre-recorded microscopic light fields of organs, microbes, and neurons using virtual reality headsets. However, rendering high-resolution light fields at interactive frame rates requires a very high rate of texture sampling, which is challenging as the resolutions of light fields and displays continue to increase. In this article, we present an efficient algorithm to visualize 4D light fields with 3D-kernel foveated rendering (3D-KFR). The 3D-KFR scheme coupled with eye-tracking has the potential to accelerate the rendering of 4D depth-cued light fields dramatically. We have developed a perceptual model for foveated light fields by extending the KFR for the rendering of 3D meshes. On datasets of high-resolution microscopic light fields, we observe 3.47×-7.28× speedup in light field rendering with minimal perceptual loss of detail. We envision that 3D-KFR will reconcile the mutually conflicting goals of visual fidelity and rendering speed for interactive visualization of light fields.},
  archive      = {J_TVCG},
  author       = {Xiaoxu Meng and Ruofei Du and Joseph F. JaJa and Amitabh Varshney},
  doi          = {10.1109/TVCG.2020.2975801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {8},
  pages        = {3350-3360},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {3D-kernel foveated rendering for light fields},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OoDAnalyzer: Interactive analysis of out-of-distribution
samples. <em>TVCG</em>, <em>27</em>(7), 3335–3349. (<a
href="https://doi.org/10.1109/TVCG.2020.2973258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major cause of performance degradation in predictive models is that the test samples are not well covered by the training data. Such not well-represented samples are called OoD samples. In this article, we propose OoDAnalyzer, a visual analysis approach for interactively identifying OoD samples and explaining them in context. Our approach integrates an ensemble OoD detection method and a grid-based visualization. The detection method is improved from deep ensembles by combining more features with algorithms in the same family. To better analyze and understand the OoD samples in context, we have developed a novel k NN-based grid layout algorithm motivated by Hall&#39;s theorem. The algorithm approximates the optimal layout and has O( k N 2 )O( k N 2 ) time complexity, faster than the grid layout algorithm with overall best performance but O(N 3 )O(N 3 ) time complexity. Quantitative evaluation and case studies were performed on several datasets to demonstrate the effectiveness and usefulness of OoDAnalyzer.},
  archive      = {J_TVCG},
  author       = {Changjian Chen and Jun Yuan and Yafeng Lu and Yang Liu and Hang Su and Songtao Yuan and Shixia Liu},
  doi          = {10.1109/TVCG.2020.2973258},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3335-3349},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OoDAnalyzer: Interactive analysis of out-of-distribution samples},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kinetic-based multiphase flow simulation. <em>TVCG</em>,
<em>27</em>(7), 3318–3334. (<a
href="https://doi.org/10.1109/TVCG.2020.2972357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiphase flows exhibit a large realm of complex behaviors such as bubbling, glugging, wetting, and splashing which emerge from air-water and water-solid interactions. Current fluid solvers in graphics have demonstrated remarkable success in reproducing each of these visual effects, but none have offered a model general enough to capture all of them concurrently. In contrast, computational fluid dynamics have developed very general approaches to multiphase flows, typically based on kinetic models. Yet, in both communities, there is dearth of methods that can simulate density ratios and Reynolds numbers required for the type of challenging real-life simulations that movie productions strive to digitally create, such as air-water flows. In this article, we propose a kinetic model of the coupling of the Navier-Stokes equations with a conservative phase-field equation, and provide a series of numerical improvements over existing kinetic-based approaches to offer a general multiphase flow solver. The resulting algorithm is embarrassingly parallel, conservative, far more stable than current solvers even for real-life conditions, and general enough to capture the typical multiphase flow behaviors. Various simulation results are presented, including comparisons to both previous work and real footage, to highlight the advantages of our new method.},
  archive      = {J_TVCG},
  author       = {Wei Li and Daoming Liu and Mathieu Desbrun and Jin Huang and Xiaopei Liu},
  doi          = {10.1109/TVCG.2020.2972357},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3318-3334},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Kinetic-based multiphase flow simulation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prominent structures for video analysis and editing.
<em>TVCG</em>, <em>27</em>(7), 3305–3317. (<a
href="https://doi.org/10.1109/TVCG.2020.2970045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present prominent structures in video, a representation of visually strong, spatially sparse and temporally stable structural units, for use in video analysis and editing. With a novel quality measurement of prominent structures in video, we develop a general framework for prominent structure computation, and an efficient hierarchical structure alignment algorithm between a pair of videos. The prominent structural unit map is proposed to encode both binary prominence guidance and numerical strength and geometry details for each video frame. Even though the detailed appearance of videos could be visually different, the proposed alignment algorithm can find matched prominent structure sub-volumes. Prominent structures in video support a wide range of video analysis and editing applications including graphic match-cut between successive videos, instant cut editing, finding transition portals from a video collection, structure-aware video re-ranking, visualizing human action differences, etc.},
  archive      = {J_TVCG},
  author       = {Miao Wang and Xiao-Nan Fang and Guo-Wei Yang and Ariel Shamir and Shi-Min Hu},
  doi          = {10.1109/TVCG.2020.2970045},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3305-3317},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Prominent structures for video analysis and editing},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing the noise robustness of deep neural networks.
<em>TVCG</em>, <em>27</em>(7), 3289–3304. (<a
href="https://doi.org/10.1109/TVCG.2020.2969185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples, generated by adding small but intentionally imperceptible perturbations to normal examples, can mislead deep neural networks (DNNs) to make incorrect predictions. Although much work has been done on both adversarial attack and defense, a fine-grained understanding of adversarial examples is still lacking. To address this issue, we present a visual analysis method to explain why adversarial examples are misclassified. The key is to compare and analyze the datapaths of both the adversarial and normal examples. A datapath is a group of critical neurons along with their connections. We formulate the datapath extraction as a subset selection problem and solve it by constructing and training a neural network. A multi-level visualization consisting of a network-level visualization of data flows, a layer-level visualization of feature maps, and a neuron-level visualization of learned features, has been designed to help investigate how datapaths of adversarial and normal examples diverge and merge in the prediction process. A quantitative evaluation and a case study were conducted to demonstrate the promise of our method to explain the misclassification of adversarial examples.},
  archive      = {J_TVCG},
  author       = {Kelei Cao and Mengchen Liu and Hang Su and Jing Wu and Jun Zhu and Shixia Liu},
  doi          = {10.1109/TVCG.2020.2969185},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3289-3304},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analyzing the noise robustness of deep neural networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of binocular vision in avoiding virtual obstacles
while walking. <em>TVCG</em>, <em>27</em>(7), 3277–3288. (<a
href="https://doi.org/10.1109/TVCG.2020.2969181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in Virtual Reality technology have enabled physical walking in virtual environments. While most Virtual Reality systems render stereoscopic images to users, the implication of binocular viewing with respect to the performance of human walking in virtual environments remains largely unknown. In the present study, we conducted two walking experiments in virtual environments using a linear treadmill and a novel projected display known as the Wide Immersive Stereo Environment (WISE) to study the role of binocular viewing in virtual locomotion. The first experiment investigated the walking performance of people stepping over obstacles while the second experiment focused on a scenario on stepping over gaps. Both experiments were conducted under both stereoscopic viewing and non-stereoscopic viewing conditions. By analysing the gait parameters, we found that binocular viewing helped people to make more accurate movements to step over obstacles and gaps in virtual locomotion.},
  archive      = {J_TVCG},
  author       = {Jingbo Zhao and Robert S. Allison},
  doi          = {10.1109/TVCG.2020.2969181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3277-3288},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of binocular vision in avoiding virtual obstacles while walking},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualization of 3D stress tensor fields using superquadric
glyphs on displacement streamlines. <em>TVCG</em>, <em>27</em>(7),
3264–3276. (<a href="https://doi.org/10.1109/TVCG.2020.2968911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress tensor fields play a central role in solid mechanics studies, but their visualization in 3D space remains challenging as the information-dense multi-variate tensor needs to be sampled in 3D space while avoiding clutter. Taking cues from current tensor visualizations, we adapted glyph-based visualization for stress tensors in 3D space. We also developed a testing framework and performed user studies to evaluate the various glyph-based tensor visualizations for objective accuracy measures, and subjective user feedback for each visualization method. To represent the stress tensor, we color encoded the original superquadric glyph, and in the user study, we compared it to superquadric glyphs developed for second-order symmetric tensors. We found that color encoding improved the user accuracy measures, while the users also rated our method the highest. We compared our method of placing stress tensor glyphs on displacement streamlines to the glyph placement on a 3D grid. In the visualization, we modified the glyph to show both the stress tensor and the displacement vector at each sample point. The participants preferred our method of glyph placement on displacement streamlines as it highlighted the underlying continuous structure in the tensor field.},
  archive      = {J_TVCG},
  author       = {Mohak Patel and David H. Laidlaw},
  doi          = {10.1109/TVCG.2020.2968911},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3264-3276},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization of 3D stress tensor fields using superquadric glyphs on displacement streamlines},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepSketchHair: Deep sketch-based 3D hair modeling.
<em>TVCG</em>, <em>27</em>(7), 3250–3263. (<a
href="https://doi.org/10.1109/TVCG.2020.2968433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DeepSketchHair, a deep learning based tool for modeling of 3D hair from 2D sketches. Given a 3D bust model as reference, our sketching system takes as input a user-drawn sketch (consisting of hair contour and a few strokes indicating the hair growing direction within a hair region), and automatically generates a 3D hair model, matching the input sketch. The key enablers of our system are three carefully designed neural networks, namely, S2ONet, which converts an input sketch to a dense 2D hair orientation field; O2VNet, which maps the 2D orientation field to a 3D vector field; and V2VNet, which updates the 3D vector field with respect to the new sketches, enabling hair editing with additional sketches in new views. All the three networks are trained with synthetic data generated from a 3D hairstyle database. We demonstrate the effectiveness and expressiveness of our tool using a variety of hairstyles and also compare our method with prior art.},
  archive      = {J_TVCG},
  author       = {Yuefan Shen and Changgeng Zhang and Hongbo Fu and Kun Zhou and Youyi Zheng},
  doi          = {10.1109/TVCG.2020.2968433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3250-3263},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeepSketchHair: Deep sketch-based 3D hair modeling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A non-linear differentiable CNN-rendering module for 3D data
enhancement. <em>TVCG</em>, <em>27</em>(7), 3238–3249. (<a
href="https://doi.org/10.1109/TVCG.2020.2968062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we introduce a differentiable rendering module which allows neural networks to efficiently process 3D data. The module is composed of continuous piecewise differentiable functions defined as a sensor array of cells embedded in 3D space. Our module is learnable and can be easily integrated into neural networks allowing to optimize data rendering towards specific learning tasks using gradient based methods in an end-to-end fashion. Essentially, the module&#39;s sensor cells are allowed to transform independently and locally focus and sense different parts of the 3D data. Thus, through their optimization process, cells learn to focus on important parts of the data, bypassing occlusions, clutter, and noise. Since sensor cells originally lie on a grid, this equals to a highly non-linear rendering of the scene into a 2D image. Our module performs especially well in presence of clutter and occlusions as well as dealing with non-linear deformations to improve classification accuracy through proper rendering of the data. In our experiments, we apply our module in various learning tasks and demonstrate that using our rendering module we accomplish efficient classification, localization, and segmentation tasks on 2D/3D cluttered and non-cluttered data.},
  archive      = {J_TVCG},
  author       = {Yonatan Svirsky and Andrei Sharf},
  doi          = {10.1109/TVCG.2020.2968062},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3238-3249},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A non-linear differentiable CNN-rendering module for 3D data enhancement},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven 3D neck modeling and animation. <em>TVCG</em>,
<em>27</em>(7), 3226–3237. (<a
href="https://doi.org/10.1109/TVCG.2020.2967036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a data-driven approach for modeling and animation of 3D necks. Our method is based on a new neck animation model that decomposes the neck animation into local deformation caused by larynx motion and global deformation driven by head poses, facial expressions, and speech. A skinning model is introduced for modeling local deformation and underlying larynx motions, while the global neck deformation caused by each factor is modeled by its corrective blendshape set, respectively. Based on this neck model, we introduce a regression method to drive the larynx motion and neck deformation from speech. Both the neck model and the speech regressor are learned from a dataset of 3D neck animation sequences captured from different identities. Our neck model significantly improves the realism of facial animation and allows users to easily create plausible neck animations from speech and facial expressions. We verify our neck model and demonstrate its advantages in 3D neck tracking and animation.},
  archive      = {J_TVCG},
  author       = {Yilong Liu and Chengwei Zheng and Feng Xu and Xin Tong and Baining Guo},
  doi          = {10.1109/TVCG.2020.2967036},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3226-3237},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data-driven 3D neck modeling and animation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VRIA: A web-based framework for creating immersive analytics
experiences. <em>TVCG</em>, <em>27</em>(7), 3213–3225. (<a
href="https://doi.org/10.1109/TVCG.2020.2965109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present VRIA, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. VRIA is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML Document Object Model (DOM). This makes VRIA ubiquitous and platform-independent. Moreover, by using WebVR&#39;s progressive enhancement, the experiences VRIA creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on open-standards Web technologies, present the VRIA creation workflow and detail the underlying mechanics of our framework. We also report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability implications of our framework, and present a series of use case applications to demonstrate the various features of VRIA. Finally, we discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.},
  archive      = {J_TVCG},
  author       = {Peter W. S. Butcher and Nigel W. John and Panagiotis D. Ritsos},
  doi          = {10.1109/TVCG.2020.2965109},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3213-3225},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VRIA: A web-based framework for creating immersive analytics experiences},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Content-preserving image stitching with piecewise
rectangular boundary constraints. <em>TVCG</em>, <em>27</em>(7),
3198–3212. (<a href="https://doi.org/10.1109/TVCG.2020.2965097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an approach to content-preserving image stitching with regular boundary constraints, which aims to stitch multiple images to generate a panoramic image with piecewise rectangular boundaries. Existing methods treat image stitching and rectangling as two separate steps, which may result in suboptimal results as the stitching process is not aware of the further warping needs for rectangling. We address these limitations by formulating image stitching with regular boundaries in a unified optimization framework. Starting from the initial stitching result produced by traditional warping-based optimization, we obtain the irregular boundary from the warped meshes by polygon Boolean operations which robustly handle arbitrary mesh compositions. By analyzing the irregular boundary, we construct a piecewise rectangular boundary. Based on this, we further incorporate line and regular boundary preservation constraints into the image stitching framework, and conduct iterative optimizations to obtain an optimal piecewise rectangular boundary. Thus we can make the boundary of the stitching result as close as possible to a rectangle, while reducing unwanted distortions. We further extend our method to video stitching, by integrating the temporal coherence into the optimization. Experiments show that our method efficiently produces visually pleasing panoramas with regular boundaries and unnoticeable distortions.},
  archive      = {J_TVCG},
  author       = {Yun Zhang and Yu-Kun Lai and Fang-Lue Zhang},
  doi          = {10.1109/TVCG.2020.2965097},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3198-3212},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Content-preserving image stitching with piecewise rectangular boundary constraints},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effect of avatar appearance on detection thresholds for
remapped hand movements. <em>TVCG</em>, <em>27</em>(7), 3182–3197. (<a
href="https://doi.org/10.1109/TVCG.2020.2964758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand interaction techniques in virtual reality often exploit visual dominance over proprioception to remap physical hand movements onto different virtual movements. However, when the offset between virtual and physical hands increases, the remapped virtual hand movements are hardly self-attributed, and the users become aware of the remapping. Interestingly, the sense of self-attribution of a body is called the sense of body ownership (SoBO) in the field of psychology, and the realistic the avatar, the stronger is the SoBO. Hence, we hypothesized that realistic avatars (i.e., human hands) can foster self-attribution of the remapped movements better than abstract avatars (i.e., spherical pointers), thus making the remapping less noticeable. In this article, we present an experiment in which participants repeatedly executed reaching movements with their right hand while different amounts of horizontal shifts were applied. We measured the remapping detection thresholds for each combination of shift directions (left or right) and avatar appearances (realistic or abstract). The results show that realistic avatars increased the detection threshold (i.e., lowered sensitivity) by 31.3 percent than the abstract avatars when the leftward shift was applied (i.e., when the hand moved in the direction away from the body-midline). In addition, the proprioceptive drift (i.e., the displacement of self-localization toward an avatar) was larger with realistic avatars for leftward shifts, indicating that visual information was given greater preference during visuo-proprioceptive integration in realistic avatars. Our findings quantifiably show that realistic avatars can make remapping less noticeable for larger mismatches between virtual and physical movements and can potentially improve a wide variety of hand-remapping techniques without changing the mapping itself.},
  archive      = {J_TVCG},
  author       = {Nami Ogawa and Takuji Narumi and Michitaka Hirose},
  doi          = {10.1109/TVCG.2020.2964758},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3182-3197},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effect of avatar appearance on detection thresholds for remapped hand movements},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EmotionCues: Emotion-oriented visual summarization of
classroom videos. <em>TVCG</em>, <em>27</em>(7), 3168–3181. (<a
href="https://doi.org/10.1109/TVCG.2019.2963659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing students’ emotions from classroom videos can help both teachers and parents quickly know the engagement of students in class. The availability of high-definition cameras creates opportunities to record class scenes. However, watching videos is time-consuming, and it is challenging to gain a quick overview of the emotion distribution and find abnormal emotions. In this article, we propose EmotionCues , a visual analytics system to easily analyze classroom videos from the perspective of emotion summary and detailed analysis, which integrates emotion recognition algorithms with visualizations. It consists of three coordinated views: a summary view depicting the overall emotions and their dynamic evolution, a character view presenting the detailed emotion status of an individual, and a video view enhancing the video analysis with further details. Considering the possible inaccuracy of emotion recognition, we also explore several factors affecting the emotion analysis, such as face size and occlusion. They provide hints for inferring the possible inaccuracy and the corresponding reasons. Two use cases and interviews with end users and domain experts are conducted to show that the proposed system could be useful and effective for analyzing emotions in the classroom videos.},
  archive      = {J_TVCG},
  author       = {Haipeng Zeng and Xinhuan Shu and Yanbang Wang and Yong Wang and Liguo Zhang and Ting-Chuen Pong and Huamin Qu},
  doi          = {10.1109/TVCG.2019.2963659},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3168-3181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EmotionCues: Emotion-oriented visual summarization of classroom videos},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vis-a-vis: Visual exploration of visualization source code
evolution. <em>TVCG</em>, <em>27</em>(7), 3153–3167. (<a
href="https://doi.org/10.1109/TVCG.2019.2963651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an algorithm for a visualization prototype often involves the direct comparison of different development stages and design decisions, and even minor modifications may dramatically affect the results. While existing development tools provide visualizations for gaining general insight into performance and structural aspects of the source code, they neglect the central importance of result images unique to graphical algorithms. In this article, we present a novel approach that enables visualization programmers to simultaneously explore the evolution of their algorithm during the development phase together with its corresponding visual outcomes by providing an automatically updating meta visualization. Our interactive system allows for the direct comparison of all development states on both the visual and the source code level, by providing easy to use navigation and comparison tools. The on-the-fly construction of difference images, source code differences, and a visual representation of the source code structure further enhance the user&#39;s insight into the states&#39; interconnected changes over time. Our solution is accessible via a web-based interface that provides GPU-accelerated live execution of C++ and GLSL code, as well as supporting a domain-specific programming language for scientific visualization.},
  archive      = {J_TVCG},
  author       = {Fabian Bolte and Stefan Bruckner},
  doi          = {10.1109/TVCG.2019.2963651},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3153-3167},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vis-a-vis: Visual exploration of visualization source code evolution},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualization of blockchain data: A systematic review.
<em>TVCG</em>, <em>27</em>(7), 3135–3152. (<a
href="https://doi.org/10.1109/TVCG.2019.2963018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a systematic review of visual analytics tools used for the analysis of blockchains-related data. The blockchain concept has recently received considerable attention and spurred applications in a variety of domains. We systematically and quantitatively assessed 76 analytics tools that have been proposed in research as well as online by professionals and blockchain enthusiasts. Our classification of these tools distinguishes (1) target blockchains, (2) blockchain data, (3) target audiences, (4) task domains, and (5) visualization types. Furthermore, we look at which aspects of blockchain data have already been explored and point out areas that deserve more investigation in the future.},
  archive      = {J_TVCG},
  author       = {Natkamon Tovanich and Nicolas Heulot and Jean-Daniel Fekete and Petra Isenberg},
  doi          = {10.1109/TVCG.2019.2963018},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3135-3152},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization of blockchain data: A systematic review},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive simulation of scattering effects in
participating media using a neural network model. <em>TVCG</em>,
<em>27</em>(7), 3123–3134. (<a
href="https://doi.org/10.1109/TVCG.2019.2963015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering participating media is important to the creation of photorealistic images. Participating media has a translucent aspect that comes from light being scattered inside the material. For materials with a small mean-free-path (mfp), multiple scattering effects dominate. Simulating these effects is computationally intensive, as it requires tracking a large number of scattering events inside the material. Existing approaches precompute multiple scattering events inside the material and store the results in a table. During rendering time, this table is used to compute the scattering effects. While these methods are faster than explicit scattering computation, they incur higher storage costs. In this paper, we present a new representation for double and multiple scattering effects that uses a neural network model. The scattering response from all homogeneous participating media is encoded into a neural network in a preprocessing step. At run time, the neural network is then used to predict the double and multiple scattering effects. We demonstrate the effects combined with Virtual Ray Lights (VRL), although our approach can be integrated with other rendering algorithms. Our algorithm is implemented on GPU. Double and multiple scattering effects for the entire participating media space are encoded using only 23.6 KB of memory. Our method achieves 50 ms per frame in typical scenes and provides results almost identical to the reference.},
  archive      = {J_TVCG},
  author       = {Liangsheng Ge and Beibei Wang and Lu Wang and Xiangxu Meng and Nicolas Holzschuch},
  doi          = {10.1109/TVCG.2019.2963015},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3123-3134},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive simulation of scattering effects in participating media using a neural network model},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ProReveal: Progressive visual analytics with safeguards.
<em>TVCG</em>, <em>27</em>(7), 3109–3122. (<a
href="https://doi.org/10.1109/TVCG.2019.2962404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new visual exploration concept - Progressive Visual Analytics with Safeguards - that helps people manage the uncertainty arising from progressive data exploration. Despite its potential benefits, intermediate knowledge from progressive analytics can be incorrect due to various machine and human factors, such as a sampling bias or misinterpretation of uncertainty. To alleviate this problem, we introduce PVA-Guards, safeguards people can leave on uncertain intermediate knowledge that needs to be verified, and derive seven PVA-Guards based on previous visualization task taxonomies. PVA-Guards provide a means of ensuring the correctness of the conclusion and understanding the reason when intermediate knowledge becomes invalid. We also present ProReveal, a proof-of-concept system designed and developed to integrate the seven safeguards into progressive data exploration. Finally, we report a user study with 14 participants, which shows people voluntarily employed PVA-Guards to safeguard their findings and ProReveal&#39;s PVA-Guard view provides an overview of uncertain intermediate knowledge. We believe our new concept can also offer better consistency in progressive data exploration, alleviating people&#39;s heterogeneous interpretation of uncertainty.},
  archive      = {J_TVCG},
  author       = {Jaemin Jo and Sehi L’Yi and Bongshin Lee and Jinwook Seo},
  doi          = {10.1109/TVCG.2019.2962404},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3109-3122},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ProReveal: Progressive visual analytics with safeguards},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local prediction models for spatiotemporal volume
visualization. <em>TVCG</em>, <em>27</em>(7), 3091–3108. (<a
href="https://doi.org/10.1109/TVCG.2019.2961893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a machine learning-based approach for detecting and visualizing complex behavior in spatiotemporal volumes. For this, we train models to predict future data values at a given position based on the past values in its neighborhood, capturing common temporal behavior in the data. We then evaluate the model&#39;s prediction on the same data. High prediction error means that the local behavior was too complex, unique or uncertain to be accurately captured during training, indicating spatiotemporal regions with interesting behavior. By training several models of varying capacity, we are able to detect spatiotemporal regions of various complexities. We aggregate the obtained prediction errors into a time series or spatial volumes and visualize them together to highlight regions of unpredictable behavior and how they differ between the models. We demonstrate two further volumetric applications: adaptive timestep selection and analysis of ensemble dissimilarity. We apply our technique to datasets from multiple application domains and demonstrate that we are able to produce meaningful results while making minimal assumptions about the underlying data.},
  archive      = {J_TVCG},
  author       = {Gleb Tkachev and Steffen Frey and Thomas Ertl},
  doi          = {10.1109/TVCG.2019.2961893},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {7},
  pages        = {3091-3108},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Local prediction models for spatiotemporal volume visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VR disability simulation reduces implicit bias towards
persons with disabilities. <em>TVCG</em>, <em>27</em>(6), 3079–3090. (<a
href="https://doi.org/10.1109/TVCG.2019.2958332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates how experiencing Virtual Reality (VR) Disability Simulation (DS) affects information recall and participants’ implicit association towards people with disabilities (PwD). Implicit attitudes are our actions or judgments towards various concepts or stereotypes (e.g., race) which we may or may not be aware of. Previous research has shown that experiencing ownership over a dark-skinned body reduces implicit racial bias. We hypothesized that a DS with a tracked Head Mounted Display (HMD) and a wheelchair interface would have a significantly larger effect on participants’ information recall and their implicit association towards PwD than a desktop monitor and gamepad. We conducted a 2 x 2 between-subjects experiment in which participants experienced a VR DS that teaches them facts about Multiple Sclerosis (MS) with factors of display (HMD, a desktop monitor) and interface (gamepad, wheelchair). Participants took two Implicit Association Tests before and after experiencing the DS. Our study results show that the participants in an immersive HMD condition performed better than the participants in the non-immersive Desktop condition in their information recall task. Moreover, a tracked HMD and a wheelchair interface had significantly larger effects on participants’ implicit association towards PwD than a desktop monitor and a gamepad.},
  archive      = {J_TVCG},
  author       = {Tanvir Irfan Chowdhury and Sharif Mohammad Shahnewaz Ferdous and John Quarles},
  doi          = {10.1109/TVCG.2019.2958332},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3079-3090},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VR disability simulation reduces implicit bias towards persons with disabilities},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Volumetric isosurface rendering with deep learning-based
super-resolution. <em>TVCG</em>, <em>27</em>(6), 3064–3078. (<a
href="https://doi.org/10.1109/TVCG.2019.2956697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering an accurate image of an isosurface in a volumetric field typically requires large numbers of data samples. Reducing this number lies at the core of research in volume rendering. With the advent of deep learning networks, a number of architectures have been proposed recently to infer missing samples in multidimensional fields, for applications such as image super-resolution. In this article, we investigate the use of such architectures for learning the upscaling of a low resolution sampling of an isosurface to a higher resolution, with reconstruction of spatial detail and shading. We introduce a fully convolutional neural network, to learn a latent representation generating smooth, edge-aware depth and normal fields as well as ambient occlusions from a low resolution depth and normal field. By adding a frame-to-frame motion loss into the learning stage, upscaling can consider temporal variations and achieves improved frame-to-frame coherence. We assess the quality of inferred results and compare it to bi-linear and cubic upscaling. We do this for isosurfaces which were never seen during training, and investigate the improvements when the network can train on the same or similar isosurfaces. We discuss remote visualization and foveated rendering as potential applications.},
  archive      = {J_TVCG},
  author       = {Sebastian Weiss and Mengyu Chu and Nils Thuerey and Rüdiger Westermann},
  doi          = {10.1109/TVCG.2019.2956697},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3064-3078},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Volumetric isosurface rendering with deep learning-based super-resolution},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The making of continuous colormaps. <em>TVCG</em>,
<em>27</em>(6), 3048–3063. (<a
href="https://doi.org/10.1109/TVCG.2019.2961674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous colormaps are integral parts of many visualization techniques, such as heat-maps, surface plots, and flow visualization. Despite that the critiques of rainbow colormaps have been around and well-acknowledged for three decades, rainbow colormaps are still widely used today. One reason behind the resilience of rainbow colormaps is the lack of tools for users to create a continuous colormap that encodes semantics specific to the application concerned. In this paper, we present a web-based software system, CCC-Tool (short for Charting Continuous Colormaps) under the URL https://ccctool.com, for creating, editing, and analyzing such application-specific colormaps. We introduce the notion of “colormap specification (CMS)” that maintains the essential semantics required for defining a color mapping scheme. We provide users with a set of advanced utilities for constructing CMS&#39;s with various levels of complexity, examining their quality attributes using different plots, and exporting them to external application software. We present two case studies, demonstrating that the CCC-Tool can help domain scientists as well as visualization experts in designing semantically-rich colormaps.},
  archive      = {J_TVCG},
  author       = {Pascal Nardini and Min Chen and Francesca Samsel and Roxana Bujack and Michael Böttinger and Gerik Scheuermann},
  doi          = {10.1109/TVCG.2019.2961674},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3048-3063},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The making of continuous colormaps},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape-driven coordinate ordering for star glyph sets via
reinforcement learning. <em>TVCG</em>, <em>27</em>(6), 3034–3047. (<a
href="https://doi.org/10.1109/TVCG.2021.3052167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural optimization model trained with reinforcement learning to solve the coordinate ordering problem for sets of star glyphs. Given a set of star glyphs associated to multiple class labels, we propose to use shape context descriptors to measure the perceptual distance between pairs of glyphs, and use the derived silhouette coefficient to measure the perception of class separability within the entire set. To find the optimal coordinate order for the given set, we train a neural network using reinforcement learning to reward orderings with high silhouette coefficients. The network consists of an encoder and a decoder with an attention mechanism. The encoder employs a recurrent neural network (RNN) to encode input shape and class information, while the decoder together with the attention mechanism employs another RNN to output a sequence with the new coordinate order. In addition, we introduce a neural network to efficiently estimate the similarity between shape context descriptors, which allows to speed up the computation of silhouette coefficients and thus the training of the axis ordering network. Two user studies demonstrate that the orders provided by our method are preferred by users for perceiving class separation. We tested our model on different settings to show its robustness and generalization abilities and demonstrate that it allows to order input sets with unseen data size, data dimension, or number of classes. We also demonstrate that our model can be adapted to coordinate ordering of other types of plots such as RadViz by replacing the proposed shape-aware silhouette coefficient with the corresponding quality metric to guide network training.},
  archive      = {J_TVCG},
  author       = {Ruizhen Hu and Bin Chen and Juzhan Xu and Oliver van Kaick and Oliver Deussen and Hui Huang},
  doi          = {10.1109/TVCG.2021.3052167},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3034-3047},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shape-driven coordinate ordering for star glyph sets via reinforcement learning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting spatio-angular trade-off in light field cameras
and extended applications in super-resolution. <em>TVCG</em>,
<em>27</em>(6), 3019–3033. (<a
href="https://doi.org/10.1109/TVCG.2019.2957761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field cameras (LFCs) have received increasing attention due to their wide-spread applications. However, current LFCs suffer from the well-known spatio-angular trade-off, which is considered an inherent and fundamental limit for LFC designs. In this article, by doing a detailed optical analysis of the sampling process in an LFC, we show that the effective resolution is generally higher than the number of micro-lenses. This contribution makes it theoretically possible to super-resolve a light field. Further optical analysis proves the “2D predictable series” nature of the 4D light field, which provides new insights for analyzing light field using series processing techniques. To model this nature, a specifically designed epipolar plane image (EPI) based CNN-LSTM network is proposed to super-resolve a light field in the spatial and angular dimensions simultaneously. Rather than leveraging semantic information, our network focuses on extracting geometric continuity in the EPI domain. This gives our method an improved generalization ability and makes it applicable to a wide range of previously unseen scenes. Experiments on both synthetic and real light fields demonstrate the improvements over state-of-the-arts, especially in large disparity areas.},
  archive      = {J_TVCG},
  author       = {Hao Zhu and Mantang Guo and Hongdong Li and Qing Wang and Antonio Robles-Kelly},
  doi          = {10.1109/TVCG.2019.2957761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3019-3033},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisiting spatio-angular trade-off in light field cameras and extended applications in super-resolution},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PRS-net: Planar reflective symmetry detection net for 3D
models. <em>TVCG</em>, <em>27</em>(6), 3007–3018. (<a
href="https://doi.org/10.1109/TVCG.2020.3003823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In geometry processing, symmetry is a universal type of high-level structural information of 3D models and benefits many geometry processing tasks including shape segmentation, alignment, matching, and completion. Thus it is an important problem to analyze various symmetry forms of 3D shapes. Planar reflective symmetry is the most fundamental one. Traditional methods based on spatial sampling can be time-consuming and may not be able to identify all the symmetry planes. In this article, we present a novel learning framework to automatically discover global planar reflective symmetry of a 3D shape. Our framework trains an unsupervised 3D convolutional neural network to extract global model features and then outputs possible global symmetry parameters, where input shapes are represented using voxels. We introduce a dedicated symmetry distance loss along with a regularization loss to avoid generating duplicated symmetry planes. Our network can also identify generalized cylinders by predicting their rotation axes. We further provide a method to remove invalid and duplicated planes and axes. We demonstrate that our method is able to produce reliable and accurate results. Our neural network based method is hundreds of times faster than the state-of-the-art methods, which are based on sampling. Our method is also robust even with noisy or incomplete input surfaces.},
  archive      = {J_TVCG},
  author       = {Lin Gao and Ling-Xiao Zhang and Hsien-Yu Meng and Yi-Hui Ren and Yu-Kun Lai and Leif Kobbelt},
  doi          = {10.1109/TVCG.2020.3003823},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {3007-3018},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PRS-net: Planar reflective symmetry detection net for 3D models},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Planar abstraction and inverse rendering of 3D indoor
environments. <em>TVCG</em>, <em>27</em>(6), 2992–3006. (<a
href="https://doi.org/10.1109/TVCG.2019.2960776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scanning and acquiring a 3D indoor environment suffers from complex occlusions and misalignment errors. The reconstruction obtained from an RGB-D scanner contains holes in geometry and ghosting in texture. These are easily noticeable and cannot be considered as visually compelling VR content without further processing. On the other hand, the well-known Manhattan World priors successfully recreate relatively simple structures. In this article, we would like to push the limit of planar representation in indoor environments. Given an initial 3D reconstruction captured by an RGB-D sensor, we use planes not only to represent the environment geometrically but also to solve an inverse rendering problem considering texture and light. The complex process of shape inference and intrinsic imaging is greatly simplified with the help of detected planes and yet produces a realistic 3D indoor environment. The generated content can adequately represent the spatial arrangements for various AR/VR applications and can be readily composited with virtual objects possessing plausible lighting and texture.},
  archive      = {J_TVCG},
  author       = {Young Min Kim and Sangwoo Ryu and Ig-Jae Kim},
  doi          = {10.1109/TVCG.2019.2960776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2992-3006},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Planar abstraction and inverse rendering of 3D indoor environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Net2Vis – a visual grammar for automatically generating
publication-tailored CNN architecture visualizations. <em>TVCG</em>,
<em>27</em>(6), 2980–2991. (<a
href="https://doi.org/10.1109/TVCG.2021.3057483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To convey neural network architectures in publications, appropriate visualizations are of great importance. While most current deep learning papers contain such visualizations, these are usually handcrafted just before publication, which results in a lack of a common visual grammar, significant time investment, errors, and ambiguities. Current automatic network visualization tools focus on debugging the network itself and are not ideal for generating publication visualizations. Therefore, we present an approach to automate this process by translating network architectures specified in Keras into visualizations that can directly be embedded into any publication. To do so, we propose a visual grammar for convolutional neural networks (CNNs), which has been derived from an analysis of such figures extracted from all ICCV and CVPR papers published between 2013 and 2019. The proposed grammar incorporates visual encoding, network layout, layer aggregation, and legend generation. We have further realized our approach in an online system available to the community, which we have evaluated through expert feedback, and a quantitative study. It not only reduces the time needed to generate network visualizations for publications, but also enables a unified and unambiguous visualization design.},
  archive      = {J_TVCG},
  author       = {Alex Bäuerle and Christian van Onzenoodt and Timo Ropinski},
  doi          = {10.1109/TVCG.2021.3057483},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2980-2991},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Net2Vis – a visual grammar for automatically generating publication-tailored CNN architecture visualizations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling data-driven dominance traits for virtual characters
using gait analysis. <em>TVCG</em>, <em>27</em>(6), 2967–2979. (<a
href="https://doi.org/10.1109/TVCG.2019.2953063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a data-driven algorithm for generating gaits of virtual characters with varying dominance traits. Our formulation utilizes a user study to establish a data-driven dominance mapping between gaits and dominance labels. We use our dominance mapping to generate walking gaits for virtual characters that exhibit a variety of dominance traits while interacting with the user. Furthermore, we extract gait features based on known criteria in visual perception and psychology literature that can be used to identify the dominance levels of any walking gait. We validate our mapping and the perceived dominance traits by a second user study in an immersive virtual environment. Our gait dominance classification algorithm can classify the dominance traits of gaits with ˜73 percent accuracy. We also present an application of our approach that simulates interpersonal relationships between virtual characters. To the best of our knowledge, ours is the first practical approach to classifying gait dominance and generate dominance traits in virtual characters.},
  archive      = {J_TVCG},
  author       = {Tanmay Randhavane and Aniket Bera and Emily Kubin and Kurt Gray and Dinesh Manocha},
  doi          = {10.1109/TVCG.2019.2953063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2967-2979},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling data-driven dominance traits for virtual characters using gait analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrated dual analysis of quantitative and qualitative
high-dimensional data. <em>TVCG</em>, <em>27</em>(6), 2953–2966. (<a
href="https://doi.org/10.1109/TVCG.2021.3056424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dual Analysis framework is a powerful enabling technology for the exploration of high dimensional quantitative data by treating data dimensions as first-class objects that can be explored in tandem with data values. In this article, we extend the Dual Analysis framework through the joint treatment of quantitative (numerical) and qualitative (categorical) dimensions. Computing common measures for all dimensions allows us to visualize both quantitative and qualitative dimensions in the same view. This enables a natural joint treatment of mixed data during interactive visual exploration and analysis. Several measures of variation for nominal qualitative data can also be applied to ordinal qualitative and quantitative data. For example, instead of measuring variability from a mean or median, other measures assess inter-data variation or average variation from a mode. In this work, we demonstrate how these measures can be integrated into the Dual Analysis framework to explore and generate hypotheses about high-dimensional mixed data. A medical case study using clinical routine data of patients suffering from Cerebral Small Vessel Disease (CSVD), conducted with a senior neurologist and a medical student, shows that a joint Dual Analysis approach for quantitative and qualitative data can rapidly lead to new insights based on which new hypotheses may be generated.},
  archive      = {J_TVCG},
  author       = {Juliane Muller and Laura Garrison and Philipp Ulbrich and Stefanie Schreiber and Stefan Bruckner and Helwig Hauser and Steffen Oeltze-Jafra},
  doi          = {10.1109/TVCG.2021.3056424},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2953-2966},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Integrated dual analysis of quantitative and qualitative high-dimensional data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaze-driven adaptive interventions for magazine-style
narrative visualizations. <em>TVCG</em>, <em>27</em>(6), 2941–2952. (<a
href="https://doi.org/10.1109/TVCG.2019.2958540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the value of gaze-driven adaptive interventions to support the processing of textual documents with embedded visualizations, i.e., Magazine Style Narrative Visualizations (MSNVs). These interventions are provided dynamically by highlighting relevant data points in the visualization when the user reads related sentences in the MSNV text, as detected by an eye-tracker. We conducted a user study during which participants read a set of MSNVs with our interventions, and compared their performance and experience with participants who received no interventions. Our work extends previous findings by showing that dynamic, gaze-driven interventions can be delivered based on reading behaviors in MSNVs, a widespread form of documents that have never been considered for gaze-driven adaptation so far. Next, we found that the interventions significantly improved the performance of users with low levels of visualization literacy, i.e., those users who need help the most due to their lower ability to process and understand data visualizations. However, high literacy users were not impacted by the interventions, providing initial evidence that gaze-driven interventions can be further improved by personalizing them to the levels of visualization literacy of their users.},
  archive      = {J_TVCG},
  author       = {Sébastien Lallé and Dereck Toker and Cristina Conati},
  doi          = {10.1109/TVCG.2019.2958540},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2941-2952},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gaze-driven adaptive interventions for magazine-style narrative visualizations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic voronoi diagram for moving disks. <em>TVCG</em>,
<em>27</em>(6), 2923–2940. (<a
href="https://doi.org/10.1109/TVCG.2019.2959321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voronoi diagrams are powerful for understanding spatial properties. However, few reports have been made for moving generators despite their important applications. We present a topology-oriented event-increment (TOI-E) algorithm for constructing a Voronoi diagram of moving circular disks in the plane over the time horizon $[0, t^{\infty })$ . The proposed TOI-E algorithm computes the event history of the Voronoi diagram over the entire time horizon in $O(k_F \log n + k_C n \log n)$ time with $O(n \log n)$ preprocessing time and $O(n + k_F + k_C)$ memory for $n$ disk generators, $k_F$ edge flips, and $k_C$ disk collisions during the time horizon. Given an event history, the Voronoi diagram of an arbitrary moment $t^{\ast}&amp;lt;t^{\infty }$ can be constructed in $O(k^{\ast} + n)$ time where $k^{\ast}$ represents the number of events in $[0, t^{\ast})$ . An example of the collision avoidance problem among moving disks is given by predicting future conjunctions among the disks using the proposed algorithm. Dynamic Voronoi diagrams will be very useful as a platform for the planning and management of the traffics of unmanned vehicles such as cars on street, vessels on surface, drones and airplanes in air, and satellites in geospace.},
  archive      = {J_TVCG},
  author       = {Chanyoung Song and Jehyun Cha and Mokwon Lee and Deok-Soo Kim},
  doi          = {10.1109/TVCG.2019.2959321},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2923-2940},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic voronoi diagram for moving disks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DimLift: Interactive hierarchical data exploration through
dimensional bundling. <em>TVCG</em>, <em>27</em>(6), 2908–2922. (<a
href="https://doi.org/10.1109/TVCG.2021.3057519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of interesting patterns and relationships is essential to exploratory data analysis. This becomes increasingly difficult in high dimensional datasets. While dimensionality reduction techniques can be utilized to reduce the analysis space, these may unintentionally bury key dimensions within a larger grouping and obfuscate meaningful patterns. With this work we introduce DimLift , a novel visual analysis method for creating and interacting with dimensional bundles . Generated through an iterative dimensionality reduction or user-driven approach, dimensional bundles are expressive groups of dimensions that contribute similarly to the variance of a dataset. Interactive exploration and reconstruction methods via a layered parallel coordinates plot allow users to lift interesting and subtle relationships to the surface, even in complex scenarios of missing and mixed data types. We exemplify the power of this technique in an expert case study on clinical cohort data alongside two additional case examples from nutrition and ecology.},
  archive      = {J_TVCG},
  author       = {Laura Garrison and Juliane Müller and Stefanie Schreiber and Steffen Oeltze-Jafra and Helwig Hauser and Stefan Bruckner},
  doi          = {10.1109/TVCG.2021.3057519},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2908-2922},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DimLift: Interactive hierarchical data exploration through dimensional bundling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Digital surface regularization with guarantees.
<em>TVCG</em>, <em>27</em>(6), 2896–2907. (<a
href="https://doi.org/10.1109/TVCG.2021.3055242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voxel based modeling is a very attractive way to represent complex multi-material objects. Beside artistic choices of pixel/voxel arts, representing objects as voxels allows efficient and dynamic interactions with the scene. For geometry processing purposes, many applications in material sciences, medical imaging or numerical simulation rely on a regular partitioning of the space with labeled voxels. In this article, we consider a variational approach to reconstruct interfaces in multi-labeled digital images. This approach efficiently produces piecewise smooth quadrangulated surfaces with some theoretical stability guarantee. Non-manifold parts at intersecting interfaces are handled naturally by our model. We illustrate the strength of our tool for digital surface regularization, as well as voxel art regularization by transferring colorimetric information to regularized quads and computing isotropic geodesic on digital surfaces.},
  archive      = {J_TVCG},
  author       = {David Coeurjolly and Jacques-Olivier Lachaud and Pierre Gueth},
  doi          = {10.1109/TVCG.2021.3055242},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2896-2907},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Digital surface regularization with guarantees},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational design of skinned quad-robots. <em>TVCG</em>,
<em>27</em>(6), 2881–2895. (<a
href="https://doi.org/10.1109/TVCG.2019.2957218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational design system that assists users to model, optimize, and fabricate quad-robots with soft skins. Our system addresses the challenging task of predicting their physical behavior by fully integrating the multibody dynamics of the mechanical skeleton and the elastic behavior of the soft skin. The developed motion control strategy uses an alternating optimization scheme to avoid expensive full space time-optimization, interleaving space-time optimization for the skeleton, and frame-by-frame optimization for the full dynamics. The output are motor torques to drive the robot to achieve a user prescribed motion trajectory. We also provide a collection of convenient engineering tools and empirical manufacturing guidance to support the fabrication of the designed quad-robot. We validate the feasibility of designs generated with our system through physics simulations and with a physically-fabricated prototype.},
  archive      = {J_TVCG},
  author       = {Xudong Feng and Jiafeng Liu and Huamin Wang and Yin Yang and Hujun Bao and Bernd Bickel and Weiwei Xu},
  doi          = {10.1109/TVCG.2019.2957218},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2881-2895},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Computational design of skinned quad-robots},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CMed: Crowd analytics for medical imaging data.
<em>TVCG</em>, <em>27</em>(6), 2869–2880. (<a
href="https://doi.org/10.1109/TVCG.2019.2953026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a visual analytics framework, CMed, for exploring medical image data annotations acquired from crowdsourcing. CMed can be used to visualize, classify, and filter crowdsourced clinical data based on a number of different metrics such as detection rate, logged events, and clustering of the annotations. CMed provides several interactive linked visualization components to analyze the crowd annotation results for a particular video and the associated workers. Additionally, all results of an individual worker can be inspected using multiple linked views in our CMed framework. We allow a crowdsourcing application analyst to observe patterns and gather insights into the crowdsourced medical data, helping him/her design future crowdsourcing applications for optimal output from the workers. We demonstrate the efficacy of our framework with two medical crowdsourcing studies: polyp detection in virtual colonoscopy videos and lung nodule detection in CT thin-slab maximum intensity projection videos. We also provide experts’ feedback to show the effectiveness of our framework. Lastly, we share the lessons we learned from our framework with suggestions for integrating our framework into a clinical workflow.},
  archive      = {J_TVCG},
  author       = {Ji Hwan Park and Saad Nadeem and Saeed Boorboor and Joseph Marino and Arie Kaufman},
  doi          = {10.1109/TVCG.2019.2953026},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2869-2880},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CMed: Crowd analytics for medical imaging data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anisotropic denoising of 3D point clouds by aggregation of
multiple surface-adaptive estimates. <em>TVCG</em>, <em>27</em>(6),
2851–2868. (<a href="https://doi.org/10.1109/TVCG.2019.2959761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point clouds commonly contain positional errors which can be regarded as noise. We propose a point cloud denoising algorithm based on aggregation of multiple anisotropic estimates computed on local coordinate systems. These local estimates are adaptive to the shape of the surface underlying the point cloud, leveraging an extension of the Local Polynomial Approximation (LPA) - Intersection of Confidence Intervals (ICI) technique to 3D point clouds. The adaptivity due to LPA-ICI is further strengthened by the dense aggregation with data-driven weights. Experimental results demonstrate state-of-the-art restoration quality of both sharp features and smooth areas.},
  archive      = {J_TVCG},
  author       = {Zhongwei Xu and Alessandro Foi},
  doi          = {10.1109/TVCG.2019.2959761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2851-2868},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Anisotropic denoising of 3D point clouds by aggregation of multiple surface-adaptive estimates},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A progressive approach to scalar field topology.
<em>TVCG</em>, <em>27</em>(6), 2833–2850. (<a
href="https://doi.org/10.1109/TVCG.2021.3060500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces progressive algorithms for the topological analysis of scalar data. Our approach is based on a hierarchical representation of the input data and the fast identification of topologically invariant vertices, which are vertices that have no impact on the topological description of the data and for which we show that no computation is required as they are introduced in the hierarchy. This enables the definition of efficient coarse-to-fine topological algorithms, which leverage fast update mechanisms for ordinary vertices and avoid computation for the topologically invariant ones. We demonstrate our approach with two examples of topological algorithms (critical point extraction and persistence diagram computation), which generate interpretable outputs upon interruption requests and which progressively refine them otherwise. Experiments on real-life datasets illustrate that our progressive strategy, in addition to the continuous visual feedback it provides, even improves run time performance with regard to non-progressive algorithms and we describe further accelerations with shared-memory parallelism. We illustrate the utility of our approach in batch-mode and interactive setups, where it respectively enables the control of the execution time of complete topological pipelines as well as previews of the topological features found in a dataset, with progressive updates delivered within interactive times.},
  archive      = {J_TVCG},
  author       = {Jules Vidal and Pierre Guillou and Julien Tierny},
  doi          = {10.1109/TVCG.2021.3060500},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2833-2850},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A progressive approach to scalar field topology},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the readability of abstract set visualizations.
<em>TVCG</em>, <em>27</em>(6), 2821–2832. (<a
href="https://doi.org/10.1109/TVCG.2021.3074615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set systems are used to model data that naturally arises in many contexts: social networks have communities, musicians have genres, and patients have symptoms. Visualizations that accurately reflect the information in the underlying set system make it possible to identify the set elements, the sets themselves, and the relationships between the sets. In static contexts, such as print media or infographics, it is necessary to capture this information without the help of interactions. With this in mind, we consider three different systems for medium-sized set data, LineSets, EulerView, and MetroSets, and report the results of a controlled human-subjects experiment comparing their effectiveness. Specifically, we evaluate the performance, in terms of time and error, on tasks that cover the spectrum of static set-based tasks. We also collect and analyze qualitative data about the three different visualization systems. Our results include statistically significant differences, suggesting that MetroSets performs and scales better.},
  archive      = {J_TVCG},
  author       = {Markus Wallinger and Ben Jacobsen and Stephen Kobourov and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2021.3074615},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2821-2832},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On the readability of abstract set visualizations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asynchronous and load-balanced union-find for distributed
and parallel scientific data visualization and analysis. <em>TVCG</em>,
<em>27</em>(6), 2808–2820. (<a
href="https://doi.org/10.1109/TVCG.2021.3074584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel distributed union-find algorithm that features asynchronous parallelism and k-d tree based load balancing for scalable visualization and analysis of scientific data. Applications of union-find include level set extraction and critical point tracking, but distributed union-find can suffer from high synchronization costs and imbalanced workloads across parallel processes. In this study, we prove that global synchronizations in existing distributed union-find can be eliminated without changing final results, allowing overlapped communications and computations for scalable processing. We also use a k-d tree decomposition to redistribute inputs, in order to improve workload balancing. We benchmark the scalability of our algorithm with up to 1,024 processes using both synthetic and application data. We demonstrate the use of our algorithm in critical point tracking and super-level set extraction with high-speed imaging experiments and fusion plasma simulations, respectively.},
  archive      = {J_TVCG},
  author       = {Jiayi Xu and Hanqi Guo and Han-Wei Shen and Mukund Raj and Xueyun Wang and Xueqiao Xu and Zhehui Wang and Tom Peterka},
  doi          = {10.1109/TVCG.2021.3074584},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2808-2820},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Asynchronous and load-balanced union-find for distributed and parallel scientific data visualization and analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smile or scowl? Looking at infographic design through the
affective lens. <em>TVCG</em>, <em>27</em>(6), 2796–2807. (<a
href="https://doi.org/10.1109/TVCG.2021.3074582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infographics are frequently promoted for their ability to communicate data to audiences affectively. To facilitate the creation of affect-stirring infographics, it is important to characterize and understand people&#39;s affective responses to infographics and derive practical design guidelines for designers. To address these research questions, we first conducted two crowdsourcing studies to identify 12 infographic-associated affective responses and collect user feedback explaining what triggered affective responses in infographics. Then, by coding the user feedback, we present a taxonomy of design heuristics that exemplifies the affect-related design factors in infographics. We evaluated the design heuristics with 15 designers. The results showed that our work supports assessing the affective design in infographics and facilitates the ideation and creation of affective infographics.},
  archive      = {J_TVCG},
  author       = {Xingyu Lan and Yang Shi and Yueyao Zhang and Nan Cao},
  doi          = {10.1109/TVCG.2021.3074582},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2796-2807},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Smile or scowl? looking at infographic design through the affective lens},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SurfRiver: Flattening stream surfaces for comparative
visualization. <em>TVCG</em>, <em>27</em>(6), 2783–2795. (<a
href="https://doi.org/10.1109/TVCG.2021.3074585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present SurfRiver, a new visual transformation approach that flattens stream surfaces in 3D to rivers in 2D for comparative visualization. Leveraging the TextFlow-like visual metaphor, SurfRiver untangles the convoluted individual stream surfaces along the flow direction and maps them along the horizontal direction of the abstract river view. It stacks multiple surfaces along the vertical direction of the river view. This visual mapping makes it easy for users to track along the flow direction and align stream surfaces for comparative study. Through brushing and linking, the river view is connected to the spatial surface view for collective reasoning. SurfRiver can be used to examine a single stream surface, investigate seeding sensitivity or variability of a family of surfaces from a group of related seeding curves, or explore a collection of representative surfaces. We describe our optimization solution to achieve the desirable mapping, present SurfRiver interface and interactions, and report results from different flow fields to demonstrate its efficacy. Feedback from a domain expert also indicates the promise of SurfRiver.},
  archive      = {J_TVCG},
  author       = {Jun Zhang and Jun Tao and Jian-Xun Wang and Chaoli Wang},
  doi          = {10.1109/TVCG.2021.3074585},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2783-2795},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SurfRiver: Flattening stream surfaces for comparative visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tac-miner: Visual tactic mining for multiple table tennis
matches. <em>TVCG</em>, <em>27</em>(6), 2770–2782. (<a
href="https://doi.org/10.1109/TVCG.2021.3074576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In table tennis, tactics specified by three consecutive strokes represent the high-level competition strategies in matches. Effective detection and analysis of tactics can reveal the playing styles of players, as well as their strengths and weaknesses. However, tactical analysis in table tennis is challenging as the analysts can often be overwhelmed by the large quantity and high dimension of the data. Statistical charts have been extensively used by researchers to explore and visualize table tennis data. However, these charts cannot support efficient comparative and correlation analysis of complicated tactic attributes. Besides, existing studies are limited to the analysis of one match. However, one player&#39;s strategy can change along with his/her opponents in different matches. Therefore, the data of multiple matches can support a more comprehensive tactical analysis. To address these issues, we introduced a visual analytics system called Tac-Miner to allow analysts to effectively analyze, explore, and compare tactics of multiple matches based on the advanced embedding and dimension reduction algorithms along with an interactive glyph. We evaluate our glyph&#39;s usability through a user study and demonstrate the system&#39;s usefulness through a case study with insights approved by coaches and domain experts.},
  archive      = {J_TVCG},
  author       = {Jiachen Wang and Jiang Wu and Anqi Cao and Zheng Zhou and Hui Zhang and Yingcai Wu},
  doi          = {10.1109/TVCG.2021.3074576},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2770-2782},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tac-miner: Visual tactic mining for multiple table tennis matches},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editors’ introduction: Special section on IEEE
PacificVis 2021. <em>TVCG</em>, <em>27</em>(6), 2768–2769. (<a
href="https://doi.org/10.1109/TVCG.2021.3074335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section of the IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) presents the five most highly rated papers from the 2021 IEEE Pacific Visualization Symposium (IEEE PacificVis). This year, IEEE PacificVis was scheduled to be hosted by Tianjin University and held in Tianjin, China, from April 19 to 22, 2021. IEEE PacificVis, sponsored by the IEEE Visualization and Graphics Technical Committee (VGTC), aims to foster greater exchange between visualization researchers and practitioners, especially in the Asia-Pacific region. This forum has grown to be a truly international event, attracting submissions and attendees from many countries in the Asia-Pacific and Europe, America, and beyond. Thus, IEEE PacificVis is serving the additional purposes of sharing the latest advances in visualization with researchers and practitioners in the region and introducing research developments in the region to the broader international visualization research community.},
  archive      = {J_TVCG},
  author       = {Nan Cao and Holger Theisel and Chaoli Wang},
  doi          = {10.1109/TVCG.2021.3074335},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {6},
  pages        = {2768-2769},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Guest editors&#39; introduction: Special section on IEEE PacificVis 2021},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Message from the program chairs and guest editors.
<em>TVCG</em>, <em>27</em>(5), v. (<a
href="https://doi.org/10.1109/TVCG.2021.3067835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section were presented at the 28th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2021), that was held virtually March 27–April 3, 2020, in Lisbon, Portugal.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2021.3067835},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {v},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the program chairs and guest editors},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Introducing the IEEE virtual reality 2021 special issue.
<em>TVCG</em>, <em>27</em>(5), iv. (<a
href="https://doi.org/10.1109/TVCG.2021.3067811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welcome to the 10 th IEEE Transactions on Visualization and Computer Graphics (TVCG) special issue on IEEE Virtual Reality and 3D User Interfaces. This volume contains a total of 25 full papers selected for and presented at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2021), held fully virtual from March 27 to April 3, 2021. Founded in 1993, IEEE VR has a long tradition as the premier venue where new research results in the field of Virtual Reality (VR) are presented. With the emergence of VR as a major technology in a diverse set of fields, such as entertainment, education, data analytics, artificial intelligence, medicine, construction, training, and many others, the papers presented at IEEE VR and published in the IEEE TVCG VR special issue mark a major highlight of the year.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller and Doug Bowman},
  doi          = {10.1109/TVCG.2021.3067811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {iv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Introducing the IEEE virtual reality 2021 special issue},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EllSeg: An ellipse segmentation framework for robust gaze
tracking. <em>TVCG</em>, <em>27</em>(5), 2757–2767. (<a
href="https://doi.org/10.1109/TVCG.2021.3067765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ellipse fitting, an essential component in pupil or iris tracking based video oculography, is performed on previously segmented eye parts generated using various computer vision techniques. Several factors, such as occlusions due to eyelid shape, camera position or eyelashes, frequently break ellipse fitting algorithms that rely on well-defined pupil or iris edge segments. In this work, we propose training a convolutional neural network to directly segment entire elliptical structures and demonstrate that such a framework is robust to occlusions and offers superior pupil and iris tracking performance (at least 10\% and 24\% increase in pupil and iris center detection rate respectively within a two-pixel error margin) compared to using standard eye parts segmentation for multiple publicly available synthetic segmentation datasets.},
  archive      = {J_TVCG},
  author       = {Rakshit S. Kothari and Aayush K. Chaudhary and Reynold J. Bailey and Jeff B. Pelz and Gabriel J. Diaz},
  doi          = {10.1109/TVCG.2021.3067765},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2757-2767},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EllSeg: An ellipse segmentation framework for robust gaze tracking},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instant panoramic texture mapping with semantic object
matching for large-scale urban scene reproduction. <em>TVCG</em>,
<em>27</em>(5), 2746–2756. (<a
href="https://doi.org/10.1109/TVCG.2021.3067768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel panoramic texture mapping-based rendering system for real-time, photorealistic reproduction of large-scale urban scenes at a street level. Various image-based rendering (IBR) methods have recently been employed to synthesize high-quality novel views, although they require an excessive number of adjacent input images or detailed geometry just to render local views. While the development of global data, such as Google Street View, has accelerated interactive IBR techniques for urban scenes, such methods have hardly been aimed at high-quality street-level rendering. To provide users with free walk-through experiences in global urban streets, our system effectively covers large-scale scenes by using sparsely sampled panoramic street-view images and simplified scene models, which are easily obtainable from open databases. Our key concept is to extract semantic information from the given street-view images and to deploy it in proper intermediate steps of the suggested pipeline, which results in enhanced rendering accuracy and performance time. Furthermore, our method supports real-time semantic 3D inpainting to handle occluded and untextured areas, which appear often when the user&#39;s viewpoint dynamically changes. Experimental results validate the effectiveness of this method in comparison with the state-of-the-art approaches. We also present real-time demos in various urban streets.},
  archive      = {J_TVCG},
  author       = {Jinwoo Park and Ik-Beom Jeon and Sung-Eui Yoon and Woontack Woo},
  doi          = {10.1109/TVCG.2021.3067768},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2746-2756},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Instant panoramic texture mapping with semantic object matching for large-scale urban scene reproduction},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LiveObj: Object semantics-based viewport prediction for live
mobile virtual reality streaming. <em>TVCG</em>, <em>27</em>(5),
2736–2745. (<a href="https://doi.org/10.1109/TVCG.2021.3067686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) video streaming (a.k.a., 360-degree video streaming) has been gaining popularity recently as a new form of multimedia providing the users with immersive viewing experience. However, the high volume of data for the 360-degree video frames creates significant bandwidth challenges. Research efforts have been made to reduce the bandwidth consumption by predicting and selectively streaming the user&#39;s viewports. However, the existing approaches require historical user or video data and cannot be applied to live streaming, the most attractive VR streaming scenario. We develop a live viewport prediction mechanism, namely LiveObj , by detecting the objects in the video based on their semantics. The detected objects are then tracked to infer the user&#39;s viewport in real time by employing a reinforcement learning algorithm. Our evaluations based on 48 users watching 10 VR videos demonstrate high prediction accuracy and significant bandwidth savings obtained by LiveObj . Also, LiveObj achieves real-time performance with low processing delays, meeting the requirement of live VR streaming.},
  archive      = {J_TVCG},
  author       = {Xianglong Feng and Zeyang Bao and Sheng Wei},
  doi          = {10.1109/TVCG.2021.3067686},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2736-2745},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LiveObj: Object semantics-based viewport prediction for live mobile virtual reality streaming},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeProCams: Simultaneous relighting, compensation and shape
reconstruction for projector-camera systems. <em>TVCG</em>,
<em>27</em>(5), 2725–2735. (<a
href="https://doi.org/10.1109/TVCG.2021.3067771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based relighting, projector compensation and depth/normal reconstruction are three important tasks of projector-camera systems (ProCams) and spatial augmented reality (SAR). Although they share a similar pipeline of finding projector-camera image mappings, in tradition, they are addressed independently, sometimes with different prerequisites, devices and sampling images. In practice, this may be cumbersome for SAR applications to address them one-by-one. In this paper, we propose a novel end-to-end trainable model named DeProCams to explicitly learn the photometric and geometric mappings of ProCams, and once trained, DeProCams can be applied simultaneously to the three tasks. DeProCams explicitly decomposes the projector-camera image mappings into three subprocesses: shading attributes estimation, rough direct light estimation and photorealistic neural rendering. A particular challenge addressed by DeProCams is occlusion, for which we exploit epipolar constraint and propose a novel differentiable projector direct light mask. Thus, it can be learned end-to-end along with the other modules. Afterwards, to improve convergence, we apply photometric and geometric constraints such that the intermediate results are plausible. In our experiments, DeProCams shows clear advantages over previous arts with promising quality and meanwhile being fully differentiable. Moreover, by solving the three tasks in a unified model, DeProCams waives the need for additional optical devices, radiometric calibrations and structured light.},
  archive      = {J_TVCG},
  author       = {Bingyao Huang and Haibin Ling},
  doi          = {10.1109/TVCG.2021.3067771},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2725-2735},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DeProCams: Simultaneous relighting, compensation and shape reconstruction for projector-camera systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the SenseMaking process through interactions and
fNIRS in immersive visualization. <em>TVCG</em>, <em>27</em>(5),
2714–2724. (<a href="https://doi.org/10.1109/TVCG.2021.3067693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theories of cognition inform our decisions when designing human-computer interfaces, and immersive systems enable us to examine these theories. This work explores the sensemaking process in an immersive environment through studying both internal and external user behaviors with a classical visualization problem: a visual comparison and clustering task. We developed an immersive system to perform a user study, collecting user behavior data from different channels: AR HMD for capturing external user interactions, functional near-infrared spectroscopy (fNIRS) for capturing internal neural sequences, and video for references. To examine sensemaking, we assessed how the layout of the interface (planar 2D vs. cylindrical 3D layout) and the challenge level of the task (low vs. high cognitive load) influenced the users&#39; interactions, how these interactions changed over time, and how they influenced task performance. We also developed a visualization system to explore joint patterns among all the data channels. We found that increased interactions and cerebral hemodynamic responses were associated with more accurate performance, especially on cognitively demanding trials. The layout types did not reliably influence interactions or task performance. We discuss how these findings inform the design and evaluation of immersive systems, predict user performance and interaction, and offer theoretical insights about sensemaking from the perspective of embodied and distributed cognition.},
  archive      = {J_TVCG},
  author       = {Alexia Galati and Riley Schoppa and Aidong Lu},
  doi          = {10.1109/TVCG.2021.3067693},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2714-2724},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the SenseMaking process through interactions and fNIRS in immersive visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hands-free interaction in immersive virtual reality: A
systematic review. <em>TVCG</em>, <em>27</em>(5), 2702–2713. (<a
href="https://doi.org/10.1109/TVCG.2021.3067687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hands are the most important tool to interact with virtual environments, and they should be available to perform the most critical tasks. For example, a surgeon in VR should keep his/her hands on the instruments and be able to do secondary tasks without performing a disruptive event to the operative task. In this common scenario, one can observe that hands are not available for interaction. The goal of this systematic review is to survey the literature and identify which hands-free interfaces are used, the performed interaction tasks, what metrics are used for interface evaluation, and the results of such evaluations. From 80 studies that met the eligibility criteria, the voice is the most studied interface, followed by the eye and head gaze. Some novel interfaces were brain interfaces and face expressions. System control and selection represent most of the interaction tasks studied and most studies evaluate interfaces for usability. Despite the best interface depending on the task and study, the voice was found to be versatile and showed good results amongst the studies. More research is recommended to improve the practical use of the interfaces and to evaluate the interfaces more formally.},
  archive      = {J_TVCG},
  author       = {Pedro Monteiro and Guilherme Gonçalves and Hugo Coelho and Miguel Melo and Maximino Bessa},
  doi          = {10.1109/TVCG.2021.3067687},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2702-2713},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hands-free interaction in immersive virtual reality: A systematic review},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quality of service impact on edge physics simulations for
VR. <em>TVCG</em>, <em>27</em>(5), 2691–2701. (<a
href="https://doi.org/10.1109/TVCG.2021.3067757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile HMDs must sacrifice compute performance to achieve ergonomic and power requirements for extended use. Consequently, applications must either reduce rendering and simulation complexity - along with the richness of the experience - or offload complexity to a server. Within the context of edge-computing, a popular way to do this is through render streaming. Render streaming has been demonstrated for desktops and consoles. It has also been explored for HMDs. However, the latency requirements of head tracking make this application much more challenging. While mobile GPUs are not yet as capable as their desktop counterparts, we note that they are becoming more powerful and efficient. With the hard requirements of VR, it is worth continuing to investigate what schemes could optimally balance load, latency and quality. We propose an alternative we call edge-physics: streaming at the scene-graph level from a simulation running on edge-resources, analogous to cluster rendering. Scene streaming is not only straightforward, but compute and bandwidth efficient. The most demanding loops run locally. Jobs that hit the power-wall of mobile CPUs are off-loaded, while improving GPUs are leveraged, maximising compute utilisation. In this paper we create a prototypical implementation and evaluate its potential in terms of fidelity, bandwidth and performance. We show that an effective system which maintains high consistencies on typical edge-links can be easily built, but that some traditional concepts are not applicable, and a better understanding of the perception of motion is required to evaluate such a system comprehensively.},
  archive      = {J_TVCG},
  author       = {Sebastian Friston and Elias Griffith and David Swapp and Caleb Lrondi and Fred Jjunju and Ryan Ward and Alan Marshall and Anthony Steed},
  doi          = {10.1109/TVCG.2021.3067757},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2691-2701},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Quality of service impact on edge physics simulations for VR},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FixationNet: Forecasting eye fixations in task-oriented
virtual environments. <em>TVCG</em>, <em>27</em>(5), 2681–2690. (<a
href="https://doi.org/10.1109/TVCG.2021.3067779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users&#39; eye fixations and other factors, i.e. users&#39; historical gaze positions, task-related objects, saliency information of the VR content, and users&#39; head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users&#39; eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8\% (from a mean error of 2.93° to 2.35°) in free-viewing and of 15.1\% (from 2.05° to 1.74°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.},
  archive      = {J_TVCG},
  author       = {Zhiming Hu and Andreas Bulling and Sheng Li and Guoping Wang},
  doi          = {10.1109/TVCG.2021.3067779},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2681-2690},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FixationNet: Forecasting eye fixations in task-oriented virtual environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Floor-vibration VR: Mitigating cybersickness using
whole-body tactile stimuli in highly realistic vehicle driving
experiences. <em>TVCG</em>, <em>27</em>(5), 2669–2680. (<a
href="https://doi.org/10.1109/TVCG.2021.3067773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses cybersickness , a major barrier to successful long-exposure immersive virtual reality (VR) experiences since user discomfort frequently leads to prematurely ending such experiences. Starting from sensory conflict theory, we posit that if a vibrating floor delivers vestibular stimuli that minimally match the vibration characteristics of a scenario, the size of the conflict between the visual and vestibular senses will be reduced and, thus, the incidence and/or severity of cybersickness will also be reduced. We integrated a custom-built, computer-controlled vibrating floor in our VR system. To evaluate the system, we implemented a realistic off-road vehicle driving simulator in which participants rode multiple laps as passengers on an off-road course. We programmed the floor to generate vertical vibrations similar to those experienced in real off-road vehicle travel. The scenario and driving conditions were designed to be cybersickness-inducing for users in both the Vibration and No-vibration conditions. We collected subjective and objective data for variables previously shown to be related to levels of cybersickness or presence. These included presence and simulator sickness questionnaires (SSQ), self-rated discomfort levels, and the physiological signals of heart rate, galvanic skin response (GSR), and pupil size. Comparing data between participants in the Vibration group (N=11) to the No-Vibration group (N=11), we found that Delta-SSQ Oculomotor response and the GSR physiological signal, both known to be positively correlated with cybersickness, were significantly lower (with large effect sizes) for the Vibration group. Other variables differed between groups in the same direction, but with trivial or small effect sizes. The results indicate that the floor vibration significantly reduced some measures of cybersickness.},
  archive      = {J_TVCG},
  author       = {Sungchul Jung and Richard Li and Ryan McKee and Mary C. Whitton and Robert W. Lindeman},
  doi          = {10.1109/TVCG.2021.3067773},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2669-2680},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Floor-vibration VR: Mitigating cybersickness using whole-body tactile stimuli in highly realistic vehicle driving experiences},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beaming displays. <em>TVCG</em>, <em>27</em>(5), 2659–2668.
(<a href="https://doi.org/10.1109/TVCG.2021.3067764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing near-eye display designs struggle to balance between multiple trade-offs such as form factor, weight, computational requirements, and battery life. These design trade-offs are major obstacles on the path towards an all-day usable near-eye display. In this work, we address these trade-offs by, paradoxically, removing the display from near-eye displays. We present the beaming displays, a new type of near-eye display system that uses a projector and an all passive wearable headset. We modify an off-the-shelf projector with additional lenses. We install such a projector to the environment to beam images from a distance to a passive wearable headset. The beaming projection system tracks the current position of a wearable headset to project distortion-free images with correct perspectives. In our system, a wearable headset guides the beamed images to a user&#39;s retina, which are then perceived as an augmented scene within a user&#39;s field of view. In addition to providing the system design of the beaming display, we provide a physical prototype and show that the beaming display can provide resolutions as high as consumer-level near-eye displays. We also discuss the different aspects of the design space for our proposal.},
  archive      = {J_TVCG},
  author       = {Yuta Itoh and Takumi Kaminokado and Kaan Akşit},
  doi          = {10.1109/TVCG.2021.3067764},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2659-2668},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beaming displays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Text entry in virtual environments using speech and a midair
keyboard. <em>TVCG</em>, <em>27</em>(5), 2648–2658. (<a
href="https://doi.org/10.1109/TVCG.2021.3067776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entering text in virtual environments can be challenging, especially without auxiliary input devices. We investigate text input in virtual reality using hand-tracking and speech. Our system visualizes users&#39; hands in the virtual environment, allowing typing on an auto-correcting midair keyboard. It also supports speaking a sentence and then correcting errors by selecting alternative words proposed by a speech recognizer. We conducted a user study in which participants wrote sentences with and without speech. Using only the keyboard, users wrote at 11 words-per-minute at a 1.2\% error rate. Speaking and correcting sentences was faster and more accurate at 28 words-per-minute and a 0.5\% error rate. Participants achieved this performance despite half of sentences containing an uncommon out-of-vocabulary word (e.g. proper name). For sentences with only in-vocabulary words, performance using speech and midair keyboard corrections was faster at 36 words-per-minute with a low 0.3\% error rate.},
  archive      = {J_TVCG},
  author       = {Jiban Adhikary and Keith Vertanen},
  doi          = {10.1109/TVCG.2021.3067776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2648-2658},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Text entry in virtual environments using speech and a midair keyboard},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A log-rectilinear transformation for foveated 360-degree
video streaming. <em>TVCG</em>, <em>27</em>(5), 2638–2647. (<a
href="https://doi.org/10.1109/TVCG.2021.3067762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapidly increasing resolutions of 360° cameras, head-mounted displays, and live-streaming services, streaming high-resolution panoramic videos over limited-bandwidth networks is becoming a critical challenge. Foveated video streaming can address this rising challenge in the context of eye-tracking-equipped virtual reality head-mounted displays. However, conventional log-polar foveated rendering suffers from a number of visual artifacts such as aliasing and flickering. In this paper, we introduce a new log-rectilinear transformation that incorporates summed-area table filtering and off-the-shelf video codecs to enable foveated streaming of 360° videos suitable for VR headsets with built-in eye-tracking. To validate our approach, we build a client-server system prototype for streaming 360° videos which leverages parallel algorithms over real-time video transcoding. We conduct quantitative experiments on an existing 360° video dataset and observe that the log-rectilinear transformation paired with summed-area table filtering heavily reduces flickering compared to log-polar subsampling while also yielding an additional 10\% reduction in bandwidth usage.},
  archive      = {J_TVCG},
  author       = {David Li and Ruofei Du and Adharsh Babu and Camelia D. Brumar and Amitabh Varshney},
  doi          = {10.1109/TVCG.2021.3067762},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2638-2647},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A log-rectilinear transformation for foveated 360-degree video streaming},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining dynamic passive haptics and haptic retargeting for
enhanced haptic feedback in virtual reality. <em>TVCG</em>,
<em>27</em>(5), 2627–2637. (<a
href="https://doi.org/10.1109/TVCG.2021.3067777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide immersive haptic experiences, proxy-based haptic feedback systems for virtual reality (VR) face two central challenges: (1) similarity, and (2) colocation. While to solve challenge (1), physical proxy objects need to be sufficiently similar to their virtual counterparts in terms of haptic properties, for challenge (2), proxies and virtual counterparts need to be sufficiently colocated to allow for seamless interactions. To solve these challenges, past research introduced, among others, two successful techniques: (a) Dynamic Passive Haptic Feedback (DPHF), a hardware-based technique that leverages actuated props adapting their physical state during the VR experience, and (b) Haptic Retargeting, a software-based technique leveraging hand redirection to bridge spatial offsets between real and virtual objects. Both concepts have, up to now, not ever been studied in combination. This paper proposes to combine both techniques and reports on the results of a perceptual and a psychophysical experiment situated in a proof-of-concept scenario focused on the perception of virtual weight distribution. We show that users in VR overestimate weight shifts and that, when DPHF and HR are combined, significantly greater shifts can be rendered, compared to using only a weight-shifting prop or unnoticeable hand redirection. Moreover, we find the combination of DPHF and HR to let significantly larger spatial dislocations of proxy and virtual counterpart go unnoticed by users. Our investigation is the first to show the value of combining DPHF and HR in practice, validating that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.},
  archive      = {J_TVCG},
  author       = {André Zenner and Kristin Ullmann and Antonio Krüger},
  doi          = {10.1109/TVCG.2021.3067777},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2627-2637},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Combining dynamic passive haptics and haptic retargeting for enhanced haptic feedback in virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tactile perceptual thresholds of electrovibration in VR.
<em>TVCG</em>, <em>27</em>(5), 2618–2626. (<a
href="https://doi.org/10.1109/TVCG.2021.3067778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic sensation plays an important role in providing physical information to users in both real environments and virtual environments. To produce high-fidelity haptic feedback, various haptic devices and tactile rendering methods have been explored in myriad scenarios, and perception deviation between a virtual environment and a real environment has been investigated. However, the tactile sensitivity for touch perception in a virtual environment has not been fully studied; thus, the necessary guidance to design haptic feedback quantitatively for virtual reality systems is lacking. This paper aims to investigate users&#39; tactile sensitivity and explore the perceptual thresholds when users are immersed in a virtual environment by utilizing electrovibration tactile feedback and by generating tactile stimuli with different waveform, frequency and amplitude characteristics. Hence, two psychophysical experiments were designed, and the experimental results were analyzed. We believe that the significance and potential of our study on tactile perceptual thresholds can promote future research that focuses on creating a favorable haptic experience for VR applications.},
  archive      = {J_TVCG},
  author       = {Lu Zhao and Yue Liu and Weitao Song},
  doi          = {10.1109/TVCG.2021.3067778},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2618-2626},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tactile perceptual thresholds of electrovibration in VR},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The influence of avatar representation on interpersonal
communication in virtual social environments. <em>TVCG</em>,
<em>27</em>(5), 2608–2617. (<a
href="https://doi.org/10.1109/TVCG.2021.3067783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current avatar representations used in immersive VR applications lack features that may be important for supporting natural behaviors and effective communication among individuals. This study investigates the impact of the visual and nonverbal cues afforded by three different types of avatar representations in the context of several cooperative tasks. The avatar types we compared are No_Avatar (HMD and controllers only), Scanned_Avatar (wearing an HMD), and Heal_Avatar (video-see-through). The subjective and objective measures we used to assess the quality of interpersonal communication include surveys of social presence, interpersonal trust, communication satisfaction, and attention to behavioral cues, plus two behavioral measures: duration of mutual gaze and number of unique words spoken. We found that participants reported higher levels of trustworthiness in the Real_Avatar condition compared to the Scanned_Avatar and No_Avatar conditions. They also reported a greater level of attentional focus on facial expressions compared to the No_Avatar condition and spent more extended time, for some tasks, attempting to engage in mutual gaze behavior compared to the Scanned_Avatar and No_Avatar conditions. In both the Heal_Avatar and Scanned_Avatar conditions, participants reported higher levels of co-presence compared with the No_Avatar condition. In the Scanned_Avatar condition, compared with the Heal_Avatar and No_Avatar conditions, participants reported higher levels of attention to body posture. Overall, our exit survey revealed that a majority of participants (66.67\%) reported a preference for the Real_Avatar, compared with 25.00\% for the Scanned_Avatar and 8.33\% for the No_Avatar, These findings provide novel insight into how a user&#39;s experience in a social VR scenario is affected by the type of avatar representation provided.},
  archive      = {J_TVCG},
  author       = {Sahar Aseeri and Victoria Interrante},
  doi          = {10.1109/TVCG.2021.3067783},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2608-2617},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The influence of avatar representation on interpersonal communication in virtual social environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GestOnHMD: Enabling gesture-based interaction on low-cost VR
head-mounted display. <em>TVCG</em>, <em>27</em>(5), 2597–2607. (<a
href="https://doi.org/10.1109/TVCG.2021.3067689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-cost virtual-reality (VR) head-mounted displays (HMDs) with the integration of smartphones have brought the immersive VR to the masses, and increased the ubiquity of VR. However, these systems are often limited by their poor interactivity. In this paper, we present GestOnHMD, a gesture-based interaction technique and a gesture-classification pipeline that leverages the stereo microphones in a commodity smartphone to detect the tapping and the scratching gestures on the front, the left, and the right surfaces on a mobile VR headset. Taking the Google Cardboard as our focused headset, we first conducted a gesture-elicitation study to generate 150 user-defined gestures with 50 on each surface. We then selected 15, 9, and 9 gestures for the front, the left, and the right surfaces respectively based on user preferences and signal detectability. We constructed a data set containing the acoustic signals of 18 users performing these on-surface gestures, and trained the deep-learning classification pipeline for gesture detection and recognition. Lastly, with the real-time demonstration of GestOnHMD, we conducted a series of online participatory-design sessions to collect a set of user-defined gesture-referent mappings that could potentially benefit from GestOnHMD.},
  archive      = {J_TVCG},
  author       = {Taizhou Chen and Lantian Xu and Xianshan Xu and Kening Zhu},
  doi          = {10.1109/TVCG.2021.3067689},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2597-2607},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GestOnHMD: Enabling gesture-based interaction on low-cost VR head-mounted display},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time omnidirectional stereo rendering: Generating 360°
surround-view panoramic images for comfortable immersive viewing.
<em>TVCG</em>, <em>27</em>(5), 2587–2596. (<a
href="https://doi.org/10.1109/TVCG.2021.3067780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surround-view panoramic images and videos have become a popular form of media for interactive viewing on mobile devices and virtual reality headsets. Viewing such media provides a sense of immersion by allowing users to control their view direction and experience an entire environment. When using a virtual reality headset, the level of immersion can be improved by leveraging stereoscopic capabilities. Stereoscopic images are generated in pairs, one for the left eye and one for the right eye, and result in providing an important depth cue for the human visual system. For computer generated imagery, rendering proper stereo pairs is well known for a fixed view. However, it is much more difficult to create omnidirectional stereo pairs for a surround-view projection that work well when looking in any direction. One major drawback of traditional omnidirectional stereo images is that they suffer from binocular misalignment in the peripheral vision as a user&#39;s view direction approaches the zenith / nadir (north / south pole) of the projection sphere. This paper presents a real-time geometry-based approach for omnidirectional stereo rendering that fits into the standard rendering pipeline. Our approach includes tunable parameters that enable pole merging - a reduction in the stereo effect near the poles that can minimize binocular misalignment. Results from a user study indicate that pole merging reduces visual fatigue and discomfort associated with binocular misalignment without inhibiting depth perception.},
  archive      = {J_TVCG},
  author       = {Thomas Marrinan and Michael E. Papka},
  doi          = {10.1109/TVCG.2021.3067780},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2587-2596},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time omnidirectional stereo rendering: Generating 360° surround-view panoramic images for comfortable immersive viewing},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-based near-eye gaze tracking beyond 10,000 hz.
<em>TVCG</em>, <em>27</em>(5), 2577–2586. (<a
href="https://doi.org/10.1109/TVCG.2021.3067784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cameras in modern gaze-tracking systems suffer from fundamental bandwidth and power limitations, constraining data acquisition speed to 300 Hz realistically. This obstructs the use of mobile eye trackers to perform, e.g., low latency predictive rendering, or to study quick and subtle eye motions like microsaccades using head-mounted devices in the wild. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial trackers when evaluated in the same conditions. Our system, previewed in Figure 1, builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the point of gaze from the parametric pupil model in real time. Using the first event-based gaze dataset, we demonstrate that our system achieves accuracies of 0.45°-1.75° for fields of view from 45° to 98°. With this technology, we hope to enable a new generation of ultra-low-latency gaze-contingent rendering and display techniques for virtual and augmented reality.},
  archive      = {J_TVCG},
  author       = {Anastasios N. Angelopoulos and Julien N.P. Martel and Amit P. Kohli and Jörg Conradt and Gordon Wetzstein},
  doi          = {10.1109/TVCG.2021.3067784},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2577-2586},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Event-based near-eye gaze tracking beyond 10,000 hz},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPinPong - virtual reality table tennis skill acquisition
using visual, haptic and temporal cues. <em>TVCG</em>, <em>27</em>(5),
2566–2576. (<a href="https://doi.org/10.1109/TVCG.2021.3067761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning an advanced skill in sports requires a huge amount of practice and players also have to overcome both physical difficulties and the dullness of repetitive training. Returning a fast spin shot in table tennis could be taken as an example, as athletes need to judge the spin type and decide the racket pose within a second, which is difficult for beginners. Therefore, in this paper, we show how to design an intuitive training system to acquire this specific skill using different cues in Virtual Reality (VR). Using VR, we can easily provide visual information, attach haptic devices, and distort the speed of time, however, it is difficult to decide which types of information could benefit the training. In an initial study, by comparing real world training with VR training, we showed the effect of VR training and obtained some insights about augmentation for training spin shots. The training system was then improved by adding three new conditions using different visualizations and temporal distortions, as well as a haptic racket for creating realistic feedback. Finally, we performed a detailed experiment, which suggest a significant improvement of skill for each condition compared to the baseline, while a qualitative evaluation indicates that both users&#39; motivation and their understanding of spin are increased by using our system.},
  archive      = {J_TVCG},
  author       = {Erwin Wu and Mitski Piekenbrock and Takuto Nakumura and Hideki Koike},
  doi          = {10.1109/TVCG.2021.3067761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2566-2576},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SPinPong - virtual reality table tennis skill acquisition using visual, haptic and temporal cues},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A privacy-preserving approach to streaming eye-tracking
data. <em>TVCG</em>, <em>27</em>(5), 2555–2565. (<a
href="https://doi.org/10.1109/TVCG.2021.3067787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye-tracking technology is being increasingly integrated into mixed reality devices. Although critical applications are being enabled, there are significant possibilities for violating user privacy expectations. We show that there is an appreciable risk of unique user identification even under natural viewing conditions in virtual reality. This identification would allow an app to connect a user&#39;s personal ID with their work ID without needing their consent, for example. To mitigate such risks we propose a framework that incorporates gatekeeping via the design of the application programming interface and via software-implemented privacy mechanisms. Our results indicate that these mechanisms can reduce the rate of identification from as much as 85\% to as low as 30\%. The impact of introducing these mechanisms is less than 1.5° error in gaze position for gaze prediction. Gaze data streams can thus be made private while still allowing for gaze prediction, for example, during foveated rendering. Our approach is the first to support privacy-by-design in the flow of eye-tracking data within mixed reality use cases.},
  archive      = {J_TVCG},
  author       = {Brendan David-John and Diane Hosfelt and Kevin Butler and Eakta Jain},
  doi          = {10.1109/TVCG.2021.3067787},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2555-2565},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A privacy-preserving approach to streaming eye-tracking data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lenslet VR: Thin, flat and wide-FOV virtual reality display
using fresnel lens and lenslet array. <em>TVCG</em>, <em>27</em>(5),
2545–2554. (<a href="https://doi.org/10.1109/TVCG.2021.3067758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new thin and flat virtual reality (VR) display design using a Fresnel lenslet array, a Fresnel lens, and a polarization-based optical folding technique. The proposed optical system has a wide field of view (FOV) of 102°x102°, a wide eye-box of 8.8 mm, and an ergonomic eye-relief of 20 mm. Simultaneously, only 3.3 mm of physical distance is required between the display panel and the lens, so that the integrated VR display can have a compact form factor like sunglasses. Moreover, since all lenslet of the lenslet array is designed to operate under on-axis condition with low aberration, the discontinuous pupil swim distortion between the lenslets is hardly observed. In addition, all on-axis lenslets can be designed identically, reducing production cost, and even off-the-shelf Fresnel optics can be used. In this paper, we introduce how we design system parameters and analyze system performance. Finally, we demonstrate two prototypes and experimentally verify that the proposed VR display system has the expected performance while having a glasses-like form factor.},
  archive      = {J_TVCG},
  author       = {Kiseung Bang and Youngjin Jo and Minseok Chae and Byoungho Lee},
  doi          = {10.1109/TVCG.2021.3067758},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2545-2554},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Lenslet VR: Thin, flat and wide-FOV virtual reality display using fresnel lens and lenslet array},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). ARC: Alignment-based redirection controller for redirected
walking in complex environments. <em>TVCG</em>, <em>27</em>(5),
2535–2544. (<a href="https://doi.org/10.1109/TVCG.2021.3067781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel redirected walking controller based on alignment that allows the user to explore large and complex virtual environments, while minimizing the number of collisions with obstacles in the physical environment. Our alignment-based redirection controller, ARC, steers the user such that their proximity to obstacles in the physical environment matches the proximity to obstacles in the virtual environment as closely as possible. To quantify a controller&#39;s performance in complex environments, we introduce a new metric, Complexity Ratio (CR), to measure the relative environment complexity and characterize the difference in navigational complexity between the physical and virtual environments. Through extensive simulation-based experiments, we show that ARC significantly outperforms current state-of-the-art controllers in its ability to steer the user on a collision-free path. We also show through quantitative and qualitative measures of performance that our controller is robust in complex environments with many obstacles. Our method is applicable to arbitrary environments and operates without any user input or parameter tweaking, aside from the layout of the environments. We have implemented our algorithm on the Oculus Quest head-mounted display and evaluated its performance in environments with varying complexity. Our project website is available at https://ganuna.umd.edu/arc/.},
  archive      = {J_TVCG},
  author       = {Niall L. Williams and Aniket Bera and Dinesh Manocha},
  doi          = {10.1109/TVCG.2021.3067781},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2535-2544},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ARC: Alignment-based redirection controller for redirected walking in complex environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group navigation for guided tours in distributed virtual
environments. <em>TVCG</em>, <em>27</em>(5), 2524–2534. (<a
href="https://doi.org/10.1109/TVCG.2021.3067756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group navigation can be an invaluable tool for performing guided tours in distributed virtual environments. Related work suggests that group navigation techniques should be comprehensible for both the guide and the attendees, assist the group in avoiding collisions with obstacles, and allow the creation of meaningful spatial arrangements with respect to objects of interest. To meet these requirements, we developed a group navigation technique based on short-distance teleportation (jumping) and evaluated its usability, comprehensibility, and scalability in an initial user study. After navigating with groups of up to 10 users through a virtual museum, participants indicated that our technique is easy to learn for guides, comprehensible also for attendees, non-nauseating for both roles, and therefore well-suited for performing guided tours.},
  archive      = {J_TVCG},
  author       = {Tim Weissker and Bernd Froehlich},
  doi          = {10.1109/TVCG.2021.3067756},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2524-2534},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Group navigation for guided tours in distributed virtual environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparing and combining virtual hand and virtual ray pointer
interactions for data manipulation in immersive analytics.
<em>TVCG</em>, <em>27</em>(5), 2513–2523. (<a
href="https://doi.org/10.1109/TVCG.2021.3067759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we evaluate two standard interaction techniques for Immersive Analytics environments: virtual hands, with actions such as grabbing and stretching, and virtual ray pointers, with actions assigned to controller buttons. We also consider a third option: seamlessly integrating both modes and allowing the user to alternate between them without explicit mode switches. Easy-to-use interaction with data visualizations in Virtual Reality enables analysts to intuitively query or filter the data, in addition to the benefit of multiple perspectives and stereoscopic 3D display. While many VR-based Immersive Analytics systems employ one of the studied interaction modes, the effect of this choice is unknown. Considering that each has different advantages, we compared the three conditions through a controlled user study in the spatio-temporal data domain. We did not find significant differences between hands and ray-casting in task performance, workload, or interactivity patterns. Yet, 60\% of the participants preferred the mixed mode and benefited from it by choosing the best alternative for each low-level task. This mode significantly reduced completion times by 23\% for the most demanding task, at the cost of a 5\% decrease in overall success rates.},
  archive      = {J_TVCG},
  author       = {Jorge Wagner and Wolfgang Stuerzlinger and Luciana Nedel},
  doi          = {10.1109/TVCG.2021.3067759},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2513-2523},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparing and combining virtual hand and virtual ray pointer interactions for data manipulation in immersive analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evidence of racial bias using immersive virtual reality:
Analysis of head and hand motions during shooting decisions.
<em>TVCG</em>, <em>27</em>(5), 2502–2512. (<a
href="https://doi.org/10.1109/TVCG.2021.3067767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shooter bias is the tendency to more quickly shoot at unarmed Black suspects compared to unarmed White suspects. The primary goal of this research was to investigate the efficacy of shooter bias simulation studies in a more realistic immersive virtual scenario instead of the traditional methodologies using desktop computers. In this paper we present results from a user study (N=99) investigating shooter and racial bias in an immersive virtual environment. Our results highlight how racial bias was observed differently in an immersive virtual environment compared to previous desktop-based simulation studies. Latency to shoot, the standard shooter bias measure, was not found to be significantly different between race or socioeconomic status in our more realistic scenarios where participants chose to raise a weapon and pull a trigger. However, more nuanced head and hand motion analysis was able to predict participants&#39; racial shooting accuracy and implicit racism scores. Discussion of how these nuanced measures can be used for detecting behavior changes for body-swap illusions, and implications of this work related to racial justice and police brutality are discussed.},
  archive      = {J_TVCG},
  author       = {Tabitha C. Peck and Jessica J. Good and Katharina Seitz},
  doi          = {10.1109/TVCG.2021.3067767},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {5},
  pages        = {2502-2512},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evidence of racial bias using immersive virtual reality: Analysis of head and hand motions during shooting decisions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene summarization via motion normalization. <em>TVCG</em>,
<em>27</em>(4), 2495–2501. (<a
href="https://doi.org/10.1109/TVCG.2020.2993195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When observing the visual world, temporal phenomena are ubiquitous: people walk, cars drive, rivers flow, clouds drift, and shadows elongate. Some of these, like water splashing and cloud motion, occur over time intervals that are either too short or too long for humans to easily observe. High-speed and timelapse videos provide a popular and compelling way to visualize these phenomena, but many real-world scenes exhibit motions occurring at a variety of rates. Once a framerate is chosen, phenomena at other rates are at best invisible, and at worst create distracting artifacts. In this article, we propose to automatically normalize the pixel-space speed of different motions in an input video to produce a seamless output with spatiotemporally varying framerate. To achieve this, we propose to analyze scenes at different timescales to isolate and analyze motions that occur at vastly different rates. Our method optionally allows a user to specify additional constraints according to artistic preferences. The motion normalized output provides a novel way to compactly visualize the changes occurring in a scene over a broad range of timescales.},
  archive      = {J_TVCG},
  author       = {Scott Wehrwein and Kavita Bala and Noah Snavely},
  doi          = {10.1109/TVCG.2020.2993195},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2495-2501},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scene summarization via motion normalization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PinNPivot: Object manipulation using pins in immersive
virtual environments. <em>TVCG</em>, <em>27</em>(4), 2488–2494. (<a
href="https://doi.org/10.1109/TVCG.2020.2987834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object manipulation techniques in immersive virtual environments are either inaccurate or slow. We present a novel technique, PinNPivot, where pins are used to constrain 1DOF/2DOF/3DOF rotations. It also supports 6DOF manipulation and 3DOF translation. A comparison with three existing techniques shows that PinNPivot is significantly more accurate and faster.},
  archive      = {J_TVCG},
  author       = {P. Christopher Gloumeau and Wolfgang Stuerzlinger and JungHyun Han},
  doi          = {10.1109/TVCG.2020.2987834},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2488-2494},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PinNPivot: Object manipulation using pins in immersive virtual environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capacitive sensing for improving contact rendering with
tangible objects in VR. <em>TVCG</em>, <em>27</em>(4), 2481–2487. (<a
href="https://doi.org/10.1109/TVCG.2020.3047689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We combine tracking information from a tangible object instrumented with capacitive sensors and an optical tracking system, to improve contact rendering when interacting with tangibles in VR. A human-subject study shows that combining capacitive sensing with optical tracking significantly improves the visuohaptic synchronization and immersion of the VR experience.},
  archive      = {J_TVCG},
  author       = {Xavier de Tinguy and Claudio Pacchierotti and Anatole Lécuyer and Maud Marchal},
  doi          = {10.1109/TVCG.2020.3047689},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2481-2487},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Capacitive sensing for improving contact rendering with tangible objects in VR},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Voting for distortion points in geometric processing.
<em>TVCG</em>, <em>27</em>(4), 2469–2480. (<a
href="https://doi.org/10.1109/TVCG.2019.2947420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low isometric distortion is often required for mesh parameterizations. A configuration of some vertices, where the distortion is concentrated, provides a way to mitigate isometric distortion, but determining the number and placement of these vertices is non-trivial. We call these vertices distortion points. We present a novel and automatic method to detect distortion points using a voting strategy. Our method integrates two components: candidate generation and candidate voting. Given a closed triangular mesh, we generate candidate distortion points by executing a three-step procedure repeatedly: (1) randomly cut an input to a disk topology; (2) compute a low conformal distortion parameterization; and (3) detect the distortion points. Finally, we count the candidate points and generate the final distortion points by voting. We demonstrate that our algorithm succeeds when employed on various closed meshes with a genus of zero or higher. The distortion points generated by our method are utilized in three applications, including planar parameterization, semi-automatic landmark correspondence, and isotropic remeshing. Compared to other state-of-the-art methods, our method demonstrates stronger practical robustness in distortion point detection.},
  archive      = {J_TVCG},
  author       = {Shuangming Chai and Xiao-Ming Fu and Ligang Liu},
  doi          = {10.1109/TVCG.2019.2947420},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2469-2480},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Voting for distortion points in geometric processing},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualizing hierarchical performance profiles of parallel
codes using CallFlow. <em>TVCG</em>, <em>27</em>(4), 2455–2468. (<a
href="https://doi.org/10.1109/TVCG.2019.2953746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calling context trees (CCTs) couple performance metrics with call paths, helping understand the execution and performance of parallel programs. To identify performance bottlenecks, programmers and performance analysts visually explore CCTs to form and validate hypotheses regarding degraded performance. However, due to the complexity of parallel programs, existing visual representations do not scale to applications running on a large number of processors. We present CallFlow, an interactive visual analysis tool that provides a high-level overview of CCTs together with semantic refinement operations to progressively explore CCTs. Using a flow-based metaphor, we visualize a CCT by treating execution time as a resource spent during the call chain, and demonstrate the effectiveness of our design with case studies on large-scale, production simulation codes.},
  archive      = {J_TVCG},
  author       = {Huu Tan Nguyen and Abhinav Bhatele and Nikhil Jain and Suraj P. Kesavan and Harsh Bhatia and Todd Gamblin and Kwan-Liu Ma and Peer-Timo Bremer},
  doi          = {10.1109/TVCG.2019.2953746},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2455-2468},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing hierarchical performance profiles of parallel codes using CallFlow},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable contour tree computation by data parallel peak
pruning. <em>TVCG</em>, <em>27</em>(4), 2437–2454. (<a
href="https://doi.org/10.1109/TVCG.2019.2948616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data sets grow to exascale, automated data analysis and visualization are increasingly important, to intermediate human understanding and to reduce demands on disk storage via in situ analysis. Trends in architecture of high performance computing systems necessitate analysis algorithms to make effective use of combinations of massively multicore and distributed systems. One of the principal analytic tools is the contour tree, which analyses relationships between contours to identify features of more than local importance. Unfortunately, the predominant algorithms for computing the contour tree are explicitly serial, and founded on serial metaphors, which has limited the scalability of this form of analysis. While there is some work on distributed contour tree computation, and separately on hybrid GPU-CPU computation, there is no efficient algorithm with strong formal guarantees on performance allied with fast practical performance. We report the first shared SMP algorithm for fully parallel contour tree computation, with formal guarantees of O(lg V lgt) parallel steps and O(V lg V) work for data with V samples and t contour tree supernodes, and implementations with more than 30× parallel speed up on both CPU using TBB and GPU using Thrust and up 70× speed up compared to the serial sweep and merge algorithm.},
  archive      = {J_TVCG},
  author       = {Hamish A. Carr and Gunther H. Weber and Christopher M. Sewell and Oliver Rübel and Patricia Fasel and James P. Ahrens},
  doi          = {10.1109/TVCG.2019.2948616},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2437-2454},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable contour tree computation by data parallel peak pruning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Programmable non-epipolar indirect light transport: Capture
and analysis. <em>TVCG</em>, <em>27</em>(4), 2421–2436. (<a
href="https://doi.org/10.1109/TVCG.2019.2946812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decomposition of light transport into direct and global components, diffuse and specular interreflections, and subsurface scattering allows for new visualizations of light in everyday scenes. In particular, indirect light contains a myriad of information about the complex appearance of materials useful for computer vision and inverse rendering applications. In this paper, we present a new imaging technique that captures and analyzes components of indirect light via light transport using a synchronized projector-camera system. The rectified system illuminates the scene with epipolar planes corresponding to projector rows, and we vary two key parameters to capture plane-to-ray light transport between projector row and camera pixel: (1) the offset between projector row and camera row in the rolling shutter (implemented as synchronization delay), and (2) the exposure of the camera row. We describe how this synchronized rolling shutter performs illumination multiplexing, and develop a nonlinear optimization algorithm to demultiplex the resulting 3D light transport operator. Using our system, we are able to capture live short and long-range non-epipolar indirect light transport, disambiguate subsurface scattering, diffuse and specular interreflections, and distinguish materials according to their subsurface scattering properties. In particular, we show the utility of indirect imaging for capturing and analyzing the hidden structure of veins in human skin.},
  archive      = {J_TVCG},
  author       = {Hiroyuki Kubo and Suren Jayasuriya and Takafumi Iwaguchi and Takuya Funatomi and Yasuhiro Mukaigawa and Srinivasa G. Narasimhan},
  doi          = {10.1109/TVCG.2019.2946812},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2421-2436},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Programmable non-epipolar indirect light transport: Capture and analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale hybrid micro-appearance modeling and realtime
rendering of thin fabrics. <em>TVCG</em>, <em>27</em>(4), 2409–2420. (<a
href="https://doi.org/10.1109/TVCG.2019.2949406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-appearance models offer state-of-the-art quality for cloth renderings. Unfortunately, they usually rely on 3D volumes or fiber meshes that are not only data-intensive but also expensive to render. Traditional surface-based models, on the other hand, are light-weight and fast to render but normally lack the fidelity and details important for design and prototyping applications. We introduce a multi-scale, hybrid model to bridge this gap for thin fabrics. Our model enjoys both the compactness and speedy rendering offered by traditional surface-based models and the rich details provided by the micro-appearance models. Further, we propose a new algorithm to convert state-of-the-art micro-appearance models into our representation while qualitatively preserving the detailed appearance. We demonstrate the effectiveness of our technique by integrating it into a real-time rendering system.},
  archive      = {J_TVCG},
  author       = {Chao Xu and Rui Wang and Shuang Zhao and Hujun Bao},
  doi          = {10.1109/TVCG.2019.2949406},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2409-2420},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-scale hybrid micro-appearance modeling and realtime rendering of thin fabrics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved deformation-driven element packing with
RepulsionPak. <em>TVCG</em>, <em>27</em>(4), 2396–2408. (<a
href="https://doi.org/10.1109/TVCG.2019.2950235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to fill a container shape with deformable instances of geometric elements selected from a library, creating a 2D artistic composition called an element packing. Each element is represented as a mass-spring system, allowing it to deform to achieve a better fit with its neighbours and the container. We start with an initial placement of small elements and gradually transform them using a physics simulation that trades off between the evenness of the packing and the deformations of the individual elements. Unlike previous work, elements can be given preferred orientations, and we can use shape matching to control the initial placement of elements in tight convex corners. We also explore the creation of tileable packings, and validate our approach using statistical measurements of the distributions of positive and negative space in packings. Our method produces compositions in which the negative space between elements is approximately uniform in width, similar to real-world examples created by artists.},
  archive      = {J_TVCG},
  author       = {Reza Adhitya Saputra and Craig S. Kaplan and Paul Asente},
  doi          = {10.1109/TVCG.2019.2950235},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2396-2408},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improved deformation-driven element packing with RepulsionPak},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicit density projection for volume conserving liquids.
<em>TVCG</em>, <em>27</em>(4), 2385–2395. (<a
href="https://doi.org/10.1109/TVCG.2019.2947437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel implicit density projection approach for hybrid Eulerian/Lagrangian methods like FLIP and APIC to enforce volume conservation of incompressible liquids. Our approach is able to robustly recover from highly degenerate configurations and incorporates volume-conserving boundary handling. A problem of the standard divergence-free pressure solver is that it only has a differential view on density changes. Numerical volume errors, which occur due to large time steps and the limited accuracy of pressure projections, are invisible to the solver and cannot be corrected. Moreover, these errors accumulate over time and can lead to drastic volume changes, especially in long-running simulations or interactive scenarios. Therefore, we introduce a novel method that enforces constant density throughout the fluid. The density itself is tracked via the particles of the hybrid Eulerian/Lagrangian simulation algorithm. To achieve constant density, we use the continuous mass conservation law to derive a pressure Poisson equation which also takes density deviations into account. It can be discretized with standard approaches and easily implemented into existing code by extending the regular pressure solver. Our method enables us to relax the strict time step and solver accuracy requirements of a regular solver, leading to significantly higher performance. Moreover, our approach is able to push fluid particles out of solid obstacles without losing volume and generates more uniform particle distributions, which makes frequent particle resampling unnecessary. We compare the proposed method to standard FLIP and APIC and to previous volume correction approaches in several simulations and demonstrate significant improvements in terms of incompressibility, visual realism, and computational performance.},
  archive      = {J_TVCG},
  author       = {Tassilo Kugelstadt and Andreas Longva and Nils Thuerey and Jan Bender},
  doi          = {10.1109/TVCG.2019.2947437},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2385-2395},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Implicit density projection for volume conserving liquids},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imma sort by two or more attributes with interpretable
monotonic multi-attribute sorting. <em>TVCG</em>, <em>27</em>(4),
2369–2384. (<a href="https://doi.org/10.1109/TVCG.2020.3043487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many choice problems often involve multiple attributes which are mentally challenging, because only one attribute is neatly sorted while others could be randomly arranged. We hypothesize that perceiving approximately monotonic trends across multiple attributes is key to the overall interpretability of sorted results, because users can easily predict the attribute values of the next items. We extend a ranking principal curve model to tune monotonic trends in attributes and present Imma Sort to sort items by multiple attributes simultaneously by trading-off the monotonicity in the primary sorted attribute to increase the human predictability for other attributes. We characterize how it performs for varying attribute correlations, attribute preferences, list lengths and number of attributes. We further extend Imma Sort with ImmaAnchor and ImmaCenter to improve the learnability and efficiency to search sorted items with conflicting attributes. We demonstrate usage scenarios for two applications and evaluate its learnability, usability, interpretability, and user performance in prediction and search tasks. We find that Imma Sort improves the interpretability and satisfaction of sorting by &gt; 2 attributes. We discuss why, when, where, and how to deploy Imma Sort for real-world applications.},
  archive      = {J_TVCG},
  author       = {Yan Lyu and Fan Gao and I-Shuen Wu and Brian Y. Lim},
  doi          = {10.1109/TVCG.2020.3043487},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2369-2384},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Imma sort by two or more attributes with interpretable monotonic multi-attribute sorting},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global beautification of 2D and 3D layouts with interactive
ambiguity resolution. <em>TVCG</em>, <em>27</em>(4), 2355–2368. (<a
href="https://doi.org/10.1109/TVCG.2019.2954321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specifying precise relationships among graphic elements is often a time-consuming process with traditional alignment tools. Automatic beautification of roughly designed layouts can provide a more efficient solution but often lead to undesired results due to ambiguity problems. To facilitate ambiguity resolution in layout beautification, we present a novel user interface for visualizing and editing inferred relationships through an automatic global layout beautification process. First, our interface provides a preview of the beautified layout with inferred constraints without directly modifying an input layout. In this way, the user can easily keep refining beautification results by interactively repositioning and/or resizing elements in the input layout. Second, we present a gestural interface for editing automatically inferred constraints by directly interacting with the visualized constraints via simple gestures. Our technique is applicable to both 2D and 3D global layout beautification, supported by efficient system implementation that provides instant user feedback. Our user study validates that our tool is capable of creating, editing, and refining layouts of graphic elements, and is significantly faster than the standard snap-dragging or command-based alignment tools for both 2D and 3D layout tasks.},
  archive      = {J_TVCG},
  author       = {Pengfei Xu and Guohang Yan and Hongbo Fu and Takeo Igarashi and Chiew-Lan Tai and Hui Huang},
  doi          = {10.1109/TVCG.2019.2954321},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2355-2368},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Global beautification of 2D and 3D layouts with interactive ambiguity resolution},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extensible spherical fibonacci grids. <em>TVCG</em>,
<em>27</em>(4), 2341–2354. (<a
href="https://doi.org/10.1109/TVCG.2019.2952131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical Fibonacci grids (SFG) yield extremely uniform point set distributions on the sphere. This feature makes SFGs particularly well-suited to a wide range of computer graphics applications, from numerical integration, to vector quantization, among others. However, the application of SFGs to problems in which further refinement of an initial point set is required is currently not possible. This is because there is currently no solution to the problem of adding new points to an existing SFG while maintaining the point set properties. In this work, we fill this gap by proposing the extensible spherical Fibonacci grids (E-SFG). We start by carrying out a formal analysis of SFGs to identify the properties which make these point sets exhibit a nearly-optimal uniform spherical distribution. Then, we propose an algorithm (E-SFG) to extend the original point set while preserving these properties. Finally, we compare the E-SFG with a other extensible spherical point sets. Our results show that the E-SFG outperforms spherical point sets based on a low discrepancy sequence both in terms of spherical cap discrepancy and in terms of root mean squared error for evaluating the rendering integral.},
  archive      = {J_TVCG},
  author       = {Ricardo Marques and Christian Bouville and Kadi Bouatouch and Josep Blat},
  doi          = {10.1109/TVCG.2019.2952131},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2341-2354},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extensible spherical fibonacci grids},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Expressive authoring of node-link diagrams with graphies.
<em>TVCG</em>, <em>27</em>(4), 2329–2340. (<a
href="https://doi.org/10.1109/TVCG.2019.2950932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expressive design environments enable visualization designers not only to specify chart types and visual mappings, but also to customize individual graphical marks, as they would in a vector graphics drawing tool. Prior work has mainly investigated how to support the expressive design of a wide range of charts generated from tabular data: bar charts, scatterplots, maps, etc. We focus here on an expressive design environment for node-link diagrams generated from multivariate networks. Such data structures raise specific challenges and opportunities in terms of visual design and interactive authoring. We discuss those specificities and describe the user-centered design process that led to Graphies, a prototype environment for expressive node-link diagram authoring. We then report on a study in which participants successfully reproduced several expressive designs, and created their own designs as well.},
  archive      = {J_TVCG},
  author       = {Hugo Romat and Caroline Appert and Emmanuel Pietriga},
  doi          = {10.1109/TVCG.2019.2950932},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2329-2340},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Expressive authoring of node-link diagrams with graphies},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrimAnalyzer: Understanding crime patterns in são paulo.
<em>TVCG</em>, <em>27</em>(4), 2313–2328. (<a
href="https://doi.org/10.1109/TVCG.2019.2947515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sao Paulo is the largest city in South America, with crime rates that reflect its size. The number and type of crimes vary considerably around the city, assuming different patterns depending on urban and social characteristics of each particular location. Previous works have mostly focused on the analysis of crimes with the intent of uncovering patterns associated to social factors, seasonality, and urban routine activities. Therefore, those studies and tools are more global in the sense that they are not designed to investigate specific regions of the city such as particular neighborhoods, avenues, or public areas. Tools able to explore specific locations of the city are essential for domain experts to accomplish their analysis in a bottom-up fashion, revealing how urban features related to mobility, passersby behavior, and presence of public infrastructures (e.g., terminals of public transportation and schools) can influence the quantity and type of crimes. In this paper, we present CrimAnalyzer, a visual analytic tool that allows users to study the behavior of crimes in specific regions of a city. The system allows users to identify local hotspots and the pattern of crimes associated to them, while still showing how hotspots and corresponding crime patterns change overtime. CrimAnalyzer has been developed from the needs of a team of experts in criminology and deals with three major challenges: i) flexibility to explore local regions and understand their crime patterns, ii) identification of spatial crime hotspots that might not be the most prevalent ones in terms of the number of crimes but that are important enough to be investigated, and iii) understand the dynamic of crime patterns overtime. The effectiveness and usefulness of the proposed system are demonstrated by qualitative and quantitative comparisons as well as by case studies run by domain experts involving real data. The experiments show the capability of CrimAnalyzer in identifying crime-related phenomena.},
  archive      = {J_TVCG},
  author       = {Germain Garcıa and Jaqueline Silveira and Jorge Poco and Afonso Paiva and Marcelo Batista Nery and Claudio T. Silva and Sérgio Adorno and Luis Gustavo Nonato},
  doi          = {10.1109/TVCG.2019.2947515},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2313-2328},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CrimAnalyzer: Understanding crime patterns in são paulo},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Content-based visual summarization for image collections.
<em>TVCG</em>, <em>27</em>(4), 2298–2312. (<a
href="https://doi.org/10.1109/TVCG.2019.2948611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the surge of images in the information era, people demand an effective and accurate way to access meaningful visual information. Accordingly, effective and accurate communication of information has become indispensable. In this article, we propose a content-based approach that automatically generates a clear and informative visual summarization based on design principles and cognitive psychology to represent image collections. We first introduce a novel method to make representative and nonredundant summarizations of image collections, thereby ensuring data cleanliness and emphasizing important information. Then, we propose a tree-based algorithm with a two-step optimization strategy to generate the final layout that operates as follows: (1) an initial layout is created by constructing a tree randomly based on the grouping results of the input image set; (2) the layout is refined through a coarse adjustment in a greedy manner, followed by gradient back propagation drawing on the training procedure of neural networks. We demonstrate the usefulness and effectiveness of our method via extensive experimental results and user studies. Our visual summarization algorithm can precisely and efficiently capture the main content of image collections better than alternative methods or commercial tools.},
  archive      = {J_TVCG},
  author       = {Xingjia Pan and Fan Tang and Weiming Dong and Chongyang Ma and Yiping Meng and Feiyue Huang and Tong-Yee Lee and Changsheng Xu},
  doi          = {10.1109/TVCG.2019.2948611},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2298-2312},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Content-based visual summarization for image collections},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing spaces and times for tactical analysis in
football. <em>TVCG</em>, <em>27</em>(4), 2280–2297. (<a
href="https://doi.org/10.1109/TVCG.2019.2952129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A possible objective in analyzing trajectories of multiple simultaneously moving objects, such as football players during a game, is to extract and understand the general patterns of coordinated movement in different classes of situations as they develop. For achieving this objective, we propose an approach that includes a combination of query techniques for flexible selection of episodes of situation development, a method for dynamic aggregation of data from selected groups of episodes, and a data structure for representing the aggregates that enables their exploration and use in further analysis. The aggregation, which is meant to abstract general movement patterns, involves construction of new time-homomorphic reference systems owing to iterative application of aggregation operators to a sequence of data selections. As similar patterns may occur at different spatial locations, we also propose constructing new spatial reference systems for aligning and matching movements irrespective of their absolute locations. The approach was tested in application to tracking data from two Bundesliga games of the 2018/2019 season. It enabled detection of interesting and meaningful general patterns of team behaviors in three classes of situations defined by football experts. The experts found the approach and the underlying concepts worth implementing in tools for football analysts.},
  archive      = {J_TVCG},
  author       = {Gennady Andrienko and Natalia Andrienko and Gabriel Anzer and Pascal Bauer and Guido Budziak and Georg Fuchs and Dirk Hecker and Hendrik Weber and Stefan Wrobel},
  doi          = {10.1109/TVCG.2019.2952129},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2280-2297},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Constructing spaces and times for tactical analysis in football},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated geometric registration for multi-projector
displays on arbitrary 3D shapes using uncalibrated devices.
<em>TVCG</em>, <em>27</em>(4), 2265–2279. (<a
href="https://doi.org/10.1109/TVCG.2019.2950942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we present a completely automated and scalable multi-projector registration system that allows multiple uncalibrated projectors and cameras on arbitrary shape surfaces. Our method estimates the parameters of multiple uncalibrated tiled or superimposed projectors, the extrinsic parameters of the observing cameras (with known intrinsic parameters), the shape of the illuminated 3D geometry and geometrically registers the projectors on it. This is achieved without using any fiducials, even if part of the surface is visible to only one camera. The method uses a completely automatic approach for cross-correlation and cross-validation of the device parameters and the surface geometry resulting in an accurate registration on the arbitrary unknown geometry that does not need an accurate prior calibration of each of the uncalibrated devices using physical patterns or fiducials. Estimating projector parameters allows for quick recalibration of the system in the face of projector movements, by re-estimating only the parameters of the moved projector and not the entire system. Thus, our work can enable easy deployment of spatially augmented reality environments of different sizes (from small table top objects to large immersive environments), different shapes (inside-looking-out or outside-looking in), and different configurations (tiled or superimposed) using the same proposed method.},
  archive      = {J_TVCG},
  author       = {Mahdi Abbaspour Tehrani and M. Gopi and Aditi Majumder},
  doi          = {10.1109/TVCG.2019.2950942},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2265-2279},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automated geometric registration for multi-projector displays on arbitrary 3D shapes using uncalibrated devices},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active arrangement of small objects in 3D indoor scenes.
<em>TVCG</em>, <em>27</em>(4), 2250–2264. (<a
href="https://doi.org/10.1109/TVCG.2019.2949295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object arrangement is very important for creating detailed and realistic 3D indoor scenes. In this article, we present an interactive framework based on active learning to help users create customized arrangements for small objects according to their preferences. To achieve this with minimal user effort, we first learn the prior knowledge about small object arrangement from a 3D indoor scene dataset through a probability mining method, which forms the initial guidance for arranging small objects. Then, users are able to express their preferences on a few small object categories, which are automatically propagated to all the other categories via a novel active learning approach. In the propagation process, we introduce a novel metric to obtain the propagation weights, which measures the degree of interchangeability between two small object categories, and is calculated based on a spatial embedding model learned from the small object neighborhood information extracted from the 3D indoor scene dataset. Experiments show that our framework is able to help users effectively create customized small object arrangements with little effort.},
  archive      = {J_TVCG},
  author       = {Suiyun Zhang and Zhizhong Han and Yu-Kun Lai and Matthias Zwicker and Hui Zhang},
  doi          = {10.1109/TVCG.2019.2949295},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {4},
  pages        = {2250-2264},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Active arrangement of small objects in 3D indoor scenes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Further towards unambiguous edge bundling: Investigating
power-confluent drawings for network visualization. <em>TVCG</em>,
<em>27</em>(3), 2244–2249. (<a
href="https://doi.org/10.1109/TVCG.2019.2944619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bach et al. [1] recently presented an algorithm for constructing confluent drawings, by leveraging power graph decomposition to generate an auxiliary routing graph. We identify two issues with their method which we call the node split and short-circuit problems, and solve both by modifying the routing graph to retain the hierarchical structure of power groups. We also classify the exact type of confluent drawings that the algorithm can produce as `power-confluent&#39;, and prove that it is a subclass of the previously studied `strict confluent&#39; drawing. A description and source code of our implementation is also provided, which additionally includes an improved method for power graph construction.},
  archive      = {J_TVCG},
  author       = {Jonathan X. Zheng and Samraat Pawar and Dan F. M. Goodman},
  doi          = {10.1109/TVCG.2019.2944619},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2244-2249},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Further towards unambiguous edge bundling: Investigating power-confluent drawings for network visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ENTROPiA: Towards infinite surface haptic displays in
virtual reality using encountered-type rotating props. <em>TVCG</em>,
<em>27</em>(3), 2237–2243. (<a
href="https://doi.org/10.1109/TVCG.2019.2963190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose an approach towards an infinite surface haptic display. Our approach, named ENcountered-Type ROtating Prop Approach (ENTROPiA) is based on a cylindrical spinning prop attached to a robot&#39;s end-effector serving as an encountered-type haptic display (ETHD). This type of haptic display permits the users to have an unconstrained, free-hand contact with a surface being provided by a robotic device for the users&#39; to encounter a surface to be touched. In our approach, the sensation of touching a virtual surface is given by an interaction technique that couples with the sliding movement of the prop under the users&#39; finger by tracking their hand location and establishing a path to be explored. This approach enables large motion for a larger surface rendering, permits to render multi-textured haptic feedback, and leverages the ETHD approach introducing large motion and sliding/friction sensations. As a part of our contribution, a proof of concept was designed for illustrating our approach. A user study was conducted to assess the perception of our approach showing a significant performance for rendering the sensation of touching a large flat surface. Our approach could be used to render large haptic surfaces in applications such as rapid prototyping for automobile design.},
  archive      = {J_TVCG},
  author       = {Víctor Mercado and Maud Marchal and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2019.2963190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2237-2243},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ENTROPiA: Towards infinite surface haptic displays in virtual reality using encountered-type rotating props},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Why visualize? Untangling a large network of arguments.
<em>TVCG</em>, <em>27</em>(3), 2220–2236. (<a
href="https://doi.org/10.1109/TVCG.2019.2940026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization has been deemed a useful technique by researchers and practitioners, alike, leaving a trail of arguments behind that reason why visualization works. In addition, examples of misleading usages of visualizations in information communication have occasionally been pointed out. Thus, to contribute to the fundamental understanding of our discipline, we require a comprehensive collection of arguments on “why visualize?” (or “why not?”), untangling the rationale behind positive and negative viewpoints. In this paper, we report a theoretical study to understand the underlying reasons of various arguments; their relationships (e.g., built-on, and conflict); and their respective dependencies on tasks, users, and data. We curated an argumentative network based on a collection of arguments from various fields, including information visualization, cognitive science, psychology, statistics, philosophy, and others. Our work proposes several categorizations for the arguments, and makes their relations explicit. We contribute the first comprehensive and systematic theoretical study of the arguments on visualization. Thereby, we provide a roadmap towards building a foundation for visualization theory and empirical research as well as for practical application in the critique and design of visualizations. In addition, we provide our argumentation network and argument collection online at https://whyvis.dbvis.de, supported by an interactive visualization.},
  archive      = {J_TVCG},
  author       = {Dirk Streeb and Mennatallah El-Assady and Daniel A. Keim and Min Chen},
  doi          = {10.1109/TVCG.2019.2940026},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2220-2236},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Why visualize? untangling a large network of arguments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual quality of 3D meshes with diffuse colors in virtual
reality: Subjective and objective evaluation. <em>TVCG</em>,
<em>27</em>(3), 2202–2219. (<a
href="https://doi.org/10.1109/TVCG.2020.3036153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface meshes associated with diffuse texture or color attributes are becoming popular multimedia contents. They provide a high degree of realism and allow six degrees of freedom (6DoF) interactions in immersive virtual reality environments. Just like other types of multimedia, 3D meshes are subject to a wide range of processing, e.g., simplification and compression, which result in a loss of quality of the final rendered scene. Thus, both subjective studies and objective metrics are needed to understand and predict this visual loss. In this work, we introduce a large dataset of 480 animated meshes with diffuse color information, and associated with perceived quality judgments. The stimuli were generated from 5 source models subjected to geometry and color distortions. Each stimulus was associated with 6 hypothetical rendering trajectories (HRTs): combinations of 3 viewpoints and 2 animations. A total of 11520 quality judgments (24 per stimulus) were acquired in a subjective experiment conducted in virtual reality. The results allowed us to explore the influence of source models, animations and viewpoints on both the quality scores and their confidence intervals. Based on these findings, we propose the first metric for quality assessment of 3D meshes with diffuse colors, which works entirely on the mesh domain. This metric incorporates perceptually-relevant curvature-based and color-based features. We evaluate its performance, as well as a number of Image Quality Metrics (IQMs), on two datasets: ours and a dataset of distorted textured meshes. Our metric demonstrates good results and a better stability than IQMs. Finally, we investigated how the knowledge of the viewpoint (i.e., the visible parts of the 3D model) may improve the results of objective metrics.},
  archive      = {J_TVCG},
  author       = {Yana Nehmé and Florent Dupont and Jean-Philippe Farrugia and Patrick Le Callet and Guillaume Lavoué},
  doi          = {10.1109/TVCG.2020.3036153},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2202-2219},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual quality of 3D meshes with diffuse colors in virtual reality: Subjective and objective evaluation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual cause analytics for traffic congestion.
<em>TVCG</em>, <em>27</em>(3), 2186–2201. (<a
href="https://doi.org/10.1109/TVCG.2019.2940580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban traffic congestion has become an important issue not only affecting our daily lives, but also limiting economic development. The primary cause of urban traffic congestion is that the number of vehicles is higher than the permissible limit of the road. Previous studies have focused on dispersing traffic volume by detecting urban traffic congestion zones and predicting future trends. However, to solve the fundamental problem, it is necessary to discover the cause of traffic congestion. Nevertheless, it is difficult to find a research which presents an approach to identify the causes of traffic congestion. In this paper, we propose a technique to analyze the cause of traffic congestion based on the traffic flow theory. We extract vehicle flows from traffic data, such as GPS trajectory and Vehicle Detector data. We detect vehicle flow changes utilizing the entropy from the information theory. Then, we build cumulative vehicle count curves (N-curve) that can quantify the flow of the vehicles in the traffic congestion area. The N-curves are classified into four different traffic congestion patterns by a convolutional neural network. Analyzing the causes and influence of traffic congestion is difficult and requires considerable experience and knowledge. Therefore, we present a visual analytics system that can efficiently perform a series of processes to analyze the cause and influence of traffic congestion. Through case studies, we have evaluated that our system can classify the causes of traffic congestion and can be used efficiently in road planning.},
  archive      = {J_TVCG},
  author       = {Mingyu Pi and Hanbyul Yeon and Hyesook Son and Yun Jang},
  doi          = {10.1109/TVCG.2019.2940580},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2186-2201},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual cause analytics for traffic congestion},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analytics of a computer-aided diagnosis system for
pancreatic lesions. <em>TVCG</em>, <em>27</em>(3), 2174–2185. (<a
href="https://doi.org/10.1109/TVCG.2019.2947037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is a powerful and effective tool for medical image analysis to perform computer-aided diagnosis (CAD). Having great potential in improving the accuracy of a diagnosis, CAD systems are often analyzed in terms of the final accuracy, leading to a limited understanding of the internal decision process, impossibility to gain insights, and ultimately to skepticism from clinicians. We present a visual analytics approach to uncover the decision-making process of a CAD system for classifying pancreatic cystic lesions. This CAD algorithm consists of two distinct components: random forest (RF), which classifies a set of predefined features, including demographic features, and a convolutional neural network (CNN), which analyzes radiological (imaging) features of the lesions. We study the class probabilities generated by the RF and the semantical meaning of the features learned by the CNN. We also use an eye tracker to better understand which radiological features are particularly useful for a radiologist to make a diagnosis and to quantitatively compare with the features that lead the CNN to its final classification decision. Additionally, we evaluate the effects and benefits of supplying the CAD system with a case-based visual aid in a second-reader setting.},
  archive      = {J_TVCG},
  author       = {Konstantin Dmitriev and Joseph Marino and Kevin Baker and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2019.2947037},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2174-2185},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics of a computer-aided diagnosis system for pancreatic lesions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward a quantitative survey of dimension reduction
techniques. <em>TVCG</em>, <em>27</em>(3), 2153–2173. (<a
href="https://doi.org/10.1109/TVCG.2019.2944182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction methods, also known as projections, are frequently used in multidimensional data exploration in machine learning, data science, and information visualization. Tens of such techniques have been proposed, aiming to address a wide set of requirements, such as ability to show the high-dimensional data structure, distance or neighborhood preservation, computational scalability, stability to data noise and/or outliers, and practical ease of use. However, it is far from clear for practitioners how to choose the best technique for a given use context. We present a survey of a wide body of projection techniques that helps answering this question. For this, we characterize the input data space, projection techniques, and the quality of projections, by several quantitative metrics. We sample these three spaces according to these metrics, aiming at good coverage with bounded effort. We describe our measurements and outline observed dependencies of the measured variables. Based on these results, we draw several conclusions that help comparing projection techniques, explain their results for different types of data, and ultimately help practitioners when choosing a projection for a given context. Our methodology, datasets, projection implementations, metrics, visualizations, and results are publicly open, so interested stakeholders can examine and/or extend this benchmark.},
  archive      = {J_TVCG},
  author       = {Mateus Espadoto and Rafael M. Martins and Andreas Kerren and Nina S. T. Hirata and Alexandru C. Telea},
  doi          = {10.1109/TVCG.2019.2944182},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2153-2173},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Toward a quantitative survey of dimension reduction techniques},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-based effectiveness of interactive contiguous area
cartograms. <em>TVCG</em>, <em>27</em>(3), 2136–2152. (<a
href="https://doi.org/10.1109/TVCG.2020.3041745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cartograms are map-based data visualizations in which the area of each map region is proportional to an associated numeric data value (e.g., population or gross domestic product). A cartogram is called contiguous if it conforms to this area principle while also keeping neighboring regions connected. Because of their distorted appearance, contiguous cartograms have been criticized as difficult to read. Some authors have suggested that cartograms may be more legible if they are accompanied by interactive features (e.g., animations, linked brushing, or infotips). We conducted an experiment to evaluate this claim. Participants had to perform visual analysis tasks with interactive and noninteractive contiguous cartograms. The task types covered various aspects of cartogram readability, ranging from elementary lookup tasks to synoptic tasks (i.e., tasks in which participants had to summarize high-level differences between two cartograms). Elementary tasks were carried out equally well with and without interactivity. Synoptic tasks, by contrast, were more difficult without interactive features. With access to interactivity, however, most participants answered even synoptic questions correctly. In a subsequent survey, participants rated the interactive features as “easy to use” and “helpful.” Our study suggests that interactivity has the potential to make contiguous cartograms accessible even for those readers who are unfamiliar with interactive computer graphics or do not have a prior affinity to working with maps. Among the interactive features, animations had the strongest positive effect, so we recommend them as a minimum of interactivity when contiguous cartograms are displayed on a computer screen.},
  archive      = {J_TVCG},
  author       = {Ian K. Duncan and Shi Tingsheng and Simon T. Perrault and Michael T. Gastner},
  doi          = {10.1109/TVCG.2020.3041745},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2136-2152},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Task-based effectiveness of interactive contiguous area cartograms},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthesizing camera noise using generative adversarial
networks. <em>TVCG</em>, <em>27</em>(3), 2123–2135. (<a
href="https://doi.org/10.1109/TVCG.2020.3012120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a technique for synthesizing realistic noise for digital photographs. It can adjust the noise level of an input photograph, either increasing or decreasing it, to match a target ISO level. Our solution learns the mappings among different ISO levels from unpaired data using generative adversarial networks. We demonstrate its effectiveness both quantitatively, using Kullback-Leibler divergence and Kolmogorov-Smirnov test, and qualitatively through a large number of examples. We also demonstrate its practical applicability by using its results to significantly improve the performance of a state-of-the-art trainable denoising method. Our technique should benefit several computer-vision applications that seek robustness to noisy scenarios.},
  archive      = {J_TVCG},
  author       = {Bernardo Henz and Eduardo S. L. Gastal and Manuel M. Oliveira},
  doi          = {10.1109/TVCG.2020.3012120},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2123-2135},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Synthesizing camera noise using generative adversarial networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Survey of immersive analytics. <em>TVCG</em>,
<em>27</em>(3), 2101–2122. (<a
href="https://doi.org/10.1109/TVCG.2019.2929033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive analytics (IA) is a new term referring to the use of immersive technologies for data analysis. Yet such applications are not new, and numerous contributions have been made in the last three decades. However, no survey reviewing all these contributions is available. Here we propose a survey of IA from the early nineties until the present day, describing how rendering technologies, data, sensory mapping, and interaction means have been used to build IA systems, as well as how these systems have been evaluated. The conclusions that emerge from our analysis are that: multi-sensory aspects of IA are under-exploited, the 3DUI and VR community knowledge regarding immersive interaction is not sufficiently utilised, the IA community should focus on converging towards best practices, as well as aim for real life IA systems.},
  archive      = {J_TVCG},
  author       = {Adrien Fonnet and Yannick Prié},
  doi          = {10.1109/TVCG.2019.2929033},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2101-2122},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey of immersive analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse data driven mesh deformation. <em>TVCG</em>,
<em>27</em>(3), 2085–2100. (<a
href="https://doi.org/10.1109/TVCG.2019.2941200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Example-based mesh deformation methods are powerful tools for realistic shape editing. However, existing techniques typically combine all the example deformation modes, which can lead to overfitting, i.e., using an overly complicated model to explain the user-specified deformation. This leads to implausible or unstable deformation results, including unexpected global changes outside the region of interest. To address this fundamental limitation, we propose a sparse blending method that automatically selects a smaller number of deformation modes to compactly describe the desired deformation. This along with a suitably chosen deformation basis including spatially localized deformation modes leads to significant advantages, including more meaningful, reliable, and efficient deformations because fewer and localized deformation modes are applied. To cope with large rotations, we develop a simple but effective representation based on polar decomposition of deformation gradients, which resolves the ambiguity of large global rotations using an as-consistent-as-possible global optimization. This simple representation has a closed form solution for derivatives, making it efficient for our sparse localized representation and thus ensuring interactive performance. Experimental results show that our method outperforms state-of-the-art data-driven mesh deformation methods, for both quality of results and efficiency.},
  archive      = {J_TVCG},
  author       = {Lin Gao and Yu-Kun Lai and Jie Yang and Ling-Xiao Zhang and Shihong Xia and Leif Kobbelt},
  doi          = {10.1109/TVCG.2019.2941200},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2085-2100},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sparse data driven mesh deformation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shallow sand equations: Real-time height field simulation of
dry granular flows. <em>TVCG</em>, <em>27</em>(3), 2073–2084. (<a
href="https://doi.org/10.1109/TVCG.2019.2944172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular media is the second-most-manipulated substance on Earth, second only to water. However, simulation of granular media is still challenging due to the complexity of granular materials and the large number of discrete solid particles. As we know, dry granular materials could form a hybrid state between a fluid and a solid, therefore we propose a two-layer model and divide the simulation domain into a dilute layer, where granules can move freely as a fluid, and a dense layer, where granules act more like a solid. Motivated by the shallow water equations, we derive a set of shallow sand equations for modeling dry granular flows by depth-integrating three-dimensional governing equations along its vertical direction. Unlike previous methods for simulating a 2D granular media, our model does not restrict the depth of the granular media to be shallow anymore. To allow efficient fluid-solid interactions, we also present a ray casting algorithm for one-way solid-fluid coupling. Finally, we introduce a particle-tracking method to improve the visual representation. Our method can be efficiently implemented based on a height field and is fully compatible with modern GPUs, therefore allows us to simulate large-scale dry granular flows in real time.},
  archive      = {J_TVCG},
  author       = {Kuixin Zhu and Xiaowei He and Sheng Li and Hongan Wang and Guoping Wang},
  doi          = {10.1109/TVCG.2019.2944172},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2073-2084},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shallow sand equations: Real-time height field simulation of dry granular flows},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Same stats, different graphs: Exploring the space of graphs
in terms of graph properties. <em>TVCG</em>, <em>27</em>(3), 2056–2072.
(<a href="https://doi.org/10.1109/TVCG.2019.2946558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data analysts commonly utilize statistics to summarize large datasets. While it is often sufficient to explore only the summary statistics of a dataset (e.g., min/mean/max), Anscombe&#39;s Quartet demonstrates how such statistics can be misleading. We consider a similar problem in the context of graph mining. To study the relationships between different graph properties, we examine low-order non-isomorphic graphs and provide a simple visual analytics system to explore correlations across multiple graph properties. However, for larger graphs, studying the entire space quickly becomes intractable. We use different random graph generation methods to further look into the distribution of graph properties for higher order graphs and investigate the impact of various sampling methodologies. We also describe a method for generating many graphs that are identical over a number of graph properties and statistics yet are clearly different and identifiably distinct.},
  archive      = {J_TVCG},
  author       = {Hang Chen and Utkarsh Soni and Yafeng Lu and Vahan Huroyan and Ross Maciejewski and Stephen Kobourov},
  doi          = {10.1109/TVCG.2019.2946558},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2056-2072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Same stats, different graphs: Exploring the space of graphs in terms of graph properties},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust reflectance estimation for projection-based
appearance control in a dynamic light environment. <em>TVCG</em>,
<em>27</em>(3), 2041–2055. (<a
href="https://doi.org/10.1109/TVCG.2019.2940453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method that robustly estimates the reflectance, even in an environment with dynamically changing light. To control the appearance of an object by using a projector-camera system, an appropriate estimate of the object&#39;s reflectance is vital to the creation of an appropriate projection image. Most conventional estimation methods assume static light conditions; however, in practice, the appearance is affected by both the reflectance and environmental light. In an environment with dynamically changing light, conventional reflectance estimation methods require calibration every time the conditions change. In contrast, our method requires no additional calibration because it simultaneously estimates both the reflectance and environmental light. Our method is based on the concept of creating two different light conditions by switching the projection at a rate higher than that perceived by the human eye and captures the images of a target object separately under each condition. The reflectance and environmental light are then simultaneously estimated by using the pair of images acquired under these two conditions. We implemented a projector-camera system that switches the projection on and off at 120 Hz. Experiments confirm the robustness of our method when changing the environmental light. Further, our method can robustly estimate the reflectance under practical indoor lighting conditions.},
  archive      = {J_TVCG},
  author       = {Ryo Akiyama and Goshiro Yamamoto and Toshiyuki Amano and Takafumi Taketomi and Alexander Plopski and Christian Sandor and Hirokazu Kato},
  doi          = {10.1109/TVCG.2019.2940453},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2041-2055},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robust reflectance estimation for projection-based appearance control in a dynamic light environment},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refocusable gigapixel panoramas for immersive VR
experiences. <em>TVCG</em>, <em>27</em>(3), 2028–2040. (<a
href="https://doi.org/10.1109/TVCG.2019.2940444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been significant advances in capturing gigapixel panoramas (GPP). However, solutions for viewing GPPs on head-mounted displays (HMDs) are lagging: an immersive experience requires ultra-fast rendering while directly loading a GPP onto the GPU is infeasible due to limited texture memory capacity. In this paper, we present a novel out-of-core rendering technique that supports not only classic panning, tilting, and zooming but also dynamic refocusing for viewing a GPP on HMD. Inspired by the network package transmission mechanisms in distributed visualization, our approach employs hierarchical image tiling and on-demand data updates across the main and the GPU memory. We further present a multi-resolution rendering scheme and a refocused light field rendering technique based on RGBD GPPs with minimal memory overhead. Comprehensive experiments demonstrate that our technique is highly efficient and reliable, able to achieve ultra-high frame rates ( $&amp;gt; 50$ fps) even on low-end GPUs. With an embedded gaze tracker, our technique enables immersive panorama viewing experiences with unprecedented resolutions, field-of-view, and focus variations while maintaining smooth spatial, angular, and focal transitions.},
  archive      = {J_TVCG},
  author       = {Wentao Lyu and Peng Ding and Yingliang Zhang and Anpei Chen and Minye Wu and Shu Yin and Jingyi Yu},
  doi          = {10.1109/TVCG.2019.2940444},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2028-2040},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Refocusable gigapixel panoramas for immersive VR experiences},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pointfilter: Point cloud filtering via encoder-decoder
modeling. <em>TVCG</em>, <em>27</em>(3), 2015–2027. (<a
href="https://doi.org/10.1109/TVCG.2020.3027069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud filtering is a fundamental problem in geometry modeling and processing. Despite of significant advancement in recent years, the existing methods still suffer from two issues: 1) they are either designed without preserving sharp features or less robust in feature preservation; and 2) they usually have many parameters and require tedious parameter tuning. In this article, we propose a novel deep learning approach that automatically and robustly filters point clouds by removing noise and preserving their sharp features. Our point-wise learning architecture consists of an encoder and a decoder. The encoder directly takes points (a point and its neighbors) as input, and learns a latent representation vector which goes through the decoder to relate the ground-truth position with a displacement vector. The trained neural network can automatically generate a set of clean points from a noisy input. Extensive experiments show that our approach outperforms the state-of-the-art deep learning techniques in terms of both visual quality and quantitative error metrics. The source code and dataset can be found at https://github.com/dongbo-BUAA-VR/Pointfilter.},
  archive      = {J_TVCG},
  author       = {Dongbo Zhang and Xuequan Lu and Hong Qin and Ying He},
  doi          = {10.1109/TVCG.2020.3027069},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2015-2027},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Pointfilter: Point cloud filtering via encoder-decoder modeling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phoenixmap: An abstract approach to visualize 2D spatial
distributions. <em>TVCG</em>, <em>27</em>(3), 2000–2014. (<a
href="https://doi.org/10.1109/TVCG.2019.2945960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multidimensional nature of spatial data poses a challenge for visualization. In this paper, we introduce Phoenixmap, a simple abstract visualization method to address the issue of visualizing multiple spatial distributions at once. The Phoenixmap approach starts by identifying the enclosed outline of the point collection, then assigns different widths to outline segments according to the segments&#39; corresponding inside regions. Thus, one 2D distribution is represented as an outline with varied thicknesses. Phoenixmap is capable of overlaying multiple outlines and comparing them across categories of objects in a 2D space. We chose heatmap as a benchmark spatial visualization method and conducted user studies to compare performances among Phoenixmap, heatmap, and dot distribution map. Based on the analysis and participant feedback, we demonstrate that Phoenixmap 1) allows users to perceive and compare spatial distribution data efficiently; 2) frees up graphics space with a concise form that can provide visualization design possibilities like overlapping; and 3) provides a good quantitative perceptual estimating capability given the proper legends. Finally, we discuss several possible applications of Phoenixmap and present one visualization of multiple species of birds&#39; active regions in a nature preserve.},
  archive      = {J_TVCG},
  author       = {Junhan Zhao and Xiang Liu and Chen Guo and Zhenyu Cheryl Qian and Yingjie Victor Chen},
  doi          = {10.1109/TVCG.2019.2945960},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {2000-2014},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Phoenixmap: An abstract approach to visualize 2D spatial distributions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MineTime insight: Visualizing meeting habits to promote
informed scheduling decisions. <em>TVCG</em>, <em>27</em>(3), 1986–1999.
(<a href="https://doi.org/10.1109/TVCG.2019.2941208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate meetings are a crucial part of business activities. While numerous academic papers investigated how to make the scheduling process of meetings faster or even automatic, little work has been done yet to facilitate the retrospective reasoning about how time is spent on meetings. Traditional calendar applications do not allow users to extract actionable statistics although it has been shown that reflection-oriented design can increase the users&#39; understanding of their habits and can thereby encourage a shift towards better practices. In this paper, we present MineTime Insight, a tool made of multiple coordinated views for the exploration of personal calendar data, with the overarching goal of improving short and long-term scheduling decisions. Despite being focused on the working environment, our work builds upon recent results in the field of Personal Visual Analytics, as it targets users not necessarily expert in visualization and data analysis. We demonstrate the potential of MineTime Insight, when applied to the agenda of an executive manager. Finally, we discuss the results of an informal user study and a field study. Our results suggest that our visual representations are perceived as easy to understand and helpful towards a change in the scheduling habits.},
  archive      = {J_TVCG},
  author       = {Marco Ancona and Marilou Beyeler and Markus Gross and Tobias Günther},
  doi          = {10.1109/TVCG.2019.2941208},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1986-1999},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MineTime insight: Visualizing meeting habits to promote informed scheduling decisions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integral curve clustering and simplification for flow
visualization: A comparative evaluation. <em>TVCG</em>, <em>27</em>(3),
1967–1985. (<a href="https://doi.org/10.1109/TVCG.2019.2940935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised clustering techniques have been widely applied to flow simulation data to alleviate clutter and occlusion in the resulting visualization. However, there is an absence of systematic guidelines for users to evaluate (both quantitatively and visually) the appropriate clustering technique and similarity measures for streamline and pathline curves. In this work, we provide an overview of a number of prevailing curve clustering techniques. We then perform a comprehensive experimental study to qualitatively and quantitatively compare these clustering techniques coupled with popular similarity measures used in the flow visualization literature. Based on our experimental results, we derive empirical guidelines for selecting the appropriate clustering technique and similarity measure given the requirements of the visualization task. We believe our work will inform the task of generating meaningful reduced representations for large-scale flow data and inspire the continuous investigation of a more refined guidance on clustering technique selection.},
  archive      = {J_TVCG},
  author       = {Lieyu Shi and Robert S. Laramee and Guoning Chen},
  doi          = {10.1109/TVCG.2019.2940935},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1967-1985},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Integral curve clustering and simplification for flow visualization: A comparative evaluation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Heter-sim: Heterogeneous multi-agent systems simulation by
interactive data-driven optimization. <em>TVCG</em>, <em>27</em>(3),
1953–1966. (<a href="https://doi.org/10.1109/TVCG.2019.2946769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive multi-agent simulation algorithms are used to compute the trajectories and behaviors of different entities in virtual reality scenarios. However, current methods involve considerable parameter tweaking to generate plausible behaviors. We introduce a novel approach (Heter-Sim) that combines physics-based simulation methods with data-driven techniques using an optimization-based formulation. Our approach is general and can simulate heterogeneous agents corresponding to human crowds, traffic, vehicles, or combinations of different agents with varying dynamics. We estimate motion states from real-world datasets that include information about position, velocity, and control direction. Our optimization algorithm considers several constraints, including velocity continuity, collision avoidance, attraction, direction control. Other constraints are implemented by introducing a novel energy function to control the motions of heterogeneous agents. To accelerate the computations, we reduce the search space for both collision avoidance and optimal solution computation. Heter-Sim can simulate tens or hundreds of agents at interactive rates and we compare its accuracy with real-world datasets and prior algorithms. We also perform user studies that evaluate the plausible behaviors generated by our algorithm and a user study that evaluates the plausibility of our algorithm via VR.},
  archive      = {J_TVCG},
  author       = {Jiaping Ren and Wei Xiang and Yangxi Xiao and Ruigang Yang and Dinesh Manocha and Xiaogang Jin},
  doi          = {10.1109/TVCG.2019.2946769},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1953-1966},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Heter-sim: Heterogeneous multi-agent systems simulation by interactive data-driven optimization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based feature-preserving mesh normal filtering.
<em>TVCG</em>, <em>27</em>(3), 1937–1952. (<a
href="https://doi.org/10.1109/TVCG.2019.2944357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguishing between geometric features and noise is of paramount importance for mesh denoising. In this paper, a graph-based feature-preserving mesh normal filtering scheme is proposed, which includes two stages: graph-based feature detection and feature-aware guided normal filtering. In the first stage, faces in the input noisy mesh are represented by patches, which are then modelled as weighted graphs. In this way, feature detection can be cast as a graph-cut problem. Subsequently, an iterative normalized cut algorithm is applied on each patch to separate the patch into smooth regions according to the detected features. In the second stage, a feature-aware guidance normal is constructed for each face, and guided normal filtering is applied to achieve robust feature-preserving mesh denoising. The results of experiments on synthetic and real scanned models indicate that the proposed scheme outperforms state-of-the-art mesh denoising works in terms of both objective and subjective evaluations.},
  archive      = {J_TVCG},
  author       = {Wenbo Zhao and Xianming Liu and Shiqi Wang and Xiaopeng Fan and Debin Zhao},
  doi          = {10.1109/TVCG.2019.2944357},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1937-1952},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Graph-based feature-preserving mesh normal filtering},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of 3D pointing accuracy in the fovea and
periphery in immersive head-mounted display environments. <em>TVCG</em>,
<em>27</em>(3), 1929–1936. (<a
href="https://doi.org/10.1109/TVCG.2019.2947504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coupling between perception and action has seldom been explored in sophisticated motor behaviour such as 3D pointing. In this study, we investigated how 3D pointing accuracy, measured by a depth estimation task, could be affected by the target appearing in different visual eccentricities. Specifically, we manipulated the visual eccentricity of the target and its depth in virtual reality. Participants wore a head-mounted-display with an integrated eye-tracker and docked a cursor into a target. We adopted a within-participants factorial design with three variables. The first variable is Eccentricity : the location of the target on one of five horizontal eccentricities (left far periphery, left near periphery, foveal, right near periphery and right far periphery). The second variable is Depth at three levels and the third variable is Feedback Loop with two levels: open/closed. Eccentricity is refactored into Motion Correspondence between the starting location of the cursor and the target location with four levels: periphery to fovea, fovea to periphery, periphery to periphery, fovea to fovea. The results showed that the pointing accuracy is modulated mainly by the target locations rather than the initial locations of the effector (hand). Visible feedback during pointing improved performance.},
  archive      = {J_TVCG},
  author       = {Nikolaos Katzakis and Lihan Chen and Oscar Ariza and Robert J. Teather and Frank Steinicke},
  doi          = {10.1109/TVCG.2019.2947504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1929-1936},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluation of 3D pointing accuracy in the fovea and periphery in immersive head-mounted display environments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational phase-modulated eyeglasses. <em>TVCG</em>,
<em>27</em>(3), 1916–1928. (<a
href="https://doi.org/10.1109/TVCG.2019.2947038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present computational phase-modulated eyeglasses, a see-through optical system that modulates the view of the user using phase-only spatial light modulators (PSLM). A PSLM is a programmable reflective device that can selectively retardate, or delay, the incoming light rays. As a result, a PSLM works as a computational dynamic lens device. We demonstrate our computational phase-modulated eyeglasses with either a single PSLM or dual PSLMs and show that the concept can realize various optical operations including focus correction, bi-focus, image shift, and field of view manipulation, namely optical zoom. Compared to other programmable optics, computational phase-modulated eyeglasses have the advantage in terms of its versatility. In addition, we also presents some prototypical focus-loop applications where the lens is dynamically optimized based on distances of objects observed by a scene camera. We further discuss the implementation, applications but also discuss limitations of the current prototypes and remaining issues that need to be addressed in future research.},
  archive      = {J_TVCG},
  author       = {Yuta Itoh and Tobias Langlotz and Stefanie Zollmann and Daisuke Iwai and Kiyokawa Kiyoshi and Toshiyuki Amano},
  doi          = {10.1109/TVCG.2019.2947038},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1916-1928},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Computational phase-modulated eyeglasses},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binned k-d tree construction for sparse volume data on
multi-core and GPU systems. <em>TVCG</em>, <em>27</em>(3), 1904–1915.
(<a href="https://doi.org/10.1109/TVCG.2019.2938957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While k-d trees are known to be effective for spatial indexing of sparse 3-d volume data, full reconstruction, e.g. due to changes to the alpha transfer function during rendering, is usually a costly operation with this hierarchical data structure. In a recent publication we showed how to port a clever state of the art k-d tree construction algorithm to a multi-core CPU architecture and by means of thorough optimization we were able to obtain interactive reconstruction rates for moderately sized to large data sets. The construction scheme is based on maintaining partial summed-volume tables that fit in the L1 cache of the multi-core CPU and that allow for fast occupancy queries. In this work we propose a GPU implementation of the parallel k-d tree construction algorithm and compare it with the original multi-core CPU implementation. We conduct a thorough comparative study that outlines performance and scalability of our implementation.},
  archive      = {J_TVCG},
  author       = {Stefan Zellmann and Jürgen P. Schulze and Ulrich Lang},
  doi          = {10.1109/TVCG.2019.2938957},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1904-1915},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Binned k-d tree construction for sparse volume data on multi-core and GPU systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic sitting pose generation for ergonomic ratings of
chairs. <em>TVCG</em>, <em>27</em>(3), 1890–1903. (<a
href="https://doi.org/10.1109/TVCG.2019.2938746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human poses play a critical role in human-centric product design. Despite considerable researches on pose synthesis and pose-driven product design, most of them adopt the simple stick figure model that captures only skeletons rather than real body geometries and do not link human poses to the environment (e.g., chairs for sitting). This paper focuses on user-tailored ergonomic design and rating of chairs using scanned human geometries. Fully utilizing the anthropometric information of the human models, our method considers more ergonomic guidelines of chair design (such as pressure distribution and support intensity) and links the geometry of 3D chair models and human-to-chair interactions into the pose deformation constraints of the human avatars. The core of our method is a pose generation algorithm which rigs the user&#39;s successive poses through coarse- and fine-level pose deformations. We define a non-linear energy function with contact, collision, and joint limit terms, and solve it using a hill-climbing algorithm. The fitting results allow us to quantitatively evaluate the chair model in terms of various ergonomic criteria. Our method is flexible and effective and can be applied to users with varying body shapes and a wide range of chairs. Moreover, the proposed technique can be easily extended to other furniture, such as desk, bed, and cabinet. Extensive evaluations and a user study demonstrate the efficiency and advantages of the proposed virtual fitting method. Given that our method avoids tedious on-site trying, facilitates the exploration/evaluation of various chair products, and provides valuable feedback for the designers and manufacturers to deliver customized products, it is ideal for online shopping of chairs.},
  archive      = {J_TVCG},
  author       = {Aihua Mao and Hong Zhang and Zhenfeng Xie and Minjing Yu and Yong-Jin Liu and Ying He},
  doi          = {10.1109/TVCG.2019.2938746},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1890-1903},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Automatic sitting pose generation for ergonomic ratings of chairs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An evaluation testbed for locomotion in virtual reality.
<em>TVCG</em>, <em>27</em>(3), 1871–1889. (<a
href="https://doi.org/10.1109/TVCG.2020.3032440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common operation performed in Virtual Reality (VR) environments is locomotion. Although real walking can represent a natural and intuitive way to manage displacements in such environments, its use is generally limited by the size of the area tracked by the VR system (typically, the size of a room) or requires expensive technologies to cover particularly extended settings. A number of approaches have been proposed to enable effective explorations in VR, each characterized by different hardware requirements and costs, and capable to provide different levels of usability and performance. However, the lack of a well-defined methodology for assessing and comparing available approaches makes it difficult to identify, among the various alternatives, the best solutions for selected application domains. To deal with this issue, this article introduces a novel evaluation testbed which, by building on the outcomes of many separate works reported in the literature, aims to support a comprehensive analysis of the considered design space. An experimental protocol for collecting objective and subjective measures is proposed, together with a scoring system able to rank locomotion approaches based on a weighted set of requirements. Testbed usage is illustrated in a use case requesting to select the technique to adopt in a given application scenario.},
  archive      = {J_TVCG},
  author       = {Alberto Cannavò and Davide Calandra and F. Gabriele Pratticò and Valentina Gatteschi and Fabrizio Lamberti},
  doi          = {10.1109/TVCG.2020.3032440},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {3},
  pages        = {1871-1889},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An evaluation testbed for locomotion in virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preface. <em>TVCG</em>, <em>27</em>(2), xviii–xxv. (<a
href="https://doi.org/10.1109/TVCG.2020.3033678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This February 2021 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG) contains the proceedings of IEEE VIS 2020, held online between 25-30 October 2020, hosted by General Chairs from the University of Utah. With IEEE VIS 2020, the conference series is in its 31st year. IEEE VIS consists of three conferences, held concurrently: the IEEE Visual Analytics Science and Technology Conference (VAST), the IEEE Information Visualization Conference (InfoVis), and the IEEE Scientific Visualization Conference (SciVis). These three conferences are the premier venues for the visualization community to exchange the latest ideas and developments, attracting researchers and practitioners alike.},
  archive      = {J_TVCG},
  author       = {Niklas Elmqvist and Brian Fisher and Peter Lindstrom and Ross Maciejewski and Miriah Meyer and Silvia Miksch and Luis Gustavo Nonato and Nathalie Riche and Han-Wei Shen and Ruediger Westermann and Jo Wood and Jing Yang},
  doi          = {10.1109/TVCG.2020.3033678},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {xviii-xxv},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preface},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Message from VIS 2020 general chairs. <em>TVCG</em>,
<em>27</em>(2), xvii. (<a
href="https://doi.org/10.1109/TVCG.2020.3033688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the messages from the VIS 2020 conference General Chairs.},
  archive      = {J_TVCG},
  author       = {Valerio Pascucci and Mike Kirby},
  doi          = {10.1109/TVCG.2020.3033688},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {xvii},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from VIS 2020 general chairs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Message from the editor-in-chief. <em>TVCG</em>,
<em>27</em>(2), xvi. (<a
href="https://doi.org/10.1109/TVCG.2020.3033687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents the introductory editorial for this issue of the publication.},
  archive      = {J_TVCG},
  author       = {Klaus Mueller},
  doi          = {10.1109/TVCG.2020.3033687},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {xvi},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Message from the editor-in-chief},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IEEE transactions on visualization and computer graphics
[conference announcement]. <em>TVCG</em>, <em>27</em>(2), C1. (<a
href="https://doi.org/10.1109/TVCG.2020.3033649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Announcement: IEEE Visual Analytics Science &amp; Technology Conference, IEEE Information Visualization Conference, and IEEE Scientific Visualization Conference. Virtual Event 25-30 October 2020.},
  archive      = {J_TVCG},
  doi          = {10.1109/TVCG.2020.3033649},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {C1},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IEEE transactions on visualization and computer graphics [Conference announcement]},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Passing the data baton: A retrospective analysis on data
science work and workers. <em>TVCG</em>, <em>27</em>(2), 1860–1870. (<a
href="https://doi.org/10.1109/TVCG.2020.3030340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work. Data availability: https://osf.io/z2xpd/?view_only=87fa24be486a473884adb9ffbe8db4ec},
  archive      = {J_TVCG},
  author       = {Anamaria Crisan and Brittany Fiore-Gartland and Melanie Tory},
  doi          = {10.1109/TVCG.2020.3030340},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1860-1870},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Passing the data baton: A retrospective analysis on data science work and workers},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foveated encoding for large high-resolution displays.
<em>TVCG</em>, <em>27</em>(2), 1850–1859. (<a
href="https://doi.org/10.1109/TVCG.2020.3030445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative exploration of scientific data sets across large high-resolution displays requires both high visual detail as well as low-latency transfer of image data (oftentimes inducing the need to trade one for the other). In this work, we present a system that dynamically adapts the encoding quality in such systems in a way that reduces the required bandwidth without impacting the details perceived by one or more observers. Humans perceive sharp, colourful details, in the small foveal region around the centre of the field of view, while information in the periphery is perceived blurred and colourless. We account for this by tracking the gaze of observers, and respectively adapting the quality parameter of each macroblock used by the H.264 encoder, considering the so-called visual acuity fall-off. This allows to substantially reduce the required bandwidth with barely noticeable changes in visual quality, which is crucial for collaborative analysis across display walls at different locations. We demonstrate the reduced overall required bandwidth and the high quality inside the foveated regions using particle rendering and parallel coordinates.},
  archive      = {J_TVCG},
  author       = {Florian Frieß and Matthias Braun and Valentin Bruder and Steffen Frey and Guido Reina and Thomas Ertl},
  doi          = {10.1109/TVCG.2020.3030445},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1850-1859},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Foveated encoding for large high-resolution displays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visualization interface to improve the transparency of
collected personal data on the internet. <em>TVCG</em>, <em>27</em>(2),
1840–1849. (<a href="https://doi.org/10.1109/TVCG.2020.3028946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online services are used for all kinds of activities, like news, entertainment, publishing content or connecting with others. But information technology enables new threats to privacy by means of global mass surveillance, vast databases and fast distribution networks. Current news are full of misuses and data leakages. In most cases, users are powerless in such situations and develop an attitude of neglect for their online behaviour. On the other hand, the GDPR (General Data Protection Regulation) gives users the right to request a copy of all their personal data stored by a particular service, but the received data is hard to understand or analyze by the common internet user. This paper presents TransparencyVis - a web-based interface to support the visual and interactive exploration of data exports from different online services. With this approach, we aim at increasing the awareness of personal data stored by such online services and the effects of online behaviour. This design study provides an online accessible prototype and a best practice to unify data exports from different sources.},
  archive      = {J_TVCG},
  author       = {Marija Schufrin and Steven Lamarr Reynolds and Arjan Kuijper and Jorn Kohlhammer},
  doi          = {10.1109/TVCG.2020.3028946},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1840-1849},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visualization interface to improve the transparency of collected personal data on the internet},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling the influence of visual density on cluster
perception in scatterplots using topology. <em>TVCG</em>,
<em>27</em>(2), 1829–1839. (<a
href="https://doi.org/10.1109/TVCG.2020.3030365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.},
  archive      = {J_TVCG},
  author       = {Ghulam Jilani Quadri and Paul Rosen},
  doi          = {10.1109/TVCG.2020.3030365},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1829-1839},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling the influence of visual density on cluster perception in scatterplots using topology},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty in continuous scatterplots, continuous parallel
coordinates, and fibers. <em>TVCG</em>, <em>27</em>(2), 1819–1828. (<a
href="https://doi.org/10.1109/TVCG.2020.3030466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce uncertainty to continuous scatterplots and continuous parallel coordinates. We derive respective models, validate them with sampling-based brute-force schemes, and present acceleration strategies for their computation. At the same time, we show that our approach lends itself as well for introducing uncertainty into the definition of fibers in bivariate data. Finally, we demonstrate the properties and the utility of our approach using specifically designed synthetic cases and simulated data.},
  archive      = {J_TVCG},
  author       = {Boyan Zheng and Filip Sadlo},
  doi          = {10.1109/TVCG.2020.3030466},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1819-1828},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty in continuous scatterplots, continuous parallel coordinates, and fibers},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-oriented ensemble data visualization and
exploration using variable spatial spreading. <em>TVCG</em>,
<em>27</em>(2), 1808–1818. (<a
href="https://doi.org/10.1109/TVCG.2020.3030377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important method of handling potential uncertainties in numerical simulations, ensemble simulation has been widely applied in many disciplines. Visualization is a promising and powerful ensemble simulation analysis method. However, conventional visualization methods mainly aim at data simplification and highlighting important information based on domain expertise instead of providing a flexible data exploration and intervention mechanism. Trial-and-error procedures have to be repeatedly conducted by such approaches. To resolve this issue, we propose a new perspective of ensemble data analysis using the attribute variable dimension as the primary analysis dimension. Particularly, we propose a variable uncertainty calculation method based on variable spatial spreading. Based on this method, we design an interactive ensemble analysis framework that provides a flexible interactive exploration of the ensemble data. Particularly, the proposed spreading curve view, the region stability heat map view, and the temporal analysis view, together with the commonly used 2D map view, jointly support uncertainty distribution perception, region selection, and temporal analysis, as well as other analysis requirements. We verify our approach by analyzing a real-world ensemble simulation dataset. Feedback collected from domain experts confirms the efficacy of our framework.},
  archive      = {J_TVCG},
  author       = {Mingdong Zhang and Li Chen and Quan Li and Xiaoru Yuan and Junhai Yong},
  doi          = {10.1109/TVCG.2020.3030377},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1808-1818},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty-oriented ensemble data visualization and exploration using variable spatial spreading},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direct volume rendering with nonparametric models of
uncertainty. <em>TVCG</em>, <em>27</em>(2), 1797–1807. (<a
href="https://doi.org/10.1109/TVCG.2020.3030394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.},
  archive      = {J_TVCG},
  author       = {Tushar M. Athawale and Bo Ma and Elham Sakhaee and Chris R. Johnson and Alireza Entezari},
  doi          = {10.1109/TVCG.2020.3030394},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1797-1807},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Direct volume rendering with nonparametric models of uncertainty},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Investigating visual analysis of differentially private
data. <em>TVCG</em>, <em>27</em>(2), 1786–1796. (<a
href="https://doi.org/10.1109/TVCG.2020.3030369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential Privacy is an emerging privacy model with increasing popularity in many domains. It functions by adding carefully calibrated noise to data that blurs information about individuals while preserving overall statistics about the population. Theoretically, it is possible to produce robust privacy-preserving visualizations by plotting differentially private data. However, noise-induced data perturbations can alter visual patterns and impact the utility of a private visualization. We still know little about the challenges and opportunities for visual data exploration and analysis using private visualizations. As a first step towards filling this gap, we conducted a crowdsourced experiment, measuring participants&#39; performance under three levels of privacy (high, low, non-private) for combinations of eight analysis tasks and four visualization types (bar chart, pie chart, line chart, scatter plot). Our findings show that for participants&#39; accuracy for summary tasks (e.g., find clusters in data) was higher that value tasks (e.g., retrieve a certain value). We also found that under DP, pie chart and line chart offer similar or better accuracy than bar chart. In this work, we contribute the results of our empirical study, investigating the task-based effectiveness of basic private visualizations, a dichotomous model for defining and measuring user success in performing visual analysis tasks under DP, and a set of distribution metrics for tuning the injection to improve the utility of private visualizations.},
  archive      = {J_TVCG},
  author       = {Dan Zhang and Ali Sarvghad and Gerome Miklau},
  doi          = {10.1109/TVCG.2020.3030369},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1786-1796},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating visual analysis of differentially private data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integrating prior knowledge in mixed-initiative social
network clustering. <em>TVCG</em>, <em>27</em>(2), 1775–1785. (<a
href="https://doi.org/10.1109/TVCG.2020.3030347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.},
  archive      = {J_TVCG},
  author       = {Alexis Pister and Paolo Buono and Jean-Daniel Fekete and Catherine Plaisant and Paola Valdivia},
  doi          = {10.1109/TVCG.2020.3030347},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1775-1785},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Integrating prior knowledge in mixed-initiative social network clustering},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supporting the problem-solving loop: Designing highly
interactive optimisation systems. <em>TVCG</em>, <em>27</em>(2),
1764–1774. (<a href="https://doi.org/10.1109/TVCG.2020.3030364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically “black boxes” that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a “human in the loop” who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.},
  archive      = {J_TVCG},
  author       = {Jie Liu and Tim Dwyer and Guido Tack and Samuel Gratzl and Kim Marriott},
  doi          = {10.1109/TVCG.2020.3030364},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1764-1774},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Supporting the problem-solving loop: Designing highly interactive optimisation systems},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boba: Authoring and visualizing multiverse analyses.
<em>TVCG</em>, <em>27</em>(2), 1753–1763. (<a
href="https://doi.org/10.1109/TVCG.2020.3028985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiverse analysis is an approach to data analysis in which all “reasonable” analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Baba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba&#39;s utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.},
  archive      = {J_TVCG},
  author       = {Yang Liu and Alex Kale and Tim Althoff and Jeffrey Heer},
  doi          = {10.1109/TVCG.2020.3028985},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1753-1763},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Boba: Authoring and visualizing multiverse analyses},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An examination of grouping and spatial organization tasks
for high-dimensional data exploration. <em>TVCG</em>, <em>27</em>(2),
1742–1752. (<a href="https://doi.org/10.1109/TVCG.2020.3028890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do analysts think about grouping and spatial operations? This overarching research question incorporates a number of points for investigation, including understanding how analysts begin to explore a dataset, the types of grouping/spatial structures created and the operations performed on them, the relationship between grouping and spatial structures, the decisions analysts make when exploring individual observations, and the role of external information. This work contributes the design and results of such a study, in which a group of participants are asked to organize the data contained within an unfamiliar quantitative dataset. We identify several overarching approaches taken by participants to design their organizational space, discuss the interactions performed by the participants, and propose design recommendations to improve the usability of future high-dimensional data exploration tools that make use of grouping (clustering) and spatial (dimension reduction) operations.},
  archive      = {J_TVCG},
  author       = {John Wenskovitch and Chris North},
  doi          = {10.1109/TVCG.2020.3028890},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1742-1752},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An examination of grouping and spatial organization tasks for high-dimensional data exploration},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CAVA: A visual analytics system for exploratory columnar
data augmentation using knowledge graphs. <em>TVCG</em>, <em>27</em>(2),
1731–1741. (<a href="https://doi.org/10.1109/TVCG.2020.3030443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.},
  archive      = {J_TVCG},
  author       = {Dylan Cashman and Shenyu Xu and Subhajit Das and Florian Heimerl and Cong Liu and Shah Rukh Humayoun and Michael Gleicher and Alex Endert and Remco Chang},
  doi          = {10.1109/TVCG.2020.3030443},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1731-1741},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CAVA: A visual analytics system for exploratory columnar data augmentation using knowledge graphs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of sampling methods for scatterplots.
<em>TVCG</em>, <em>27</em>(2), 1720–1730. (<a
href="https://doi.org/10.1109/TVCG.2020.3030432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but “good” scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.},
  archive      = {J_TVCG},
  author       = {Jun Yuan and Shouxing Xiang and Jiazhi Xia and Lingyun Yu and Shixia Liu},
  doi          = {10.1109/TVCG.2020.3030432},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1720-1730},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluation of sampling methods for scatterplots},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-aware sampling of large networks via graph
representation learning. <em>TVCG</em>, <em>27</em>(2), 1709–1719. (<a
href="https://doi.org/10.1109/TVCG.2020.3030440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.},
  archive      = {J_TVCG},
  author       = {Zhiguang Zhou and Chen Shi and Xilong Shen and Lihong Cai and Haoxuan Wang and Yuhua Liu and Ying Zhao and Wei Chen},
  doi          = {10.1109/TVCG.2020.3030440},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1709-1719},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Context-aware sampling of large networks via graph representation learning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preserving minority structures in graph sampling.
<em>TVCG</em>, <em>27</em>(2), 1698–1708. (<a
href="https://doi.org/10.1109/TVCG.2020.3030428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. By comprehensively analyzing the literature on graph sampling, we assume that existing algorithms cannot effectively preserve minority structures that are rare and small in a graph but are very important in graph analysis. In this work, we initially conduct a pilot user study to investigate representative minority structures that are most appealing to human viewers. We then perform an experimental study to evaluate the performance of existing graph sampling algorithms regarding minority structure preservation. Results confirm our assumption and suggest key points for designing a new graph sampling approach named mino-centric graph sampling (MCGS). In this approach, a triangle-based algorithm and a cut-point-based algorithm are proposed to efficiently identify minority structures. A set of importance assessment criteria are designed to guide the preservation of important minority structures. Three optimization objectives are introduced into a greedy strategy to balance the preservation between minority and majority structures and suppress the generation of new minority structures. A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS.},
  archive      = {J_TVCG},
  author       = {Ying Zhao and Haojin Jiang and Qi&#39;an Chen and Yaqi Qin and Huixuan Xie and Yitao Wu and Shixia Liu and Zhiguang Zhou and Jiazhi Xia and Fangfang Zhou},
  doi          = {10.1109/TVCG.2020.3030428},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1698-1708},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preserving minority structures in graph sampling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SafetyLens: Visual data analysis of functional safety of
vehicles. <em>TVCG</em>, <em>27</em>(2), 1688–1697. (<a
href="https://doi.org/10.1109/TVCG.2020.3030382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern automobiles have evolved from just being mechanical machines to having full-fledged electronics systems that enhance vehicle dynamics and driver experience. However, these complex hardware and software systems, if not properly designed, can experience failures that can compromise the safety of the vehicle, its occupants, and the surrounding environment. For example, a system to activate the brakes to avoid a collision saves lives when it functions properly, but could lead to tragic outcomes if the brakes were applied in a way that&#39;s inconsistent with the design. Broadly speaking, the analysis performed to minimize such risks falls into a systems engineering domain called Functional Safety. In this paper, we present SafetyLens, a visual data analysis tool to assist engineers and analysts in analyzing automotive Functional Safety datasets. SafetyLens combines techniques including network exploration and visual comparison to help analysts perform domain-specific tasks. This paper presents the design study with domain experts that resulted in the design guidelines, the tool, and user feedback.},
  archive      = {J_TVCG},
  author       = {Arpit Narechania and Ahsan Qamar and Alex Endert},
  doi          = {10.1109/TVCG.2020.3030382},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1688-1697},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SafetyLens: Visual data analysis of functional safety of vehicles},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalability of network visualisation from a cognitive load
perspective. <em>TVCG</em>, <em>27</em>(2), 1677–1687. (<a
href="https://doi.org/10.1109/TVCG.2020.3030459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in ‘hairball’ visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.},
  archive      = {J_TVCG},
  author       = {Vahan Yoghourdjian and Yalong Yang and Tim Dwyer and Lee Lawrence and Michael Wybrow and Kim Marriott},
  doi          = {10.1109/TVCG.2020.3030459},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1677-1687},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalability of network visualisation from a cognitive load perspective},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DRGraph: An efficient graph layout algorithm for large-scale
graphs by dimensionality reduction. <em>TVCG</em>, <em>27</em>(2),
1666–1676. (<a href="https://doi.org/10.1109/TVCG.2020.3030447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.},
  archive      = {J_TVCG},
  author       = {Minfeng Zhu and Wei Chen and Yuanzhe Hu and Yuxuan Hou and Liangjun Liu and Kaiyuan Zhang},
  doi          = {10.1109/TVCG.2020.3030447},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1666-1676},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DRGraph: An efficient graph layout algorithm for large-scale graphs by dimensionality reduction},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exemplar-based layout fine-tuning for node-link diagrams.
<em>TVCG</em>, <em>27</em>(2), 1655–1665. (<a
href="https://doi.org/10.1109/TVCG.2020.3030393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.},
  archive      = {J_TVCG},
  author       = {Jiacheng Pan and Wei Chen and Xiaodong Zhao and Shuyue Zhou and Wei Zeng and Minfeng Zhu and Jian Chen and Siwei Fu and Yingcai Wu},
  doi          = {10.1109/TVCG.2020.3030393},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1655-1665},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exemplar-based layout fine-tuning for node-link diagrams},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Responsive matrix cells: A focus+context approach for
exploring and editing multivariate graphs. <em>TVCG</em>,
<em>27</em>(2), 1644–1654. (<a
href="https://doi.org/10.1109/TVCG.2020.3030371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix visualizations are a useful tool to provide a general overview of a graph&#39;s structure. For multivariate graphs, a remaining challenge is to cope with the attributes that are associated with nodes and edges. Addressing this challenge, we propose responsive matrix cells as a focus+context approach for embedding additional interactive views into a matrix. Responsive matrix cells are local zoomable regions of interest that provide auxiliary data exploration and editing facilities for multivariate graphs. They behave responsively by adapting their visual contents to the cell location, the available display space, and the user task. Responsive matrix cells enable users to reveal details about the graph, compare node and edge attributes, and edit data values directly in a matrix without resorting to external views or tools. We report the general design considerations for responsive matrix cells covering the visual and interactive means necessary to support a seamless data exploration and editing. Responsive matrix cells have been implemented in a web-based prototype based on which we demonstrate the utility of our approach. We describe a walk-through for the use case of analyzing a graph of soccer players and report on insights from a preliminary user feedback session.},
  archive      = {J_TVCG},
  author       = {Tom Horak and Philip Berger and Heidrun Schumann and Raimund Dachselt and Christian Tominski},
  doi          = {10.1109/TVCG.2020.3030371},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1644-1654},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Responsive matrix cells: A Focus+Context approach for exploring and editing multivariate graphs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SineStream: Improving the readability of streamgraphs by
minimizing sine illusion effects. <em>TVCG</em>, <em>27</em>(2),
1634–1643. (<a href="https://doi.org/10.1109/TVCG.2020.3030404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose SineStream, a new variant of streamgraphs that improves their readability by minimizing sine illusion effects. Such effects reflect the tendency of humans to take the orthogonal rather than the vertical distance between two curves as their distance. In SineStream, we connect the readability of streamgraphs with minimizing sine illusions and by doing so provide a perceptual foundation for their design. As the geometry of a streamgraph is controlled by its baseline (the bottom-most curve) and the ordering of the layers, we re-interpret baseline computation and layer ordering algorithms in terms of reducing sine illusion effects. For baseline computation, we improve previous methods by introducing a Gaussian weight to penalize layers with large thickness changes. For layer ordering, three design requirements are proposed and implemented through a hierarchical clustering algorithm. Quantitative experiments and user studies demonstrate that SineStream improves the readability and aesthetics of streamgraphs compared to state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Chuan Bu and Quanjie Zhang and Qianwen Wang and Jian Zhang and Michael Sedlmair and Oliver Deussen and Yunhai Wang},
  doi          = {10.1109/TVCG.2020.3030404},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1634-1643},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SineStream: Improving the readability of streamgraphs by minimizing sine illusion effects},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MultiSegVA: Using visual analytics to segment biologging
time series on multiple scales. <em>TVCG</em>, <em>27</em>(2),
1623–1633. (<a href="https://doi.org/10.1109/TVCG.2020.3030386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting biologging time series of animals on multiple temporal scales is an essential step that requires complex techniques with careful parameterization and possibly cross-domain expertise. Yet, there is a lack of visual-interactive tools that strongly support such multi-scale segmentation. To close this gap, we present our MultiSegVA platform for interactively defining segmentation techniques and parameters on multiple temporal scales. MultiSegVA primarily contributes tailored, visual-interactive means and visual analytics paradigms for segmenting unlabeled time series on multiple scales. Further, to flexibly compose the multi-scale segmentation, the platform contributes a new visual query language that links a variety of segmentation techniques. To illustrate our approach, we present a domain-oriented set of segmentation techniques derived in collaboration with movement ecologists. We demonstrate the applicability and usefulness of MultiSegVA in two real-world use cases from movement ecology, related to behavior analysis after environment-aware segmentation, and after progressive clustering. Expert feedback from movement ecologists shows the effectiveness of tailored visual-interactive means and visual analytics paradigms at segmenting multi-scale data, enabling them to perform semantically meaningful analyses. A third use case demonstrates that MultiSegVA is generalizable to other domains.},
  archive      = {J_TVCG},
  author       = {Philipp Meschenmoser and Juri F. Buchmüller and Daniel Seebacher and Martin Wikelski and Daniel A. Keim},
  doi          = {10.1109/TVCG.2020.3030386},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1623-1633},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MultiSegVA: Using visual analytics to segment biologging time series on multiple scales},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-bridges: Pair-wise visual connection and comparison for
multi-item data streams. <em>TVCG</em>, <em>27</em>(2), 1612–1622. (<a
href="https://doi.org/10.1109/TVCG.2020.3030411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users&#39; capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.},
  archive      = {J_TVCG},
  author       = {Siming Chen and Natalia Andrienko and Gennady Andrienko and Jie Li and Xiaoru Yuan},
  doi          = {10.1109/TVCG.2020.3030411},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1612-1622},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Co-bridges: Pair-wise visual connection and comparison for multi-item data streams},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visual analytics framework for reviewing multivariate
time-series data with dimensionality reduction. <em>TVCG</em>,
<em>27</em>(2), 1601–1611. (<a
href="https://doi.org/10.1109/TVCG.2020.3028889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts&#39; ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.},
  archive      = {J_TVCG},
  author       = {Takanori Fujiwara and Shilpika and Naohisa Sakamoto and Jorji Nonaka and Keiji Yamamoto and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2020.3028889},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1601-1611},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics framework for reviewing multivariate time-series data with dimensionality reduction},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-driven space-filling curves. <em>TVCG</em>,
<em>27</em>(2), 1591–1600. (<a
href="https://doi.org/10.1109/TVCG.2020.3030473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a data-driven space-filling curve method for 2D and 3D visualization. Our flexible curve traverses the data elements in the spatial domain in a way that the resulting linearization better preserves features in space compared to existing methods. We achieve such data coherency by calculating a Hamiltonian path that approximately minimizes an objective function that describes the similarity of data values and location coherency in a neighborhood. Our extended variant even supports multiscale data via quadtrees and octrees. Our method is useful in many areas of visualization including multivariate or comparative visualization ensemble visualization of 2D and 3D data on regular grids or multiscale visual analysis of particle simulations. The effectiveness of our method is evaluated with numerical comparisons to existing techniques and through examples of ensemble and multivariate datasets.},
  archive      = {J_TVCG},
  author       = {Liang Zhou and Chris R. Johnson and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2020.3030473},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1591-1600},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data-driven space-filling curves},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analysis of large multivariate scattered data using
clustering and probabilistic summaries. <em>TVCG</em>, <em>27</em>(2),
1580–1590. (<a href="https://doi.org/10.1109/TVCG.2020.3030379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly growing data sizes of scientific simulations pose significant challenges for interactive visualization and analysis techniques. In this work, we propose a compact probabilistic representation to interactively visualize large scattered datasets. In contrast to previous approaches that represent blocks of volumetric data using probability distributions, we model clusters of arbitrarily structured multivariate data. In detail, we discuss how to efficiently represent and store a high-dimensional distribution for each cluster. We observe that it suffices to consider low-dimensional marginal distributions for two or three data dimensions at a time to employ common visual analysis techniques. Based on this observation, we represent high-dimensional distributions by combinations of low-dimensional Gaussian mixture models. We discuss the application of common interactive visual analysis techniques to this representation. In particular, we investigate several frequency-based views, such as density plots in 1D and 2D, density-based parallel coordinates, and a time histogram. We visualize the uncertainty introduced by the representation, discuss a level-of-detail mechanism, and explicitly visualize outliers. Furthermore, we propose a spatial visualization by splatting anisotropic 3D Gaussians for which we derive a closed-form solution. Lastly, we describe the application of brushing and linking to this clustered representation. Our evaluation on several large, real-world datasets demonstrates the scaling of our approach.},
  archive      = {J_TVCG},
  author       = {Tobias Rapp and Christoph Peters and Carsten Dachsbacher},
  doi          = {10.1109/TVCG.2020.3030379},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1580-1590},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of large multivariate scattered data using clustering and probabilistic summaries},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-perspective, simultaneous embedding. <em>TVCG</em>,
<em>27</em>(2), 1569–1579. (<a
href="https://doi.org/10.1109/TVCG.2020.3030373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions.},
  archive      = {J_TVCG},
  author       = {Md Iqbal Hossain and Vahan Huroyan and Stephen Kobourov and Raymundo Navarrete},
  doi          = {10.1109/TVCG.2020.3030373},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1569-1579},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-perspective, simultaneous embedding},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Implicit multidimensional projection of local subspaces.
<em>TVCG</em>, <em>27</em>(2), 1558–1568. (<a
href="https://doi.org/10.1109/TVCG.2020.3030368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.},
  archive      = {J_TVCG},
  author       = {Rongzheng Bian and Yumeng Xue and Liang Zhou and Jian Zhang and Baoquan Chen and Daniel Weiskopf and Yunhai Wang},
  doi          = {10.1109/TVCG.2020.3030368},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1558-1568},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Implicit multidimensional projection of local subspaces},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StackGenVis: Alignment of data, algorithms, and models for
stacking ensemble learning using performance metrics. <em>TVCG</em>,
<em>27</em>(2), 1547–1557. (<a
href="https://doi.org/10.1109/TVCG.2020.3030352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called “stacked generalization”) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.},
  archive      = {J_TVCG},
  author       = {Angelos Chatzimparmpas and Rafael M. Martins and Kostiantyn Kucher and Andreas Kerren},
  doi          = {10.1109/TVCG.2020.3030352},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1547-1557},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StackGenVis: Alignment of data, algorithms, and models for stacking ensemble learning using performance metrics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LineSmooth: An analytical framework for evaluating the
effectiveness of smoothing techniques on line charts. <em>TVCG</em>,
<em>27</em>(2), 1536–1546. (<a
href="https://doi.org/10.1109/TVCG.2020.3030421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive framework for evaluating line chart smoothing methods under a variety of visual analytics tasks. Line charts are commonly used to visualize a series of data samples. When the number of samples is large, or the data are noisy, smoothing can be applied to make the signal more apparent. However, there are a wide variety of smoothing techniques available, and the effectiveness of each depends upon both nature of the data and the visual analytics task at hand. To date, the visualization community lacks a summary work for analyzing and classifying the various smoothing methods available. In this paper, we establish a framework, based on 8 measures of the line smoothing effectiveness tied to 8 low-level visual analytics tasks. We then analyze 12 methods coming from 4 commonly used classes of line chart smoothing-rank filters, convolutional filters, frequency domain filters, and subsampling. The results show that while no method is ideal for all situations, certain methods, such as Gaussian filters and TOPOLOGY-based subsampling, perform well in general. Other methods, such as low-pass CUTOFF filters and Douglas-peucker subsampling, perform well for specific visual analytics tasks. Almost as importantly, our framework demonstrates that several methods, including the commonly used UNIFORM subsampling, produce low-quality results, and should, therefore, be avoided, if possible.},
  archive      = {J_TVCG},
  author       = {Paul Rosen and Ghulam Jilani Quadri},
  doi          = {10.1109/TVCG.2020.3030421},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1536-1546},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LineSmooth: An analytical framework for evaluating the effectiveness of smoothing techniques on line charts},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative layouts revisited: Design space, guidelines, and
future directions. <em>TVCG</em>, <em>27</em>(2), 1525–1535. (<a
href="https://doi.org/10.1109/TVCG.2020.3030419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers&#39; approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.&#39;s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.},
  archive      = {J_TVCG},
  author       = {Sehi LYi and Jaemin Jo and Jinwook Seo},
  doi          = {10.1109/TVCG.2020.3030419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1525-1535},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparative layouts revisited: Design space, guidelines, and future directions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composition and configuration patterns in multiple-view
visualizations. <em>TVCG</em>, <em>27</em>(2), 1514–1524. (<a
href="https://doi.org/10.1109/TVCG.2020.3030338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.},
  archive      = {J_TVCG},
  author       = {Xi Chen and Wei Zeng and Yanna Lin and Hayder Mahdi AI-maneea and Jonathan Roberts and Remco Chang},
  doi          = {10.1109/TVCG.2020.3030338},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1514-1524},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Composition and configuration patterns in multiple-view visualizations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guidelines for pursuing and revealing data abstractions.
<em>TVCG</em>, <em>27</em>(2), 1503–1513. (<a
href="https://doi.org/10.1109/TVCG.2020.3030355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.},
  archive      = {J_TVCG},
  author       = {Alex Bigelow and Katy Williams and Katherine E. Isaacs},
  doi          = {10.1109/TVCG.2020.3030355},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1503-1513},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Guidelines for pursuing and revealing data abstractions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What makes a data-GIF understandable? <em>TVCG</em>,
<em>27</em>(2), 1492–1502. (<a
href="https://doi.org/10.1109/TVCG.2020.3030396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, “What makes a data-GIF understandable?” While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for “data-GIFs”. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF&#39;s core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.},
  archive      = {J_TVCG},
  author       = {Xinhuan Shu and Aoyu Wu and Junxiu Tang and Benjamin Bach and Yingcai Wu and Huamin Qu},
  doi          = {10.1109/TVCG.2020.3030396},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1492-1502},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {What makes a data-GIF understandable?},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Selection-bias-corrected visualization via dynamic
reweighting. <em>TVCG</em>, <em>27</em>(2), 1481–1491. (<a
href="https://doi.org/10.1109/TVCG.2020.3030455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collection and visual analysis of large-scale data from complex systems, such as electronic health records or clickstream data, has become increasingly common across a wide range of industries. This type of retrospective visual analysis, however, is prone to a variety of selection bias effects, especially for high-dimensional data where only a subset of dimensions is visualized at any given time. The risk of selection bias is even higher when analysts dynamically apply filters or perform grouping operations during ad hoc analyses. These bias effects threaten the validity and generalizability of insights discovered during visual analysis as the basis for decision making. Past work has focused on bias transparency, helping users understand when selection bias may have occurred. However, countering the effects of selection bias via bias mitigation is typically left for the user to accomplish as a separate process. Dynamic reweighting (DR) is a novel computational approach to selection bias mitigation that helps users craft bias-corrected visualizations. This paper describes the DR workflow, introduces key DR visualization designs, and presents statistical methods that support the DR process. Use cases from the medical domain, as well as findings from domain expert user interviews, are also reported.},
  archive      = {J_TVCG},
  author       = {David Borland and Jonathan Zhang and Smiti Kaul and David Gotz},
  doi          = {10.1109/TVCG.2020.3030455},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1481-1491},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Selection-bias-corrected visualization via dynamic reweighting},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analysis of discrimination in machine learning.
<em>TVCG</em>, <em>27</em>(2), 1470–1480. (<a
href="https://doi.org/10.1109/TVCG.2020.3030471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.},
  archive      = {J_TVCG},
  author       = {Qianwen Wang and Zhenhua Xu and Zhutian Chen and Yong Wang and Shixia Liu and Huamin Qu},
  doi          = {10.1109/TVCG.2020.3030471},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1470-1480},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of discrimination in machine learning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auditing the sensitivity of graph-based ranking with visual
analytics. <em>TVCG</em>, <em>27</em>(2), 1459–1469. (<a
href="https://doi.org/10.1109/TVCG.2020.3028958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.},
  archive      = {J_TVCG},
  author       = {Tiankai Xie and Yuxin Ma and Hanghang Tong and My T. Thai and Ross Maciejewski},
  doi          = {10.1109/TVCG.2020.3028958},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1459-1469},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Auditing the sensitivity of graph-based ranking with visual analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visual analytics approach for exploratory causal analysis:
Exploration, validation, and applications. <em>TVCG</em>,
<em>27</em>(2), 1448–1458. (<a
href="https://doi.org/10.1109/TVCG.2020.3028957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in marketing and student advising to demonstrate that users can effectively explore causal relations and design action plans for reaching their goals.},
  archive      = {J_TVCG},
  author       = {Xiao Xie and Fan Du and Yingcai Wu},
  doi          = {10.1109/TVCG.2020.3028957},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1448-1458},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics approach for exploratory causal analysis: Exploration, validation, and applications},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DECE: Decision explorer with counterfactual explanations for
machine learning models. <em>TVCG</em>, <em>27</em>(2), 1438–1447. (<a
href="https://doi.org/10.1109/TVCG.2020.3030342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models&#39; decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model&#39;s decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.},
  archive      = {J_TVCG},
  author       = {Furui Cheng and Yao Ming and Huamin Qu},
  doi          = {10.1109/TVCG.2020.3030342},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1438-1447},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DECE: Decision explorer with counterfactual explanations for machine learning models},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable matrix - visualization for global and local
interpretability of random forest classification ensembles.
<em>TVCG</em>, <em>27</em>(2), 1427–1437. (<a
href="https://doi.org/10.1109/TVCG.2020.3030354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models&#39; decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models&#39; decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models&#39; interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.},
  archive      = {J_TVCG},
  author       = {Mário Popolin Neto and Fernando V. Paulovich},
  doi          = {10.1109/TVCG.2020.3030354},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1427-1437},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explainable matrix - visualization for global and local interpretability of random forest classification ensembles},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HypoML: Visual analysis for hypothesis-based evaluation of
machine learning models. <em>TVCG</em>, <em>27</em>(2), 1417–1426. (<a
href="https://doi.org/10.1109/TVCG.2020.3030449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a “concept” or “feature” may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.},
  archive      = {J_TVCG},
  author       = {Qianwen Wang and William Alexander and Jack Pegg and Huamin Qu and Min Chen},
  doi          = {10.1109/TVCG.2020.3030449},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1417-1426},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HypoML: Visual analysis for hypothesis-based evaluation of machine learning models},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HyperTendril: Visual analytics for user-driven
hyperparameter optimization of deep neural networks. <em>TVCG</em>,
<em>27</em>(2), 1407–1416. (<a
href="https://doi.org/10.1109/TVCG.2020.3030380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods significantly depend on initial configurations, making it a non-trivial task to find a proper configuration. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in a model-agnostic environment. HyperTendril takes a novel approach to effectively steering hyperparameter optimization through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of the AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present the evaluation demonstrating how HyperTendril helps users steer their tuning processes via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.},
  archive      = {J_TVCG},
  author       = {Heungseok Park and Yoonsoo Nam and Ji-Hoon Kim and Jaegul Choo},
  doi          = {10.1109/TVCG.2020.3030380},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1407-1416},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HyperTendril: Visual analytics for user-driven hyperparameter optimization of deep neural networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNN explainer: Learning convolutional neural networks with
interactive visualization. <em>TVCG</em>, <em>27</em>(2), 1396–1406. (<a
href="https://doi.org/10.1109/TVCG.2020.3030418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning&#39;s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN E xplainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN E xplainer tightly integrates a model overview that summarizes a CNN&#39;s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN E xplainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN E xplainer runs locally in users&#39; web browsers without the need for installation or specialized hardware, broadening the public&#39;s education access to modern deep learning techniques.},
  archive      = {J_TVCG},
  author       = {Zijie J. Wang and Robert Turko and Omar Shaikh and Haekyu Park and Nilaksh Das and Fred Hohman and Minsuk Kahng and Duen Horng Polo Chau},
  doi          = {10.1109/TVCG.2020.3030418},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1396-1406},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CNN explainer: Learning convolutional neural networks with interactive visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visual analytics framework for explaining and diagnosing
transfer learning processes. <em>TVCG</em>, <em>27</em>(2), 1385–1395.
(<a href="https://doi.org/10.1109/TVCG.2020.3028888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.},
  archive      = {J_TVCG},
  author       = {Yuxin Ma and Arlen Fan and Jingrui He and Arun Reddy Nelakurthi and Ross Maciejewski},
  doi          = {10.1109/TVCG.2020.3028888},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1385-1395},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics framework for explaining and diagnosing transfer learning processes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual neural decomposition to explain multivariate data
sets. <em>TVCG</em>, <em>27</em>(2), 1374–1384. (<a
href="https://doi.org/10.1109/TVCG.2020.3030420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating relationships between variables in multi-dimensional data sets is a common task for data analysts and engineers. More specifically, it is often valuable to understand which ranges of which input variables lead to particular values of a given target variable. Unfortunately, with an increasing number of independent variables, this process may become cumbersome and time-consuming due to the many possible combinations that have to be explored. In this paper, we propose a novel approach to visualize correlations between input variables and a target output variable that scales to hundreds of variables. We developed a visual model based on neural networks that can be explored in a guided way to help analysts find and understand such correlations. First, we train a neural network to predict the target from the input variables. Then, we visualize the inner workings of the resulting model to help understand relations within the data set. We further introduce a new regularization term for the backpropagation algorithm that encourages the neural network to learn representations that are easier to interpret visually. We apply our method to artificial and real-world data sets to show its utility.},
  archive      = {J_TVCG},
  author       = {Johannes Knittel and Andres Lalama and Steffen Koch and Thomas Ertl},
  doi          = {10.1109/TVCG.2020.3030420},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1374-1384},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual neural decomposition to explain multivariate data sets},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CNNPruner: Pruning convolutional neural networks with visual
analytics. <em>TVCG</em>, <em>27</em>(2), 1364–1373. (<a
href="https://doi.org/10.1109/TVCG.2020.3030461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.},
  archive      = {J_TVCG},
  author       = {Guan Li and Junpeng Wang and Han-Wei Shen and Kaixin Chen and Guihua Shan and Zhonghua Lu},
  doi          = {10.1109/TVCG.2020.3030461},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1364-1373},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CNNPruner: Pruning convolutional neural networks with visual analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequence braiding: Visual overviews of temporal event
sequences and attributes. <em>TVCG</em>, <em>27</em>(2), 1353–1363. (<a
href="https://doi.org/10.1109/TVCG.2020.3030442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal event sequence alignment has been used in many domains to visualize nuanced changes and interactions overtime. Existing approaches align one or two sentinel events. Overview tasks require examining all alignments of interest using interaction and time or juxtaposition of many visualizations. Furthermore, any event attribute overviews are not closely tied to sequence visualizations. We present SEQUENCE BRAIDING, a novel overview visualization for temporal event sequences and attributes using a layered directed acyclic network. SEQUENCE BRAIDING visually aligns many temporal events and attribute groups simultaneously and supports arbitrary ordering, absence, and duplication of events. In a controlled experiment we compare SEQUENCE BRAIDING and IDMVis on user task completion time, correctness, error, and confidence. Our results provide good evidence that users of SEQUENCE BRAIDING can understand high-level patterns and trends faster and with similar error. A full version of this paper with all appendices; the evaluation stimuli, data, and analysis code; and source code are available at osf.io/mq2wt.},
  archive      = {J_TVCG},
  author       = {Sara Di Bartolomeo and Yixuan Zhang and Fangfang Sheng and Cody Dunne},
  doi          = {10.1109/TVCG.2020.3030442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1353-1363},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sequence braiding: Visual overviews of temporal event sequences and attributes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual causality analysis of event sequence data.
<em>TVCG</em>, <em>27</em>(2), 1343–1352. (<a
href="https://doi.org/10.1109/TVCG.2020.3030465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.},
  archive      = {J_TVCG},
  author       = {Zhuochen Jin and Shunan Guo and Nan Chen and Daniel Weiskopf and David Gotz and Nan Cao},
  doi          = {10.1109/TVCG.2020.3030465},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1343-1352},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual causality analysis of event sequence data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Once upon a time in visualization: Understanding the use of
textual narratives for causality. <em>TVCG</em>, <em>27</em>(2),
1332–1342. (<a href="https://doi.org/10.1109/TVCG.2020.3030358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or the interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations-causal graphs and Hasse diagrams-with and without an associated textual narrative. Finally, we describe Causeworks, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate Causeworks through interviews with experts who used the system for understanding complex events.},
  archive      = {J_TVCG},
  author       = {Arjun Choudhry and Mandar Sharma and Pramod Chundury and Thomas Kapler and Derek W. S. Gray and Naren Ramakrishnan and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2020.3030358},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1332-1342},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Once upon a time in visualization: Understanding the use of textual narratives for causality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PassVizor: Toward better understanding of the dynamics of
soccer passes. <em>TVCG</em>, <em>27</em>(2), 1322–1331. (<a
href="https://doi.org/10.1109/TVCG.2020.3030359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In soccer, passing is the most frequent interaction between players and plays a significant role in creating scoring chances. Experts are interested in analyzing players&#39; passing behavior to learn passing tactics, i.e., how players build up an attack with passing. Various approaches have been proposed to facilitate the analysis of passing tactics. However, the dynamic changes of a team&#39;s employed tactics over a match have not been comprehensively investigated. To address the problem, we closely collaborate with domain experts and characterize requirements to analyze the dynamic changes of a team&#39;s passing tactics. To characterize the passing tactic employed for each attack, we propose a topic-based approach that provides a high-level abstraction of complex passing behaviors. Based on the model, we propose a glyph-based design to reveal the multi-variate information of passing tactics within different phases of attacks, including player identity, spatial context, and formation. We further design and develop PassVizor, a visual analytics system, to support the comprehensive analysis of passing dynamics. With the system, users can detect the changing patterns of passing tactics and examine the detailed passing process for evaluating passing tactics. We invite experts to conduct analysis with PassVizor and demonstrate the usability of the system through an expert interview.},
  archive      = {J_TVCG},
  author       = {Xiao Xie and Jiachen Wang and Hongye Liang and Dazhen Deng and Shoubin Cheng and Hui Zhang and Wei Chen and Yingcai Wu},
  doi          = {10.1109/TVCG.2020.3030359},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1322-1331},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PassVizor: Toward better understanding of the dynamics of soccer passes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive visual study of multiple attributes learning
model of x-ray scattering images. <em>TVCG</em>, <em>27</em>(2),
1312–1321. (<a href="https://doi.org/10.1109/TVCG.2020.3030384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing interactive visualization tools for deep learning are mostly applied to the training, debugging, and refinement of neural network models working on natural images. However, visual analytics tools are lacking for the specific application of x-ray image classification with multiple structural attributes. In this paper, we present an interactive system for domain scientists to visually study the multiple attributes learning models applied to x-ray scattering images. It allows domain scientists to interactively explore this important type of scientific images in embedded spaces that are defined on the model prediction output, the actual labels, and the discovered feature space of neural networks. Users are allowed to flexibly select instance images, their clusters, and compare them regarding the specified visual representation of attributes. The exploration is guided by the manifestation of model performance related to mutual relationships among attributes, which often affect the learning accuracy and effectiveness. The system thus supports domain scientists to improve the training dataset and model, find questionable attributes labels, and identify outlier images or spurious data clusters. Case studies and scientists feedback demonstrate its functionalities and usefulness.},
  archive      = {J_TVCG},
  author       = {Xinyi Huang and Suphanut Jamonnak and Ye Zhao and Boyu Wang and Minh Hoai and Kevin Yager and Wei Xu},
  doi          = {10.1109/TVCG.2020.3030384},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1312-1321},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visual study of multiple attributes learning model of X-ray scattering images},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VC-net: Deep volume-composition networks for segmentation
and visualization of highly sparse and noisy image data. <em>TVCG</em>,
<em>27</em>(2), 1301–1311. (<a
href="https://doi.org/10.1109/TVCG.2020.3030374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.},
  archive      = {J_TVCG},
  author       = {Yifan Wang and Guoli Yan and Haikuan Zhu and Sagar Buch and Ying Wang and Ewart Mark Haacke and Jing Hua and Zichun Zhong},
  doi          = {10.1109/TVCG.2020.3030374},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1301-1311},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VC-net: Deep volume-composition networks for segmentation and visualization of highly sparse and noisy image data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). V2V: A deep learning approach to variable-to-variable
selection and translation for multivariate time-varying data.
<em>TVCG</em>, <em>27</em>(2), 1290–1300. (<a
href="https://doi.org/10.1109/TVCG.2020.3030346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).},
  archive      = {J_TVCG},
  author       = {Jun Han and Hao Zheng and Yunhao Xing and Danny Z. Chen and Chaoli Wang},
  doi          = {10.1109/TVCG.2020.3030346},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1290-1300},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {V2V: A deep learning approach to variable-to-variable selection and translation for multivariate time-varying data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fluid flow data set for machine learning and its
application to neural flow map interpolation. <em>TVCG</em>,
<em>27</em>(2), 1279–1289. (<a
href="https://doi.org/10.1109/TVCG.2020.3028947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.},
  archive      = {J_TVCG},
  author       = {Jakob Jakob and Markus Gross and Tobias Günther},
  doi          = {10.1109/TVCG.2020.3028947},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1279-1289},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A fluid flow data set for machine learning and its application to neural flow map interpolation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep volumetric ambient occlusion. <em>TVCG</em>,
<em>27</em>(2), 1268–1278. (<a
href="https://doi.org/10.1109/TVCG.2020.3030344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel deep learning based technique for volumetric ambient occlusion in the context of direct volume rendering. Our proposed Deep Volumetric Ambient Occlusion (DVAO) approach can predict per-voxel ambient occlusion in volumetric data sets, while considering global information provided through the transfer function. The proposed neural network only needs to be executed upon change of this global information, and thus supports real-time volume interaction. Accordingly, we demonstrate DVAO&#39;s ability to predict volumetric ambient occlusion, such that it can be applied interactively within direct volume rendering. To achieve the best possible results, we propose and analyze a variety of transfer function representations and injection strategies for deep neural networks. Based on the obtained results we also give recommendations applicable in similar volume learning scenarios. Lastly, we show that DVAO generalizes to a variety of modalities, despite being trained on computed tomography data only.},
  archive      = {J_TVCG},
  author       = {Dominik Engel and Timo Ropinski},
  doi          = {10.1109/TVCG.2020.3030344},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1268-1278},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep volumetric ambient occlusion},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MetroSets: Visualizing sets as metro maps. <em>TVCG</em>,
<em>27</em>(2), 1257–1267. (<a
href="https://doi.org/10.1109/TVCG.2020.3030475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose MetroSets, a new, flexible online tool for visualizing set systems using the metro map metaphor. We model a given set system as a hypergraph $\mathcal{H}=(V,\ \mathcal{S})$ , consisting of a set $V$ of vertices and a set $\mathcal{S}$ , which contains subsets of $V$ called hyperedges. Our system then computes a metro map representation of $\mathcal{H}$ , where each hyperedge $E$ in $\mathcal{S}$ corresponds to a metro line and each vertex corresponds to a metro station. Vertices that appear in two or more hyperedges are drawn as interchanges in the metro map, connecting the different sets. MetroSets is based on a modular 4-step pipeline which constructs and optimizes a path-based hypergraph support, which is then drawn and schematized using metro map layout algorithms. We propose and implement multiple algorithms for each step of the MetroSet pipeline and provide a functional prototype with easy-to-use preset configurations. Furthermore, using several real-world datasets, we perform an extensive quantitative evaluation of the impact of different pipeline stages on desirable properties of the generated maps, such as octolinearity, monotonicity, and edge uniformity.},
  archive      = {J_TVCG},
  author       = {Ben Jacobsen and Markus Wallinger and Stephen Kobourov and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2020.3030475},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1257-1267},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MetroSets: Visualizing sets as metro maps},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zoomless maps: External labeling methods for the interactive
exploration of dense point sets at a fixed map scale. <em>TVCG</em>,
<em>27</em>(2), 1247–1256. (<a
href="https://doi.org/10.1109/TVCG.2020.3030399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualizing spatial data on small-screen devices such as smartphones and smartwatches poses new challenges in computational cartography. The current interfaces for map exploration require their users to zoom in and out frequently. Indeed, zooming and panning are tools suitable for choosing the map extent corresponding to an area of interest. They are not as suitable, however, for resolving the graphical clutter caused by a high feature density since zooming in to a large map scale leads to a loss of context. Therefore, in this paper, we present new external labeling methods that allow a user to navigate through dense sets of points of interest while keeping the current map extent fixed. We provide a unified model, in which labels are placed at the boundary of the map and visually associated with the corresponding features via connecting lines, which are called leaders. Since the screen space is limited, labeling all features at the same time is impractical. Therefore, at any time, we label a subset of the features. We offer interaction techniques to change the current selection of features systematically and, thus, give the user access to all features. We distinguish three methods, which allow the user either to slide the labels along the bottom side of the map or to browse the labels based on pages or stacks. We present a generic algorithmic framework that provides us with the possibility of expressing the different variants of interaction techniques as optimization problems in a unified way. We propose both exact algorithms and fast and simple heuristics that solve the optimization problems taking into account different criteria such as the ranking of the labels, the total leader length as well as the distance between leaders. In experiments on real-world data we evaluate these algorithms and discuss the three variants with respect to their strengths and weaknesses proving the flexibility of the presented algorithmic framework.},
  archive      = {J_TVCG},
  author       = {Sven Gedicke and Annika Bonerath and Benjamin Niedermann and Jan-Henrik Haunert},
  doi          = {10.1109/TVCG.2020.3030399},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1247-1256},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Zoomless maps: External labeling methods for the interactive exploration of dense point sets at a fixed map scale},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple pipeline for coherent grid maps. <em>TVCG</em>,
<em>27</em>(2), 1236–1246. (<a
href="https://doi.org/10.1109/TVCG.2020.3028953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid maps are spatial arrangements of simple tiles (often squares or hexagons), each of which represents a spatial element. They are an established, effective way to show complex data per spatial element, using visual encodings within each tile ranging from simple coloring to nested small-multiples visualizations. An effective grid map is coherent with the underlying geographic space: the tiles maintain the contiguity, neighborhoods and identifiability of the corresponding spatial elements, while the grid map as a whole maintains the global shape of the input. Of particular importance are salient local features of the global shape which need to be represented by tiles assigned to the appropriate spatial elements. State-of-the-art techniques can adequately deal only with simple cases, such as close-to-uniform spatial distributions or global shapes that have few characteristic features. We introduce a simple fully-automated 3-step pipeline for computing coherent grid maps. Each step is a well-studied problem: shape decomposition based on salient features, tile-based Mosaic Cartograms, and point-set matching. Our pipeline is a seamless composition of existing techniques for these problems and results in high-quality grid maps. We provide an implementation, demonstrate the efficacy of our approach on various complex datasets, and compare it to the state-of-the-art.},
  archive      = {J_TVCG},
  author       = {Wouter Meulemans and Max Sondag and Bettina Speckmann},
  doi          = {10.1109/TVCG.2020.3028953},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1236-1246},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A simple pipeline for coherent grid maps},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cartographic relief shading with neural networks.
<em>TVCG</em>, <em>27</em>(2), 1225–1235. (<a
href="https://doi.org/10.1109/TVCG.2020.3030456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.},
  archive      = {J_TVCG},
  author       = {Bernhard Jenny and Magnus Heitzler and Dilpreet Singh and Marianna Farmakis-Serebryakova and Jeffery Chieh Liu and Lorenz Hurni},
  doi          = {10.1109/TVCG.2020.3030456},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1225-1235},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cartographic relief shading with neural networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embodied navigation in immersive abstract data
visualization: Is overview+detail or zooming better for 3D scatterplots?
<em>TVCG</em>, <em>27</em>(2), 1214–1224. (<a
href="https://doi.org/10.1109/TVCG.2020.3030427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale. Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied. We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview. We find significant differences in participants&#39; response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.},
  archive      = {J_TVCG},
  author       = {Yalong Yang and Maxime Cordeil and Johanna Beyer and Tim Dwyer and Kim Marriott and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2020.3030427},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1214-1224},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Embodied navigation in immersive abstract data visualization: Is Overview+Detail or zooming better for 3D scatterplots?},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisited: Comparison of empirical methods to evaluate
visualizations supporting crafting and assembly purposes. <em>TVCG</em>,
<em>27</em>(2), 1204–1213. (<a
href="https://doi.org/10.1109/TVCG.2020.3030400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach&#39;s primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.},
  archive      = {J_TVCG},
  author       = {Maximilian Weiß and Katrin Angerbauer and Alexandra Voit and Magdalena Schwarzl and Michael Sedlmair and Sven Mayer},
  doi          = {10.1109/TVCG.2020.3030400},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1204-1213},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisited: Comparison of empirical methods to evaluate visualizations supporting crafting and assembly purposes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uplift: A tangible and immersive tabletop system for casual
collaborative visual analytics. <em>TVCG</em>, <em>27</em>(2),
1193–1203. (<a href="https://doi.org/10.1109/TVCG.2020.3030334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short &#39;bursts&#39; of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such &#39;casual collaborative&#39; scenarios will require engaging features to draw users&#39; attention, with intuitive, &#39;walk-up and use&#39; interfaces. This paper presents Uplift, a novel prototype system to support &#39;casual collaborative visual analytics&#39; for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.},
  archive      = {J_TVCG},
  author       = {Barrett Ens and Sarah Goodwin and Arnaud Prouzeau and Fraser Anderson and Florence Y. Wang and Samuel Gratzl and Zac Lucarelli and Brendan Moyle and Jim Smiley and Tim Dwyer},
  doi          = {10.1109/TVCG.2020.3030334},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1193-1203},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uplift: A tangible and immersive tabletop system for casual collaborative visual analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personal augmented reality for information visualization on
large interactive displays. <em>TVCG</em>, <em>27</em>(2), 1182–1192.
(<a href="https://doi.org/10.1109/TVCG.2020.3030460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.},
  archive      = {J_TVCG},
  author       = {Patrick Reipschlager and Tamara Flemisch and Raimund Dachselt},
  doi          = {10.1109/TVCG.2020.3030460},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1182-1192},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Personal augmented reality for information visualization on large interactive displays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shared surfaces and spaces: Collaborative data visualisation
in a co-located immersive environment. <em>TVCG</em>, <em>27</em>(2),
1171–1181. (<a href="https://doi.org/10.1109/TVCG.2020.3030450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner&#39;s personal workspace.},
  archive      = {J_TVCG},
  author       = {Benjamin Lee and Xiaoyun Hu and Maxime Cordeil and Arnaud Prouzeau and Bernhard Jenny and Tim Dwyer},
  doi          = {10.1109/TVCG.2020.3030450},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1171-1181},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shared surfaces and spaces: Collaborative data visualisation in a co-located immersive environment},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention flows: Analyzing and comparing attention
mechanisms in language models. <em>TVCG</em>, <em>27</em>(2), 1160–1170.
(<a href="https://doi.org/10.1109/TVCG.2020.3028976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model&#39;s attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.},
  archive      = {J_TVCG},
  author       = {Joseph F. DeRose and Jiayao Wang and Matthew Berger},
  doi          = {10.1109/TVCG.2020.3028976},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1160-1170},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Attention flows: Analyzing and comparing attention mechanisms in language models},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of text alignment visualization. <em>TVCG</em>,
<em>27</em>(2), 1149–1159. (<a
href="https://doi.org/10.1109/TVCG.2020.3028975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text alignment is one of the fundamental techniques text-related domains like natural language processing, computational linguistics, and digital humanities. It compares two or more texts with each other aiming to find similar textual patterns, or to estimate in general how different or similar the texts are. Visualizing alignment results is an essential task, because it helps researchers getting a comprehensive overview of individual findings and the overall pattern structure. Different approaches have been developed to visualize and help making sense of these patterns depending on text size, alignment methods, and, most importantly, the underlying research tasks demanding for alignment. On the basis of those tasks, we reviewed existing text alignment visualization approaches, and discuss their advantages and drawbacks. We finally derive design implications and shed light on related future challenges.},
  archive      = {J_TVCG},
  author       = {Tariq Yousef and Stefan Janicke},
  doi          = {10.1109/TVCG.2020.3028975},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1149-1159},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of text alignment visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analysis of argumentation in essays. <em>TVCG</em>,
<em>27</em>(2), 1139–1148. (<a
href="https://doi.org/10.1109/TVCG.2020.3030425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.},
  archive      = {J_TVCG},
  author       = {Dora Kiesel and Patrick Riehmann and Henning Wachsmuth and Benno Stein and Bernd Froehlich},
  doi          = {10.1109/TVCG.2020.3030425},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1139-1148},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of argumentation in essays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A structured review of data management technology for
interactive visualization and analysis. <em>TVCG</em>, <em>27</em>(2),
1128–1138. (<a href="https://doi.org/10.1109/TVCG.2020.3028891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.},
  archive      = {J_TVCG},
  author       = {Leilani Battle and Carlos Scheidegger},
  doi          = {10.1109/TVCG.2020.3028891},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1128-1138},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A structured review of data management technology for interactive visualization and analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A design space of vision science methods for visualization
research. <em>TVCG</em>, <em>27</em>(2), 1117–1127. (<a
href="https://doi.org/10.1109/TVCG.2020.3029413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.},
  archive      = {J_TVCG},
  author       = {Madison A. Elliott and Christine Nothelfer and Cindy Xiong and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2020.3029413},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1117-1127},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A design space of vision science methods for visualization research},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Insights from experiments with rigor in an EvoBio design
study. <em>TVCG</em>, <em>27</em>(2), 1106–1116. (<a
href="https://doi.org/10.1109/TVCG.2020.3030405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.},
  archive      = {J_TVCG},
  author       = {Jen Rogers and Austin H. Patton and Luke Harmon and Alexander Lex and Miriah Meyer},
  doi          = {10.1109/TVCG.2020.3030405},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1106-1116},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Insights from experiments with rigor in an EvoBio design study},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data visceralization: Enabling deeper understanding of data
using virtual reality. <em>TVCG</em>, <em>27</em>(2), 1095–1105. (<a
href="https://doi.org/10.1109/TVCG.2020.3030435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.},
  archive      = {J_TVCG},
  author       = {Benjamin Lee and Dave Brown and Bongshin Lee and Christophe Hurter and Steven Drucker and Tim Dwyer},
  doi          = {10.1109/TVCG.2020.3030435},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1095-1105},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data visceralization: Enabling deeper understanding of data using virtual reality},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Introducing layers of meaning (LoM): A framework to reduce
semantic distance of visualization in humanistic research.
<em>TVCG</em>, <em>27</em>(2), 1084–1094. (<a
href="https://doi.org/10.1109/TVCG.2020.3030426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information visualization (infovis) is a powerful tool for exploring rich datasets. Within humanistic research, rich qualitative data and domain culture make traditional infovis approaches appear reductive and disconnected, leading to low adoption. In this paper, we use a multi-step approach to scrutinize the relationship between infovis and the humanities and suggest new directions for it. We first look into infovis from the humanistic perspective by exploring the humanistic literature around infovis. We validate and expand those findings though a co-design workshop with humanist and infovis experts. Then, we translate our findings into guidelines for designers and conduct a design critique exercise to explore their effect on the perception of humanist researchers. Based on these steps, we introduce Layers of Meaning, a framework to reduce the semantic distance between humanist researchers and visualizations of their research material, by grounding infovis tools in time and space, physicality, terminology, nuance, and provenance.},
  archive      = {J_TVCG},
  author       = {Houda Lamqaddam and Andrew Vande Moere and Vero Vanden Abeele and Koenraad Brosens and Katrien Verbert},
  doi          = {10.1109/TVCG.2020.3030426},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1084-1094},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Introducing layers of meaning (LoM): A framework to reduce semantic distance of visualization in humanistic research},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revealing perceptual proxies with adversarial examples.
<em>TVCG</em>, <em>27</em>(2), 1073–1083. (<a
href="https://doi.org/10.1109/TVCG.2020.3030429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer-the series with the larger arithmetic mean or range-was pitted against an “adversarial” series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.},
  archive      = {J_TVCG},
  author       = {Brian D. Ondov and Fumeng Yang and Matthew Kay and Niklas Elmqvist and Steven Franconeri},
  doi          = {10.1109/TVCG.2020.3030429},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1073-1083},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revealing perceptual proxies with adversarial examples},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). No mark is an island: Precision and category repulsion
biases in data reproductions. <em>TVCG</em>, <em>27</em>(2), 1063–1072.
(<a href="https://doi.org/10.1109/TVCG.2020.3030345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data visualization is powerful in large part because it facilitates visual extraction of values. Yet, existing measures of perceptual precision for data channels (e.g., position, length, orientation, etc.) are based largely on verbal reports of ratio judgments between two values (e.g., [7]). Verbal report conflates multiple sources of error beyond actual visual precision, introducing a ratio computation between these values and a requirement to translate that ratio to a verbal number. Here we observe raw measures of precision by eliminating both ratio computations and verbal reports; we simply ask participants to reproduce marks (a single bar or dot) to match a previously seen one. We manipulated whether the mark was initially presented (and later drawn) alone, paired with a reference (e.g. a second `100\%&#39; bar also present at test, or a y-axis for the dot), or integrated with the reference (merging that reference bar into a stacked bar graph, or placing the dot directly on the axis). Reproductions of smaller values were overestimated, and larger values were underestimated, suggesting systematic memory biases. Average reproduction error was around 10\% of the actual value, regardless of whether the reproduction was done on a common baseline with the original. In the reference and (especially) the integrated conditions, responses were repulsed from an implicit midpoint of the reference mark, such that values above 50\% were overestimated, and values below 50\% were underestimated. This reproduction paradigm may serve within a new suite of more fundamental measures of the precision of graphical perception.},
  archive      = {J_TVCG},
  author       = {Caitlyn M. McColeman and Lane Harrison and Mi Feng and Steven Franconeri},
  doi          = {10.1109/TVCG.2020.3030345},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1063-1072},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {No mark is an island: Precision and category repulsion biases in data reproductions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Truth or square: Aspect ratio biases recall of position
encodings. <em>TVCG</em>, <em>27</em>(2), 1054–1062. (<a
href="https://doi.org/10.1109/TVCG.2020.3030422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bar charts are among the most frequently used visualizations, in part because their position encoding leads them to convey data values precisely. Yet reproductions of single bars or groups of bars within a graph can be biased. Curiously, some previous work found that this bias resulted in an overestimation of reproduced data values, while other work found an underestimation. Across three empirical studies, we offer an explanation for these conflicting findings: this discrepancy is a consequence of the differing aspect ratios of the tested bar marks. Viewers are biased to remember a bar mark as being more similar to a prototypical square, leading to an overestimation of bars with a wide aspect ratio, and an underestimation of bars with a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of the bar marks indeed influenced the direction of this bias. Experiment 3 confirmed that this pattern of misestimation bias was present for reproductions from memory, suggesting that this bias may arise when comparing values across sequential displays or views. We describe additional visualization designs that might be prone to this bias beyond bar charts (e.g., Mekko charts and treemaps), and speculate that other visual channels might hold similar biases toward prototypical values.},
  archive      = {J_TVCG},
  author       = {Cristina R. Ceja and Caitlyn M. McColeman and Cindy Xiong and Steven L. Franconeri},
  doi          = {10.1109/TVCG.2020.3030422},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1054-1062},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Truth or square: Aspect ratio biases recall of position encodings},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A testing environment for continuous colormaps.
<em>TVCG</em>, <em>27</em>(2), 1043–1053. (<a
href="https://doi.org/10.1109/TVCG.2020.3028955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computer science disciplines (e.g., combinatorial optimization, natural language processing, and information retrieval) use standard or established test suites for evaluating algorithms. In visualization, similar approaches have been adopted in some areas (e.g., volume visualization), while user testimonies and empirical studies have been the dominant means of evaluation in most other areas, such as designing colormaps. In this paper, we propose to establish a test suite for evaluating the design of colormaps. With such a suite, the users can observe the effects when different continuous colormaps are applied to planar scalar fields that may exhibit various characteristic features, such as jumps, local extrema, ridge or valley lines, different distributions of scalar values, different gradients, different signal frequencies, different levels of noise, and so on. The suite also includes an expansible collection of real-world data sets including the most popular data for colormap testing in the visualization literature. The test suite has been integrated into a web-based application for creating continuous colormaps (https://ccctool.com/), facilitating close inter-operation between design and evaluation processes. This new facility complements traditional evaluation methods such as user testimonies and empirical studies.},
  archive      = {J_TVCG},
  author       = {P. Nardini and M. Chen and R. Bujack and M. Bottinger and G. Scheuermann},
  doi          = {10.1109/TVCG.2020.3028955},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1043-1053},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A testing environment for continuous colormaps},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rainbows revisited: Modeling effective colormap design for
graphical inference. <em>TVCG</em>, <em>27</em>(2), 1032–1042. (<a
href="https://doi.org/10.1109/TVCG.2020.3030439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color mapping is a foundational technique for visualizing scalar data. Prior literature offers guidelines for effective colormap design, such as emphasizing luminance variation while limiting changes in hue. However, empirical studies of color are largely focused on perceptual tasks. This narrow focus inhibits our understanding of how generalizable these guidelines are, particularly to tasks like visual inference that require synthesis and judgement across multiple percepts. Furthermore, the emphasis on traditional ramp designs (e.g., sequential or diverging) may sideline other key metrics or design strategies. We study how a cognitive metric-color name variation-impacts people&#39;s ability to make model-based judgments. In two graphical inference experiments, participants saw a series of color-coded scalar fields sampled from different models and assessed the relationships between these models. Contrary to conventional guidelines, participants were more accurate when viewing colormaps that cross a variety of uniquely nameable colors. We modeled participants&#39; performance using this metric and found that it provides a better fit to the experimental data than do existing design principles. Our findings indicate cognitive advantages for colorful maps like rainbow, which exhibit high color categorization, despite their traditionally undesirable perceptual properties. We also found no evidence that color categorization would lead observers to infer false data features. Our results provide empirically grounded metrics for predicting a colormap&#39;s performance and suggest alternative guidelines for designing new quantitative colormaps to support inference. The data and materials for this paper are available at: https://osf.io/tck2r/.},
  archive      = {J_TVCG},
  author       = {Khairi Reda and Danielle Albers Szafir},
  doi          = {10.1109/TVCG.2020.3030439},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1032-1042},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Rainbows revisited: Modeling effective colormap design for graphical inference},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic discriminability for visual communication.
<em>TVCG</em>, <em>27</em>(2), 1022–1031. (<a
href="https://doi.org/10.1109/TVCG.2020.3030434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To interpret information visualizations, observers must determine how visual features map onto concepts. First and foremost, this ability depends on perceptual discriminability; observers must be able to see the difference between different colors for those colors to communicate different meanings. However, the ability to interpret visualizations also depends on semantic discriminability, the degree to which observers can infer a unique mapping between visual features and concepts, based on the visual features and concepts alone (i.e., without help from verbal cues such as legends or labels). Previous evidence suggested that observers were better at interpreting encoding systems that maximized semantic discriminability (maximizing association strength between assigned colors and concepts while minimizing association strength between unassigned colors and concepts), compared to a system that only maximized color-concept association strength. However, increasing semantic discriminability also resulted in increased perceptual distance, so it is unclear which factor was responsible for improved performance. In the present study, we conducted two experiments that tested for independent effects of semantic distance and perceptual distance on semantic discriminability of bar graph data visualizations. Perceptual distance was large enough to ensure colors were more than just noticeably different. We found that increasing semantic distance improved performance, independent of variation in perceptual distance, and when these two factors were uncorrelated, responses were dominated by semantic distance. These results have implications for navigating trade-offs in color palette design optimization for visual communication.},
  archive      = {J_TVCG},
  author       = {Karen B. Schloss and Zachary Leggon and Laurent Lessard},
  doi          = {10.1109/TVCG.2020.3030434},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1022-1031},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Semantic discriminability for visual communication},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Insight beyond numbers: The impact of qualitative factors on
visual data analysis. <em>TVCG</em>, <em>27</em>(2), 1011–1021. (<a
href="https://doi.org/10.1109/TVCG.2020.3030376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.},
  archive      = {J_TVCG},
  author       = {Benjamin Karer and Hans Hagen and Dirk J. Lehmann},
  doi          = {10.1109/TVCG.2020.3030376},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1011-1021},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Insight beyond numbers: The impact of qualitative factors on visual data analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards modeling visualization processes as dynamic bayesian
networks. <em>TVCG</em>, <em>27</em>(2), 1000–1010. (<a
href="https://doi.org/10.1109/TVCG.2020.3030395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.},
  archive      = {J_TVCG},
  author       = {Christian Heine},
  doi          = {10.1109/TVCG.2020.3030395},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {1000-1010},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards modeling visualization processes as dynamic bayesian networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian-assisted inference from visualized data.
<em>TVCG</em>, <em>27</em>(2), 989–999. (<a
href="https://doi.org/10.1109/TVCG.2020.3028984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian view of data interpretation suggests that a visualization user should update their existing beliefs about a parameter&#39;s value in accordance with the amount of information about the parameter value captured by the new observations. Extending recent work applying Bayesian models to understand and evaluate belief updating from visualizations, we show how the predictions of Bayesian inference can be used to guide more rational belief updating. We design a Bayesian inference-assisted uncertainty analogy that numerically relates uncertainty in observed data to the user&#39;s subjective uncertainty, and a posterior visualization that prescribes how a user should update their beliefs given their prior beliefs and the observed data. In a pre-registered experiment on 4,800 people, we find that when a newly observed data sample is relatively small (N=158), both techniques reliably improve people&#39;s Bayesian updating on average compared to the current best practice of visualizing uncertainty in the observed data. For large data samples (N=5208), where people&#39;s updated beliefs tend to deviate more strongly from the prescriptions of a Bayesian model, we find evidence that the effectiveness of the two forms of Bayesian assistance may depend on people&#39;s proclivity toward trusting the source of the data. We discuss how our results provide insight into individual processes of belief updating and subjective uncertainty, and how understanding these aspects of interpretation paves the way for more sophisticated interactive visualizations for analysis and communication.},
  archive      = {J_TVCG},
  author       = {Yea-Seul Kim and Paula Kayongo and Madeleine Grunde-McLaughlin and Jessica Hullman},
  doi          = {10.1109/TVCG.2020.3028984},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {989-999},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bayesian-assisted inference from visualized data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bayesian cognition approach for belief updating of
correlation judgement through uncertainty visualizations. <em>TVCG</em>,
<em>27</em>(2), 978–988. (<a
href="https://doi.org/10.1109/TVCG.2020.3029412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding correlation judgement is important to designing effective visualizations of bivariate data. Prior work on correlation perception has not considered how factors including prior beliefs and uncertainty representation impact such judgements. The present work focuses on the impact of uncertainty communication when judging bivariate visualizations. Specifically, we model how users update their beliefs about variable relationships after seeing a scatterplot with and without uncertainty representation. To model and evaluate the belief updating, we present three studies. Study 1 focuses on a proposed “Line + Cone” visual elicitation method for capturing users&#39; beliefs in an accurate and intuitive fashion. The findings reveal that our proposed method of belief solicitation reduces complexity and accurately captures the users&#39; uncertainty about a range of bivariate relationships. Study 2 leverages the “Line + Cone” elicitation method to measure belief updating on the relationship between different sets of variables when seeing correlation visualization with and without uncertainty representation. We compare changes in users beliefs to the predictions of Bayesian cognitive models which provide normative benchmarks for how users should update their prior beliefs about a relationship in light of observed data. The findings from Study 2 revealed that one of the visualization conditions with uncertainty communication led to users being slightly more confident about their judgement compared to visualization without uncertainty information. Study 3 builds on findings from Study 2 and explores differences in belief update when the bivariate visualization is congruent or incongruent with users&#39; prior belief. Our results highlight the effects of incorporating uncertainty representation, and the potential of measuring belief updating on correlation judgement with Bayesian cognitive models.},
  archive      = {J_TVCG},
  author       = {Alireza Karduni and Douglas Markant and Ryan Wesslen and Wenwen Dou},
  doi          = {10.1109/TVCG.2020.3029412},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {978-988},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A bayesian cognition approach for belief updating of correlation judgement through uncertainty visualizations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data comics for reporting controlled user studies in
human-computer interaction. <em>TVCG</em>, <em>27</em>(2), 967–977. (<a
href="https://doi.org/10.1109/TVCG.2020.3030433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by data comics, this paper introduces a novel format for reporting controlled studies in the domain of human-computer interaction (HCI). While many studies in HCI follow similar steps in explaining hypotheses, laying out a study design, and reporting results, many of these decisions are buried in blocks of dense scientific text. We propose leveraging data comics as study reports to provide an open and glanceable view of studies by tightly integrating text and images, illustrating design decisions and key insights visually, resulting in visual narratives that can be compelling to non-scientists and researchers alike. Use cases of data comics study reports range from illustrations for non-scientific audiences to graphical abstracts, study summaries, technical talks, textbooks, teaching, blogs, supplementary submission material, and inclusion in scientific articles. This paper provides examples of data comics study reports alongside a graphical repertoire of examples, embedded in a framework of guidelines for creating comics reports which was iterated upon and evaluated through a series of collaborative design sessions.},
  archive      = {J_TVCG},
  author       = {Zezhong Wang and Jacob Ritchie and Jingtao Zhou and Fanny Chevalier and Benjamin Bach},
  doi          = {10.1109/TVCG.2020.3030433},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {967-977},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Data comics for reporting controlled user studies in human-computer interaction},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Table scraps: An actionable framework for multi-table data
wrangling from an artifact study of computational journalism.
<em>TVCG</em>, <em>27</em>(2), 957–966. (<a
href="https://doi.org/10.1109/TVCG.2020.3030462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.},
  archive      = {J_TVCG},
  author       = {Stephen Kasica and Charles Berret and Tamara Munzner},
  doi          = {10.1109/TVCG.2020.3030462},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {957-966},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Table scraps: An actionable framework for multi-table data wrangling from an artifact study of computational journalism},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communicative visualizations as a learning problem.
<em>TVCG</em>, <em>27</em>(2), 946–956. (<a
href="https://doi.org/10.1109/TVCG.2020.3030375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant research has provided robust task and evaluation languages for the analysis of exploratory visualizations. Unfortunately, these taxonomies fail when applied to communicative visualizations. Instead, designers often resort to evaluating communicative visualizations from the cognitive efficiency perspective: “can the recipient accurately decode my message/insight?” However, designers are unlikely to be satisfied if the message went `in one ear and out the other.&#39; The consequence of this inconsistency is that it is difficult to design or select between competing options in a principled way. The problem we address is the fundamental mismatch between how designers want to describe their intent, and the language they have. We argue that visualization designers can address this limitation through a learning lens: that the recipient is a student and the designer a teacher. By using learning objectives, designers can better define, assess, and compare communicative visualizations. We illustrate how the learning-based approach provides a framework for understanding a wide array of communicative goals. To understand how the framework can be applied (and its limitations), we surveyed and interviewed members of the Data Visualization Society using their own visualizations as a probe. Through this study we identified the broad range of objectives in communicative visualizations and the prevalence of certain objective types.},
  archive      = {J_TVCG},
  author       = {Eytan Adar and Elsie Lee},
  doi          = {10.1109/TVCG.2020.3030375},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {946-956},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Communicative visualizations as a learning problem},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sea of genes: A reflection on visualising metagenomic data
for museums. <em>TVCG</em>, <em>27</em>(2), 935–945. (<a
href="https://doi.org/10.1109/TVCG.2020.3030412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the process of designing an exhibit to communicate scientific findings from a complex dataset and unfamiliar domain to the public in a science museum. Our exhibit sought to communicate new lessons based on scientific findings from the domain of metagenomics. This multi-user exhibit had three goals: (1) to inform the public about microbial communities and their daily cycles; (2) to link microbes&#39; activity to the concept of gene expression; (3) and to highlight scientists&#39; use of gene expression data to understand the role of microbes. To address these three goals, we derived visualization designs with three corresponding stories, each corresponding to a goal. We present three successive rounds of design and evaluation of our attempts to convey these goals. We could successfully present one story but had limited success with our second and third goals. This work presents a detailed account of an attempt to explain tightly coupled relationships through storytelling and animation in a multi-user, informal learning environment to a public with varying prior knowledge on the domain and identify lessons for future design.},
  archive      = {J_TVCG},
  author       = {Keshav Dasu and Kwan-Liu Ma and Joyce Ma and Jennifer Frazier},
  doi          = {10.1109/TVCG.2020.3030412},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {935-945},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sea of genes: A reflection on visualising metagenomic data for museums},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing narrative-focused role-playing games for
visualization literacy in young children. <em>TVCG</em>, <em>27</em>(2),
924–934. (<a href="https://doi.org/10.1109/TVCG.2020.3030464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.},
  archive      = {J_TVCG},
  author       = {Elaine Huynh and Angela Nyhout and Patricia Ganea and Fanny Chevalier},
  doi          = {10.1109/TVCG.2020.3030464},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {924-934},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing narrative-focused role-playing games for visualization literacy in young children},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chemicals in the creek: Designing a situated data
physicalization of open government data with the community.
<em>TVCG</em>, <em>27</em>(2), 913–923. (<a
href="https://doi.org/10.1109/TVCG.2020.3030472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade growing amounts of government data have been made available in an attempt to increase transparency and civic participation, but it is unclear if this data serves non-expert communities due to gaps in access and the technical knowledge needed to interpret this “open” data. We conducted a two-year design study focused on the creation of a community-based data display using the United States Environmental Protection Agency data on water permit violations by oil storage facilities on the Chelsea Creek in Massachusetts to explore whether situated data physicalization and Participatory Action Research could support meaningful engagement with open data. We selected this data as it is of interest to local groups and available online, yet remains largely invisible and inaccessible to the Chelsea community. The resulting installation, Chemicals in the Creek, responds to the call for community-engaged visualization processes and provides an application of situated methods of data representation. It proposes event-centered and power-aware modes of engagement using contextual and embodied data representations. The design of Chemicals in the Creek is grounded in interactive workshops and we analyze it through event observation, interviews, and community outcomes. We reflect on the role of community engaged research in the Information Visualization community relative to recent conversations on new approaches to design studies and evaluation.},
  archive      = {J_TVCG},
  author       = {Laura J. Perovich and Sara Ann Wylie and Roseann Bongiovanni},
  doi          = {10.1109/TVCG.2020.3030472},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {913-923},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Chemicals in the creek: Designing a situated data physicalization of open government data with the community},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visualization framework for multi-scale coherent
structures in taylor-couette turbulence. <em>TVCG</em>, <em>27</em>(2),
902–912. (<a href="https://doi.org/10.1109/TVCG.2020.3028892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taylor-Couette flow (TCF) is the turbulent fluid motion created between two concentric and independently rotating cylinders. It has been heavily researched in fluid mechanics thanks to the various nonlinear dynamical phenomena that are exhibited in the flow. As many dense coherent structures overlap each other in TCF, it is challenging to isolate and visualize them, especially when the cylinder rotation ratio is changing. Previous approaches rely on 2D cross sections to study TCF due to its simplicity, which cannot provide the complete information of TCF. In the meantime, standard visualization techniques, such as volume rendering / iso-surfacing of certain attributes and the placement of integral curves/surfaces, usually produce cluttered visualization. To address this challenge and to support domain experts in the analysis of TCF, we developed a visualization framework to separate large-scale structures from the dense, small-scale structures and provide an effective visual representation of these structures. Instead of using a single physical attribute as the standard approach which cannot efficiently separate structures in different scales for TCF, we adapt the feature level-set method to combine multiple attributes and use them as a filter to separate large- and small-scale structures. To visualize these structures, we apply the iso-surface extraction on the kernel density estimate of the distance field generated from the feature level-set. The proposed methods successfully reveal 3D large-scale coherent structures of TCF with different control parameter settings, which are difficult to achieve with the conventional methods.},
  archive      = {J_TVCG},
  author       = {Duong B. Nguyen and Rodolfo Ostilla Monico and Guoning Chen},
  doi          = {10.1109/TVCG.2020.3028892},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {902-912},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visualization framework for multi-scale coherent structures in taylor-couette turbulence},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ChemVA: Interactive visual analysis of chemical compound
similarity in virtual screening. <em>TVCG</em>, <em>27</em>(2), 891–901.
(<a href="https://doi.org/10.1109/TVCG.2020.3030438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.},
  archive      = {J_TVCG},
  author       = {María Virginia Sabando and Pavol Ulbrich and Matías Selzer and Jan Byška and Jan Mičan and Ignacio Ponzoni and Axel J. Soto and María Luján Ganuza and Barbora Kozlíková},
  doi          = {10.1109/TVCG.2020.3030438},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {891-901},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChemVA: Interactive visual analysis of chemical compound similarity in virtual screening},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visilant: Visual support for the exploration and analytical
process tracking in criminal investigations. <em>TVCG</em>,
<em>27</em>(2), 881–890. (<a
href="https://doi.org/10.1109/TVCG.2020.3030356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive conclusions, while the data can be incomplete or inconsistent and is changed and updated throughout the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study.},
  archive      = {J_TVCG},
  author       = {Kristína Zákopčanová and Marko Řeháček and Jozef Bátrna and Daniel Plakinger and Sergej Stoppel and Barbora Kozlíková},
  doi          = {10.1109/TVCG.2020.3030356},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {881-890},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visilant: Visual support for the exploration and analytical process tracking in criminal investigations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QLens: Visual analytics of MUlti-step problem-solving
behaviors for improving question design. <em>TVCG</em>, <em>27</em>(2),
870–880. (<a href="https://doi.org/10.1109/TVCG.2020.3030337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students&#39; problem-solving processes unfold step by step to infer whether students&#39; problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students&#39; problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.},
  archive      = {J_TVCG},
  author       = {Meng Xia and Reshika Palaniyappan Velumani and Yong Wang and Huamin Qu and Xiaojuan Ma},
  doi          = {10.1109/TVCG.2020.3030337},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {870-880},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {QLens: Visual analytics of MUlti-step problem-solving behaviors for improving question design},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ShuttleSpace: Exploring and analyzing movement trajectory in
immersive visualization. <em>TVCG</em>, <em>27</em>(2), 860–869. (<a
href="https://doi.org/10.1109/TVCG.2020.3030392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players&#39; performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player&#39;s perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.},
  archive      = {J_TVCG},
  author       = {Shuainan Ye and Zhutian Chen and Xiangtong Chu and Yifan Wang and Siwei Fu and Lejun Shen and Kun Zhou and Yingcai Wu},
  doi          = {10.1109/TVCG.2020.3030392},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {860-869},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ShuttleSpace: Exploring and analyzing movement trajectory in immersive visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TaxThemis: Interactive mining and exploration of suspicious
tax evasion groups. <em>TVCG</em>, <em>27</em>(2), 849–859. (<a
href="https://doi.org/10.1109/TVCG.2020.3030370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tax evasion is a serious economic problem for many countries, as it can undermine the government&#39;s tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.},
  archive      = {J_TVCG},
  author       = {Yating Lin and Kamkwai Wong and Yong Wang and Rong Zhang and Bo Dong and Huamin Qu and Qinghua Zheng},
  doi          = {10.1109/TVCG.2020.3030370},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {849-859},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TaxThemis: Interactive mining and exploration of suspicious tax evasion groups},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting the modifiable areal unit problem in deep traffic
prediction with visual analytics. <em>TVCG</em>, <em>27</em>(2),
839–848. (<a href="https://doi.org/10.1109/TVCG.2020.3030410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran&#39;s I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.},
  archive      = {J_TVCG},
  author       = {Wei Zeng and Chengqiao Lin and Juncong Lin and Jincheng Jiang and Jiazhi Xia and Cagatay Turkay and Wei Chen},
  doi          = {10.1109/TVCG.2020.3030410},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {839-848},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Revisiting the modifiable areal unit problem in deep traffic prediction with visual analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology density map for urban data visualization and
analysis. <em>TVCG</em>, <em>27</em>(2), 828–838. (<a
href="https://doi.org/10.1109/TVCG.2020.3030469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.},
  archive      = {J_TVCG},
  author       = {Zezheng Feng and Haotian Li and Wei Zeng and Shuang-Hua Yang and Huamin Qu},
  doi          = {10.1109/TVCG.2020.3030469},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {828-838},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Topology density map for urban data visualization and analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards better bus networks: A visual analytics approach.
<em>TVCG</em>, <em>27</em>(2), 817–827. (<a
href="https://doi.org/10.1109/TVCG.2020.3030458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bus routes are typically updated every 3–5 years to meet constantly changing travel demands. However, identifying deficient bus routes and finding their optimal replacements remain challenging due to the difficulties in analyzing a complex bus network and the large solution space comprising alternative routes. Most of the automated approaches cannot produce satisfactory results in real-world settings without laborious inspection and evaluation of the candidates. The limitations observed in these approaches motivate us to collaborate with domain experts and propose a visual analytics solution for the performance analysis and incremental planning of bus routes based on an existing bus network. Developing such a solution involves three major challenges, namely, a) the in-depth analysis of complex bus route networks, b) the interactive generation of improved route candidates, and c) the effective evaluation of alternative bus routes. For challenge a, we employ an overview-to-detail approach by dividing the analysis of a complex bus network into three levels to facilitate the efficient identification of deficient routes. For challenge b, we improve a route generation model and interpret the performance of the generation with tailored visualizations. For challenge c, we incorporate a conflict resolution strategy in the progressive decision-making process to assist users in evaluating the alternative routes and finding the most optimal one. The proposed system is evaluated with two usage scenarios based on real-world data and received positive feedback from the experts.},
  archive      = {J_TVCG},
  author       = {Di Weng and Chengbo Zheng and Zikun Deng and Mingze Ma and Jie Bao and Yu Zheng and Mingliang Xu and Yingcai Wu},
  doi          = {10.1109/TVCG.2020.3030458},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {817-827},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards better bus networks: A visual analytics approach},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polyphorm: Structural analysis of cosmological datasets via
interactive physarum polycephalum visualization. <em>TVCG</em>,
<em>27</em>(2), 806–816. (<a
href="https://doi.org/10.1109/TVCG.2020.3030407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm&#39;s simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Oskar Elek and Joseph N. Burchett and J. Xavier Prochaska and Angus G. Forbes},
  doi          = {10.1109/TVCG.2020.3030407},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {806-816},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Polyphorm: Structural analysis of cosmological datasets via interactive physarum polycephalum visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive black-hole visualization. <em>TVCG</em>,
<em>27</em>(2), 796–805. (<a
href="https://doi.org/10.1109/TVCG.2020.3030452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient algorithm for visualizing the effect of black holes on its distant surroundings as seen from an observer nearby in orbit. Our solution is GPU-based and builds upon a two-step approach, where we first derive an adaptive grid to map the 360-view around the observer to the distorted celestial sky, which can be directly reused for different camera orientations. Using a grid, we can rapidly trace rays back to the observer through the distorted spacetime, avoiding the heavy workload of standard tracing solutions at real-time rates. By using a novel interpolation technique we can also simulate an observer path by smoothly transitioning between multiple grids. Our approach accepts real star catalogues and environment maps of the celestial sky and generates the resulting black-hole deformations in real time.},
  archive      = {J_TVCG},
  author       = {Annemieke Verbraeck and Elmar Eisemann},
  doi          = {10.1109/TVCG.2020.3030452},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {796-805},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive black-hole visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive visualization of atmospheric effects for
celestial bodies. <em>TVCG</em>, <em>27</em>(2), 785–795. (<a
href="https://doi.org/10.1109/TVCG.2020.3030333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an atmospheric model tailored for the interactive visualization of planetary surfaces. As the exploration of the solar system is progressing with increasingly accurate missions and instruments, the faithful visualization of planetary environments is gaining increasing interest in space research, mission planning, and science communication and education. Atmospheric effects are crucial in data analysis and to provide contextual information for planetary data. Our model correctly accounts for the non-linear path of the light inside the atmosphere (in Earth&#39;s case), the light absorption effects by molecules and dust particles, such as the ozone layer and the Martian dust, and a wavelength-dependent phase function for Mie scattering. The mode focuses on interactivity, versatility, and customization, and a comprehensive set of interactive controls make it possible to adapt its appearance dynamically. We demonstrate our results using Earth and Mars as examples. However, it can be readily adapted for the exploration of other atmospheres found on, for example, of exoplanets. For Earth&#39;s atmosphere, we visually compare our results with pictures taken from the International Space Station and against the CIE clear sky model. The Martian atmosphere is reproduced based on available scientific data, feedback from domain experts, and is compared to images taken by the Curiosity rover. The work presented here has been implemented in the OpenSpace system, which enables interactive parameter setting and real-time feedback visualization targeting presentations in a wide range of environments, from immersive dome theaters to virtual reality headsets.},
  archive      = {J_TVCG},
  author       = {Jonathas Costa and Alexander Bock and Carter Emmart and Charles Hansen and Anders Ynnerman and Cláudio Silva},
  doi          = {10.1109/TVCG.2020.3030333},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {785-795},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive visualization of atmospheric effects for celestial bodies},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IsoTrotter: Visually guided empirical modelling of
atmospheric convection. <em>TVCG</em>, <em>27</em>(2), 775–784. (<a
href="https://doi.org/10.1109/TVCG.2020.3030389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.},
  archive      = {J_TVCG},
  author       = {Juraj Palenik and Thomas Spengler and Helwig Hauser},
  doi          = {10.1109/TVCG.2020.3030389},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {775-784},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {IsoTrotter: Visually guided empirical modelling of atmospheric convection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extraction and visualization of poincare map topology for
spacecraft trajectory design. <em>TVCG</em>, <em>27</em>(2), 765–774.
(<a href="https://doi.org/10.1109/TVCG.2020.3030402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mission designers must study many dynamical models to plan a low-cost spacecraft trajectory that satisfies mission constraints. They routinely use Poincare maps to search for a suitable path through the interconnected web of periodic orbits and invariant manifolds found in multi-body gravitational systems. This paper is concerned with the extraction and interactive visual exploration of this structural landscape to assist spacecraft trajectory planning. We propose algorithmic solutions that address the specific challenges posed by the characterization of the topology in astrodynamics problems and allow for an effective visual analysis of the resulting information. This visualization framework is applied to the circular restricted three-body problem (CR3BP), where it reveals novel periodic orbits with their relevant invariant manifolds in a suitable format for interactive transfer selection. Representative design problems illustrate how spacecraft path planners can leverage our topology visualization to fully exploit the natural dynamics pathways for energy-efficient trajectory designs.},
  archive      = {J_TVCG},
  author       = {Xavier Tricoche and Wayne Schlei and Kathleen C. Howell},
  doi          = {10.1109/TVCG.2020.3030402},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {765-774},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extraction and visualization of poincare map topology for spacecraft trajectory design},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). InCorr: Interactive data-driven correlation panels for
digital outcrop analysis. <em>TVCG</em>, <em>27</em>(2), 755–764. (<a
href="https://doi.org/10.1109/TVCG.2020.3030409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of ancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022 Rosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in seeking signs of past life on Mars. Geologists measure and interpret 3D DOMs, create sedimentary logs and combine them in `correlation panels&#39; to map the extents of key geological horizons, and build a stratigraphic model to understand their position in the ancient landscape. Currently, the creation of correlation panels is completely manual and therefore time-consuming, and inflexible. With InCorr we present a visualization solution that encompasses a 3D logging tool and an interactive data-driven correlation panel that evolves with the stratigraphic analysis. For the creation of InCorr we closely cooperated with leading planetary geologists in the form of a design study. We verify our results by recreating an existing correlation analysis with InCorr and validate our correlation panel against a manually created illustration. Further, we conducted a user-study with a wider circle of geologists. Our evaluation shows that InCorr efficiently supports the domain experts in tackling their research questions and that it has the potential to significantly impact how geologists work with digital outcrop representations in general.},
  archive      = {J_TVCG},
  author       = {Thomas Ortner and Andreas Walch and Rebecca Nowak and Robert Barnes and Thomas Höllt and M. Eduard Gröller},
  doi          = {10.1109/TVCG.2020.3030409},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {755-764},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {InCorr: Interactive data-driven correlation panels for digital outcrop analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the usability of virtual reality neuron tracing
with topological elements. <em>TVCG</em>, <em>27</em>(2), 744–754. (<a
href="https://doi.org/10.1109/TVCG.2020.3030363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users&#39; tracing speed increased, while retaining similar accuracy compared to a fully manual approach.},
  archive      = {J_TVCG},
  author       = {Torin McDonald and Will Usher and Nate Morrical and Attila Gyulassy and Steve Petruzza and Frederick Federer and Alessandra Angelucci and Valerio Pascucci},
  doi          = {10.1109/TVCG.2020.3030363},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {744-754},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving the usability of virtual reality neuron tracing with topological elements},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual cohort comparison for spatial single-cell omics-data.
<em>TVCG</em>, <em>27</em>(2), 733–743. (<a
href="https://doi.org/10.1109/TVCG.2020.3030336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.},
  archive      = {J_TVCG},
  author       = {Antonios Somarakis and Marieke E. Ijsselsteijn and Sietse J. Luk and Boyd Kenkhuis and Noel F.C.C. de Miranda and Boudewijn P.F. Lelieveldt and Thomas Höllt},
  doi          = {10.1109/TVCG.2020.3030336},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {733-743},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual cohort comparison for spatial single-cell omics-data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling in the time of COVID-19: Statistical and rule-based
mesoscale models. <em>TVCG</em>, <em>27</em>(2), 722–732. (<a
href="https://doi.org/10.1109/TVCG.2020.3030415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new technique for the rapid modeling and construction of scientifically accurate mesoscale biological models. The resulting 3D models are based on a few 2D microscopy scans and the latest knowledge available about the biological entity, represented as a set of geometric relationships. Our new visual-programming technique is based on statistical and rule-based modeling approaches that are rapid to author, fast to construct, and easy to revise. From a few 2D microscopy scans, we determine the statistical properties of various structural aspects, such as the outer membrane shape, the spatial properties, and the distribution characteristics of the macromolecular elements on the membrane. This information is utilized in the construction of the 3D model. Once all the imaging evidence is incorporated into the model, additional information can be incorporated by interactively defining the rules that spatially characterize the rest of the biological entity, such as mutual interactions among macromolecules, and their distances and orientations relative to other structures. These rules are defined through an intuitive 3D interactive visualization as a visual-programming feedback loop. We demonstrate the applicability of our approach on a use case of the modeling procedure of the SARS-CoV-2 virion ultrastructure. This atomistic model, which we present here, can steer biological research to new promising directions in our efforts to fight the spread of the virus.},
  archive      = {J_TVCG},
  author       = {Ngan Nguyen and Ondřej Strnad and Tobias Klein and Deng Luo and Ruwayda Alharbi and Peter Wonka and Martina Maritan and Peter Mindek and Ludovic Autin and David S. Goodsell and Ivan Viola},
  doi          = {10.1109/TVCG.2020.3030415},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {722-732},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Modeling in the time of COVID-19: Statistical and rule-based mesoscale models},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In search of patient zero: Visual analytics of pathogen
transmission pathways in hospitals. <em>TVCG</em>, <em>27</em>(2),
711–721. (<a href="https://doi.org/10.1109/TVCG.2020.3030437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathogen outbreaks (i.e., outbreaks of bacteria and viruses) in hospitals can cause high mortality rates and increase costs for hospitals significantly. An outbreak is generally noticed when the number of infected patients rises above an endemic level or the usual prevalence of a pathogen in a defined population. Reconstructing transmission pathways back to the source of an outbreak - the patient zero or index patient - requires the analysis of microbiological data and patient contacts. This is often manually completed by infection control experts. We present a novel visual analytics approach to support the analysis of transmission pathways, patient contacts, the progression of the outbreak, and patient timelines during hospitalization. Infection control experts applied our solution to a real outbreak of Klebsiella pneumoniae in a large German hospital. Using our system, our experts were able to scale the analysis of transmission pathways to longer time intervals (i.e., several years of data instead of days) and across a larger number of wards. Also, the system is able to reduce the analysis time from days to hours. In our final study, feedback from twenty-five experts from seven German hospitals provides evidence that our solution brings significant benefits for analyzing outbreaks.},
  archive      = {J_TVCG},
  author       = {T. Baumgartl and M. Petzold and M. Wunderlich and M. Hohn and D. Archambault and M. Lieser and A. Dalpke and S. Scheithauer and M. Marschollek and V. M. Eichel and N. T. Mutters and Highmed Consortium and T. Von Landesberger},
  doi          = {10.1109/TVCG.2020.3030437},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {711-721},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {In search of patient zero: Visual analytics of pathogen transmission pathways in hospitals},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visualization of human spine biomechanics for spinal
surgery. <em>TVCG</em>, <em>27</em>(2), 700–710. (<a
href="https://doi.org/10.1109/TVCG.2020.3030388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. By linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. In a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.},
  archive      = {J_TVCG},
  author       = {Pepe Eulzer and Sabine Bauer and Francis Kilian and Kai Lawonn},
  doi          = {10.1109/TVCG.2020.3030388},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {700-710},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualization of human spine biomechanics for spinal surgery},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QualDash: Adaptable generation of visualisation dashboards
for healthcare quality improvement. <em>TVCG</em>, <em>27</em>(2),
689–699. (<a href="https://doi.org/10.1109/TVCG.2020.3030424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting dashboard design to different contexts of use is an open question in visualisation research. Dashboard designers often seek to strike a balance between dashboard adaptability and ease-of-use, and in hospitals challenges arise from the vast diversity of key metrics, data models and users involved at different organizational levels. In this design study, we present QualDash, a dashboard generation engine that allows for the dynamic configuration and deployment of visualisation dashboards for healthcare quality improvement (QI). We present a rigorous task analysis based on interviews with healthcare professionals, a co-design workshop and a series of one-on-one meetings with front line analysts. From these activities we define a metric card metaphor as a unit of visual analysis in healthcare QI, using this concept as a building block for generating highly adaptable dashboards, and leading to the design of a Metric Specification Structure (MSS). Each MSS is a JSON structure which enables dashboard authors to concisely configure unit-specific variants of a metric card, while offloading common patterns that are shared across cards to be preset by the engine. We reflect on deploying and iterating the design of OualDash in cardiology wards and pediatric intensive care units of five NHS hospitals. Finally, we report evaluation results that demonstrate the adaptability, ease-of-use and usefulness of QualDash in a real-world scenario.},
  archive      = {J_TVCG},
  author       = {Mai Elshehaly and Rebecca Randell and Matthew Brehmer and Lynn McVey and Natasha Alvarado and Chris P. Gale and Roy A. Ruddle},
  doi          = {10.1109/TVCG.2020.3030424},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {689-699},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {QualDash: Adaptable generation of visualisation dashboards for healthcare quality improvement},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Humane visual AI: Telling the stories behind a medical
condition. <em>TVCG</em>, <em>27</em>(2), 678–688. (<a
href="https://doi.org/10.1109/TVCG.2020.3030391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A biological understanding is key for managing medical conditions, yet psychological and social aspects matter too. The main problem is that these two aspects are hard to quantify and inherently difficult to communicate . To quantify psychological aspects, this work mined around half a million Reddit posts in the sub-communities specialised in 14 medical conditions, and it did so with a new deep-learning framework. In so doing, it was able to associate mentions of medical conditions with those of emotions. To then quantify social aspects, this work designed a probabilistic approach that mines open prescription data from the National Health Service in England to compute the prevalence of drug prescriptions, and to relate such a prevalence to census data. To finally visually communicate each medical condition&#39;s biological, psychological, and social aspects through storytelling, we designed a narrative-style layered Martini Glass visualization. In a user study involving 52 participants, after interacting with our visualization, a considerable number of them changed their mind on previously held opinions: 10\% gave more importance to the psychological aspects of medical conditions, and 27\% were more favourable to the use of social media data in healthcare, suggesting the importance of persuasive elements in interactive visualizations.},
  archive      = {J_TVCG},
  author       = {Wonyoung So and Edyta P. Bogucka and Sanja Šćepanović and Sagar Joglekar and Ke Zhou and Daniele Quercia},
  doi          = {10.1109/TVCG.2020.3030391},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {678-688},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Humane visual AI: Telling the stories behind a medical condition},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CcNav: Understanding compiler optimizations in binary code.
<em>TVCG</em>, <em>27</em>(2), 667–677. (<a
href="https://doi.org/10.1109/TVCG.2020.3030357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.},
  archive      = {J_TVCG},
  author       = {Sabin Devkota and Pascal Aschwanden and Adam Kunen and Matthew Legendre and Katherine E. Isaacs},
  doi          = {10.1109/TVCG.2020.3030357},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {667-677},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {CcNav: Understanding compiler optimizations in binary code},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Githru: Visual analytics for understanding software
development history through git metadata analysis. <em>TVCG</em>,
<em>27</em>(2), 656–666. (<a
href="https://doi.org/10.1109/TVCG.2020.3030414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Git metadata contains rich information for developers to understand the overall context of a large software development project. Thus it can help new developers, managers, and testers understand the history of development without needing to dig into a large pile of unfamiliar source code. However, the current tools for Git visualization are not adequate to analyze and explore the metadata: They focus mainly on improving the usability of Git commands instead of on helping users understand the development history. Furthermore, they do not scale for large and complex Git commit graphs, which can play an important role in understanding the overall development history. In this paper, we present Githru, an interactive visual analytics system that enables developers to effectively understand the context of development history through the interactive exploration of Git metadata. We design an interactive visual encoding idiom to represent a large Git graph in a scalable manner while preserving the topological structures in the Git graph. To enable scalable exploration of a large Git commit graph, we propose novel techniques (graph reconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods) to abstract a large-scale Git commit graph. Based on these Git commit graph abstraction techniques, Githru provides an interactive summary view to help users gain an overview of the development history and a comparison view in which users can compare different clusters of commits. The efficacy of Githru has been demonstrated by case studies with domain experts using real-world, in-house datasets from a large software development team at a major international IT company. A controlled user study with 12 developers comparing Githru to previous tools also confirms the effectiveness of Githru in terms of task completion time.},
  archive      = {J_TVCG},
  author       = {Youngtaek Kim and Jaeyoung Kim and Hyeon Jeon and Young-Ho Kim and Hyunjoo Song and Bohyoung Kim and Jinwook Seo},
  doi          = {10.1109/TVCG.2020.3030414},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {656-666},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Githru: Visual analytics for understanding software development history through git metadata analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The mixture graph-a data structure for compressing,
rendering, and querying segmentation histograms. <em>TVCG</em>,
<em>27</em>(2), 645–655. (<a
href="https://doi.org/10.1109/TVCG.2020.3030451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel data structure, called the Mixture Graph. This data structure allows us to compress, render, and query segmentation histograms. Such histograms arise when building a mipmap of a volume containing segmentation IDs. Each voxel in the histogram mipmap contains a convex combination (mixture) of segmentation IDs. Each mixture represents the distribution of IDs in the respective voxel&#39;s children. Our method factorizes these mixtures into a series of linear interpolations between exactly two segmentation IDs. The result is represented as a directed acyclic graph (DAG) whose nodes are topologically ordered. Pruning replicate nodes in the tree followed by compression allows us to store the resulting data structure efficiently. During rendering, transfer functions are propagated from sources (leafs) through the DAG to allow for efficient, pre-filtered rendering at interactive frame rates. Assembly of histogram contributions across the footprint of a given volume allows us to efficiently query partial histograms, achieving up to 178 x speed-up over naive parallelized range queries. Additionally, we apply the Mixture Graph to compute correctly pre-filtered volume lighting and to interactively explore segments based on shape, geometry, and orientation using multi-dimensional transfer functions.},
  archive      = {J_TVCG},
  author       = {Khaled Al-Thelaya and Marco Agus and Jens Schneider},
  doi          = {10.1109/TVCG.2020.3030451},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {645-655},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The mixture graph-A data structure for compressing, rendering, and querying segmentation histograms},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Homomorphic-encrypted volume rendering. <em>TVCG</em>,
<em>27</em>(2), 635–644. (<a
href="https://doi.org/10.1109/TVCG.2020.3030436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computationally demanding tasks are typically calculated in dedicated data centers, and real-time visualizations also follow this trend. Some rendering tasks, however, require the highest level of confidentiality so that no other party, besides the owner, can read or see the sensitive data. Here we present a direct volume rendering approach that performs volume rendering directly on encrypted volume data by using the homomorphic Paillier encryption algorithm. This approach ensures that the volume data and rendered image are uninterpretable to the rendering server. Our volume rendering pipeline introduces novel approaches for encrypted-data compositing, interpolation, and opacity modulation, as well as simple transfer function design, where each of these routines maintains the highest level of privacy. We present performance and memory overhead analysis that is associated with our privacy-preserving scheme. Our approach is open and secure by design, as opposed to secure through obscurity. Owners of the data only have to keep their secure key confidential to guarantee the privacy of their volume data and the rendered images. Our work is, to our knowledge, the first privacy-preserving remote volume-rendering approach that does not require that any server involved be trustworthy; even in cases when the server is compromised, no sensitive data will be leaked to a foreign party.},
  archive      = {J_TVCG},
  author       = {Sebastian Mazza and Daniel Patel and Ivan Viola},
  doi          = {10.1109/TVCG.2020.3030436},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {635-644},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Homomorphic-encrypted volume rendering},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ray tracing structured AMR data using ExaBricks.
<em>TVCG</em>, <em>27</em>(2), 625–634. (<a
href="https://doi.org/10.1109/TVCG.2020.3030470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to adapt the domain resolution to save computation and storage, and has become one of the dominant data representations used by scientific simulations; however, efficiently rendering such data remains a challenge. We present an efficient approach for volume- and iso-surface ray tracing of Structured AMR data on GPU-equipped workstations, using a combination of two different data structures. Together, these data structures allow a ray tracing based renderer to quickly determine which segments along the ray need to be integrated and at what frequency, while also providing quick access to all data values required for a smooth sample reconstruction kernel. Our method makes use of the RTX ray tracing hardware for surface rendering, ray marching, space skipping, and adaptive sampling; and allows for interactive changes to the transfer function and implicit iso-surfacing thresholds. We demonstrate that our method achieves high performance with little memory overhead, enabling interactive high quality rendering of complex AMR data sets on individual GPU workstations.},
  archive      = {J_TVCG},
  author       = {Ingo Wald and Stefan Zellmann and Will Usher and Nate Morrical and Ulrich Lang and Valerio Pascucci},
  doi          = {10.1109/TVCG.2020.3030470},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {625-634},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Ray tracing structured AMR data using ExaBricks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advanced rendering of line data with ambient occlusion and
transparency. <em>TVCG</em>, <em>27</em>(2), 614–624. (<a
href="https://doi.org/10.1109/TVCG.2020.3028954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Lines are a widespread rendering primitive for the visualization of data from research fields like fluid dynamics or fiber tractography. Global illumination effects and transparent rendering improve the perception of three-dimensional features and decrease occlusion within the data set, thus enabling better understanding of complex line data. We present an efficient approach for high quality GPU-based rendering of line data with ambient occlusion and transparency effects. Our approach builds on GPU-based raycasting of rounded cones, which are geometric primitives similar to truncated cones, but with spherical endcaps. Object space ambient occlusion is provided by an efficient voxel cone tracing approach. Our core contribution is a new fragment visibility sorting strategy that allows for interactive visualization of line data sets with millions of line segments. We improve performance further by exploiting hierarchical opacity maps.},
  archive      = {J_TVCG},
  author       = {David Groß and Stefan Gumhold},
  doi          = {10.1109/TVCG.2020.3028954},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {614-624},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Advanced rendering of line data with ambient occlusion and transparency},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and flexible hierarchical data layouts for a
unified encoding of scalar field precision and resolution.
<em>TVCG</em>, <em>27</em>(2), 603–613. (<a
href="https://doi.org/10.1109/TVCG.2020.3030381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem of ever-growing scientific data sizes making data movement a major hindrance to analysis, we introduce a novel encoding for scalar fields: a unified tree of resolution and precision, specifically constructed so that valid cuts correspond to sensible approximations of the original field in the precision-resolution space. Furthermore, we introduce a highly flexible encoding of such trees that forms a parameterized family of data hierarchies. We discuss how different parameter choices lead to different trade-offs in practice, and show how specific choices result in known data representation schemes such as zfp[52], idx[58], and jpeg2000 [76]. Finally, we provide system-level details and empirical evidence on how such hierarchies facilitate common approximate queries with minimal data movement and time, using real-world data sets ranging from a few gigabytes to nearly a terabyte in size. Experiments suggest that our new strategy of combining reductions in resolution and precision is competitive with state-of-the-art compression techniques with respect to data quality, while being significantly more flexible and orders of magnitude faster, and requiring significantly reduced resources.},
  archive      = {J_TVCG},
  author       = {Duong Hoang and Brian Summa and Harsh Bhatia and Peter Lindstrom and Pavol Klacansky and Will Usher and Peer-Timo Bremer and Valerio Pascucci},
  doi          = {10.1109/TVCG.2020.3030381},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {603-613},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Efficient and flexible hierarchical data layouts for a unified encoding of scalar field precision and resolution},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A suggestive interface for untangling mathematical knots.
<em>TVCG</em>, <em>27</em>(2), 593–602. (<a
href="https://doi.org/10.1109/TVCG.2020.3028893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a user-friendly sketching-based suggestive interface for untangling mathematical knots with complicated structures. Rather than treating mathematical knots as if they were 3D ropes, our interface is designed to assist the user to interact with knots with the right sequence of mathematically legal moves. Our knot interface allows one to sketch and untangle knots by proposing the Reidemeister moves, and can guide the user to untangle mathematical knots to the fewest possible number of crossings by suggesting the moves needed. The system highlights parts of the knot where the Reidemeister moves are applicable, suggests the possible moves, and constrains the user&#39;s drawing to legal moves only. This ongoing suggestion is based on a Reidemeister move analyzer, that reads the evolving knot in its Gauss code and predicts the needed Reidemeister moves towards the fewest possible number of crossings. For our principal test case of mathematical knot diagrams, this for the first time permits us to visualize, analyze, and deform them in a mathematical visual interface. In addition, understanding of a fairly long mathematical deformation sequence in our interface can be aided by visual analysis and comparison over the identified “key moments” where only critical changes occur in the sequence. Our knot interface allows users to track and trace mathematical knot deformation with a significantly reduced number of visual frames containing only the Reidemeister moves being applied. All these combine to allow a much cleaner exploratory interface for us to analyze and study mathematical knots and their dynamics in topological space.},
  archive      = {J_TVCG},
  author       = {Huan Liu and Hui Zhang},
  doi          = {10.1109/TVCG.2020.3028893},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {593-602},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A suggestive interface for untangling mathematical knots},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mode surfaces of symmetric tensor fields: Topological
analysis and seamless extraction. <em>TVCG</em>, <em>27</em>(2),
583–592. (<a href="https://doi.org/10.1109/TVCG.2020.3030431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mode surfaces are the generalization of degenerate curves and neutral surfaces, which constitute 3D symmetric tensor field topology. Efficient analysis and visualization of mode surfaces can provide additional insight into not only degenerate curves and neutral surfaces, but also how these features transition into each other. Moreover, the geometry and topology of mode surfaces can help domain scientists better understand the tensor fields in their applications. Existing mode surface extraction methods can miss features in the surfaces. Moreover, the mode surfaces extracted from neighboring cells have gaps, which make their subsequent analysis difficult. In this paper, we provide novel analysis on the topological structures of mode surfaces, including a common parameterization of all mode surfaces of a tensor field using 2D asymmetric tensors. This allows us to not only better understand the structures in mode surfaces and their interactions with degenerate curves and neutral surfaces, but also develop an efficient algorithm to seamlessly extract mode surfaces, including neutral surfaces. The seamless mode surfaces enable efficient analysis of their geometric structures, such as the principal curvature directions. We apply our analysis and visualization to a number of solid mechanics data sets.},
  archive      = {J_TVCG},
  author       = {Botong Qu and Lawrence Roy and Yue Zhang and Eugene Zhang},
  doi          = {10.1109/TVCG.2020.3030431},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {583-592},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mode surfaces of symmetric tensor fields: Topological analysis and seamless extraction},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localized topological simplification of scalar data.
<em>TVCG</em>, <em>27</em>(2), 572–582. (<a
href="https://doi.org/10.1109/TVCG.2020.3030353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field f and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to f and only exhibits the selected set of extrema. Specifically, suband superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed-from a combinatorial perspective-to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70\%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to ×36. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging.},
  archive      = {J_TVCG},
  author       = {Jonas Lukasczyk and Christoph Garth and Ross Maciejewski and Julien Tierny},
  doi          = {10.1109/TVCG.2020.3030353},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {572-582},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Localized topological simplification of scalar data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TopoMap: A 0-dimensional homology preserving projection of
high-dimensional data. <em>TVCG</em>, <em>27</em>(2), 561–571. (<a
href="https://doi.org/10.1109/TVCG.2020.3030441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multidimensional Projection is a fundamental tool for high-dimensional data analytics and visualization. With very few exceptions, projection techniques are designed to map data from a high-dimensional space to a visual space so as to preserve some dissimilarity (similarity) measure, such as the Euclidean distance for example. In fact, although adopting distinct mathematical formulations designed to favor different aspects of the data, most multidimensional projection methods strive to preserve dissimilarity measures that encapsulate geometric properties such as distances or the proximity relation between data objects. However, geometric relations are not the only interesting property to be preserved in a projection. For instance, the analysis of particular structures such as clusters and outliers could be more reliably performed if the mapping process gives some guarantee as to topological invariants such as connected components and loops. This paper introduces TopoMap, a novel projection technique which provides topological guarantees during the mapping process. In particular, the proposed method performs the mapping from a high-dimensional space to a visual space, while preserving the 0-dimensional persistence diagram of the Rips filtration of the high-dimensional data, ensuring that the filtrations generate the same connected components when applied to the original as well as projected data. The presented case studies show that the topological guarantee provided by TopoMap not only brings confidence to the visual analytic process but also can be used to assist in the assessment of other projection methods.},
  archive      = {J_TVCG},
  author       = {Harish Doraiswamy and Julien Tierny and Paulo J. S. Silva and Luis Gustavo Nonato and Claudio Silva},
  doi          = {10.1109/TVCG.2020.3030441},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {561-571},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TopoMap: A 0-dimensional homology preserving projection of high-dimensional data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analytics for temporal hypergraph model exploration.
<em>TVCG</em>, <em>27</em>(2), 550–560. (<a
href="https://doi.org/10.1109/TVCG.2020.3030408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many processes, from gene interaction in biology to computer networks to social media, can be modeled more precisely as temporal hypergraphs than by regular graphs. This is because hypergraphs generalize graphs by extending edges to connect any number of vertices, allowing complex relationships to be described more accurately and predict their behavior over time. However, the interactive exploration and seamless refinement of such hypergraph-based prediction models still pose a major challenge. We contribute Hyper-Matrix, a novel visual analytics technique that addresses this challenge through a tight coupling between machine-learning and interactive visualizations. In particular, the technique incorporates a geometric deep learning model as a blueprint for problem-specific models while integrating visualizations for graph-based and category-based data with a novel combination of interactions for an effective user-driven exploration of hypergraph models. To eliminate demanding context switches and ensure scalability, our matrix-based visualization provides drill-down capabilities across multiple levels of semantic zoom, from an overview of model predictions down to the content. We facilitate a focused analysis of relevant connections and groups based on interactive user-steering for filtering and search tasks, a dynamically modifiable partition hierarchy, various matrix reordering techniques, and interactive model feedback. We evaluate our technique in a case study and through formative evaluation with law enforcement experts using real-world internet forum communication data. The results show that our approach surpasses existing solutions in terms of scalability and applicability, enables the incorporation of domain knowledge, and allows for fast search-space traversal. With the proposed technique, we pave the way for the visual analytics of temporal hypergraphs in a wide variety of domains.},
  archive      = {J_TVCG},
  author       = {Maximilian T. Fischer and Devanshu Arya and Dirk Streeb and Daniel Seebacher and Daniel A. Keim and Marcel Worring},
  doi          = {10.1109/TVCG.2020.3030408},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {550-560},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analytics for temporal hypergraph model exploration},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Staged animation strategies for online dynamic networks.
<em>TVCG</em>, <em>27</em>(2), 539–549. (<a
href="https://doi.org/10.1109/TVCG.2020.3030385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks-networks that change over time-can be categorized into two types: offline dynamic networks, where all states of the network are known, and online dynamic networks, where only the past states of the network are known. Research on staging animated transitions in dynamic networks has focused more on offline data, where rendering strategies can take into account past and future states of the network. Rendering online dynamic networks is a more challenging problem since it requires a balance between timeliness for monitoring tasks-so that the animations do not lag too far behind the events-and clarity for comprehension tasks-to minimize simultaneous changes that may be difficult to follow. To illustrate the challenges placed by these requirements, we explore three strategies to stage animations for online dynamic networks: time-based, event-based, and a new hybrid approach that we introduce by combining the advantages of the first two. We illustrate the advantages and disadvantages of each strategy in representing low- and high-throughput data and conduct a user study involving monitoring and comprehension of dynamic networks. We also conduct a follow-up, think-aloud study combining monitoring and comprehension with experts in dynamic network visualization. Our findings show that animation staging strategies that emphasize comprehension do better for participant response times and accuracy. However, the notion of “comprehension” is not always clear when it comes to complex changes in highly dynamic networks, requiring some iteration in staging that the hybrid approach affords. Based on our results, we make recommendations for balancing event-based and time-based parameters for our hybrid approach.},
  archive      = {J_TVCG},
  author       = {Tarik Crnovrsanin and Shilpika and Senthil Chandrasegaran and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2020.3030385},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {539-549},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Staged animation strategies for online dynamic networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The effectiveness of interactive visualization techniques
for time navigation of dynamic graphs on large displays. <em>TVCG</em>,
<em>27</em>(2), 528–538. (<a
href="https://doi.org/10.1109/TVCG.2020.3030446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks can be challenging to analyze visually, especially if they span a large time range during which new nodes and edges can appear and disappear. Although it is straightforward to provide interfaces for visualization that represent multiple states of the network (i.e., multiple timeslices) either simultaneously (e.g., through small multiples) or interactively (e.g., through interactive animation), these interfaces might not support tasks in which disjoint timeslices need to be compared. Since these tasks are key for understanding the dynamic aspects of the network, understanding which interactive visualizations best support these tasks is important. We present the results of a series of laboratory experiments comparing two traditional approaches (small multiples and interactive animation), with a more recent approach based on interactive timeslicing. The tasks were performed on a large display through a touch interface. Participants completed 24 trials of three tasks with all techniques. The results show that interactive timeslicing brings benefit when comparing distant points in time, but less benefits when analyzing contiguous intervals of time.},
  archive      = {J_TVCG},
  author       = {Alexandra Lee and Daniel Archambault and Miguel A. Nacenta},
  doi          = {10.1109/TVCG.2020.3030446},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {528-538},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effectiveness of interactive visualization techniques for time navigation of dynamic graphs on large displays},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale snapshots: Visual analysis of temporal summaries
in dynamic graphs. <em>TVCG</em>, <em>27</em>(2), 517–527. (<a
href="https://doi.org/10.1109/TVCG.2020.3030398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.},
  archive      = {J_TVCG},
  author       = {Eren Cakmak and Udo Schlegel and Dominik Jäckle and Daniel Keim and Tobias Schreck},
  doi          = {10.1109/TVCG.2020.3030398},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {517-527},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multiscale snapshots: Visual analysis of temporal summaries in dynamic graphs},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visual analytics approach for ecosystem dynamics based on
empirical dynamic modeling. <em>TVCG</em>, <em>27</em>(2), 506–516. (<a
href="https://doi.org/10.1109/TVCG.2020.3028956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important approach for scientific inquiry across many disciplines involves using observational time series data to understand the relationships between key variables to gain mechanistic insights into the underlying rules that govern the given system. In real systems, such as those found in ecology, the relationships between time series variables are generally not static; instead, these relationships are dynamical and change in a nonlinear or state-dependent manner. To further understand such systems, we investigate integrating methods that appropriately characterize these dynamics (i.e., methods that measure interactions as they change with time-varying system states) with visualization techniques that can help analyze the behavior of the system. Here, we focus on empirical dynamic modeling (EDM) as a state-of-the-art method that specifically identifies causal variables and measures changing state-dependent relationships between time series variables. Instead of using approaches centered on parametric equations, EDM is an equation-free approach that studies systems based on their dynamic attractors. We propose a visual analytics system to support the identification and mechanistic interpretation of system states using an EDM-constructed dynamic graph. This work, as detailed in four analysis tasks and demonstrated with a GUI, provides a novel synthesis of EDM and visualization techniques such as brush-link visualization and visual summarization to interpret dynamic graphs representing ecosystem dynamics. We applied our proposed system to ecological simulation data and real data from a marine mesocosm study as two key use cases. Our case studies show that our visual analytics tools support the identification and interpretation of the system state by the user, and enable us to discover both confirmatory and new findings in ecosystem dynamics. Overall, we demonstrated that our system can facilitate an understanding of how systems function beyond the intuitive analysis of high-dimensional information based on specific domain knowledge.},
  archive      = {J_TVCG},
  author       = {Hiroaki Natsukawa and Ethan R. Deyle and Gerald M. Pao and Koji Koyamada and George Sugihara},
  doi          = {10.1109/TVCG.2020.3028956},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {506-516},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A visual analytics approach for ecosystem dynamics based on empirical dynamic modeling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VizCommender: Computing text-based similarity in
visualization repositories for content-based recommendations.
<em>TVCG</em>, <em>27</em>(2), 495–505. (<a
href="https://doi.org/10.1109/TVCG.2020.3030387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.},
  archive      = {J_TVCG},
  author       = {Michael Oppermann and Robert Kincaid and Tamara Munzner},
  doi          = {10.1109/TVCG.2020.3030387},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {495-505},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VizCommender: Computing text-based similarity in visualization repositories for content-based recommendations},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gemini: A grammar and recommender system for animated
transitions in statistical graphics. <em>TVCG</em>, <em>27</em>(2),
485–494. (<a href="https://doi.org/10.1109/TVCG.2020.3030360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animated transitions help viewers follow changes between related visualizations. Specifying effective animations demands significant effort: authors must select the elements and properties to animate, provide transition parameters, and coordinate the timing of stages. To facilitate this process, we present Gemini, a declarative grammar and recommendation system for animated transitions between single-view statistical graphics. Gemini specifications define transition “steps” in terms of high-level visual components (marks, axes, legends) and composition rules to synchronize and concatenate steps. With this grammar, Gemini can recommend animation designs to augment and accelerate designers&#39; work. Gemini enumerates staged animation designs for given start and end states, and ranks those designs using a cost function informed by prior perceptual studies. To evaluate Gemini, we conduct both a formative study on Mechanical Turk to assess and tune our ranking function, and a summative study in which 8 experienced visualization developers implement animations in D3 that we then compare to Gemini&#39;s suggestions. We find that most designs (9/11) are exactly replicable in Gemini, with many (8/11) achievable via edits to suggestions, and that Gemini suggestions avoid multiple participant errors.},
  archive      = {J_TVCG},
  author       = {Younghoon Kim and Jeffrey Heer},
  doi          = {10.1109/TVCG.2020.3030360},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {485-494},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Gemini: A grammar and recommender system for animated transitions in statistical graphics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Palettailor: Discriminable colorization for categorical
data. <em>TVCG</em>, <em>27</em>(2), 475–484. (<a
href="https://doi.org/10.1109/TVCG.2020.3030406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.},
  archive      = {J_TVCG},
  author       = {Kecheng Lu and Mi Feng and Xin Chen and Michael Sedlmair and Oliver Deussen and Dani Lischinski and Zhanglin Cheng and Yunhai Wang},
  doi          = {10.1109/TVCG.2020.3030406},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {475-484},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Palettailor: Discriminable colorization for categorical data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MobileVisFixer: Tailoring web visualizations for mobile
phones leveraging an explainable reinforcement learning framework.
<em>TVCG</em>, <em>27</em>(2), 464–474. (<a
href="https://doi.org/10.1109/TVCG.2020.3030423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.},
  archive      = {J_TVCG},
  author       = {Aoyu Wu and Wai Tong and Tim Dwyer and Bongshin Lee and Petra Isenberg and Huamin Qu},
  doi          = {10.1109/TVCG.2020.3030423},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {464-474},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MobileVisFixer: Tailoring web visualizations for mobile phones leveraging an explainable reinforcement learning framework},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calliope: Automatic visual data story generation from a
spreadsheet. <em>TVCG</em>, <em>27</em>(2), 453–463. (<a
href="https://doi.org/10.1109/TVCG.2020.3030403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users&#39; skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.},
  archive      = {J_TVCG},
  author       = {Danqing Shi and Xinyue Xu and Fuling Sun and Yang Shi and Nan Cao},
  doi          = {10.1109/TVCG.2020.3030403},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {453-463},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Calliope: Automatic visual data story generation from a spreadsheet},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Retrieve-then-adapt: Example-based automatic generation for
proportion-related infographics. <em>TVCG</em>, <em>27</em>(2), 443–452.
(<a href="https://doi.org/10.1109/TVCG.2020.3030448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.},
  archive      = {J_TVCG},
  author       = {Chunyao Qian and Shizhao Sun and Weiwei Cui and Jian-Guang Lou and Haidong Zhang and Dongmei Zhang},
  doi          = {10.1109/TVCG.2020.3030448},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {443-452},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Retrieve-then-adapt: Example-based automatic generation for proportion-related infographics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Argus: Interactive a priori power analysis. <em>TVCG</em>,
<em>27</em>(2), 432–442. (<a
href="https://doi.org/10.1109/TVCG.2020.3028894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge HCl researchers face when designing a controlled experiment is choosing the appropriate number of participants, or sample size. A priori power analysis examines the relationships among multiple parameters, including the complexity associated with human participants, e.g., order and fatigue effects, to calculate the statistical power of a given experiment design. We created Argus, a tool that supports interactive exploration of statistical power: Researchers specify experiment design scenarios with varying confounds and effect sizes. Argus then simulates data and visualizes statistical power across these scenarios, which lets researchers interactively weigh various trade-offs and make informed decisions about sample size. We describe the design and implementation of Argus, a usage scenario designing a visualization experiment, and a think-aloud study.},
  archive      = {J_TVCG},
  author       = {Xiaoyi Wang and Alexander Eiselmayer and Wendy E. Mackay and Kasper Hornbaek and Chat Wacharamanotham},
  doi          = {10.1109/TVCG.2020.3028894},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {432-442},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Argus: Interactive a priori power analysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). II-20: Intelligent and pragmatic analytic categorization of
image collections. <em>TVCG</em>, <em>27</em>(2), 422–431. (<a
href="https://doi.org/10.1109/TVCG.2020.3030383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce 11-20 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11-20 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user&#39;s interactions and dynamically models her categories of relevance. II-20&#39;s machine model, in addition to matching and exceeding the state of the art&#39;s ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (“fast-forward”) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20&#39;s machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor&#39;s analytic quality. User studies further confirm that II-20 is an intuitive, efficient, and effective multimedia analytics tool.},
  archive      = {J_TVCG},
  author       = {Jan Zahálka and Marcel Worring and Jarke J. Van Wijk},
  doi          = {10.1109/TVCG.2020.3030383},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {422-431},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {II-20: Intelligent and pragmatic analytic categorization of image collections},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Competing models: Inferring exploration patterns and
information relevance via bayesian model selection. <em>TVCG</em>,
<em>27</em>(2), 412–421. (<a
href="https://doi.org/10.1109/TVCG.2020.3030430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique&#39;s ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users&#39; goals and adapt to improve the exploration process.},
  archive      = {J_TVCG},
  author       = {Shayan Monadjemi and Roman Garnett and Alvitta Ottley},
  doi          = {10.1109/TVCG.2020.3030430},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {412-421},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Competing models: Inferring exploration patterns and information relevance via bayesian model selection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kyrix-s: Authoring scalable scatterplot visualizations of
big data. <em>TVCG</em>, <em>27</em>(2), 401–411. (<a
href="https://doi.org/10.1109/TVCG.2020.3030372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static scatterplots often suffer from the overdraw problem on big datasets where object overlap causes undesirable visual clutter. The use of zooming in scatterplots can help alleviate this problem. With multiple zoom levels, more screen real estate is available, allowing objects to be placed in a less crowded way. We call this type of visualization scalable scatterplot visualizations, or SSV for short. Despite the potential of SSVs, existing systems and toolkits fall short in supporting the authoring of SSVs due to three limitations. First, many systems have limited scalability, assuming that data fits in the memory of one computer. Second, too much developer work, e.g., using custom code to generate mark layouts or render objects, is required. Third, many systems focus on only a small subset of the SSV design space (e.g. supporting a specific type of visual marks). To address these limitations, we have developed Kyrix-S, a system for easy authoring of SSVs at scale. Kyrix-S derives a declarative grammar that enables specification of a variety of SSVs in a few tens of lines of code, based on an existing survey of scatterplot tasks and designs. The declarative grammar is supported by a distributed layout algorithm which automatically places visual marks onto zoom levels. We store data in a multi-node database and use multi-node spatial indexes to achieve interactive browsing of large SSVs. Extensive experiments show that 1) Kyrix-S enables interactive browsing of SSVs of billions of objects, with response times under 500ms and 2) Kyrix-S achieves 4X-9X reduction in specification compared to a state-of-the-art authoring system.},
  archive      = {J_TVCG},
  author       = {Wenbo Tao and Xinli Hou and Adam Sah and Leilani Battle and Remco Chang and Michael Stonebraker},
  doi          = {10.1109/TVCG.2020.3030372},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {401-411},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Kyrix-S: Authoring scalable scatterplot visualizations of big data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PipelineProfiler: A visual analytics tool for the
exploration of AutoML pipelines. <em>TVCG</em>, <em>27</em>(2), 390–400.
(<a href="https://doi.org/10.1109/TVCG.2020.3030361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.},
  archive      = {J_TVCG},
  author       = {Jorge Piazentin Ono and Sonia Castelo and Roque Lopez and Enrico Bertini and Juliana Freire and Claudio Silva},
  doi          = {10.1109/TVCG.2020.3030361},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {390-400},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PipelineProfiler: A visual analytics tool for the exploration of AutoML pipelines},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). P6: A declarative language for integrating machine learning
in visual analytics. <em>TVCG</em>, <em>27</em>(2), 380–389. (<a
href="https://doi.org/10.1109/TVCG.2020.3030453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6&#39;s capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.},
  archive      = {J_TVCG},
  author       = {Jianping Kelvin Li and Kwan-Liu Ma},
  doi          = {10.1109/TVCG.2020.3030453},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {380-389},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {P6: A declarative language for integrating machine learning in visual analytics},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NL4DV: A toolkit for generating analytic specifications for
data visualization from natural language queries. <em>TVCG</em>,
<em>27</em>(2), 369–379. (<a
href="https://doi.org/10.1109/TVCG.2020.3030378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV&#39;s usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.},
  archive      = {J_TVCG},
  author       = {Arpit Narechania and Arjun Srinivasan and John Stasko},
  doi          = {10.1109/TVCG.2020.3030378},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {369-379},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NL4DV: A toolkit for generating analytic specifications for data visualization from natural language queries},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generic framework and library for exploration of small
multiples through interactive piling. <em>TVCG</em>, <em>27</em>(2),
358–368. (<a href="https://doi.org/10.1109/TVCG.2020.3028948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small multiples are miniature representations of visual information used generically across many domains. Handling large numbers of small multiples imposes challenges on many analytic tasks like inspection, comparison, navigation, or annotation. To address these challenges, we developed a framework and implemented a library called PILlNG.JS for designing interactive piling interfaces. Based on the piling metaphor, such interfaces afford flexible organization, exploration, and comparison of large numbers of small multiples by interactively aggregating visual objects into piles. Based on a systematic analysis of previous work, we present a structured design space to guide the design of visual piling interfaces. To enable designers to efficiently build their own visual piling interfaces, PILlNG.JS provides a declarative interface to avoid having to write low-level code and implements common aspects of the design space. An accompanying GUI additionally supports the dynamic configuration of the piling interface. We demonstrate the expressiveness of PILlNG.JS with examples from machine learning, immunofluorescence microscopy, genomics, and public health.},
  archive      = {J_TVCG},
  author       = {Fritz Lekschas and Xinyi Zhou and Wei Chen and Nils Gehlenborg and Benjamin Bach and Hanspeter Pfister},
  doi          = {10.1109/TVCG.2020.3028948},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {358-368},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A generic framework and library for exploration of small multiples through interactive piling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VisConnect: Distributed event synchronization for
collaborative visualization. <em>TVCG</em>, <em>27</em>(2), 347–357. (<a
href="https://doi.org/10.1109/TVCG.2020.3030366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tools and interfaces are increasingly expected to be synchronous and distributed to accommodate remote collaboration. Yet, adoption of these techniques for data visualization is low partly because development is difficult: existing collaboration software systems either do not support simultaneous interaction or require expensive redevelopment of existing visualizations. We contribute VisConnect: a web-based synchronous distributed collaborative visualization system that supports most web-based SVG data visualizations, balances system safety with responsiveness, and supports simultaneous interaction from many collaborators. VisConnect works with existing visualization implementations with little-to-no code changes by synchronizing low-level JavaScript events across clients such that visualization updates proceed transparently across clients. This is accomplished via a peer-to-peer system that establishes consensus among clients on the per-element sequence of events, and uses a lock service to grant access over elements to clients. We contribute collaborative extensions of traditional visualization interaction techniques, such as drag, brush, and lasso, and discuss different strategies for collaborative visualization interactions. To demonstrate the utility of VisConnect, we present novel examples of collaborative visualizations in the healthcare domain, remote collaboration with annotation, and show in an education case study for e-learning with 22 participants that students found the ability to remotely collaborate on class activities helpful and enjoyable for understanding concepts. A free copy of this paper and source code are available on OSF at osf.io/ut7e6 and at visconnect.us.},
  archive      = {J_TVCG},
  author       = {Michail Schwab and David Saffo and Yixuan Zhang and Shash Sinha and Cristina Nita-Rotaru and James Tompkin and Cody Dunne and Michelle A. Borkin},
  doi          = {10.1109/TVCG.2020.3030366},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {347-357},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisConnect: Distributed event synchronization for collaborative visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Chartem: Reviving chart images with data embedding.
<em>TVCG</em>, <em>27</em>(2), 337–346. (<a
href="https://doi.org/10.1109/TVCG.2020.3030351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires creating a completely new chart, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data-embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.},
  archive      = {J_TVCG},
  author       = {Jiayun Fu and Bin Zhu and Weiwei Cui and Song Ge and Yun Wang and Haidong Zhang and He Huang and Yuanyuan Tang and Dongmei Zhang and Xiaojing Ma},
  doi          = {10.1109/TVCG.2020.3030351},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {337-346},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Chartem: Reviving chart images with data embedding},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VisCode: Embedding information in visualization images using
encoder-decoder network. <em>TVCG</em>, <em>27</em>(2), 326–336. (<a
href="https://doi.org/10.1109/TVCG.2020.3030343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.},
  archive      = {J_TVCG},
  author       = {Peiying Zhang and Chenhui Li and Changbo Wang},
  doi          = {10.1109/TVCG.2020.3030343},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {326-336},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VisCode: Embedding information in visualization images using encoder-decoder network},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StructGraphics: Flexible visualization design through
data-agnostic and reusable graphical structures. <em>TVCG</em>,
<em>27</em>(2), 315–325. (<a
href="https://doi.org/10.1109/TVCG.2020.3030476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information visualization research has developed powerful systems that enable users to author custom data visualizations without textual programming. These systems can support graphics-driven practices by bridging lazy data-binding mechanisms with vector-graphics editing tools. Yet, despite their expressive power, visualization authoring systems often assume that users want to generate visual representations that they already have in mind rather than explore designs. They also impose a data-to-graphics workflow, where binding data dimensions to graphical properties is a necessary step for generating visualization layouts. In this paper, we introduce StructGraphics, an approach for creating data-agnostic and fully reusable visualization designs. StructGraphics enables designers to construct visualization designs by drawing graphics on a canvas and then structuring their visual properties without relying on a concrete dataset or data schema. In StructGraphics, tabular data structures are derived directly from the structure of the graphics. Later, designers can link these structures with real datasets through a spreadsheet user interface. StructGraphics supports the design and reuse of complex data visualizations by combining graphical property sharing, by-example design specification, and persistent layout constraints. We demonstrate the power of the approach through a gallery of visualization examples and reflect on its strengths and limitations in interaction with graphic designers and data visualization experts.},
  archive      = {J_TVCG},
  author       = {Theophanis Tsandilas},
  doi          = {10.1109/TVCG.2020.3030476},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {315-325},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {StructGraphics: Flexible visualization design through data-agnostic and reusable graphical structures},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lyra 2: Designing interactive visualizations by
demonstration. <em>TVCG</em>, <em>27</em>(2), 304–314. (<a
href="https://doi.org/10.1109/TVCG.2020.3030367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.},
  archive      = {J_TVCG},
  author       = {Jonathan Zong and Dhiraj Barnwal and Rupayan Neogy and Arvind Satyanarayan},
  doi          = {10.1109/TVCG.2020.3030367},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {304-314},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Lyra 2: Designing interactive visualizations by demonstration},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PlotThread: Creating expressive storyline visualizations
using reinforcement learning. <em>TVCG</em>, <em>27</em>(2), 294–303.
(<a href="https://doi.org/10.1109/TVCG.2020.3030467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.},
  archive      = {J_TVCG},
  author       = {Tan Tang and Renzhong Li and Xinke Wu and Shuhan Liu and Johannes Knittel and Steffen Koch and Thomas Ertl and Lingyun Yu and Peiran Ren and Yingcai Wu},
  doi          = {10.1109/TVCG.2020.3030467},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {294-303},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PlotThread: Creating expressive storyline visualizations using reinforcement learning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Objective observer-relative flow visualization in curved
spaces for unsteady 2D geophysical flows. <em>TVCG</em>, <em>27</em>(2),
283–293. (<a href="https://doi.org/10.1109/TVCG.2020.3030454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.},
  archive      = {J_TVCG},
  author       = {Peter Rautek and Matej Mlejnek and Johanna Beyer and Jakob Troidl and Hanspeter Pfister and Thomas Theußl and Markus Hadwiger},
  doi          = {10.1109/TVCG.2020.3030454},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {283-293},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Objective observer-relative flow visualization in curved spaces for unsteady 2D geophysical flows},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual reasoning strategies for effect size judgments and
decisions. <em>TVCG</em>, <em>27</em>(2), 272–282. (<a
href="https://doi.org/10.1109/TVCG.2020.3030335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95\% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user&#39;s sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users&#39; strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.},
  archive      = {J_TVCG},
  author       = {Alex Kale and Matthew Kay and Jessica Hullman},
  doi          = {10.1109/TVCG.2020.3030335},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {272-282},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual reasoning strategies for effect size judgments and decisions},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VATLD: A visual analytics system to assess, understand and
improve traffic light detection. <em>TVCG</em>, <em>27</em>(2), 261–271.
(<a href="https://doi.org/10.1109/TVCG.2020.3030350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic light detection is crucial for environment perception and decision-making in autonomous driving. State-of-the-art detectors are built upon deep Convolutional Neural Networks (CNNs) and have exhibited promising performance. However, one looming concern with CNN based detectors is how to thoroughly evaluate the performance of accuracy and robustness before they can be deployed to autonomous vehicles. In this work, we propose a visual analytics system, VATLD, equipped with a disentangled representation learning and semantic adversarial learning, to assess, understand, and improve the accuracy and robustness of traffic light detectors in autonomous driving applications. The disentangled representation learning extracts data semantics to augment human cognition with human-friendly visual summarization, and the semantic adversarial learning efficiently exposes interpretable robustness risks and enables minimal human interaction for actionable insights. We also demonstrate the effectiveness of various performance improvement strategies derived from actionable insights with our visual analytics system, VATLD, and illustrate some practical implications for safety-critical applications in autonomous driving.},
  archive      = {J_TVCG},
  author       = {Liang Gou and Lincan Zou and Nanxiang Li and Michael Hofmann and Arvind Kumar Shekar and Axel Wendt and Liu Ren},
  doi          = {10.1109/TVCG.2020.3030350},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {2},
  pages        = {261-271},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VATLD: A visual analytics system to assess, understand and improve traffic light detection},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable mesh refinement for canonical polygonal schemas of
extremely high genus shapes. <em>TVCG</em>, <em>27</em>(1), 254–260. (<a
href="https://doi.org/10.1109/TVCG.2020.3010736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Any closed manifold of genus g can be cut open to form a topological disk and then mapped to a regular polygon with 4g sides. This construction is called the canonical polygonal schema of the manifold, and is a key ingredient for many applications in graphics and engineering, where a parameterization between two shapes with same topology is often needed. The sides of the 4g-gon define on the manifold a system of loops, which all intersect at a single point and are disjoint elsewhere. Computing a shortest system of loops of this kind is NP-hard. A computationally tractable alternative consists of computing a set of shortest loops that are not fully disjoint in polynomial time using the greedy homotopy basis algorithm proposed by Erickson and Whittlesey and then detach them in post processing via mesh refinement. Despite this operation is conceptually simple, known refinement strategies do not scale well for high genus shapes, triggering a mesh growth that may exceed the amount of memory available in modern computers, leading to failures. In this article we study various local refinement operators to detach cycles in a system of loops, and show that there are important differences between them, both in terms of mesh complexity and preservation of the original surface. We ultimately propose two novel refinement approaches: the former greatly reduces the number of new elements in the mesh, possibly at the cost of a deviation from the input geometry. The latter allows to trade mesh complexity for geometric accuracy, bounding deviation from the input surface. Both strategies are trivial to implement, and experiments confirm that they allow to realize canonical polygonal schemas even for extremely high genus shapes where previous methods fail.},
  archive      = {J_TVCG},
  author       = {Marco Livesu},
  doi          = {10.1109/TVCG.2020.3010736},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {254-260},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scalable mesh refinement for canonical polygonal schemas of extremely high genus shapes},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual analysis of class separations with locally linear
segments. <em>TVCG</em>, <em>27</em>(1), 241–253. (<a
href="https://doi.org/10.1109/TVCG.2020.3011155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional labeled data widely exists in many real-world applications such as classification and clustering. One main task in analyzing such datasets is to explore class separations and class boundaries derived from machine learning models. Dimension reduction techniques are commonly applied to support analysts in exploring the underlying decision boundary structures by depicting a low-dimensional representation of the data distributions from multiple classes. However, such projection-based analyses are limited due to their lack of ability to show separations in complex non-linear decision boundary structures and can suffer from heavy distortion and low interpretability. To overcome these issues of separability and interpretability, we propose a visual analysis approach that utilizes the power of explainability from linear projections to support analysts when exploring non-linear separation structures. Our approach is to extract a set of locally linear segments that approximate the original non-linear separations. Unlike traditional projection-based analysis where the data instances are mapped to a single scatterplot, our approach supports the exploration of complex class separations through multiple local projection results. We conduct case studies on two labeled datasets to demonstrate the effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Yuxin Ma and Ross Maciejewski},
  doi          = {10.1109/TVCG.2020.3011155},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {241-253},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual analysis of class separations with locally linear segments},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vectorized painting with temporal diffusion curves.
<em>TVCG</em>, <em>27</em>(1), 228–240. (<a
href="https://doi.org/10.1109/TVCG.2019.2929808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a vector painting system for digital artworks. We first propose Temporal Diffusion Curve (TDC), a new form of vector graphics, and a novel random-access solver for modeling the evolution of strokes. With the help of a procedural stroke processing function, the TDC strokes can achieve various shapes and effects for multiple art styles. Based on these, we build a painting system of great potential. Thanks to the random-access solver, our method has real-time performance regardless of the rendering resolution, provides straightforward editing possibilities on strokes both at runtime and afterward, and is effective and straightforward for art production. Compared with the previous Diffusion Curve, our method uses strokes as the basic graphics primitives, which are able to intersect each other and much more consistent with the intuition and painting habits of human. We finally demonstrate that professional artists can create multiple genres of artworks with our painting system.},
  archive      = {J_TVCG},
  author       = {Yingjia Li and Xiao Zhai and Fei Hou and Yawen Liu and Aimin Hao and Hong Qin},
  doi          = {10.1109/TVCG.2019.2929808},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {228-240},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Vectorized painting with temporal diffusion curves},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal manifold learning for human motions via
long-horizon modeling. <em>TVCG</em>, <em>27</em>(1), 216–227. (<a
href="https://doi.org/10.1109/TVCG.2019.2936810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. Recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. However, previous deep learning methods can be sub-optimal for two reasons. First, the skeletal information has not been fully utilized for feature extraction. Unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. Second, motion is time-series data with strong multi-modal temporal correlations between frames. On the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning are correlated with a number of frames later. Ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. In this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. The network has a new spatial component for feature extraction. It is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. With our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. It can also be used for denoising corrupted motions and synthesizing new motions with given control signals. We demonstrate that our system can create superior results comparing to existing work in multiple applications.},
  archive      = {J_TVCG},
  author       = {He Wang and Edmond S. L. Ho and Hubert P. H. Shum and Zhanxing Zhu},
  doi          = {10.1109/TVCG.2019.2936810},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {216-227},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spatio-temporal manifold learning for human motions via long-horizon modeling},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating balance recovery techniques for users wearing
head-mounted display in VR. <em>TVCG</em>, <em>27</em>(1), 204–215. (<a
href="https://doi.org/10.1109/TVCG.2019.2927477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Room-scale 3D position tracking enables users to explore a virtual environment by physically walking, which improves comfort and the level of immersion. However, when users walk with their eyesight blocked by a head-mounted display, they may unexpectedly lose their balance and fall if they bump into real-world obstacles or unintentionally shift their center of mass outside the margin of stability. This paper evaluates balance recovery methods and intervention timing during the use of VR with the assumption that the onset of a fall is given. Our experiment followed the tether-release protocol during clinical research and induced a fall while a subject was engaged in a secondary 3D object selection task. The experiment employed a two-by-two design that evaluated two assistive techniques, i.e., video-see-through and auditory warning at two different timings, i.e., at fall onset and 500ms prior to fall onset. The data from 17 subjects showed that video-see-through triggered 500 ms before the onset of fall can effectively help users recover from falls. Surprisingly, video-see-through at fall onset has a significant negative impact on balance recovery and produces similar results to those of the baseline condition (no intervention).},
  archive      = {J_TVCG},
  author       = {Carlos A. Tirado Cortes and Hsiang-Ting Chen and Daina L. Sturnieks and Jaime Garcia and Stephen R. Lord and Chin-Teng Lin},
  doi          = {10.1109/TVCG.2019.2927477},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {204-215},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating balance recovery techniques for users wearing head-mounted display in VR},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Realtime and accurate 3D eye gaze capture with DCNN-based
iris and pupil segmentation. <em>TVCG</em>, <em>27</em>(1), 190–203. (<a
href="https://doi.org/10.1109/TVCG.2019.2938165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a realtime and accurate method for 3D eye gaze tracking with a monocular RGB camera. Our key idea is to train a deep convolutional neural network(DCNN) that automatically extracts the iris and pupil pixels of each eye from input images. To achieve this goal, we combine the power of Unet [1] and Squeezenet [2] to train an efficient convolutional neural network for pixel classification. In addition, we track the 3D eye gaze state in the Maximum A Posteriori (MAP) framework, which sequentially searches for the most likely state of the 3D eye gaze at each frame. When eye blinking occurs, the eye gaze tracker can obtain an inaccurate result. We further extend the convolutional neural network for eye close detection in order to improve the robustness and accuracy of the eye gaze tracker. Our system runs in realtime on desktop PCs and smart phones. We have evaluated our system on live videos and Internet videos, and our results demonstrate that the system is robust and accurate for various genders, races, lighting conditions, poses, shapes and facial expressions. A comparison against Wang et al. [3] shows that our method advances the state of the art in 3D eye tracking using a single RGB camera.},
  archive      = {J_TVCG},
  author       = {Zhiyong Wang and Jinxiang Chai and Shihong Xia},
  doi          = {10.1109/TVCG.2019.2938165},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {190-203},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Realtime and accurate 3D eye gaze capture with DCNN-based iris and pupil segmentation},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual-aware sketch simplification based on integrated
VGG layers. <em>TVCG</em>, <em>27</em>(1), 178–189. (<a
href="https://doi.org/10.1109/TVCG.2019.2930512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been recently demonstrated as an effective tool for raster-based sketch simplification. Nevertheless, it remains challenging to simplify extremely rough sketches. We found that a simplification network trained with a simple loss, such as pixel loss or discriminator loss, may fail to retain the semantically meaningful details when simplifying a very sketchy and complicated drawing. In this paper, we show that, with a well-designed multi-layer perceptual loss, we are able to obtain aesthetic and neat simplification results preserving semantically important global structures as well as fine details without blurriness and excessive emphasis on local structures. To do so, we design a multi-layer discriminator by fusing all VGG feature layers to differentiate sketches and clean lines. The weights used in layer fusing are automatically learned via an intelligent adjustment mechanism. Furthermore, to evaluate our method, we compare our method to state-of-the-art methods through multiple experiments, including visual comparison and intensive user study.},
  archive      = {J_TVCG},
  author       = {Xuemiao Xu and Minshan Xie and Peiqi Miao and Wei Qu and Wenpeng Xiao and Huaidong Zhang and Xueting Liu and Tien-Tsin Wong},
  doi          = {10.1109/TVCG.2019.2930512},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {178-189},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual-aware sketch simplification based on integrated VGG layers},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NaviBoard and NaviChair: Limited translation combined with
full rotation for efficient virtual locomotion. <em>TVCG</em>,
<em>27</em>(1), 165–177. (<a
href="https://doi.org/10.1109/TVCG.2019.2935730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Walking has always been considered as the gold standard for navigation in Virtual Reality research. Though full rotation is no longer a technical challenge, physical translation is still restricted through limited tracked areas. While rotational information has been shown to be important, the benefit of the translational component is still unclear with mixed results in previous work. To address this gap, we conducted a mixed-method experiment to compare four levels of translational cues and control: none (using the trackpad of the HTC Vive controller to translate), upper-body leaning (sitting on a “NaviChair”, leaning the upper-body to locomote), whole-body leaning/stepping (standing on a platform called NaviBoard, leaning the whole body or stepping one foot off the center to navigate), and full translation (physically walking). Results showed that translational cues and control had significant effects on various measures including task performance, task load, and simulator sickness. While participants performed significantly worse when they used a controller with no embodied translational cues, there was no significant difference between the NaviChair, NaviBoard, and actual walking. These results suggest that translational body-based motion cues and control from a low-cost leaning/stepping interface might provide enough sensory information for supporting spatial updating, spatial awareness, and efficient locomotion in VR, although future work will need to investigate how these results might or might not generalize to other tasks and scenarios.},
  archive      = {J_TVCG},
  author       = {Thinh Nguyen-Vo and Bernhard E. Riecke and Wolfgang Stuerzlinger and Duc-Minh Pham and Ernst Kruijff},
  doi          = {10.1109/TVCG.2019.2935730},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {165-177},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NaviBoard and NaviChair: Limited translation combined with full rotation for efficient virtual locomotion},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mesh saliency via weakly supervised
classification-for-saliency CNN. <em>TVCG</em>, <em>27</em>(1), 151–164.
(<a href="https://doi.org/10.1109/TVCG.2019.2928794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, effort has been made to apply deep learning to the detection of mesh saliency. However, one major barrier is to collect a large amount of vertex-level annotation as saliency ground truth for training the neural networks. Quite a few pilot studies showed that this task is difficult. In this work, we solve this problem by developing a novel network trained in a weakly supervised manner. The training is end-to-end and does not require any saliency ground truth but only the class membership of meshes. Our Classification-for-Saliency CNN (CfS-CNN) employs a multi-view setup and contains a newly designed two-channel structure which integrates view-based features of both classification and saliency. It essentially transfers knowledge from 3D object classification to mesh saliency. Our approach significantly outperforms the existing state-of-the-art methods according to extensive experimental results. Also, the CfS-CNN can be directly used for scene saliency. We showcase two novel applications based on scene saliency to demonstrate its utility.},
  archive      = {J_TVCG},
  author       = {Ran Song and Yonghuai Liu and Paul L. Rosin},
  doi          = {10.1109/TVCG.2019.2928794},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {151-164},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mesh saliency via weakly supervised classification-for-saliency CNN},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanics-aware modeling of cloth appearance. <em>TVCG</em>,
<em>27</em>(1), 137–150. (<a
href="https://doi.org/10.1109/TVCG.2019.2937301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-appearance models have brought unprecedented fidelity and details to cloth rendering. Yet, these models neglect fabric mechanics: when a piece of cloth interacts with the environment, its yarn and fiber arrangement usually changes in response to external contact and tension forces. Since subtle changes of a fabric&#39;s microstructures can greatly affect its macroscopic appearance, mechanics-driven appearance variation of fabrics has been a phenomenon that remains to be captured. We introduce a mechanics-aware model that adapts the microstructures of cloth yarns in a physics-based manner. Our technique works on two distinct physical scales: using physics-based simulations of individual yarns, we capture the rearrangement of yarn-level structures in response to external forces. These yarn structures are further enriched to obtain appearance-driving fiber-level details. The cross-scale enrichment is made practical through a new parameter fitting algorithm for simulation, an augmented procedural yarn model coupled with a custom-design regression neural network. We train the network using a dataset generated by joint simulations at both the yarn and the fiber levels. Through several examples, we demonstrate that our model is capable of synthesizing photorealistic cloth appearance in a mechanically plausible way.},
  archive      = {J_TVCG},
  author       = {Zahra Montazeri and Chang Xiao and Yun Fei and Changxi Zheng and Shuang Zhao},
  doi          = {10.1109/TVCG.2019.2937301},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {137-150},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mechanics-aware modeling of cloth appearance},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Locomotion in place in virtual reality: A comparative
evaluation of joystick, teleport, and leaning. <em>TVCG</em>,
<em>27</em>(1), 125–136. (<a
href="https://doi.org/10.1109/TVCG.2019.2928304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent VR head-mounted displays for consumers feature 3-DOF or 6-DOF head tracking. However, position tracking (when available) is limited to a small area. Moreover, in small or cluttered physical spaces, users can safely experience VR only by staying in place, standing or seated. Different locomotion techniques have been proposed to allow users to explore virtual environments by staying in place. Two in-place locomotion techniques, frequently employed in the literature and in consumer applications, are based on joystick and teleport. Some authors explored leaning with the aim of proposing a more natural in-place locomotion technique. However, more research is needed to understand the effects of the three techniques, since no user study thoroughly compared them all together on a variety of fundamental aspects. Therefore, this paper presents a comparative evaluation with 75 users, assessing the effects of the three techniques on performance, sickness, presence, usability, and different aspects of comfort. Performance of teleport was better than the other techniques, and performance of leaning was better than joystick. Teleport also caused less nausea than the other techniques. Unexpectedly, no significant differences were found for presence. Teleport received a higher usability score than the other techniques. Finally, the techniques had different effects on comfort that we discuss in detail.},
  archive      = {J_TVCG},
  author       = {Fabio Buttussi and Luca Chittaro},
  doi          = {10.1109/TVCG.2019.2928304},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {125-136},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Locomotion in place in virtual reality: A comparative evaluation of joystick, teleport, and leaning},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive architectural design with diverse solution
exploration. <em>TVCG</em>, <em>27</em>(1), 111–124. (<a
href="https://doi.org/10.1109/TVCG.2019.2938961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In architectural design, architects explore a vast amount of design options to maximize various performance criteria, while adhering to specific constraints. In an effort to assist architects in such a complex endeavour, we propose IDOME, an interactive system for computer-aided design optimization. Our approach balances automation and control by efficiently exploring, analyzing, and filtering space layouts to inform architects&#39; decision-making better. At each design iteration, IDOME provides a set of alternative building layouts which satisfy user-defined constraints and optimality criteria concerning a user-defined space parametrization. When the user selects a design generated by IDOME, the system performs a similar optimization process with the same (or different) parameters and objectives. A user may iterate this exploration process as many times as needed. In this work, we focus on optimizing built environments using architectural metrics by improving the degree of visibility, accessibility, and information gaining for navigating a proposed space. This approach, however, can be extended to support other kinds of analysis as well. We demonstrate the capabilities of IDOME through a series of examples, performance analysis, user studies, and a usability test. The results indicate that IDOME successfully optimizes the proposed designs concerning the chosen metrics and offers a satisfactory experience for users with minimal training.},
  archive      = {J_TVCG},
  author       = {Glen Berseth and Brandon Haworth and Muhammad Usman and Davide Schaumann and Mahyar Khayatkhoei and Mubbasir Kapadia and Petros Faloutsos},
  doi          = {10.1109/TVCG.2019.2938961},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {111-124},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactive architectural design with diverse solution exploration},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ImaCytE: Visual exploration of cellular micro-environments
for imaging mass cytometry data. <em>TVCG</em>, <em>27</em>(1), 98–110.
(<a href="https://doi.org/10.1109/TVCG.2019.2931299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tissue functionality is determined by the characteristics of tissue-resident cells and their interactions within their microenvironment. Imaging Mass Cytometry offers the opportunity to distinguish cell types with high precision and link them to their spatial location in intact tissues at sub-cellular resolution. This technology produces large amounts of spatially-resolved high-dimensional data, which constitutes a serious challenge for the data analysis. We present an interactive visual analysis workflow for the end-to-end analysis of Imaging Mass Cytometry data that was developed in close collaboration with domain expert partners. We implemented the presented workflow in an interactive visual analysis tool; ImaCytE. Our workflow is designed to allow the user to discriminate cell types according to their protein expression profiles and analyze their cellular microenvironments, aiding in the formulation or verification of hypotheses on tissue architecture and function. Finally, we show the effectiveness of our workflow and ImaCytE through a case study performed by a collaborating specialist.},
  archive      = {J_TVCG},
  author       = {Antonios Somarakis and Vincent Van Unen and Frits Koning and Boudewijn Lelieveldt and Thomas Höllt},
  doi          = {10.1109/TVCG.2019.2931299},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {98-110},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ImaCytE: Visual exploration of cellular micro-environments for imaging mass cytometry data},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-quality textured 3D shape reconstruction with cascaded
fully convolutional networks. <em>TVCG</em>, <em>27</em>(1), 83–97. (<a
href="https://doi.org/10.1109/TVCG.2019.2937300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a learning-based approach to reconstructing high-resolution three-dimensional (3D) shapes with detailed geometry and high-fidelity textures. Albeit extensively studied, algorithms for 3D reconstruction from multi-view depth-and-color (RGB-D) scans are still prone to measurement noise and occlusions; limited scanning or capturing angles also often lead to incomplete reconstructions. Propelled by recent advances in 3D deep learning techniques, in this paper, we introduce a novel computation- and memory-efficient cascaded 3D convolutional network architecture, which learns to reconstruct implicit surface representations as well as the corresponding color information from noisy and imperfect RGB-D maps. The proposed 3D neural network performs reconstruction in a progressive and coarse-to-fine manner, achieving unprecedented output resolution and fidelity. Meanwhile, an algorithm for end-to-end training of the proposed cascaded structure is developed. We further introduce Human10, a newly created dataset containing both detailed and textured full-body reconstructions as well as corresponding raw RGB-D scans of 10 subjects. Qualitative and quantitative experimental results on both synthetic and real-world datasets demonstrate that the presented approach outperforms existing state-of-the-art work regarding visual quality and accuracy of reconstructed models.},
  archive      = {J_TVCG},
  author       = {Zheng-Ning Liu and Yan-Pei Cao and Zheng-Fei Kuang and Leif Kobbelt and Shi-Min Hu},
  doi          = {10.1109/TVCG.2019.2937300},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {83-97},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {High-quality textured 3D shape reconstruction with cascaded fully convolutional networks},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FlyFusion: Realtime dynamic scene reconstruction using a
flying depth camera. <em>TVCG</em>, <em>27</em>(1), 68–82. (<a
href="https://doi.org/10.1109/TVCG.2019.2930691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While dynamic scene reconstruction has made revolutionary progress from the earliest setup using a mass of static cameras in studio environment to the latest egocentric or hand-held moving camera based schemes, it is still restricted by the recording volume, user comfortability, human labor and expertise. In this paper, a novel solution is proposed through a real-time and robust dynamic fusion scheme using a single flying depth camera, denoted as FlyFusion. By proposing a novel topology compactness strategy for effectively regularizing the complex topology changes, and the Geometry And Motion Energy (GAME) metric for guiding the viewpoint optimization in the volumetric space, FlyFusion succeeds to enable intelligent viewpoint selection based on the immediate dynamic reconstruction result. The merit of FlyFusion lies in its concurrent robustness, efficiency, and adaptation in producing fused and denoised 3D geometry and motions of a moving target interacting with different non-rigid objects in a large space.},
  archive      = {J_TVCG},
  author       = {Lan Xu and Wei Cheng and Kaiwen Guo and Lei Han and Yebin Liu and Lu Fang},
  doi          = {10.1109/TVCG.2019.2930691},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {68-82},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FlyFusion: Realtime dynamic scene reconstruction using a flying depth camera},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature-preserving tensor voting model for mesh
steganalysis. <em>TVCG</em>, <em>27</em>(1), 57–67. (<a
href="https://doi.org/10.1109/TVCG.2019.2929041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard tensor voting technique shows its versatility in tasks such as object recognition and semantic segmentation by recognizing feature points and sharp edges that can segment a model into several patches. We propose a neighborhood-level representation-guided tensor voting model for 3D mesh steganalysis. Because existing steganalytic methods do not analyze correlations among neighborhood faces, they are not very effective at discriminating stego meshes from cover meshes. In this paper, we propose to utilize a tensor voting model to reveal the artifacts caused by embedding data. In the proposed steganalytic scheme, the normal voting tensor (NVT) operation is performed on original mesh faces and smoothed mesh faces separately. Then, the absolute values of the differences between the eigenvalues of the two tensors (from the original face and the smoothed face) are regarded as features that capture intricate relationships among the vertices. Subsequently, the extracted features are processed with a nonlinear mapping to boost the feature effectiveness. The experimental results show that the proposed feature sets prevail over state-of-the-art feature sets including LFS64 and ELFS124 under various steganographic schemes.},
  archive      = {J_TVCG},
  author       = {Hang Zhou and Kejiang Chen and Weiming Zhang and Chuan Qin and Nenghai Yu},
  doi          = {10.1109/TVCG.2019.2929041},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {57-67},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Feature-preserving tensor voting model for mesh steganalysis},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and optimization of conforming lattice structures.
<em>TVCG</em>, <em>27</em>(1), 43–56. (<a
href="https://doi.org/10.1109/TVCG.2019.2938946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by natural cellular materials such as trabecular bone, lattice structures have been developed as a new type of lightweight material. In this paper we present a novel method to design lattice structures that conform with both the principal stress directions and the boundary of the optimized shape. Our method consists of two major steps: the first optimizes concurrently the shape (including its topology) and the distribution of orthotropic lattice materials inside the shape to maximize stiffness under application-specific external loads; the second takes the optimized configuration (i.e., locally-defined orientation, porosity, and anisotropy) of lattice materials from the previous step, and extracts a globally consistent lattice structure by field-aligned parameterization. Our approach is robust and works for both 2D planar and 3D volumetric domains. Numerical results and physical verifications demonstrate remarkable structural properties of conforming lattice structures generated by our method.},
  archive      = {J_TVCG},
  author       = {Jun Wu and Weiming Wang and Xifeng Gao},
  doi          = {10.1109/TVCG.2019.2938946},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {43-56},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Design and optimization of conforming lattice structures},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence-controlled local isosurfacing. <em>TVCG</em>,
<em>27</em>(1), 29–42. (<a
href="https://doi.org/10.1109/TVCG.2020.3016327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel framework that can generate a high-fidelity isosurface model of X-ray computed tomography (CT) data. CT surfaces with subvoxel precision and smoothness can be simply modeled via isosurfacing, where a single CT value represents an isosurface. However, this inevitably results in geometric distortion of the CT data containing CT artifacts. An alternative is to treat this challenge as a segmentation problem. However, in general, segmentation techniques are not robust against noisy data and require heavy computation to handle the artifacts that occur in three-dimensional CT data. Furthermore, the surfaces generated from segmentation results may contain jagged, overly smooth, or distorted geometries. We present a novel local isosurfacing framework that can address these issues simultaneously. The proposed framework exploits two primary techniques: 1) Canny edge approach for obtaining surface candidate boundary points and evaluating their confidence and 2) screened Poisson optimization for fitting a surface to the boundary points in which the confidence term is incorporated. This combination facilitates local isosurfacing that can produce high-fidelity surface models. We also implement an intuitive user interface to alleviate the burden of selecting the appropriate confidence computing parameters. Our experimental results demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TVCG},
  author       = {Dongjoon Kim and Heewon Kye and Jeongjin Lee and Yeong-Gil Shin},
  doi          = {10.1109/TVCG.2020.3016327},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {29-42},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Confidence-controlled local isosurfacing},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Combining recurrent neural networks and adversarial
training for human motion synthesis and control. <em>TVCG</em>,
<em>27</em>(1), 14–28. (<a
href="https://doi.org/10.1109/TVCG.2019.2938520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new generative deep learning network for human motion synthesis and control. Our key idea is to combine recurrent neural networks (RNNs) and adversarial training for human motion modeling. We first describe an efficient method for training an RNN model from prerecorded motion data. We implement RNNs with long short-term memory (LSTM) cells because they are capable of addressing the nonlinear dynamics and long term temporal dependencies present in human motions. Next, we train a refiner network using an adversarial loss, similar to generative adversarial networks (GANs), such that refined motion sequences are indistinguishable from real mocap data using a discriminative network. The resulting model is appealing for motion synthesis and control because it is compact, contact-aware, and can generate an infinite number of naturally looking motions with infinite lengths. Our experiments show that motions generated by our deep learning model are always highly realistic and comparable to high-quality motion capture data. We demonstrate the power and effectiveness of our models by exploring a variety of applications, ranging from random motion synthesis, online/offline motion control, and motion filtering. We show the superiority of our generative model by comparison against baseline models.},
  archive      = {J_TVCG},
  author       = {Zhiyong Wang and Jinxiang Chai and Shihong Xia},
  doi          = {10.1109/TVCG.2019.2938520},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {14-28},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Combining recurrent neural networks and adversarial training for human motion synthesis and control},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analyzing dynamic hypergraphs with parallel aggregated
ordered hypergraph visualization. <em>TVCG</em>, <em>27</em>(1), 1–13.
(<a href="https://doi.org/10.1109/TVCG.2019.2933196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel Aggregated Ordered Hypergraph(PAOH) is a novel technique to visualize dynamic hypergraphs. Hypergraphs are a generalization of graphs where edges can connect several vertices. Hypergraphs can be used to model networks of business partners or co-authorship networks with multiple authors per article. A dynamic hypergraph evolves over discrete time slots. PAOH represents vertices as parallel horizontal bars and hyperedges as vertical lines, using dots to depict the connections to one or more vertices. We describe a prototype implementation of Parallel Aggregated Ordered Hypergraph, report on a usability study with 9 participants analyzing publication data, and summarize the improvements made. Two case studies and several examples are provided. We believe that PAOH is the first technique to provide a highly readable representation of dynamic hypergraphs. It is easy to learn and well suited for medium size dynamic hypergraphs (50-500 vertices) such as those commonly generated by digital humanities projects-our driving application domain.},
  archive      = {J_TVCG},
  author       = {Paola Valdivia and Paolo Buono and Catherine Plaisant and Nicole Dufournaud and Jean-Daniel Fekete},
  doi          = {10.1109/TVCG.2019.2933196},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  number       = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analyzing dynamic hypergraphs with parallel aggregated ordered hypergraph visualization},
  volume       = {27},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
