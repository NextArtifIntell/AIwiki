<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PIEEE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pieee---104">PIEEE - 104</h2>
<ul>
<li><details>
<summary>
(2021b). Swedish cryptology II [scanning our past]. <em>PIEEE</em>,
<em>109</em>(12), 1942–1951. (<a
href="https://doi.org/10.1109/JPROC.2021.3084438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this second installment about Swedish cryptology, we will describe the successes and failures in intercepting, cryptanalyzing, and reading the secret messages of potentially hostile and other countries during WW2. The information gathered aided the government in its quest to stay neutral and kept the military leadership informed about possible threats of invasion.},
  archive      = {J_PIEEE},
  author       = {Kjell-Ove Widman and Anders Wik},
  doi          = {10.1109/JPROC.2021.3084438},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1942-1951},
  shortjournal = {Proc. IEEE},
  title        = {Swedish cryptology II [Scanning our past]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extraction and utilization of excitation information of
speech: A review. <em>PIEEE</em>, <em>109</em>(12), 1920–1941. (<a
href="https://doi.org/10.1109/JPROC.2021.3126493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech production can be regarded as a process where a time-varying vocal tract system (filter) is excited by a time-varying excitation. In addition to its linguistic message, the speech signal also carries information about, for example, the gender and age of the speaker. Moreover, the speech signal includes acoustical cues about several speaker traits, such as the emotional state and the state of health of the speaker. In order to understand the production of these acoustical cues by the human speech production mechanism and utilize this information in speech technology, it is necessary to extract features describing both the excitation and the filter of the human speech production mechanism. While the methods to estimate and parameterize the vocal tract system are well established, the excitation appears less studied. This article provides a review of signal processing approaches used for the extraction of excitation information from speech. This article highlights the importance of excitation information in the analysis and classification of phonation type and vocal emotions, in the analysis of nonverbal laughter sounds, and in studying pathological voices. Furthermore, recent developments of deep learning techniques in the context of extraction and utilization of the excitation information are discussed.},
  archive      = {J_PIEEE},
  author       = {Sudarsana Reddy Kadiri and Paavo Alku and B. Yegnanarayana},
  doi          = {10.1109/JPROC.2021.3126493},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1920-1941},
  shortjournal = {Proc. IEEE},
  title        = {Extraction and utilization of excitation information of speech: A review},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building in-the-cloud network functions: Security and
privacy challenges. <em>PIEEE</em>, <em>109</em>(12), 1888–1919. (<a
href="https://doi.org/10.1109/JPROC.2021.3127277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV) has been promising to improve the availability, programmability, and flexibility of network function deployment and communication facilities. Meanwhile, with the advancements of cloud technologies, there has been a trend to outsource network functions through virtualization to a cloud service provider, so as to alleviate the local burdens on provisioning and managing such hardware resources. Promising as it is, redirecting the communication traffic to a third-party service provider has drawn various security and privacy concerns. Traditional end-to-end encryption can protect the traffic in transmit, but it also hinders data usability. This dilemma has raised wide interests from both industry and academia, and great efforts have been made to realize privacy-preserving network function outsourcing that can guarantee the confidentiality of network communications while preserving the ability to inspect the traffic. In this article, we conduct a comprehensive survey of the state-of-the-art literature on network function outsourcing, with a special focus on privacy and security issues. We first give a brief introduction to NFV and pinpoint its challenges and security risks in the cloud context. Then, we present detailed descriptions and comparisons of existing secure network function outsourcing schemes in terms of functionality, efficiency, and security. Finally, we conclude by discussing possible future research directions.},
  archive      = {J_PIEEE},
  author       = {Peipei Jiang and Qian Wang and Muqi Huang and Cong Wang and Qi Li and Chao Shen and Kui Ren},
  doi          = {10.1109/JPROC.2021.3127277},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1888-1919},
  shortjournal = {Proc. IEEE},
  title        = {Building in-the-cloud network functions: Security and privacy challenges},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do dense 5G networks increase exposure to electromagnetic
fields? [Point of view]. <em>PIEEE</em>, <em>109</em>(12), 1880–1887.
(<a href="https://doi.org/10.1109/JPROC.2021.3125528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The debate over whether electromagnetic fields (EMFs) exposure poses a danger to human health is recurring and goes back centuries to when our society first began to rely on electricity [1] .},
  archive      = {J_PIEEE},
  author       = {Luca Chiaraviglio and Sara Turco and Giuseppe Bianchi and Nicola Blefari-Melazzi},
  doi          = {10.1109/JPROC.2021.3125528},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1880-1887},
  shortjournal = {Proc. IEEE},
  title        = {Do dense 5G networks increase exposure to electromagnetic fields? [Point of view]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Scanning the issue. <em>PIEEE</em>, <em>109</em>(12),
1878–1879. (<a
href="https://doi.org/10.1109/JPROC.2021.3129230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents information on the papers included in this issue of the publication.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3129230},
  journal      = {Proceedings of the IEEE},
  number       = {12},
  pages        = {1878-1879},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IEEE membership. <em>PIEEE</em>, <em>109</em>(11), 1876. (<a
href="https://doi.org/10.1109/JPROC.2021.3122483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3122483},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1876},
  shortjournal = {Proc. IEEE},
  title        = {IEEE membership},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). IEEE women in engineering. <em>PIEEE</em>,
<em>109</em>(11), 1875. (<a
href="https://doi.org/10.1109/JPROC.2021.3122481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Women in Engineering.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3122481},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1875},
  shortjournal = {Proc. IEEE},
  title        = {IEEE women in engineering},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Swedish cryptology i [scanning our past]. <em>PIEEE</em>,
<em>109</em>(11), 1864–1872. (<a
href="https://doi.org/10.1109/JPROC.2021.3084460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This month’s article is the first of two which describe cryptographic inventions and codebreaking in Sweden from early times up until 1945. In the first half of the 20th century, Sweden created for itself a strong presence in the world of cryptography. Whether this process was the consequence of actions by individuals or followed from the general industrial and cultural advancements is arguable. Perhaps, both played a role. From the 16th century onward, manual systems of enciphering, such as substitution ciphers and codes, were used by the court, the army, and other official organizations. In this article, though, we will take as a starting point the design of what appears to be the world’s first ciphering machine in 1786, finishing the presentation with the outbreak of the Second World War (WW2) in 1939. In a sequel, we will describe the successes and failures of Swedish Sigint, cryptanalysis, and defensive cryptography during WW2.},
  archive      = {J_PIEEE},
  author       = {Kjell-Ove Widman and Anders Wik},
  doi          = {10.1109/JPROC.2021.3084460},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1864-1872},
  shortjournal = {Proc. IEEE},
  title        = {Swedish cryptology i [Scanning our past]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video summarization using deep neural networks: A survey.
<em>PIEEE</em>, <em>109</em>(11), 1838–1863. (<a
href="https://doi.org/10.1109/JPROC.2021.3117472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization technologies aim to create a concise and complete synopsis by selecting the most informative parts of the video content. Several approaches have been developed over the last couple of decades, and the current state of the art is represented by methods that rely on modern deep neural network architectures. This work focuses on the recent advances in the area and provides a comprehensive survey of the existing deep-learning-based methods for generic video summarization. After presenting the motivation behind the development of technologies for video summarization, we formulate the video summarization task and discuss the main characteristics of a typical deep-learning-based analysis pipeline. Then, we suggest a taxonomy of the existing algorithms and provide a systematic review of the relevant literature that shows the evolution of the deep-learning-based video summarization technologies and leads to suggestions for future developments. We then report on protocols for the objective evaluation of video summarization algorithms, and we compare the performance of several deep-learning-based approaches. Based on the outcomes of these comparisons, as well as some documented considerations about the amount of annotated data and the suitability of evaluation protocols, we indicate potential future research directions.},
  archive      = {J_PIEEE},
  author       = {Evlampios Apostolidis and Eleni Adamantidou and Alexandros I. Metsai and Vasileios Mezaris and Ioannis Patras},
  doi          = {10.1109/JPROC.2021.3117472},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1838-1863},
  shortjournal = {Proc. IEEE},
  title        = {Video summarization using deep neural networks: A survey},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Edge intelligence: Empowering intelligence to the edge of
network. <em>PIEEE</em>, <em>109</em>(11), 1778–1837. (<a
href="https://doi.org/10.1109/JPROC.2021.3119950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence refers to a set of connected systems and devices for data collection, caching, processing, and analysis proximity to where data are captured based on artificial intelligence. Edge intelligence aims at enhancing data processing and protects the privacy and security of the data and users. Although recently emerged, spanning the period from 2011 to now, this field of research has shown explosive growth over the past five years. In this article, we present a thorough and comprehensive survey of the literature surrounding edge intelligence. We first identify four fundamental components of edge intelligence, i.e., edge caching, edge training, edge inference, and edge offloading based on theoretical and practical results pertaining to proposed and deployed systems. We then aim for a systematic classification of the state of the solutions by examining research results and observations for each of the four components and present a taxonomy that includes practical problems, adopted techniques, and application goals. For each category, we elaborate, compare, and analyze the literature from the perspectives of adopted techniques, objectives, performance, advantages and drawbacks, and so on. This article provides a comprehensive survey of edge intelligence and its application areas. In addition, we summarize the development of the emerging research fields and the current state of the art and discuss the important open issues and possible theoretical and technical directions.},
  archive      = {J_PIEEE},
  author       = {Dianlei Xu and Tong Li and Yong Li and Xiang Su and Sasu Tarkoma and Tao Jiang and Jon Crowcroft and Pan Hui},
  doi          = {10.1109/JPROC.2021.3119950},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1778-1837},
  shortjournal = {Proc. IEEE},
  title        = {Edge intelligence: Empowering intelligence to the edge of network},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How fast do algorithms improve? [Point of view].
<em>PIEEE</em>, <em>109</em>(11), 1768–1777. (<a
href="https://doi.org/10.1109/JPROC.2021.3107219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms determine which calculations computers use to solve problems and are one of the central pillars of computer science. As algorithms improve, they enable scientists to tackle larger problems and explore new domains and new scientific techniques [1] , [2] . Bold claims have been made about the pace of algorithmic progress. For example, the President’s Council of Advisors on Science and Technology (PCAST), a body of senior scientists that advise the U.S. President, wrote in 2010 that “in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed” [3] . However, this conclusion was supported based on data from progress in linear solvers [4] , which is just a single example. With no guarantee that linear solvers are representative of algorithms in general, it is unclear how broadly conclusions, such as PCAST’s, should be interpreted. Is progress faster in most algorithms? Just some? How much on average?},
  archive      = {J_PIEEE},
  author       = {Yash Sherry and Neil C. Thompson},
  doi          = {10.1109/JPROC.2021.3107219},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1768-1777},
  shortjournal = {Proc. IEEE},
  title        = {How fast do algorithms improve? [Point of view]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Scanning the issue. <em>PIEEE</em>, <em>109</em>(11),
1766–1767. (<a
href="https://doi.org/10.1109/JPROC.2021.3120227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms are one of the fundamental building blocks of computing. However, current evidence about how fast algorithms improve is anecdotal, using small numbers of case studies to extrapolate. In this month’s Point of View article, the authors present the first systematic, quantitative evidence that supports the argument that algorithms are one of the most important sources of improvement in computing. Analyzing data from 57 textbooks and more than 1137 research papers reveals enormous variation. Around half of all algorithm families experience little or no improvement. At the other extreme, 14\% experience transformative improvements, radically changing how and where they can be used. Overall, for moderate-sized This month’s articles take a comprehensive look at edge intelligence and the use of deep-learning-based methods for video summarization. problems, 30\%–43\% of algorithmic families had improvements comparable or greater than those that users experienced from Moore’s Law and other hardware advances.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3120227},
  journal      = {Proceedings of the IEEE},
  number       = {11},
  pages        = {1766-1767},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stay informed. Become inspired. [Back cover -
advertisement]. <em>PIEEE</em>, <em>109</em>(10), C4. (<a
href="https://doi.org/10.1109/JPROC.2021.3109714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3109714},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {C4},
  shortjournal = {Proc. IEEE},
  title        = {Stay informed. become inspired. [Back cover - advertisement]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Birth of the first ruby laser in china [scanning our past].
<em>PIEEE</em>, <em>109</em>(10), 1753–1763. (<a
href="https://doi.org/10.1109/JPROC.2021.3103559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In September 1961, the first ruby laser successfully debuted at Changchun Institute of Optics, Fine Mechanics and Physics (CIOMP), Chinese Academy of Sciences (CAS), marking the beginning of laser science and technology in China [1] . It operated with unique features about one year after T. H. Maiman’s first working model in the United States, and three months after the first laser in the former Soviet Union. It was unusual for China to achieve such an accomplishment given its technically backward state in the 1950s, and one cannot help but wonder how the Chinese scientists caught up with international trends in laser research. We want to answer some important questions about this historical event and explain the birth of the first ruby laser in China by examining the archives of the developmental process and interviewing some of the key members who pioneered China’s laser research.},
  archive      = {J_PIEEE},
  author       = {Chen Chongbin and He Junmin and Sun Hongqing},
  doi          = {10.1109/JPROC.2021.3103559},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1753-1763},
  shortjournal = {Proc. IEEE},
  title        = {Birth of the first ruby laser in china [Scanning our past]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hardware acceleration of sparse and irregular tensor
computations of ML models: A survey and insights. <em>PIEEE</em>,
<em>109</em>(10), 1706–1752. (<a
href="https://doi.org/10.1109/JPROC.2021.3098483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models are widely used in many important domains. For efficiently processing these computational- and memory-intensive applications, tensors of these overparameterized models are compressed by leveraging sparsity, size reduction, and quantization of tensors. Unstructured sparsity and tensors with varying dimensions yield irregular computation, communication, and memory access patterns; processing them on hardware accelerators in a conventional manner does not inherently leverage acceleration opportunities. This article provides a comprehensive survey on the efficient execution of sparse and irregular tensor computations of ML models on hardware accelerators. In particular, it discusses enhancement modules in the architecture design and the software support, categorizes different hardware designs and acceleration techniques, analyzes them in terms of hardware and execution costs, analyzes achievable accelerations for recent DNNs, and highlights further opportunities in terms of hardware/software/model codesign optimizations (inter/intramodule). The takeaways from this article include the following: understanding the key challenges in accelerating sparse, irregular shaped, and quantized tensors; understanding enhancements in accelerator systems for supporting their efficient computations; analyzing tradeoffs in opting for a specific design choice for encoding, storing, extracting, communicating, computing, and load-balancing the nonzeros; understanding how structured sparsity can improve storage efficiency and balance computations; understanding how to compile and map models with sparse tensors on the accelerators; and understanding recent design trends for efficient accelerations and further opportunities.},
  archive      = {J_PIEEE},
  author       = {Shail Dave and Riyadh Baghdadi and Tony Nowatzki and Sasikanth Avancha and Aviral Shrivastava and Baoxin Li},
  doi          = {10.1109/JPROC.2021.3098483},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1706-1752},
  shortjournal = {Proc. IEEE},
  title        = {Hardware acceleration of sparse and irregular tensor computations of ML models: A survey and insights},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of millimeter-wave communication: Physical-layer
technology specifications and enabling transmission technologies.
<em>PIEEE</em>, <em>109</em>(10), 1666–1705. (<a
href="https://doi.org/10.1109/JPROC.2021.3107494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) frequency bands, which offer abundant underutilized spectral resources, have been explored and exploited in the past several years to meet the requirements of emerging wireless services highlighted by high data rates, ultrareliability, and ultralow delivery latency. Yet, the unique characteristics of mmWave, e.g., continuous wide bandwidth, large path, and penetration losses, along with hardware constraints, call for innovative technologies for mmWave communication. Recently, an extensive amount of work on mmWave communication has been carried out by researchers and practitioners from both academia and industry, and various technologies have been developed for mmWave communication systems to fulfill the full potential of mmWave frequency bands. In this article, we present a comprehensive survey of the standardization of mmWave communication, the latest progress and outcomes of the research on mmWave communication technologies, and the emerging applications of mmWave communication. In particular, we provide a timely and in-depth summary of the state-of-the-art technology specifications of mmWave communication with an emphasis on the physical (PHY) layer. Then, we elaborate on a number of well-established or promising antenna architectures in mmWave communication systems and investigate the enabling PHY layer transmission technologies. Finally, we show some existing and emerging applications of mmWave communication and discuss the potential open research issues.},
  archive      = {J_PIEEE},
  author       = {Shiwen He and Yan Zhang and Jiaheng Wang and Jian Zhang and Ju Ren and Yaoxue Zhang and Weihua Zhuang and Xuemin Shen},
  doi          = {10.1109/JPROC.2021.3107494},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1666-1705},
  shortjournal = {Proc. IEEE},
  title        = {A survey of millimeter-wave communication: Physical-layer technology specifications and enabling transmission technologies},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An overview of signal processing techniques for terahertz
communications. <em>PIEEE</em>, <em>109</em>(10), 1628–1665. (<a
href="https://doi.org/10.1109/JPROC.2021.3100811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terahertz (THz)-band communications are a key enabler for future-generation wireless communication systems that promise to integrate a wide range of data-demanding applications. Recent advances in photonic, electronic, and plasmonic technologies are closing the gap in THz transceiver design. Consequently, prospect THz signal generation, modulation, and radiation methods are converging, and corresponding channel model, noise, and hardware-impairment notions are emerging. Such progress establishes a foundation for well-grounded research into THz-specific signal processing techniques for wireless communications. This tutorial overviews these techniques, emphasizing ultramassive multiple-input–multiple-output (UM-MIMO) systems and reconfigurable intelligent surfaces, vital for overcoming the distance problem at very high frequencies. We focus on the classical problems of waveform design and modulation, beamforming and precoding, index modulation, channel estimation, channel coding, and data detection. We also motivate signal processing techniques for THz sensing and localization.},
  archive      = {J_PIEEE},
  author       = {Hadi Sarieddeen and Mohamed-Slim Alouini and Tareq Y. Al-Naffouri},
  doi          = {10.1109/JPROC.2021.3100811},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1628-1665},
  shortjournal = {Proc. IEEE},
  title        = {An overview of signal processing techniques for terahertz communications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Scanning the issue. <em>PIEEE</em>, <em>109</em>(10),
1626–1627. (<a
href="https://doi.org/10.1109/JPROC.2021.3108375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wireless communication’s frequency spectrum has been continuously expanding to satisfy ever-increasing bandwidth demands. Although millimeter-wave (mmWave) band communications are already shaping the fifth-generation (5G) wireless mobile communications, terahertz (THz) band communications will be essential to the future sixth generation (6G) and beyond for a number of reasons. In contrast to mmWave communications, THz communications can exploit the available spectrum to achieve a terabits per second data rate without additional spectral efficiency enhancement techniques. Furthermore, due to the shorter wavelengths, THz signals are less susceptible to free-space diffraction and inter-antenna interference and exhibit higher resilience to eavesdropping. THz systems can also support higher link directionality and can be achieved in much smaller footprints. In contrast, compared to visible light communications, THz signals are not as severely affected by alignment issues, ambient light, atmospheric turbulence, scintillation, fog, and temporary spatial variation in light intensity.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3108375},
  journal      = {Proceedings of the IEEE},
  number       = {10},
  pages        = {1626-1627},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An introduction to MPEG-g: The first open ISO/IEC standard
for the compression and exchange of genomic sequencing data.
<em>PIEEE</em>, <em>109</em>(9), 1607–1622. (<a
href="https://doi.org/10.1109/JPROC.2021.3082027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and progress of high-throughput sequencing technologies have transformed the sequencing of DNA from a scientific research challenge to practice. With the release of the latest generation of sequencing machines, the cost of sequencing a whole human genome has dropped to less than $\$ $ 600. Such achievements open the door to personalized medicine, where it is expected that genomic information of patients will be analyzed as a standard practice. However, the associated costs, related to storing, transmitting, and processing the large volumes of data, are already comparable to the costs of sequencing. To support the design of new and interoperable solutions for the representation, compression, and management of genomic sequencing data, the Moving Picture Experts Group (MPEG) jointly with working group 5 of ISO/TC276 “Biotechnology” has started to produce the ISO/IEC 23092 series, known as MPEG-G. MPEG-G does not only offer higher levels of compression compared with the state of the art but it also provides new functionalities, such as built-in support for random access in the compressed domain, support for data protection mechanisms, flexible storage, and streaming capabilities. MPEG-G only specifies the decoding syntax of compressed bitstreams, as well as a file format and a transport format. This allows for the development of new encoding solutions with higher degrees of optimization while maintaining compatibility with any existing MPEG-G decoder.},
  archive      = {J_PIEEE},
  author       = {Jan Voges and Mikel Hernaez and Marco Mattavelli and Jörn Ostermann},
  doi          = {10.1109/JPROC.2021.3082027},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1607-1622},
  shortjournal = {Proc. IEEE},
  title        = {An introduction to MPEG-G: The first open ISO/IEC standard for the compression and exchange of genomic sequencing data},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An overview of omnidirectional MediA format (OMAF).
<em>PIEEE</em>, <em>109</em>(9), 1590–1606. (<a
href="https://doi.org/10.1109/JPROC.2021.3063544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During recent years, there have been product launches and research for enabling immersive audio-visual media experiences. For example, a variety of head-mounted displays and 360° cameras are available in the market. To facilitate interoperability between devices and media system components by different vendors, the Moving Picture Experts Group (MPEG) developed the Omnidirectional MediA Format (OMAF), which is arguably the first virtual reality (VR) system standard. OMAF is a storage and streaming format for omnidirectional media, including 360° video and images, spatial audio, and associated timed text. This article provides a comprehensive overview of OMAF.},
  archive      = {J_PIEEE},
  author       = {Miska M. Hannuksela and Ye-Kui Wang},
  doi          = {10.1109/JPROC.2021.3063544},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1590-1606},
  shortjournal = {Proc. IEEE},
  title        = {An overview of omnidirectional MediA format (OMAF)},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPEG standards for compressed representation of immersive
audio. <em>PIEEE</em>, <em>109</em>(9), 1578–1589. (<a
href="https://doi.org/10.1109/JPROC.2021.3075390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The term “immersive audio” is frequently used to describe an audio experience that provides the listener the sensation of being fully immersed or “present” in a sound scene. This can be achieved via different presentation modes, such as surround sound (several loudspeakers horizontally arranged around the listener), 3D audio (with loudspeakers at, above, and below listener ear level), and binaural audio to headphones. This article provides an overview of two recent standards that support the bitrate-efficient carriage of high-quality immersive sound. The first is MPEG-H 3D audio, which is a versatile standard that supports multiple immersive sound signal formats (channels, objects, and higher order ambisonics) and is now being adopted in broadcast and streaming applications. The second is MPEG-I immersive audio, an extension of 3D audio, currently under development, which is targeted for virtual and augmented reality applications. This will support rendering of fully user-interactive immersive sound for three degrees of user movement [three degrees of freedom (3DoF)], i.e., yaw, pitch, and roll head movement, and for six degrees of user movement [six degrees of freedom (6DoF)], i.e., 3DoF plus translational x, y, and z user position movements.},
  archive      = {J_PIEEE},
  author       = {Schuyler R. Quackenbush and Jürgen Herre},
  doi          = {10.1109/JPROC.2021.3075390},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1578-1589},
  shortjournal = {Proc. IEEE},
  title        = {MPEG standards for compressed representation of immersive audio},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JPEG XS—a new standard for visually lossless low-latency
lightweight image coding. <em>PIEEE</em>, <em>109</em>(9), 1559–1577.
(<a href="https://doi.org/10.1109/JPROC.2021.3080916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint Photographic Experts Group (JPEG) XS is a new International Standard from the JPEG Committee (formally known as ISO/International Electrotechnical Commission (IEC) JTC1/SC29/WG1). It defines an interoperable, visually lossless low-latency lightweight image coding that can be used for mezzanine compression within any AV market. Among the targeted use cases, one can cite video transport over professional video links (serial digital interface (SDI), internet protocol (IP), and Ethernet), real-time video storage, memory buffers, omnidirectional video capture and rendering, and sensor compression (for example, in cameras and the automotive industry). The core coding system is composed of an optional color transform, a wavelet transform, and a novel entropy encoder, processing groups of coefficients by coding their magnitude level and packing the magnitude refinement. Such a design allows for visually transparent quality at moderate compression ratios, scalable end-to-end latency that ranges from less than one line to a maximum of 32 lines of the image, and a low-complexity real-time implementation in application-specific integrated circuit (ASIC), field-programmable gate array (FPGA), central processing unit (CPU), and graphics processing unit (GPU). This article details the key features of this new standard and the profiles and formats that have been defined so far for the various applications. It also gives a technical description of the core coding system. Finally, the latest performance evaluation results of recent implementations of the standard are presented, followed by the current status of the ongoing standardization process and future milestones.},
  archive      = {J_PIEEE},
  author       = {Antonin Descampe and Thomas Richter and Touradj Ebrahimi and Siegfried Foessel and Joachim Keinert and Tim Bruylants and Pascal Pellegrin and Charles Buysschaert and Gaël Rouvroy},
  doi          = {10.1109/JPROC.2021.3080916},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1559-1577},
  shortjournal = {Proc. IEEE},
  title        = {JPEG XS—A new standard for visually lossless low-latency lightweight image coding},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compression of sparse and dense dynamic point clouds—methods
and standards. <em>PIEEE</em>, <em>109</em>(9), 1537–1558. (<a
href="https://doi.org/10.1109/JPROC.2021.3085957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a survey of the point cloud compression (PCC) methods by organizing them with respect to the data structure, coding representation space, and prediction strategies is presented. Two paramount families of approaches reported in the literature-the projection- and octree-based methods-are proven to be efficient for encoding dense and sparse point clouds, respectively. These approaches are the pillars on which the Moving Picture Experts Group Committee developed two PCC standards published as final international standards in 2020 and early 2021, respectively, under the names: video-based PCC and geometry-based PCC. After surveying the current approaches for PCC, the technologies underlying the two standards are described in detail from an encoder perspective, providing guidance for potential standard implementors. In addition, experiment evaluations in terms of compression performances for both solutions are provided.},
  archive      = {J_PIEEE},
  author       = {Chao Cao and Marius Preda and Vladyslav Zakharchenko and Euee S. Jang and Titus Zaharia},
  doi          = {10.1109/JPROC.2021.3085957},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1537-1558},
  shortjournal = {Proc. IEEE},
  title        = {Compression of sparse and dense dynamic point Clouds—Methods and standards},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MPEG immersive video coding standard. <em>PIEEE</em>,
<em>109</em>(9), 1521–1536. (<a
href="https://doi.org/10.1109/JPROC.2021.3062590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the ISO/IEC MPEG Immersive Video (MIV) standard, MPEG-I Part 12, which is undergoing standardization. The draft MIV standard provides support for viewing immersive volumetric content captured by multiple cameras with six degrees of freedom (6DoF) within a viewing space that is determined by the camera arrangement in the capture rig. The bitstream format and decoding processes of the draft specification along with aspects of the Test Model for Immersive Video (TMIV) reference software encoder, decoder, and renderer are described. The use cases, test conditions, quality assessment methods, and experimental results are provided. In the TMIV, multiple texture and geometry views are coded as atlases of patches using a legacy 2-D video codec, while optimizing for bitrate, pixel rate, and quality. The design of the bitstream format and decoder is based on the visual volumetric video-based coding (V3C) and video-based point cloud compression (V-PCC) standard, MPEG-I Part 5.},
  archive      = {J_PIEEE},
  author       = {Jill M. Boyce and Renaud Doré and Adrian Dziembowski and Julien Fleureau and Joel Jung and Bart Kroon and Basel Salahieh and Vinod Kumar Malamal Vadakital and Lu Yu},
  doi          = {10.1109/JPROC.2021.3062590},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1521-1536},
  shortjournal = {Proc. IEEE},
  title        = {MPEG immersive video coding standard},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advances in video compression system using deep neural
network: A review and case studies. <em>PIEEE</em>, <em>109</em>(9),
1494–1520. (<a
href="https://doi.org/10.1109/JPROC.2021.3059994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant advances in video compression systems have been made in the past several decades to satisfy the near-exponential growth of Internet-scale video traffic. From the application perspective, we have identified three major functional blocks, including preprocessing, coding, and postprocessing, which have been continuously investigated to maximize the end-user quality of experience (QoE) under a limited bit rate budget. Recently, artificial intelligence (AI)-powered techniques have shown great potential to further increase the efficiency of the aforementioned functional blocks, both individually and jointly. In this article, we review recent technical advances in video compression systems extensively, with an emphasis on deep neural network (DNN)-based approaches, and then present three comprehensive case studies. On preprocessing, we show a switchable texture-based video coding example that leverages DNN-based scene understanding to extract semantic areas for the improvement of a subsequent video coder. On coding, we present an end-to-end neural video coding framework that takes advantage of the stacked DNNs to efficiently and compactly code input raw videos via fully data-driven learning. On postprocessing, we demonstrate two neural adaptive filters to, respectively, facilitate the in-loop and postfiltering for the enhancement of compressed frames. Finally, a companion website hosting the contents developed in this work can be accessed publicly at https://purdueviper.github.io/dnn-coding/.},
  archive      = {J_PIEEE},
  author       = {Dandan Ding and Zhan Ma and Di Chen and Qingshuang Chen and Zoe Liu and Fengqing Zhu},
  doi          = {10.1109/JPROC.2021.3059994},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1494-1520},
  shortjournal = {Proc. IEEE},
  title        = {Advances in video compression system using deep neural network: A review and case studies},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Developments in international video coding standardization
after AVC, with an overview of versatile video coding (VVC).
<em>PIEEE</em>, <em>109</em>(9), 1463–1493. (<a
href="https://doi.org/10.1109/JPROC.2020.3043399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 17 years, since the finalization of the first version of the now-dominant H.264/Moving Picture Experts Group-4 (MPEG-4) Advanced Video Coding (AVC) standard in 2003, two major new generations of video coding standards have been developed. These include the standards known as High Efficiency Video Coding (HEVC) and Versatile Video Coding (VVC). HEVC was finalized in 2013, repeating the ten-year cycle time set by its predecessor and providing about 50\% bit-rate reduction over AVC. The cycle was shortened by three years for the VVC project, which was finalized in July 2020, yet again achieving about a 50\% bit-rate reduction over its predecessor (HEVC). This article summarizes these developments in video coding standardization after AVC. It especially focuses on providing an overview of the first version of VVC, including comparisons against HEVC. Besides further advances in hybrid video compression, as in previous development cycles, the broad versatility of the application domain that is highlighted in the title of VVC is explained. Included in VVC is the support for a wide range of applications beyond the typical standard- and high-definition camera-captured content codings, including features to support computer-generated/screen content, high dynamic range content, multilayer and multiview coding, and support for immersive media such as 360° video.},
  archive      = {J_PIEEE},
  author       = {Benjamin Bross and Jianle Chen and Jens-Rainer Ohm and Gary J. Sullivan and Ye-Kui Wang},
  doi          = {10.1109/JPROC.2020.3043399},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1463-1493},
  shortjournal = {Proc. IEEE},
  title        = {Developments in international video coding standardization after AVC, with an overview of versatile video coding (VVC)},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A technical overview of AV1. <em>PIEEE</em>,
<em>109</em>(9), 1435–1462. (<a
href="https://doi.org/10.1109/JPROC.2021.3058584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AV1 video compression format is developed by the Alliance for Open Media consortium. It achieves more than a 30\% reduction in bit rate compared to its predecessor VP9 for the same decoded video quality. This article provides a technical overview of the AV1 codec design that enables the compression performance gains with considerations for hardware feasibility.},
  archive      = {J_PIEEE},
  author       = {Jingning Han and Bohan Li and Debargha Mukherjee and Ching-Han Chiang and Adrian Grange and Cheng Chen and Hui Su and Sarah Parker and Sai Deng and Urvang Joshi and Yue Chen and Yunqing Wang and Paul Wilkins and Yaowu Xu and James Bankoski},
  doi          = {10.1109/JPROC.2021.3058584},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1435-1462},
  shortjournal = {Proc. IEEE},
  title        = {A technical overview of AV1},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Special issue on open media compression: Overview, design
criteria, and outlook on emerging standards. <em>PIEEE</em>,
<em>109</em>(9), 1423–1434. (<a
href="https://doi.org/10.1109/JPROC.2021.3098048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal access to and provisioning of multimedia content is now a reality. It is easy to generate, distribute, share, and consume any multimedia content, anywhere, anytime, or any device. Open media standards took a crucial role toward enabling all these use cases leading to a plethora of applications and services that have now become a commodity in our daily life. Interestingly, most of these services adopt a streaming paradigm, are typically deployed over the open, unmanaged Internet, and account for most of today’s Internet traffic. Currently, the global video traffic is greater than 60\% of all Internet traffic [1] , and it is expected that this share will grow to more than 80\% in the near future [2] . In addition, Nielsen’s law of Internet bandwidth states that the users’ bandwidth grows by 50\% per year, which roughly fits data from 1983 to 2019 [3] . Thus, the users’ bandwidth can be expected to reach approximately 1 Gb/s by 2022. At the same time, network applications will grow and utilize the bandwidth provided, just like programs and their data expand to fill the memory available in a computer system. Most of the available bandwidth today is consumed by video applications, and the amount of data is further increasing due to already established and emerging applications, e.g., ultrahigh definition, high dynamic range, or virtual, augmented, mixed realities, or immersive media applications in general.},
  archive      = {J_PIEEE},
  author       = {Christian Timmerer and Mathias Wien and Lu Yu and Amy Reibman},
  doi          = {10.1109/JPROC.2021.3098048},
  journal      = {Proceedings of the IEEE},
  number       = {9},
  pages        = {1423-1434},
  shortjournal = {Proc. IEEE},
  title        = {Special issue on open media compression: Overview, design criteria, and outlook on emerging standards},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). IEEE women in engineering. <em>PIEEE</em>, <em>109</em>(8),
1420. (<a href="https://doi.org/10.1109/JPROC.2021.3094862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Women in Engineering.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3094862},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1420},
  shortjournal = {Proc. IEEE},
  title        = {IEEE women in engineering},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spintronics for energy- efficient computing: An overview and
outlook. <em>PIEEE</em>, <em>109</em>(8), 1398–1417. (<a
href="https://doi.org/10.1109/JPROC.2021.3084997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the discovery of giant magnetoresistance (GMR) to tunnel magnetoresistance (TMR), their subsequent application in large capacity hard disk drives (HDDs) greatly speeded up the information era over the past decades. However, the growing demand for big-data storage and processing is limited by the von-Neumann architecture due to the memory bottleneck and power dissipation. Taking advantage of nonvolatility, high speed, and low power, magnetic random access memory (MRAM) becomes a promising candidate to overcome this limitation through processing-in-memory (PIM) architectures. In this article, we provide an overview of existing technology and give a roadmap of spintronic devices for future energy-efficient computing and its relevant integration architectures. We begin with the fundamentals of Toggle-MRAM and spin-transfer torque (STT)-MRAM, which already have commercial applications. We then introduce spin-orbit torque (SOT), a critical mechanism to realize low-power data manipulation in the next generation of MRAM and summarize the recent experimental breakthroughs of field-free SOT switching schemes. Finally, we present MRAM-based PIM architectures and novel spintronic devices, provide an application outlook, and deliver the future development potential of energy-efficient computing systems.},
  archive      = {J_PIEEE},
  author       = {Zongxia Guo and Jialiang Yin and Yue Bai and Daoqian Zhu and Kewen Shi and Gefei Wang and Kaihua Cao and Weisheng Zhao},
  doi          = {10.1109/JPROC.2021.3084997},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1398-1417},
  shortjournal = {Proc. IEEE},
  title        = {Spintronics for energy- efficient computing: An overview and outlook},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Review of nanocomposite dielectric materials with high
thermal conductivity. <em>PIEEE</em>, <em>109</em>(8), 1364–1397. (<a
href="https://doi.org/10.1109/JPROC.2021.3085836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dielectric materials with high thermal conductivity (TC) can enable disruptive performance enhancement in the areas of electronics packaging, thermal management, energy storage, cabling, and heat sinks. There have been widespread global efforts over the past decade on developing novel nanocomposite dielectric materials. As a baseline, legacy polymers and epoxies used in the abovementioned applications have very low thermal conductivities ranging from 0.1-0.5 W·m -1 ·K -1 . Recent advances have led to the commercial availability of polymeric materials with thermal conductivities approaching 10 W·m -1 ·K -1 . Importantly, several fundamental studies report novel nanocomposites with thermal conductivities &gt; 50 W·m -1 ·K -1 . This article summarizes progress in the development of such materials with a focus on developments that show promise for improved practical dielectrics. This review highlights that high TC alone is inadequate to characterize the suitability of any material for the above applications. Other thermal properties, such as thermal diffusivity, glass transition temperature, and the ratio of the in-plane to out-of-plane TC, are important to quantify the thermal performance of novel nanocomposites. In addition, characterization and understanding of mechanical properties (coefficient of thermal expansion (CTE), tensile strength and elastic modulus) and electrical properties (dielectric strength and dielectric permittivity) are critical for holistic multifunctional assessments of these materials. There are other parameters and properties that influence performance, life, and manufacturability, such as viscosity and moisture absorption. This study reviews all the above aspects of nanocomposite dielectric materials reported in the literature. More specifically, we analyze various filler-polymer combinations, and the influence of approaches to incorporate fillers in the polymer on the thermal, mechanical, and electrical properties. While the addition of fillers leads to huge enhancements in TC, the TC is highly anisotropic, with out-of-plane TC lower than in-plane TC by an order of magnitude. It is seen that most present-day materials are still inadequate for future applications due to their low glass transition temperatures; specific promising materials are highlighted. While the addition of fillers reduces the CTE, further reduction is needed to favorably improve the mechanical performance of these materials. While the electrical insulating properties of these composite materials are adequate, there is very little data reported on other electrical properties. In summary, while there is an understandable focus on enhancing the TC, other properties are underreported, and there is insufficient information to support the assessment of most novel materials for practical applications. Overall, this study summarizes the state-of-the-art dielectric nanocomposites and outlines directions for future research to bridge the gap between basic materials science and applications.},
  archive      = {J_PIEEE},
  author       = {Manojkumar Lokanathan and Palash V. Acharya and Abdelhamid Ouroua and Shannon M. Strank and Robert E. Hebner and Vaibhav Bahadur},
  doi          = {10.1109/JPROC.2021.3085836},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1364-1397},
  shortjournal = {Proc. IEEE},
  title        = {Review of nanocomposite dielectric materials with high thermal conductivity},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed fusion of heterogeneous remote sensing and
social media data: A review and new developments. <em>PIEEE</em>,
<em>109</em>(8), 1350–1363. (<a
href="https://doi.org/10.1109/JPROC.2021.3079176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the wide availability of remote sensing big data from numerous different Earth Observation (EO) instruments, the limitations in the spatial and temporal resolution of such EO sensors (as well as atmospheric opacity and other kinds of interferers) have led to many situations in which using only remote sensing data cannot fully meet the requirements of applications in which a (near) real-time response is needed. Examples of these applications include floods, earthquakes, and other kinds of natural disasters, such as typhoons. To address this issue, social media data have gradually been adopted to fill possible gaps in the analysis when remote sensing data are lacking or incomplete. In this case, the fusion of heterogeneous big data streams from multiple data sources introduces significant demands from a computational viewpoint. In order to meet these challenges, distributed computing is increasingly viewed as a feasible solution to parallelize the analysis of massive data coming from different sources (e.g., remote sensing and social media data). In this article, we provide an overview of available and new distributed strategies to address the computational challenges brought by massive heterogeneous data processing and fusion for real-time environmental monitoring and decision-making. The 2013 Boulder (Colorado) flood event is taken as a case study to evaluate several new distributed data fusion frameworks. Experimental results demonstrate that the proposed distributed frameworks are suitable in terms of response time and computational requirements for fusing large-volume heterogeneous data sources.},
  archive      = {J_PIEEE},
  author       = {Jun Li and Zhenjie Liu and Xinya Lei and Lizhe Wang},
  doi          = {10.1109/JPROC.2021.3079176},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1350-1363},
  shortjournal = {Proc. IEEE},
  title        = {Distributed fusion of heterogeneous remote sensing and social media data: A review and new developments},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed deep learning for remote sensing data
interpretation. <em>PIEEE</em>, <em>109</em>(8), 1320–1349. (<a
href="https://doi.org/10.1109/JPROC.2021.3063258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a newly emerging technology, deep learning (DL) is a very promising field in big data applications. Remote sensing often involves huge data volumes obtained daily by numerous in-orbit satellites. This makes it a perfect target area for data-driven applications. Nowadays, technological advances in terms of software and hardware have a noticeable impact on Earth observation applications, more specifically in remote sensing techniques and procedures, allowing for the acquisition of data sets with greater quality at higher acquisition ratios. This results in the collection of huge amounts of remotely sensed data, characterized by their large spatial resolution (in terms of the number of pixels per scene), and very high spectral dimensionality, with hundreds or even thousands of spectral bands. As a result, remote sensing instruments on spaceborne and airborne platforms are now generating data cubes with extremely high dimensionality, imposing several restrictions in terms of both processing runtimes and storage capacity. In this article, we provide a comprehensive review of the state of the art in DL for remote sensing data interpretation, analyzing the strengths and weaknesses of the most widely used techniques in the literature, as well as an exhaustive description of their parallel and distributed implementations (with a particular focus on those conducted using cloud computing systems). We also provide quantitative results, offering an assessment of a DL technique in a specific case study (source code available: https://github.com/mhaut/cloud-dnn-HSI). This article concludes with some remarks and hints about future challenges in the application of DL techniques to distributed remote sensing data interpretation problems. We emphasize the role of the cloud in providing a powerful architecture that is now able to manage vast amounts of remotely sensed data due to its implementation simplicity, low cost, and high efficiency compared to other parallel and distributed architectures, such as grid computing or dedicated clusters.},
  archive      = {J_PIEEE},
  author       = {Juan M. Haut and Mercedes E. Paoletti and Sergio Moreno-Álvarez and Javier Plaza and Juan-Antonio Rico-Gallego and Antonio Plaza},
  doi          = {10.1109/JPROC.2021.3063258},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1320-1349},
  shortjournal = {Proc. IEEE},
  title        = {Distributed deep learning for remote sensing data interpretation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel and distributed computing for anomaly detection
from hyperspectral remote sensing imagery. <em>PIEEE</em>,
<em>109</em>(8), 1306–1319. (<a
href="https://doi.org/10.1109/JPROC.2021.3076455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection from remote sensing images is to detect pixels whose spectral signatures are different from their background. Anomalies are often man-made targets. With such target signatures being unknown, anomaly detection has many important applications, such as water quality monitoring, crop stress surveying, and law enforcement-related uses, where prior information of targets is often unavailable. The key to success is accurate background modeling. Anomaly detection from remote sensing images is challenging because spatial coverage is very large and the background is highly heterogeneous. For pixel-based anomaly detection, computing cost in background modeling and a spatial-convolution-type detection process is very expensive. Thus, parallel and distributed computing is critical in reducing execution time, which can fit the need for real-time or near real-time detection from airborne and spaceborne platforms in support of immediate decision-making. This article reviews the recent advances in anomaly detection from hyperspectral remote sensing images and their implementation using parallel and distributed systems. The classical methods, i.e., the Reed–Xiaoli (RX) algorithm and its variants, including its real-time processing version, are illustrated in commodity graphic processing units (GPUs), cloud, and field-programmable gate array (FPGA) implementations. Practical issues and future development trends are also discussed.},
  archive      = {J_PIEEE},
  author       = {Qian Du and Bo Tang and Weiying Xie and Wei Li},
  doi          = {10.1109/JPROC.2021.3076455},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1306-1319},
  shortjournal = {Proc. IEEE},
  title        = {Parallel and distributed computing for anomaly detection from hyperspectral remote sensing imagery},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recent developments in parallel and distributed computing
for remotely sensed big data processing. <em>PIEEE</em>,
<em>109</em>(8), 1282–1305. (<a
href="https://doi.org/10.1109/JPROC.2021.3087029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article gives a survey of state-of-the-art methods for processing remotely sensed big data and thoroughly investigates existing parallel implementations on diverse popular high-performance computing platforms. The pros/cons of these approaches are discussed in terms of capability, scalability, reliability, and ease of use. Among existing distributed computing platforms, cloud computing is currently the most promising solution to efficient and scalable processing of remotely sensed big data due to its advanced capabilities for high-performance and service-oriented computing. We further provide an in-depth analysis of state-of-the-art cloud implementations that seek for exploiting the parallelism of distributed processing of remotely sensed big data. In particular, we study a series of scheduling algorithms (GSs) aimed at distributing the computation load across multiple cloud computing resources in an optimized manner. We conduct a thorough review of different GSs and reveal the significance of employing scheduling strategies to fully exploit parallelism during the remotely sensed big data processing flow. We present a case study on large-scale remote sensing datasets to evaluate the parallel and distributed approaches and algorithms. Evaluation results demonstrate the advanced capabilities of cloud computing in processing remotely sensed big data and the improvements in computational efficiency obtained by employing scheduling strategies.},
  archive      = {J_PIEEE},
  author       = {Zebin Wu and Jin Sun and Yi Zhang and Zhihui Wei and Jocelyn Chanussot},
  doi          = {10.1109/JPROC.2021.3087029},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1282-1305},
  shortjournal = {Proc. IEEE},
  title        = {Recent developments in parallel and distributed computing for remotely sensed big data processing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed computing for remotely sensed data processing
[scanning the section]. <em>PIEEE</em>, <em>109</em>(8), 1278–1281. (<a
href="https://doi.org/10.1109/JPROC.2021.3094335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section investigates the state-of-the-art in the analysis and processing of remotely sensed big data employing distributed computing architectures.},
  archive      = {J_PIEEE},
  author       = {Jón Atli Benediktsson and Zebin Wu},
  doi          = {10.1109/JPROC.2021.3094335},
  journal      = {Proceedings of the IEEE},
  number       = {8},
  pages        = {1278-1281},
  shortjournal = {Proc. IEEE},
  title        = {Distributed computing for remotely sensed data processing [Scanning the section]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wide bandgap DC–DC converter topologies for power
applications. <em>PIEEE</em>, <em>109</em>(7), 1253–1275. (<a
href="https://doi.org/10.1109/JPROC.2021.3072170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, dc–dc power converters have attracted significant attention due to their increased use in a number of applications from aerospace to renewable energy. The interest in wide bandgap (WBG) power semiconductor devices stems from outstanding features of WBG materials, power device operation at higher temperatures, larger breakdown voltages, and the ability to sustain larger switching transients than silicon (Si) devices. As a result, recent progress and development of converter topologies, based on WBG power devices, are well-established for power conversion applications in which classical Si-based power devices show limited operation. Currently, Si carbide (SiC) and gallium nitride (GaN) are the most promising semiconductor materials that are being considered for the new generation of power devices. The use of new power semiconductor devices, such as GaN high electron mobility transistors (GaN HEMTs), leads to minimization of switching losses, allowing high switching frequencies (from kHz to MHz) for realizing compact power converters. Finally, design recommendations and future research trends are also presented.},
  archive      = {J_PIEEE},
  author       = {Mohammad Parvez and Aaron T. Pereira and Nesimi Ertugrul and Neil H. E. Weste and Derek Abbott and Said F. Al-Sarawi},
  doi          = {10.1109/JPROC.2021.3072170},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {1253-1275},
  shortjournal = {Proc. IEEE},
  title        = {Wide bandgap DC–DC converter topologies for power applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New trends in stochastic geometry for wireless networks: A
tutorial and survey. <em>PIEEE</em>, <em>109</em>(7), 1200–1252. (<a
href="https://doi.org/10.1109/JPROC.2021.3061778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation wireless networks are expected to be highly heterogeneous, multilayered, with embedded intelligence at both the core and edge of the network. In such a context, system-level performance evaluation will be very important to formulate relevant insights into tradeoffs that govern such a complex system. Over the past decade, SG has emerged as a powerful analytical tool to evaluate the system-level performance of wireless networks and capture their tendency toward heterogeneity. However, with the imminent onset of this crucial new decade, where global commercialization of fifth generation (5G) is expected to emerge and essential research questions related to beyond 5G (B5G) are intended to be identified, we are wondering about the role that a powerful tool, such as SG, should play. In this article, we first aim to track and summarize the novel SG models and techniques developed during the last decade in the evaluation of wireless networks. Next, we will outline how SG has been used to capture the properties of emerging RANs for 5G/B5G and quantify the benefits of key enabling technologies. Finally, we will discuss new horizons that will breathe new life into the use of SG in the foreseeable future, for instance, using SG to evaluate performance metrics in the visionary paradigm of molecular communications. Also, we will review how SG is envisioned to cooperate with machine learning that is seen as a crucial component in the race toward ubiquitous wireless intelligence. Another important insight is Grothendieck’s toposes, which is considered as a powerful mathematical concept that can help to solve long-standing problems formulated in SG.},
  archive      = {J_PIEEE},
  author       = {Yassine Hmamouche and Mustapha Benjillali and Samir Saoudi and Halim Yanikomeroglu and Marco Di Renzo},
  doi          = {10.1109/JPROC.2021.3061778},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {1200-1252},
  shortjournal = {Proc. IEEE},
  title        = {New trends in stochastic geometry for wireless networks: A tutorial and survey},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 6G wireless systems: Vision, requirements, challenges,
insights, and opportunities. <em>PIEEE</em>, <em>109</em>(7), 1166–1199.
(<a href="https://doi.org/10.1109/JPROC.2021.3061701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile communications have been undergoing a generational change every ten years or so. However, the time difference between the so-called “G&#39;s” is also decreasing. While fifth-generation (5G) systems are becoming a commercial reality, there is already significant interest in systems beyond 5G, which we refer to as the sixth generation (6G) of wireless systems. In contrast to the already published papers on the topic, we take a top-down approach to 6G. More precisely, we present a holistic discussion of 6G systems beginning with lifestyle and societal changes driving the need for next-generation networks. This is followed by a discussion into the technical requirements needed to enable 6G applications, based on which we dissect key challenges and possibilities for practically realizable system solutions across all layers of the Open Systems Interconnection stack (i.e., from applications to the physical layer). Since many of the 6G applications will need access to an order-of-magnitude more spectrum, utilization of frequencies between 100 GHz and 1 THz becomes of paramount importance. As such, the 6G ecosystem will feature a diverse range of frequency bands, ranging from below 6 GHz up to 1 THz. We comprehensively characterize the limitations that must be overcome to realize working systems in these bands and provide a unique perspective on the physical and higher layer challenges relating to the design of next-generation core networks, new modulation and coding methods, novel multiple-access techniques, antenna arrays, wave propagation, radio frequency transceiver design, and real-time signal processing. We rigorously discuss the fundamental changes required in the core networks of the future, such as the redesign or significant reduction of the transport architecture that serves as a major source of latency for time-sensitive applications. This is in sharp contrast to the present hierarchical network architectures that are not suitable to realize many of the anticipated 6G services. While evaluating the strengths and weaknesses of key candidate 6G technologies, we differentiate what may be practically achievable over the next decade, relative to what is possible in theory. Keeping this in mind, we present concrete research challenges for each of the discussed system aspects, providing inspiration for what follows.},
  archive      = {J_PIEEE},
  author       = {Harsh Tataria and Mansoor Shafi and Andreas F. Molisch and Mischa Dohler and Henrik Sjöland and Fredrik Tufvesson},
  doi          = {10.1109/JPROC.2021.3061701},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {1166-1199},
  shortjournal = {Proc. IEEE},
  title        = {6G wireless systems: Vision, requirements, challenges, insights, and opportunities},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Swarm robotics: Past, present, and future [point of view].
<em>PIEEE</em>, <em>109</em>(7), 1152–1165. (<a
href="https://doi.org/10.1109/JPROC.2021.3072740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm robotics deals with the design, construction, and deployment of large groups of robots that coordinate and cooperatively solve a problem or perform a task. It takes inspiration from natural self-organizing systems, such as social insects, fish schools, or bird flocks, characterized by emergent collective behavior based on simple local interaction rules [1], [2]. Typically, swarm robotics extracts engineering principles from the study of those natural systems in order to provide multirobot systems with comparable abilities. This way, it aims to build systems that are more robust, fault-tolerant, and flexible than single robots and that can better adapt their behavior to changes in the environment.},
  archive      = {J_PIEEE},
  author       = {Marco Dorigo and Guy Theraulaz and Vito Trianni},
  doi          = {10.1109/JPROC.2021.3072740},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {1152-1165},
  shortjournal = {Proc. IEEE},
  title        = {Swarm robotics: Past, present, and future [Point of view]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Scanning the issue. <em>PIEEE</em>, <em>109</em>(7),
1150–1151. (<a
href="https://doi.org/10.1109/JPROC.2021.3086510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles in this month’s issue touch upon the design of 6G wireless systems, novel stochastic geometry tools for wireless networks and wide bandgap dc–dc converter topologies.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3086510},
  journal      = {Proceedings of the IEEE},
  number       = {7},
  pages        = {1150-1151},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Recruit a member. <em>PIEEE</em>, <em>109</em>(6), 1148.
(<a href="https://doi.org/10.1109/JPROC.2021.3077182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3077182},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1148},
  shortjournal = {Proc. IEEE},
  title        = {Recruit a member},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). IEEE women in engineering. <em>PIEEE</em>, <em>109</em>(6),
1147. (<a href="https://doi.org/10.1109/JPROC.2021.3077180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Women in Engineering.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3077180},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1147},
  shortjournal = {Proc. IEEE},
  title        = {IEEE women in engineering},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Electric vehicles for smart buildings: A survey on
applications, energy management methods, and battery degradation.
<em>PIEEE</em>, <em>109</em>(6), 1128–1144. (<a
href="https://doi.org/10.1109/JPROC.2020.3038585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plug-in electric vehicles (PEVs) have the highest promise for dramatically reducing transportation emissions. No other option has comparable emission reduction potential or as a promising pathway. Still, PEVs can offer more than green transportation. In particular, their onboard storage can further serve the society by providing an energy buffer to increase the reliability, affordability, and sustainability of electric services. These benefits are only achievable by fully exploiting the multifaceted flexibility provided by PEVs&#39; mobility, charging adaptability, and bidirectional flow of power, as well as adopting effective decision-making and control algorithms, while minding the likely unfavorable side effects, such as shortened battery life span. This work takes a closer look at different elements of this puzzle. The main subject of this survey is behind the meter energy management with vehicle to building (V2B). We focus on different V2B application ideas and review energy management methods in smart buildings with V2B integration. Recent findings on battery capacity fade resulting from the bidirectional flow of power and extra discharging cycles with V2B are reviewed, and the methods for integrating the battery degradation in energy management formulation are discussed. Finally, the main findings of this review and research gaps are summarized and clarified.},
  archive      = {J_PIEEE},
  author       = {Shima Nazari and Francesco Borrelli and Anna Stefanopoulou},
  doi          = {10.1109/JPROC.2020.3038585},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1128-1144},
  shortjournal = {Proc. IEEE},
  title        = {Electric vehicles for smart buildings: A survey on applications, energy management methods, and battery degradation},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Electric/hybrid-electric aircraft propulsion systems.
<em>PIEEE</em>, <em>109</em>(6), 1115–1127. (<a
href="https://doi.org/10.1109/JPROC.2021.3073291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of electric propulsion for transportation is not new; indeed, the first cars, nearly 200 years ago, were electric. However, our dependence on fossil fuels over the last 100 years is now being questioned, and as a global society, we are moving toward more-electric transportation solutions. Electric propulsion of aircraft is part of this trend, either all-electric or through a large variety of the proposed hybrid propulsion systems. This article considers some of these systems, their technological requirements, and the ongoing research and development in motors and drives necessary to make this technological change a feasible option for the future of passenger flight.},
  archive      = {J_PIEEE},
  author       = {Patrick Wheeler and Thusara Samith Sirimanna and Serhiy Bozhko and Kiruba S. Haran},
  doi          = {10.1109/JPROC.2021.3073291},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1115-1127},
  shortjournal = {Proc. IEEE},
  title        = {Electric/Hybrid-electric aircraft propulsion systems},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward holistic energy management strategies for fuel cell
hybrid electric vehicles in heavy-duty applications. <em>PIEEE</em>,
<em>109</em>(6), 1094–1114. (<a
href="https://doi.org/10.1109/JPROC.2021.3055136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing need to slow down climate change for environmental protection demands further advancements toward regenerative energy and sustainable mobility. While individual mobility applications are assumed to be satisfied with improving battery electric vehicles (BEVs), the growing sector of freight transport and heavy-duty applications requires alternative solutions to meet the requirements of long ranges and high payloads. Fuel cell hybrid electric vehicles (FCHEVs) emerge as a capable technology for high-energy applications. This technology comprises a fuel cell system (FCS) for energy supply combined with buffering energy storages, such as batteries or ultracapacitors. In this article, recent successful developments regarding FCHEVs in various heavy-duty applications are presented. Subsequently, an overview of the FCHEV drivetrain, its main components, and different topologies with an emphasis on heavy-duty trucks is given. In order to enable system layout optimization and energy management strategy (EMS) design, functionality and modeling approaches for the FCS, battery, ultracapacitor, and further relevant subsystems are briefly described. Afterward, common methodologies for EMSs are structured, presenting a new taxonomy for dynamic optimization-based EMSs from a control engineering perspective. Finally, the findings lead to a guideline toward holistic EMSs, encouraging the co-optimization of system design, and EMS development for FCHEVs. For the EMS, we propose a layered model predictive control (MPC) approach, which takes velocity planning, the mitigation of degradation effects, and the auxiliaries into account simultaneously.},
  archive      = {J_PIEEE},
  author       = {Thomas Rudolf and Tobias Schürmann and Stefan Schwab and Sören Hohmann},
  doi          = {10.1109/JPROC.2021.3055136},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1094-1114},
  shortjournal = {Proc. IEEE},
  title        = {Toward holistic energy management strategies for fuel cell hybrid electric vehicles in heavy-duty applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid and electric vehicle (HEV/EV) technologies for
off-road applications. <em>PIEEE</em>, <em>109</em>(6), 1077–1093. (<a
href="https://doi.org/10.1109/JPROC.2020.3045721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid and electric vehicle (HEV/EV) technology is reasonably mature at this time, with a few million vehicles around in the world, and there is a significant amount of literature in the public domain on this subject. However, there is not enough literature on the application of this technology for off-road vehicles, including construction equipment, other industrial utility vehicles, and nonautomotive applications, such as a locomotive, ships, or airborne vehicles. With this in mind, the author presents here the topic and its current status. In addition, the author discusses the issue related to the decision-making process before the above technology is introduced for any HEV/EV application so that one is assured that the technology will bring benefit if applied for a particular purpose.},
  archive      = {J_PIEEE},
  author       = {M. Abul Masrur},
  doi          = {10.1109/JPROC.2020.3045721},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1077-1093},
  shortjournal = {Proc. IEEE},
  title        = {Hybrid and electric vehicle (HEV/EV) technologies for off-road applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability of power electronic systems for EV/HEV
applications. <em>PIEEE</em>, <em>109</em>(6), 1060–1076. (<a
href="https://doi.org/10.1109/JPROC.2020.3031041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrification of the transportation sector is moving on at a fast pace. All car manufacturers have strong programs to electrify their car fleet to fulfill the demands of society and customers by offering carbon-neutral technologies to bring goods and persons from one location to another. Power electronics technology is, in this evolution, essential and also in a rapid development technology-wise. Some of the introduced technologies are quite mature, and the systems designed must have high reliability as they can be quite complicated from an electrical perspective. Therefore, this article focuses on the reliability of the used power electronic systems applied in electric vehicles (EVs) and hybrid EVs (HEVs). It introduces the reliability requirements and challenges given for the power electronics applied in EV/HEV applications. Then, the advances in power electronic components to address the reliability challenges are introduced as they individually contribute to the overall system reliability. The reliability-oriented design methodology is also discussed, including two examples: an EV onboard charger and the drive train inverter. Finally, an outlook in terms of research opportunities in power electronics reliability related to EV/HEVs is provided. It can be concluded that many topics are already well handled in terms of reliability, but issues related to complete new technology introduction are important to keep the focus on.},
  archive      = {J_PIEEE},
  author       = {Frede Blaabjerg and Huai Wang and Ionut Vernica and Bochen Liu and Pooya Davari},
  doi          = {10.1109/JPROC.2020.3031041},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1060-1076},
  shortjournal = {Proc. IEEE},
  title        = {Reliability of power electronic systems for EV/HEV applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Electric drive technology trends, challenges, and
opportunities for future electric vehicles. <em>PIEEE</em>,
<em>109</em>(6), 1039–1059. (<a
href="https://doi.org/10.1109/JPROC.2020.3046112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition to electric road transport technologies requires electric traction drive systems to offer improved performances and capabilities, such as fuel efficiency (in terms of MPGe, i.e., miles per gallon of gasoline-equivalent), extended range, and fast-charging options. The enhanced electrification and transformed mobility are translating to a demand for higher power and more efficient electric traction drive systems that lead to better fuel economy for a given battery charge. To accelerate the mass-market adoption of electrified transportation, the U.S. Department of Energy (DOE), in collaboration with the automotive industry, has announced the technical targets for light-duty electric vehicles (EVs) for 2025. This article discusses the electric drive technology trends for passenger electric and hybrid EVs with commercially available solutions in terms of materials, electric machine and inverter designs, maximum speed, component cooling, power density, and performance. The emerging materials and technologies for power electronics and electric motors are presented, identifying the challenges and opportunities for even more aggressive designs to meet the need for next-generation EVs. Some innovative drive and motor designs with the potential to meet the DOE 2025 targets are also discussed.},
  archive      = {J_PIEEE},
  author       = {Iqbal Husain and Burak Ozpineci and Md Sariful Islam and Emre Gurpinar and Gui-Jia Su and Wensong Yu and Shajjad Chowdhury and Lincoln Xue and Dhrubo Rahman and Raj Sahu},
  doi          = {10.1109/JPROC.2020.3046112},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1039-1059},
  shortjournal = {Proc. IEEE},
  title        = {Electric drive technology trends, challenges, and opportunities for future electric vehicles},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Status and gap in rechargeable lithium battery supply chain:
Importance of quantitative failure analysis. <em>PIEEE</em>,
<em>109</em>(6), 1029–1038. (<a
href="https://doi.org/10.1109/JPROC.2020.3047880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rechargeable lithium batteries (RLBs), including lithium-ion batteries (LIBs), are accelerating the electrification of transportation and grid energy storage. This transformation of the transportation and energy sector could bring more clean energy into our energy security. The RLB technology is growing rapidly in these sectors due to substantial cost reductions and mobility needs. Yet, the durability, reliability, and safety issues of RLB remain concerns due to the nature of high-energy content in RLB. The concern of limited resources of the critical materials to sustain the RLB use is also escalated. Reuse and recycling of RLB to extend the useful life and recovery of the critical materials become important. Here, we provide a critical review of these topics to give a timely assessment of the status and gap of the RLB technologies and their supply chain. A key concept to use a quantitative failure mode and effect analysis is proposed to help advance RLB design, development, manufacturing, and deployment. The approach can be a viable method to enable physical principle-based technology assessment, failure identification, quantification, and verification of reliability and safety issues in the RLB supply chain.},
  archive      = {J_PIEEE},
  author       = {Yulun Zhang and Ruby T. Nguyen and Boryann Liaw},
  doi          = {10.1109/JPROC.2020.3047880},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1029-1038},
  shortjournal = {Proc. IEEE},
  title        = {Status and gap in rechargeable lithium battery supply chain: Importance of quantitative failure analysis},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A critical review of advanced electric machines and control
strategies for electric vehicles. <em>PIEEE</em>, <em>109</em>(6),
1004–1028. (<a
href="https://doi.org/10.1109/JPROC.2020.3041417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation electrification has attracted much attention in modern society. Among all electrified transportation tools, electric vehicle (EV) is absolutely the one that has great potential to compete with and further take the place of traditional fossil fuel vehicles. This article is to outline and investigate advanced electric machines and their control strategies for EV applications. The key is not only to reveal new design ideas, topologies, structures, methodologies, control strategies, pros and cons, and foresight for advanced electric machines but also to fully integrate these ideas into practical EV applications. This critical review will clarify the development trends of electric machines and their controls.},
  archive      = {J_PIEEE},
  author       = {Chunhua Liu and K. T. Chau and Christopher H. T. Lee and Zaixin Song},
  doi          = {10.1109/JPROC.2020.3041417},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {1004-1028},
  shortjournal = {Proc. IEEE},
  title        = {A critical review of advanced electric machines and control strategies for electric vehicles},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revolution of electric vehicle charging technologies
accelerated by wide bandgap devices. <em>PIEEE</em>, <em>109</em>(6),
985–1003. (<a href="https://doi.org/10.1109/JPROC.2021.3071977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the state-of-the-art electric vehicle (EV) charging technologies that benefit from the wide bandgap (WBG) devices, which is regarded as the most significant revolution in the power electronics industry in the past few decades. First, the recent WBG device technology evolution, performance comparison, and reliability issues are introduced. Then, topology development, efficiency and power density boost, and cost reduction brought by the WBG devices for EV charging equipment, such as onboard chargers, fast-charging stations, and wireless chargers, are discussed. A figure of merit (FOM) for evaluating the performance of wireless chargers is also proposed. Finally, the EV charging technology roadmap forecast is presented based on the WBG devices&#39; evolutionary trends.},
  archive      = {J_PIEEE},
  author       = {Siqi Li and Sizhao Lu and Chunting Chris Mi},
  doi          = {10.1109/JPROC.2021.3071977},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {985-1003},
  shortjournal = {Proc. IEEE},
  title        = {Revolution of electric vehicle charging technologies accelerated by wide bandgap devices},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State of the art and trends in electric and hybrid electric
vehicles. <em>PIEEE</em>, <em>109</em>(6), 967–984. (<a
href="https://doi.org/10.1109/JPROC.2021.3072788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric and hybrid electric vehicles (EV/HEV) are promising solutions for fossil fuel conservation and pollution reduction for a safe environment and sustainable transportation. The design of these energy-efficient powertrains requires optimization of components, systems, and controls. Controls entail battery management, fuel consumption, driver performance demand emissions, and management strategy. The hardware optimization entails powertrain architecture, transmission type, power electronic converters, and energy storage systems. In this overview, all these factors are addressed and reviewed. Major challenges and future technologies for EV/HEV are also discussed. Published suggestions and recommendations are surveyed and evaluated in this review. The outcomes of detailed studies are presented in tabular form to compare the strengths and weaknesses of various methods. Furthermore, issues in the current research are discussed, and suggestions toward further advancement of the technology are offered. This article analyzes current research and suggests challenges and scope of future research in EV/HEV and can serve as a reference for those working in this field.},
  archive      = {J_PIEEE},
  author       = {Mehrdad Ehsani and Krishna Veer Singh and Hari Om Bansal and Ramin Tafazzoli Mehrjardi},
  doi          = {10.1109/JPROC.2021.3072788},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {967-984},
  shortjournal = {Proc. IEEE},
  title        = {State of the art and trends in electric and hybrid electric vehicles},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Electric and hybrid vehicles. <em>PIEEE</em>,
<em>109</em>(6), 962–966. (<a
href="https://doi.org/10.1109/JPROC.2021.3075306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land transportation over the past two centuries has experienced astonishing advancement. Up until the 1860s, it took more than six months to get from the East Coast to the West Coast of the United States. Today, it may take only three days by automobile. We are even considering flying cars and there are air-taxi startup companies that have announced going public [1] . Vehicle propulsion electrification is at the core of this modern land vehicle revolution. However, the concept of electric vehicle traction is not new.},
  archive      = {J_PIEEE},
  author       = {Mehrdad Ehsani and Chunting Chris Mi},
  doi          = {10.1109/JPROC.2021.3075306},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {962-966},
  shortjournal = {Proc. IEEE},
  title        = {Electric and hybrid vehicles},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Why engineers are right to avoid the quantum reality offered
by the orthodox theory? [Point of view]. <em>PIEEE</em>,
<em>109</em>(6), 955–961. (<a
href="https://doi.org/10.1109/JPROC.2021.3067110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are currently in the middle of a second quantum revolution where the rules discovered a century ago to understand the quantum world are being applied to develop new quantum technologies [1]. Yet, most of the understanding of quantum physics is developed from the orthodox (also known as the Copenhagen) interpretation of the quantum phenomena [2], [3]. However, the orthodox theory has important difficulties in providing an intuitive view of quantum technologies because it states that a quantum object has only real properties when it is measured by an observer. This view of the reality of quantum objects is unnatural to engineers and, thus, is ignored by them when analyzing their realworld devices. What is natural to engineers is to imagine that the reality of an object (their properties) is independent of whether the object is measured or not.},
  archive      = {J_PIEEE},
  author       = {Xavier Oriols and David K. Ferry},
  doi          = {10.1109/JPROC.2021.3067110},
  journal      = {Proceedings of the IEEE},
  number       = {6},
  pages        = {955-961},
  shortjournal = {Proc. IEEE},
  title        = {Why engineers are right to avoid the quantum reality offered by the orthodox theory? [point of view]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Brain-inspired learning on neuromorphic substrates.
<em>PIEEE</em>, <em>109</em>(5), 935–950. (<a
href="https://doi.org/10.1109/JPROC.2020.3045625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic hardware strives to emulate brain-like neural networks and thus holds the promise for scalable, low-power information processing on temporal data streams. Yet, to solve real-world problems, these networks need to be trained. However, training on neuromorphic substrates creates significant challenges due to the offline character and the required nonlocal computations of gradient-based learning algorithms. This article provides a mathematical framework for the design of practical online learning algorithms for neuromorphic substrates. Specifically, we show a direct connection between real-time recurrent learning (RTRL), an online algorithm for computing gradients in conventional recurrent neural networks (RNNs), and biologically plausible learning rules for training spiking neural networks (SNNs). Furthermore, we motivate a sparse approximation based on block-diagonal Jacobians, which reduces the algorithm&#39;s computational complexity, diminishes the nonlocal information requirements, and empirically leads to good learning performance, thereby improving its applicability to neuromorphic substrates. In summary, our framework bridges the gap between synaptic plasticity and gradient-based approaches from deep learning and lays the foundations for powerful information processing on future neuromorphic hardware systems.},
  archive      = {J_PIEEE},
  author       = {Friedemann Zenke and Emre O. Neftci},
  doi          = {10.1109/JPROC.2020.3045625},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {935-950},
  shortjournal = {Proc. IEEE},
  title        = {Brain-inspired learning on neuromorphic substrates},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advancing neuromorphic computing with loihi: A survey of
results and outlook. <em>PIEEE</em>, <em>109</em>(5), 911–934. (<a
href="https://doi.org/10.1109/JPROC.2021.3067593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep artificial neural networks apply principles of the brain&#39;s information processing that led to breakthroughs in machine learning spanning many problem domains. Neuromorphic computing aims to take this a step further to chips more directly inspired by the form and function of biological neural circuits, so they can process new knowledge, adapt, behave, and learn in real time at low power levels. Despite several decades of research, until recently, very few published results have shown that today&#39;s neuromorphic chips can demonstrate quantitative computational value. This is now changing with the advent of Intel&#39;s Loihi, a neuromorphic research processor designed to support a broad range of spiking neural networks with sufficient scale, performance, and features to deliver competitive results compared to state-of-the-art contemporary computing architectures. This survey reviews results that are obtained to date with Loihi across the major algorithmic domains under study, including deep learning approaches and novel approaches that aim to more directly harness the key features of spike-based neuromorphic hardware. While conventional feedforward deep neural networks show modest if any benefit on Loihi, more brain-inspired networks using recurrence, precise spike-timing relationships, synaptic plasticity, stochasticity, and sparsity perform certain computation with orders of magnitude lower latency and energy compared to state-of-the-art conventional approaches. These compelling neuromorphic networks solve a diverse range of problems representative of brain-like computation, such as event-based data processing, adaptive control, constrained optimization, sparse feature regression, and graph search.},
  archive      = {J_PIEEE},
  author       = {Mike Davies and Andreas Wild and Garrick Orchard and Yulia Sandamirskaya and Gabriel A. Fonseca Guerra and Prasad Joshi and Philipp Plank and Sumedh R. Risbud},
  doi          = {10.1109/JPROC.2021.3067593},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {911-934},
  shortjournal = {Proc. IEEE},
  title        = {Advancing neuromorphic computing with loihi: A survey of results and outlook},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational media intelligence: Human-centered machine
analysis of media. <em>PIEEE</em>, <em>109</em>(5), 891–910. (<a
href="https://doi.org/10.1109/JPROC.2020.3047978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Media is created by humans for humans to tell stories. There exists a natural and imminent need for creating human-centered media analytics to illuminate the stories being told and to understand their impact on individuals and society at large. An objective understanding of media content has numerous applications for different stakeholders, from creators to decision-/policy-makers to consumers. Advances in multimodal signal processing and machine learning (ML) can enable detailed and nuanced characterization of media content (of who, what, how, where, and why) at scale. They can also aid our understanding of the impact of media on a range of issues, including individual experiences, behavioral, cultural, and societal trends, and commercial outcomes. Modern deep learning models combined with audiovisual signal processing can analyze entertainment media, such as Film &amp; TV content to quantify gender, age, and race representations. This creates awareness in an objective way that was hitherto impossible. On the other hand, text mining and natural language processing allow nuanced understanding of language use and spoken interactions in media, such as News to track patterns and trends across different contexts. Moreover, advances in human sensing have enabled us to directly measure the influence of media on an individual’s physiology (and brain), while social media analysis enables tracking the societal impact of media content on different cross sections of the society. This article reviews representative methodologies and algorithms, tools, and systems advancing human-centered media understanding through ML in the pursuit of developing computational media intelligence.},
  archive      = {J_PIEEE},
  author       = {Krishna Somandepalli and Tanaya Guha and Victor R. Martinez and Naveen Kumar and Hartwig Adam and Shrikanth Narayanan},
  doi          = {10.1109/JPROC.2020.3047978},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {891-910},
  shortjournal = {Proc. IEEE},
  title        = {Computational media intelligence: Human-centered machine analysis of media},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tensor methods in computer vision and deep learning.
<em>PIEEE</em>, <em>109</em>(5), 863–890. (<a
href="https://doi.org/10.1109/JPROC.2021.3074329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase in tensor methods, especially in deep learning architectures and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of this article and implementing them, step-by-step with TensorLy.},
  archive      = {J_PIEEE},
  author       = {Yannis Panagakis and Jean Kossaifi and Grigorios G. Chrysos and James Oldfield and Mihalis A. Nicolaou and Anima Anandkumar and Stefanos Zafeiriou},
  doi          = {10.1109/JPROC.2021.3074329},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {863-890},
  shortjournal = {Proc. IEEE},
  title        = {Tensor methods in computer vision and deep learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative adversarial networks for image and video
synthesis: Algorithms and applications. <em>PIEEE</em>, <em>109</em>(5),
839–862. (<a href="https://doi.org/10.1109/JPROC.2021.3049196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.},
  archive      = {J_PIEEE},
  author       = {Ming-Yu Liu and Xun Huang and Jiahui Yu and Ting-Chun Wang and Arun Mallya},
  doi          = {10.1109/JPROC.2021.3049196},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {839-862},
  shortjournal = {Proc. IEEE},
  title        = {Generative adversarial networks for image and video synthesis: Algorithms and applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A review of deep learning in medical imaging: Imaging
traits, technology trends, case studies with progress highlights, and
future promises. <em>PIEEE</em>, <em>109</em>(5), 820–838. (<a
href="https://doi.org/10.1109/JPROC.2021.3054390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its renaissance, deep learning (DL) has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high-performance computing. However, medical imaging presents unique challenges that confront DL approaches. In this survey article, we first present traits of medical imaging, highlight both clinical needs and technical challenges in medical imaging, and describe how emerging trends in DL are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, and so on. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.},
  archive      = {J_PIEEE},
  author       = {S. Kevin Zhou and Hayit Greenspan and Christos Davatzikos and James S. Duncan and Bram Van Ginneken and Anant Madabhushi and Jerry L. Prince and Daniel Rueckert and Ronald M. Summers},
  doi          = {10.1109/JPROC.2021.3054390},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {820-838},
  shortjournal = {Proc. IEEE},
  title        = {A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communication-efficient and distributed learning over
wireless networks: Principles and applications. <em>PIEEE</em>,
<em>109</em>(5), 796–819. (<a
href="https://doi.org/10.1109/JPROC.2021.3055679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is a promising enabler for the fifth-generation (5G) communication systems and beyond. By imbuing intelligence into the network edge, edge nodes can proactively carry out decision-making and, thereby, react to local environmental changes and disturbances while experiencing zero communication latency. To achieve this goal, it is essential to cater for high ML inference accuracy at scale under the time-varying channel and network dynamics, by continuously exchanging fresh data and ML model updates in a distributed way. Taming this new kind of data traffic boils down to improving the communication efficiency of distributed learning by optimizing communication payload types, transmission techniques, and scheduling, as well as ML architectures, algorithms, and data processing methods. To this end, this article aims to provide a holistic overview of relevant communication and ML principles and, thereby, present communication-efficient and distributed learning frameworks with selected use cases.},
  archive      = {J_PIEEE},
  author       = {Jihong Park and Sumudu Samarakoon and Anis Elgabli and Joongheon Kim and Mehdi Bennis and Seong-Lyun Kim and Mérouane Debbah},
  doi          = {10.1109/JPROC.2021.3055679},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {796-819},
  shortjournal = {Proc. IEEE},
  title        = {Communication-efficient and distributed learning over wireless networks: Principles and applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unifying review of deep and shallow anomaly detection.
<em>PIEEE</em>, <em>109</em>(5), 756–795. (<a
href="https://doi.org/10.1109/JPROC.2021.3052449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic “shallow” and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
  archive      = {J_PIEEE},
  author       = {Lukas Ruff and Jacob R. Kauffmann and Robert A. Vandermeulen and Grégoire Montavon and Wojciech Samek and Marius Kloft and Thomas G. Dietterich and Klaus-Robert Müller},
  doi          = {10.1109/JPROC.2021.3052449},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {756-795},
  shortjournal = {Proc. IEEE},
  title        = {A unifying review of deep and shallow anomaly detection},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tropical geometry and machine learning. <em>PIEEE</em>,
<em>109</em>(5), 728–755. (<a
href="https://doi.org/10.1109/JPROC.2021.3065238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tropical geometry is a relatively recent field in mathematics and computer science, combining elements of algebraic geometry and polyhedral geometry. The scalar arithmetic of its analytic part preexisted in the form of max-plus and min-plus semiring arithmetic used in finite automata, nonlinear image processing, convex analysis, nonlinear control, optimization, and idempotent mathematics. Tropical geometry recently emerged in the analysis and extension of several classes of problems and systems in both classical machine learning and deep learning. Three such areas include: 1) deep neural networks with piecewise linear (PWL) activation functions; 2) probabilistic graphical models; and 3) nonlinear regression with PWL functions. In this article, we first summarize introductory ideas and objects of tropical geometry, providing a theoretical framework for both the max-plus algebra that underlies tropical geometry and its extensions to general max algebras. This unifies scalar and vector/signal operations over a class of nonlinear spaces, called weighted lattices, and allows us to provide optimal solutions for algebraic equations used in tropical geometry and generalize tropical geometric objects. Then, we survey the state of the art and recent progress in the aforementioned areas. First, we illustrate a purely geometric approach for studying the representation power of neural networks with PWL activations. Then, we review the tropical geometric analysis of parametric statistical models, such as HMMs; later, we focus on the Viterbi algorithm and related methods for weighted finite-state transducers and provide compact and elegant representations via their formal tropical modeling. Finally, we provide optimal solutions and an efficient algorithm for the convex regression problem, using concepts and tools from tropical geometry and max-plus algebra. Throughout this article, we also outline problems and future directions in machine learning that can benefit from the tropical-geometric point of view.},
  archive      = {J_PIEEE},
  author       = {Petros Maragos and Vasileios Charisopoulos and Emmanouil Theodosis},
  doi          = {10.1109/JPROC.2021.3065238},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {728-755},
  shortjournal = {Proc. IEEE},
  title        = {Tropical geometry and machine learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mad max: Affine spline insights into deep learning.
<em>PIEEE</em>, <em>109</em>(5), 704–727. (<a
href="https://doi.org/10.1109/JPROC.2020.3042100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs) that provide a powerful portal through which we view and analyze their inner workings. For instance, conditioned on the spline partition region containing the input signal, the output of an MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space that is implicitly induced by an MASO directly links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up new geometric avenues to study how DNs organize signals in a hierarchical fashion. To validate the utility of the VQ interpretation, we develop and validate a new distance metric for signals and images that quantify the difference between their VQ encodings.},
  archive      = {J_PIEEE},
  author       = {Randall Balestriero and Richard G. Baraniuk},
  doi          = {10.1109/JPROC.2020.3042100},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {704-727},
  shortjournal = {Proc. IEEE},
  title        = {Mad max: Affine spline insights into deep learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mathematical models of overparameterized neural networks.
<em>PIEEE</em>, <em>109</em>(5), 683–703. (<a
href="https://doi.org/10.1109/JPROC.2020.3048020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has received considerable empirical success in recent years. However, while many ad hoc tricks have been discovered by practitioners, until recently, there has been a lack of theoretical understanding for tricks invented in the deep learning literature. Known by practitioners that overparameterized neural networks (NNs) are easy to learn, in the past few years, there have been important theoretical developments in the analysis of overparameterized NNs. In particular, it was shown that such systems behave like convex systems under various restricted settings, such as for two-layer NNs, and when learning is restricted locally in the so-called neural tangent kernel space around specialized initializations. This article discusses some of these recent signs of progress leading to a significantly better understanding of NNs. We will focus on the analysis of two-layer NNs and explain the key mathematical models, with their algorithmic implications. We will then discuss challenges in understanding deep NNs and some current research directions.},
  archive      = {J_PIEEE},
  author       = {Cong Fang and Hanze Dong and Tong Zhang},
  doi          = {10.1109/JPROC.2020.3048020},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {683-703},
  shortjournal = {Proc. IEEE},
  title        = {Mathematical models of overparameterized neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph neural networks: Architectures, stability, and
transferability. <em>PIEEE</em>, <em>109</em>(5), 660–682. (<a
href="https://doi.org/10.1109/JPROC.2021.3055400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are information processing architectures for signals supported on graphs. They are presented here as generalizations of convolutional neural networks (CNNs) in which individual layers contain banks of graph convolutional filters instead of banks of classical convolutional filters. Otherwise, GNNs operate as CNNs. Filters are composed of pointwise nonlinearities and stacked in layers. It is shown that GNN architectures exhibit equivariance to permutation and stability to graph deformations. These properties help explain the good performance of GNNs that can be observed empirically. It is also shown that if graphs converge to a limit object, a graphon, GNNs converge to a corresponding limit object, a graphon neural network. This convergence justifies the transferability of GNNs across networks with different numbers of nodes. Concepts are illustrated by the application of GNNs to recommendation systems, decentralized collaborative control, and wireless communication networks.},
  archive      = {J_PIEEE},
  author       = {Luana Ruiz and Fernando Gama and Alejandro Ribeiro},
  doi          = {10.1109/JPROC.2021.3055400},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {660-682},
  shortjournal = {Proc. IEEE},
  title        = {Graph neural networks: Architectures, stability, and transferability},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimism in the face of adversity: Understanding and
improving deep learning through adversarial robustness. <em>PIEEE</em>,
<em>109</em>(5), 635–659. (<a
href="https://doi.org/10.1109/JPROC.2021.3050042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by massive amounts of data and important advances in computational resources, new deep learning systems have achieved outstanding results in a large spectrum of applications. Nevertheless, our current theoretical understanding of the mathematical foundations of deep learning lags far behind its empirical success. However, the field of adversarial robustness has recently become one of the main sources of explanations of our deep models. In this article, we provide an in-depth review of the field and give a self-contained introduction to its main notions. However, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails. We highlight the intuitive connection between adversarial examples and the geometry of deep neural networks and, eventually, explore how the geometric study of adversarial examples can serve as a powerful tool to understand deep learning. Furthermore, we demonstrate the broad applicability of adversarial robustness, providing an overview of the main emerging applications of adversarial robustness beyond security. The goal of this article is to provide readers with a set of new perspectives to understand deep learning and supply them with intuitive tools and insights on how to use adversarial robustness to improve it.},
  archive      = {J_PIEEE},
  author       = {Guillermo Ortiz-Jiménez and Apostolos Modas and Seyed-Mohsen Moosavi-Dezfooli and Pascal Frossard},
  doi          = {10.1109/JPROC.2021.3050042},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {635-659},
  shortjournal = {Proc. IEEE},
  title        = {Optimism in the face of adversity: Understanding and improving deep learning through adversarial robustness},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward causal representation learning. <em>PIEEE</em>,
<em>109</em>(5), 612–634. (<a
href="https://doi.org/10.1109/JPROC.2021.3058954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  archive      = {J_PIEEE},
  author       = {Bernhard Schölkopf and Francesco Locatello and Stefan Bauer and Nan Rosemary Ke and Nal Kalchbrenner and Anirudh Goyal and Yoshua Bengio},
  doi          = {10.1109/JPROC.2021.3058954},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {612-634},
  shortjournal = {Proc. IEEE},
  title        = {Toward causal representation learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Advances in machine learning and deep neural networks.
<em>PIEEE</em>, <em>109</em>(5), 607–611. (<a
href="https://doi.org/10.1109/JPROC.2021.3072172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are currently experiencing the dawn of what is known as the fourth industrial revolution. At the center of this historical happening, as one of the key enabling technologies, lies a discipline that deals with data and whose goal is to extract information and related knowledge that is hidden in it, in order to make predictions and, subsequently, take decisions. Machine learning (ML) is the name that is used as an umbrella to cover a wide range of theories, methods, algorithms, and architectures that are used to this end. The articles in this special issue cover promising developments in the related areas of machine learning and deep neural networks and offers possible paths for the future.},
  archive      = {J_PIEEE},
  author       = {Rama Chellappa and Sergios Theodoridis and Andre van Schaik},
  doi          = {10.1109/JPROC.2021.3072172},
  journal      = {Proceedings of the IEEE},
  number       = {5},
  pages        = {607-611},
  shortjournal = {Proc. IEEE},
  title        = {Advances in machine learning and deep neural networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A platform programming paradigm for heterogeneous systems
integration. <em>PIEEE</em>, <em>109</em>(4), 582–603. (<a
href="https://doi.org/10.1109/JPROC.2020.3035874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with growing computing performance requirements, cyber-physical systems architectures are moving toward heterogeneous high-performance computer architectures and networks. Such architectures, however, incur intricate side effects that challenge traditional software design and integration. The programming paradigm can take a key role in mastering software design, as experience in automotive design demonstrates. To cope with the integration challenge, this industry has started introducing a programming paradigm that efficiently preserves application data flow under platform integration and changes with minimum performance loss. This article will revisit this paradigm that is currently used for lock-free multicore programming and explain its extension to the system level. It will then explore its application to two important developments in industrial design. This article will conclude with an evaluation of its properties, its overhead, and its application toward a robust design process.},
  archive      = {J_PIEEE},
  author       = {Kai-Björn Gemlau and Leonie Köhler and Rolf Ernst},
  doi          = {10.1109/JPROC.2020.3035874},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {582-603},
  shortjournal = {Proc. IEEE},
  title        = {A platform programming paradigm for heterogeneous systems integration},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A connective framework to support the lifecycle of
cyber–physical production systems. <em>PIEEE</em>, <em>109</em>(4),
568–581. (<a href="https://doi.org/10.1109/JPROC.2020.3046525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential benefits of the adoption of cyber–physical production systems (CPPSs) and their significant role in enabling smart manufacturing is well recognized today. However, it is less clear how such CPPS can be most effectively and consistently engineered and maintained throughout their lifecycle due to the existing divide in the information technology (IT) and operational technology (OT) landscape and ad hoc integration practices that result in inconsistent data and data models at various levels of manufacturing processes. The work presented in this article addresses this problem by envisioning a connective framework to support the engineering of CPPS through the use of a set of digital twins consistent with the real system throughout its lifecycle, not just used in the design and deployment phases. A review of the latest perspectives on using digital integration frameworks, methods, and solutions for lifecycle engineering of CPPS is provided in this article. This article demonstrates how a suitable framework, named SIMPLE, can be realized to effectively address the lack of consistent data models throughout the engineering lifecycle, including implementation details and example cases developed by the authors at the Warwick Manufacturing Group (WMG) in selected industrial sectors. Consideration is given to supporting cyber-to-physical systems’ connectivity and extendable engineering toolsets, forming the basis for multidisciplinary digital engineering environments. Key discussion points include the role and importance of effective integration of IT and OT, suitable frameworks for integration and collaboration.},
  archive      = {J_PIEEE},
  author       = {Robert Harrison and Daniel A. Vera and Bilal Ahmad},
  doi          = {10.1109/JPROC.2020.3046525},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {568-581},
  shortjournal = {Proc. IEEE},
  title        = {A connective framework to support the lifecycle of Cyber–Physical production systems},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A methodology for digital twin modeling and deployment for
industry 4.0. <em>PIEEE</em>, <em>109</em>(4), 556–567. (<a
href="https://doi.org/10.1109/JPROC.2020.3032444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital twin (DT) is a virtual representation of a physical object, which has been proposed as one of the key concepts for Industry 4.0. The DT provides a virtual representation of products along their lifecycle that enables the prediction and optimization of the behavior of a production system and its components. A methodology design using model-driven engineering (MDE) is proposed that strives toward being both flexible and generic. This approach is presented at two levels: first, a DT is modeled as a composition of basic components that provide basic functionalities, such as identification, storage, communication, security, data management, human-machine interface (HMI), and simulation; second, an aggregated DT is defined as a hierarchical composition of other DTs. A generic reference architecture based on these concepts and a concrete implementation methodology are proposed using AutomationML. This methodology follows an MDE approach that supports most of the DT features currently proposed in the literature. A case study has been developed, the proposed ideas are being evaluated with industrial case studies, and some of the preliminary results are described in this article. With the case study, it is possible to verify that the proposed methodology supports the creation and the deployment process of a DT.},
  archive      = {J_PIEEE},
  author       = {Greyce N. Schroeder and Charles Steinmetz and Ricardo Nagel Rodrigues and Renato Ventura Bayan Henriques and Achim Rettberg and Carlos Eduardo Pereira},
  doi          = {10.1109/JPROC.2020.3032444},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {556-567},
  shortjournal = {Proc. IEEE},
  title        = {A methodology for digital twin modeling and deployment for industry 4.0},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). (Re)deployment of smart algorithms in cyber–physical
production systems using DSL4hDNCS. <em>PIEEE</em>, <em>109</em>(4),
542–555. (<a href="https://doi.org/10.1109/JPROC.2021.3050860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent algorithms and learning are the basis for evolving smart cyber-physical production systems (CPPSs) and Industrie 4.0. A smart image detection algorithm shall be added to reduce downtime due to the extended operation of glass bottles in a yogurt producing plant. To support the engineers in doing so, a comprehensive domain-specific language (DSL), DSL4hDNCS, is introduced, enabling rapid analysis, addressing hardware/software architectures and network-related delays and uncertainties. DSL4hDNCS is defined by a metamodel to avoid ambiguity and enriched by aspects, such as safety, calculation power, and network transmission time. DSL4hDNCS is used to compare (re)deployment alternatives using different technologies, such as edge, fog, and cloud to implement the additional smart algorithm. The evaluation of DSL4hDNCS using an acknowledged Industrie 4.0 demonstrator plant as a case study confirmed the benefit for engineers during the redesign.},
  archive      = {J_PIEEE},
  author       = {Birgit Vogel-Heuser and Emanuel Trunzer and Dominik Hujo and Michael Sollfrank},
  doi          = {10.1109/JPROC.2021.3050860},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {542-555},
  shortjournal = {Proc. IEEE},
  title        = {(Re)deployment of smart algorithms in Cyber–Physical production systems using DSL4hDNCS},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified architectural approach for cyberattack-resilient
industrial control systems. <em>PIEEE</em>, <em>109</em>(4), 517–541.
(<a href="https://doi.org/10.1109/JPROC.2020.3034595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of functional requirements in the emerging Industry 4.0 era, modern industrial control systems (ICSs) are no longer isolated islands, making them more vulnerable to various cyberattack threats. Cyberattacks on ICSs may have disruptive consequences, such as significant social and economic losses. To proactively address the security issue of ICSs, this article presents a unified architectural approach from the perspectives of cyberthreats on ICSs, security-related ICS technologies, and methods for ICSs. It incorporates secure networks, secure control systems, secure physical processes, and their interactions seamlessly into a unified framework. To increase the resistance of ICSs against intrusions, the network security in our architectural approach is to secure the data in motion through the integration of secure network architecture, secure industrial network protocols, and secure end-to-end communications. The protection of control systems in our architectural approach is risk-based and hierarchical and encompasses prevention- and tolerance-centric defenses. It provides a layer-by-layer defense so that an acceptable level of cybersecurity risk is achieved and maintained. Aiming to maintain the stable operation of physical ICS processes, the secure control in our architectural approach implements a security process against process-aware attacks through a resilient safety control scheme. The global and systematic architectural approach presented in this article for the ICS cybersecurity will help facilitate the design and implementation of cyberattack-resilient ICSs in the networked world. For further development of ICS security technologies, emerging challenges are identified and discussed to motivate future research efforts.},
  archive      = {J_PIEEE},
  author       = {Chunjie Zhou and Bowen Hu and Yang Shi and Yu-Chu Tian and Xuan Li and Yue Zhao},
  doi          = {10.1109/JPROC.2020.3034595},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {517-541},
  shortjournal = {Proc. IEEE},
  title        = {A unified architectural approach for cyberattack-resilient industrial control systems},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of cybersecurity of digital manufacturing.
<em>PIEEE</em>, <em>109</em>(4), 495–516. (<a
href="https://doi.org/10.1109/JPROC.2020.3032074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industry 4.0 concept promotes a digital manufacturing (DM) paradigm that can enhance quality and productivity, which reduces inventory and the lead time for delivering custom, batch-of-one products based on achieving convergence of additive, subtractive, and hybrid manufacturing machines, automation and robotic systems, sensors, computing, and communication networks, artificial intelligence, and big data. A DM system consists of embedded electronics, sensors, actuators, control software, and interconnectivity to enable the machines and the components within them to exchange data with other machines, components therein, the plant operators, the inventory managers, and customers. This article presents the cybersecurity risks in the emerging DM context, assesses the impact on manufacturing, and identifies approaches to secure DM.},
  archive      = {J_PIEEE},
  author       = {Priyanka Mahesh and Akash Tiwari and Chenglu Jin and Panganamala R. Kumar and A. L. Narasimha Reddy and Satish T. S. Bukkapatanam and Nikhil Gupta and Ramesh Karri},
  doi          = {10.1109/JPROC.2020.3032074},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {495-516},
  shortjournal = {Proc. IEEE},
  title        = {A survey of cybersecurity of digital manufacturing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wireless networked multirobot systems in smart factories.
<em>PIEEE</em>, <em>109</em>(4), 468–494. (<a
href="https://doi.org/10.1109/JPROC.2020.3033753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart manufacturing based on artificial intelligence and information communication technology will become the main contributor to the digital economy of the upcoming decades. In order to execute flexible production, smart manufacturing must holistically integrate wireless networking, computing, and automatic control technologies. This article discusses the challenges of this complex system engineering from a wireless networking perspective. Starting from enabling flexible reconfiguration of a smart factory, we discuss existing wireless technology and the trends of wireless networking evolution to facilitate multirobot smart factories. Furthermore, the special sequential decision-making of a multirobot manufacturing system is examined. Social learning can be used to extend the resilience of precision operation in a multirobot system by taking network topology into consideration, which also introduces a new vision for the cybersecurity of smart factories. A summary of highlights of technological opportunities for holistic facilitation of wireless networked multirobot smart factories rounds off this article.},
  archive      = {J_PIEEE},
  author       = {Kwang-Cheng Chen and Shih-Chun Lin and Jen-Hao Hsiao and Chun-Hung Liu and Andreas F. Molisch and Gerhard P. Fettweis},
  doi          = {10.1109/JPROC.2020.3033753},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {468-494},
  shortjournal = {Proc. IEEE},
  title        = {Wireless networked multirobot systems in smart factories},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wireless control for smart manufacturing: Recent approaches
and open challenges. <em>PIEEE</em>, <em>109</em>(4), 441–467. (<a
href="https://doi.org/10.1109/JPROC.2020.3032633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart manufacturing aims to overcome the limitations of today&#39;s rigid assembly lines by making the material flow and manufacturing process more flexible, versatile, and scalable. The main economic drivers are higher resource and cost efficiency as the manufacturers can more quickly adapt to changing market needs and also increase the lifespan of their production sites. The ability to close feedback loops fast and reliably over long distances among mobile robots, remote sensors, and human operators is a key enabler for smart manufacturing. Thus, this article provides a perspective on control and coordination over wireless networks. Based on an analysis of real-world use cases, we identify the main technical challenges that need to be solved to close the large gap between the current state of the art in industry and the vision of smart manufacturing. We discuss to what extent existing control-over-wireless solutions in the literature address those challenges, including our own approach toward a tight integration of control and wireless communication. In addition to a theoretical analysis of closed-loop stability, practical experiments on a cyber-physical testbed demonstrate that our approach supports relevant smart manufacturing scenarios. This article concludes with a discussion of open challenges and future research directions.},
  archive      = {J_PIEEE},
  author       = {Dominik Baumann and Fabian Mager and Ulf Wetzker and Lothar Thiele and Marco Zimmerling and Sebastian Trimpe},
  doi          = {10.1109/JPROC.2020.3032633},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {441-467},
  shortjournal = {Proc. IEEE},
  title        = {Wireless control for smart manufacturing: Recent approaches and open challenges},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning-based automation of robotic assembly for smart
manufacturing. <em>PIEEE</em>, <em>109</em>(4), 423–440. (<a
href="https://doi.org/10.1109/JPROC.2021.3063154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For smart manufacturing, an automated robotic assembly system built upon an autoprogramming environment is necessary to reduce setup time and cost for robots that are engaged in frequent task reassignment. This article presents an approach to the autoprogramming of robotic assembly tasks with minimal human assistance. The approach integrates “robotic learning of assembly tasks from observation” and “robotic embodiment of learned assembly tasks in the form of skills.” In the former, robots observe human assembly operations to learn a sequence of assembly tasks, which is formalized into a human assembly script. The latter transforms the human assembly script into a robot assembly script in which a sequence of robot-executable assembly tasks are defined based on action planning supported by workspace modeling and simulated retargeting. The assembly tasks, in the form of the robot assembly script, are then implemented via pretrained robot skills. These skills aim to enable robots to execute difficult tasks that involve inherent uncertainties and variations. We validate the proposed approach by building a prototype of the automated robotic assembly system for a power breaker and an electronic set-top box. The results verify that the proposed automated robotic assembly system is not only feasible but also viable, as it is associated with a dramatic reduction in the human effort required for automating robotic assembly.},
  archive      = {J_PIEEE},
  author       = {Sanghoon Ji and Sukhan Lee and Sujeong Yoo and Ilhong Suh and Inso Kwon and Frank C. Park and Sanghyoung Lee and Hongseok Kim},
  doi          = {10.1109/JPROC.2021.3063154},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {423-440},
  shortjournal = {Proc. IEEE},
  title        = {Learning-based automation of robotic assembly for smart manufacturing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manufacturing as a data-driven practice: Methodologies,
technologies, and tools. <em>PIEEE</em>, <em>109</em>(4), 399–422. (<a
href="https://doi.org/10.1109/JPROC.2021.3056006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the introduction and exploitation of innovative information technologies in industrial contexts have led to the continuous growth of digital shop floor environments. The new Industry 4.0 model allows smart factories to become very advanced IT industries, generating an ever-increasing amount of valuable data. As a consequence, the necessity of powerful and reliable software architectures is becoming prominent along with data-driven methodologies to extract useful and hidden knowledge supporting the decision-making process. This article discusses the latest software technologies needed to collect, manage, and elaborate all data generated through innovative Internet-of-Things (IoT) architectures deployed over the production line, with the aim of extracting useful knowledge for the orchestration of high-level control services that can generate added business value. This survey covers the entire data life cycle in manufacturing environments, discussing key functional and methodological aspects along with a rich and properly classified set of technologies and tools, useful to add intelligence to data-driven services. Therefore, it serves both as a first guided step toward the rich landscape of the literature for readers approaching this field and as a global yet detailed overview of the current state of the art in the Industry 4.0 domain for experts. As a case study, we discuss, in detail, the deployment of the proposed solutions for two research project demonstrators, showing their ability to mitigate manufacturing line interruptions and reduce the corresponding impacts and costs.},
  archive      = {J_PIEEE},
  author       = {Tania Cerquitelli and Daniele Jahier Pagliari and Andrea Calimera and Lorenzo Bottaccioli and Edoardo Patti and Andrea Acquaviva and Massimo Poncino},
  doi          = {10.1109/JPROC.2021.3056006},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {399-422},
  shortjournal = {Proc. IEEE},
  title        = {Manufacturing as a data-driven practice: Methodologies, technologies, and tools},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial-intelligence-driven customized manufacturing
factory: Key technologies, applications, and challenges. <em>PIEEE</em>,
<em>109</em>(4), 377–398. (<a
href="https://doi.org/10.1109/JPROC.2020.3034808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional production paradigm of large batch production does not offer flexibility toward satisfying the requirements of individual customers. A new generation of smart factories is expected to support new multivariety and small-batch customized production modes. For this, artificial intelligence (AI) is enabling higher value-added manufacturing by accelerating the integration of manufacturing and information communication technologies, including computing, communication, and control. The characteristics of a customized smart factory are: self-perception, operations optimization, dynamic reconfiguration, and intelligent decision-making. The AI technologies will allow manufacturing systems to perceive the environment, adapt to the external needs, and extract the process knowledge, including business models, such as intelligent production, networked collaboration, and extended service models. This article focuses on the implementation of AI in customized manufacturing (CM). The architecture of an AI-driven customized smart factory is presented. Details of intelligent manufacturing devices, intelligent information interaction, and construction of a flexible manufacturing line are showcased. The state-of-the-art AI technologies of potential use in CM, that is, machine learning, multiagent systems, Internet of Things, big data, and cloud-edge computing, are surveyed. The AI-enabled technologies in a customized smart factory are validated with a case study of customized packaging. The experimental results have demonstrated that the AI-assisted CM offers the possibility of higher production flexibility and efficiency. Challenges and solutions related to AI in CM are also discussed.},
  archive      = {J_PIEEE},
  author       = {Jiafu Wan and Xiaomin Li and Hong-Ning Dai and Andrew Kusiak and Miguel Martínez-García and Di Li},
  doi          = {10.1109/JPROC.2020.3034808},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {377-398},
  shortjournal = {Proc. IEEE},
  title        = {Artificial-intelligence-driven customized manufacturing factory: Key technologies, applications, and challenges},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Six-sigma quality management of additive manufacturing.
<em>PIEEE</em>, <em>109</em>(4), 347–376. (<a
href="https://doi.org/10.1109/JPROC.2020.3034519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality is a key determinant in deploying new processes, products, or services and influences the adoption of emerging manufacturing technologies. The advent of additive manufacturing (AM) as a manufacturing process has the potential to revolutionize a host of enterprise-related functions from production to the supply chain. The unprecedented level of design flexibility and expanded functionality offered by AM, coupled with greatly reduced lead times, can potentially pave the way for mass customization. However, widespread application of AM is currently hampered by technical challenges in process repeatability and quality management. The breakthrough effect of six sigma (6S) has been demonstrated in traditional manufacturing industries (e.g., semiconductor and automotive industries) in the context of quality planning, control, and improvement through the intensive use of data, statistics, and optimization. 6S entails a data-driven DMAIC methodology of five steps—define, measure, analyze, improve, and control. Notwithstanding the sustained successes of the 6S knowledge body in a variety of established industries ranging from manufacturing, healthcare, logistics, and beyond, there is a dearth of concentrated application of 6S quality management approaches in the context of AM. In this article, we propose to design, develop, and implement the new DMAIC methodology for the 6S quality management of AM. First, we define the specific quality challenges arising from AM layerwise fabrication and mass customization (even one-of-a-kind production). Second, we present a review of AM metrology and sensing techniques, from materials through design, process, and environment, to postbuild inspection. Third, we contextualize a framework for realizing the full potential of data from AM systems and emphasize the need for analytical methods and tools. We propose and delineate the utility of new data-driven analytical methods, including deep learning, machine learning, and network science, to characterize and model the interrelationships between engineering design, machine setting, process variability, and final build quality. Fourth, we present the methodologies of ontology analytics, design of experiments (DOE), and simulation analysis for AM system improvements. In closing, new process control approaches are discussed to optimize the action plans, once an anomaly is detected, with specific consideration of lead time and energy consumption. We posit that this work will catalyze more in-depth investigations and multidisciplinary research efforts to accelerate the application of 6S quality management in AM.},
  archive      = {J_PIEEE},
  author       = {Hui Yang and Prahalad Rao and Timothy Simpson and Yan Lu and Paul Witherell and Abdalla R. Nassar and Edward Reutzel and Soundar Kumara},
  doi          = {10.1109/JPROC.2020.3034519},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {347-376},
  shortjournal = {Proc. IEEE},
  title        = {Six-sigma quality management of additive manufacturing},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing quality inspection and control in powder bed
metal additive manufacturing: Challenges and research directions.
<em>PIEEE</em>, <em>109</em>(4), 326–346. (<a
href="https://doi.org/10.1109/JPROC.2021.3054628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key targets of Industry 4.0 and digital production, in general, is the support of faster, cleaner, and increasingly customizable manufacturing processes. Additive manufacturing (AM) is a natural fit in this context, as it offers the possibility to produce complex parts without the design constraints of traditional manufacturing routes, typically reducing both material waste and time to market. Nonetheless, the lack of repeatability of the manufacturing process, which typically translates into a lack of reproducibility and reliability of the quality of the final products compared to traditional subtractive technologies, is currently one of the major barriers to the widespread adoption of AM in mass production. To overcome this limitation, there are growing efforts in recent years toward better integration of advanced information technologies into AM, exploiting the layer-by-layer nature of the build. The consequence of these efforts is twofold: 1) the integration of advanced sensing technologies into the AM systems, making possible the in situ monitoring of huge amounts of data at multiple time scales and resolutions and 2) the ever-increasing role of data-driven approaches [especially machine learning (ML)] in the analysis of such data to provide real-time quality monitoring and process optimization. This article introduces and reviews the key technological developments of this phenomenon, with a special focus on metal powder bed fusion (PBF) technologies that are attracting the highest attention by the industrial AM community. After introducing the main manufacturing quality issues and needs that have to be developed and optimized, we provide a wide overview of the latest progress of in situ monitoring and control in metal PBF, with special regards to sensing technologies and ML approaches. Finally, we identify the open challenges and future research directions in this field.},
  archive      = {J_PIEEE},
  author       = {Santa Di Cataldo and Sara Vinco and Gianvito Urgese and Flaviana Calignano and Elisa Ficarra and Alberto Macii and Enrico Macii},
  doi          = {10.1109/JPROC.2021.3054628},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {326-346},
  shortjournal = {Proc. IEEE},
  title        = {Optimizing quality inspection and control in powder bed metal additive manufacturing: Challenges and research directions},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leading information and communication technologies for smart
manufacturing: Facing the new challenges and opportunities of the 4th
industrial revolution. <em>PIEEE</em>, <em>109</em>(4), 320–325. (<a
href="https://doi.org/10.1109/JPROC.2021.3064103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first three industrial revolutions came about as a result of mechanization, electricity, and information technology (IT), respectively. Now, the introduction of the Internet of Things and Services into the manufacturing environment is fostering a 4th industrial revolution (Industry 4.0), where the smart optimization and computerization of all the actors and phases of the manufacturing process, including the conceptualization and design of a product, as well as its production and transaction, are taking a leading role.},
  archive      = {J_PIEEE},
  author       = {Santa Di Cataldo and Sukhan Lee and Enrico Macii and Birgit Vogel-Heuser},
  doi          = {10.1109/JPROC.2021.3064103},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {320-325},
  shortjournal = {Proc. IEEE},
  title        = {Leading information and communication technologies for smart manufacturing: Facing the new challenges and opportunities of the 4th industrial revolution},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward an electromagnetic event resilient grid.
<em>PIEEE</em>, <em>109</em>(4), 315–319. (<a
href="https://doi.org/10.1109/JPROC.2021.3062297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The threat of a large-scale electromagnetic event having a negative impact on the electric grid is real. Whether human-caused, via the detonation of a nuclear device, or natural, via a high-intensity burst of solar radiation, our historic experience with these phenomena indicates that as a global community, we should be prepared for such events and know how to mitigate their impacts. Current studies and related discussions provide a wide range of damage assessments for these events. We recommend continuing current technical investigations and research as well as strengthening collaboration between stakeholders and experts. This would ensure future threats are addressed in a timely and effective manner.},
  archive      = {J_PIEEE},
  author       = {H. M. Pennington and C. J. Hanley and J. D. Rogers},
  doi          = {10.1109/JPROC.2021.3062297},
  journal      = {Proceedings of the IEEE},
  number       = {4},
  pages        = {315-319},
  shortjournal = {Proc. IEEE},
  title        = {Toward an electromagnetic event resilient grid},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Recruit a member. <em>PIEEE</em>, <em>109</em>(3), 312. (<a
href="https://doi.org/10.1109/JPROC.2021.3061195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3061195},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {312},
  shortjournal = {Proc. IEEE},
  title        = {Recruit a member},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). IEEE women in engineering. <em>PIEEE</em>, <em>109</em>(3),
311. (<a href="https://doi.org/10.1109/JPROC.2021.3061173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE Women in Engineering.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3061173},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {311},
  shortjournal = {Proc. IEEE},
  title        = {IEEE women in engineering},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of powertrain technologies for energy-efficient
heavy-duty machinery. <em>PIEEE</em>, <em>109</em>(3), 279–308. (<a
href="https://doi.org/10.1109/JPROC.2021.3051555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive, multidisciplinary overview of the development of powertrain technologies for energy-efficient heavy-duty earthmoving machines. The heavy-duty earthmoving equipment industry has been among the biggest contributors to emissions globally. However, due to high power demand and multidisciplinary powertrain structures, improving the energy efficiency of heavy-duty mobile machines has been a pressing and challenging task in the industry. To cope with this challenge, hydraulics and power electronics (PE) have been the key driving forces. As such, the relative developments in both fields are covered in this article. For hydraulics, developments of efficient hydraulic circuits will be overviewed in detail along with the introduction of hydraulic energy recovery technologies. In addition, developments of PE architectures in hybrid and electrified machines will be introduced. Furthermore, potential medium-voltage dc mining site power distribution and the valves of wide bandgap devices will also be discussed with the hope to open up new research opportunities in PE. Moreover, emerging hybrid electrohydraulic drive technology is introduced. Based on the overview in this article, it is anticipated that electrohydraulic hybridization will be the future trend in the earthmoving machine industry. Deeper collaboration between the two areas is desirable.},
  archive      = {J_PIEEE},
  author       = {Zhongyi Quan and Lei Ge and Zhongbao Wei and Yun Wei Li and Long Quan},
  doi          = {10.1109/JPROC.2021.3051555},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {279-308},
  shortjournal = {Proc. IEEE},
  title        = {A survey of powertrain technologies for energy-efficient heavy-duty machinery},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining deep neural networks and beyond: A review of
methods and applications. <em>PIEEE</em>, <em>109</em>(3), 247–278. (<a
href="https://doi.org/10.1109/JPROC.2021.3060483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on “post hoc” explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of ML; and 4) demonstrate successful usage of XAI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of ML.},
  archive      = {J_PIEEE},
  author       = {Wojciech Samek and Grégoire Montavon and Sebastian Lapuschkin and Christopher J. Anders and Klaus-Robert Müller},
  doi          = {10.1109/JPROC.2021.3060483},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {247-278},
  shortjournal = {Proc. IEEE},
  title        = {Explaining deep neural networks and beyond: A review of methods and applications},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A tutorial on ultrareliable and low-latency communications
in 6G: Integrating domain knowledge into deep learning. <em>PIEEE</em>,
<em>109</em>(3), 204–246. (<a
href="https://doi.org/10.1109/JPROC.2021.3053601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the key communication scenarios in the fifth-generation and also the sixth-generation (6G) mobile communication networks, ultrareliable and low-latency communications (URLLCs) will be central for the development of various emerging mission-critical applications. State-of-the-art mobile communication systems do not fulfill the end-to-end delay and overall reliability requirements of URLLCs. In particular, a holistic framework that takes into account latency, reliability, availability, scalability, and decision-making under uncertainty is lacking. Driven by recent breakthroughs in deep neural networks, deep learning algorithms have been considered as promising ways of developing enabling technologies for URLLCs in future 6G networks. This tutorial illustrates how domain knowledge (models, analytical tools, and optimization frameworks) of communications and networking can be integrated into different kinds of deep learning algorithms for URLLCs. We first provide some background of URLLCs and review promising network architectures and deep learning frameworks for 6G. To better illustrate how to improve learning algorithms with domain knowledge, we revisit model-based analytical tools and cross-layer optimization frameworks for URLLCs. Following this, we examine the potential of applying supervised/unsupervised deep learning and deep reinforcement learning in URLLCs and summarize related open problems. Finally, we provide simulation and experimental results to validate the effectiveness of different learning algorithms and discuss future directions.},
  archive      = {J_PIEEE},
  author       = {Changyang She and Chengjian Sun and Zhouyou Gu and Yonghui Li and Chenyang Yang and H. Vincent Poor and Branka Vucetic},
  doi          = {10.1109/JPROC.2021.3053601},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {204-246},
  shortjournal = {Proc. IEEE},
  title        = {A tutorial on ultrareliable and low-latency communications in 6G: Integrating domain knowledge into deep learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). Scanning the issue. <em>PIEEE</em>, <em>109</em>(3),
202–203. (<a href="https://doi.org/10.1109/JPROC.2021.3060237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the key communication scenarios in the fifth generation (5G) and also the sixth generation (6G) of mobile communication networks, ultrareliable and low-latency communications (URLLC) will be central for the development of various emerging applications such as industry automation, tactile internet, augmented/virtual reality, and more. In order to support such mission-critical applications, according to the 5G standard requirements, the End-to-End (E2E) delay and the packet loss probability in current mobile networks need to be improved by at least two orders of magnitude. This capability gap cannot be fully resolved by the 5G New Radio, i.e., the physical-layer technology for 5G, even though the transmission delay in radio access networks achieves the 1-ms target. Transmission delay contributes only a small fraction of the E2E delay, as the stochastic delays in upper networking layers, such as queuing delay, processing delay, and access delay, are key bottlenecks for achieving URLLC.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2021.3060237},
  journal      = {Proceedings of the IEEE},
  number       = {3},
  pages        = {202-203},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multidimensional index modulation for 5G and beyond wireless
networks. <em>PIEEE</em>, <em>109</em>(2), 170–199. (<a
href="https://doi.org/10.1109/JPROC.2020.3040589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Index modulation (IM) provides a novel way for the transmission of additional data bits via the indices of the available transmit entities compared with classical communication schemes. This study examines the flexible utilization of existing IM techniques in a comprehensive manner to satisfy the challenging and diverse requirements of 5G and beyond services. After spatial modulation (SM), which transmits information bits through antenna indices, application of IM to orthogonal frequency-division multiplexing (OFDM) subcarriers has opened the door for the extension of IM into different dimensions, such as radio frequency (RF) mirrors, time slots, codes, and dispersion matrices. Recent studies have introduced the concept of multidimensional IM by various combinations of 1-D IM techniques to provide higher spectral efficiency (SE) and better bit error rate (BER) performance at the expense of higher transmitter (Tx) and receiver (Rx) complexity. Despite the ongoing research on the design of new IM techniques and their implementation challenges, proper use of the available IM techniques to address different requirements of 5G and beyond networks is an open research area in the literature. For this reason, we first provide the dimensional-based categorization of available IM domains and review the existing IM types regarding this categorization. Then, we develop a framework that investigates the efficient utilization of these techniques and establishes a link between the IM schemes and 5G services, namely, enhanced mobile broadband (eMBB), massive machine-type communications (mMTCs), and ultrareliable low-latency communication (URLLC). In addition, this work defines key performance indicators (KPIs) to quantify the advantages and disadvantages of IM techniques in time, frequency, space, and code dimensions. Finally, future recommendations are given regarding the design of flexible IM-based communication systems for 5G and beyond wireless networks.},
  archive      = {J_PIEEE},
  author       = {Seda Doğan Tusha and Armed Tusha and Ertugrul Basar and Huseyin Arslan},
  doi          = {10.1109/JPROC.2020.3040589},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {170-199},
  shortjournal = {Proc. IEEE},
  title        = {Multidimensional index modulation for 5G and beyond wireless networks},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A survey of test and reliability solutions for magnetic
random access memories. <em>PIEEE</em>, <em>109</em>(2), 149–169. (<a
href="https://doi.org/10.1109/JPROC.2020.3029600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memories occupy most of the silicon area in nowadays&#39; system-on-chips and contribute to a significant part of system power consumption. Though widely used, nonvolatile Flash memories still suffer from several drawbacks. Magnetic random access memories (MRAMs) have the potential to mitigate most of the Flash shortcomings. Moreover, it is predicted that they could be used for DRAM and SRAM replacement. However, they are prone to manufacturing defects and runtime failures as any other type of memory. This article provides an up-to-date and practical coverage of MRAM test and reliability solutions existing in the literature. After some background on existing MRAM technologies, defectiveness and reliability issues are discussed, as well as functional fault models used for MRAM. This article is dedicated to a summarized description of existing test and reliability improvement methods developed so far for various MRAM technologies. The last part of this article gives some perspectives on this hot topic.},
  archive      = {J_PIEEE},
  author       = {Patrick Girard and Yuanqing Cheng and Arnaud Virazel and Weisheng Zhao and Rajendra Bishnoi and Mehdi B. Tahoori},
  doi          = {10.1109/JPROC.2020.3029600},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {149-169},
  shortjournal = {Proc. IEEE},
  title        = {A survey of test and reliability solutions for magnetic random access memories},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Far-field automatic speech recognition. <em>PIEEE</em>,
<em>109</em>(2), 124–148. (<a
href="https://doi.org/10.1109/JPROC.2020.3018668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine recognition of speech spoken at a distance from the microphones, known as far-field automatic speech recognition (ASR), has received a significant increase in attention in science and industry, which caused or was caused by an equally significant improvement in recognition accuracy. Meanwhile, it has entered the consumer market with digital home assistants with a spoken language interface being its most prominent application. Speech recorded at a distance is affected by various acoustic distortions, and consequently, quite different processing pipelines have emerged compared with ASR for close-talk speech. A signal enhancement front end for dereverberation, source separation, and acoustic beamforming is employed to clean up the speech, and the back-end ASR engine is robustified by multicondition training and adaptation. We will also describe the so-called end-to-end approach to ASR, which is a new promising architecture that has recently been extended to the far-field scenario. This tutorial article gives an account of the algorithms used to enable accurate speech recognition from a distance, and it will be seen that, although deep learning has a significant share in the technological breakthroughs, a clever combination with traditional signal processing can lead to surprisingly effective solutions.},
  archive      = {J_PIEEE},
  author       = {Reinhold Haeb-Umbach and Jahn Heymann and Lukas Drude and Shinji Watanabe and Marc Delcroix and Tomohiro Nakatani},
  doi          = {10.1109/JPROC.2020.3018668},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {124-148},
  shortjournal = {Proc. IEEE},
  title        = {Far-field automatic speech recognition},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021f). Scanning the issue. <em>PIEEE</em>, <em>109</em>(2),
122–123. (<a href="https://doi.org/10.1109/JPROC.2020.3046146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this month’s Point-of-View article, the authors propose a novel paradigm for vehicular traffic in the era of connected and automated vehicles (CAVs), which includes two combined principles: lane-free traffic and vehicle nudging; the latter implying that vehicles may be “pushing” from a distance (using communication or sensors) other vehicles in front of them. This traffic paradigm features several advantages, including: smoother and safer driving, increase of roadway capacity, and no need for the anisotropy restriction. The proposed concept provides the possibility to actively design (rather than model or describe) the traffic flow characteristics in an optimal way, i.e., to engineer the future CAV traffic flow as an efficient artificial fluid. Options, features, related prior work, application domains, and required research topics are discussed. Preliminary simulation results illustrate some basic features of the concept.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2020.3046146},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {122-123},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lane-free artificial-fluid concept for vehicular traffic.
<em>PIEEE</em>, <em>109</em>(2), 114–121. (<a
href="https://doi.org/10.1109/JPROC.2020.3042681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular traffic has evolved as a crucial means for the transport of persons and goods, and its importance for the economic and social life of modern society cannot be overemphasized. On the other hand, recurrent vehicular traffic congestion, which appears on a daily basis, particularly in metropolitan areas, around the globe, has been a (increasingly) serious, in fact threatening, problem that calls for drastic solutions. Traffic congestion causes excessive travel delays, substantial fuel consumption and environmental pollution, and reduced traffic safety. Conventional traffic management measures are valuable [1]-[3] but not sufficient to address the heavily congested traffic conditions, which must be addressed in a more comprehensive way that exploits gradually emerging and future ground-breaking capabilities of vehicles and the infrastructure.},
  archive      = {J_PIEEE},
  author       = {Markos Papageorgiou and Kyriakos-Simon Mountakis and Iasson Karafyllis and Ioannis Papamichail and Yibing Wang},
  doi          = {10.1109/JPROC.2020.3042681},
  journal      = {Proceedings of the IEEE},
  number       = {2},
  pages        = {114-121},
  shortjournal = {Proc. IEEE},
  title        = {Lane-free artificial-fluid concept for vehicular traffic},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Recruit a member. <em>PIEEE</em>, <em>109</em>(1), 112. (<a
href="https://doi.org/10.1109/JPROC.2020.3040577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2020.3040577},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {112},
  shortjournal = {Proc. IEEE},
  title        = {Recruit a member},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021e). IEEE women in engineering. <em>PIEEE</em>, <em>109</em>(1),
111. (<a href="https://doi.org/10.1109/JPROC.2020.3040575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement, IEEE.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2020.3040575},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {111},
  shortjournal = {Proc. IEEE},
  title        = {IEEE women in engineering},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The first eight years of electric power transmission and
distribution—1873–1880 [scanning our past]. <em>PIEEE</em>,
<em>109</em>(1), 96–108. (<a
href="https://doi.org/10.1109/JPROC.2020.3036714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arc lighting powered from central stations preceded the better-known incandescent electric lighting powered from central stations and thrived well into the 20th century. After its introduction in 1880, incandescent electric lighting began to displace gas lighting in small indoor environments, but it failed to make inroads into arc lighting for streetlights and large indoor areas before high-intensity light bulbs were developed. In 1887, one-half of the generating capacity of central stations in the United States came from arc light dynamos [1] . In 1900, only 158 000 arc lamps valued at $1.8 million were in use in the United States, compared to about 25 million incandescent bulbs valued at $4 million, but that comparison is incomplete because 173 million arc carbons were consumed that year, with a total price tag of $1.3 million [2] , each carbon burning only for about 8 h, compared to hundreds of hours per incandescent bulb.},
  archive      = {J_PIEEE},
  author       = {Adam Allerhand},
  doi          = {10.1109/JPROC.2020.3036714},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {96-108},
  shortjournal = {Proc. IEEE},
  title        = {The first eight years of electric power transmission and distribution—1873–1880 [Scanning our past]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radiofrequency switches based on emerging resistive memory
technologies - a survey. <em>PIEEE</em>, <em>109</em>(1), 77–95. (<a
href="https://doi.org/10.1109/JPROC.2020.3011953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance radio frequency (RF) switches play a critical role in allowing radio transceivers to provide access to shared resources such as antennas. They are important, especially in reconfigurable radios, where the connections within and between the different blocks (e.g., filters, amplifiers, and mixers) can be changed to customize and compose RF functions for distinct frequency bands. To realize this potential, new concepts for RF switch devices that can be integrated with standard transistor process, while providing better transmission performance, lower power consumption, smaller area, and lower actuation voltage than current RF switch technologies [e.g., field-effect transistors (FETs) and microelectromechanical systems (MEMSs)] are needed. Recently, RF switches based on emerging memory technologies, such as conductive-bridge RAM (CBRAM), resistive RAM (ReRAM), and phase-change memory (PCM), have been proposed. These nanoscale switches present major advantages, for example, high cutoff frequency, nonvolatility, fast and low-energy switching, small footprint, and compatibility with back-end-of-line (BEOL) of the standard CMOS process, thus placing them as possible contenders for high-performance RF switches. This article surveys high-performance RF switches based on resistive memories, comparing them with mature RF switching technologies like RF MEMS, p-i-n diodes, and FETs. We discuss the physical mechanisms, device structure, performance characteristics, and applications of these novel RF switches. Furthermore, we examine the prospects and future research directions of these technologies that could lead to their adoption at the industrial level.},
  archive      = {J_PIEEE},
  author       = {Nicolás Wainstein and Gina Adam and Eilam Yalon and Shahar Kvatinsky},
  doi          = {10.1109/JPROC.2020.3011953},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {77-95},
  shortjournal = {Proc. IEEE},
  title        = {Radiofrequency switches based on emerging resistive memory technologies - a survey},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comprehensive survey on transfer learning. <em>PIEEE</em>,
<em>109</em>(1), 43–76. (<a
href="https://doi.org/10.1109/JPROC.2020.3004555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archive      = {J_PIEEE},
  author       = {Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
  doi          = {10.1109/JPROC.2020.3004555},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {43-76},
  shortjournal = {Proc. IEEE},
  title        = {A comprehensive survey on transfer learning},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In-memory learning with analog resistive switching memory: A
review and perspective. <em>PIEEE</em>, <em>109</em>(1), 14–42. (<a
href="https://doi.org/10.1109/JPROC.2020.3004543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we review the existing analog resistive switching memory (RSM) devices and their hardware technologies for in-memory learning, as well as their challenges and prospects. Since the characteristics of the devices are different for in-memory learning and digital memory applications, it is important to have an in-depth understanding across different layers from devices and circuits to architectures and algorithms. First, based on a top-down view from architecture to devices for analog computing, we define the main figures of merit (FoMs) and perform a comprehensive analysis of analog RSM hardware including the basic device characteristics, hardware algorithms, and the corresponding mapping methods for device arrays, as well as the architecture and circuit design considerations for neural networks. Second, we classify the FoMs of analog RSM devices into two levels. Level 1 FoMs are essential for achieving the functionality of a system (e.g., linearity, symmetry, dynamic range, level numbers, fluctuation, variability, and yield). Level 2 FoMs are those that make a functional system more efficient and reliable (e.g., area, operational voltage, energy consumption, speed, endurance, retention, and compatibility with back-end-of-line processing). By constructing a device-to-application simulation framework, we perform an in-depth analysis of how these FoMs influence in-memory learning and give a target list of the device requirements. Lastly, we evaluate the main FoMs of most existing devices with analog characteristics and review optimization methods from programming schemes to materials and device structures. The key challenges and prospects from the device to system level for analog RSM devices are discussed.},
  archive      = {J_PIEEE},
  author       = {Yue Xi and Bin Gao and Jianshi Tang and An Chen and Meng-Fan Chang and Xiaobo Sharon Hu and Jan Van Der Spiegel and He Qian and Huaqiang Wu},
  doi          = {10.1109/JPROC.2020.3004543},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {14-42},
  shortjournal = {Proc. IEEE},
  title        = {In-memory learning with analog resistive switching memory: A review and perspective},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021g). Scanning the issue. <em>PIEEE</em>, <em>109</em>(1), 11–13.
(<a href="https://doi.org/10.1109/JPROC.2020.3040098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This month’s regular papers focus on analog resistive switching memory devices, RF switches based on resistive memories, and transfer learning.},
  archive      = {J_PIEEE},
  doi          = {10.1109/JPROC.2020.3040098},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {11-13},
  shortjournal = {Proc. IEEE},
  title        = {Scanning the issue},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging the digital divide: Success depends on content
provider and application developer involvement [point of view].
<em>PIEEE</em>, <em>109</em>(1), 2–10. (<a
href="https://doi.org/10.1109/JPROC.2020.3028611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global connectivity is at an all-time high, and more users than ever before are participating in the online ecosystem. Despite this exciting phenomenon, opportunities to access the online world are not shared equally among the global population. Socioeconomic, political, and geographic factors all play roles in determining the extent to which one can be an online participant [1]. This resultant digital divide creates a social disparity between those who have reliable access to online content and can take advantage of all that the Internet has to offer and those who do not and miss out on those opportunities [2].},
  archive      = {J_PIEEE},
  author       = {Andrew Lappalainen and Catherine Rosenberg},
  doi          = {10.1109/JPROC.2020.3028611},
  journal      = {Proceedings of the IEEE},
  number       = {1},
  pages        = {2-10},
  shortjournal = {Proc. IEEE},
  title        = {Bridging the digital divide: Success depends on content provider and application developer involvement [point of view]},
  volume       = {109},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
