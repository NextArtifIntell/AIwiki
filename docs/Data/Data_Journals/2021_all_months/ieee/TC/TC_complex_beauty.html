<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---171">TC - 171</h2>
<ul>
<li><details>
<summary>
(2021). Affinity-aware VNF placement in mobile edge clouds via
leveraging GPUs. <em>TC</em>, <em>70</em>(12), 2234–2248. (<a
href="https://doi.org/10.1109/TC.2020.3041629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing becomes a promising technology to mitigate the latency of various cloud services. In addition, network function virtualization (NFV) has been shown a great potential in reducing the operational cost of cloud services while enhancing the flexibility of virtual network function deployments, by implementing dedicated hardware network functions as pieces of software in generic servers. Recently, the GPU acceleration has been investigated to speed up flow processing in virtual network functions (VNFs), by leveraging the parallelism of GPUs. VNFs that need accelerations prefer to stay at cloudlets (locations) equipped with GPUs. However, little attention has been paid for the VNF placement that takes into account GPU-affinity in cloudlets of mobile edge clouds. In this paper, we consider the affinity-aware throughput maximization problem in a mobile edge cloud via leveraging the parallelism on GPUs for user requests with VNF requirements. We consider two types of affinities in the VNF placement: The soft-affinity that allows VNFs to be executed by either CPUs or GPUs in cloudlets; and the hard-affinity that only allows VNFs to be placed to the GPUs of a specified set of cloudlets. We formulate two corresponding VNF placement problems in a mobile edge cloud. Specifically, we first propose an exact solution to the soft-affinity throughput maximization problem by formulating an Integer Linear Program (ILP). We then propose an efficient algorithm for the problem, by proposing a randomized algorithm with a provable approximation ratio for the hard-affinity-aware throughput maximization problem and extending the proposed approximation algorithm to the soft-affinity throughput maximization problem. Furthermore, assuming that user requests arrive into the mobile edge cloud one by one without the knowledge of future arrivals, we devise an online algorithm with a good competitive ratio for this dynamic hard-affinity-aware throughput maximization problem. Finally, we evaluate the performance of the proposed algorithms, through simulations and implementations in a real test-bed. Experimental results show that the performance of the proposed algorithms outperform their existing counterparts and achieve higher throughput.},
  archive      = {J_TC},
  author       = {Zichuan Xu and Zhiheng Zhang and John C. S. Lui and Weifa Liang and Qiufen Xia and Pan Zhou and Wenzheng Xu and Guowei Wu},
  doi          = {10.1109/TC.2020.3041629},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2234-2248},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Affinity-aware VNF placement in mobile edge clouds via leveraging GPUs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Genome sequence alignment - design space exploration for
optimal performance and energy architectures. <em>TC</em>,
<em>70</em>(12), 2218–2233. (<a
href="https://doi.org/10.1109/TC.2020.3041402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next generation workloads, such as genome sequencing, have an astounding impact in the healthcare sector. Sequence alignment, the first step in genome sequencing, has experienced recent breakthroughs, which resulted in next generation sequencing (NGS). As NGS applications are memory bounded with random memory access patterns, we propose the use of high bandwidth memories like 3D stacked HBM2, instead of traditional DRAMs like DDR4, along with energy efficient compute cores to improve both performance and energy efficiency. Three state-of-the-art NGS applications, Bowtie2, BWA-MEM, and HISAT2 are used as case studies to explore and optimize NGS computing architectures. Then, using the gem5-X architectural simulator, we obtain an overall 68 percent performance improvement and 71 percent energy savings using HBM2 instead of DDR4. Furthermore, we propose an architecture based on ARMv8 cores and demonstrate that 16 ARMv8 64-bit OoO cores with HBM2 outperforms 32-cores of Intel Xeon Phi Knights Landing (KNL) processor with 3D stacked memory. Moreover, we show that by using frequency scaling we can achieve up to 59 percent and 61 percent energy savings for ARM in-order and OoO cores, respectively. Lastly, we show that many ARMv8 in-order cores at 1.5GHz match the performance of fewer OoO cores at 2GHz, while attaining 4.5x energy savings.},
  archive      = {J_TC},
  author       = {Yasir Mahmood Qureshi and Jose Manuel Herruzo and Marina Zapater and Katzalin Olcoz and Sonia Gonzalez-Navarro and Oscar Plata and David Atienza},
  doi          = {10.1109/TC.2020.3041402},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2218-2233},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Genome sequence alignment - design space exploration for optimal performance and energy architectures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and scalable external sort framework for NVMe SSD.
<em>TC</em>, <em>70</em>(12), 2211–2217. (<a
href="https://doi.org/10.1109/TC.2020.3041220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the size of data grows in modern applications, the efficient usage of limited resources is becoming crucial. In order to reorganize large data under memory limitations, many data-intensive applications utilize external sort as a critical component. To streamline external sort, a storage framework is especially needed since the entire dataset must be loaded and flushed a couple of times during the sorting process. Most existing frameworks have attempted to simplify the storage access pattern by associating each thread with a separate storage device. This prevents randomized and concurrent I/O requests, which impose a huge overhead for legacy drives in order to enable the parallelism needed for external sort. However, such regulations excessively restrain the capabilities of NVMe-based SSDs that deliver high throughput with abundant parallelism. In this article, we present a new framework for external sort that exploits both external and internal parallelism. Externally, any number of threads are mobilized to parallel external sort in a scalable way, even with one NVMe SSD. Meanwhile, some arbitration schemes, such as adaptive resource allocation and fairness control, are adopted to preserve the internal efficiency of storage devices. Our evaluation results demonstrate that our scheme can greatly improve both the I/O efficiency and scalability compared to the existing frameworks.},
  archive      = {J_TC},
  author       = {Kihyeon Myung and Sunggon Kim and Heon Young Yeom and Jiwoong Park},
  doi          = {10.1109/TC.2020.3041220},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2211-2217},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient and scalable external sort framework for NVMe SSD},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ROCKY: A robust hybrid on-chip memory kit for the processors
with STT-MRAM cache technology. <em>TC</em>, <em>70</em>(12), 2198–2210.
(<a href="https://doi.org/10.1109/TC.2020.3040152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {STT-MRAM is regarded as an extremely promising NVM technology for replacing SRAM-based on-chip memories. While STT-MRAM memories benefit from ultra-low leakage power and high density, they suffer from some reliability challenges, namely, read disturbance, write failure, and retention failure. The write failure; storing a wrong value in an STT-MRAM cell during a write operation, is the most crucial reliability challenge. In this article, we propose ROCKY; a robust architecture equipped with efficient replacement policies for STT-MRAM-based cache memory hierarchy to improve the robustness of STT-MRAM part against the write failures. ROCKY reduces susceptible transitions in STT-MRAM cache memories leading to more reliable STT-MRAM write operations. The simulation results through comparison with traditional cache memory hierarchy demonstrate ROCKY decreases the WER of STT-MRAM cache memories by up to 35.4 percent while imposing less than 1 percent performance overhead to the system.},
  archive      = {J_TC},
  author       = {Mahdi Talebi and Arash Salahvarzi and Amir Mahdi Hosseini Monazzah and Kevin Skadron and Mahdi Fazeli},
  doi          = {10.1109/TC.2020.3040152},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2198-2210},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ROCKY: A robust hybrid on-chip memory kit for the processors with STT-MRAM cache technology},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coordinative scheduling of computation and communication in
data-parallel systems. <em>TC</em>, <em>70</em>(12), 2182–2197. (<a
href="https://doi.org/10.1109/TC.2020.3039238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many data-parallel computing systems like Spark, a job usually consists of multiple computation stages and inter-stage communication (i.e., coflows). Many efforts have been done to schedule coflows and jobs independently. The simple combination of coflow scheduling and job scheduling, however, would prolong the average job completion time (JCT) due to the conflict. For this reason, we propose a new abstraction of scheduling unit, named coBranch, which takes the dependency between computation stages and coflows into consideration, to schedule coflows and jobs jointly. Besides, mainstream coflow schedulers are order-preserving, i.e., all coflows of a high-priority job are prioritized than those of a low-priority job. We observe that the order-preserving constraint incurs low inter-job parallelism. To overcome the problem, we employ an urgency-based mechanism to schedule coBranches, which aims to decrease the average JCT by enhancing the inter-job parallelism. We implement the urgency-based coBranch Scheduling (BS) method on Apache Spark, conduct prototype-based experiments, and evaluate the performance of our method against the shortest-job-first critical-path method and the FIFO method. Results show that our method achieves around 10 and 15 percent reduction in the average JCT, respectively. Large-scale simulations based on the Google trace show that our method performs better and reduces JCT by 23 and 35 percent, respectively.},
  archive      = {J_TC},
  author       = {Dongsheng Li and Zhiyao Hu and Zhiquan Lai and Yiming Zhang and Kai Lu},
  doi          = {10.1109/TC.2020.3039238},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2182-2197},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Coordinative scheduling of computation and communication in data-parallel systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task splitting and load balancing of dynamic real-time
workloads for semi-partitioned EDF. <em>TC</em>, <em>70</em>(12),
2168–2181. (<a href="https://doi.org/10.1109/TC.2020.3038286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-time software systems, such as those commonly found in the context of multimedia, cloud computing, robotics, and real-time databases, are characterized by a dynamic workload, where applications can join and leave the system at runtime. Global schedulers can transparently support dynamic workload without requiring any off-line task-allocation phase, thus providing advantages to the system designer. Nevertheless, such schedulers exhibit poor worst-case performance when compared to semi-partitioned schedulers, which instead can achieve near-optimal schedulability performance when used in conjunction with smart task splitting and partitioning techniques, and they are also lighter in terms of run-time overhead. This article proposes an approach to efficiently schedule dynamic real-time workloads on multiprocessor systems by means of semi-partitioned scheduling. A linear-time approximation scheme for the C=D splitting algorithm under partitioned EDF scheduling is proposed. Then, a load-balancing algorithm is presented to admit new real-time workloads with a limited number of re-allocations. The article finally reports on a large-scale experimental study showing that (i) the linear-time approximation is characterized by a very limited utilization loss compared with the corresponding exact approach (that has a much higher complexity), and that (ii) the whole approach allows achieving considerable improvements with respect to global and partitioned EDF scheduling.},
  archive      = {J_TC},
  author       = {Daniel Casini and Alessandro Biondi and Giorgio Buttazzo},
  doi          = {10.1109/TC.2020.3038286},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2168-2181},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Task splitting and load balancing of dynamic real-time workloads for semi-partitioned EDF},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differential fault attack on kreyvium &amp; FLIP.
<em>TC</em>, <em>70</em>(12), 2161–2167. (<a
href="https://doi.org/10.1109/TC.2020.3038236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose key recovery attack on two stream ciphers: Kreyvium and FLIP $_{530}(42,128,360)$ using Differential Fault Attack (DFA) technique. These two ciphers are being used in Fully Homomorphic Encryption (FHE) due to their low error growth during keystream generation. Kreyvium is an NFSR-based stream cipher and FLIP is a permutation-based stream cipher. We first show that the complete state of the Kreyvium can be recovered by injecting 3 faults and considering 450 many keystream bits. In case of FLIP, we show that if there is a 1-bit fault in the state of the cipher then from 9000 normal and faulty keystream bits the state (i.e., the secret key) of the cipher can be recovered. For single bit fault, one will require to solve a system of equations for each 530 possible fault locations to recover the correct key of FLIP. To the best of our knowledge, this is the first article which analyzes the security of these two FHE supported stream ciphers under DFA and it has been observed that DFA completely reveals the secret keys of these two ciphers with very minimal faults.},
  archive      = {J_TC},
  author       = {Dibyendu Roy and Bhagwan Bathe and Subhamoy Maitra},
  doi          = {10.1109/TC.2020.3038236},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2161-2167},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Differential fault attack on kreyvium &amp; FLIP},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). <span class="math inline">${\sf FORESEE}$</span>FORESEE: An
effective and efficient framework for estimating the execution times of
IO traces on the SSD. <em>TC</em>, <em>70</em>(12), 2146–2160. (<a
href="https://doi.org/10.1109/TC.2020.3038189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If we had the performance information of every application on every SSD, it would be very beneficial to both SSD users and SSD manufacturers. For SSD users, they can buy the SSD that is fastest for the most frequently using applications; for SSD manufacturers, they can figure out the strength and weakness of their SSD for every application. Toward this end, this article proposes a framework named ${\sf FORESEE}$FORESEE that estimates accurately the execution time of a given IO trace (i.e., query IO trace ) of a given application on a target SSD without its actual execution . ${\sf FORESEE}$ is developed based on the observation that if two IO traces are similar to each other in their IO behavior, their execution times tend to be similar when they are executed on the same SSD . In ${\sf FORESEE}$ , the execution time of a query IO trace is estimated by using the execution times of the IO traces in a database similar to the query IO trace. Our technical contributions in ${\sf FORESEE}$ are as follows: (1) we propose a goodness function that efficiently evaluates the quality of sets of features that are used to measure the similarity of IO traces; (2) we propose a DB structure and a searching method for efficiently searching for similar IO traces to a query IO trace; (3) we propose an aggregation method that aggregates the execution times of similar IO traces to a query IO trace for accurately estimating the execution time of the query IO trace; and (4) we verify the effectiveness of ${\sf FORESEE}$ via extensive experiments by using real-world application IO traces. According to the results, the Pearson correlation coefficient (PCC) of the actual execution time and the estimated execution time by ${\sf FORESEE}$ is found to be 0.87, indicating ${\sf FORESEE}$ estimates the execution time accurately.},
  archive      = {J_TC},
  author       = {Yoonsuk Kang and Yong-Yeon Jo and Jaehyuk Cha and Wan D. Bae and Wonjun Lee and Sang-Wook Kim},
  doi          = {10.1109/TC.2020.3038189},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2146-2160},
  shortjournal = {IEEE Trans. Comput.},
  title        = {${\sf FORESEE}$FORESEE: An effective and efficient framework for estimating the execution times of IO traces on the SSD},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Credit risk analysis using quantum computers. <em>TC</em>,
<em>70</em>(12), 2136–2145. (<a
href="https://doi.org/10.1109/TC.2020.3038063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present and analyze a quantum algorithm to estimate credit risk more efficiently than Monte Carlo simulations can do on classical computers. More precisely, we estimate the economic capital requirement, i.e. the difference between the Value at Risk and the expected value of a given loss distribution. The economic capital requirement is an important risk metric because it summarizes the amount of capital required to remain solvent at a given confidence level. We implement this problem for a realistic loss distribution and analyze its scaling to a realistic problem size. In particular, we provide estimates of the total number of required qubits, the expected circuit depth, and how this translates into an expected runtime under reasonable assumptions on future fault-tolerant quantum hardware.},
  archive      = {J_TC},
  author       = {Daniel J. Egger and Ricardo García Gutiérrez and Jordi Cahué Mestre and Stefan Woerner},
  doi          = {10.1109/TC.2020.3038063},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2136-2145},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Credit risk analysis using quantum computers},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Circuit-based quantum random access memory for classical
data with continuous amplitudes. <em>TC</em>, <em>70</em>(12),
2125–2135. (<a href="https://doi.org/10.1109/TC.2020.3037932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loading data in a quantum device is required in several quantum computing applications. Without an efficient loading procedure, the cost to initialize the algorithms can dominate the overall computational cost. A circuit-based quantum random access memory named FF-QRAM can load $M$ $n$ -bit patterns with computational cost $O(CMn)$ to load continuous data where $C$ depends on the data distribution. In this article, we propose a strategy to load continuous data without post-selection with computational cost $O(Mn$ ). The proposed method is based on the probabilistic quantum memory, a strategy to load binary data in quantum devices, and the FF-QRAM using standard quantum gates, and is suitable for noisy intermediate-scale quantum computers.},
  archive      = {J_TC},
  author       = {Tiago M. L. de Veras and Ismael C. S. de Araujo and Daniel K. Park and Adenilton J. da Silva},
  doi          = {10.1109/TC.2020.3037932},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2125-2135},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Circuit-based quantum random access memory for classical data with continuous amplitudes},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A throughput-oriented NVMe storage virtualization with
workload-aware management. <em>TC</em>, <em>70</em>(12), 2112–2124. (<a
href="https://doi.org/10.1109/TC.2020.3037817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storage virtualization is an important component of large-scale online services in multi-tenant clouds. It typically shares the physical storage among guest machines and performs transactional operations for high-performance data processing. However, even with the recent mediated pass-through virtualization optimization, the operations of multi-tenant storage I/O meet the bottleneck, and thus degrade the throughput performance of the cloud storage services. We observe that the root cause of the problem is the unawareness of varying and imbalanced workload inefficiency of resource management in the multi-tenant cloud storage setting. In this paper, we present FinNVMe, a new throughput-oriented NVMe storage virtualization management mechanism, that (1) passes-through I/O performance-critical resources and emulates privileged resources to provide high throughput in a workload-aware manner among multi-tenant VMs, (2) enables fine-grained scheduling for I/O resources to achieve promising flexibility and scalability with respective to virtualization, and (3) adopts the queue binding and the queue shuffling to reduce the virtualization and management overhead, and involves active polling for further I/O acceleration. This article subsequently evaluates FinNVMe with micro benchmarks on two typical scenarios (both balanced and imbalanced workload) and the real-world storage workloads to show its high throughput performance, along with the flexibility and scalability of virtualization and resource management. For example, FinNVMe achieves up to 20 percent throughput improvement with more stable latency in the varying and imbalanced workload.},
  archive      = {J_TC},
  author       = {Bo Peng and Ming Yang and Jianguo Yao and Haibing Guan},
  doi          = {10.1109/TC.2020.3037817},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2112-2124},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A throughput-oriented NVMe storage virtualization with workload-aware management},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing predictable cache coherence protocols for
multi-core real-time systems. <em>TC</em>, <em>70</em>(12), 2098–2111.
(<a href="https://doi.org/10.1109/TC.2020.3037747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the challenge of allowing simultaneous and predictable accesses to shared data on multi-core systems. We propose a collection of predictable cache coherence protocols, which mandate the use of certain design invariants to ensure predictability. In particular, we enforce these invariants by augmenting the classic modify-share-invalid (MSI) protocol and modify-exclusive-share-invalid (MESI) protocol. This allows us to derive worst-case latency bounds on the resulting predictable MSI (PMSI) and predictable MESI (PMESI) protocols. Our analysis shows that while the arbitration latency scales linearly, the coherence latency scales quadratically with the number of cores, which emphasizes the importance of accounting for cache coherence effects on latency bounds. We implement PMSI and PMESI in a detailed micro-architectural simulator, and execute SPLASH-2 and synthetic workloads. Results show that our approach is always within the analytical worst-case latency bounds, and that PMSI and PMESI improve average-case performance by up to 4× over cache bypassing mechanisms that disallow caching of shared data in the cores’ private caches. PMSI and PMESI have average slowdowns of 1.45× and 1.46× compared to conventional MSI and MESI protocols, respectively.},
  archive      = {J_TC},
  author       = {Anirudh Mohan Kaushik and Mohamed Hassan and Hiren Patel},
  doi          = {10.1109/TC.2020.3037747},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2098-2111},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Designing predictable cache coherence protocols for multi-core real-time systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing proportional IO sharing on containerized big data
file systems. <em>TC</em>, <em>70</em>(12), 2083–2097. (<a
href="https://doi.org/10.1109/TC.2020.3037078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big Data platforms recently employ resource management systems, such as YARN, Mesos, and Google Borg, to provision computational resources. These systems adopt containerization to share the computing resources in a multi-tenant setting with low performance overhead and interference. However, it may be observed that tenants often interfere with each other on the underlying Big Data File Systems (BDFS), e.g., Hadoop File System, which have been widely deployed as a persistent layer in current data centers. A solution with systematic generality is to containerize BDFS itself to isolate and allocate its IO sources to multiple tenants. To this end, we conduct analysis on the ineffectiveness of proportionally sharing BDFS IO resource via containerization. This ineffectiveness is due to the scheduler of containerization in “pseudo-starvation” status, in which most of IO requests are backlogged in BDFS rather than in containerization scheduler. Without enough backlogged IO requests, existing schedulers might have to maximize device utilization rather than enforce proportional sharing policy. To resolve this ineffectiveness issue, we develop a cross-layer system called BDFS-Container , which containerizes BDFS at the Linux block IO level. Central to BDFS-Container, we propose and design a proactive IOPS throttling-based mechanism named IOPS Regulator , which achieves a trade-off between maximizing IO utilization and accurately proportional IO sharing. The evaluation results show that our method can improve proportionally sharing BDFS IO resources by 74.4 percent on average.},
  archive      = {J_TC},
  author       = {Dan Huang and Jun Wang and Qing Liu and Nong Xiao and Huafeng Wu and Jiangling Yin},
  doi          = {10.1109/TC.2020.3037078},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2083-2097},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing proportional IO sharing on containerized big data file systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast resource and timing aware design optimisation for
high-level synthesis. <em>TC</em>, <em>70</em>(12), 2070–2082. (<a
href="https://doi.org/10.1109/TC.2021.3112260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-Programmable Gate Arrays (FPGA) are often present in energy-efficient systems, although its non-trivial development flow is an obstacle for massive adoption. High-Level Synthesis (HLS) approaches attempt to mitigate the gap by targetting FPGAs from software languages, however manual tuning is still essential to meet performance demands. We present a high-level design space exploration framework with timing and resource awareness that uses an estimator named Lina to evaluate each design point. Lina is a profiling-based approach that avoids the costly static analyses performed by HLS compilers, allowing a significantly faster exploration of optimisations. Estimations are improved by supporting a continuous range of operating frequencies and by considering resource usage for both floating-point and integer datapaths. For a given set of C kernels, the estimated solutions are among the best 1\% for execution time and resource footprint. The exploration of each kernel using Lina was performed on average two orders of magnitude faster than using early HLS compiler reports, and four orders of magnitude faster than fully compiling each design point. By considering the design spaces traversed, our solutions reached 70\% of the maximum speed-up achievable. This represents an average speed-up of 14-16× compared to the baseline designs with no optimisations enabled.},
  archive      = {J_TC},
  author       = {André B. Perina and Arthur Silitonga and Jürgen Becker and Vanderlei Bonato},
  doi          = {10.1109/TC.2021.3112260},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2070-2082},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast resource and timing aware design optimisation for high-level synthesis},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analytical model for memory-centric high level
synthesis-generated applications. <em>TC</em>, <em>70</em>(12),
2056–2069. (<a href="https://doi.org/10.1109/TC.2021.3115056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High performance computing (HPC) demands huge memory bandwidth and computing resources to achieve maximum performance and energy efficiency. FPGAs can provide both, and with the help of High Level Synthesis, those HPC applications can be easily written in high level languages. However, the optimization process remains time-consuming, especially when based on trial-and-error bitstream generation. Model-based performance prediction is a practical and fast approach for kernel optimization, specially if done with information from pre-synthesis reports. This article presents an analytical model focused on memory intensive applications that captures the memory behavior and accurately predicts the kernel execution time within seconds rather than hours, as bitstream generation requires. The model has been validated with two DRAM technologies: DDR4 and HBM2, with a set of microbenchmarks and high performance computing applications showing an average error of 11\% for DDR4 and 10\% for HBM2. Compared with previous studies, our predictions at least halve the estimation error.},
  archive      = {J_TC},
  author       = {Maria Angélica Dávila-Guzmán and Rubén Gran Tejero and María Villarroya-Gaudó and Darío Suárez Gracia},
  doi          = {10.1109/TC.2021.3115056},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2056-2069},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Analytical model for memory-centric high level synthesis-generated applications},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing high-level synthesis using a meta-programming
approach. <em>TC</em>, <em>70</em>(12), 2043–2055. (<a
href="https://doi.org/10.1109/TC.2021.3096429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s increasingly heterogeneous compute landscape, there is high demand for design tools that offer seemingly contradictory features: portable programming abstractions that hide underlying architectural detail, and the capability to optimise and exploit architectural features. Our meta-programming approach, Artisan, decouples application functionality from optimisation concerns to address the complexity of mapping high-level application descriptions onto heterogeneous platforms from which they are abstracted. With Artisan, application experts focus on algorithmic behaviour, while platform and domain experts focus on optimisation and mapping. Artisan offers complete design-flow orchestration in a unified programming environment based on Python 3 to enable accessible codification of reusable optimisation strategies that can be automatically applied to high-level application descriptions. We have developed and evaluated an Artisan prototype and a set of customised meta-programs used to automatically optimise six case study applications for CPU+FPGA targets. In our experiments, Artisan-optimised designs achieve the same order of magnitude speedup as manually optimised designs compared to corresponding unoptimised software.},
  archive      = {J_TC},
  author       = {Jessica Vandebon and Jose G. F. Coutinho and Wayne Luk and Eriko Nurvitadhi},
  doi          = {10.1109/TC.2021.3096429},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2043-2055},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing high-level synthesis using a meta-programming approach},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OmpSs@FPGA framework for high performance FPGA computing.
<em>TC</em>, <em>70</em>(12), 2029–2042. (<a
href="https://doi.org/10.1109/TC.2021.3086106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the new features of the OmpSs@FPGA framework. OmpSs is a data-flow programming model that supports task nesting and dependencies to target asynchronous parallelism and heterogeneity. OmpSs@FPGA is the extension of the programming model addressed specifically to FPGAs. OmpSs environment is built on top of Mercurium source to source compiler and Nanos++ runtime system. To address FPGA specifics Mercurium compiler implements several FPGA related features as local variable caching, wide memory accesses or accelerator replication. In addition, part of the Nanos++ runtime has been ported to hardware. Driven by the compiler this new hardware runtime adds new features to FPGA codes, such as task creation and dependence management, providing both performance increases and ease of programming. To demonstrate these new capabilities, different high performance benchmarks have been evaluated over different FPGA platforms using the OmpSs programming model. The results demonstrate that programs that use the OmpSs programming model achieve very competitive performance with low to moderate porting effort compared to other FPGA implementations.},
  archive      = {J_TC},
  author       = {Juan Miguel de Haro and Jaume Bosch and Antonio Filgueras and Miquel Vidal and Daniel Jiménez-González and Carlos Álvarez and Xavier Martorell and Eduard Ayguadé and Jesús Labarta},
  doi          = {10.1109/TC.2021.3086106},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2029-2042},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OmpSs@FPGA framework for high performance FPGA computing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PyLog: An algorithm-centric python-based FPGA programming
and synthesis flow. <em>TC</em>, <em>70</em>(12), 2015–2028. (<a
href="https://doi.org/10.1109/TC.2021.3123465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploding complexity and computation efficiency requirements of applications are stimulating a strong demand for hardware acceleration with heterogeneous platforms such as FPGAs. However, a high-quality FPGA design is very hard to create and optimize as it requires FPGA expertise and a long design iteration time. In contrast, software applications are typically developed in a short development cycle, with high-level languages like Python, which have much higher levels of abstraction than all existing hardware design flows. To close this gap between hardware design flows and software applications, and simplify FPGA programming, we create PyLog, a high-level, algorithm-centric Python-based programming and synthesis flow for FPGA. PyLog is powered by a set of compiler optimization passes and a type inference system to generate high-quality hardware design. It abstracts away the implementation details, and allows designers to focus on algorithm specification. PyLog takes in Python functions, generates PyLog intermediate representation (PyLog IR), performs several optimization passes, including pragma insertion, design space exploration, and memory customization, etc., and creates complete FPGA system designs. PyLog also has a runtime that allows users to run the PyLog code directly on the target FPGA platform without any extra code development. The whole design flow is automated. Evaluation shows that PyLog significantly improves FPGA design productivity and generates highly efficient FPGA designs that outperform highly optimized CPU implementation and state-of-the-art FPGA implementation by $3.17\times$ and $1.24\times$ on average.},
  archive      = {J_TC},
  author       = {Sitao Huang and Kun Wu and Hyunmin Jeong and Chengyue Wang and Deming Chen and Wen-Mei Hwu},
  doi          = {10.1109/TC.2021.3123465},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2015-2028},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PyLog: An algorithm-centric python-based FPGA programming and synthesis flow},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: IEEE TC special section on compiler
optimizations for FPGA-based systems. <em>TC</em>, <em>70</em>(12),
2013–2014. (<a href="https://doi.org/10.1109/TC.2021.3117316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on compiler optimization for FPGA-based systems. Reconfigurable computing (RC) is growing in importance in many computing domains and systems, from embedded, mobile to cloud, and high-performance computing. We have witnessed important advancements regarding the programming of RC-based systems, but further improvements are needed, especially regarding efficient techniques for automatic mapping of computations described in high-level languages to the RC resources. The resources of high-end FPGAs allow these devices to implement complex Systemson- a-Chip (SoCs) and substantial computational components of software applications, e.g., when used as hardware accelerators and/or as more energy-efficient computing platforms. This, however, increases the continuous need for efficient compilers targeting FPGAs, and other RC platforms, from high-level programming languages.},
  archive      = {J_TC},
  author       = {João MP Cardoso and André DeHon and Laura Pozzi},
  doi          = {10.1109/TC.2021.3117316},
  journal      = {IEEE Transactions on Computers},
  number       = {12},
  pages        = {2013-2014},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE TC special section on compiler optimizations for FPGA-based systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LPC: An error correction code for mitigating faults in 3D
memories. <em>TC</em>, <em>70</em>(11), 2001–2012. (<a
href="https://doi.org/10.1109/TC.2020.3034400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The radiation sensitivity of memory cells increases dramatically as CMOS manufacture technology scales down; therefore, the reliability of memories has become a challenge. 3D technology has gained attention for having several advantages compared to the 2D counterpart, such as high integration density, high performance, low power, and high communication speed. Although several studies are targeting 3D memories, the effects on reliability using this technology have received little attention. This work introduces Line Product Code (LPC), a modified product code-based Error Correction Code (ECC) that uses both Hamming and parity in both rows and columns to implement reliable 3D memories. We implemented two lightweight LPC-based decoding algorithms in interleaved (LPCa-I) and non-interleaved (LPCa) versions, which allowed us to analyze LPC through a set of simulation cases that considers four severity levels of error incidence. The experimental results showed the effectiveness of the LPC-based algorithms, reaching correction rates of up 2.3 times higher compared to other Hamming-based algorithms.},
  archive      = {J_TC},
  author       = {David C. C. Freitas and David F. M. Mota and César Marcon and Jarbas A. N. Silveira and João C. M. Mota},
  doi          = {10.1109/TC.2020.3034400},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {2001-2012},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LPC: An error correction code for mitigating faults in 3D memories},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal optimization of deep neural networks for
reconfigurable FPGA SoCs. <em>TC</em>, <em>70</em>(11), 1988–2000. (<a
href="https://doi.org/10.1109/TC.2020.3033730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a technique for optimizing the timing performance and the resource consumption of hardware accelerators for deep neural network (DNN) inference on FPGA-based system-on-chips (SoC). When required, the accelerators are decomposed into chunks, each exploiting at best the available FPGA area, and dynamic partial reconfiguration (DPR) is leveraged to schedule such chunks at run-time. To this end, the article presents accurate models of the resource consumption and timing of DNN accelerators provided by the Xilinx FINN framework. The models are then used to formulate an optimization problem that computes the optimal decomposition of DNN accelerators (and their configuration) by minimizing the inference time while ensuring area constraints on the FPGA. Experimental results on Zynq-7000 platforms demonstrate that the proposed technique provides consistent improvements with respect to both stock configurations of the accelerators and other configurations that can be obtained with a static FPGA allocation.},
  archive      = {J_TC},
  author       = {Biruk Seyoum and Marco Pagani and Alessandro Biondi and Sara Balleri and Giorgio Buttazzo},
  doi          = {10.1109/TC.2020.3033730},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1988-2000},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Spatio-temporal optimization of deep neural networks for reconfigurable FPGA SoCs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revealing DRAM operating GuardBands through workload-aware
error predictive modeling. <em>TC</em>, <em>70</em>(11), 1976–1987. (<a
href="https://doi.org/10.1109/TC.2020.3033627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the energy efficiency of DRAMs becomes very challenging due to the growing demand for storage capacity and failures induced by the manufacturing process. To protect against failures, vendors adopt conservative margins in the refresh period and supply voltage. Previously, it was shown that these margins are too pessimistic and will become impractical due to high-power costs, especially in future DRAM technologies. In this article, we present a new technique for automatic scaling the DRAM refresh period under reduced supply voltage that minimizes the probability of failures. The main idea behind the proposed approach is that DRAM error behavior is workload-dependent and can be predicted based on particular program inherent features. We use a Machine Learning (ML) method to build a workload-aware DRAM error behavior model based on the program features which we extract from real workloads during our DRAM error characterization campaign. With such a model, we identify the marginal value of the DRAM refresh period under relaxed voltage for each DRAM module of a server that enable us to reduce the DRAM power. We implement a temperature-driven OS governor which automatically sets the module-specific marginal DRAM parameters discovered by the ML model. Our governor reduces the DRAM power by 24 percent on average while minimizing the probability of failures. Unlike previous studies, our technique: i) does not require intrusive changes to hardware; ii) is implemented on a real server; iii) uses a mechanism that prevents any abnormal DRAM error behavior; and iv) can be easily deployed in data centers.},
  archive      = {J_TC},
  author       = {Lev Mukhanov and Konstantinos Tovletoglou and Hans Vandierendonck and Dimitrios S. Nikolopoulos and Georgios Karakonstantis},
  doi          = {10.1109/TC.2020.3033627},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1976-1987},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Revealing DRAM operating GuardBands through workload-aware error predictive modeling},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A voting approach for adaptive network-on-chip power-gating.
<em>TC</em>, <em>70</em>(11), 1962–1975. (<a
href="https://doi.org/10.1109/TC.2020.3033163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scalable Networks-on-Chip (NoCs) have become the standard interconnection mechanisms in large-scale multicore architectures. These NoCs consume a large fraction of the on-chip power budget, where the static portion is becoming dominant as technology scales down to sub-10nm node. Therefore, it is essential to reduce static power so as to achieve power- and energy-efficient computing. Power-Gating as an effective static power saving technique can be used to power off inactive routers for static power saving. However, packet deliveries in irregular power-gated networks suffer from detour or waiting time overhead to either route around or wake up power-gated routers. In this article, we propose Fly-Over ( Flov ) , a voting approach for dynamic router power-gating in a light-weight and distributed manner, which includes Flov router microarchitecture, adaptive power-gating policy, and low-latency dynamic routing algorithms. We evaluate Flov using synthetic workloads as well as real workloads from PARSEC 2.1 benchmark suite. Our full-system evaluations show that Flov reduces the power consumption of NoC by 31 and 20 percent, respectively, on average across several benchmarks, compared to the baseline and the state-of-the-art while maintaining the similar performance.},
  archive      = {J_TC},
  author       = {Jiayi Huang and Shilpa Bhosekar and Rahul Boyapati and Ningyuan Wang and Byul Hur and Ki Hwan Yum and Eun Jung Kim},
  doi          = {10.1109/TC.2020.3033163},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1962-1975},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A voting approach for adaptive network-on-chip power-gating},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic value-deviation-bounded source-dependent
bit-level channel adaptation for approximate communication. <em>TC</em>,
<em>70</em>(11), 1949–1961. (<a
href="https://doi.org/10.1109/TC.2020.3031494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing systems that can tolerate effects of errors in their communicated data values can trade this tolerance for improved resource efficiency. Many important applications of computing, such as embedded sensor systems, can tolerate errors that are bounded in their distribution of deviation from correctness (distortion). We present a channel adaptation technique which modulates properties of I/O channels typical in embedded sensor systems, to provide a tradeoff between I/O power dissipation and distortion of communicated data. We provide an efficient-to-compute formulation for the distribution of integer distortion accounting for the distribution of transmitted values. Using this formulation we implement our value-deviation-bounded (VDB) channel adaptation. We experimentally quantify the achieved reduction in power dissipation on a hardware prototype integrated with the required programmable channel modulation circuitry. We augment these experimental measurements with an analysis of the distributions of distortions. We show that our probabilistic VDB channel adaptation can provide up to a 2× reduction in I/O power dissipation. When synthesized for a miniature low-power FPGA intended for use in sensor interfaces, a register transfer level implementation of the channel adaptation control logic requires only 106 flip-flops and 224 4-input LUTs for implementing per-bit channel adaptation on serialized streams of 8-bit sensor data.},
  archive      = {J_TC},
  author       = {Bilgesu Arif Bilgin and Phillip Stanley-Marbell},
  doi          = {10.1109/TC.2020.3031494},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1949-1961},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Probabilistic value-deviation-bounded source-dependent bit-level channel adaptation for approximate communication},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient out-of-core and out-of-place rectangular matrix
transposition and rotation. <em>TC</em>, <em>70</em>(11), 1942–1948. (<a
href="https://doi.org/10.1109/TC.2020.3030592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computers keep following the traditional model of addressing memory linearly for their main memory and out-of-core storage. While this model allows efficient row access to row-major 2D matrices, it introduces complexity to perform efficient column access. A common strategy to improve these accesses is to transpose or rotate the matrix beforehand, thus the accessing complexity is centralized in one transformation operation. Further column accesses are performed as row accesses to the transposed matrix therefore they are optimized to the memory model. In this article, we propose an efficient solution to perform in-memory or out-of-core rectangular matrix transposition and rotation by using an out-of-place strategy, reading a matrix from an input file and writing the transformed matrix to another (output) file. An originality of our processing algorithm is to rely on an optimized use of the page cache mechanism. It is parallel, optimized by several levels of tiling and independent of any disk block size. We evaluate our approach on five common storage configurations: HDD, hybrid HDD-SSD, SSD, software RAID 0 of several SSDs, and NVMe. We show that it brings significant performance improvement over a hand-tuned optimized reference implementation developed by the Caldera company and we confront it against the baseline speed of a straight file copy.},
  archive      = {J_TC},
  author       = {Paul Godard and Vincent Loechner and Cédric Bastoul},
  doi          = {10.1109/TC.2020.3030592},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1942-1948},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient out-of-core and out-of-place rectangular matrix transposition and rotation},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Remote control: A simple deadlock avoidance scheme for
modular systems-on-chip. <em>TC</em>, <em>70</em>(11), 1928–1941. (<a
href="https://doi.org/10.1109/TC.2020.3029682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ever increasing performance demand and shrinking in the transistor size together result in complex and dense packing in large chips. That motivates designers to opt for many small specialized hardware modules in a chip to extract maximum performance benefits with relatively lower complexity and cost. These altogether opens up new directions for heterogeneous modular System-on-Chip (SoC) research, where a large system is built by assembling small independently designed chiplets (small chips). We focus on the communication aspect of such SoCs, especially the newly observed deadlock among chiplets. Even though deadlock is a classic problem in networks and many solutions are available, the modular SoC design demands customized solutions that preserves the design flexibility for chiplet designers. We propose Remote Control (RC) , a simple routing oblivious deadlock avoidance scheme based on selective injection-control mechanism. Along with guarantee on deadlock freedom, RC aims to provide a methodology to make each independently designed chiplet seamlessly integrate in any modular SoCs. We achieve up to 56.34\% throughput and 15.49\% zero load latency improvements on synthetic traffic and up to 20\% speedup on real workloads taken from vast range of benchmark suites, over the state-of-the-art turn restriction based technique applied in the modular SoC domain.},
  archive      = {J_TC},
  author       = {Pritam Majumder and Sungkeun Kim and Jiayi Huang and Ki Hwan Yum and Eun Jung Kim},
  doi          = {10.1109/TC.2020.3029682},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1928-1941},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Remote control: A simple deadlock avoidance scheme for modular systems-on-chip},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving the performance of block-based DRAM caches via
tag-data decoupling. <em>TC</em>, <em>70</em>(11), 1914–1927. (<a
href="https://doi.org/10.1109/TC.2020.3029615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-package DRAM-based Last-Level-Caches (LLCs) that cache data in small chunks (i.e., blocks) are promising for improving system performance due to their efficient main memory bandwidth utilization. However, in these high-capacity DRAM caches, managing metadata (i.e., tags) at low cost is challenging. Storing the tags in SRAM has the advantage of quick tag access but is impractical due to a large area overhead. Storing the tags in DRAM reduces the area overhead but incurs tag serialization latency for an associative LLC design, which is inevitable for achieving high cache hit rate. To address the area and latency overhead problem, we propose a block-based DRAM LLC design that decouples tag and data into two regions in DRAM. Our design stores the tags in a latency-optimized DRAM region as the tags are accessed more often than the data. In contrast, we optimize the data region for area efficiency and map spatially-adjacent cache blocks to the same DRAM row to exploit spatial locality. Our design mitigates the tag serialization latency of existing associative DRAM LLCs via selective in-DRAM tag comparison, which overlaps the latency of tag and data accesses. This efficiently enables LLC bypassing via a novel DRAM Absence Table (DAT) that not only provides fast LLC miss detection but also reduces in-package bandwidth requirements. Our evaluation using SPEC2006 benchmarks shows that our tag-data decoupled LLC improves system performance by 11.7 percent compared to a state-of-the-art direct-mapped LLC design and by 7.2 percent compared to an existing associative LLC design.},
  archive      = {J_TC},
  author       = {Fazal Hameed and Asif Ali Khan and Jeronimo Castrillon},
  doi          = {10.1109/TC.2020.3029615},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1914-1927},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving the performance of block-based DRAM caches via tag-data decoupling},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precise worst-case blocking time of tasks under priority
inheritance protocol. <em>TC</em>, <em>70</em>(11), 1901–1913. (<a
href="https://doi.org/10.1109/TC.2020.3029328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of precisely computing the worst-case blocking time that tasks may experience is one of the fundamental issues of schedulability analysis of real-time applications. While exact methods have been proposed for more sophisticated protocols, the problem is indeed complex in case of the Priority Inheritance Protocol, even restricting the attention to uniprocessor systems, non-nested resource accesses, and non-self-suspending tasks. Besides a very simple method leading in general to loose upper bounds, only one algorithm of exponential complexity has been so far reported in literature to tighten such bounds. In this article, we describe a novel approach which, leveraging an operational research technique for modeling the problem, computes the same tight bounds in polynomial time. We then discuss the scenarios in which, assuming no conditional statements in the tasks’ code, the computed bounds derive from an actually impossible blocking chain, and we refine the initial model to more precisely compute the worst-case blocking times for any task set in any possible operating condition.},
  archive      = {J_TC},
  author       = {Eugenio Faldella and Daniela Loreti},
  doi          = {10.1109/TC.2020.3029328},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1901-1913},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Precise worst-case blocking time of tasks under priority inheritance protocol},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The nebula benchmark suite: Implications of lightweight
neural networks. <em>TC</em>, <em>70</em>(11), 1887–1900. (<a
href="https://doi.org/10.1109/TC.2020.3029327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a benchmark suite named Nebula that implements lightweight neural network benchmarks. Recent neural networks tend to form deeper and sizable networks to enhance accuracy and applicability. However, the massive volume of heavy networks makes them highly challenging to use in conventional research environments such as microarchitecture simulators. We notice that neural network computations are mainly comprised of matrix and vector calculations that repeat on multi-dimensional data encompassing batches, channels, layers, etc. This observation motivates us to develop a variable-sized neural network benchmark suite that provides users with options to select appropriate size of benchmarks for different research purposes or experiment conditions. Inspired by the implementations of well-known benchmarks such as PARSEC and SPLASH suites, Nebula offers various size options from large to small datasets for diverse types of neural networks. The Nebula benchmark suite is comprised of seven representative neural networks built on a C++ framework. The variable-sized benchmarks can be executed i) with acceleration libraries (e.g., BLAS, cuDNN) for faster and realistic application runs or ii) without the external libraries if execution environments do not support them, e.g., microarchitecture simulators. This article presents a methodology to develop the variable-sized neural network benchmarks, and their performance and characteristics are evaluated based on hardware measurements. The results demonstrate that the Nebula benchmarks reduce execution time as much as 25x while preserving similar architectural behaviors as the full-fledged neural networks.},
  archive      = {J_TC},
  author       = {Bogil Kim and Sungjae Lee and Chanho Park and Hyeonjin Kim and William J. Song},
  doi          = {10.1109/TC.2020.3029327},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1887-1900},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The nebula benchmark suite: Implications of lightweight neural networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending performance-energy trade-offs via dynamic core
scaling. <em>TC</em>, <em>70</em>(11), 1875–1886. (<a
href="https://doi.org/10.1109/TC.2020.3029306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern processors often need to switch among different power states based on usage scenarios, energy availability, and thermal conditions. Dynamic voltage and frequency scaling (DVFS) is a commonly used power management strategy to trade off performance and energy. As transistor scaling is reaching its limit, the viable supply voltage range where DVFS can operate is shrinking, which limits its effectiveness. To extend the performance-energy trade-off capabilities in modern processors, this article proposes dynamic core scaling (DCS) that does not rely on voltage scaling. DCS dynamically adjusts the active superscalar datapath resources so that programs run at a given percentage of their maximum speed while minimizing energy consumption at the same time. Since DCS does not need voltage scaling, it can be combined with DVFS to achieve greater energy savings. To effectively manage performance-energy trade-offs using a combination of DCS and DVFS, this article proposes an oracle controller that demonstrates the optimal control strategy, and two practical controllers that are applicable in real implementations. Evaluations using an 8-way superscalar processor implemented in a 45-nm circuit show that DCS is more effective in performance-energy trade-offs than DVFS at the high performance end for a number of SPEC CPU2000 benchmarks. When used together with DVFS, DCS saves an additional 20 percent of a full-size core’s energy on average. At the minimum operating voltage, DVFS hits its limit, while DCS is still able to achieve an average of 46 percent further energy reduction.},
  archive      = {J_TC},
  author       = {Wei Zhang and Hang Zhang and John Lach},
  doi          = {10.1109/TC.2020.3029306},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1875-1886},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Extending performance-energy trade-offs via dynamic core scaling},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster-aware scattered repair in erasure-coded storage:
Design and analysis. <em>TC</em>, <em>70</em>(11), 1861–1874. (<a
href="https://doi.org/10.1109/TC.2020.3028353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure coding is a storage-efficient means to guarantee data reliability in today&#39;s commodity storage systems, yet its repair performance is seriously hindered by the substantial repair traffic. Repair in clustered storage systems is even complicated because of the scarcity of the cross-cluster bandwidth. We present ${\sf ClusterSR}$ , a cluster-aware scattered repair approach. ${\sf ClusterSR}$ minimizes the cross-cluster repair traffic by carefully choosing the clusters for reading and repairing chunks. It further balances the cross-cluster repair traffic by scheduling the repair of multiple chunks. Large-scale simulation and Alibaba Cloud ECS experiments show that ${\sf ClusterSR}$ can reduce 5.6-52.7 percent of the cross-cluster repair traffic and improve 14.4–68.8 percent of the repair throughput.},
  archive      = {J_TC},
  author       = {Zhirong Shen and Shiyao Lin and Jiwu Shu and Chengxin Xie and Zhijie Huang and Yingxun Fu},
  doi          = {10.1109/TC.2020.3028353},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1861-1874},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cluster-aware scattered repair in erasure-coded storage: Design and analysis},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Snitch: A tiny pseudo dual-issue processor for area and
energy efficient execution of floating-point intensive workloads.
<em>TC</em>, <em>70</em>(11), 1845–1860. (<a
href="https://doi.org/10.1109/TC.2020.3027900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-parallel applications, such as data analytics, machine learning, and scientific computing, are placing an ever-growing demand on floating-point operations per second on emerging systems. With increasing integration density, the quest for energy efficiency becomes the number one design concern. While dedicated accelerators provide high energy efficiency, they are over-specialized and hard to adjust to algorithmic changes. We propose an architectural concept that tackles the issues of achieving extreme energy efficiency while still maintaining high flexibility as a general-purpose compute engine. The key idea is to pair a tiny 10kGE (kilo gate equivalent) control core, called Snitch, with a double-precision floating-point unit (FPU) to adjust the compute to control ratio. While traditionally minimizing non-floating-point unit (FPU) area and achieving high floating-point utilization has been a trade-off, with Snitch, we achieve them both, by enhancing the ISA with two minimally intrusive extensions: stream semantic registers (SSR) and a floating-point repetition instruction (FREP). SSRs allow the core to implicitly encode load/store instructions as register reads/writes, eliding many explicit memory instructions. The FREP extension decouples the floating-point and integer pipeline by sequencing instructions from a micro-loop buffer. These ISA extensions significantly reduce the pressure on the core and free it up for other tasks, making Snitch and FPU effectively dual-issue at a minimal incremental cost of 3.2 percent. The two low overhead ISA extensions make Snitch more flexible than a contemporary vector processor lane, achieving a $2\times$ energy-efficiency improvement. We have evaluated the proposed core and ISA extensions on an octa-core cluster in 22 nm technology. We achieve more than $6\times$ multi-core speed-up and a $3.5\times$ gain in energy efficiency on several parallel microkernels.},
  archive      = {J_TC},
  author       = {Florian Zaruba and Fabian Schuiki and Torsten Hoefler and Luca Benini},
  doi          = {10.1109/TC.2020.3027900},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1845-1860},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Snitch: A tiny pseudo dual-issue processor for area and energy efficient execution of floating-point intensive workloads},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OurRocks: Offloading disk scan directly to GPU in
write-optimized database system. <em>TC</em>, <em>70</em>(11),
1831–1844. (<a href="https://doi.org/10.1109/TC.2020.3027671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The log structured merge (LSM) tree has been widely adopted by database systems owing to its superior write performance. However, LSM-tree based databases face vulnerabilities when processing analytical queries due to the read amplification caused by its architecture and the limited use of storage devices with high bandwidth. To flexibly handle transactional and analytical workloads, we proposed and implemented OurRocks taking full advantage of NVMe SSD and GPU devices, which improves scan performance. Although the NVMe SSD serves multi GB/s I/O rates, it is necessary to solve the data transfer overhead which limits the benefits of the GPU processing. The primary idea is to offload the scan operation to the GPU with filtering predicate pushdown and resolve the bottleneck from the data transfer between devices with direct memory access (DMA). OurRocks benefits from all the features of write-optimized database systems, in addition to accelerating the analytic queries using the aforementioned idea. Experimental results indicate that OurRocks effectively leverages resources of the NVMe SSD and GPU and significantly improves the execution of queries in the YCSB and TPC-H benchmarks, compared to the conventional write-optimized database. Our research demonstrates that the proposed approach can speed up the handling of the data-intensive workloads.},
  archive      = {J_TC},
  author       = {Won Gi Choi and Doyoung Kim and Hongchan Roh and Sanghyun Park},
  doi          = {10.1109/TC.2020.3027671},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1831-1844},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OurRocks: Offloading disk scan directly to GPU in write-optimized database system},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On performance optimization and quality control for
approximate-communication-enabled networks-on-chip. <em>TC</em>,
<em>70</em>(11), 1817–1830. (<a
href="https://doi.org/10.1109/TC.2020.3027182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many applications showing error forgiveness, approximate computing is a new design paradigm that trades application output accuracy for mitigating computation/communication effort, which results in performance/energy benefit. Since networks-on-chip (NoCs) are one of the major contributors to system performance and power consumption, the underlying communication is approximated to achieve time/energy improvement. However, performing approximation blindly causes unacceptable quality loss. In this article, first, an optimization problem to maximize NoC performance is formulated with the constraint of application quality requirement, and the application quality loss is studied. Second, a congestion-aware quality control method is proposed to improve system performance by aggressively dropping network data, which is based on flow prediction and a lightweight heuristic. In the experiments, two recent approximation methods for NoCs are augmented with our proposed control method to compare with their original ones. Experimental results show that our proposed method can speed up execution by as much as 29.42\% over the two state-of-the-art works.},
  archive      = {J_TC},
  author       = {Siyuan Xiao and Xiaohang Wang and Maurizio Palesi and Amit Kumar Singh and Liang Wang and Terrence Mak},
  doi          = {10.1109/TC.2020.3027182},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1817-1830},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On performance optimization and quality control for approximate-communication-enabled networks-on-chip},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MIPSGPU: Minimizing pipeline stalls for GPUs with
non-blocking execution. <em>TC</em>, <em>70</em>(11), 1804–1816. (<a
href="https://doi.org/10.1109/TC.2020.3026043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the latency hiding ability is important for GPU performance. Although existing works, which mainly target on either improving thread level parallelism or optimizing memory hierarchy, are effective at improving GPUs’ latency hiding ability, warps are still blocked after executing long latency operations, reducing the number of schedulable warps. This article revisits the recently proposed non-blocking execution for GPUs to improve the latency hiding ability of GPUs. With non-blocking execution, instructions from warps blocked by long latency operations can be pre-executed to make full use of GPU resources. However, we find that the state-of-the-art non-blocking GPU architecture gains limited performance improvement. Through in-depth analysis, we observe that the poor performance is largely due to inefficient pre-execution state management, duplicate instruction extraction, frequent early eviction and severe resource congestion. To make non-blocking execution actually useful for GPUs and minimize hardware overheads, we carefully redesign the non-blocking architecture for GPUs based on our analysis and propose MIPSGPU . Our evaluations show that MIPSGPU , relative to the state-of-the-art non-blocking GPU architecture, improves performance of memory intensive applications by 19.05 percent, and reduces memory to SM traffics by 14 percent.},
  archive      = {J_TC},
  author       = {Chao Yu and Yuebin Bai and Rui Wang},
  doi          = {10.1109/TC.2020.3026043},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1804-1816},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MIPSGPU: Minimizing pipeline stalls for GPUs with non-blocking execution},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Secure lightweight key exchange using ECC for user-gateway
paradigm. <em>TC</em>, <em>70</em>(11), 1789–1803. (<a
href="https://doi.org/10.1109/TC.2020.3026027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT)- based services connect the user with sensing devices through intermediary devices like a gateway. The authentication with secure key exchange assures security trio of confidentiality, integrity, and availability for the complete IoT-based system. It also ensures trusted privacy of communicated information. Recently, numerous authors proposed a Remote User Authentication scheme (RUA) on User-Gateway based secure key exchange for the tiny and lightweight sensor based paradigms such as an IoT and a wireless sensor network (WSN) using lightweight mathematical approaches. In this article, we propose a reliable and efficient lightweight key exchange scheme with the secure RUA for the user-gateway model. For designing a lightweight RUA scheme, we use elliptic curve cryptography (ECC). We use BAN-Logic for the validation of mutual authentication. We use widely accepted random oracle-based ROR model for formal security analysis and Dolev-Yao channel for informal security analysis. For the implementation and result collection of the proposed scheme, we used publisher-subscriber-based lightweight Message Queuing Telemetry Transport (MQTT) protocol. Overall, in this article, we propose a highly secure and efficient RUA scheme for the sensor-based environment with the formal security analysis and real-time implementation.},
  archive      = {J_TC},
  author       = {Chintan Patel and Nishant Doshi},
  doi          = {10.1109/TC.2020.3026027},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1789-1803},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Secure lightweight key exchange using ECC for user-gateway paradigm},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Qubit mapping based on subgraph isomorphism and filtered
depth-limited search. <em>TC</em>, <em>70</em>(11), 1777–1788. (<a
href="https://doi.org/10.1109/TC.2020.3023247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping logical quantum circuits to Noisy Intermediate-Scale Quantum (NISQ) devices is a challenging problem which has attracted rapidly increasing interests from both quantum and classical computing communities. This article proposes an efficient method by (i) selecting an initial mapping that takes into consideration the similarity between the architecture graph of the given NISQ device and a graph induced by the input logical circuit and (ii) searching, in a filtered and depth-limited way, a most useful swap combination that makes executable as many as possible two-qubit gates in the logical circuit. The proposed circuit transformation algorithm can significantly decrease the number of auxiliary two-qubit gates required to be added to the logical circuit, especially when it has a large number of two-qubit gates. For an extensive benchmark set of 131 circuits and IBM&#39;s current premium Q system, viz., IBM Q Tokyo, our algorithm needs, in average, 0.3801 extra two-qubit gates per input two-qubit gate, while the corresponding figures for three state-of-the-art algorithms are 0.4705, 0.8154, and 1.0066, respectively.},
  archive      = {J_TC},
  author       = {Sanjiang Li and Xiangzhen Zhou and Yuan Feng},
  doi          = {10.1109/TC.2020.3023247},
  journal      = {IEEE Transactions on Computers},
  number       = {11},
  pages        = {1777-1788},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Qubit mapping based on subgraph isomorphism and filtered depth-limited search},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SSD-assisted ransomware detection and data recovery
techniques. <em>TC</em>, <em>70</em>(10), 1762–1776. (<a
href="https://doi.org/10.1109/TC.2020.3011214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As ransomware attacks have been prevalent, it becomes crucial to make anti-ransomware solutions that defend against ransomwares. In this article, we propose a new ransomware defense system, called SSD-Insider++ , which prevents users’ files from being damaged by ransomware attacks. SSD-Insider++ is embedded into an SSD controller as a form of firmware. By being separated from a host machine, it not only provides more robust data protection than software-based ones which are vulnerable to evasion attacks, but also offers interoperability with various platforms. SSD-Insider++ is composed of two novel features, ransomware detection and perfect data recovery, which are tightly integrated with each other. The detection algorithm observes I/O patterns of a host system and decides whether the host is being attacked by ransomwares in an early stage. Once an encryption attack is detected, the recovery algorithm is triggered to recover original files by leveraging a delayed deletion feature of an SSD at a low cost. Our experimental results show that SSD-Insider++ achieves high accuracy of detecting ransomwares with 0 percent FRR/FAR in most cases and provides an instant data recovery with 0 percent data loss. The overhead of running SSD-Insider++ is negligible – only 80 $n$ s and 226 $n$ s are spent more for handling 4-KB reads and writes, respectively.},
  archive      = {J_TC},
  author       = {Sungha Baek and Youngdon Jung and David Mohaisen and Sungjin Lee and DaeHun Nyang},
  doi          = {10.1109/TC.2020.3011214},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1762-1776},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SSD-assisted ransomware detection and data recovery techniques},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The HPC-DAG task model for heterogeneous real-time systems.
<em>TC</em>, <em>70</em>(10), 1747–1761. (<a
href="https://doi.org/10.1109/TC.2020.3023169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent commercial hardware platforms for embedded real-time systems feature heterogeneous processing units and computing accelerators on the same System-on-Chip. When designing complex real-time applications for such architectures, the designer is exposed to a number of difficult choices, like deciding on which compute engine to execute a certain task, or what degree of parallelism to adopt for a given function. To help the designer exploring the wide space of design choices and tune the scheduling parameters, we propose a novel real-time application model, called HPC-DAG ( Heterogeneous Parallel Condition Directed Acyclic Graph Model ), specifically conceived for heterogeneous platforms. An HPC-DAG allows the system designer to specify alternative implementations of a software component for different processing engines, as well as conditional branches to model if-then-else statements. We also propose a schedulability analysis for the HPC-DAG model and a set of heuristic allocation algorithms aimed at improving schedulability for latency sensitive applications. Our analysis takes into account the cost of preempting a task, which can be non-negligible on certain processors. We show the use of our approach on a realistic case study, and we demonstrate its effectiveness by comparing it with state-of-the-art algorithms previously proposed in literature.},
  archive      = {J_TC},
  author       = {Zahaf Houssam-Eddine and Nicola Capodieci and Roberto Cavicchioli and Giuseppe Lipari and Marko Bertogna},
  doi          = {10.1109/TC.2020.3023169},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1747-1761},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The HPC-DAG task model for heterogeneous real-time systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel <span
class="math inline"><em>G</em><em>F</em>(2<sup><em>m</em></sup>)</span>GF2m
digit-serial PISO multipliers for the self-dual gaussian normal bases.
<em>TC</em>, <em>70</em>(10), 1732–1746. (<a
href="https://doi.org/10.1109/TC.2020.3023131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security protocols such as Transport Layer Security implement the Elliptic curve digital signature algorithm (ECDSA) over different binary extension fields defined by the National Institute of Standards and Technology (NIST). Specifically, such multiple cipher-suite support is a security recommendation. Binary extension field arithmetic processors are expensive, especially if more than one field is supported. In this context, this article introduces a novel lightweight digit-serial parallel-in-serial-out (DS-PISO) design for versatile multiplication (DS-VPISO) targeting the NIST-like fields in resource constrained embedded systems where the crypto module allocation is limited. The proposed DS-VPISO multiplier offers competitive area (up to 40 percent area savings) compared to existing multiple field multiplier schemes based on results conducted on bit-serial implementations using Intel&#39;s Field programmable gate arrays. The article first presents a DS-PISO self-dual Gaussian normal basis multiplication architecture based on the trace mapping. After this, the article extends the new trace based DS-PISO multiplier to construct an architecture for the first versatile DS-PISO multiplication (DS-VPISO) targeting NIST&#39;s binary fields. The latter extension to versatile multiplication is based on novel architectures for versatile cyclic shifts and versatile multiplication by normal elements.},
  archive      = {J_TC},
  author       = {Hayssam El-Razouk and Kirthi Kotha and Mahidhar Puligunta},
  doi          = {10.1109/TC.2020.3023131},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1732-1746},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Novel $GF\left(2^{m}\right)$GF2m digit-serial PISO multipliers for the self-dual gaussian normal bases},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel measurement for network reliability. <em>TC</em>,
<em>70</em>(10), 1719–1731. (<a
href="https://doi.org/10.1109/TC.2020.3023120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attackers in a network may have a tendency of targeting on a group of clustered nodes, and they hope to avoid the existence of significant large communication groups in the remaining network, such as botnet attack, DDoS attack, and Local Area Network Denial attack. Current various kinds of connectivity do not well reflect the fault tolerance of a network under these attacks. This observation inspires a new measure for network reliability to resist the block attack by taking into account of the dispersity of the remaining nodes. Let $G$ be a network, $C\subset V(G)$ and $G[C]$ be a connected subgraph . Then $C$ is called an $h$h-faulty-block of $G$ if $G-C$ is disconnected, and every component of $G-C$ has at least $h+1$ nodes. The minimum cardinality over all $h$ -faulty-blocks of $G$ is called $h$h-faulty-block connectivity of $G$ , denoted by ${FB}\kappa _h(G)$ . In this article, we determine ${FB}\kappa _h(Q_n)$ for $n$ -dimensional hypercube $Q_n$ ( $n\geq 4$ ), a classic interconnection network. We establish that ${FB}\kappa _h(Q_n)=(h+2)n-3h-1$ for $0\leq h\leq 1$ , and ${FB}\kappa _h(Q_n)=(h+2)n-4h+1$ for $2\leq h\leq n-2$ , respectively. Larger $h$ -faulty-block connectivity implies that an attacker will have to stage an attack to a bigger block of connected nodes, so that each remaining components will not be too small, which will in turn limit the size of large components. In other words, there will not be great disparity in sizes between any two remaining components, and hence there will less likely be a significantly large remaining communication group. The larger the $h$ -faulty-block, the more difficult for an attacker to achieve that goal. As a consequence, the resistance of the network against the attacker will increase. Our experiments also show that as $h$ increases, the $h$ -faulty-block gets larger, and the size disparity between any two remaining components decreases. In turn, as expected, the size of the largest remaining communication group becomes smaller.},
  archive      = {J_TC},
  author       = {Limei Lin and Yanze Huang and Dajin Wang and Sun-Yuan Hsieh and Li Xu},
  doi          = {10.1109/TC.2020.3023120},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1719-1731},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel measurement for network reliability},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supersingular isogeny key encapsulation (SIKE) round 2 on
ARM cortex-m4. <em>TC</em>, <em>70</em>(10), 1705–1718. (<a
href="https://doi.org/10.1109/TC.2020.3023045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first practical software implementation of Supersingular Isogeny Key Encapsulation (SIKE) round 2, targeting NIST&#39;s 1, 2, 3, and 5 security levels on 32-bit ARM Cortex-M4 microcontrollers. The proposed library introduces a new speed record of all SIKE Round 2 protocols with reasonable memory consumption on the low-end target platform. We achieved this record by adopting several state-of-the-art engineering techniques as well as highly-optimized hand-crafted assembly implementation of finite field arithmetic. In particular, we carefully redesign the previous optimized implementations of finite field arithmetic on the 32-bit ARM Cortex-M4 platform and propose a set of novel techniques which are explicitly suitable for SIKE primes. The benchmark result on STM32F4 Discovery board equipped with 32-bit ARM Cortex-M4 microcontrollers shows that entire key encapsulation and decapsultation over SIKEp434 take about 184 million clock cycles (i.e., 1.09 seconds @168 MHz). In contrast to the previous optimized implementation of the isogeny-based key exchange on low-end 32-bit ARM Cortex-M4, our performance evaluation shows feasibility of using SIKE mechanism on the low-end platform. In comparison to the most of the post-quantum candidates, SIKE requires an excessive number of arithmetic operations, resulting in significantly slower timings. However, its small key size makes this scheme as a promising candidate on low-end microcontrollers in the quantum era by ensuring the lower energy consumption for key transmission than other schemes.},
  archive      = {J_TC},
  author       = {Hwajeong Seo and Mila Anastasova and Amir Jalali and Reza Azarderakhsh},
  doi          = {10.1109/TC.2020.3023045},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1705-1718},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Supersingular isogeny key encapsulation (SIKE) round 2 on ARM cortex-m4},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural network-based performance prediction for task
migration on s-NUCA many-cores. <em>TC</em>, <em>70</em>(10), 1691–1704.
(<a href="https://doi.org/10.1109/TC.2020.3023022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a task running on a many-core with distributed shared last-level cache (LLC) strongly depends on two parameters: the power budget needed to guarantee thermally-safe operation and the LLC latency. The task&#39;s thread-to-core mapping determines both the parameters and needs to make a trade-off because both cannot be simultaneously optimal. Arrival and departure of tasks on a many-core deployed in an open system can change its state significantly in terms of available cores and power budgets. Task migrations can thereupon be used as a tool to keep the many-core operating at peak performance. Furthermore, the relative impacts of power budget and LLC latency on a task&#39;s performance may change with its different execution phases mandating its migration on-the-fly. We propose the first run-time algorithm PCMig that increases the performance of a many-core with distributed shared LLC by migrating tasks based on their phases and the many-core&#39;s state. PCMig is based on a model that predicts the performance impact of migrations. We propose a performance prediction model based on a lightweight neural network (NN). To serve as a reference, we also propose an analytical model of the many-core that operates on CPI stacks. We demonstrate an NN-based model achieves a higher prediction accuracy at a lower overhead than an analytical model. PCMig is based on the NN prediction model and results in an up to 7.3 percent increase in performance under a thermal constraint for mixed workloads compared to architecture-aware state-of-the-art (up to 20 percent increase for individual applications). This is achieved with a run-time overhead of less than 0.5 percent.},
  archive      = {J_TC},
  author       = {Martin Rapp and Anuj Pathania and Tulika Mitra and Jörg Henkel},
  doi          = {10.1109/TC.2020.3023022},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1691-1704},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Neural network-based performance prediction for task migration on S-NUCA many-cores},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hardware private circuits: From trivial composition to full
verification. <em>TC</em>, <em>70</em>(10), 1677–1690. (<a
href="https://doi.org/10.1109/TC.2020.3022979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of glitch-resistant higher-order masking schemes is an important challenge in cryptographic engineering. A recent work by Moos et al. (CHES 2019) showed that most published schemes (and all efficient ones) exhibit local or composability flaws at high security orders, leaving a critical gap in the literature on hardware masking. In this article, we first extend the simulatability framework of Belaïd et al. (EUROCRYPT 2016) and prove that a compositional strategy that is correct without glitches remains valid with glitches. We then use this extended framework to prove the first masked gadgets that enable trivial composition with glitches at arbitrary orders. We show that the resulting “Hardware Private Circuits” approach the implementation efficiency of previous (flawed) schemes. We finally investigate how trivial composition can serve as a basis for a tool that allows verifying full masked hardware implementations (e.g., of complete block ciphers) at any security order from their HDL code. As side products, we improve the randomness complexity of the best published refreshing gadgets, show that some S-box representations allow latency reductions and confirm practical claims based on implementation results.},
  archive      = {J_TC},
  author       = {Gaëtan Cassiers and Benjamin Grégoire and Itamar Levi and François-Xavier Standaert},
  doi          = {10.1109/TC.2020.3022979},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1677-1690},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware private circuits: From trivial composition to full verification},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A change-detection-based thompson sampling framework for
non-stationary bandits. <em>TC</em>, <em>70</em>(10), 1670–1676. (<a
href="https://doi.org/10.1109/TC.2020.3022634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a non-stationary two-armed bandit framework and propose a change-detection-based Thompson sampling (TS) algorithm, named TS with change-detection (TS-CD), to keep track of the dynamic environment. The non-stationarity is modeled using a Poisson arrival process, which changes the mean of the rewards on each arrival. The proposed strategy compares the empirical mean of the recent rewards of an arm with the estimate of the mean of the rewards from its history. It detects a change when the empirical mean deviates from the mean estimate by a value larger than a threshold. Then, we characterize the lower bound on the duration of the time-window for which the bandit framework must remain stationary for TS-CD to successfully detect a change when it occurs. Consequently, our results highlight an upper bound on the parameter for the Poisson arrival process, for which the TS-CD achieves asymptotic regret optimality with high probability. Finally, we validate the efficacy of TS-CD by testing it for edge-control of radio access technique (RAT)-selection in a wireless network. Our results show that TS-CD not only outperforms the classical max-power RAT selection strategy but also other actively adaptive and passively adaptive bandit algorithms that are designed for non-stationary environments.},
  archive      = {J_TC},
  author       = {Gourab Ghatak},
  doi          = {10.1109/TC.2020.3022634},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1670-1676},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A change-detection-based thompson sampling framework for non-stationary bandits},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of optimized CNNs on heterogeneous accelerators
using a novel benchmarking approach. <em>TC</em>, <em>70</em>(10),
1654–1669. (<a href="https://doi.org/10.1109/TC.2020.3022318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous algorithmic optimization techniques have been proposed to alleviate the computational complexity of convolutional neural networks. Given the broad selection of AI accelerators, it is not obvious which approach benefits from which optimization most. The design space includes a large number of deployment settings (batch sizes, power modes, etc.) and unclear measurement methods. This research provides clarity into this design space, leveraging a novel benchmarking approach. We provide a theoretical evaluation of different CNNs and hardware platforms, focusing on understanding the impact of pruning and quantization as primary optimization techniques. We benchmark across a spectrum of FPGA, GPU, TPU, and VLIW processors for systematically pruned and quantized neural networks (ResNet50, GoogLeNetv1, MobileNetv1, a VGG derivative, a multilayer perceptron) over many deployment options, considering power, latency, and throughput at a specific accuracy. Our findings show that channel pruning is most effective and works across most hardware platforms, with speedups directly correlated to the reduction in compute load, while FPGAs benefit the most from quantization. Pruning and quantization are orthogonal, and yield optimal design points when combined. Further in-depth results can be found at our web portal, where we share all experimental data, provide data analytics, and invite the community to contribute.},
  archive      = {J_TC},
  author       = {Michaela Blott and Nicholas J. Fraser and Giulio Gambardella and Lisa Halder and Johannes Kath and Zachary Neveu and Yaman Umuroglu and Alina Vasilciuc and Miriam Leeser and Linda Doyle},
  doi          = {10.1109/TC.2020.3022318},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1654-1669},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Evaluation of optimized CNNs on heterogeneous accelerators using a novel benchmarking approach},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual wall: Filtering rootkit attacks to protect linux
kernel functions. <em>TC</em>, <em>70</em>(10), 1640–1653. (<a
href="https://doi.org/10.1109/TC.2020.3022023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linux servers are being used in almost all clouds, datacenters and supercomputers today. Linux Kernel functions are facing a kind of malware attacks, known as rootkits with root-access capability. The rootkits appear as loadable kernel modules (LKM) in today&#39;s Linux servers. These modules hide from other kernel objects, and can redirect the kernel control flow by tampering with the metadata needed in kernel service functions. The kernel rootkits are invisible to users after loading, which may bypass most security shields. Both spatial and temporal appearance of rootkits are randomly distributed, which makes it difficult to detect or removal. To deal with rootkit threats, we propose a novel Virtual Wall (VTW) approach to filtering out the rootkit-embedded LKMs by tracing the incurred kernel activities. This VTW is essentially a lightweight hypervisor built with rootkit detection and event tracing capabilities. Normally, the Linux runs in a guest mode. When a LKM execution violates the security policy set by the VTW, the OS control will switch to a host mode. The VTW at host mode enables the detection and tracing of rootkit events timely. In other words, potential rootkit attacks are detected, traced and classified to make meaningful filtering decisions. The whole detection and tracing process is based on memory access control and event injection mechanisms. Experimental results show that the VTW defense system is effective to detect and defend against kernel rootkits timely. The CPU overhead for executing VTW is less than 2 percent. Compared with other defense schemes (such as DIKernel, etc.), our vs is easier to implement with low performance degradation on Linux servers. We will demonstrate the advantages of VTW through its simplicity in implementation and potential performance gains. We will also compare our system with seven other rootkit defense systems.},
  archive      = {J_TC},
  author       = {Yong-Gang Li and Yeh-Ching Chung and Kai Hwang and Yue-Jin Li},
  doi          = {10.1109/TC.2020.3022023},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1640-1653},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Virtual wall: Filtering rootkit attacks to protect linux kernel functions},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CRIME: Input-dependent collaborative inference for recurrent
neural networks. <em>TC</em>, <em>70</em>(10), 1626–1639. (<a
href="https://doi.org/10.1109/TC.2020.3021199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The excellent accuracy of Recurrent Neural Networks (RNNs) for time-series and natural language processing comes at the cost of computational complexity. Therefore, the choice between edge and cloud computing for RNN inference, with the goal of minimizing response time or energy consumption, is not trivial. An edge approach must deal with the aforementioned complexity, while a cloud solution pays large time and energy costs for data transmission. Collaborative inference is a technique that tries to obtain the best of both worlds, by splitting the inference task among a network of collaborating devices. While already investigated for other types of neural networks, collaborative inference for RNNs poses completely new challenges, such as the strong influence of input length on processing time and energy, and is greatly unexplored. In this article, we introduce a Collaborative RNN Inference Mapping Engine (CRIME), which automatically selects the best inference device for each input. CRIME is flexible with respect to the connection topology among collaborating devices, and adapts to changes in the connections statuses and in the devices loads. With experiments on several RNNs and datasets, we show that CRIME can reduce the execution time (or end-node energy) by more than 25 percent compared to any single-device approach.},
  archive      = {J_TC},
  author       = {Daniele Jahier Pagliari and Roberta Chiaro and Enrico Macii and Massimo Poncino},
  doi          = {10.1109/TC.2020.3021199},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1626-1639},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CRIME: Input-dependent collaborative inference for recurrent neural networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical and secure SVM classification for cloud-based
remote clinical decision services. <em>TC</em>, <em>70</em>(10),
1612–1625. (<a href="https://doi.org/10.1109/TC.2020.3020545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) classification techniques have been widely adopted for building clinical decision models. In cloud-based remote clinical decision services, a healthcare center outsources the clinical decision model to a cloud server, which then provides remote clinical decision services to end users. In this article, we propose a practical and secure SVM classification scheme ( ${\sf SSVMC}$ ) for cloud-based remote clinical decision services. Specifically, we first extract SVM decision rules from an SVM classifier. Then, we leverage symmetric key encryption to protect the confidentiality of medical data and prevent the cloud service provider from misusing intellectual property of the outsourced clinical model. Finally, we build encrypted indexes to achieve efficient SVM classification. We define a leakage function, formulate a security definition, and provide a simulation-based security proof for ${\sf SSVMC}$ . The performance analysis demonstrates that ${\sf SSVMC}$ achieves linear computational complexity when an SVM classifier (a.k.a., the clinical decision model) is pre-trained. The simulations evaluate the impact of several parameters on time costs. The experimental evaluations show the performance differences between ${\sf SSVMC}$ and several existing schemes in terms of time costs, storage costs, communication costs, and precisions in a real-world clinical dataset, which demonstrate that ${\sf SSVMC}$ is computationally efficient with high decision accuracy.},
  archive      = {J_TC},
  author       = {Jinwen Liang and Zheng Qin and Jianbing Ni and Xiaodong Lin and Xuemin Shen},
  doi          = {10.1109/TC.2020.3020545},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1612-1625},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Practical and secure SVM classification for cloud-based remote clinical decision services},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enclavisor: A hardware-software co-design for enclaves on
untrusted cloud. <em>TC</em>, <em>70</em>(10), 1598–1611. (<a
href="https://doi.org/10.1109/TC.2020.3019704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The releases of Intel SGX and AMD SEV mark the transition of hardware-based enclaves from research prototypes to mainstream products. These two paradigms of secure enclaves are attractive to both the cloud providers and tenants, since security is one of the key pillars of cloud computing. However, it is found that current hardware-defined enclaves are not flexible and efficient enough for the cloud. For example, although SGX can provide strong memory protection with both confidentiality and integrity, the size of secure memory is tightly restricted. On the contrary, SEV enables enclaves to use more memory but has critical security flaws due to no memory integrity protection. Meanwhile, both types of enclaves have relatively long booting latency, which makes them not suitable for short-term tasks like serverless workloads. After an in-depth analysis, we find that there are some intrinsic tradeoffs between security and performance due to the limitation of architectural designs. In this article, we investigate a novel hardware-software co-design of enclaves to meet the requirements of cloud by placing a part of the logic of the enclave mechanism into a lightweight software layer, named Enclavisor, to achieve a balance between security, performance, and flexibility. Specifically, our implementation is based on AMD&#39;s SEV and, Enclavisor is placed in the guest kernel mode of SEV&#39;s secure virtual machines. Enclavisor inherently supports memory encryption with no memory limitation and also achieves efficient booting, multiple enclave granularities, and post-launch remote attestation. Meanwhile, we also propose hardware/software solutions to mitigate the security flaws caused by the lack of memory integrity. We implement a prototype of Enclavisor on an AMD SEV server. The experiments on both micro-benchmarks and application benchmarks show that enclaves on Enclavisor can have close-to-native performance.},
  archive      = {J_TC},
  author       = {Jinyu Gu and Xinyue Wu and Bojun Zhu and Yubin Xia and Binyu Zang and Haibing Guan and Haibo Chen},
  doi          = {10.1109/TC.2020.3019704},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1598-1611},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enclavisor: A hardware-software co-design for enclaves on untrusted cloud},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PStream: A popularity-aware differentiated distributed
stream processing system. <em>TC</em>, <em>70</em>(10), 1582–1597. (<a
href="https://doi.org/10.1109/TC.2020.3019689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world stream data with skewed distributions raises unique challenges to distributed stream processing systems. Existing stream workload partitioning schemes usually use a “one size fits all” design, which leverages either a shuffle grouping or a key grouping strategy for partitioning the stream workloads among multiple processing units, leading to notable problems of unsatisfied system throughput and processing latency. In this article, we show that the key grouping based schemes result in serious load imbalance and low computation efficiency in the presence of data skewness while the shuffle grouping schemes are not scalable in terms of memory space. We argue that the key to efficient stream scheduling is the popularity of the stream data. We propose PStream, a popularity-aware differentiated distributed stream processing system which assigns the hot keys using shuffle grouping while assigns rare ones using key grouping. PStream leverages a novel light-weighted probabilistic counting scheme for identifying the currently hot keys in dynamic real-time streams. The scheme is extremely efficient in computation and memory consumption, so that the predictor based on it can be well integrated into processing instances in the system. We further design an adaptive threshold configuration scheme, which can quickly adapt to the dynamical popularity changes in highly dynamical real-time streams. We implement PStream on top of Apache Storm and conduct comprehensive experiments using large-scale traces from real-world systems to evaluate the performance of this design. Results show that PStream achieves a 2.3× improvement in terms of processing throughput and reduces the processing latency by 64 percent compared to state-of-the-art designs.},
  archive      = {J_TC},
  author       = {Hanhua Chen and Fan Zhang and Hai Jin},
  doi          = {10.1109/TC.2020.3019689},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1582-1597},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PStream: A popularity-aware differentiated distributed stream processing system},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guardauto: A decentralized runtime protection system for
autonomous driving. <em>TC</em>, <em>70</em>(10), 1569–1581. (<a
href="https://doi.org/10.1109/TC.2020.3018329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the broad attack surface and the lack of runtime protection, potential safety and security threats hinder the real-life adoption of autonomous vehicles. Although efforts have been made to mitigate some specific attacks, there are few works on the protection of the autonomous driving system, i.e., the control software system performing such as perception, decision making, and motion tracking. This article presents a decentralized self-protection framework called Guardauto to protect the autonomous driving system against runtime threats. First, Guardauto proposes an isolation model to decouple the autonomous driving system and isolate its components with a set of partitions. Second, Guardauto provides self-protection mechanisms for each target component, which combines different methods to monitor the target execution and plan adaption actions accordingly. Third, Guardauto provides cooperation among local self-protection mechanisms to identify the root-cause component in the case of cascading failures affecting multiple components. A prototype has been implemented and evaluated on the open-source autonomous driving system Autoware. Results show that Guardauto could effectively mitigate runtime failures and attacks, and protect the control system with acceptable performance overhead.},
  archive      = {J_TC},
  author       = {Kun Cheng and Yuan Zhou and Bihuan Chen and Rui Wang and Yuebin Bai and Yang Liu},
  doi          = {10.1109/TC.2020.3018329},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1569-1581},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guardauto: A decentralized runtime protection system for autonomous driving},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A fast lock for explicit message passing architectures.
<em>TC</em>, <em>70</em>(10), 1555–1568. (<a
href="https://doi.org/10.1109/TC.2020.3015727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronization is a crucial issue for multi-threaded programs. Mutex locks are widely used in legacy programs and are still popular for the intuition semantics. The SW26010 architecture, deployed on the supercomputer Sunway TaihuLight, introduces a hardware-supported inter-core message passing mechanism and exposes explicit interfaces for developers to use its fast on-chip network. This emerging architectural feature brings both opportunities and challenges for mutex lock implementation. However, there is still no general lock mechanism, especially designed and optimized for architectures with this new feature. In this article, we propose mLock, a fast lock designed and optimized for architectures that support Explicit inter-core Message Passing (EMP). mLock uses partial cores as lock servers and leverages the fast on-chip network to implement high-performance mutual exclusive locks. In this article, we propose a series of novel techniques to improve the performance of EMP locks. First, we propose the concepts of chaining lock and hierarchical lock to reduce message count and mitigate network congestion. Second, we propose a fair lock approach to improve the fairness of EMP locks. Third, server reusing is introduced to reduce the number of lock servers. We implement and evaluate mLock on an SW26010 processor. Experimental results show that our proposed techniques can improve the performance of EMP locks by up to $16.2\times$ over a basic design.},
  archive      = {J_TC},
  author       = {Xiongchao Tang and Chen Zhang and Jidong Zhai and Xuehai Qian and Wenguang Chen and Yong Jiang},
  doi          = {10.1109/TC.2020.3015727},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1555-1568},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A fast lock for explicit message passing architectures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Runtime performance optimization of 3-d microprocessors in
dark silicon. <em>TC</em>, <em>70</em>(10), 1539–1554. (<a
href="https://doi.org/10.1109/TC.2020.3015711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because the increasing power density is limited by the thermal constraint, multi-core integrated systems have stepped into the dark silicon era recently, meaning not all parts of the system can be powered on at the same time. Dark silicon effects are, especially severe for 3-D microprocessors due to the even higher power density caused by the stacked structures, which greatly limit the system performances. In this article, we propose a greedy based core-cache co-optimization algorithm to optimize the performance of 3-D microprocessors in dark silicon at runtime. The new method determines many runtime settings of the 3-D system on the fly, including the active core and cache bank positions, active cache bank number, and the voltage/frequency (V/f) level of each active core, which optimizes the performance of the 3-D microprocessor under thermal constraint. Because the core-cache settings are co-optimized in the 3-D space and the power budgets are computed dynamically according to the running state of the 3-D microprocessor, the new method leads to a higher system performance compared with the existing methods. Experiments on two 3-D microprocessors show the greedy-based core-cache co-optimization algorithm outperforms the state-of-the-art 3-D dark silicon microprocessor performance optimization method by achieving a higher processing throughput with guaranteed thermal safety.},
  archive      = {J_TC},
  author       = {Hai Wang and Wei Li and Wenjie Qi and Diya Tang and Letian Huang and He Tang},
  doi          = {10.1109/TC.2020.3015711},
  journal      = {IEEE Transactions on Computers},
  number       = {10},
  pages        = {1539-1554},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Runtime performance optimization of 3-D microprocessors in dark silicon},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling data reuse in deep neural networks by taking
data-types into cognizance. <em>TC</em>, <em>70</em>(9), 1526–1538. (<a
href="https://doi.org/10.1109/TC.2020.3015531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, researchers have focused on reducing the model size and number of computations (measured as “multiply-accumulate” or MAC operations) of DNNs. The energy consumption of a DNN depends on both the number of MAC operations and the energy efficiency of each MAC operation. The former can be estimated at design time; however, the latter depends on the intricate data reuse patterns and underlying hardware architecture. Hence, estimating it at design time is challenging. This article shows that the conventional approach to estimate the data reuse, viz. arithmetic intensity, does not always correctly estimate the degree of data reuse in DNNs since it gives equal importance to all the data types. We propose a novel model, termed “data type aware weighted arithmetic intensity” (DI), which accounts for the unequal importance of different data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs on two GPUs. We show that our model accurately models data-reuse for all possible data reuse patterns for different types of convolution and different types of layers. We show that our model is a better indicator of the energy efficiency of DNNs. We also show its generality using the central limit theorem.},
  archive      = {J_TC},
  author       = {Nandan Kumar Jha and Sparsh Mittal},
  doi          = {10.1109/TC.2020.3015531},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1526-1538},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Modeling data reuse in deep neural networks by taking data-types into cognizance},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EnGN: A high-throughput and energy-efficient accelerator for
large graph neural networks. <em>TC</em>, <em>70</em>(9), 1511–1525. (<a
href="https://doi.org/10.1109/TC.2020.3014632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) emerge as a powerful approach to process non-euclidean data structures and have been proved powerful in various application domains such as social networks and e-commerce. While such graph data maintained in real-world systems can be extremely large and sparse, thus employing GNNs to deal with them requires substantial computational and memory overhead, which induces considerable energy and resource cost on CPUs and GPUs. In this article, we present a specialized accelerator architecture, EnGN, to enable high-throughput and energy-efficient processing of large-scale GNNs. The proposed EnGN is designed to accelerate the three key stages of GNN propagation, which is abstracted as common computing patterns shared by typical GNNs. To support the key stages simultaneously, we propose the ring-edge-reduce(RER) dataflow that tames the poor locality of sparsely-and-randomly connected vertices, and the RER PE-array to practice RER dataflow. In addition, we utilize a graph tiling strategy to fit large graphs into EnGN and make good use of the hierarchical on-chip buffers through adaptive computation reordering and tile scheduling. Overall, EnGN achieves performance speedup by 1802.9X, 19.75X, and 2.97X and energy efficiency by 1326.35X, 304.43X, and 6.2X on average compared to CPU, GPU, and a state-of-the-art GCN accelerator HyGCN, respectively.},
  archive      = {J_TC},
  author       = {Shengwen Liang and Ying Wang and Cheng Liu and Lei He and Huawei LI and Dawen Xu and Xiaowei Li},
  doi          = {10.1109/TC.2020.3014632},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1511-1525},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TSE: Two-step elimination for MLC STT-RAM last-level cache.
<em>TC</em>, <em>70</em>(9), 1498–1510. (<a
href="https://doi.org/10.1109/TC.2020.3014361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spin-transfer torque RAM (STT-RAM) is an emerging non-volatile memory that has been recognized as the potential candidate to replace SRAM. Compared with SRAM, STT-RAM has advantages of non-volatility, zero leakage power, and higher density. To further improve data density, multi-level cell (MLC) STT-RAM that can store two bits per cell has been proposed. However, writing hard bit of a cell would write its soft bit to the same value as well, which complicates the write operation of MLC STT-RAM. Although two-step transition (TT) is usually adopted to ensure the data correctness during a write operation, it incurs overhead of additional energy consumption and performance degradation. In this article, we propose the two-step elimination (TSE) scheme to eliminate TTs while ensure data integrity. By flipping hard bits of the cells that suffered from TTs, the TSE scheme could reduce TTs to soft transitions (STs) or zero transitions (ZTs), which incur much less overhead than TTs. To keep track of the flipped cells effectively, 6-bit TSE tag is introduced. We exploit tag reversing and advanced mode of the TSE scheme to further improve the performance. The experimental results showed that our scheme could reduce 61 percent TTs and achieve significant lifetime improvement, compared with conventional MLC STT-RAM (CMLC) scheme.},
  archive      = {J_TC},
  author       = {Jen-Wei Hsieh and Yi-Yu Liu and Hung-Tse Lee and Tai Chang},
  doi          = {10.1109/TC.2020.3014361},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1498-1510},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TSE: Two-step elimination for MLC STT-RAM last-level cache},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Power-efficient heterogeneous many-core design with NCFET
technology. <em>TC</em>, <em>70</em>(9), 1484–1497. (<a
href="https://doi.org/10.1109/TC.2020.3013567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-/many-core, homogeneous or heterogeneous architectures, using the existing CMOS technology are inevitably approaching the limit of attainable power efficiency due to the fundamental limits in scaling. Negative Capacitance Field-Effect Transistor (NCFET) is rapidly emerging as an alternative technology that promises a multi-fold increase in the power efficiency of transistors, yet is compatible with the existing CMOS fabrication process. NCFET incorporates a ferroelectric (FE) layer within the transistor&#39;s gate stack, which exhibits a negative capacitance effect amplifying the internal voltage. NCFET has been in detail studied in both physics and devices/circuits communities where its superiority has been demonstrated in semiconductor measurements. However, the full promise of NCFET remains unmodeled and unquantified unless the research is further continued to the microarchitecture and system levels. This article, for the first time, explores system- and application-level benefits of NCFET-based multi-/many-core designs in terms of performance and power-efficiency compared to state-of-the-art FinFET-based designs. This exploration is done first through analytical modeling in which we extend Amdahl&#39;s law for NCFET multi-/many-cores, and then through quantitative modeling. The latter is achieved through RTL- and system-level simulations of NCFET-based multi-cores. The analytical modeling shows that a novel type of technology-based heterogeneity in which cores with the same microarchitecture but different FE thickness are combined is highly beneficial. Our exploration shows that this novel heterogeneity increases the power-efficiency by up to 3.5× over homogeneous systems and even achieves 8.3\% better performance and 20\% higher power-efficiency than conventional heterogeneity in the microarchitecture without having to cope with the complexity of managing different microarchitectures.},
  archive      = {J_TC},
  author       = {Sami Salamin and Martin Rapp and Anuj Pathania and Arka Maity and Jörg Henkel and Tulika Mitra and Hussam Amrouch},
  doi          = {10.1109/TC.2020.3013567},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1484-1497},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Power-efficient heterogeneous many-core design with NCFET technology},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adaptive CPU-GPU governing framework for mobile games on
big.LITTLE architectures. <em>TC</em>, <em>70</em>(9), 1472–1483. (<a
href="https://doi.org/10.1109/TC.2020.3012987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Games have been one of the most popular applications on smartphones. In order to meet the increasing computational complexity of mobile games, smartphones are now equipped with heterogeneous CPU multi-core architectures like big.LITTLE as well as high-performance GPUs. However, the integrated CPUs and GPUs drain the battery quickly, which has become a bottleneck for improving user experience. In addition to traditional Dynamic Voltage and Frequency Scaling (DVFS) technique for CPUs and GPUs power reduction, heterogeneous multi-core processors, such as the big.LITTLE architecture, have been designed to offer more opportunity for performance-energy tradeoffs. But current processor governors in smartphones can not exploit these power-saving mechanisms wisely, causing considerable energy waste. In this article, we propose a CPU-GPU governing framework that recognizes the performance demand for different game scenes, and select the most energy-efficient hardware configuration for the corresponding scenes. We implement our framework on an ODROID-XU4 mobile platform, and the experiments show that our framework can achieve 26.7, 16.6, and 10.5 percent power saving on average without compromising user experience when compared to the default governor used in our platform and two governors proposed by other researchers, respectively.},
  archive      = {J_TC},
  author       = {Xianfeng Li and Gengchao Li},
  doi          = {10.1109/TC.2020.3012987},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1472-1483},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An adaptive CPU-GPU governing framework for mobile games on big.LITTLE architectures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing the response time of memcached systems via model
and quantitative analysis. <em>TC</em>, <em>70</em>(9), 1458–1471. (<a
href="https://doi.org/10.1109/TC.2020.3011619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memcached is a widely used in-memory caching solution in large-scale searching scenarios. The most crucial metric of Memcached systems is the response time, which is affected by various factors such as workload, service rate, unbalanced load distribution, and cache miss ratio. This article aims to quantify the influence of each factor on the response time of Memcached systems. First, we establish a theoretical model for Memcached systems that captures their main features, including burst and concurrent key arrival, unbalanced load distribution, and cache miss process. By solving this model using queuing and stochastic theories, we obtain an estimate of the response time in Memcached systems. Intensive experiments based on real-world components demonstrate that the estimate always matches perfectly with the actual value. Furthermore, we obtain a comprehensive and quantitative understanding of all factors. The main insights are threefold. 1) There exists an optimum range of utilization at Memcached servers in which the response time is kept at a low level with a small penalty. 2) The influence of the cache miss ratio on the response time is logarithmic rather than linear. 3) The number of keys generated from an end-user request has the greatest impact in Memcached systems.},
  archive      = {J_TC},
  author       = {Wenxue Cheng and Fengyuan Ren and Wanchun Jiang and Tong Zhang},
  doi          = {10.1109/TC.2020.3011619},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1458-1471},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing the response time of memcached systems via model and quantitative analysis},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DMRlib: Easy-coding and efficient resource management for
job malleability. <em>TC</em>, <em>70</em>(9), 1443–1457. (<a
href="https://doi.org/10.1109/TC.2020.3022933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process malleability has proved to have a highly positive impact on the resource utilization and global productivity in data centers compared with the conventional static resource allocation policy. However, the non-negligible additional development effort this solution imposes has constrained its adoption by the scientific programming community. In this work, we present DMRlib, a library designed to offer the global advantages of process malleability while providing a minimalist MPI-like syntax. The library includes a series of predefined communication patterns that greatly ease the development of malleable applications. In addition, we deploy several scenarios to demonstrate the positive impact of process malleability featuring different scalability patterns. Concretely, we study two job submission modes (rigid and moldable) in order to identify the best-case scenarios for malleability using metrics such as resource allocation rate, completed jobs per second, and energy consumption. The experiments prove that our elastic approach may improve global throughput by a factor higher than 3x compared to the traditional workloads of non-malleable jobs.},
  archive      = {J_TC},
  author       = {Sergio Iserte and Rafael Mayo and Enrique S. Quintana-Ortí and Antonio J. Peña},
  doi          = {10.1109/TC.2020.3022933},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1443-1457},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DMRlib: Easy-coding and efficient resource management for job malleability},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LrGAN: A compact and energy efficient PIM-based architecture
for GAN training. <em>TC</em>, <em>70</em>(9), 1427–1442. (<a
href="https://doi.org/10.1109/TC.2020.3011122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful unsupervised learning method, Generative Adversarial Network (GAN) plays an essential role in many domains. However, training a GAN imposes four more challenges: (1) intensive communication caused by complex train phases of GAN; (2) much more ineffectual computations caused by peculiar convolutions; (3) more frequent off-chip memory accesses for exchanging intermediate data between the generator and the discriminator; and (4) high energy consumption of unnecessary fine-grained MLC programming. In this article, we propose LrGAN, a PIM-based GAN accelerator, to address the challenges of training GAN. We first propose a zero-free data reshaping scheme for ReRAM-based PIM, which removes the zero-related computations. We then propose a 3D-connected PIM, which can reconfigure connections inside PIM dynamically according to dataflows of propagation and updating. After that, we propose an approximate weight update algorithm to avoid unnecessary fine-grain MLC programming. Finally, we propose LrGAN based on these three techniques, providing different levels of accelerating GAN for programmers. Experiments show that LrGAN achieves 47.2×, 21.42×, and 7.46× speedup over FPGA-based GAN accelerator, GPU platform, and ReRAM-based neural network accelerator respectively. Besides, LrGAN achieves 13.65×, 10.75×, and 1.34× energy saving on average over GPU platform, PRIME, and FPGA-based GAN accelerator, respectively.},
  archive      = {J_TC},
  author       = {Haiyu Mao and Jiwu Shu and Mingcong Song and Tao Li},
  doi          = {10.1109/TC.2020.3011122},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1427-1442},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LrGAN: A compact and energy efficient PIM-based architecture for GAN training},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hardware acceleration of hash operations in modern
microprocessors. <em>TC</em>, <em>70</em>(9), 1412–1426. (<a
href="https://doi.org/10.1109/TC.2020.3010855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern microprocessors contain several special function units (SFUs) such as specialized arithmetic units, cryptographic processors, etc. In recent times, applications such as cloud computing, web-based search engines, and network applications are widely used, and place new demands on the microprocessor. Hashing is a key algorithm that is extensively used in such applications. Hashing can reduce the complexity of search and lookup from O(N) to O(N/n), where n bins are used. Hashing is typically performed in software. Thus, implementing a hardware-based hash unit on a modern microprocessor would potentially increase performance significantly. In this article, we propose a novel hardware hash unit (HU) design for use in modern microprocessors, at the microarchitecture level and at the circuit level. First, we present the design of the HU at the microarchitecture level. We simulate the HU to compare its performance with a software-based hash implementation. We demonstrate a significant speedup (up to 15×) for the HU. Furthermore, the performance scales elegantly with increasing database size and application diversity, without increasing the hardware cost. Second, we present the circuit design of the HU for use in modern microprocessors, using a 45nm technology. Our proposed hardware hash unit is based on the use of a content-addressable memory (CAM) to implement each bin of the hash table. We simulate the HU circuit and compare it with a traditional CAM design. We demonstrate an average power reduction of 5.48× using the HU over the traditional CAM. Also, we show that the HU can operate at a maximum frequency of 1.39 GHz (after accounting for process, voltage and temperature (PVT) variations and accounting for wiring parasitics). Furthermore, we present the delay, power and area trade-offs of the HU design with varying hash table sizes.},
  archive      = {J_TC},
  author       = {Abbas A. Fairouz and Monther Abusultan and Viacheslav V. Fedorov and Sunil P. Khatri},
  doi          = {10.1109/TC.2020.3010855},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1412-1426},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware acceleration of hash operations in modern microprocessors},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid quantum-classical approach to mitigating
measurement errors in quantum algorithms. <em>TC</em>, <em>70</em>(9),
1401–1411. (<a href="https://doi.org/10.1109/TC.2020.3009664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When noisy intermediate scalable quantum (NISQ) devices are applied in information processing, all of the stages through preparation, manipulation, and measurement of multipartite qubit states contain various types of noise that are generally hard to be verified in practice. In this article, we present a scheme to deal with unknown quantum noise and show that it can be used to mitigate errors in measurement readout with NISQ devices. Quantum detector tomography that identifies a type of noise in a measurement can be circumvented. The scheme applies single-qubit operations only, that are with relatively higher precision than measurement readout or two-qubit gates. A classical post-processing is then performed with measurement outcomes. The scheme is implemented in quantum algorithms with NISQ devices: the Bernstein-Vazirani algorithm and a quantum amplitude estimation algorithm in IBMQ_yorktown and IBMQ_essex. The enhancement in the statistics of the measurement outcomes is presented for both of the algorithms with NISQ devices.},
  archive      = {J_TC},
  author       = {Hyeokjea Kwon and Joonwoo Bae},
  doi          = {10.1109/TC.2020.3009664},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1401-1411},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A hybrid quantum-classical approach to mitigating measurement errors in quantum algorithms},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability enhanced heterogeneous phase change memory
architecture for performance and energy efficiency. <em>TC</em>,
<em>70</em>(9), 1388–1400. (<a
href="https://doi.org/10.1109/TC.2020.3009498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation memories have been actively researched to replace the existing memories like DRAM and flash in deep sub-micron process technology. Unlike the conventional charge-based memories, next-generation memories utilize the resistive properties of different materials to store and read a data. Among the next-generation memories, Phase Change Memory (PCM) is seen as a good choice for future memory systems, given its good read performance, process compatibility and scaling potential. To enhance the storage density, multi-level cell (MLC) operation is seemed promising which can store more than one bit in each PCM cell. However, MLC operation significantly degrades the reliability of PCM, thus requiring a strong Error Correction Code (ECC) to guarantee correct memory operation. The use of heavyweight ECC comes at cost of significant degradations in storage density, performance and energy efficiency. In this article, we propose a heterogeneous PCM architecture which uses both multi-level cell and single-level cell (SLC) together for a single word line. With highly-reliable SLC cells, the overall array reliability is enhanced. To improve the reliability further, a dynamic self-encoding/decoding scheme is performed before the data is written to the PCM cells. The dynamic scheme automatically determines the locations of MLC and SLC cells and sets the corresponding resistance levels to be programmed. Since the proposed encoding/decoding scheme does not require any additional stages or storages for encoding and decoding, the overhead is negligible. The improved reliability allows to use lighter ECC scheme which in turn helps to improve performance and energy efficiency of the MLC PCM. The experimental results show that the reliability is improved by approximately $10^6$ times compared to the conventional 4LC and more than $10^3$ times compared to the existing encoding methods. The performance improvement is 21.5 percent over the conventional 4LC and is more than 4.1 percent higher than the prior encoding techniques. The proposed method is 30.3 percent more energy efficient than the conventional 4LC and this is similar or higher than other energy efficiency improvement methods.},
  archive      = {J_TC},
  author       = {Taehyun Kwon and Muhammad Imran and Joon-Sung Yang},
  doi          = {10.1109/TC.2020.3009498},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1388-1400},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliability enhanced heterogeneous phase change memory architecture for performance and energy efficiency},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting the capacitance-based gamepad for protecting
mobile game fairness. <em>TC</em>, <em>70</em>(9), 1374–1387. (<a
href="https://doi.org/10.1109/TC.2020.3009374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile game has become a big industry, whose success heavily depends on the game fairness. Recently, a new type of physical cheating instrument, the capacitance-based gamepad (CBG), has been wildly used in popular mobile games. CBG players can obtain an unfairly overwhelming control advantage (e.g., more sensitive clicking and sliding) over benign players. Moreover, as a physical peripheral, CBG is completely transparent to the game application and the underlying system. This makes it inherently immune to existing cheating detection techniques. In this study, by disassembling the CBG device, we find a leverageable physical limitation that the distributions of generated clicking and sliding are more concentrated around a limited area or a boundary respectively. Accordingly, a novel method is proposed to detect the CBG-based cheating. Specifically, to detect the CBG clicking, we employ the entropy to measure the uncertainty of the clicking coordinates; and to detect the CBG sliding, we introduce the convex hull identification algorithm to recognize the potential sliding boundary. We have applied our detection method to four popular mobile games. The evaluation results demonstrate the effectiveness of the proposed method. We believe that the proposed method can be easily adopted by the manufacturers to fight against the CBG-based cheating and protect the game fairness.},
  archive      = {J_TC},
  author       = {Shilei Bai and Bin Liang and Jianjun Huang and Wei You and Jiachun Li and Yaping Li and Wenchang Shi},
  doi          = {10.1109/TC.2020.3009374},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1374-1387},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Detecting the capacitance-based gamepad for protecting mobile game fairness},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimality study of existing quantum computing layout
synthesis tools. <em>TC</em>, <em>70</em>(9), 1363–1373. (<a
href="https://doi.org/10.1109/TC.2020.3009140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layout synthesis, an important step in quantum computing, processes quantum circuits to satisfy device layout constraints. In this paper, we construct QUEKO benchmarks for this problem, which have known optimal depths and gate counts. We use QUEKO to evaluate the optimality of current layout synthesis tools, including Cirq from Google, Qiskit from IBM, t|ket) from Cambridge Quantum Computing, and a recent academic work. To our surprise, despite over a decade of research and development by academia and industry on compilation and synthesis for quantum circuits, we are still able to demonstrate large optimality gaps: 1.5-12x on average on a smaller device and 5-45x on average on a larger device. This suggests substantial room for improvement of the efficiency of quantum computer by better layout synthesis tools. Finally, we also prove the NP-completeness of the layout synthesis problem for quantum computing. We have made the QUEKO benchmarks open-source.},
  archive      = {J_TC},
  author       = {Bochen Tan and Jason Cong},
  doi          = {10.1109/TC.2020.3009140},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1363-1373},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimality study of existing quantum computing layout synthesis tools},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zweilous: A decoupled and flexible memory management
framework. <em>TC</em>, <em>70</em>(9), 1350–1362. (<a
href="https://doi.org/10.1109/TC.2020.3009124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, with the booming growth of cloud computing, workloads from broad ranges of functions and demands are crammed into a single physical machine. They lay considerable stress on the need of evolution of the operating system underneath, especially the memory subsystem. Even enhancing large pages with main memory compression is not intuitively straightforward due to rigid rules imposed by the state-of-the-art manager Buddy System from the beginning of the design. To relieve the aforementioned problems and provide broader design space for system designers, we propose Zweilous, a clean slate physical memory management framework. It is self-contained, highly decoupled, and thus can co-exist with the vanilla memory manager. Separate self-contained metadata/functions guarantee a flexible extension with little modification to current frameworks. To show it is easy to add enhanced functions that accelerate the evolution of the memory management subsystem, we implement Hzmem, a new large page memory manager redesign enhanced with the function of main memory compression. Our method achieves competitive performance compared with native and virtualized large page support, effective memory size increased and fewer impacts on other parts of the operating system.},
  archive      = {J_TC},
  author       = {Guoxi Li and Wenzhi Chen and Yang Xiang},
  doi          = {10.1109/TC.2020.3009124},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1350-1362},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Zweilous: A decoupled and flexible memory management framework},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harnessing CPU electromagnetic emanations for
resonance-induced voltage-noise characterization. <em>TC</em>,
<em>70</em>(9), 1338–1349. (<a
href="https://doi.org/10.1109/TC.2020.3008851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Worst-case dI/dt voltage noise is typically characterized post-silicon using direct voltage measurements through either on-package measurement points or on-chip dedicated circuitry. These approaches consume expensive pad resources or suffer from design-time and run-time overheads. This work proposes an alternative non-intrusive, zero-overhead approach for post-silicon dI/dt voltage noise characterization based on sensing CPU electromagnetic emanations using an antenna and a spectrum analyzer. This approach is based on the observation that high amplitude electromagnetic emanations are correlated with high resonance-induced voltage-noise. This approach enables essential Power-Delivery Network characterization tasks such as: a) obtaining the first-order resonance-frequency of the Power-Delivery LC-tank network, and b) automatically generating voltage noise (dI/dt) stress tests with a genetic-algorithm that is driven by the electromagnetic signal amplitude. The generality of the approach is established by successfully applying it to four different CPUs: two ARM multi-core mobile CPU clusters hosted on a big.LITTLE configuration, one x86-64 AMD desktop CPU and one ARM 64bit 8-core clustered architecture server CPU. The efficacy of the proposed methodology is validated through V MIN and direct voltage-noise measurements. Furthermore, the effectiveness of the EM approach to generate dI/dt viruses that have higher V MIN that conventional workloads is demonstrated with a dynamic-voltage-scaling (DVS) governor that scales the voltage according to the V MIN of the EM generated dI/dt viruses for various core-allocations scenarios. For a 62-hour test with a varying workload mix and core allocations, this governor provides safe below nominal-voltage operation.},
  archive      = {J_TC},
  author       = {Zacharias Hadjilambrou and Shidhartha Das and Marco A. Antoniades and Yiannakis Sazeides},
  doi          = {10.1109/TC.2020.3008851},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1338-1349},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Harnessing CPU electromagnetic emanations for resonance-induced voltage-noise characterization},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Truth discovery with multi-modal data in social sensing.
<em>TC</em>, <em>70</em>(9), 1325–1337. (<a
href="https://doi.org/10.1109/TC.2020.3008561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes unsupervised truth-finding algorithms that combine consideration of multi-modal content features with analysis of propagation patterns to evaluate the veracity of observations in social sensing applications. A key social sensing challenge is to develop effective algorithms for estimating both the reliability of sources and the veracity of their observations without prior knowledge. In contrast to prior solutions that use labeled examples to learn content features that are correlated with veracity, our approach is entirely unsupervised. Hence, given no prior training data, we jointly learn the importance of different content features together with the veracity of observations using propagation patterns as an indicator of perceived content reliability. A novel penalized expectation maximization (PEM) algorithm is proposed to improve the quality of estimation results for observations bolstered by multiple features. In addition, we develop a constrained expectation maximum likelihood with multiple features (CEM-MultiF) that introduces a novel constraint to boost the probability of correctness of some claims. Finally, we evaluate the performance of the proposed algorithms, called EM-Multi, CEM-Multi and PEM-MultiF, respectively, on real-world data sets collected from Twitter. The evaluation results demonstrate that the proposed algorithms outperform the existing fact-finding approaches, and offer tunable knobs for controlling robustness/performance trade-offs in the presence of malicious sources.},
  archive      = {J_TC},
  author       = {Huajie Shao and Dachun Sun and Shuochao Yao and Lu Su and Zhibo Wang and Dongxin Liu and Shengzhong Liu and Lance Kaplan and Tarek Abdelzaher},
  doi          = {10.1109/TC.2020.3008561},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1325-1337},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Truth discovery with multi-modal data in social sensing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A CASTLE with TOWERs for reliable, secure phase-change
memory. <em>TC</em>, <em>70</em>(9), 1311–1324. (<a
href="https://doi.org/10.1109/TC.2020.3006852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of hardware encryption and new memory technologies such as phase change memory (PCM) are gaining popularity in a variety of server applications such as cloud systems. While PCM provides energy and density advantages over conventional DRAM memory, it faces endurance challenges. Such challenges are exacerbated when employing memory encryption as the stored data is essentially randomized, losing data similarity and reducing or eliminating the effectiveness of energy and endurance oriented encoding techniques. This results in increasing dynamic energy consumption and accelerated wear out. In this article, we propose CASTLE, a technique for in-memory encryption to leverage this encryption process to improve reliability in the presence of endurance faults. We also propose TOWERs for CASTLE that improve reliability as well as energy for encrypted data through a novel application of compression and encoding. CASTLE and TOWERs are compatible with error-correction codes (ECC) and error correction pointers (ECP), the standard for mitigating endurance faults in PCM. When combining CASTLE and TOWERS, we achieve an average lifetime improvement of over 45× compared to SECDED ECC, 7.1× compared to SECRET, and 3.6× compared to the leading partition-and-flip fault-tolerance approach (AEGIS) for the same area overhead.},
  archive      = {J_TC},
  author       = {Stephen Longofono and Donald Kline and Rami Melhem and Alex K. Jones},
  doi          = {10.1109/TC.2020.3006852},
  journal      = {IEEE Transactions on Computers},
  number       = {9},
  pages        = {1311-1324},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A CASTLE with TOWERs for reliable, secure phase-change memory},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ameliorate performance of memristor-based ANNs in edge
computing. <em>TC</em>, <em>70</em>(8), 1299–1310. (<a
href="https://doi.org/10.1109/TC.2021.3081985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy efficiency and delay time in the Internet of Things (IoT) system are becoming increasingly significant, especially for the emerging memristor-based crossbar arrays for smart edge computing. This article aims to find a solution for increasing energy efficiency and reducing the delay time, thereby improving the performance of ANNs in edge computing systems. The Number of Pulses Compression (NPC) method is proposed to optimize pulse distribution, energy consumption, and latency by compressing the number of pulses in every weight update step. The NPC method is implemented and verified in a memristor-based hardware simulator based on the MNIST and CIFAR-10 dataset under different circumstances of variations, failure rates, aging effects, architectures, and algorithms. The experimental results show that the NPC method can not only alleviate the uneven distribution of writing pulses but also save the writing energy of the crossbar array by 7.7--26.9 percent and reduce the writing latency by 30.0--50.0 percent. Additionally, the timing regularity of the system is enhanced by the NPC method.},
  archive      = {J_TC},
  author       = {Zhiheng Liao and Jingyan Fu and Jinhui Wang},
  doi          = {10.1109/TC.2021.3081985},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1299-1310},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ameliorate performance of memristor-based ANNs in edge computing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EIHDP: Edge-intelligent hierarchical dynamic pricing based
on cloud-edge-client collaboration for IoT systems. <em>TC</em>,
<em>70</em>(8), 1285–1298. (<a
href="https://doi.org/10.1109/TC.2021.3060484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, IoT systems can better satisfy the service requirements of users with effectively utilizing edge computing resources. Designing an appropriate pricing scheme is critical for users to obtain the optimal computing resources at a reasonable price and for service providers to maximize profits. This problem is complicated with incomplete information. The state-of-the-art solutions focus on the pricing game between a single service provider and users, which ignoring the competition among multiple edge service providers. To address this challenge, we design an edge-intelligent hierarchical dynamic pricing mechanism based on cloud-edge-client collaboration. We introduce an improved double-layer Stackelberg game model to describe the cloud-edge-client collaboration. Technically, we propose a novel pricing prediction algorithm based on double-label Radius K-nearest Neighbors, thereby reducing the number of invalid games to accelerate the game convergence. The experimental results show that our proposed mechanism effectively improves the quality of service for users and realizes the maximum benefit equilibrium for service providers, compared with the traditional pricing scheme. Our proposed mechanism is highly suitable for the IoT applications (e.g., intelligent agriculture or Internet of Vehicles), where there are multiple competing edge service providers for resource allocation.},
  archive      = {J_TC},
  author       = {Tian Wang and Yucheng Lu and Jianhuang Wang and Hong-Ning Dai and Xi Zheng and Weijia Jia},
  doi          = {10.1109/TC.2021.3060484},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1285-1298},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EIHDP: Edge-intelligent hierarchical dynamic pricing based on cloud-edge-client collaboration for IoT systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MulTa-HDC: A multi-task learning framework for
hyperdimensional computing. <em>TC</em>, <em>70</em>(8), 1269–1284. (<a
href="https://doi.org/10.1109/TC.2021.3073409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired Hyperdimensional computing (HDC) has shown its effectiveness in low-power/energy designs for edge computing in the Internet of Things (IoT). Due to limited resources available on edge devices, multi-task learning (MTL), which accommodates multiple cognitive tasks in one model, is considered a more efficient deployment of HDC. However, as the number of tasks increases, MTL-based HDC (MTL-HDC) suffers from the huge overhead of associative memory (AM) and performance degradation. This hinders MTL-HDC from the practical realization on edge devices. This article aims to establish an MTL framework for HDC to achieve a flexible and efficient trade-off between memory overhead and performance degradation. For the shared-AM approach, we propose Dimension Ranking for Effective AM Sharing (DREAMS) to effectively merge multiple AMs while preserving as much information of each task as possible. For the independent-AM approach, we propose Dimension Ranking for Independent MEmory Retrieval (DRIMER) to extract and concatenate informative components of AMs while mitigating interferences among tasks. By leveraging both mechanisms, we propose a hybrid framework of Multi-Tasking HDC, called MulTa-HDC. To adapt an MTL-HDC system to an edge device given a memory resource budget, MulTa-HDC utilizes three parameters to flexibly adjust the proportion of the shared AM and independent AMs. The proposed MulTa-HDC is widely evaluated across three common benchmarks under two standard task protocols. The simulation results of ISOLET, UCIHAR, and MNIST datasets demonstrate that the proposed MulTa-HDC outperforms other state-of-the-art compressed HD models, including SparseHD and CompHD, by up to 8.23\% in terms of classification accuracy.},
  archive      = {J_TC},
  author       = {Cheng-Yang Chang and Yu-Chuan Chuang and En-Jui Chang and An-Yeu Andy Wu},
  doi          = {10.1109/TC.2021.3073409},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1269-1284},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MulTa-HDC: A multi-task learning framework for hyperdimensional computing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DORY: Automatic end-to-end deployment of real-world DNNs on
low-cost IoT MCUs. <em>TC</em>, <em>70</em>(8), 1253–1268. (<a
href="https://doi.org/10.1109/TC.2021.3066883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme edge of the Internet-of-Things is a critical enabler to support pervasive Deep Learning-enhanced applications. Low-Cost MCU-based end-nodes have limited on-chip memory and often replace caches with scratchpads, to reduce area overheads and increase energy efficiency - requiring explicit DMA-based memory transfers between different levels of the memory hierarchy. Mapping modern DNNs on these systems requires aggressive topology-dependent tiling and double-buffering. In this work, we propose DORY (Deployment Oriented to memoRY) - an automatic tool to deploy DNNs on low cost MCUs with typically less than 1MB of on-chip SRAM memory. DORY abstracts tiling as a Constraint Programming (CP) problem: it maximizes L1 memory utilization under the topological constraints imposed by each DNN layer. Then, it generates ANSI C code to orchestrate off- and on-chip transfers and computation phases. Furthermore, to maximize speed, DORY augments the CP formulation with heuristics promoting performance-effective tile sizes. As a case study for DORY, we target GreenWaves Technologies GAP8, one of the most advanced parallel ultra-low power MCU-class devices on the market. On this device, DORY achieves up to 2.5× better MAC/cycle than the GreenWaves proprietary software solution and 18.1× better than the state-of-the-art result on an STM32-H743 MCU on single layers. Using our tool, GAP-8 can perform end-to-end inference of a 1.0-MobileNet-128 network consuming just 63 pJ/MAC on average @ 4.3 fps - 15.4× better than an STM32-H743. We release all our developments - the DORY framework, the optimized backend kernels, and the related heuristics - as open-source software.},
  archive      = {J_TC},
  author       = {Alessio Burrello and Angelo Garofalo and Nazareno Bruschi and Giuseppe Tagliavini and Davide Rossi and Francesco Conti},
  doi          = {10.1109/TC.2021.3066883},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1253-1268},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DORY: Automatic end-to-end deployment of real-world DNNs on low-cost IoT MCUs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed deep convolutional neural networks for the
internet-of-things. <em>TC</em>, <em>70</em>(8), 1239–1252. (<a
href="https://doi.org/10.1109/TC.2021.3062227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Severe constraints on memory and computation characterizing the Internet-of-Things (IoT) units may prevent the execution of Deep Learning (DL)-based solutions, which typically demand large memory and high processing load. In order to support a real-time execution of the considered DL model at the IoT unit level, DL solutions must be designed having in mind constraints on memory and processing capability exposed by the chosen IoT technology. In this article, we introduce a design methodology aiming at allocating the execution of Convolutional Neural Networks (CNNs) on a distributed IoT application. Such a methodology is formalized as an optimization problem where the latency between the data-gathering phase and the subsequent decision-making one is minimized, within the given constraints on memory and processing load at the units level. The methodology supports multiple sources of data as well as multiple CNNs in execution on the same IoT system allowing the design of CNN-based applications demanding autonomy, low decision-latency, and high Quality-of-Service.},
  archive      = {J_TC},
  author       = {Simone Disabato and Manuel Roveri and Cesare Alippi},
  doi          = {10.1109/TC.2021.3062227},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1239-1252},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed deep convolutional neural networks for the internet-of-things},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AILC: Accelerate on-chip incremental learning with
compute-in-memory technology. <em>TC</em>, <em>70</em>(8), 1225–1238.
(<a href="https://doi.org/10.1109/TC.2021.3053199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI applications become pervasive on edge device, incrementally learning new tasks is demanded for deep neural network (DNN) models. In this article, we proposed AILC, a compute-in-memory (CIM)-based accelerator for on-chip incremental learning using STT-MRAM technology. On the software side, a network-expansion-based low-precision training algorithm is proposed for incremental learning, where the loss function is modified to handle the unbalanced training dataset. On the hardware side, the detailed CIM accelerator design for incremental learning is illustrated. A workload-aware hardware resources assignment protocol is proposed to improve the throughput when the workload of weight gradient calculation is low. The software simulation results on CIFAR-100 dataset show that the proposed algorithm can effectively support incremental learning despite the device conductance variation exists. System-level benchmark shows that AILC could achieve 147×, 3.7×~28.7×, 2.05×~2.9× higher energy efficiency than Nvidia Titan-V GPU, RRAM-based CIM accelerators and edge TPU/GPU, respectively. Compared to the baselines, the throughput of AILC is improved by 2.0×~2.2× on average with the hardware resources assignment protocol, which results in 4.1×~21.4× higher throughput than edge TPU/GPU.},
  archive      = {J_TC},
  author       = {Yandong Luo and Shimeng Yu},
  doi          = {10.1109/TC.2021.3053199},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1225-1238},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AILC: Accelerate on-chip incremental learning with compute-in-memory technology},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and simulation of a hybrid architecture for edge
computing in 5G and beyond. <em>TC</em>, <em>70</em>(8), 1213–1224. (<a
href="https://doi.org/10.1109/TC.2021.3066579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing in 5G and Beyond is a promising solution for ultra-low latency applications (e.g., Autonomous Vehicle, Augmented Reality, and Remote Surgery), which have an extraordinarily low tolerance for delay and require fast data processing for a very high volume of data. The requirements of delay-sensitive applications (e.g., Low latency, proximity, and Location/Context-awareness) cannot be satisfied by Cloud Computing due to the high latency between User Equipment and Cloud. Nevertheless, Edge Computing in 5G and beyond can promise ultra-high-speed data processing thanks to the placement of computation capabilities closer to endpoint devices, where 5G encourages the speed rate that is 10 times faster than 4G LTE-Advanced. This paper deeply investigates Edge Computing in 5G and characterizes it based on the requirements of ultra-low latency applications. As a contribution, we propose a hybrid architecture that takes advantage of novel and sustainable technologies (e.g., D2D communication, Massive MIMO, SDN, and NFV) and has major features such as scalability, reliability, and ultra-low latency support. The proposed architecture is evaluated based on agent-based simulations demonstrating that our proposal can satisfy requirements and has the ability to respond to high volume demands with low latency.},
  archive      = {J_TC},
  author       = {Hamed Rahimi and Yvan Picaud and Kamal Deep Singh and Giyyarpuram Madhusudan and Salvatore Costanzo and Olivier Boissier},
  doi          = {10.1109/TC.2021.3066579},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1213-1224},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Design and simulation of a hybrid architecture for edge computing in 5G and beyond},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E2CNNs: Ensembles of convolutional neural networks to
improve robustness against memory errors in edge-computing devices.
<em>TC</em>, <em>70</em>(8), 1199–1212. (<a
href="https://doi.org/10.1109/TC.2021.3061086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce energy consumption, it is possible to operate embedded systems at sub-nominal conditions (e.g., reduced voltage, limited eDRAM refresh rate) that can introduce bit errors in their memories. These errors can affect the stored values of convolutional neural network (CNN) weights and activations, compromising their accuracy. In this article, we introduce Embedded Ensemble CNNs (E 2 CNNs), our architectural design methodology to conceive ensembles of convolutional neural networks to improve robustness against memory errors compared to a single-instance network. Ensembles of CNNs have been previously proposed to increase accuracy at the cost of replicating similar or different architectures. Unfortunately, state-of-the-art (SoA) ensembles do not suit well embedded systems, in which memory and processing constraints limit the number of deployable models. Our proposed architecture solves that limitation applying SoA compression methods to produce an ensemble with the same memory requirements of the original architecture, but with improved error robustness. Then, as part of our new E 2 CNNs design methodology, we propose a heuristic method to automate the design of the voter-based ensemble architecture that maximizes accuracy for the expected memory error rate while bounding the design effort. To evaluate the robustness of E 2 CNNs for different error types and densities, and their ability to achieve energy savings, we propose three error models that simulate the behavior of SRAM and eDRAM operating at sub-nominal conditions. Our results show that E 2 CNNs achieves energy savings of up to 80 percent for LeNet-5, 90 percent for AlexNet, 60 percent for GoogLeNet, 60 percent for MobileNet and 60 percent for an optimized industrial CNN, while minimizing the impact on accuracy. Furthermore, the memory size can be decreased up to 54 percent by reducing the number of members in the ensemble, with a more limited impact on the original accuracy than obtained through pruning alone.},
  archive      = {J_TC},
  author       = {Flavio Ponzina and Miguel Peón-Quirós and Andreas Burg and David Atienza},
  doi          = {10.1109/TC.2021.3061086},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1199-1212},
  shortjournal = {IEEE Trans. Comput.},
  title        = {E2CNNs: Ensembles of convolutional neural networks to improve robustness against memory errors in edge-computing devices},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimal complexity machines under weight quantization.
<em>TC</em>, <em>70</em>(8), 1189–1198. (<a
href="https://doi.org/10.1109/TC.2021.3064301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementing machine learning models on resource-constrained platforms such as hardware devices requires sparse models that can generalize well. This article analyzes the effect of parameter (or weight) quantization on the performance, number of support vectors, model size in bits, L 2 norm and training time on various Support Vector Machines (SVM) and Minimal Complexity Machhines (MCM)-based kernel methods. We show that, Empirical Feature Space (EFS) and hinge loss-based MCM algorithms result in comparable accuracy, (8-190)x smaller model size in bits and (10k-16k)x smaller L 2 norm at full precision compared with LIBSVM. The Least Squares (LS) variants of MCM based methods results in ≈ 2\% improvement in accuracy, upto 16x reduction in model size and upto 3x reduction in L 2 norm at full precision compared with its state-of-the-art counterpart Sparse Fixed Size variant of LS-SVM (SFS-LS-SVM). We quantize the weights of the compared variants post-training and demonstrate that our methods can retain their accuracies even with 7 bits as opposed to 10 and 14 bits used by LIBSVM and SFS-LS-SVM, respectively. Our experiments illustrate that quantization further improves upon the model sizes used by our methods by upto 300x and 30x compared with the LIBSVM and SFS-LS-SVM. This has significant implications for implementation in Internet of Things (IoT) devices, which benefit from model sparsity and good generalization.},
  archive      = {J_TC},
  author       = {Mayank Sharma and Sumit Soman and Jayadeva},
  doi          = {10.1109/TC.2021.3064301},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1189-1198},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Minimal complexity machines under weight quantization},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time detection of hogweed: UAV platform empowered by
deep learning. <em>TC</em>, <em>70</em>(8), 1175–1188. (<a
href="https://doi.org/10.1109/TC.2021.3059819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hogweed of Sosnowskyi (lat. Heracleum sosnówskyi) is poisonous for humans, dangerous for farming crops, and local ecosystems. This plant is fast-growing and has already spread all over Eurasia: from Germany to the Siberian part of Russia, and its distribution expands year-by-year. In-situ detection of this harmful plant is a tremendous challenge for many countries. Meanwhile, there are no automatic systems for detection and localization of hogweed. In this article, we report on an approach for fast and accurate detection of hogweed. The approach includes the Unmanned Aerial Vehicle (UAV) with an embedded system on board running various Fully Convolutional Neural Networks (FCNN). We propose the optimal architecture of FCNN for the embedded system relying on the trade-off between the detection quality and frame rate. We propose a model that achieves ROC AUC 0.96 in the hogweed segmentation task, which can process 4K frames at 0.46 FPS on NVIDIA Jetson Nano. The developed system can recognize the hogweed on the scale of individual plants and leaves. This system opens up a wide vista for obtaining comprehensive and relevant data about the spreading of harmful plants allowing for the elimination of their expansion.},
  archive      = {J_TC},
  author       = {Alexander Menshchikov and Dmitrii Shadrin and Viktor Prutyanov and Daniil Lopatkin and Sergey Sosnin and Evgeny Tsykunov and Evgeny Iakovlev and Andrey Somov},
  doi          = {10.1109/TC.2021.3059819},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1175-1188},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time detection of hogweed: UAV platform empowered by deep learning},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ZigZag: Enlarging joint architecture-mapping design space
exploration for DNN accelerators. <em>TC</em>, <em>70</em>(8),
1160–1174. (<a href="https://doi.org/10.1109/TC.2021.3059962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building efficient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataflow. However, owing to the large joint design space, finding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efficient solutions are found compared to other SotAs, due to ZigZag&#39;s uneven mapping capabilities.},
  archive      = {J_TC},
  author       = {Linyan Mei and Pouya Houshmand and Vikram Jain and Sebastian Giraldo and Marian Verhelst},
  doi          = {10.1109/TC.2021.3059962},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1160-1174},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ZigZag: Enlarging joint architecture-mapping design space exploration for DNN accelerators},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task mapping and scheduling for OpenVX applications on
heterogeneous multi/many-core architectures. <em>TC</em>,
<em>70</em>(8), 1148–1159. (<a
href="https://doi.org/10.1109/TC.2021.3059528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision applications have stringent performance constraints that must be satisfied when they are run at the edge on programmable low-power embedded devices. OpenVX has emerged as the de-facto reference standard to develop such applications. OpenVX uses a primitive-based programming model that results in a directed-acyclic graph (DAG) representation of the application, which can then be used for automatic system-level optimizations and synthesis to heterogeneous multi- and many-core platforms. Although OpenVX has been standardized, its state-of-the-art algorithm for task mapping and scheduling does not deliver the performance necessary for such applications to be deployed on heterogeneous multi-/many-core platforms. This article focuses on addressing this challenge with three main contributions: First, we implemented a static task scheduling and mapping approach for OpenVX using the heterogeneous earliest finish time (HEFT) heuristic. We show that HEFT allows us to improve the system performance up to 70 percent on one of the most widespread smart systems for applying computer vision and intelligent video analytics in general at the edge (i.e., NVIDIA VisionWorks on NVIDIA Jetson TX2). Second, we show that HEFT, in the context of a vision application for edge computing where some primitives may have multiple implementations (e.g., for CPU and GPU), can lead to load imbalance amongst heterogeneous computing elements (CEs), thus suffering from degraded performance. Third, we present an algorithm called exclusive earliest finish time (XEFT) that introduces the notion of exclusive overlap between single implementation primitives to improve the load balancing. We show that XEFT can further improve the system performance up to 33 percent over HEFT, and 82 percent over the native OpenVX scheduler. We present the results on a large set of benchmarks, including a real-world localization and mapping application (ORB-SLAM) combined with an NVIDIA inference application based on convolutional neural networks (CNNs) for object detection.},
  archive      = {J_TC},
  author       = {Francesco Lumpp and Stefano Aldegheri and Hiren D. Patel and Nicola Bombieri},
  doi          = {10.1109/TC.2021.3059528},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1148-1159},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Task mapping and scheduling for OpenVX applications on heterogeneous Multi/Many-core architectures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: IEEE TC special issue on smart edge
computing and IoT. <em>TC</em>, <em>70</em>(8), 1146–1147. (<a
href="https://doi.org/10.1109/TC.2021.3082675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focus on smart edge computing and the Internet of Things (IoT). The evolution of the (IoT) is changing the nature of edge-computing devices. Availability of novel sensor interfaces, efficient digital low power processors, and high-bandwidth low-power communication protocols have generated a perfect storm within the IoT ecosystem. Next generation IoT end-nodes have to support, in place, an increasing range of functionality: multi-sensory data processing and analysis, complex systems control strategies, and, ultimately, artificial intelligence. These new capabilities will enable disruptive innovation in wearable and implantable biomedical devices, autonomous insect-sized drones, miniaturized devices for environmental sensing and continuous monitoring of buildings, industrial machinery, power grids. As a result, we witness a paradigm shift towards computationally demanding tasks on tiny form-factor devices at extreme energy efficiency.},
  archive      = {J_TC},
  author       = {Luca Benini and Simone Benatti and Taekwang Jang and Abbas Rahimi},
  doi          = {10.1109/TC.2021.3082675},
  journal      = {IEEE Transactions on Computers},
  number       = {8},
  pages        = {1146-1147},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE TC special issue on smart edge computing and IoT},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CID: Co-architecting instruction cache and decompression
system for embedded systems. <em>TC</em>, <em>70</em>(7), 1132–1145. (<a
href="https://doi.org/10.1109/TC.2020.3010062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code compression is widely used to reduce the footprint of code memory in cost-sensitive embedded systems. However, despite the small code size, the decompressor and the address translator required to support the code compression incur energy and area overheads. To reduce such overheads while still supporting code compression, we co-architect the instruction cache and decompression system (CID). In CID, each component is placed at the optimal location and the instruction cache is redesigned to recognize the compression state and retain the original address, through the cache division and address space decompression process. As a result of the cache division, the energy consumption and area overheads of the CID instruction cache are reduced. Since the decompressor overhead depends on the code compression technique, we propose a new code compression technique called entropy-based pattern code compression, which reduces overheads of the decompressor. Our experimental results show that the total energy consumption of the instruction cache and decompression system is reduced by up to 29.7 percent and their area is reduced by up to 15.4 percent compared to the post-cache architecture with almost no performance degradation, while achieving an 18.8 percent improvement in the compression ratio compared to the state-of-the-art code compression technique.},
  archive      = {J_TC},
  author       = {Jinkwon Kim and Seokin Hong and Jeongkyu Hong and Soontae Kim},
  doi          = {10.1109/TC.2020.3010062},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1132-1145},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CID: Co-architecting instruction cache and decompression system for embedded systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leakage-free dissemination of authenticated tree-structured
data with multi-party control. <em>TC</em>, <em>70</em>(7), 1120–1131.
(<a href="https://doi.org/10.1109/TC.2020.3006835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing development of cloud computing, a number of users choose to outsource their data to a remote database service provider and enjoy flexible data sharing with multiple parties. However, the integrity and confidentiality of outsourced data in a remote cloud server are the main threats users are concerned about. The tree structure is one of the most widely used data organization structures. It is crucial to guarantee the integrity and privacy not only for the content but also for the structure if sensitive information is organized in this structure. At present, despite some solutions have been put forward, none of these considers the additional redaction attack on tree-structured data from attackers while sharing data with others. In this article, we provide a construction of secure redactable signature scheme for tree-structured data which features a multi-party control on the redaction of vertexes in a tree while the authenticity and privacy are assured. Then, we prove that our scheme is unforgeable, private, and transparent. Furthermore, extensive theoretical and experimental analyses are conducted to assess the efficiency of our scheme. The results demonstrate our scheme achieves multi-party redaction control without sacrificing sensible resources especially when a tree has a larger number of vertexes and siblings per vertex.},
  archive      = {J_TC},
  author       = {Jianghua Liu and Jingyu Hou and Wenjie Yang and Yang Xiang and Wanlei Zhou and Wei Wu and Xinyi Huang},
  doi          = {10.1109/TC.2020.3006835},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1120-1131},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Leakage-free dissemination of authenticated tree-structured data with multi-party control},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Longevity framework: Leveraging online integrated
aging-aware hierarchical mapping and VF-selection for lifetime
reliability optimization in manycore processors. <em>TC</em>,
<em>70</em>(7), 1106–1119. (<a
href="https://doi.org/10.1109/TC.2020.3006571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid device aging in the nano era threatens system lifetime reliability, posing a major intrinsic threat to system functionality. Traditional techniques to overcome the aging-induced device slowdown, such as guardbanding are static and incur performance, power, and area penalties. In a manycore processor, the system-level design abstraction offers dynamic opportunities through the control of task-to-core mappings and per-core operation frequency towards more balanced core aging profile across the chip, optimizing the system lifetime reliability while meeting the application performance requirements. This article presents Longevity Framework (LF) that leverages online integrated aging-aware hierarchical mapping and voltage frequency (VF)-selection for lifetime reliability optimization in manycore processors. The mapping exploration is hierarchical to achieve scalability. The VF-selection builds on the trade-offs involved between power, performance, and aging as the VF is scaled while leveraging the per-core DVFS capabilities. The methodology takes the chip-wide process variation into account. Extensive experimentation, comparing the proposed approach with two state-of-the-art methods, for 64-core and 256-core systems running applications from PARSEC and SPLASH-2 benchmark suites, show an improvement of up to 3.2 years in the system lifetime reliability and 4× improvement in the average core health.},
  archive      = {J_TC},
  author       = {Vijeta Rathore and Vivek Chaturvedi and Amit K. Singh and Thambipillai Srikanthan and Muhammad Shafique},
  doi          = {10.1109/TC.2020.3006571},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1106-1119},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Longevity framework: Leveraging online integrated aging-aware hierarchical mapping and VF-selection for lifetime reliability optimization in manycore processors},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stateful DRF: Considering the past in a multi-resource
allocation. <em>TC</em>, <em>70</em>(7), 1094–1105. (<a
href="https://doi.org/10.1109/TC.2020.3006007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-resource allocation problem arises in different scenarios. Different mechanisms have been proposed to fairly divide multiple resources, most notably, Dominant Resource Fairness (DRF). Even though DRF satisfies several desirable properties, it considers fairness only in the static setting. We propose Stateful DRF (SDRF), an extension of DRF that looks at past allocations and enforces fairness in the long run while keeping the fundamental properties of DRF. We prove that SDRF is strategyproof, since users cannot manipulate the system by misreporting their demands; incentivizes sharing, because no user is better off if resources are equally partitioned; and is efficient, as no allocation can be improved without decreasing another. In SDRF, users&#39; priorities change over time. To avoid recalculating priorities at every task scheduling decision, we also propose Live Tree, a data structure that keeps elements with predictable time-varying priorities ordered. We implement SDRF on Mesos and run it in a real cluster. Moreover, we conduct large-scale simulations based on Google cluster traces of 30 million tasks over one month. Results show that SDRF reduces users&#39; waiting time on average. This improves fairness, by increasing the number of completed tasks for users with lower demands, with negligible impact on high-demand users.},
  archive      = {J_TC},
  author       = {Hugo Sadok and Miguel Elias M. Campista and Luís Henrique M. K. Costa},
  doi          = {10.1109/TC.2020.3006007},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1094-1105},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stateful DRF: Considering the past in a multi-resource allocation},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). XMeter: Finding approximable functions and predicting their
accuracy. <em>TC</em>, <em>70</em>(7), 1081–1093. (<a
href="https://doi.org/10.1109/TC.2020.3005083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing has significant potential to improve the efficiency of a computing system. Numerous techniques have been proposed in literature. Virtually, all of them require programmers to either experiment with every instance of a specific type of code region exhaustively to find approximable code regions or annotate such regions manually. Both approaches are error-prone and can lead to missed opportunities. Therefore, we propose XMeter to automatically find and quantify approximable code regions. XMeter, first, analyzes the application code statically using a novel algorithm based on memory location updates. Also, XMeter provides a deep learning-based predictor to predict the accuracy of the application when different code regions are approximated. Our proposed scheme does not require the programmer to experiment exhaustively for all possible error rates and types of approximation techniques. Moreover, the scheme does not require any domain knowledge and is not specific to any approximation technique. Therefore, it is general enough to be applicable for any approximation technique. We developed XMeter using LLVM and experimented with 10 applications. We analyzed 43 approximable functions and found 21 to be highly tolerant of errors. We validated our results using 4 well-known approximation techniques and showed that XMeter can predict an application&#39;s accuracy accurately.},
  archive      = {J_TC},
  author       = {Riad Akram and Shantanu Mandal and Abdullah Muzahid},
  doi          = {10.1109/TC.2020.3005083},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1081-1093},
  shortjournal = {IEEE Trans. Comput.},
  title        = {XMeter: Finding approximable functions and predicting their accuracy},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Digit stability inference for iterative methods using
redundant number representation. <em>TC</em>, <em>70</em>(7), 1074–1080.
(<a href="https://doi.org/10.1109/TC.2020.3003529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our recent work on iterative computation in hardware, we showed that arbitrary-precision solvers can perform more favorably than their traditional arithmetic equivalents when the latter&#39;s precisions are either under- or over-budgeted for the solution of the problem at hand. Significant proportions of these performance improvements stem from the ability to infer the existence of identical most-significant digits between iterations. This technique uses properties of algorithms operating on redundantly represented numbers to allow the generation of those digits to be skipped, increasing efficiency. It is unable, however, to guarantee that digits will stabilize, i.e., never change in any future iteration. In this article, we address this shortcoming, using interval and forward error analyses to prove that digits of high significance will become stable when computing the approximants of systems of linear equations using stationary iterative methods. We formalize the relationship between matrix conditioning and the rate of growth in most-significant digit stability, using this information to converge to our desired results more quickly. Versus our previous work, an exemplary hardware realization of this new technique achieves an up-to 2.2× speedup in the solution of a set of variously conditioned systems using the Jacobi method.},
  archive      = {J_TC},
  author       = {He Li and Ian McInerney and James J. Davis and George A. Constantinides},
  doi          = {10.1109/TC.2020.3003529},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1074-1080},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Digit stability inference for iterative methods using redundant number representation},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control performance optimization for application integration
on automotive architectures. <em>TC</em>, <em>70</em>(7), 1059–1073. (<a
href="https://doi.org/10.1109/TC.2020.3003083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automotive software implements different functionalities as multiple control applications sharing common platform resources. Although such applications are often developed independently, the control performance of the resulting system depends on how these applications are integrated. A key integration challenge is to efficiently schedule these applications on shared resources with minimal control performance degradation. We formulate this problem as that of scheduling multiple distributed periodic control tasks that communicate via messages with non-zero jitter. The optimization criterion used is a piecewise linear representation of the control performance degradation as a function of the end-to-end latency of the application. The three main contributions of this article are: 1) a constraint programming (CP) formulation to solve this integration problem optimally on time-triggered architectures; 2) an efficient heuristic called Flexi; and 3) an experimental evaluation of the scalability and efficiency of the proposed approaches. In contrast to the CP formulation, which for many real-life problems might have unacceptably long running times, Flexi returns nearly optimal results (0.5 percent loss in control performance compared to optimal) for most problems with more acceptable running times.},
  archive      = {J_TC},
  author       = {Anna Minaeva and Debayan Roy and Benny Akesson and Zdeněk Hanzálek and Samarjit Chakraborty},
  doi          = {10.1109/TC.2020.3003083},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1059-1073},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Control performance optimization for application integration on automotive architectures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emulating round-to-nearest ties-to-zero “augmented”
floating-point operations using round-to-nearest ties-to-even
arithmetic. <em>TC</em>, <em>70</em>(7), 1046–1058. (<a
href="https://doi.org/10.1109/TC.2020.3002702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2019 version of the IEEE 754 Standard for Floating-Point Arithmetic recommends that new “augmented” operations should be provided for the binary formats. These operations use a new “rounding direction”: round-to-nearest ties-to-zero . We show how they can be implemented using the currently available operations, using round-to-nearest ties-to-even with a partial formal proof of correctness.},
  archive      = {J_TC},
  author       = {Sylvie Boldo and Christoph Lauter and Jean-Michel Muller},
  doi          = {10.1109/TC.2020.3002702},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1046-1058},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Emulating round-to-nearest ties-to-zero “Augmented” floating-point operations using round-to-nearest ties-to-even arithmetic},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contour: A process variation aware wear-leveling mechanism
for inodes of persistent memory file systems. <em>TC</em>,
<em>70</em>(7), 1034–1045. (<a
href="https://doi.org/10.1109/TC.2020.3002537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing persistent memory file systems exploit the fast, byte-addressable persistent memory (PM) to boost storage performance but ignore the limited endurance of PM. Particularly, the PM storing the inode section is extremely vulnerable for the inodes are most frequently updated, fixed on a location throughout lifetime, and require immediate persistency. The huge endurance variation of persistent memory domains caused by process variation makes things even worse. In this article, we propose a process variation aware wear leveling mechanism called Contour for the inode section of persistent memory file system. Contour first enables the movement of inodes by virtualizing the inodes with a deflection table. Then, Contour adopts cross-domain migration algorithm and intra-domain migration algorithm to balance the writes across and within the memory domains. We implement the proposed Contour mechanism in Linux kernel 4.4.30 based on a real persistent memory file system, SIMFS. We use standard benchmarks, including Filebench, MySQL, and FIO, to evaluate Contour. Extensive experimental results show Contour can improve the wear ratios of pages 417.8× and 4.5× over the original SIMFS and PCV, the state-of-the-art inode wear-leveling algorithm, respectively. Meanwhile, the average performance overhead and wear overhead of Contour are 0.87 and 0.034 percent in application-level workloads, respectively.},
  archive      = {J_TC},
  author       = {Xianzhang Chen and Edwin H.-M. Sha and Xinxin Wang and Chaoshu Yang and Weiwen Jiang and Qingfeng Zhuge},
  doi          = {10.1109/TC.2020.3002537},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1034-1045},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Contour: A process variation aware wear-leveling mechanism for inodes of persistent memory file systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-performance constant-time discrete gaussian sampling.
<em>TC</em>, <em>70</em>(7), 1019–1033. (<a
href="https://doi.org/10.1109/TC.2020.3001170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete Gaussian distribution plays an essential role in lattice cryptography whereas naive implementations suffer from timing attacks. Unfortunately, conversion to secure constant-time variant incurs severe deterioration in performance. In Knuth-Yao sampling, we demonstrate several properties of the discrete distribution generation tree involving structural features and finite node height. Accordingly we propose a generic method independent of standard deviations, which focuses on minimizing the Boolean expressions for the mapping from input bit strings to output sample values, along with an in-depth efficiency analysis. Two optimization techniques are devised to further propel the minimization by replacing and adjusting nodes. To strike the balance of computational overhead and closeness to optimum, heuristic strategies are introduced. Finally, performance evaluation is conducted both in software and hardware. Running on a 3.4GHz Intel Core i7-6700 processor, our method improves sampling rate by up to 29.5 percent compared to the latest technique. Targeting hardware FPGA devices, our approach can be 2.7 times faster and achieves 57.3 percent resource reduction than the original constant-time Knuth-Yao sampling. Compared to the Cumulative Distribution Table algorithm with fixed step binary search, our sampler can be at least 12.6 times faster and gains 79/61 percent better area-time product than its counterpart without/with BRAM.},
  archive      = {J_TC},
  author       = {Liang Kong and Shuguo Li and Ruirui Liu},
  doi          = {10.1109/TC.2020.3001170},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1019-1033},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-performance constant-time discrete gaussian sampling},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Priority assignment on partitioned multiprocessor systems
with shared resources. <em>TC</em>, <em>70</em>(7), 1006–1018. (<a
href="https://doi.org/10.1109/TC.2020.3000051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by industry demand, there is an increasing need to develop real-time multiprocessor systems which contain shared resources. The Multiprocessor Stack Resource Policy (MSRP) and Multiprocessor resource sharing Protocol (MrsP) are two major protocols that manage access to shared resources. Both of them can be applied to Fixed-Priority Preemptive Scheduling (FPPS), which is enforced by most commercial real-time systems regulations, and which requires task priorities to be assigned before deployment. Along with MSRP and MrsP, there exist two forms of schedulability tests that bound the worst-case blocking time due to resource accesses: the traditional ones being more widely adopted and the more recently developed holistic ones which deliver tighter analysis. On uniprocessor systems, there are several well-established optimal priority assignment algorithms. Unfortunately, on multiprocessor systems with shared resources, the issue of priority assignment has not been adequately understood. In this article, we investigate three mainstream priority assignment algorithms-Deadline Monotonic Priority Ordering (DMPO), Audsley&#39;s Optimal Priority Assignment (OPA), and Robust Priority Assignment (RPA), in the context of partitioned multiprocessor systems with shared resources. Our contributions are multifold: First, we prove that DMPO is optimal with the traditional schedulability tests. Second, two counter examples are given as evidence that DMPO is not optimal with the tighter holistic schedulability tests. Third, we then analyze the pessimism arising from the adoption of OPA and RPA with the holistic tests. Lastly, we propose a Slack-based Priority Ordering (SPO) algorithm that minimises such pessimism, and has polynomial time complexity. Comprehensive experiments show that SPO outperforms (i.e., results in a larger number of schedulable systems) DMPO, OPA, and RPA in general with the holistic schedulability tests, by up to 15 percent. With the theoretical contributions, this paper is a useful guide to priority assignment in real-time partitioned multiprocessor systems with shared resources.},
  archive      = {J_TC},
  author       = {Shuai Zhao and Wanli Chang and Ran Wei and Weichen Liu and Nan Guan and Alan Burns and Andy Wellings},
  doi          = {10.1109/TC.2020.3000051},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {1006-1018},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Priority assignment on partitioned multiprocessor systems with shared resources},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating parallel applications in cloud platforms via
adaptive time-slice control. <em>TC</em>, <em>70</em>(7), 992–1005. (<a
href="https://doi.org/10.1109/TC.2020.2999619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud platforms can provide flexible and cost-effective environments for parallel applications. However, the resource over-commitment issues, i.e., cloud providers often provide much more executable virtual CPUs than available physical CPUs, still impede the synchronization operations of parallel applications, causing severe performance degradation. Existing methods optimize parallel applications by promoting the priorities of involved VMs. They cannot fully explore the performance of parallel applications, because they ignore the time-slice requirements of different phases of parallel applications. Furthermore, non-parallel applications experience unsatisfied performance because of low scheduling priorities. Given empirical analysis on time-slices of virtual machines (VMs), we find that shortening time-slices can mitigate synchronization overhead which incurs during communication phases, while over-short time-slices cause frequent cache misses in computation phases. Accordingly, we propose an Adaptive Time-slice Control (ATC) mechanism. ATC first detects the phases of parallel applications based on lock latency or cache misses. Then, ATC shortens time-slices during communication phases and prolongs time-slices during computation phases for parallel applications, and sets a uniform time-slice for non-parallel applications. We evaluate ATC using seven well-known benchmarks with 25+ applications. Experiments show that ATC obtains 1.5-75× performance gain for running parallel applications than state-of-the-art solutions, with nearly unaffected impact on non-parallel applications.},
  archive      = {J_TC},
  author       = {Hao Fan and Song Wu and Xinyu Zhao and Zhenjiang Xie and Sheng Di and Jiang Xiao and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2020.2999619},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {992-1005},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating parallel applications in cloud platforms via adaptive time-slice control},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable concolic testing of RTL models. <em>TC</em>,
<em>70</em>(7), 979–991. (<a
href="https://doi.org/10.1109/TC.2020.2997644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation is widely used for validation of Register-Transfer-Level (RTL) models. While simulating with millions of random or constrained-random tests can cover majority of the functional scenarios, the number of remaining scenarios can still be huge (hundreds or thousands) in case of today&#39;s industrial designs. Hard-to-activate branches are one of the major contributors for such remaining/untested scenarios. While directed test generation techniques using formal methods are promising in activating branches, it is infeasible to apply them on large designs due to state space explosion. In this article, we propose a fully automated and scalable approach to cover the hard-to-activate branches using concolic testing of RTL models. While application of concolic testing on hardware designs has shown some promising results in improving the overall coverage, they are not designed to activate specific targets such as uncovered corner cases and rare scenarios. In other words, existing concolic testing approaches address state space explosion problem but leads to path explosion problem while searching for the uncovered targets. Our proposed approach maps directed test generation problem to target search problem while avoiding overlapping searches involving multiple targets. This article makes two important contributions. (1) We propose a directed test generation technique to activate a target by effective utilization of concolic testing on RTL models. (2) We develop efficient learning and clustering techniques to minimize the overlapping searches across targets to drastically reduce the overall test generation effort. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods in terms of test generation time (up to 205X, 69X on average) as well as memory requirements (up to 31X, 7X on average).},
  archive      = {J_TC},
  author       = {Yangdi Lyu and Prabhat Mishra},
  doi          = {10.1109/TC.2020.2997644},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {979-991},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalable concolic testing of RTL models},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting security dependence for conditional speculation
against spectre attacks. <em>TC</em>, <em>70</em>(7), 963–978. (<a
href="https://doi.org/10.1109/TC.2020.2997555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speculative execution side-channel vulnerabilities such as Spectre reveal that conventional architecture designs lack security consideration. This article proposes a software transparent defense framework, named as Conditional Speculation, against Spectre vulnerabilities found on traditional out-of-order microprocessors. It introduces the concept of security dependence to mark speculative memory instructions which could leak information with potential security risks. More specifically, security-dependent instructions are detected and marked with suspect speculation flags in the Issue Queue. All the instructions can be speculatively issued for execution in accordance with the classic out-of-order pipeline. For those instructions with suspect speculation flags, they are considered as safe instructions if their speculative execution dose not refill new cache lines with unauthorized privilege data. Otherwise, they are considered as unsafe instructions and thus not allowed to execute speculatively. To pursue a balance of performance and security, we investigate two filtering mechanisms, Cache-hit-based Hazard Filter and Trusted Page Buffer-based Hazard Filter to filter out false security hazards. As for true security hazards, we have two approaches to prevent them from changing cache states. One is to block all unsafe access, the other is to fetch them from lower-level caches or memory to a speculative buffer temporarily, and refill them after confirming that they are on the correct execution path. Our design philosophy is to speculatively execute safe instructions to maintain the performance benefits of out-of-order execution while delaying the cache updates for speculative execution of unsafe instructions for security consideration. We evaluate Conditional Speculation in terms of performance, security, and area. The experimental results show that the hardware overhead is marginal and the performance overhead is minimal.},
  archive      = {J_TC},
  author       = {Lutan Zhao and Peinan Li and Rui Hou and Michael C. Huang and Peng Liu and Lixin Zhang and Dan Meng},
  doi          = {10.1109/TC.2020.2997555},
  journal      = {IEEE Transactions on Computers},
  number       = {7},
  pages        = {963-978},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting security dependence for conditional speculation against spectre attacks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Plasticity-on-chip design: Exploiting self-similarity for
data communications. <em>TC</em>, <em>70</em>(6), 950–962. (<a
href="https://doi.org/10.1109/TC.2021.3071507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for distributed big data analytics and data-intensive programs which contribute to large volumes of packets among processing elements (PEs) and memory banks, we witness a pressing need for new mathematical models and algorithms that can engineer a brain-inspired plasticity into the computing platforms by mining the topological complexity of high-level programs (HLPs) and exploiting their self-similar and fractal characteristics for designing reconfigurable domain-specific computing architectures. In this article, we present Plasticity-on-Chip (PoC) by engineering plasticity into ”artificial brains” to mine and exploit the self-similarity of HLPs. First, we present a communication modeling of HLPs (e.g., C/C++ implementations of various applications) that relies on static and dynamic compiler analysis of programs with varying input seeds, performing comprehensive program analysis of all traces, and representing the HLPs as weighted directed acyclic graphs while capturing the intrinsic timing constraints and data/control flow requirements. Second, we propose a rigorous mathematical framework for determining the optimal parallel degree of executing a set of interacting HLPs (by partitioning them into clusters of densely interconnected supernodes - tasks) which helps us decide the number of available heterogeneous PEs, the amount of required memory and the structure of the synthesized deadlock-free irregular NoC topology that offers an efficient communication medium. These clusters serve as abstract models of computation for the synthesized PEs within the parallel execution model. Finally, exploiting the fractal and complex networks concepts, we extract in-depth features from graphs that serve as inputs for distributed reinforcement learning. Our experimental results on synthesized PEs and NoCs show performance improvements as high as 7.61x when compared to the traditional NoC and 2.6x compared to gem5-Aladdin.},
  archive      = {J_TC},
  author       = {Yao Xiao and Shahin Nazarian and Paul Bogdan},
  doi          = {10.1109/TC.2021.3071507},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {950-962},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Plasticity-on-chip design: Exploiting self-similarity for data communications},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing vertex pressure dynamic graph partitioning in
many-core systems. <em>TC</em>, <em>70</em>(6), 936–949. (<a
href="https://doi.org/10.1109/TC.2021.3059386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of graph-based algorithms in many applications, dynamic graphs have become critical for applications that work with real-time or time-series relationship data. Due to their large storage footprint, these applications require high parallelism and locality. Many-core architectures offer high parallelism for both static and dynamic graphs. However, systems operating on dynamic graphs must continuously repartition graph data across storage units to achieve good locality, load balancing, and performance. In this work, we examine the effectiveness and efficiency of different vertex-pressure repartitioning schemes, which move vertices so to co-locate them near their most relevant neighbors. We describe key repartitioning design choices and provide a thorough evaluation of the impact of a range of design features with different datasets. Our evaluation indicates that optimized dynamic repartitioning techniques can often provide over 2x performance speedup over state-of-the-art static solutions.},
  archive      = {J_TC},
  author       = {Andrew McCrabb and Valeria Bertacco},
  doi          = {10.1109/TC.2021.3059386},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {936-949},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing vertex pressure dynamic graph partitioning in many-core systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient pipelined execution of CNNs based on in-memory
computing and graph homomorphism verification. <em>TC</em>,
<em>70</em>(6), 922–935. (<a
href="https://doi.org/10.1109/TC.2021.3073255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing is an emerging computing paradigm enabling deep-learning inference at significantly higher energy-efficiency and reduced latency. The essential idea is mapping the synaptic weights of each layer to one or more in-memory computing (IMC) cores. During inference, these cores perform the associated matrix-vector multiplications in place with O(1) time complexity, obviating the need to move the synaptic weights to additional processing units. Moreover, this architecture enables the execution of these networks in a highly pipelined fashion. However, a key challenge is designing an efficient communication fabric for the IMC cores. In this work, we present one such communication fabric based on a graph topology that is well-suited for the widely successful convolutional neural networks (CNNs). We show that this communication fabric facilitates the pipelined execution of all state-of-the-art CNNs by proving the existence of a homomorphism between the graph representations of these networks and that corresponding to the proposed communication fabric. We then present a quantitative comparison with established communication topologies and show that our proposed topology achieves the lowest bandwidth requirements per communication channel. Finally, we present one hardware implementation and show a concrete example of mapping ResNet-32 onto an IMC core array interconnected via the proposed communication fabric.},
  archive      = {J_TC},
  author       = {Martino Dazzi and Abu Sebastian and Thomas Parnell and Pier Andrea Francese and Luca Benini and Evangelos Eleftheriou},
  doi          = {10.1109/TC.2021.3073255},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {922-935},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient pipelined execution of CNNs based on in-memory computing and graph homomorphism verification},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing en-route for near-data processing. <em>TC</em>,
<em>70</em>(6), 906–921. (<a
href="https://doi.org/10.1109/TC.2021.3063378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data explosion and faster data analysis demand have spawned emerging applications that operate over myriads of data and exhibit large memory footprints with low data reuse rate. Such characteristics lead to enormous data movements across the memory hierarchy and pose significant pressure on modern communication fabrics and memory subsystems. To mitigate the worsening gap between high processor computation density and deficient memory bandwidth, memory networks, and near-data processing techniques are proposed to keep improving system performance and energy efficiency. In this article, we propose Active-Routing, an in-network near-data processing architecture for data-flow execution, which enables computation en-route by exploiting patterns of aggregation over intermediate results. The proposed architecture leverages the massive memory cube- and vault-level parallelism as well as network concurrency to optimize the aggregation operations along a dynamically built Active-Routing Tree. It also introduces page granular computation offloading to amortize the offloading overhead and improve the throughput. Compared to the state-of-the-art processing-in-memory architecture, the evaluations show that the baseline Active-Routing can achieve up to 7× speedup with an average of 60 percent performance improvement, and reduce the energy-delay product by 80 percent across various benchmarks. Further optimizations with vault-level parallelism and page granular offloading can achieve an extra order of magnitude improvement.},
  archive      = {J_TC},
  author       = {Jiayi Huang and Pritam Majumder and Sungkeun Kim and Troy Fulton and Ramprakash Reddy Puli and Ki Hwan Yum and Eun Jung Kim},
  doi          = {10.1109/TC.2021.3063378},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {906-921},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Computing en-route for near-data processing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Opportunistic caching in NoC: Exploring ways to reduce miss
penalty. <em>TC</em>, <em>70</em>(6), 892–905. (<a
href="https://doi.org/10.1109/TC.2021.3069968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to limited on-chip caching, data-driven applications with large memory footprint encounter frequent cache misses. Such applications suffer from recurring miss penalty when they re-reference recently evicted cache blocks. To meet the worst-case performance requirements, Network-on-Chip (NoC) routers are provisioned with input port buffers. However, recent studies reveal that these buffers remain underutilised except during network congestion. Trace buffers are Design-for-Debug (DfD) hardware employed in NoC routers for post-silicon debug and validation. Nevertheless, they become non-functional once a design goes into production and remain in the routers left unused. In this article, we exploit the underutilised NoC router buffers and the unused trace buffers to store recently evicted cache blocks. While these blocks are stored in the buffers, future re-reference to these blocks can be replied from the NoC router. Such an opportunistic caching of evicted blocks in NoC routers significantly reduce the miss penalty. Experimental analysis shows that the proposed architectures can achieve up to 21 percent (16 percent on average) reduction in miss penalty and 19 percent (14 percent on average) improvement in overall system performance. While we have a negligible area and leakage power overhead of 2.58 and 3.94 percent, respectively, dynamic power reduces by 6.12 percent due to the improvement in performance.},
  archive      = {J_TC},
  author       = {Abhijit Das and Abhishek Kumar and John Jose and Maurizio Palesi},
  doi          = {10.1109/TC.2021.3069968},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {892-905},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Opportunistic caching in NoC: Exploring ways to reduce miss penalty},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PIT: Processing-in-transmission with fine-grained data
manipulation networks. <em>TC</em>, <em>70</em>(6), 877–891. (<a
href="https://doi.org/10.1109/TC.2020.3048233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of data parallel computation, most works focus on data flow optimization inside the PE array and favorable memory hierarchy to pursue the maximum parallelism and efficiency, while the importance of data contents has been overlooked for a long time. As we observe, for structured data, insights on the contents (i.e., their values and locations within a structured form) can greatly benefit the computation performance, as fine-grained data manipulation can be performed. In this paper, we claim that by providing a flexible and adaptive data path, an efficient architecture with capability of fine-grained data manipulation can be built. Specifically, we design SOM, a portable and highly-adaptive data transmission network, with the capability of operand sorting, non-blocking self-route ordering and multicasting. Based on SOM, we propose the processing-in-transmission architecture (PITA), which extends the traditional SIMD architecture to perform some fundamental data processing during its transmission, by embedding multiple levels of SOM networks on the data path. We evaluate the performance of PITA in two irregular computation problems. We first map the matrix inversion task onto PITA and show considerable performance gain can be achieved, resulting in 3x-20x speedup against Intel MKL, and 20x-40x against cuBLAS. Then we evaluate our PITA on sparse CNNs. The results indicate that PITA can greatly improve computation efficiency and reduce memory bandwidth pressure. We achieved 2x-9x speedup against several state-of-art accelerators on sparse CNN, where nearly 100 percent PE efficiency is maintained under high sparsity. We believe the concept of PIT is a promising computing paradigm that can enlarge the capability of traditional parallel architecture.},
  archive      = {J_TC},
  author       = {Pengchen Zong and Tian Xia and Haoran Zhao and Jianming Tong and Zehua Li and Wenzhe Zhao and Nanning Zheng and Pengju Ren},
  doi          = {10.1109/TC.2020.3048233},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {877-891},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PIT: Processing-in-transmission with fine-grained data manipulation networks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new optoelectronic hybrid network based on scheduling
optimization of optical links. <em>TC</em>, <em>70</em>(6), 863–876. (<a
href="https://doi.org/10.1109/TC.2021.3054308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of exascale computers will represent a milestone in high-performance computing (HPC). Optoelectronic interconnections and configurable switches will change the traditional supercomputer architecture. However, new hardware is not easily adapted to dynamic running conditions. Based on scheduling optimization of optical links, we propose a new optoelectronic hybrid network, the software-defined network accelerator (sDNA), for an exascale computer. Our scheduling optimization contains an optical interconnection method and an adaptive routing method. The main contribution of our work is an extended edge forwarding index (E-EFI) optical interconnection method based on slow-switching optical devices. The optical link connections are established by evaluating the traffic offloading revenue for each optical link candidate. To support optical interconnection, sDNA selects a suitable routing strategy according to the job-schedule information and prior HPC application knowledge. We tested sDNA in a network simulator and a prototype exascale computer system using both the US Department of Energy (DOE) application and real-world communication benchmarks. The verification results for traffic offloading reveal that our optical interconnection method not only offloads traffic from electrical links to optical links but also avoids the congestion inherent to electrical links. sDNA maintains a throughput of more than 80 percent bandwidth and reduces the communication delay by 10 percent in our real prototype system and simulator. Thus, sDNA is an ideal candidate for accelerating the communication performance of exascale computers.},
  archive      = {J_TC},
  author       = {En Shao and Guangming Tan and Zhan Wang and Guojun Yuan and Zheng Cao and Ninghui Sun},
  doi          = {10.1109/TC.2021.3054308},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {863-876},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A new optoelectronic hybrid network based on scheduling optimization of optical links},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OPTWEB: A lightweight fully connected inter-FPGA network for
efficient collectives. <em>TC</em>, <em>70</em>(6), 849–862. (<a
href="https://doi.org/10.1109/TC.2021.3068715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern FPGA accelerators can be equipped with many high-bandwidth network I/Os, e.g., 64 x 50 Gbps, enabled by onboard optics or co-packaged optics. Some dozens of tightly coupled FPGA accelerators form an emerging computing platform for distributed data processing. However, a conventional indirect packet network using Ethernet&#39;s Intellectual Properties imposes an unacceptably large amount of the logic for handling such high-bandwidth interconnects on an FPGA. Besides the indirect network, another approach builds a direct packet network. Existing direct inter-FPGA networks have a low-radix network topology, e.g., 2-D torus. However, the low-radix network has the disadvantage of a large diameter and large average shortest path length that increases the latency of collectives. To mitigate both problems, we propose a lightweight, fully connected inter-FPGA network called OPTWEB for efficient collectives. Since all end-to-end separate communication paths are statically established using onboard optics, raw block data can be transferred with simple link-level synchronization. Once each source FPGA assigns a communication stream to a path by its internal switch logic between memory-mapped and stream interfaces for remote direct memory access (RDMA), a one-hop transfer is provided. Since each FPGA performs input/output of the remote memory access between all FPGAs simultaneously, multiple RDMAs efficiently form collectives. The OPTWEB network provides 0.71-μsec start-up latency of collectives among multiple Intel Stratix 10 MX FPGA cards with onboard optics. The OPTWEB network consumes 31.4 and 57.7 percent of adaptive logic modules for aggregate 400-Gbps and 800-Gbps interconnects on a custom Stratix 10 MX 2100 FPGA, respectively. The OPTWEB network reduces by 40 percent the cost compared to a conventional packet network.},
  archive      = {J_TC},
  author       = {Kenji Mizutani and Hiroshi Yamaguchi and Yutaka Urino and Michihiro Koibuchi},
  doi          = {10.1109/TC.2021.3068715},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {849-862},
  shortjournal = {IEEE Trans. Comput.},
  title        = {OPTWEB: A lightweight fully connected inter-FPGA network for efficient collectives},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HAM: Hotspot-aware manager for improving communications with
3D-stacked memory. <em>TC</em>, <em>70</em>(6), 833–848. (<a
href="https://doi.org/10.1109/TC.2021.3066982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging High-Performance Computing (HPC) workloads, such as graph analytics, machine learning, and big data science, are data-intensive. Data-intensive workloads usually present fine-grained memory accesses with limited or no data locality, and thus incur frequent cache misses and low utilization of memory bandwidth. 3D-stacked memory devices such as Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM) can provide significantly higher bandwidth than conventional memory modules. However, the traditional interfaces and optimization methods for JEDEC DDR devices do not allow to fully exploit the potential performance of 3D-stacked memory with the massive amount of irregular memory accesses of data-intensive applications. In this article, we propose a novel Hotspot-Aware Manager (HAM) infrastructure for 3D-stacked memory devices capable of optimizing memory access streams via request aggregation, hotspot detection, and in-memory prefetching. We present the HAM design and implementation, and simulate it on a system using RISC-V embedded cores with attached HMC devices. We extensively evaluate HAM with over 12 benchmarks and applications representing diverse irregular memory access patterns. The results show that, on average, HAM reduces redundant requests by 37.51 percent and increases the prefetch buffer hit rate by 4.2 times, compared to a baseline streaming prefetcher. On the selected benchmark set, HAM provides performance gains of 21.81 percent in average (up to 34.28 percent), and power savings of 35.07 percent over a standard 3D-stacked memory.},
  archive      = {J_TC},
  author       = {Xi Wang and Antonino Tumeo and John D. Leidel and Jie Li and Yong Chen},
  doi          = {10.1109/TC.2021.3066982},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {833-848},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HAM: Hotspot-aware manager for improving communications with 3D-stacked memory},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). S-SMART++: A low-latency NoC leveraging speculative bypass
requests. <em>TC</em>, <em>70</em>(6), 819–832. (<a
href="https://doi.org/10.1109/TC.2021.3068615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many-core processors demand scalable, efficient and low latency NoCs. Bypass routers are an affordable solution to attain low latency in relatively simple topologies like the mesh. SMART improves on traditional bypass routers implementing multi-hop bypass which reduces the importance of the distance between pairs of nodes. Nevertheless, the conservative buffer reallocation policy of SMART requires a large number of Virtual Channels (VCs) to offer high performance, penalizing its implementation cost. Besides, SMART zero-load latency values highly depend on HPC Max HPCMax, the maximum number of hops that can be jumped per cycle. In this article, we present Speculative-SMART++ (S-SMART++), with two mechanisms that significantly improve multi-hop bypass. First, zero-load latency is reduced by speculatively setting consecutive multi-hops. Second, the inefficient buffer reallocation policy of SMART is reduced by combining multi-packet buffers, Non-Empty Buffer Bypass and per-packet allocation. These proposals are evaluated using functional simulation, with synthetic and real loads, and synthesis tools. S-SMART++ does not need VCs to obtain the performance of SMART with 8 VCs, reducing notably logic resources and dynamic power. Additionally, S-SMART++ reduces the base-latency of SMART by at least 29.2 percent, even when using the biggest HPC Max HPCMax possible.},
  archive      = {J_TC},
  author       = {Iván Pérez and Enrique Vallejo and Ramón Beivide},
  doi          = {10.1109/TC.2021.3068615},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {819-832},
  shortjournal = {IEEE Trans. Comput.},
  title        = {S-SMART++: A low-latency NoC leveraging speculative bypass requests},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guest editorial: IEEE TC special issue on communications for
many-core processors and accelerators. <em>TC</em>, <em>70</em>(6),
817–818. (<a href="https://doi.org/10.1109/TC.2021.3068060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The papers in this special section focuses on communications for mmany core processors and accelerators. Communications in various forms have become increasingly important with the advent of diverse computing platforms, such as many-core CPUs, GPUs, FPGAs, machine learning accelerators, and other domain-specific processors. Meanwhile, data-intensive workloads pose greater challenges to both on-chip and off-chip communications, whereas emerging technologies offer new opportunities for interconnection networks. These trends on architecture, application and technology require innovative communication designs for the next generation of computing systems. This special section is intended to explore academic and industrial research on all topics related to the communication issues in general-purpose and domain-specific processors.},
  archive      = {J_TC},
  author       = {Zhonghai Lu},
  doi          = {10.1109/TC.2021.3068060},
  journal      = {IEEE Transactions on Computers},
  number       = {6},
  pages        = {817-818},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Guest editorial: IEEE TC special issue on communications for many-core processors and accelerators},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DVFS-based quality maximization for adaptive applications
with diminishing return. <em>TC</em>, <em>70</em>(5), 803–816. (<a
href="https://doi.org/10.1109/TC.2020.2997242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application-level approximate computing exploits inherent resilience of adaptive applications, and trades off application output quality for runtime system resources. Existing methods treat computing quality as the number of clock cycles to execute a task, but they overlook the fact that the quality of many real-life applications exhibit the characteristic of diminishing return as the processor continues executing. The diminishing return of the quality is largely due to the features of iterative processing or successive refinement inherent in those applications. Ignoring it leads to large over-estimation in contemporary quality optimization approaches. In this article, we exploit the application adaptability to achieve quality maximization by taking both system resource constraints and diminishing return of the quality into account. We first reveal that the diminishing return of the quality is inherent in several well-known applications, and suggest an exponential model that accurately captures it. Second, we propose a dynamic frequency scaling (DFS) methodology to optimally decide the processor execution cycles for such applications, in order to maximize the output quality under system energy, timing, and temperature constraints. We transform the DFS problem to an iterative pseudo quadratic programming heuristic that can be efficiently solved. Third, we present a wrapping dynamic voltage scaling (wDVS) methodology to achieve further quality improvement, by judiciously adjusting the supply voltage to provide extra frequency scaling space. Compared to state-of-the-art algorithms, our approach produces at least 19.1 percent quality improvement on all evaluated cases, with negligible execution overhead.},
  archive      = {J_TC},
  author       = {Heng Yu and Yajun Ha and Bharadwaj Veeravalli and Fupeng Chen and Hesham El-Sayed},
  doi          = {10.1109/TC.2020.2997242},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {803-816},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DVFS-based quality maximization for adaptive applications with diminishing return},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BaPa: A novel approach of improving load balance in parallel
matrix factorization for recommender systems. <em>TC</em>,
<em>70</em>(5), 789–802. (<a
href="https://doi.org/10.1109/TC.2020.2997051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simplified approach to accelerate matrix factorization of big data is to parallelize it. A commonly used method is to divide the matrix into multiple non-intersecting blocks and concurrently calculate them. This operation causes the Load balance problem, which significantly impacts parallel performance and is a big concern. A general belief is that the load balance across blocks is impossible by balancing rows and columns separately. We challenge the belief by proposing an approach of “Balanced Partitioning (BaPa)”. We demonstrate under what circumstance independently balancing rows and columns can lead to the balanced intersection of rows and columns, why, and how. We formally prove the feasibility of BaPa by observing the variance of rating numbers across blocks, and empirically validate its soundness by applying it to two standard parallel matrix factorization algorithms, DSGD and CCD++. Besides, we establish a mathematical model of “Imbalance Degree” to explain further why BaPa works well. BaPa is applied to synchronous parallel matrix factorization, but as a general load balance solution, it has significant application potential.},
  archive      = {J_TC},
  author       = {Ruixin Guo and Feng Zhang and Lizhe Wang and Wusheng Zhang and Xinya Lei and Rajiv Ranjan and Albert Y. Zomaya},
  doi          = {10.1109/TC.2020.2997051},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {789-802},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BaPa: A novel approach of improving load balance in parallel matrix factorization for recommender systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient repair analysis algorithm exploration for memory
with redundancy and in-memory ECC. <em>TC</em>, <em>70</em>(5), 775–788.
(<a href="https://doi.org/10.1109/TC.2020.2996747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory error correction code (ECC) is a promising technique to improve the yield and reliability of high density memory design. However, the use of in-memory ECC poses a new problem to memory repair analysis algorithm, which has not been explored before. This article first makes a quantitative evaluation and demonstrates that the straightforward algorithms for memory with redundancy and in-memory ECC have serious deficiency on either repair rate or repair analysis speed. Accordingly, an optimal repair analysis algorithm that leverages preprocessing/filter algorithms, hybrid search tree, and depth-first search strategy is proposed to achieve low computational complexity and optimal repair rate in the meantime. In addition, a heuristic repair analysis algorithm that uses a greedy strategy is proposed to efficiently find repair solutions. Experimental results demonstrate that the proposed optimal repair analysis algorithm can achieve optimal repair rate and increase the repair analysis speed by up to 10 5 ×105× compared with the straightforward exhaustive search algorithm. The proposed heuristic repair analysis algorithm is approximately 28 percent faster than the proposed optimal algorithm, at the expense of 5.8 percent repair rate loss.},
  archive      = {J_TC},
  author       = {Minjie Lv and Hongbin Sun and Jingmin Xin and Nanning Zheng},
  doi          = {10.1109/TC.2020.2996747},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {775-788},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient repair analysis algorithm exploration for memory with redundancy and in-memory ECC},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MUSE: A multi-tierd and SLA-driven deduplication framework
for cloud storage systems. <em>TC</em>, <em>70</em>(5), 759–774. (<a
href="https://doi.org/10.1109/TC.2020.2996638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For cloud storage service vendors, balancing the client-perceived IO performance and the self-perceived space cost is always one of the standing challenges. When applying deduplication techniques for the cloud storage systems, the demand for optimizing such tradeoff becomes more pressing. Enabling deduplication decreases the storage space cost, whereas the IO performance will be somewhat affected due to extra processing overhead and data fragmentation. In this article, we address this challenge by proposing MUSE, a MUti-tiered and SLA-drivEn deduplication framework for cloud storage systems. First, we propose a novel notation of Dedup-SLA (deduplication-oriented service level agreement). With different levels of quantified performance/space-cost combinations, the Dedup-SLA serves as a refined service quality protocol between service vendor and customer. Second, MUSE adopts multi-tiered deduplication that orchestrates several combinational forms of deduplication into multiple tiers with varied “deduplication strength”. Third, we implement a mechanism called dynamic deduplication regulation (DDR) to adjust the deduplication behavior during runtime. MUSE&#39;s deduplication behavior is periodically switched between tiers according to the predefined Dedup-SLA and instant system status. We conduct comprehensive experiments to compare MUSE with several other types of deduplication schemes. The results demonstrate that MUSE significantly optimizes the IO-performance/space-cost balance compared to other schemes, hence delivering higher deduplication service quality for deduplication-enabled cloud storage systems.},
  archive      = {J_TC},
  author       = {Jianwei Yin and Yan Tang and Shuiguang Deng and Bangpeng Zheng and Albert Y. Zomaya},
  doi          = {10.1109/TC.2020.2996638},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {759-774},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MUSE: A multi-tierd and SLA-driven deduplication framework for cloud storage systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection of SLA violation for big data analytics
applications in cloud. <em>TC</em>, <em>70</em>(5), 746–758. (<a
href="https://doi.org/10.1109/TC.2020.2995881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SLA violations do happen in real world. An SLA violation represents the failure of guaranteeing a service, which leads to unwanted consequences such as penalty payments, profit margin reduction, reputation degradation, customer churn and service interruptions. Hence, in the context of cloud-hosted big data analytics applications (BDAAs), it is paramount for providers to predict and prevent SLA violations. While machine learning-based techniques have been applied to detect SLA violations for web service or general cloud service, the study on detecting SLA violations dedicated for cloud-hosted BDAAs is still lacking. In this article, we propose four machine learning techniques and integrate 12 resampling methods to detect SLA violations for batch-based BDAAs in the cloud. We evaluate the efficiency of the proposed techniques in comparison with ideal and baseline classifiers based on a real-world trace dataset (Alibaba). Our work not only helps providers to choose the best performing prediction technique, but also provides them capabilities to uncover the hidden pattern of multiple configurations of BDAAs across layers.},
  archive      = {J_TC},
  author       = {Xuezhi Zeng and Saurabh Garg and Mutaz Barika and Sanat Bista and Deepak Puthal and Albert Y. Zomaya and Rajiv Ranjan},
  doi          = {10.1109/TC.2020.2995881},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {746-758},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Detection of SLA violation for big data analytics applications in cloud},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random CFI (RCFI): Efficient fine-grained control-flow
integrity through random verification. <em>TC</em>, <em>70</em>(5),
733–745. (<a href="https://doi.org/10.1109/TC.2020.2995838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In theory, Control-Flow Integrity (CFI) is considered a principled solution against control-data attacks. However, most fine-grained CFI schemes that ensure such high security suffer from significant performance overhead. Existing practical implementations have been proposed to overcome this performance overhead problem, but they have proven unable to guarantee high security because development of these implementations has focused on only improving performance, at the expense of the security guarantee. Even though it is important for CFI schemes to provide both high security and low performance overhead, existing research on CFI is limited either by way of performance or security guarantee. We propose a new approach of verification method in fine-grained CFI to achieve these two goals. Our scheme performs selective and random verifications for certain branches rather than all branches, and thus, can reduce performance overhead. We show improved performance by evaluating our proof-of-concept implementation on SPEC CPU 2017. In addition, we also show that our scheme does not significantly sacrifice the security guarantee of fine-grained CFI by analyzing the structure of existing control-data attack exploits, which were collected from real-world exploits DB and related literature.},
  archive      = {J_TC},
  author       = {Moon Chan Park and Dong Hoon Lee},
  doi          = {10.1109/TC.2020.2995838},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {733-745},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Random CFI (RCFI): Efficient fine-grained control-flow integrity through random verification},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An FPGA based accelerator for clustering algorithms with
custom instructions. <em>TC</em>, <em>70</em>(5), 725–732. (<a
href="https://doi.org/10.1109/TC.2020.2995761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms are becoming popular and widely applied in many academic fields, such as machine learning, pattern recognition, and artificial intelligence. It has posed significant challenges to accelerate the algorithms due to the explosive data scale and wide variety of applications. However, previous studies mainly focus on the raw speedup with insufficient attention to the flexibility of the accelerator to support various applications. In order to accelerate different clustering algorithms in one accelerator, in this article, we design an accelerating framework based on FPGA for four state-of-the-art clustering methods, including K-means, PAM, SLINK, and DBSCAN algorithms. Moreover, we provide both euclidean and Manhattan distances as similarity metrics in the accelerator design paradigm. Moreover, we provide a custom instruction set to operate the accelerators within each application. In order to evaluate the performance and hardware cost of the accelerator, we constructed a hardware prototype on the state-of-the-art Xilinx FPGA platform. Experimental results demonstrate that the accelerator framework is able to achieve up to 23× speedup than Intel Xeon processor, and is 9.46× more energy efficient than NVIDIA GTX 750 GPU accelerators.},
  archive      = {J_TC},
  author       = {Chao Wang and Lei Gong and Fahui Jia and Xuehai Zhou},
  doi          = {10.1109/TC.2020.2995761},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {725-732},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An FPGA based accelerator for clustering algorithms with custom instructions},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VISE: Combining intel SGX and homomorphic encryption for
cloud industrial control systems. <em>TC</em>, <em>70</em>(5), 711–724.
(<a href="https://doi.org/10.1109/TC.2020.2995638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting data-in-use from privileged attackers is challenging. New CPU extensions (notably: Intel SGX) and cryptographic techniques (specifically: Homomorphic Encryption) can guarantee privacy even in untrusted third-party systems. HE allows sensitive processing on ciphered data. However, it is affected by i) a dramatic ciphertext expansion making HE unusable when bandwidth is narrow, ii) unverifiable conditional variables requiring off-premises support. Intel SGX allows sensitive processing in a secure enclave. Unfortunately, it is i) strictly bonded to the hosting server making SGX unusable when the live migration of cloud VMs/Containers is desirable, ii) limited in terms of usable memory, which is in contrast with resource-consuming data processing. In this article, we propose the VIrtual Secure Enclave (VISE), an approach that effectively combines the two aforementioned techniques, to overcome their limitations and ultimately make them usable in a typical cloud setup. VISE moves the execution of sensitive HE primitives (e.g., encryption) to the cloud in a remotely attested SGX enclave, and then performs sensitive processing on HE data-outside the enclave-leveraging all the memory resources available. We demonstrate that VISE meets the challenging security and performance requirements of a substantial application in the Industrial Control Systems domain. Our experiments prove the practicability of the proposed solution.},
  archive      = {J_TC},
  author       = {Luigi Coppolino and Salvatore D’Antonio and Valerio Formicola and Giovanni Mazzeo and Luigi Romano},
  doi          = {10.1109/TC.2020.2995638},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {711-724},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VISE: Combining intel SGX and homomorphic encryption for cloud industrial control systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VecQ: Minimal loss DNN model compression with vectorized
weight quantization. <em>TC</em>, <em>70</em>(5), 696–710. (<a
href="https://doi.org/10.1109/TC.2020.2995593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization has been proven to be an effective method for reducing the computing and/or storage cost of DNNs. However, the trade-off between the quantization bitwidth and final accuracy is complex and non-convex, which makes it difficult to be optimized directly. Minimizing direct quantization loss (DQL) of the coefficient data is an effective local optimization method, but previous works often neglect the accurate control of the DQL, resulting in a higher loss of the final DNN model accuracy. In this paper, we propose a novel metric, called Vector Loss. Using this new metric, we decompose the minimization of the DQL to two independent optimization processes, which significantly outperform the traditional iterative L2 loss minimization process in terms of effectiveness, quantization loss as well as final DNN accuracy. We also develop a new DNN quantization solution called VecQ, which provides minimal direct quantization loss and achieve higher model accuracy. In order to speed up the proposed quantization process during model training, we accelerate the quantization process with a parameterized probability estimation method and template-based derivation calculation. We evaluate our proposed algorithm on MNIST, CIFAR, ImageNet, IMDB movie review and THUCNews text data sets with numerical DNN models. The results demonstrate that our proposed quantization solution is more accurate and effective than the state-of-the-art approaches yet with more flexible bitwidth support. Moreover, the evaluation of our quantized models on Salient Object Detection (SOD) tasks maintains comparable feature extraction quality with up to 16× weight size reduction.},
  archive      = {J_TC},
  author       = {Cheng Gong and Yao Chen and Ye Lu and Tao Li and Cong Hao and Deming Chen},
  doi          = {10.1109/TC.2020.2995593},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {696-710},
  shortjournal = {IEEE Trans. Comput.},
  title        = {VecQ: Minimal loss DNN model compression with vectorized weight quantization},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COUNTDOWN: A run-time library for performance-neutral energy
saving in MPI applications. <em>TC</em>, <em>70</em>(5), 682–695. (<a
href="https://doi.org/10.1109/TC.2020.2995269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power and energy consumption are becoming key challenges for the supercomputers&#39; exascale race. HPC systems&#39; processors waist active power during communication and synchronization among the MPI processes in large-scale HPC applications. However, due to the time scale at which communication happens, transitioning into low-power states while waiting for the completion of each communication may introduce unacceptable overhead. In this article, we present COUNTDOWN, a run-time library for identifying and automatically reducing the power consumption of the CPUs during communication and synchronization. COUNTDOWN saves energy without penalizing the time-to-completion by lowering CPUs power consumption only during idle times for which power state transition overhead is negligible. This is done transparently to the user, without requiring labor-intensive and error-prone application code modifications, nor requiring recompilation of the application. We test our methodology on a production Tier-1 system. For the NAS benchmarks, COUNTDOWN saves between 6 and 50 percent energy, with a time-to-solution penalty lower than 5 percent. In a complete production-Quantum ESPRESSO-for a 3.5K cores run, COUNTDOWN saves 22.36 percent energy, with a performance penalty below 3 percent. Energy saving increases to 37 percent with a performance penalty of 6.38 percent, if the application is executed without communication tuning.},
  archive      = {J_TC},
  author       = {Daniele Cesarini and Andrea Bartolini and Pietro Bonfà and Carlo Cavazzoni and Luca Benini},
  doi          = {10.1109/TC.2020.2995269},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {682-695},
  shortjournal = {IEEE Trans. Comput.},
  title        = {COUNTDOWN: A run-time library for performance-neutral energy saving in MPI applications},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GenoDedup: Similarity-based deduplication and delta-encoding
for genome sequencing data. <em>TC</em>, <em>70</em>(5), 669–681. (<a
href="https://doi.org/10.1109/TC.2020.2994774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast datasets produced in human genomics must be efficiently stored, transferred, and processed while prioritizing storage space and restore performance . Balancing these two properties becomes challenging when resorting to traditional data compression techniques. In fact, specialized algorithms for compressing sequencing data favor the former, while large genome repositories widely resort to generic compressors (e.g., GZIP) to benefit from the latter. Notably, human beings have approximately 99.9 percent of DNA sequence similarity, vouching for an excellent opportunity for deduplication and its assets: leveraging inter-file similarity and achieving higher read performance. However, identity-based deduplication fails to provide a satisfactory reduction in the storage requirements of genomes. In this article, we balance space savings and restore performance by proposing ${\sf GenoDedup}$ , the first method that integrates efficient similarity-based deduplication and specialized delta-encoding for genome sequencing data. Our solution currently achieves 67.8 percent of the reduction gains of SPRING (i.e., the best specialized tool in this metric) and restores data $1.62\times$ faster than SeqDB (i.e., the fastest competitor). Additionally, $\mathsf{ GenoDedup}$ restores data $9.96\times$ faster than SPRING and compresses files $2.05\times$ more than SeqDB.},
  archive      = {J_TC},
  author       = {Vinicius Cogo and João Paulo and Alysson Bessani},
  doi          = {10.1109/TC.2020.2994774},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {669-681},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GenoDedup: Similarity-based deduplication and delta-encoding for genome sequencing data},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAFA: A semi-asynchronous protocol for fast federated
learning with low overhead. <em>TC</em>, <em>70</em>(5), 655–668. (<a
href="https://doi.org/10.1109/TC.2020.2994391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has attracted increasing attention as a promising approach to driving a vast number of end devices with artificial intelligence. However, it is very challenging to guarantee the efficiency of FL considering the unreliable nature of end devices while the cost of device-server communication cannot be neglected. In this article, we propose SAFA, a semi-asynchronous FL protocol, to address the problems in federated learning such as low round efficiency and poor convergence rate in extreme conditions (e.g., clients dropping offline frequently). We introduce novel designs in the steps of model distribution, client selection and global aggregation to mitigate the impacts of stragglers, crashes and model staleness in order to boost efficiency and improve the quality of the global model. We have conducted extensive experiments with typical machine learning tasks. The results demonstrate that the proposed protocol is effective in terms of shortening federated round duration, reducing local resource wastage, and improving the accuracy of the global model at an acceptable communication cost.},
  archive      = {J_TC},
  author       = {Wentai Wu and Ligang He and Weiwei Lin and Rui Mao and Carsten Maple and Stephen Jarvis},
  doi          = {10.1109/TC.2020.2994391},
  journal      = {IEEE Transactions on Computers},
  number       = {5},
  pages        = {655-668},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SAFA: A semi-asynchronous protocol for fast federated learning with low overhead},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ECC-united cache: Maximizing efficiency of error
detection/correction codes in associative cache memories. <em>TC</em>,
<em>70</em>(4), 640–654. (<a
href="https://doi.org/10.1109/TC.2020.2994067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Error Detection/Correction Codes (EDCs/ECCs) are the most conventional approaches to protect on-chip caches against radiation-induced soft errors. The overhead of EDCs/ECCs is a major concern and is of decisive importance when a higher protection capability is required to tolerate multiple adjacent bit errors (burst errors). This article proposes the ECC-United Cache (EUC) architecture to improve the efficiency of EDCs/ECCs in set-associative L1 caches. EUC architecture extends the data protection granularity from a single word to multiple words by exploiting the parallel cache lines access, which is inherently available in the cache. As compared with the conventional architecture, EUC can be configured to provide: 1) the same protection capability with a significantly lower overhead, 2) a significantly higher protection capability with the same number of check bits, or 3) a trade-off between the former two features. Simulation results show that, when configured to minimize the overhead, EUC reduces the number of check bits by 69 and 75 percent in data-cache and instruction-cache, respectively. When configured to maximize the protection capability, EUC provides fourfold higher burst error detection/correction capability. Moreover, EUC is orthogonal to previous protection schemes and they can be redesigned based on the EUC architecture to further improve their efficiency.},
  archive      = {J_TC},
  author       = {Hamed Farbeh and Leila Delshadtehrani and Hyeonggyu Kim and Soontae Kim},
  doi          = {10.1109/TC.2020.2994067},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {640-654},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ECC-united cache: Maximizing efficiency of error Detection/Correction codes in associative cache memories},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An energy-aware high performance task allocation strategy in
heterogeneous fog computing environments. <em>TC</em>, <em>70</em>(4),
626–639. (<a href="https://doi.org/10.1109/TC.2020.2993561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the Internet-of-Things (IoT) technology with cloud computing is a significant alternative for powering the utilization of computing resources in a connected environment. A grand challenge in communications is raised by the emergence of big data, due to the large-sized data transmissions and frequent data exchanges. Applying fog computing is considered an option for resolving the communication challenge. However, a high extent of available heterogeneous computing attached to fog computing servers leads to a restriction of the resource management. This Article addresses the resource management issue by proposing a novel approach - named Energy-aware Fog Resource Optimization (EFRO) model- to optimizing the utilization of connected devices in fog computing. We develop a heuristic algorithm minimizing both energy cost and time consumption in a holistic way. A salient feature of EFRO lies in the integration of the standardization and smart shift operations fueled by a hill-climbing mechanism to produce near-optimal resource allocation solutions. Experimental results demonstrate that our EFRO is adroit at making near-optimal decisions in managing resources in fog computing environments. In particular, EFRO boosts the energy efficiency of the existing MESF and RR schemes by 54.83 and 71.28 percent, respectively. EFRO shortens DECM&#39;s allocation-generation time by up to a factor of 507.},
  archive      = {J_TC},
  author       = {Keke Gai and Xiao Qin and Liehuang Zhu},
  doi          = {10.1109/TC.2020.2993561},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {626-639},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An energy-aware high performance task allocation strategy in heterogeneous fog computing environments},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An improved logarithmic multiplier for energy-efficient
neural computing. <em>TC</em>, <em>70</em>(4), 614–625. (<a
href="https://doi.org/10.1109/TC.2020.2992113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplication is the most resource-hungry operation in neural networks (NNs). Logarithmic multipliers (LMs) simplify multiplication to shift and addition operations and thus reduce the energy consumption. Since implementing the logarithm in a compact circuit often introduces approximation, some accuracy loss is inevitable in LMs. However, this inaccuracy accords with the inherent error tolerance of NNs and their associated applications. This article proposes an improved logarithmic multiplier (ILM) that, unlike existing designs, rounds both inputs to their nearest powers of two by using a proposed nearest-one detector (NOD) circuit. Considering that the output of the NOD uses a one-hot representation, some entries in the truth table of a conventional adder cannot occur. Hence, a compact adder is designed for the reduced truth table. The 8x8 ILM achieves up to 17.48 percent saving in power consumption compared to a recent LM in the literature while being almost 8 percent more accurate. Moreover, the evaluation of the ILM for two benchmark NN workloads shows up to 21.85 percent reduction in energy consumption compared to the NNs implemented with other LMs. Interestingly, using the ILM increases the classification accuracy of the considered NNs by up to 1.4 percent compared to a NN implementation that uses exact multipliers.},
  archive      = {J_TC},
  author       = {Mohammad Saeed Ansari and Bruce F. Cockburn and Jie Han},
  doi          = {10.1109/TC.2020.2992113},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {614-625},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An improved logarithmic multiplier for energy-efficient neural computing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ECDR<span class="math inline"><sup>2</sup></span><!-- -->2:
Error corrector and detector relocation router for network-on-chip.
<em>TC</em>, <em>70</em>(4), 606–613. (<a
href="https://doi.org/10.1109/TC.2020.2991749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-on-chip (NoC) is commonly used in modern many-core systems due to their high bandwidth and flexibility. As the manufacturing process keeps scaling, the reliability challenge in NoCs increases as well. The error correction code (ECC) is widely adopted in error correction NoCs to improve the data correctness. At the same time, extra stages are introduced in the router pipeline to improve the error correction capability. As a result, conventional error correction routers suffer from high network latency. Motivated by this limitation, i.e., we remove the extra pipeline stages delicately introduced for error correction. We propose an error correction router, called error corrector and detector relocation router (ECDR 2 ), whose architecture optimizes the pipeline flow of the router. As a result, it can achieve both low latency and high error correction. Experimental results show that, compared with the baseline design, ECDR 2 obtains 13.67 and 39.4 percent less average latency under the uniform traffic pattern and Dedup benchmark, respectively, in an 8 × 8 mesh NoC. The circuit area of ECR is also 7.9 percent less than that of the baseline design under 45-nm technology.},
  archive      = {J_TC},
  author       = {Letian Huang and Chikun Yuan and Junshi Wang and Masoumeh Ebrahimi and Xuan Xie and Qiang Li},
  doi          = {10.1109/TC.2020.2991749},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {606-613},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ECDR$^{2}$2: Error corrector and detector relocation router for network-on-chip},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Device-circuit-architecture co-exploration for
computing-in-memory neural accelerators. <em>TC</em>, <em>70</em>(4),
595–605. (<a href="https://doi.org/10.1109/TC.2020.2991575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-exploration of neural architectures and hardware design is promising due to its capability to simultaneously optimize network accuracy and hardware efficiency. However, state-of-the-art neural architecture search algorithms for the co-exploration are dedicated for the conventional von-Neumann computing architecture, whose performance is heavily limited by the well-known memory wall. In this article, we are the first to bring the computing-in-memory architecture, which can easily transcend the memory wall, to interplay with the neural architecture search, aiming to find the most efficient neural architectures with high network accuracy and maximized hardware efficiency. Such a novel combination makes opportunities to boost performance, but also brings a bunch of challenges: The optimization space spans across multiple design layers from device type and circuit topology to neural architecture; and the presence of device variation may drastically degrade the neural network performance. To address these challenges, we propose a cross-layer exploration framework, namely NACIM, which jointly explores device, circuit and architecture design space and takes device variation into consideration to find the most robust neural architectures, coupled with the most efficient hardware design. Experimental results demonstrate that NACIM can find the robust neural network with 0.45 percent accuracy loss in the presence of device variation, compared with a 76.44 percent loss from the state-of-the-art NAS without consideration of variation; in addition, NACIM achieves an energy efficiency up to 16.3 TOPs/W, 3.17× higher than the state-of-the-art NAS.},
  archive      = {J_TC},
  author       = {Weiwen Jiang and Qiuwen Lou and Zheyu Yan and Lei Yang and Jingtong Hu and Xiaobo Sharon Hu and Yiyu Shi},
  doi          = {10.1109/TC.2020.2991575},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {595-605},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Device-circuit-architecture co-exploration for computing-in-memory neural accelerators},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning-based modeling and optimization for real-time
system availability. <em>TC</em>, <em>70</em>(4), 581–594. (<a
href="https://doi.org/10.1109/TC.2020.2991177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the density of integrated circuits continues to increase, the possibility that real-time systems suffer from soft and hard errors rises significantly, resulting in a degraded availability of system. In this article, we investigate the dynamic modeling of cross-layer soft error rate based on the Back Propagation (BP) neural network, and propose optimization strategies for system availability based on Cross Entropy (CE) and Q-learning algorithms. Specifically, the BP neural network is trained using cross-layer simulation data obtained from SPICE simulation while the optimization for system availability is achieved by judiciously selecting an optimal supply voltage for processors under timing constraints. Simulation results show that the CE-based method can improve system availability by up to 32 percent compared to state-of-the-art methods, and the Q-learning-based algorithm can further enhance system availability by up to 20 percent compared to the proposed CE-based method.},
  archive      = {J_TC},
  author       = {Liying Li and Junlong Zhou and Tongquan Wei and Mingsong Chen and Xiaobo Sharon Hu},
  doi          = {10.1109/TC.2020.2991177},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {581-594},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Learning-based modeling and optimization for real-time system availability},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving write performance on cross-point RRAM arrays by
leveraging multidimensional non-uniformity of cell effective voltage.
<em>TC</em>, <em>70</em>(4), 566–580. (<a
href="https://doi.org/10.1109/TC.2020.2990884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive cross-point memory arrays can be used to construct high-density storage-class memory. However, coupled IR drop and sneak currents cause multidimensional non-uniformity of cell effective voltage in cross-point arrays. The voltage non-uniformity significantly degrades write performance on cross-point memory if only adopting the worst-case write latency at partial dimensions. Furthermore, the non-uniformity of cell effective voltage in cross-point arrays depends on multidimensional dynamic write operation parameters: row, column as well as layer address, the number of selected cells, and the number of half-selected low-resistance state cells. In this article, we aim to improve the write performance by leveraging multidimensional non-uniformity of cell effective voltage. First, we analyze the impact of multidimensional write parameters on effective voltage and write latency. Then, we design the memory array write scheme that measures the write parameters and sets the write latency accordingly. We further analyze the features and effects of interlayer sneak currents and extend the scheme to 3D cross-point memory. The evaluation shows that the proposed memory array write scheme can reduce the memory access latency by 75.6 and 64.1 percent, and improve the system performance by 4.5 times and 3.4 times on average, compared with the baseline and the state-of-the-art approach, respectively.},
  archive      = {J_TC},
  author       = {Chengning Wang and Dan Feng and Wei Tong and Jingning Liu and Bing Wu and Wei Zhao and Yang Zhang and Yiran Chen},
  doi          = {10.1109/TC.2020.2990884},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {566-580},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving write performance on cross-point RRAM arrays by leveraging multidimensional non-uniformity of cell effective voltage},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TurboDL: Improving the CNN training on GPU with fine-grained
multi-streaming scheduling. <em>TC</em>, <em>70</em>(4), 552–565. (<a
href="https://doi.org/10.1109/TC.2020.2990321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) have evolved as powerful co-processors for the CNN training. Many new features have been introduced into GPUs such as concurrent kernel execution and hyper-Q technology. It is challenging to orchestrate concurrency for CNN (convolutional neural networks) training on GPUs since it may introduce synchronization overhead and poor resource utilization. Unlike previous research which mainly focuses on single layer or coarse-grained optimization, we introduce a critical-path based, asynchronous parallelization mechanism, and propose the optimization technique for the CNN training that takes into account global network architecture and GPU resource usage together. The proposed methods can effectively overlap the synchronization and the computation in different streams. As a result, the training process of CNN is accelerated. We have integrated our methods into Caffe. The experimental results show that the Caffe integrated with our methods can achieve 1.30X performance speedup on average compared with Caffe+cuDNN, and even higher performance speedup can be achieved for deeper, wider, and more complicated networks.},
  archive      = {J_TC},
  author       = {Hai Jin and Wenchao Wu and Xuanhua Shi and Ligang He and Bing Bing Zhou},
  doi          = {10.1109/TC.2020.2990321},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {552-565},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TurboDL: Improving the CNN training on GPU with fine-grained multi-streaming scheduling},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Amnesiac DRAM: A proactive defense mechanism against cold
boot attacks. <em>TC</em>, <em>70</em>(4), 539–551. (<a
href="https://doi.org/10.1109/TC.2019.2946365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DRAMs in modern computers or hand-held devices store private or often security-sensitive data. Unfortunately, one known attack vector, called a cold boot attack, remains threatening and easy-to-exploit, especially when attackers have physical access to the device. It exploits the fundamental property of current DRAMs: remanence effects that retain the stored contents for a certain period of time even after powering off. To magnify the remanence effect, cold boot attacks typically freeze the victim DRAM, thereby providing a chance to detach, move, and reattach it to an attacker&#39;s computer. Once power is on, attackers can steal all the security-critical information from the victim&#39;s DRAM, such as a master decryption key for an encrypted disk storage. Two types of defenses were proposed in the past: 1) CPU-bound cryptography, where keys are stored in CPU registers and caches instead of in DRAMs, and 2) full or partial memory encryption, where sensitive data are stored encrypted. However, both methods impose non-negligible performance or energy overheads to the running systems, and worse, significantly increase the hardware and software manufacturing costs. We found that these proposed solutions attempted to address the cold boot attacks passively: either by avoiding or by indirectly addressing the root cause of the problem, the remanence effect. In this article, we propose and evaluate a proactive defense mechanism, Amnesiac DRAM, that comprehensively prevents the cold boot attacks. The key idea is to discard the contents in the DRAM when attackers attempt to retrieve (i.e., power on) them from the stolen DRAM. When Amnesiac DRAM senses a physical separation, it locks itself and deletes all the remaining contents, making it amnesiac. The Amnesiac DRAM causes neither performance nor energy overhead in ordinary operations (e.g., load and store) and can be easily implemented with negligible area overhead in commodity DRAM architectures.},
  archive      = {J_TC},
  author       = {Hoseok Seol and Minhye Kim and Taesoo Kim and Yongdae Kim and Lee-Sup Kim},
  doi          = {10.1109/TC.2019.2946365},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {539-551},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Amnesiac DRAM: A proactive defense mechanism against cold boot attacks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficiently solving partial differential equations in a
partially reconfigurable specialized hardware. <em>TC</em>,
<em>70</em>(4), 524–538. (<a
href="https://doi.org/10.1109/TC.2021.3060700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific computations with a wide range of applications in domains such as developing vaccines, forecasting the weather, predicting natural disasters, simulating aerodynamics of spacecraft, and exploring oil resources, create the main workloads of supercomputers. The key integration of such scientific computations is modeling physical phenomena that are done with the aid of partial differential equations (PDEs). Solving PDEs on supercomputers, even with those equipped with GPUs, consumes a large amount of power and yet is not as fast as desired. The main reason behind such slow processing is data dependency. The key challenge is that software techniques cannot resolve these dependencies, therefore, such applications cannot benefit from the parallelism provided by processors such as GPUs. Our key insight to address this challenge is that although we cannot resolve the dependencies, we can reduce their negative impacts by using hardware/software co-optimization. To this end, we propose breaking down the data-dependent operations into two groups of operations: a majority of parallelizable and the minority of data-dependent operations. We execute these two groups in the desired order: first, we put together all parallelizable operations and execute them all, subsequently; then, we switch to execute the small data-dependent part. As long as the data-dependent part is small, we can accelerate them by using fast hardware mechanisms. Besides, our proposed hardware mechanisms guarantee quickly switching between the two groups of operations. To follow the same order of execution, dictated by our software mechanism, and implemented in hardware, we also propose a new low-overhead compression format - sparsity is another attribute of PDEs that require compression. Furthermore, the core generic architecture of our proposed hardware allows the execution of other applications including sparse matrix-vector multiplication (SpMV) and graph algorithms. The key feature of the proposed hardware is partial reconfigurability, which on one hand, facilitates the execution of data-dependent computations, and on the other hand, allows executing broad application without changing the entire configuration. Our evaluations show that compared to GPUs, we achieve an average speedup of 15.6x for scientific computations while consuming 14x less energy.},
  archive      = {J_TC},
  author       = {Bahar Asgari and Ramyad Hadidi and Tushar Krishna and Hyesoon Kim and Sudhakar Yalamanchili},
  doi          = {10.1109/TC.2021.3060700},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {524-538},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficiently solving partial differential equations in a partially reconfigurable specialized hardware},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leaking information through cache LRU states in commercial
processors and secure caches. <em>TC</em>, <em>70</em>(4), 511–523. (<a
href="https://doi.org/10.1109/TC.2021.3059531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Least-Recently Used (LRU) cache replacement policy and its variants are widely deployed in modern processors. This article shows in detail that the LRU states of caches can be used to leak information: any access to a cache by a sender will modify the LRU state, and the receiver is able to observe this through a timing measurement. This article presents LRU timing-based channels both when the sender and the receiver have access to shared memory, e.g., shared library, and when they are separate processes without shared memory. In addition, the new LRU timing-based channels are demonstrated on both Intel and AMD processors in scenarios where the sender and the receiver are sharing the cache in both hyper-threaded setting and time-sliced setting. The transmission rates of the LRU channels can be up to 600 Kbps per cache set in the hyper-threaded setting. Different from the majority of existing cache channels which require the sender to trigger cache misses, the new LRU channels work with the sender only having cache hits, making the channel faster and stealthier. This article further discusses the effectiveness of the new LRU channels against a number of secure cache designs. Especially, the LRU channels are demonstrated to work against two representative secure caches, Partition-Locked (PL) cache and Random Fill (RF) cache, in the gem5 simulator, showing possible vulnerabilities in the secure cache designs in which the security of the replacement state is not protected properly.},
  archive      = {J_TC},
  author       = {Wenjie Xiong and Stefan Katzenbeisser and Jakub Szefer},
  doi          = {10.1109/TC.2021.3059531},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {511-523},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Leaking information through cache LRU states in commercial processors and secure caches},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enabling highly efficient capsule networks processing
through software-hardware co-design. <em>TC</em>, <em>70</em>(4),
495–510. (<a href="https://doi.org/10.1109/TC.2021.3056929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for the image processing increases, the image features become increasingly complicated. Although the Convolutional Neural Network (CNN) have been widely adopted for the imaging processing tasks, it has been found easily misled due to the massive usage of pooling operations. A novel neural network structure called Capsule Networks (CapsNet) is proposed to address the CNN challenge and essentially enhance the learning ability for the image segmentation and object detection. Since the CapsNet contains the high volume of the matrix execution, it has been generally accelerated on modern GPU platforms with the highly optimized deep-learning library. However, the routing procedure of CapsNet introduces the special program and execution features,including massive unshareable intermediate variables and intensive synchronizations, causing inefficient CapsNet execution on modern GPU. To address these challenges, we propose the software-hardware co-designed optimizations, SH-CapsNet, which includes the software-level optimizations named S-CapsNet and a hybrid computing architecture design named PIM-CapsNet . In software-level, S-CapsNet reduces the computation and memory accesses by exploiting the computational redundancy and data similarity of the routing procedure. In hardware-level, the PIM-CapsNet leverages the processing-in-memory capability of today&#39;s 3D stacked memory to conduct the off-chip in-memory acceleration solution for the routing procedure, while pipelining with the GPU&#39;s on-chip computing capability for accelerating CNN types of layers in CapsNet. Evaluation results demonstrate that either our software or hardware optimizations can significantly improve the CapsNet execution efficiency. Together, our co-design can achieve greatly improvement on both performance (3.41 x) and energy savings (68.72 percent) for CapsNet inference, with negligible accuracy loss.},
  archive      = {J_TC},
  author       = {Xingyao Zhang and Xin Fu and Donglin Zhuang and Chenhao Xie and Shuaiwen Leon Song},
  doi          = {10.1109/TC.2021.3056929},
  journal      = {IEEE Transactions on Computers},
  number       = {4},
  pages        = {495-510},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling highly efficient capsule networks processing through software-hardware co-design},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compiler-assisted data streaming for regular code
structures. <em>TC</em>, <em>70</em>(3), 483–494. (<a
href="https://doi.org/10.1109/TC.2020.2990302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of modern processors is often limited by execution stalls resulting from long memory access latencies. Compile-time optimizations, deep cache hierarchies and prefetching mechanisms already provide significant performance gains, by performing memory accesses in parallel with computation. However, they are reaching a throughput improvement limit. Hence, new solutions that effectively exploit the memory access patterns to improve processing throughput are required. To achieve this objective, a new compiler-assisted data streaming method is proposed. It leverages static analysis and code transformations with an on-chip data streaming support as a viable alternative to prefetching mechanisms for regular code structures. Static analysis is used to identify and encode memory accesses with a dedicated representation. Then, a code transformation algorithm detaches data indexation and address calculation from computation, allowing for a significant code reduction. An on-chip data stream controller, attached to the L1 data cache, is used to autonomously generate memory accesses from the pattern representation and reorganize the data transfers in streams, with the aid of stream buffers. When compared with state-of-the-art prefetchers, the proposed solution provides up to 26 percent of code reduction, an IPC improvement of 2.4x, and an average performance improvement of 40 percent.},
  archive      = {J_TC},
  author       = {Nuno Neves and Pedro Tomás and Nuno Roma},
  doi          = {10.1109/TC.2020.2990302},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {483-494},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Compiler-assisted data streaming for regular code structures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAG-fluid: A real-time scheduling algorithm for DAGs.
<em>TC</em>, <em>70</em>(3), 471–482. (<a
href="https://doi.org/10.1109/TC.2020.2990282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various scheduling algorithms have been proposed for real-time parallel tasks modeled as a Directed Acyclic Graph (DAG). The capacity augmentation bound is a quantitative metric widely used in this field to compare the algorithms. Among the existing algorithms, the lowest capacity augmentation bound for DAG tasks with implicit deadlines is 2, which has been achieved by federated scheduling. To improve the schedulability and lower the capacity augmentation bound, this paper proposes DAG-Fluid, an algorithm based on fluid scheduling. We prove that DAG-Fluid has a capacity augmentation bound of $2-\frac{1}{m+1}$ , in which $m$ is the number of processors in the system. Experiments show that DAG-Fluid performs better than the state of the art scheduling algorithms.},
  archive      = {J_TC},
  author       = {Fei Guan and Jiaqing Qiao and Yu Han},
  doi          = {10.1109/TC.2020.2990282},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {471-482},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DAG-fluid: A real-time scheduling algorithm for DAGs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized mixed-criticality static scheduling for periodic
directed acyclic graphs on multi-core processors. <em>TC</em>,
<em>70</em>(3), 457–470. (<a
href="https://doi.org/10.1109/TC.2020.2990229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In safety-critical systems many software components of different criticalities or assurance levels need to interact in a timely manner to keep the system and environment safe. Nowadays, these systems are challenged by technological progress resulting in rapid increases in both software complexity and processing demands. Efficiently designing safety-critical systems subject to stringent timing requirements is therefore a challenge and a necessity. In this article, we consider the mixed-criticality execution model and homogeneous multi-core processors. We begin by defining a task model incorporating mixed-criticality, real-time and precedence constraints in the form of directed acyclic graphs. A meta-heuristic to solve the scheduling problem of this task model is then defined and proved to respect deadlines, even when the system needs to give more processing power to the most critical tasks. The state-of-the-art techniques capable of scheduling a similar task model have only been developed for dual-criticality systems. Conversely, the meta-heuristic we propose has been generalized to support an arbitrary number of criticality levels. We instantiated our meta-heuristic adopting scheduling algorithms such as G-EDF, G-LLF, or G-EDZL for each level of criticality. The experiments show excellent results in terms of acceptance ratio and number of preemptions.},
  archive      = {J_TC},
  author       = {Roberto Medina and Etienne Borde and Laurent Pautet},
  doi          = {10.1109/TC.2020.2990229},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {457-470},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Generalized mixed-criticality static scheduling for periodic directed acyclic graphs on multi-core processors},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SECRET: Semantically enhanced classification of real-world
tasks. <em>TC</em>, <em>70</em>(3), 440–456. (<a
href="https://doi.org/10.1109/TC.2020.2989642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning (ML) algorithms are aimed at maximizing classification performance under available energy and storage constraints. They try to map the training data to the corresponding labels while ensuring generalizability to unseen data. However, they do not integrate meaning-based relationships among labels in the decision process. On the other hand, natural language processing (NLP) algorithms emphasize the importance of semantic information. In this article, we synthesize the complementary advantages of supervised ML and NLP algorithms into one method that we refer to as SECRET (Semantically Enhanced Classification of REal-world Tasks). SECRET performs classifications by fusing the semantic information of the labels with the available data: it combines the feature space of the supervised algorithms with the semantic space of the NLP algorithms and predicts labels based on this joint space. Experimental results indicate that, compared to traditional supervised learning, SECRET achieves up to 14.0 percent accuracy and 13.1 percent F1 score improvements. Moreover, compared to ensemble methods, SECRET achieves up to 12.7 percent accuracy and 13.3 percent F1 score improvements. This points to a new research direction for supervised classification based on incorporation of semantic information.},
  archive      = {J_TC},
  author       = {Ayten Ozge Akmandor and Jorge Ortiz and Irene Manotas and Bongjun Ko and Niraj K. Jha},
  doi          = {10.1109/TC.2020.2989642},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {440-456},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SECRET: Semantically enhanced classification of real-world tasks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On minimizing internal data migrations of flash devices via
lifetime-retention harmonization. <em>TC</em>, <em>70</em>(3), 428–439.
(<a href="https://doi.org/10.1109/TC.2020.2989554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emerge of high-density triple-level-cell (TLC) and 3D NAND flash, the access performance and endurance of flash devices are degraded due to the downscaling of flash cells. In addition, we observe that the mismatch between data lifetime requirement and flash block retention capability could further worsen the access performance and endurance. This is because the “lifetime-retention mismatch” could result in massive internal data migrations during garbage collection and data refreshing, and further aggravate the already-worsened access performance and endurance of high-density NAND flash devices. Such an observation motivates us to resolve the lifetime-retention mismatch problem by proposing a “time harmonization strategy”, which coordinates the flash block retention capability with the data lifetime requirement to enhance the performance of flash devices with very limited endurance degradation. Specifically, this study aims to lower the amount of internal data migrations caused by garbage collection and data refreshing via storing data of different lifetime requirement in flash blocks with suitable retention capability. The trace-driven evaluation results reveal that the proposed design can effectively reduce the average response time by about 99 percent on average without sacrificing the overall endurance, as compared with the state-of-the-art designs.},
  archive      = {J_TC},
  author       = {Ming-Chang Yang and Chun-Feng Wu and Shuo-Han Chen and Yi-Ling Lin and Che-Wei Chang and Yuan-Hao Chang},
  doi          = {10.1109/TC.2020.2989554},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {428-439},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On minimizing internal data migrations of flash devices via lifetime-retention harmonization},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NOSTalgy: Near-optimum run-time STT-MRAM quality-energy knob
management for approximate computing applications. <em>TC</em>,
<em>70</em>(3), 414–427. (<a
href="https://doi.org/10.1109/TC.2020.2989243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic switching feature of Spin-Transfer Torque Magnetic RAM (STT-MRAM) provides an attractive knob to trade quality for energy consumption in approximate computing applications. Indeed, the quality of STT-MRAM functionalities (mainly write operation) is increased by consuming more energy to achieve a more stable write. On the other hand, in approximate computing applications, we do not need 100 percent quality for all of the data. Accordingly, in recent years, several approaches have been proposed to find a balance between output threshold quality and energy consumption in approximate computing applications employing STT-MRAM on-chip memories. While approximate computing application output qualities are highly affected by the fluctuations of the environmental-conditions or input variations, none of the previously proposed approaches have considered the effects of these fluctuations on the output quality. In this article, we propose NOSTalgy, a closed-loop cross-layer approach to dynamically trade off the quality of STT-MRAM based cache memories for energy saving in approximate computing applications. NOSTalgy utilizes a feedback managed fine-grained cache-line-level actuation knobs with different levels of quality for individual write accesses. These knobs are adjusted with the support of the operating system and programmer at run-time. Our experimental results using a set of benchmarks show that NOSTalgy satisfies the output quality thresholds while delivering up to 52 percent energy savings with negligible performance and area overheads.},
  archive      = {J_TC},
  author       = {Arash Salahvarzi and Amir Mahdi Hosseini Monazzah and Mahdi Fazeli and Kevin Skadron},
  doi          = {10.1109/TC.2020.2989243},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {414-427},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NOSTalgy: Near-optimum run-time STT-MRAM quality-energy knob management for approximate computing applications},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Specification-driven conformance checking for
virtual/silicon devices using mutation testing. <em>TC</em>,
<em>70</em>(3), 400–413. (<a
href="https://doi.org/10.1109/TC.2020.2988906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern software systems, either system or application software, are increasingly being developed on top of virtualized software platforms. They may simply intend to execute on virtual machines or they may be expected to port to physical machines eventually. In either case, the devices, virtual or silicon, in the target virtual or physical machines are expected to conform to the specifications based on which the software systems have been developed. Non-conformance of these devices to the specifications can cause catastrophic failures of the software systems. In this article, we propose a mutation-based framework for effective and efficient conformance checking between virtual/silicon device implementations and their specifications. Based on our defined mutation operators, device specifications can be automatically instrumented with weak mutant-killing constraints to model potential erroneous device behaviors. To kill all feasible mutants, our approach adopts a cooperative symbolic execution mechanism that can efficiently automate the test case generation and conformance checking for virtual/silicon devices. By symbolically executing the instrumented specifications with virtual/silicon device traces obtained from the cooperative execution, our method can accurately measure whether the designs have been sufficiently validated and report the inconsistencies between device specifications and implementations. Comprehensive experiments on two industrial network adapters and their virtual devices demonstrate the effectiveness of our proposed approach in conformance checking for both virtual and silicon devices.},
  archive      = {J_TC},
  author       = {Haifeng Gu and Jianning Zhang and Mingsong Chen and Tongquan Wei and Li Lei and Fei Xie},
  doi          = {10.1109/TC.2020.2988906},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {400-413},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Specification-driven conformance checking for Virtual/Silicon devices using mutation testing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Schnorr-based implicit certification: Improving the security
and efficiency of vehicular communications. <em>TC</em>, <em>70</em>(3),
393–399. (<a href="https://doi.org/10.1109/TC.2020.2988637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the implicit certification model, the process of verifying the validity of the signer&#39;s public key is combined with the verification of the signature itself. When compared to traditional, explicit certificates, the main advantage of the implicit approach lies in the shorter public key validation data. This property is particularly important in resource-constrained scenarios where public key validation is performed very often, which is common in vehicular communications (V2X) that employ pseudonym certificates. In this article, we show an alternative Schnorr-based implicit certification procedure that can improve the efficiency of a popular V2X-oriented Vehicular Public Key Infrastructure (VPKI), the Security Credential Management System (SCMS). As an additional contribution, we show that SCMS&#39;s underlying certificate provisioning procedure, based on butterfly keys, is vulnerable to existential forgery attacks under certain conditions. We then discuss how this issue can be fixed in an effective and efficient manner.},
  archive      = {J_TC},
  author       = {Paulo S. L. M. Barreto and Marcos A. Simplicio and Jefferson E. Ricardini and Harsh Kupwade Patil},
  doi          = {10.1109/TC.2020.2988637},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {393-399},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Schnorr-based implicit certification: Improving the security and efficiency of vehicular communications},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Area-optimized accurate and approximate softcore signed
multiplier architectures. <em>TC</em>, <em>70</em>(3), 384–392. (<a
href="https://doi.org/10.1109/TC.2020.2988404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplication is one of the most extensively used arithmetic operations in a wide range of applications. In order to provide resource-efficient and high-performance multipliers, previous works have proposed different designs of accurate and approximate multipliers-mainly for ASIC-based systems. However, the architectural differences between ASICs- and FPGA-based systems limit the effectiveness of these multipliers for FPGA-based systems. Moreover, most of these multiplier designs are valid only for unsigned numbers. To bridge this gap, we propose a novel implementation technique for designing resource-efficient and low-power accurate and approximate signed multipliers which are optimized for FPGA-based systems. Compared to Vivado&#39;s area-optimized multiplier IPs, the designs obtained using our proposed technique occupy 47 to 63 percent less area (Lookup Tables). To accelerate further research in this direction and reproduce the presented results, the RTL and behavioral models of our proposed methodology are available as an open-source library. 1 1.Online. [Available]: https://cfaed.tu-dresden.de/pd-downloads.},
  archive      = {J_TC},
  author       = {Salim Ullah and Hendrik Schmidl and Siva Satyendra Sahoo and Semeen Rehman and Akash Kumar},
  doi          = {10.1109/TC.2020.2988404},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {384-392},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Area-optimized accurate and approximate softcore signed multiplier architectures},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time schedulability analysis and enhancement of
transiently powered processors with NVMs. <em>TC</em>, <em>70</em>(3),
372–383. (<a href="https://doi.org/10.1109/TC.2020.2988282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent Internet-of-Things or Wireless Sensor Network devices are often operated with energy harvesters. As there are no energy storages in those devices, power is not consistently provided to the devices at all times. In such transiently powered systems, in order to keep the system reliable without losing any execution contexts, non-volatile memories (NVMs) are typically used for swift backup/restoration of execution contexts. In this article, we perform a real-time schedulability analysis of the transiently powered processors with NVMs. We first quantitatively characterize the charging and discharging behaviors of the energy harvester and extract the compute capability of the system in time interval domain. Then, based on Real-Time Calculus, we determine whether the given multi-task workload is schedulable or not with respect to the earliest deadline first (EDF) or fixed-priority (FP) scheduling policies. In addition, we study how the choice of the threshold voltage parameter affects the schedulability, then propose a feasible threshold selection algorithm to enhance schedulability. We verify the effectiveness of the proposed technique with extensive simulations. Compared to the naive selection method, the proposed technique always shows improvements in schedulability in various workloads.},
  archive      = {J_TC},
  author       = {Dasom Lee and Hyeonseok Jung and Hoeseok Yang},
  doi          = {10.1109/TC.2020.2988282},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {372-383},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time schedulability analysis and enhancement of transiently powered processors with NVMs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and predictable non-volatile data memory for real-time
embedded systems. <em>TC</em>, <em>70</em>(3), 359–371. (<a
href="https://doi.org/10.1109/TC.2020.2988261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption and predictability are two important constraints in designing real-time embedded systems and one of the recently proposed solutions for the energy consumption problem is the use of non-volatile memories instead of conventional SRAM due to their lower leakage power consumption and smaller cell area. Furthermore, because of their non-volatile nature, the use of these memories helps normally-off computing and energy harvesting systems to resume their execution without a large startup delay. However, the write access latency of non-volatile memories is considerably more than that of SRAM which can decrease the performance and predictability of the system if not managed correctly. In this article, we present a predictable fully non-volatile data memory for real-time embedded systems which improves both worst-case execution time (WCET) and performance of the system using a hybrid hardware-software solution. As part of this solution, we add a special write buffer to the memory controller and adopt a multi-bank memory configuration which improves the overall latency of write operations. Since write buffers usually help with the performance problem but they make WCET estimation more complex, we also present a new low-overhead software-based optimization technique that makes the proposed system more predictable without imposing considerable overhead. Furthermore, we present the WCET analysis algorithm which can be used to estimate the WCET of applications during the design time. The results show that compared to a hybrid SRAM-NVM architecture, the proposed solution improves the WCET and performance by 33 and 47 percent, respectively.},
  archive      = {J_TC},
  author       = {Mostafa Bazzaz and Ali Hoseinghorban and Alireza Ejlali},
  doi          = {10.1109/TC.2020.2988261},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {359-371},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast and predictable non-volatile data memory for real-time embedded systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TrackLace: Data management for interlaced magnetic
recording. <em>TC</em>, <em>70</em>(3), 347–358. (<a
href="https://doi.org/10.1109/TC.2020.2988257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interlaced Magnetic Recording (IMR) is a promising technology which achieves higher data density and lower write amplification (WA) than Shingled Magnetic Recording (SMR). In IMR, top tracks and bottom tracks are interlaced so each bottom track is partially overlapped with two adjacent top tracks. Top tracks can be updated without any WA, but bottom track updates require reading and rewriting of affected valid data on the two neighboring top tracks. There are few published studies discussing WA in IMR drives. We propose TrackLace to reduce WA for IMR. TrackLace consists of three techniques: Z-Alloc allocates user data to the tracks in alternating directions and spreads unallocated tracks among allocated tracks; Top-Buffer opportunistically utilizes unallocated top tracks to buffer bottom track updates; and Block-Swap progressively swaps bottom track hot data with top track cold data during high space utilization. To further optimize TrackLace performance, we propose a virtual frame design that can keep the relocated block (due to Top-Buffer or Block-Swap) close to its original location and an adaptive buffering mechanism that can avoid unnecessary redirections depending on the write locality. Evaluations show that TrackLace can reduce WA by 45 percent and lower average latency by 31percent compared with baseline schemes.},
  archive      = {J_TC},
  author       = {Fenggang Wu and Bingzhe Li and Baoquan Zhang and Zhichao Cao and Jim Diehl and Hao Wen and David H.C. Du},
  doi          = {10.1109/TC.2020.2988257},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {347-358},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TrackLace: Data management for interlaced magnetic recording},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Idempotence-based preemptive GPU kernel scheduling for
embedded systems. <em>TC</em>, <em>70</em>(3), 332–346. (<a
href="https://doi.org/10.1109/TC.2020.2988251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mission-critical embedded systems simultaneously run multiple graphics-processing-unit (GPU) computing tasks with different criticality and timeliness requirements. Considerable research effort has been dedicated to supporting the preemptive priority scheduling of GPU kernels. However, hardware-supported preemption leads to lengthy scheduling delays and complicated designs, and most software approaches depend on the voluntary yielding of GPU resources from restructured kernels. We propose a preemptive GPU kernel scheduling scheme that harnesses the idempotence property of kernels. The proposed scheme distinguishes idempotent kernels through static source code analysis. If a kernel is not idempotent, then GPU kernels are transactionized at the operating system (OS) level. Both idempotent and transactionized kernels can be aborted at any point during their execution and rolled back to their initial state for reexecution. Therefore, low-priority kernel instances can be preempted for high-priority kernel instances and reexecuted after the GPU becomes available again. Our evaluation using the Rodinia benchmark suite showed that the proposed approach limits the preemption delay to 18 ms in the 99.9th percentile, with an average delay in execution time of less than 10 percent for high-priority tasks under a heavy load in most cases.},
  archive      = {J_TC},
  author       = {Hyeonsu Lee and Hyunjun Kim and Cheolgi Kim and Hwansoo Han and Euiseong Seo},
  doi          = {10.1109/TC.2020.2988251},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {332-346},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Idempotence-based preemptive GPU kernel scheduling for embedded systems},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A reduced architecture for ReRAM-based neural network
accelerator and its software stack. <em>TC</em>, <em>70</em>(3),
316–331. (<a href="https://doi.org/10.1109/TC.2020.2988248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network (NN) accelerators based on resistive random access memory (ReRAM) have been widely investigated as a promising solution to address the memory wall challenge, due to its capability of processing-in-memory with extremely high density. However, the performance of these accelerators is bounded by the peripheral circuits and the interconnection. And they also suffer from accuracy issue and flexibility issue caused by the device-variation and the in-situ computation mode respectively. Solving these issues with hardware will further offset the performance. Enlightened by the design principle of conventional reduced instruction set computer (RISC), this article proposes a new software/hardware system for ReRAM-based NN acceleration, which achieves complex functions with sophisticated software tools while making the hardware much more compact and efficient, in order to fully utilize ReRAM potential. The hardware architecture, Field Programmable Synapse Array (FPSA), provides high-density reconfigurable logic, wires, and computational resources based on ReRAM-crossbar. Accordingly, the software can convert various types of NNs, including dynamic NNs, into equivalent networks that meet hardware constraints with negligible accuracy loss and optimize the scheduling and mapping of the latter onto FPSA. Further evaluations show that compared to one state-of-the-art ReRAM-based NN accelerator, PRIME, our approach achieves up to 1000× speedup.},
  archive      = {J_TC},
  author       = {Yu Ji and Zixin Liu and Youhui Zhang},
  doi          = {10.1109/TC.2020.2988248},
  journal      = {IEEE Transactions on Computers},
  number       = {3},
  pages        = {316-331},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reduced architecture for ReRAM-based neural network accelerator and its software stack},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modularized morphing of deep convolutional neural networks:
A graph approach. <em>TC</em>, <em>70</em>(2), 305–315. (<a
href="https://doi.org/10.1109/TC.2020.2988006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network morphism is an effective learning scheme to morph a well-trained neural network to a new one with the network function completely preserved. However, existing network morphism scheme addresses only basic morphing types on the layer level. In this research, we address the central problem of network morphism at a higher level, i.e., how a convolutional layer can be morphed into an arbitrary module of a neural network. To simplify the representation of a network, we abstract a module as a graph with blobs as vertices and convolutional layers as edges. Based on this graph, the morphing process can be formulated as a graph transformation problem. Two atomic morphing operations are introduced to construct the graphs, based on which modules are classified into two families, i.e., simple morphable modules and complex modules. We present practical morphing solutions for both families, and prove that any module can be morphed from a single convolutional layer. Extensive experiments have been conducted based on the state-of-the-art ResNet on benchmarks to verify the effectiveness of the proposed solution.},
  archive      = {J_TC},
  author       = {Tao Wei and Changhu Wang and Chang Wen Chen},
  doi          = {10.1109/TC.2020.2988006},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {305-315},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Modularized morphing of deep convolutional neural networks: A graph approach},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tiler: An autonomous region-based scheme for SMR storage.
<em>TC</em>, <em>70</em>(2), 291–304. (<a
href="https://doi.org/10.1109/TC.2020.2988004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shingled Magnetic Recording (SMR) Disks are adopted as a high-density, non-volatile media that significantly precedes conventional disks in both the storage capacity and cost. However, inefficient read-modify-writes (RMWs) greatly challenge the management of SMR disks. This article for the first time presents an approach called Tiler to manage SMR disks by dividing the physical space into small autonomous regions (ARs). Each AR can manage its space allocation, address mapping, and cleaning independently. By managing these ARs in a log-structured way, RMWs can be avoided; besides, ARs can also help update data when the adjacent tracks contain no valid data. Tiler is capable of partitioning a large-scale cleaning into self-contained-small-scale cleaning and thus, the data that need to be relocated are limited inside independent ARs, which further minimizes the performance overhead. Our experimental results show that Tiler can shorten the overall system response time by 50.21 percent and reduce the cleaning time by 90.24 percent on average.},
  archive      = {J_TC},
  author       = {Chenlin Ma and Zhaoyan Shen and Jihe Wang and Yi Wang and Renhai Chen and Yong Guan and Zili Shao},
  doi          = {10.1109/TC.2020.2988004},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {291-304},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tiler: An autonomous region-based scheme for SMR storage},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft error tolerant count min sketches. <em>TC</em>,
<em>70</em>(2), 284–290. (<a
href="https://doi.org/10.1109/TC.2020.2987890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the frequency of the elements on a set is needed in a wide range of computing applications. For example, to estimate the number of hits that a video gets or the number of packets in a network flow. In some cases, the number of elements in the set is very large and it is not practical to maintain a table with the exact count for each of them. Instead, simpler and more efficient data structures, commonly referred to as sketches, that provide an estimate are used. Among those structures the Count Min Sketch (CMS) is one of the most popular sketches. The CMS provides estimates that have one sided errors. In more detail, the CMS returns an estimate that is equal to or larger than the actual value. An update or check requires a small and constant number of memory accesses and the memory footprint is fixed and does not depend on the number of elements. The CMS relies on several arrays of counters that are stored in memory. Memories are prone to suffer soft errors that flip the contents of memory cells due for example to ionizing radiation. Therefore, it is of interest to study the impact that soft errors can have on the CMS estimates and to propose protection techniques that minimize their effect while requiring low overhead in terms of additional memory and circuitry. To the best of our knowledge this has not been done before. In this article, first the effect of soft errors on the CMS is evaluated by injecting errors. Then, in the second part a protection technique that does not require additional memory bits is presented and compared with the protection using a parity bit. In the last part of the article, the technique is extended to protect also against double adjacent bit errors.},
  archive      = {J_TC},
  author       = {Pedro Reviriego and Jorge Martínez and Marco Ottavi},
  doi          = {10.1109/TC.2020.2987890},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {284-290},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Soft error tolerant count min sketches},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enforcing predictability of many-cores with DCFNoC.
<em>TC</em>, <em>70</em>(2), 270–283. (<a
href="https://doi.org/10.1109/TC.2020.2987797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever need for higher performance forces industry to include technology based on multi-processors system on chip (MPSoCs) in their safety-critical embedded systems. MPSoCs include a network-on-chip (NoC) to interconnect the cores between them and with memory and the rest of shared resources. Unfortunately, the inclusion of NoCs compromises guaranteeing time predictability as network-level conflicts may occur. To overcome this problem, in this article we propose DCFNoC, a new time-predictable NoC design paradigm where conflicts within the network are eliminated by design. This new paradigm builds on top of the Channel Dependency Graph (CDG) in order to deterministically avoid network conflicts. The network guarantees predictability to applications and is able to naturally inject messages using a TDM period equal to the optimal theoretical bound without the need of using a computationally demanding offline process. DCFNoC is integrated in a tile-based many-core system and adapted to its memory hierarchy. Our results show that DCFNoC guarantees time predictability avoiding network interference among multiple running applications. DCFNoC always guarantees performance and also improves wormhole performance in a 4 x 4 setting by a factor of 3.7x when interference traffic is injected. For a 8 x 8 network differences are even larger. In addition, DCFNoC obtains a total area saving of 10.79 percent over a standard wormhole implementation.},
  archive      = {J_TC},
  author       = {Tomás Picornell and José Flich and Carles Hernández and José Duato},
  doi          = {10.1109/TC.2020.2987797},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {270-283},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enforcing predictability of many-cores with DCFNoC},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting buffered updates for fast streaming graph
analysis. <em>TC</em>, <em>70</em>(2), 255–269. (<a
href="https://doi.org/10.1109/TC.2020.2987571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming graph analysis extracts timely insights from evolving graphs, and has gained increasing popularity. In current practice of streaming graph analysis, incoming updates are simply cached in a buffer, until being applied onto existing graph structure to construct a new snapshot. Graph algorithms then work on the new snapshot to produce up-to-date analysis result. Nevertheless, we find that for widely used monotonic graph algorithms, the analysis process can be accelerated by preprocessing buffered updates. To this end, we propose GraPU, a streaming graph analytics system for monotonic graph algorithms. Before applying updates, GraPU preprocesses buffered updates in three consecutive stages: 1) Components-based Classification first identifies the effective graph data that are actually affected by current updates, by classifying the vertices involved in buffered updates according to the predetermined connected components in underlying graph; 2) In-buffer Precomputation generates the safe and profitable intermediate values that can be later merged onto underlying graph to facilitate convergence on new snapshots, by precomputing the values of vertices involved in buffered updates; 3) Hub-vertices Division eliminates the vertex-level load imbalance for analysis on new snapshots, by automatically identifying the high-degree vertices involved in updates and efficiently distributing their high-cost computation over multiple machines. After buffered updates are applied, GraPU calculates vertex values in new snapshots using the subgraph-centric model. GraPU further presents Load-factors Guided Balancing to achieve load balance at subgraph-level, by reassigning some vertices and edges among subgraphs beforehand. Our experimental result shows that, GraPU outperforms state-of-the-art KineoGraph by up to 20.43x.},
  archive      = {J_TC},
  author       = {Feng Sheng and Qiang Cao and Jie Yao},
  doi          = {10.1109/TC.2020.2987571},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {255-269},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploiting buffered updates for fast streaming graph analysis},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A case for application-managed flash. <em>TC</em>,
<em>70</em>(2), 240–254. (<a
href="https://doi.org/10.1109/TC.2020.2987569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new I/O architecture for NAND flash-based SSDs, called application-managed flash (AMF) and present two case studies to show its usefulness. In a typical SSD controller, an intermediate software layer, called the flash translation layer (FTL), is employed between NAND flash chips and a host interface. The main responsibility of an FTL is to provide interoperability with conventional HDDs, but this interoperability comes at the cost of extra hardware resources and degraded I/O performance. The proposed AMF refactors the flash storage architecture so that an SSD controller exposes append-only segments, which do not permit overwriting. This refactoring dramatically improves performance of applications and reduces hardware costs by allowing applications to directly manage flash storage with minimal supports from the SSD controller. In order to understand the benefits of AMF, we study two popular applications: a log-structured file system (F2FS) and a key-value store (RocksDB). Our experiments show that the DRAM in the flash controller is reduced by 128X and the performances of the file system and the key-value store improve by 80 and 54 percent, respectively, over conventional SSDs.},
  archive      = {J_TC},
  author       = {Jinhyung Koo and Chanwoo Chung and Arvind and Sungjin Lee},
  doi          = {10.1109/TC.2020.2987569},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {240-254},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A case for application-managed flash},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A3C-DO: A regional resource scheduling framework based on
deep reinforcement learning in edge scenario. <em>TC</em>,
<em>70</em>(2), 228–239. (<a
href="https://doi.org/10.1109/TC.2020.2987567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, huge amounts of data are produced by edge device. Considering the heavy burden of network bandwidth and the service delay requirements of delay-sensitive applications, processing the data at network edge is a great choice. However, edge devices such as smart wearables, connected and autonomous vehicles usually have several limitations on computational capacity and energy which will influence the quality of service. As an effective and efficient strategy, offloading is widely used to address this issue. But when facing device heterogeneity problem and task complexity increase, service quality degradation and resource utility decrease often occur due to unreasonable task distribution. Since conventional simplex offloading strategies show limited performance in complex environment, we are motivated to design a dynamic regional resource scheduling framework which is able to work effectively taking different indexes into consideration. Thus, in this article we first propose a double offloading framework to simulate the offloading process in real edge scenario which consists of different edge servers and devices. Then we formulate the offloading as a Markov Decision Process (MDP) and utilize a deep reinforcement learning (DRL) algorithm named asynchronous advantage actor-critic (A3C) as the offloading decision making strategy to balance the workload of edge servers and finally reduce the overhead in terms of energy and time. Comparison experiments for local computing and wide-used DRL algorithm DQN are conducted in a comprehensive benchmark and the results show that our work performs much better on self-adjusting and overhead reduction.},
  archive      = {J_TC},
  author       = {Junfeng Zou and Tongbo Hao and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2020.2987567},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {228-239},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A3C-DO: A regional resource scheduling framework based on deep reinforcement learning in edge scenario},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stream semantic registers: A lightweight RISC-v ISA
extension achieving full compute utilization in single-issue cores.
<em>TC</em>, <em>70</em>(2), 212–227. (<a
href="https://doi.org/10.1109/TC.2020.2987314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-issue processor cores are very energy efficient but suffer from the von Neumann bottleneck, in that they must explicitly fetch and issue the loads/storse necessary to feed their ALU/FPU. Each instruction spent on moving data is a cycle not spent on computation, limiting ALU/FPU utilization to 33 percent on reductions. We propose “Stream Semantic Registers” to boost utilization and increase energy efficiency. SSR is a lightweight, non-invasive RISC-V ISA extension which implicitly encodes memory accesses as register reads/writes, eliminating a large number of loads/stores. We implement the proposed extension in the RTL of an existing multi-core cluster and synthesize the design for a modern 22 nm technology. Our extension provides a significant, 2x to 5x, architectural speedup across different kernels at a small 11 percent increase in core area. Sequential code runs 3x faster on a single core, and 3x fewer cores are needed in a cluster to achieve the same performance. The utilization increase to almost 100 percent in leads to a 2x energy efficiency improvement in a multi-core cluster. The extension reduces instruction fetches by up to 3.5x and instruction cache power consumption by up to 5.6x. Compilers can automatically map loop nests to SSRs, making the changes transparent to the programmer.},
  archive      = {J_TC},
  author       = {Fabian Schuiki and Florian Zaruba and Torsten Hoefler and Luca Benini},
  doi          = {10.1109/TC.2020.2987314},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {212-227},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stream semantic registers: A lightweight RISC-V ISA extension achieving full compute utilization in single-issue cores},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the analysis of parallel real-time tasks with spin locks.
<em>TC</em>, <em>70</em>(2), 199–211. (<a
href="https://doi.org/10.1109/TC.2020.2987300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locking protocol is an essential component in resource management of real-time systems, which coordinates mutually exclusive accesses to shared resources from different tasks. Although the design and analysis of locking protocols have been intensively studied for sequential real-time tasks, there has been a little work on this topic for parallel real-time tasks. In this article, we study the analysis of parallel real-time tasks using spin locks to protect accesses to shared resources in three commonly used request serving orders (unordered, FIFO-order, and priority-order). A remarkable feature making our analysis method more accurate is to systematically analyze the blocking time which may delay a task&#39;s finishing time, where the impact to the total workload and the longest path length is jointly considered, rather than analyzing them separately and counting all blocking time as the workload that delays a task&#39;s finishing time, as commonly assumed in the state-of-the-art.},
  archive      = {J_TC},
  author       = {Xu Jiang and Nan Guan and He Du and Weichen Liu and Wang Yi},
  doi          = {10.1109/TC.2020.2987300},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {199-211},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On the analysis of parallel real-time tasks with spin locks},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the health degree of hard disk drives with
asymmetric and ordinal deep neural models. <em>TC</em>, <em>70</em>(2),
188–198. (<a href="https://doi.org/10.1109/TC.2020.2987018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting failures in Hard Disk Drives (HDD) is a major challenge that has been faced by both industry and academy in recent years. Being able to predict failure events may incur in avoiding data losses and also improve service availability. Among all failure prediction strategies, the health degree prediction is one of the most popular. The task of health degree prediction consists of, given a finite set of health states that are related to the degradation of the equipment, estimate which state reflects the actual degradation of the equipment. This problem is usually modeled as a classification task. Although many health degree prediction methods have been proposed, some practical details regarding this prediction task have been neglected in previous works. In this work we tackle two of these aspects: the ordinal nature of the problem and the different costs associated with miss-classifications. The problem can be considered as ordinal since classifying a HDD in a health level that is far from is true health level shall be more penalized than classifying in a near health level, thus a classical classification framework is not recommended. The different costs associated with mis-classifications are related to the fact that early predictions are preferred than late prediction since the later can result in failures. Such aspects are considered in a framework based on Deep Recurrent Neural Networks (DRNN). The choice of DRNN is given its remarkable performances in many applications including HDDs failure prediction. The resulting methods outperformed state-of-the-art approaches in a metric that consider the new aspects that motivated our proposal.},
  archive      = {J_TC},
  author       = {Fernando D. S. Lima and Francisco Lucas F. Pereira and Iago C. Chaves and Javam C. Machado and Joao Paulo P. Gomes},
  doi          = {10.1109/TC.2020.2987018},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {188-198},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Predicting the health degree of hard disk drives with asymmetric and ordinal deep neural models},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluations on deep neural networks training using posit
number system. <em>TC</em>, <em>70</em>(2), 174–187. (<a
href="https://doi.org/10.1109/TC.2020.2985971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of Deep Neural Networks (DNNs) brings enormous memory requirements and computational complexity, which makes it a challenge to train DNN models on resource-constrained devices. Training DNNs with reduced-precision data representation is crucial to mitigate this problem. In this article, we conduct a thorough investigation on training DNNs with low-bit posit numbers, a Type-III universal number (Unum). Through a comprehensive analysis of quantization with various data formats, it is demonstrated that the posit format shows great potential to be employed in the training of DNNs. Moreover, a DNN training framework using 8-bit posit is proposed with a novel tensor-wise scaling scheme. The experiments show the same performance as the state-of-the-art (SOTA) across multiple datasets (MNIST, CIFAR-10, ImageNet, and Penn Treebank) and model architectures (LeNet-5, AlexNet, ResNet, MobileNet-V2, and LSTM). We further design an energy-efficient hardware prototype for our framework. Compared to the standard floating-point counterpart, our design achieves a reduction of 68, 51, and 75 percent in terms of area, power, and memory capacity, respectively.},
  archive      = {J_TC},
  author       = {Jinming Lu and Chao Fang and Mingyang Xu and Jun Lin and Zhongfeng Wang},
  doi          = {10.1109/TC.2020.2985971},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {174-187},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Evaluations on deep neural networks training using posit number system},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PermCNN: Energy-efficient convolutional neural network
hardware architecture with permuted diagonal structure. <em>TC</em>,
<em>70</em>(2), 163–173. (<a
href="https://doi.org/10.1109/TC.2020.2981068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the emerging artificial intelligence (AI) era, efficient hardware accelerator design for deep neural networks (DNNs) is very important to enable real-time energy-efficient DNN model deployment. To this end, various DNN model compression approaches and the corresponding hardware architectures have been intensively investigated. Recently, PermDNN, as a permuted diagonal structure-imposing model compression approach, was proposed with promising classification performance and hardware performance. However, the existing PermDNN hardware architecture is specifically designed for fully-connected (FC) layer-contained DNN models; while its support for convolutional (CONV) layer is missing. To fill this gap, this article proposes PermCNN, an energy-efficient hardware architecture for permuted diagonal structured convolutional neural networks (CNNs). By fully utilizing the strong structured sparsity in the trained models as well as dedicatedly leveraging the dynamic activation sparsity, PermCNN delivers very high hardware performance for inference tasks on CNN models. A design example with 28 nm CMOS technology shows that, compared the to state-of-the-art CNN accelerator, PermCNN achieves 3.74× and 3.11× improvement on area and energy efficiency, respectively, on AlexNet workload, and 17.49× and 14.22× improvement on area and energy efficiency, respectively, on VGG model. After including energy consumption incurred by DRAM access, PermCNN achieves 2.60× and 9.62× overall energy consumption improvement on AlexNet and VGG workloads, respectively.},
  archive      = {J_TC},
  author       = {Chunhua Deng and Siyu Liao and Bo Yuan},
  doi          = {10.1109/TC.2020.2981068},
  journal      = {IEEE Transactions on Computers},
  number       = {2},
  pages        = {163-173},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PermCNN: Energy-efficient convolutional neural network hardware architecture with permuted diagonal structure},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LFSR-based bit-serial <span
class="math inline"><em>G</em><em>F</em>(2<sup><em>m</em></sup>)</span>
g f ( 2 m ) multipliers using irreducible trinomials. <em>TC</em>,
<em>70</em>(1), 156–162. (<a
href="https://doi.org/10.1109/TC.2020.2980259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new architecture of bit-serial polynomial basis (PB) multipliers over the binary extension field GF(2 m ) generated by irreducible trinomials is presented. Bit-serial GF(2 m ) PB multiplication offers a performance/ area trade-off that is very useful in resource constrained applications. The architecture here proposed is based on LFSR (Linear-Feedback Shift Register) and can perform a multiplication in m clock cycles with a constant propagation delay of TA þ TX. These values match the best time results found in the literature for bit-serial PB multipliers with a slight reduction of the space complexity. Furthermore, the proposed architecture can perform the multiplication of two operands fort different finite fields GF(2 m ) generated by t irreducible trinomials simultaneously in m clock cycles with the inclusion of t(m - 1Þ flipflops and tm XOR gates.},
  archive      = {J_TC},
  author       = {José L. Imaña},
  doi          = {10.1109/TC.2020.2980259},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {156-162},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LFSR-based bit-serial $GF(2^m)$ g f ( 2 m ) multipliers using irreducible trinomials},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Falcon: Addressing stragglers in heterogeneous parameter
server via multiple parallelism. <em>TC</em>, <em>70</em>(1), 139–155.
(<a href="https://doi.org/10.1109/TC.2020.2974461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parameter server architecture has shown promising performance advantages when handling deep learning (DL) applications. One crucial issue in this regard is the presence of stragglers, which significantly retards DL training progress. Previous solutions for solving stragglers may not fully exploit the computation resource of the cluster as evidenced by our experiments, especially in the heterogeneous environment. This motivates us to design a heterogeneity-aware parameter server paradigm that addresses stragglers and accelerates DL training from the perspective of computation parallelism. We introduce a novel methodology named straggler projection to give a comprehensive inspection of stragglers and reveal practical guidelines to solve this problem in two aspects: (1) controlling each worker&#39;s training speed via elastic training parallelism control and (2) transferring blocked tasks from stragglers to pioneers to fully utilize the computation resource. Following these guidelines, we propose the abstraction of parallelism as an infrastructure and design the Elastic-Parallelism Synchronous Parallel (EPSP) algorithm to handle distributed training and parameter synchronization, supporting both enforcedand slack-synchronization schemes. The whole idea has been implemented into a prototype called Falcon which effectively accelerates the DL training speed with the presence of stragglers. Evaluation under various benchmarks with baseline comparison demonstrates the superiority of our system. Specifically, Falcon reduces the training convergence time, by up to 61.83, 55.19, 38.92, and 23.68 percent shorter than FlexRR, Sync-opt, ConSGD, and DynSGD, respectively.},
  archive      = {J_TC},
  author       = {Qihua Zhou and Song Guo and Haodong Lu and Li Li and Minyi Guo and Yanfei Sun and Kun Wang},
  doi          = {10.1109/TC.2020.2974461},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {139-155},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Falcon: Addressing stragglers in heterogeneous parameter server via multiple parallelism},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Karnaugh map method for memristive and spintronic asymmetric
basis logic functions. <em>TC</em>, <em>70</em>(1), 128–138. (<a
href="https://doi.org/10.1109/TC.2020.2986970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of beyond-CMOS technologies with alternative basis logic functions necessitates the introduction of novel design automation techniques. In particular, recently proposed computing systems based on memristors and bilayer avalanche spin-diodes both provide asymmetric functions as basis logic gates - the implication and inverted-input AND, respectively. This article therefore proposes a method by which Karnaugh maps can be directly applied to systems with asymmetric basis logic functions. A set of identities is defined for these memristor and spintronic logic functions, enabling the formal demonstration of the Karnaugh map method and an explanation of the proposed technique. This method thus, enables the direct minimization of spintronic and memristive logic circuits without translation to conventional Boolean algebra, facilitating the further development of these novel computing paradigms. Preliminary analyses demonstrate that this Karnaugh map minimization approach can provide a 28 percent reduction in step count as compared to previous manual optimization.},
  archive      = {J_TC},
  author       = {Vaibhav Vyas and Lucian Jiang-Wei and Peng Zhou and Xuan Hu and Joseph S. Friedman},
  doi          = {10.1109/TC.2020.2986970},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {128-138},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Karnaugh map method for memristive and spintronic asymmetric basis logic functions},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3-d partitioning for large-scale graph processing.
<em>TC</em>, <em>70</em>(1), 111–127. (<a
href="https://doi.org/10.1109/TC.2020.2986736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disk I/O is the major performance bottleneck of existing out-of-core graph processing systems. We found that the total I/O amount can be reduced by loading more vertices into memory every time. Although task partitioning of a graph processing system is traditionally considered equivalent to the graph partition problem, this assumption is untrue for many Machine Learning and Data Mining (MLDM) problems: instead of a single value, a vector of data elements is defined as the property for each vertex/edge. By dividing each vertex into multiple sub-vertices, more vertices can be loaded into memory every time, leading to less amount of disk I/O. To explore this new opportunity, we propose a category of 3-D partitioning algorithm that considers the hidden dimension to partition the property vector. The 3-D partitioning algorithm provides a new tradeoff to reduce communication costs, which is adaptive to both distributed and out-of-core scenarios. Based on it, we build a distributed graph processing system CUBE and an out-of-core system SINGLECUBE. Since network traffic is significantly reduced, CUBE outperforms state-of-the-art graph-parallel system PowerLyra by up to 4.7X. By largely reducing the disk I/O amount, the performance of SINGLECUBE is significantly better than state-of-the-art out-of-core system GridGraph (up to 4.5X).},
  archive      = {J_TC},
  author       = {Xue Li and Mingxing Zhang and Kang Chen and Yongwei Wu and Xuehai Qian and Weimin Zheng},
  doi          = {10.1109/TC.2020.2986736},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {111-127},
  shortjournal = {IEEE Trans. Comput.},
  title        = {3-D partitioning for large-scale graph processing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Read-ahead efficiency on mobile devices: Observation,
characterization, and optimization. <em>TC</em>, <em>70</em>(1), 99–110.
(<a href="https://doi.org/10.1109/TC.2020.2984755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Read-ahead schemes have been widely used in page cache to improve read performance of Linux systems. As the Android system inherits the Linux kernel, the traditional read-ahead scheme is directly transplanted to mobile devices. However, request sizes and page cache sizes on mobile devices are much smaller, which may degrade read-ahead efficiency and therefore hurt user experience. This article first observes that many pages pre-fetched by read-ahead are unused, which causes frequent page cache eviction. And these evict operations could induce extra access latency, especially when write-back is conducting. Then, this article proposes a new analysis model to characterize the factors that closely relate to the access latency. It is found that there exists a trade-off between read-ahead size and access latency. Finally, this article proposes two optimized read-ahead schemes to exploit this trade-off under different situations. Size-tuning scheme aims to find the proper maximum size of read-ahead according to the characteristics of mobile devices. While MobiRA scheme improves the read-ahead efficiency by dynamically tuning read-ahead size and stop-settings. Experimental results on real mobile devices show that the proposed schemes can increase the efficiency of read-ahead scheme and improve the overall performance of mobile devices.},
  archive      = {J_TC},
  author       = {Yu Liang and Riwei Pan and Yajuan Du and Chenchen Fu and Liang Shi and Tei-Wei Kuo and Chun Jason Xue},
  doi          = {10.1109/TC.2020.2984755},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {99-110},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Read-ahead efficiency on mobile devices: Observation, characterization, and optimization},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-target adaptive reconfigurable acceleration for
low-power IoT processing. <em>TC</em>, <em>70</em>(1), 83–98. (<a
href="https://doi.org/10.1109/TC.2020.2984736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-power processors for the Internet-of-Things (IoT) demand a high degree of adaptability to efficiently execute applications with different resource requirements under varying scenarios. Current single-ISA heterogeneous Chip Multiprocessors (CMPs), such as ARM&#39;s big.LITTLE, provide multiple cores and voltage/frequency levels to address this challenge. However, finding the best possible type of core and the corresponding voltage/frequency level for all the execution scenarios, which involve different applications and phases, remains far from being reached. In this article, we propose extending such a single-ISA heterogeneous CMP with a Coarse-Grained Reconfigurable Array (CGRA) and a hardware-based dynamic binary translation (DBT) module that transparently maps application code onto the CGRA for acceleration. To achieve low-energy levels and efficiently manage the power consumption of the CGRA, we introduce an additional voltage rail that enables operation in the Near-Threshold Voltage (NTV) regime when needed, leveraging key features of the CGRA&#39;s structure to address the implementation challenges of NTV computing. For less than 35 percent area overhead to the baseline CMP, performance and energy consumption are improved as follows. Compared to: (a) power-efficient execution in the LITTLE core, MuTARe achieves 29 percent reduction in energy consumption, and $2\times$ speedup; (b) performance-efficient execution in the big core, a speedup of $1.6\times$ with an energy reduction of 41 percent is achieved.},
  archive      = {J_TC},
  author       = {Marcelo Brandalero and Luigi Carro and Antonio Carlos Schneider Beck and Muhammad Shafique},
  doi          = {10.1109/TC.2020.2984736},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {83-98},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-target adaptive reconfigurable acceleration for low-power IoT processing},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fairness-aware energy efficient scheduling on heterogeneous
multi-core processors. <em>TC</em>, <em>70</em>(1), 72–82. (<a
href="https://doi.org/10.1109/TC.2020.2984607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multi-core processors (HMP) with the same instruction set architecture (ISA) integrate complex high performance big cores with power efficient small cores on the same chip. In comparison with homogeneous architectures, HMPs have been shown to significantly increase energy efficiency. However, current techniques to exploit the energy efficiency of HMPs do not consider fair usage of resources that leads to reduced performance predictability, a longer makespan, starvation, and QoS degradation. The effect of different cluster voltage and frequency levels on fairness is another issue neglected by previous task scheduling algorithms. The present study investigates both the fairness problem and energy efficiency in HMPs. This article proposes a heterogeneous fairness-aware energy efficient framework (HFEE) that employs DVFS to meet fairness constraints and provide energy efficient scheduling. The proposed framework is implemented and evaluated on a real heterogeneous multi-core processor. The experimental results indicate that the introduced technique can significantly improve energy efficiency and fairness when compared to Linux standard scheduler and two energy efficient and fairness-aware schedulers.},
  archive      = {J_TC},
  author       = {Bagher Salami and Hamid Noori and Mahmoud Naghibzadeh},
  doi          = {10.1109/TC.2020.2984607},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {72-82},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fairness-aware energy efficient scheduling on heterogeneous multi-core processors},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Algorithms for computing the WCRT bound of OpenMP task
systems with conditional branches. <em>TC</em>, <em>70</em>(1), 57–71.
(<a href="https://doi.org/10.1109/TC.2020.2984502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-cores are becoming mainstream hardware platforms for embedded and real-time systems. To fully utilize the processing capacity of multi-cores, software should be parallelized. Recently, much work has been done on real-time scheduling of parallel tasks modeled as directed acyclic graphs (DAG), motivated by the parallel task structures supported by popular parallel programming frameworks such as OpenMP. The DAG-based task models in existing real-time scheduling research assume well-nested graph structures recursively composed by single-source-single-sink parallel and conditional components. However, realistic OpenMP task systems in general have more flexible structures that do not comply with those assumptions. In this article, we model the behavior of general OpenMP task systems with non-well-nested structures. The worst-case response time analysis problem for such systems is more difficult due to the flexible graph structure. As the major technical contribution, we develop two efficient algorithms to compute the worst-case response time bounds, with different trade-offs between efficiency and precision. Evaluation with both randomly generated task graphs and realistic OpenMP programs shows good performance of our approaches in terms of both precision and efficiency.},
  archive      = {J_TC},
  author       = {Jinghao Sun and Nan Guan and Jingchang Sun and Xi Zhang and Yaoyao Chi and Feng Li},
  doi          = {10.1109/TC.2020.2984502},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {57-71},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Algorithms for computing the WCRT bound of OpenMP task systems with conditional branches},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Software-defined design space exploration for an efficient
DNN accelerator architecture. <em>TC</em>, <em>70</em>(1), 45–56. (<a
href="https://doi.org/10.1109/TC.2020.2983694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been shown to outperform conventional machine learning algorithms across a wide range of applications, e.g., image recognition, object detection, robotics, and natural language processing. However, the high computational complexity of DNNs often necessitates extremely fast and efficient hardware. The problem gets worse as the size of neural networks grows exponentially. As a result, customized hardware accelerators have been developed to accelerate DNN processing without sacrificing model accuracy. However, previous accelerator design studies have not fully considered the characteristics of the target applications, which may lead to sub-optimal architecture designs. On the other hand, new DNN models have been developed for better accuracy, but their compatibility with the underlying hardware accelerator is often overlooked. In this article, we propose an application-driven framework for architectural design space exploration of DNN accelerators. This framework is based on a hardware analytical model of individual DNN operations. It models the accelerator design task as a multi-dimensional optimization problem. We demonstrate that it can be efficaciously used in application-driven accelerator architecture design: we use the framework to optimize the accelerator configurations for eight representative DNNs and select the configuration with the highest geometric mean performance. The geometric mean performance improvement of the selected DNN configuration relative to the architectural configuration optimized only for each individual DNN ranges from 12.0 to 117.9 percent. Given a target DNN, the framework can generate efficient accelerator design solutions with optimized performance and area. Furthermore, we explore the opportunity to use the framework for accelerator configuration optimization under simultaneous diverse DNN applications. The framework is also capable of improving neural network models to best fit the underlying hardware resources. We demonstrate that it can be used to analyze the relationship between the operations of the target DNNs and the corresponding accelerator configurations, based on which the DNNs can be tuned for better processing efficiency on the given accelerator without sacrificing accuracy.},
  archive      = {J_TC},
  author       = {Ye Yu and Yingmin Li and Shuai Che and Niraj K. Jha and Weifeng Zhang},
  doi          = {10.1109/TC.2020.2983694},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {45-56},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Software-defined design space exploration for an efficient DNN accelerator architecture},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical resilience analysis of GPGPU applications in the
presence of single- and multi-bit faults. <em>TC</em>, <em>70</em>(1),
30–44. (<a href="https://doi.org/10.1109/TC.2020.2980541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing for a broad range of scientific areas. While GPUs achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can significantly affect the application output quality. Understanding the resilience of general purpose GPU (GPGPU) applications is especially challenging because unlike CPU applications, which are mostly single-threaded, GPGPU applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space in the order of billions, even for some simple applications and even when considering the occurrence of just a single-bit fault. We present a systematic way to progressively prune the fault site space aiming to dramatically reduce the number of fault injections such that assessment for GPGPU application error resilience becomes practical. The key insight behind our proposed methodology stems from the fact that while GPGPU applications spawn a lot of threads, many of them execute the same set of instructions. Therefore, several fault sites are redundant and can be pruned by careful analysis. We identify important features across a set of 10 applications (16 kernels) from Rodinia and Polybench suites and conclude that threads can be primarily classified based on the number of the dynamic instructions they execute. We therefore achieve significant fault site reduction by analyzing only a small subset of threads that are representative of the dynamic instruction behavior (and therefore error resilience behavior) of the GPGPU applications. Further pruning is achieved by identifying the dynamic instruction commonalities (and differences) across code blocks within this representative set of threads, a subset of loop iterations within the representative threads, and a subset of destination register bit positions. The above steps result in a tremendous reduction of fault sites by up to seven orders of magnitude. Yet, this reduced fault site space accurately captures the error resilience profile of GPGPU applications. We show the effectiveness of the proposed progressive pruning technique for a single-bit model and illustrate its application to even more challenging cases with three distinct multi-bit fault models.},
  archive      = {J_TC},
  author       = {Lishan Yang and Bin Nie and Adwait Jog and Evgenia Smirni},
  doi          = {10.1109/TC.2020.2980541},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {30-44},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Practical resilience analysis of GPGPU applications in the presence of single- and multi-bit faults},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HePREM: A predictable execution model for GPU-based
heterogeneous SoCs. <em>TC</em>, <em>70</em>(1), 17–29. (<a
href="https://doi.org/10.1109/TC.2020.2980520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing need for computational power in embedded devices has led to the adoption heterogeneous SoCs combining a general purpose CPU with a data parallel accelerator. These systems rely on a shared main memory (DRAM), which makes them highly susceptible to memory interference. A promising software technique to counter such effects is the Predictable Execution Model (PREM). PREM ensures robustness to interference by separating programs into a sequence of memory and compute phases, and by enforcing a platform-level schedule where only a single processing subsystem is permitted to execute a memory phase at a time. This article demonstrates for the first time how PREM can be applied to heterogeneous SoCs, based on a synchronization technique for memory isolation between CPU and GPU plus a compiler to transform GPU kernels into PREM-compliant codes. For compute bound GPU workloads sharing the DRAM bandwidth 50/50 with the CPU we guarantee near-zero timing varibility at a performance loss of just 59 percent, which is one to two orders of magnitude smaller than the worst case we see for unmodified programs under memory interference.},
  archive      = {J_TC},
  author       = {Björn Forsberg and Luca Benini and Andrea Marongiu},
  doi          = {10.1109/TC.2020.2980520},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {17-29},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HePREM: A predictable execution model for GPU-based heterogeneous SoCs},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent adaptation of hardware knobs for improving
performance and power consumption. <em>TC</em>, <em>70</em>(1), 1–16.
(<a href="https://doi.org/10.1109/TC.2020.2980230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current microprocessors include several knobs to modify the hardware behavior in order to improve performance, power, and energy under different workload demands. An impractical and time consuming offline profiling is needed to evaluate the design space to find the optimal knob configuration. Different knobs are typically configured in a decoupled manner to avoid the time-consuming offline profiling process. This can often lead to underperforming configurations and conflicting decisions that jeopardize system power-performance efficiency. Thus, a dynamic management of the different hardware knobs is necessary to find the knob configuration that maximizes system power-performance efficiency without the burden of offline profiling. In this article, we propose libPRISM, an infrastructure that enables the transparent management of multiple hardware knobs in order to adapt the system to the evolving demands of hardware resources in different workloads. libPRISM can minimize execution time, energy-delay product or power consumption by dynamically managing the SMT level, the data prefetcher, and the DVFS hardware knobs. Overall, the proposed solutions increase performance up to 130 percent (16.9 percent on average), reduce energy-delay product up to 80 percent, and reduce power consumption up to 33 percent depending on the target metric compared to the default knob configuration of the system.},
  archive      = {J_TC},
  author       = {Cristobal Ortega and Lluc Alvarez and Marc Casas and Ramon Bertran and Alper Buyuktosunoglu and Alexandre E. Eichenberger and Pradip Bose and Miquel Moretó},
  doi          = {10.1109/TC.2020.2980230},
  journal      = {IEEE Transactions on Computers},
  number       = {1},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Intelligent adaptation of hardware knobs for improving performance and power consumption},
  volume       = {70},
  year         = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
